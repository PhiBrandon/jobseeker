job_title,company_name,location,job_link,job_id,job_description
Data Engineer,Aristotle,"Provo, UT (Hybrid)",https://www.linkedin.com/jobs/view/3756511926/?eBP=JOB_SEARCH_ORGANIC&refId=9d6yvzdGCVaInRKevoO1FA%3D%3D&trackingId=8PIhSgCor9Wq3TSr%2BiXuWQ%3D%3D&trk=flagship3_search_srp_jobs,3756511926,"About the job
            
 
Aristotle, a leading player in the computer software industry, is currently seeking a talented and driven Data Engineer to join our dynamic team. At Aristotle, we have a deep-rooted belief in the importance of the democratic process, which serves as the foundation of everything we do. We are committed to advancing democracy around the world through innovative software solutions that empower organizations and individuals alike.As a Data Engineer, you'll be an integral part of our mission to revolutionize the way data is utilized. You'll have the opportunity to work and learn in a collaborative environment where your opinions truly matter. We welcome passionate individuals who are dedicated to advancing the democratic process, regardless of their political affiliation. Join us at Aristotle and love what you do while contributing to a greater cause.Aristotle’s Integrity division is a leading provider of identity and age verification services across numerous vertical markets. Our age/identity verification solutions are used by companies to comply with various regulatory requirements such as AML, KYC and Age Verification.Please visit https://integrity.aristotle.com for more information about this division.Responsibilities  Data Load and Transformation: Develop data load processes for efficient storage and retrieval of data from databases and other file systems. Create data conversion and transformation processes and utilities for handling large datasets Solution Development: Utilize .NET/SSIS/SQL Server technologies to design and implement solutions that align with data consumer requirements and adhere to business rules Web-Based Reporting: Build web-based reporting systems to monitor system performance, transaction metrics, and error rates Data Transformation Rules: Collaborate in defining and documenting data transformation rules to ensure data integrity and accuracy ETL Design and Performance: Focus on the design, development, and performance tuning of ETL (Extract, Transform, Load) processes to optimize data processing efficiency Collaborative Development: Work closely with other developers to provide data services to both existing and new applications. This includes modifying production data, creating and optimizing stored procedures, functions, views, and more System Performance Enhancement: Develop and analyze strategies to enhance system performance, ensuring efficient data processing and retrieval Documentation and Testing: Prepare comprehensive documentation and test procedures to ensure the reliability and quality of developed solutions Industry Standard Practices: Develop software using industry-standard programming techniques to maintain code quality and consistency Unit Testing and Debugging: Perform unit testing and debugging of application components to identify and resolve issues promptly
Requirements Bachelor's or Associate's degree in Computer Science or a related fieldMinimum of 2 years of hands-on experience with ETL (Extract, Transform, Load) processesProficiency in T-SQL programming and working with Microsoft SQL Server 2005 and 2008. A deep understanding of the SQL Server Query Processing Engine is requiredKnowledge and experience in designing, developing, debugging, and deploying SQL Server stored procedures, T-SQL scripts, DTS (Data Transformation Services), and SSIS (SQL Server Integration Services) packagesAbility to manage multiple priorities, adhere to project plans, and consistently meet project deliverablesProficiency in Microsoft SQL versions 2000, 2005, and 2008, as well as Microsoft SQL Server Reporting Services. Familiarity with MS Office applications, including Word and ExcelDemonstrated ability to quickly learn and adapt to new technologies as needed
Desired Requirements Proficiency in using Microsoft Visio for creating sequence diagrams, component diagrams, and other UML (Unified Modeling Language) diagramsroficiency in data modeling using tools such as MS Visio or Erwin data modelerFamiliarity with identity verification for fraud, marketing, and risk mitigation solutions within the industryKnowledge of internet technologies, including XML, DHTML, CSS, and JavaScriptFamiliarity with ASP.NET 2.0, C#, and Traditional ASP (Active Server Pages)
This role is located in Provo, Utah. If you live within commuting distance of Provo, Utah or are willing to relocate, please include this in your cover letter. BenefitsAll positions are Full-Time, with competitive compensation, medical benefits, paid vacation, 401k plan and stock options. Casual dress code and a non-corporate atmosphere make this a fun place to work and learn in a team environment. Please visit our website at www.aristotle.com."
Data Engineer-locals,Steneral Consulting,"Chicago, IL (Hybrid)",https://www.linkedin.com/jobs/view/3746290763/?eBP=JOB_SEARCH_ORGANIC&refId=9d6yvzdGCVaInRKevoO1FA%3D%3D&trackingId=Twv3E4%2FgfMN4YjDlkCR5Bg%3D%3D&trk=flagship3_search_srp_jobs,3746290763,"About the job
            
 
Title: Data EngineerLocation: Chicago IL (3 days onsite)Duration: 6 MonthsStatus: Must be USC or GCDescription As a member of Data Engineering teams, this role will develop the enterprise data platform in Azure cloud. It will entail loading of the raw data from SQL and Mainframe data sources and processing them by developing data transformation pipelines using Azure technologies like Terraform, Data Factory, Data Bricks, Azure Functions, PySpark/Python, SQL etc.
Top Three Experience in Data, ETL processing, SQL programming, and exposure to Azure cloudProgramming experience using Python or any object oriented language like Java, C# etc.Ability to learn the Azure tech stack mentioned above and get certified when on the job
AdditionalA Data Engineer will build and operate our enterprise data platform, prepare and deliver curated datasets for operational and analytical use cases. DE will design and provide analytical datasets to citizen data analysts, prepare training data and extract features to accelerate data science model development.In a complex enterprise data environment with numerous systems of record and systems of engagement, the Data Engineer will ensure that the Data Engineering services are delivered in a scalable, reliable, secure, and maintainable manner to support the service levels. The Data Engineer works closely with the Data Architects, System Architects, and Application Architects to align with the Architectural direction of the enterprise and will be required to understand the Architectural landscape and possess the ability to gather and analyze business requirements. In addition, the Data Engineer promotes data sharing across the enterprise while maintaining the accuracy, consistency, integrity, and security of the data sets in the Enterprise Data Repository.Essential Job Duties & Responsibilities Liberate data from source data sources into the enterprise data lake and operational data storesMap data from the source systems into curated entities and their functional viewsImplement master data management strategiesDesign, implement and deploy data pipeline solutions with data lineage capabilities using Azure Data Factory and Databricks.Manage and publish curated data sets.Work with the business to manage data quality. Monitor availability, performance, capacity, continuity, security, and service levels of the enterprise data platform and its servicesProvide subject matter expertise to other technical teams leveraging services from the enterprise data platformDelivering and providing production support for Data Engineering services that meet performance standards and service level agreements.Participating in continually improving processes and procedures for enhancing the efficiency and effectiveness of Data Engineering services to analytic users.Participating in leveraging emerging innovations related to Data Engineering and work closely with Enterprise Architecture and other teams to operationalize themSupporting the Data Engineering department in conducting User Groups and other forums as needed to evangelize and promote the adoption of Data Engineering best practices for accessing and using data sets efficiently, effectively, and securelySupporting enterprise initiatives in building a culture of data-driven decisions to contribute to operational excellence.Participating in the development of Data Engineering practices focusing on long-term sustainability, reuse, and technical debt reduction in the domain of data management.Works with stakeholders including the Executive, Product, Data, and Design teams to assist with data-related technical issues and support their data infrastructure needs.Publishes dashboards that summarize the utilization of data sets in delivering business value impact
Other Duties Supports enterprise architecture function in establishing Policies, Standards, Architecture Patterns, Guidelines, and Best Practices related to Data Engineering discipline. Designs, implements and identifies internal process improvements: automating manual processes, optimizing data delivery, redesigning infrastructure for greater scalability,Supports the Enterprise Data Repository operations as needed in non-production environmentsDevelops tools to automate Enterprise Data Repository operations as needed and liaises with the Service Operations teams that manage production environmentsOther duties as assigned.
Knowledge, Skills & Abilities Expertise in programming, debugging, and testing utilizing various programming languages, tools and technologies related to Data Engineering.Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.Experience working with various operating systems, RDB platforms, NoSQL platforms, Big Data platforms, and environments for managing Data over its full lifecycle.Strong knowledge of how to compose and implement structural data models.Strong knowledge of using data as an enterprise asset – building and managing data marts and data warehousesProven Experience in managing structured data (application data bases, operational data stores, data marts and data warehouses).Experience in managing structured data, unstructured data, and big data at scale on cloud platforms, preferably on Microsoft AzureExperience in managing large-scale storage solutions, preferably using Microsoft Azure.Experience molding fresh environments into cost-efficient, high performance, secure, and mature data platforms.Understanding of data security concepts including encryption at rest and in transitFamiliarity with data visualization techniques and tools.Flexible with a strong sense of urgency.Ability to effectively communicate technical issues both verbally and in writing.Ability to work in an Agile Scrum and Kanban environment.Excellent analytical and problem-solving skills.Experience in successfully managing large-scale data ecosystems preferred.Willingness to use of new software aids and programming techniques that are adopted within IT as needed for the role.
Education And Experience Required Bachelor’s degree in Computer Science or equivalent education.5-7 years of experience in information technology systems or data services delivery3+ years of overall experience in designing and delivering Data Engineering services using programming/scripting languages such as Python, Scala, R, SQL, C#, Java, or U-SQL.1+ year(s) experience in delivering Data Engineering services using a NoSQL platform such as Cosmos DB, Mongo DB, Hadoop, CouchBase, or HDInsightsExperience in using ETL/ELT, Data Quality Management, Meta Data Management, and Master Data Management platforms is highly preferredAzure Data Engineer certification strongly preferredExperience with Microsoft Azure Data Management Platform is strongly preferredFamiliarity with the financial services and/or life insurance industry is preferred."
Data Engineer-locals,Steneral Consulting,"Chicago, IL (Hybrid)",https://www.linkedin.com/jobs/view/3745369544/?eBP=JOB_SEARCH_ORGANIC&refId=9d6yvzdGCVaInRKevoO1FA%3D%3D&trackingId=0KtC2Kw2MahZuaD0ADXD%2Fg%3D%3D&trk=flagship3_search_srp_jobs,3745369544,"About the job
            
 
Title: Data EngineerLocation: Chicago IL (3 days onsite)Duration: 6 MonthsStatus: Must be USC or GCDescription As a member of Data Engineering teams, this role will develop the enterprise data platform in Azure cloud. It will entail loading of the raw data from SQL and Mainframe data sources and processing them by developing data transformation pipelines using Azure technologies like Terraform, Data Factory, Data Bricks, Azure Functions, PySpark/Python, SQL etc.
Top Three Experience in Data, ETL processing, SQL programming, and exposure to Azure cloudProgramming experience using Python or any object oriented language like Java, C# etc.Ability to learn the Azure tech stack mentioned above and get certified when on the job
AdditionalA Data Engineer will build and operate our enterprise data platform, prepare and deliver curated datasets for operational and analytical use cases. DE will design and provide analytical datasets to citizen data analysts, prepare training data and extract features to accelerate data science model development.In a complex enterprise data environment with numerous systems of record and systems of engagement, the Data Engineer will ensure that the Data Engineering services are delivered in a scalable, reliable, secure, and maintainable manner to support the service levels. The Data Engineer works closely with the Data Architects, System Architects, and Application Architects to align with the Architectural direction of the enterprise and will be required to understand the Architectural landscape and possess the ability to gather and analyze business requirements. In addition, the Data Engineer promotes data sharing across the enterprise while maintaining the accuracy, consistency, integrity, and security of the data sets in the Enterprise Data Repository.Essential Job Duties & Responsibilities Liberate data from source data sources into the enterprise data lake and operational data storesMap data from the source systems into curated entities and their functional viewsImplement master data management strategiesDesign, implement and deploy data pipeline solutions with data lineage capabilities using Azure Data Factory and Databricks.Manage and publish curated data sets.Work with the business to manage data quality. Monitor availability, performance, capacity, continuity, security, and service levels of the enterprise data platform and its servicesProvide subject matter expertise to other technical teams leveraging services from the enterprise data platformDelivering and providing production support for Data Engineering services that meet performance standards and service level agreements.Participating in continually improving processes and procedures for enhancing the efficiency and effectiveness of Data Engineering services to analytic users.Participating in leveraging emerging innovations related to Data Engineering and work closely with Enterprise Architecture and other teams to operationalize themSupporting the Data Engineering department in conducting User Groups and other forums as needed to evangelize and promote the adoption of Data Engineering best practices for accessing and using data sets efficiently, effectively, and securelySupporting enterprise initiatives in building a culture of data-driven decisions to contribute to operational excellence.Participating in the development of Data Engineering practices focusing on long-term sustainability, reuse, and technical debt reduction in the domain of data management.Works with stakeholders including the Executive, Product, Data, and Design teams to assist with data-related technical issues and support their data infrastructure needs.Publishes dashboards that summarize the utilization of data sets in delivering business value impact
Other Duties Supports enterprise architecture function in establishing Policies, Standards, Architecture Patterns, Guidelines, and Best Practices related to Data Engineering discipline. Designs, implements and identifies internal process improvements: automating manual processes, optimizing data delivery, redesigning infrastructure for greater scalability,Supports the Enterprise Data Repository operations as needed in non-production environmentsDevelops tools to automate Enterprise Data Repository operations as needed and liaises with the Service Operations teams that manage production environmentsOther duties as assigned.
Knowledge, Skills & Abilities Expertise in programming, debugging, and testing utilizing various programming languages, tools and technologies related to Data Engineering.Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.Experience working with various operating systems, RDB platforms, NoSQL platforms, Big Data platforms, and environments for managing Data over its full lifecycle.Strong knowledge of how to compose and implement structural data models.Strong knowledge of using data as an enterprise asset – building and managing data marts and data warehousesProven Experience in managing structured data (application data bases, operational data stores, data marts and data warehouses).Experience in managing structured data, unstructured data, and big data at scale on cloud platforms, preferably on Microsoft AzureExperience in managing large-scale storage solutions, preferably using Microsoft Azure.Experience molding fresh environments into cost-efficient, high performance, secure, and mature data platforms.Understanding of data security concepts including encryption at rest and in transitFamiliarity with data visualization techniques and tools.Flexible with a strong sense of urgency.Ability to effectively communicate technical issues both verbally and in writing.Ability to work in an Agile Scrum and Kanban environment.Excellent analytical and problem-solving skills.Experience in successfully managing large-scale data ecosystems preferred.Willingness to use of new software aids and programming techniques that are adopted within IT as needed for the role.
Education And Experience Required Bachelor’s degree in Computer Science or equivalent education.5-7 years of experience in information technology systems or data services delivery3+ years of overall experience in designing and delivering Data Engineering services using programming/scripting languages such as Python, Scala, R, SQL, C#, Java, or U-SQL.1+ year(s) experience in delivering Data Engineering services using a NoSQL platform such as Cosmos DB, Mongo DB, Hadoop, CouchBase, or HDInsightsExperience in using ETL/ELT, Data Quality Management, Meta Data Management, and Master Data Management platforms is highly preferredAzure Data Engineer certification strongly preferredExperience with Microsoft Azure Data Management Platform is strongly preferredFamiliarity with the financial services and/or life insurance industry is preferred."
Data Engineer,Steneral Consulting,"Chicago, IL (Hybrid)",https://www.linkedin.com/jobs/view/3748441510/?eBP=JOB_SEARCH_ORGANIC&refId=9d6yvzdGCVaInRKevoO1FA%3D%3D&trackingId=A1CAwoM%2BlLnF27CzKbD%2BVQ%3D%3D&trk=flagship3_search_srp_jobs,3748441510,"About the job
            
 
Title- Data EngineerLocation- Chicago, IL – 1 day per week onsite – hybrid- local onlyLinkedin must.JD-Overview/SummaryThe Product Analytics team at United Airlines is on a transformational journey to unlock the full potential of enterprise data, build a dynamic, diverse, and inclusive culture and develop a modern cloud-based data lake architecture to scale our applications, and drive growth using data and machine learning. Our objective is to enable the enterprise to unleash the potential of data through innovation and agile thinking, and to execute on an effective data strategy to transform business processes, rapidly accelerate time to market and enable insightful decision making.Job Overview And ResponsibilitiesIn this role you will partner with various teams to define and execute data acquisition, storage, transformation, processing and make data actionable for operational and analytics initiatives that create sustainable revenue and share growth. This role requires expertise in Uniteds data sources and technology business intuition, and a working knowledge of data transformation and analytical tools.  Manage data integration and technology projects across the various platforms that comprise Uniteds data strategy Assemble large, complex data sets that meet functional / non-functional business requirements. Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. Build the infrastructure required for optimal extraction, storage, transformation, and loading (ETL) of data from a wide variety of data sources Work with stakeholders including the Senior management, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. Work with data and analytics experts to strive for greater functionality in our data systems. Build and maintain databases, data pipelines, and data warehouses that provide a comprehensive understanding of the business. Execute unit tests and validating expected results to ensure accuracy & integrity of data and applications through analysis, coding, writing clear documentation and problem resolution. Leverage technical and analytical skills to understand and solve business centric questions Coordinate and guide cross-functional projects that involve team members across all areas of the enterprise, vendors, external agencies, and partners Ability to manage multiple deliverables both short and long-term in a busy and aggressive environment, while staying flexible to dynamic needs and priority levels Manage agile development and delivery by collaborating with project manager, product owner and development leads
Required  Bachelor's degree in quantitative field (statistics, software engineering, business analytics, information systems, aviation management or related degree) 5+ years of experience in data engineering or ETL development role Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. Strong analytic skills related to working with structure, semi- structure, and unstructured datasets. Experience with Teradata, SQL server, etc. Experience with AWS cloud services: Redshift, S3, Athena, etc. Experience with Databricks and lakehouse architecture Passionate about solving problems through data and analytics, and creating data products including data models Strong initiative to take ownership of data-focused projects, get involved in the details of validation and testing, as well as provide a business user perspective to their work Ability to communicate complex quantitative concepts in a clear, precise, and actionable manner Proven proficiency with Microsoft Excel and PowerPoint Strong problem-solving skills, using data to tackle problems Outstanding writing, communication, and presentation skills
Preferred  Master's degree Experience with Quantum Metrics and Akamai Experience with languages: Python, R, etc. Strong experience with continuous integration & delivery using Agile methodologies Data engineering experience with transportation/airline industry Strong problem-solving skills"
Data Engineer [NYC],CarbonChain,"New York, United States (Hybrid)",https://www.linkedin.com/jobs/view/3733044947/?eBP=JOB_SEARCH_ORGANIC&refId=9d6yvzdGCVaInRKevoO1FA%3D%3D&trackingId=VXsipG1K%2F4rrysxOUrvAEQ%3D%3D&trk=flagship3_search_srp_jobs,3733044947,"About the job
            
 
Do you want to work on the most pressing problem of our generation?We’re building the infrastructure for the net zero transition, and we’re looking for brilliant builders who want to help define a low carbon future.Decarbonizing the economy requires a granular, real-time view of where emissions come from and how they might be reduced. We build software to automate the carbon footprinting of supply chains. Banks, traders, and manufacturers use our product to tame the complexity of international supply networks, identify the most carbon-intensive parts, and find greener alternatives. Having developed technology which is significant in advance of competitive solutions, we are now investing heavily in market adoption.You can find out more about interviewing at CarbonChain at https://www.carbonchain.com/careers/interview-process .About UsDo you want to work on the most pressing problem of our generation?We’re building the infrastructure for the net zero transition, and we’re looking for brilliant engineers, designers, and data scientists who want to help define a low carbon future.Decarbonizing the economy requires a granular, real-time view of where emissions come from and how they might be reduced. We build software to automate the carbon footprinting of supply chains. Banks, traders, and manufacturers use our product to tame the complexity of international supply networks, identify the most carbon-intensive parts, and find greener alternatives.To join CarbonChain, you’ll be a keen technologist who loves to learn from others. Our company is made up of 10 passionate people with expertise ranging from oil refining to deep learning. Between us we’ve run Amazon’s European supply chain, built JustEat’s corporate meal delivery platform, and monitored industrial emissions with satellites for Al Gore. We’ve got MBAs and PhDs but we know that there’s a lot we don’t know, and we’re hoping you can help fill that gap.Working at CarbonChainWe encourage every member of our team to be the best they can be, whether that’s developing skills through their learning budget or participating in reading groups. Our work requires everybody to straddle the physical and digital worlds, and a growth mindset is essential. One week you might become an expert in the production of copper and figure out how to represent copper electrorefining in Java, whilst the next might be spent modelling the emissions of container ships and visualizing how they compare to LNG tankers in D3.js.Our software has to be clever to represent the insane complexity of real-world supply chains, but clever software is worthless if nobody wants to use it. We’re focused on building intuitive software which minimizes cognitive load. If people love our tools, they’ll use them to make the changes the planet needs.What will you be responsible for at Carbon Chain?We are at the start of our journey and want to build the right things, the right way. We are cloud-first, allowing us to scale at will and experiment with adding new services quickly. As a Senior Data Engineer you’ll play a key role within the company, taking responsibility for our core data infrastructure and its evolution to support the company’s growth. You will lead the design, implementation, and maintenance of a performant and cost efficient data pipelines, as well as contribute to the overall CarbonChain data architecture, with the goal of expanding our data capabilities - feature development, analytics, and ML. You will work closely with cross-functional teams to ensure the delivery of high-quality data sets and features, that empower our key business and product decision makings, and enable CarbonChain’s growth and customer success.You can expect to have:  Ownership of your projects  An independent path to production  The ability to make real changes with tangible business value 
The core of the system relies on a Python application, but we aren’t shy to add other tools that make us ship valuable features better and faster. Other tools comprise Java (Spring Boot), Angular/Typescript, PostgreSQL, Redis, Neo4j, Google Identity, but our cloud-based infrastructure enables us to explore new technologies parallel with our existing platform. We’d love to bring on engineers who come from diverse backgrounds to help us explore new angles of solving business challenges and deliver solutions that wow our customers.Which tools, technologies, and processes will you work with? Containerised applications are the key to our technology vision allowing us to replicate production environments locally and scale services at will.Object-oriented code forms the bulk of our codebase.PostgreSQL, Neo4j and DynamoDB form the persistence layer - you’ll learn to navigate relational, graph, and document databases and appreciate their respective values.Infrastructure automation is owned by the whole team, helping to spread the DevOps mindset across the whole tech department (and beyond!).Testing approaches from unit testing to integration testing are key to delivering reliable and maintainable solutions.Quality assurance is owned by the team themselves - we believe that the ownership of the full development cycle should sit tightly within the team.Agile development, using various approaches from Scrum to Elephant Carpaccio , helps us deliver software in small iterations, learning and course-correcting in the process to make sure that what we deliver has an impact.GitHub PRs are an integral part of our core development flow - with reviews to improve quality and share knowledge.Continuous delivery is the approach we strive for - with metrics that help us optimise parts of the pipeline that need it the most.Teamwork and collaboration are fundamental to the delivery of our solutions. We encourage solutionizing between tech and the wider business.
What we require from applicants  Right to work in the UK and willingness to come to London office 2+ days a week A passion for environmental issuesMinimum of 7 years of experience in data engineering or relevant backend development, with a proven track record of building highly scalable, performant, and reliable data pipelinesExtensive experience with data modelling for large distributed data warehouses and/or lakehouses (or a similar cloud based) solutions, you’re able to discuss in-depth general principles and trade-offs of different modelling approachesExperience in working with “modern data stack” standard data frameworks and adjacent tools – such as Fivetran, Airbyte, BigQuery, Databricks, Snowflake, Apache Spark, Kafka, Airflow, Kubernetes, dbt, etc.Excellent collaboration and communication skills, with a demonstrated ability to work effectively with cross-functional partners, especially with teams outside of the product and engineering groupWorking proficiency in one of the programming languages (Python, Go, Scala etc.)Product mindset to embrace business needs and produce scalable data/engineering solutionsExperience leading technical design discussions and providing guidance on best practices, coding standards, and architecture principlesThe grit and energy to work in an early stage startup
What we’re offering Competitive salary + generous equity packageFlexible working hours - we encourage regular breaks and being AFK (away from keyboard) to support your wellbeingFlexible working location (we like to meet in the office couple of times every week)£1000 annual development allowance for you to spend on developing your current skills and learning new thingsTech equipment of your choiceRegular social activitiesGenerous amount of holidays
We're striving to build a diverse team and we would love to hear from applicants from backgrounds less frequently represented in technology, be that in terms of gender, race, or professional background.If you think your skills and experience match what we’re looking for and you’d like to join a Carbon Tech industry unicorn, please get in touch!If you think your skills and experience match what we’re looking for and you’d like to join a Carbon Tech industry unicorn, please get in touch!"
SSIS Data Engineer,Steneral Consulting,"Appleton, WI (Hybrid)",https://www.linkedin.com/jobs/view/3745150072/?eBP=JOB_SEARCH_ORGANIC&refId=9d6yvzdGCVaInRKevoO1FA%3D%3D&trackingId=qZ0AMcCP%2Bfh8Q3vM30H5Dw%3D%3D&trk=flagship3_search_srp_jobs,3745150072,"About the job
            
 
Onsite once every 2 weeks, must be local to WI/MI/MN/IL who can travel or relocate to Appleton, WINeed valid LinkedInThis is a pure SSIS ETL dev/support person working with the business.PERSONALITY IS MOST IMPORTANT - good team, very interactive and this person will be presenting to high levels of the businessGood-natured and personable team members. Looking for a data engineer who can build strong relationships with the business. Need to know data warehousing, dims, and facts (SSIS) Strong in SQL Queries, joins, triggers, etc. Experience with SQL Server Agent to set up the jobs to run, maintain jobs, and review job execution and failures in SSIS jobs. SSAS would be a plus, not a requirementThe batch process runs overnight, and sometimes needs on-call action, as well as day support.Look into requests against warehouse tables that cause issues. Want to know lineage in a warehouse table to see the root error cause."
"Data Engineer || Chicago, IL",Steneral Consulting,"Chicago, IL (Hybrid)",https://www.linkedin.com/jobs/view/3712855368/?eBP=JOB_SEARCH_ORGANIC&refId=9d6yvzdGCVaInRKevoO1FA%3D%3D&trackingId=EfZzfHI5dVa9kN6kFhiJ1g%3D%3D&trk=flagship3_search_srp_jobs,3712855368,"About the job
            
 
Local candidates onlyShare 2 profiles onlyTitle :Data EngineerLocation : Chicago , IL -- Hybrid 2-3 days onsiteDuration : 6+ monthsInterview process : Two rounds one video and then onsiteNEED LOCALS ONLYNEED DL COPYNEED SSN ( LAST 4 DIGITS )NO VIOP OR GOOGLE VOICE NUMBERSNBEED LINKEDINSkill Set Category: Data EngineerMust haves  Experience with Python & SQLExperience working with large data setsExperience with Azure Databricks and Data FactoryExperience with Apache Spark
Skills 5+ experience hands-on experience in data engineering (developing, maintaining, testing, analyzing and evaluating data.Must have experience with Python and SQLExperience working with large datasetsExperience with loading data using any MPP architectureExperience working with Azure, Databricks and Data FactoryResponsible for building and maintaining the logic and functionality needed to power the user-facing components of a websiteExperience working with algorithms in translating business logic to procedural codeExperience with Big Data preferably Apache SparkStrongly prefer experience with C#"
Lead Data Engineer,"Princeton IT Services, Inc","Fort Washington, PA (Hybrid)",https://www.linkedin.com/jobs/view/3704039689/?eBP=JOB_SEARCH_ORGANIC&refId=sXFJza93p7CggsxCgSymqg%3D%3D&trackingId=dKGQ67bjzTdASBwOj5OFRA%3D%3D&trk=flagship3_search_srp_jobs,3704039689,"About the job
            
 
Position: Lead Data EngineerLocation: Fort Washington, PAPosition Type: Contract to hireResponsibilities: Working with IT and business resources, support the Data Warehouse strategy and Business Intelligence initiatives: Lead development activities to migrate the current on-premises SQL Server DW to Snowflake on AWS.Gather requirements for Data Warehouse improvements and translate them to high level design documents including Physical Data Models. Provide technical architecture vision and recommend strategy/solutions. Design and develop transformation processes and data structures for the Data Warehouse following best practice procedures. Lead resolution of Data Warehouse load issues.Support the analytical needs of the business users. Build strong relationships to help identify opportunities to enhance the analytical capabilities of the Data Warehouse. Partnering with business stakeholders and technical report developers, establish, maintain, and promote consistent methodology for reporting and analytics deployment. Perform ad hoc data analysis to meet business unit data validation needs.Participate in Data Quality initiatives and lead the data transformation component design to improve and maintain high quality data. Support performance tuning.Foster teamwork through cooperative interactions with co-workers. Where needed, ensure project integration by communicating activities and status to project manager, appropriate project team members, and business users.
Job Qualifications Education BS degree, preferably in Information Technology, Management Information Systems, Computer Science or similar discipline. Post-college training in Data Management and vendor tool use Experience 5-8+ years of Data Warehousing/ BI experience.
  Knowledge/Skills- This role serves as the primary technical resource for managing and moving data in and out of the Data Warehouse. The candidate must: Strong SQL skills with SSIS. SnowFlake is an added plus. Be well versed in Data Warehouse design concepts including Kimball and Inmon methodologiesHave hands-on experience with ETL and ELT methodologies and tools. Have experience with SQL Server 2016 and SSIS. Experience with business reporting requirements analysis. Familiarity with Data Management and Business Intelligence tools, such as Business Objects, Microsoft Power BI, QlikSense, Tableau, Looker, AWS S3 and Redshift, or Snowflake.Demonstrated ability to clearly communicate with all levels within an organization.Have experience leading small or offshore development teamsunderstanding and experience with building and deploying Business Intelligence and analytics applications.Familiarity with Financial, Sales, Marketing, and Logistics reporting environments.Strong sense of leadership, strong analytical skills, excellent communication Have expert skills and good technical abilities.

  Preferred candidates will have experience with SSAS. Analytics design and development is preferred.Hands on development experience with .Net is a plus.Comfortable working in a fast-paced team. Capable of grasping new concepts quickly and acting with a sense of urgency. 
Licenses / Certifications Preferred candidate may have certifications similar to: CBIP (Certified Business Intelligence Professional)CDMP (Certified Data Management Professional)

 Vendor-specific certifications"
Sr. Data Engineer,CurbWaste,"Los Angeles, CA (Hybrid)",https://www.linkedin.com/jobs/view/3776466282/?eBP=JOB_SEARCH_ORGANIC&refId=sXFJza93p7CggsxCgSymqg%3D%3D&trackingId=HZFEV5OTsRRx4oY%2BlJSsUQ%3D%3D&trk=flagship3_search_srp_jobs,3776466282,"About the job
            
 
About The RoleWe’re looking for a Sr Data Engineer who will align with our own passion for delivering an innovative, world-class product that makes our customers successful and gives them joy along the way. The ideal candidate has an eye for performance, best practices & industry standards, and takes pride/ownership of whatever initiative they are working on.. In this position the candidate will work closely with the Director of Engineering, Product Managers, and stakeholders to execute on the product roadmap in effort to address business and customer needs.About UsWe are an early stage vertical SaaS business dedicated to bringing the most innovative technology to the historically underserved Waste Management Industry. We care deeply about the hard working, dedicated, and humble people of the Waste Management industry and will stop at nothing to ensure that they get the most value technology can offer. We lean on each other to deliver the best value to our customer and we constantly challenge each other to be the best version of ourselves every day. We settle at nothing short of being the best at what we do.Responsibilities: Design, develop, and maintain scalable and lightning-fast data modelCollaborate with cross-functional teams to optimize performance of systemsTroubleshoot and resolve complex technical issues and identify areas for improvementDatabase administration across all environmentsLead technical team on reporting and analytics strategyGenerate meaning reports with actionable insightsCreate tools for onboarding customer dataAnalyze and organize data. Interpret trends and patternsPrepare data for prescriptive and predictive modelingBuild algorithms and prototypesCommunicate effectively with stakeholders, both technical and non-technical, to ensure project success
RequirementsWhat You Will Need  5+ years of professional software development experience, with a Bachelor’s degree or 3+ years with a Master’s degreeAdvanced working SQL knowledge and experience working with relational databases and familiarity with PostgresExperience with data transformation, data structures, and workload managementTrack record of manipulating, processing and extracting value from large disconnected datasetsKnowledge of queuing, stream processing, and highly scalable ‘big data lakes’Proven experience in taking initiative on large-scale projectsExcellent problem-solving, analytical, and critical thinking skillsStrong written and verbal communication skills, with the ability to effectively articulate complex technical concepts to diverse audiencesFamiliarity with Agile development methodologies and tools (e.g., Scrum, JIRA)
BenefitsWhat We Offer:This is not just a job. This is a career, a chance to make a direct impact. Here’s how we help: Flextime, recognition, and support for autonomous work: Flexible time off with ample learning and development opportunities to continue growing your careerHealth benefits: Company-paid medical, dental, and vision
Our Mission: We aim to change the way waste companies run their business. We are a software founded by haulers and built for haulers. We care about the environment and want to play a positive role in the future of the waste industry. Software helps create solutions and we are focused on being the leaders in change.At CurbWaste we celebrate individuality and uniqueness. We believe that the convergence of fresh perspectives and experiences from all walks of life is what makes our product and culture so great. We strongly encourage people from underrepresented groups to apply. We do not discriminate against employees based on race, color, religion, sex, national origin, gender identity or expression, age, disability, pregnancy (including childbirth, breastfeeding, or related medical condition), genetic information, protected military or veteran status, sexual orientation, or any other characteristic protected by applicable federal, state or local laws."
"Data Engineer with Netsuite at Houston, Texas, Hybrid",Steneral Consulting,"Houston, TX (Hybrid)",https://www.linkedin.com/jobs/view/3682449785/?eBP=JOB_SEARCH_ORGANIC&refId=sXFJza93p7CggsxCgSymqg%3D%3D&trackingId=cnBAHOT5bPw74N3cQKyyww%3D%3D&trk=flagship3_search_srp_jobs,3682449785,"About the job
            
 
Must be local DLShare only 2 profiles onlyUrgently seeking Houston based Data Migration Engineers for a 4 - 5 month consulting project. Selected individual will be migrating financial data and reports from a legacy system over to Netsuite.Required: Netsuite, SQL, Excel and ability to sit onsite 2-3 days a week at a minimum.Location is Houston, Texas they must be localThey must have Netsuite -if they don't, we can't submit. They must be local to Houston or we can't submit.Data Migration Engineer with Netsuite (required)Urgently seeking Houston based Data Migration Engineers for a 4 - 5 month consulting project. Selected individual will be migrating financial data and reports from a legacy system over to Netsuite.Required: Netsuite, SQL, Excel and ability to sit onsite 3+ days a week at a minimum."
Google Databrick Data Engineer - 10+ years,TekIntegral,"Tampa, FL (Hybrid)",https://www.linkedin.com/jobs/view/3687051162/?eBP=JOB_SEARCH_ORGANIC&refId=sXFJza93p7CggsxCgSymqg%3D%3D&trackingId=XftQAiqW4R5wt5yaNUMMNQ%3D%3D&trk=flagship3_search_srp_jobs,3687051162,"About the job
            
 
Title: Google Databrick Data EngineerLocation: Candidates will work Hybrid from their location (Texas, Florida, Georgia, Chicago, Pennsylvania, etc. etc.)Please don't submit candidates from NY or CADuration: Long term contractWork Auth: USC/GCLinkedIn: Yes Must have both Google (recently) and Databricks(extensive)
They'll need to have a Github demonstrating their history of workRequired Skills (Very Hands-on) Google Cloud (Big Query, Data Proc, etc.)Google Cloud Functions(Java, Python, Go, Node. js, and Rust)Databricks (Python, PySpark, Scala, Airflow)"
Data QA Engineer -locals,Steneral Consulting,"Spring, TX (Hybrid)",https://www.linkedin.com/jobs/view/3762496226/?eBP=JOB_SEARCH_ORGANIC&refId=sXFJza93p7CggsxCgSymqg%3D%3D&trackingId=aUNxJ3tVaGIfP%2F1i4nAPlQ%3D%3D&trk=flagship3_search_srp_jobs,3762496226,"About the job
            
 
Spring TXUSC or GC Holder12 month contract Hybrid (onsite Tuesday – Thursday)Top 3 Skills Experience extracting from user storiesSQL (needs to be a 3-5 out of 10)Experience working in a continuous integration team – must know the principlesMust have tested data warehouses
Data QA Engineer The Data and Analytics team at is seeking a skilled and detail-oriented Quality Engineer with expertise in ETL/ELT data pipelines to join our dynamic team. As a Data QA Engineer, you will play a critical role in ensuring that data processed into our data lakehouse is of high quality and reliability, and that we avoid functional and regression defects. You will collaborate closely with cross-functional teams, including product managers, data engineers, BI developers and data scientists, to identify, report, and help resolve issues in our data-driven applications.Responsibilities  Utilize Azure DevOps for test case management and issue/defect tracking. Write clear and concise defect details describing actual versus expected behaviors. Identify and implement automated test cases required for Azure DevOps user stories and/or defects based on ticket descriptions, ensuring accuracy of D&A’s ETL/ELT pipelines. Conduct thorough manual and automated testing of data pipelines and transformations to identify defects and inconsistencies. Collaborate with the development and product management teams to understand complex data requirements, transformations, and analytical processes, providing early feedback on potential issues. Continuously improve testing processes, methodologies, and tools related to data quality, analytics testing, and Azure DevOps integration. Perform regression testing of data-related components to validate bug fixes and new features across different releases.
Qualifications  Strong understanding of software testing methodologies, tools, and processes, with a specific focus on data validation, analytics testing, and Azure DevOps integration. 5 years of experience as a QA Engineer in writing clear and concise test cases and test documentation for software applications"
"Position - Data Engineer: III (Senior) - Dallas, TX (Only Local Candidates on W2)",Biogensys,"Dallas, TX (Hybrid)",https://www.linkedin.com/jobs/view/3649285515/?eBP=JOB_SEARCH_ORGANIC&refId=sXFJza93p7CggsxCgSymqg%3D%3D&trackingId=xp%2BMIURPv%2BHh0d1hlzREBQ%3D%3D&trk=flagship3_search_srp_jobs,3649285515,"About the job
            
 
We are hiring Data Engineer: III (Senior) for one of our clients in Dallas, TX.Job Description This role will be part of a team focused on cloud transformation, modernizing analytics platforms and improving agility.The role requires hands-on experience in building and managing analytics solutions in Snowflake, Provide direction on adoption of Cloud technologies (Snowflake) and industry best practices in the field of Data Engineering architecture and Development.
Primary Duties And Responsibilities Establish Data Engineering architecture strategy, best practices, standards, and roadmap.Experience developing ETL Pipeline using Python, Snowflake and IDMC.Experience with loading batch data and streaming data via KafkaBuild Data Flows mapping Source systems and Process flows.Assemble large, complex data sets that meet nonfunctional and functional business requirements.Mentor team members on best practices, efficient implementations and delivering high quality data products.Lead onshore and offshore teams.Perform code reviews and assist developers in optimization and troubleshooting.Expertise in Snowflake advanced concepts like setting up resource monitors, RBAC controls, virtual warehouse sizing, query performance tuning, Zero copy clone, time travel and understand how to use these features.Knowledge in AWS and management technologies such as S3.Strong written communication skillsIs effective and persuasive in both written and oral communication.
Experience Required 5-10 years of experience within data engineeringPython experienceSnowflake experience (developing ETL pipelines)IDMC (intelligent data management cloud) experienceKnowledge of AWS
Preferred Experience Kafka experience (loading batch and streaming data)City: DallasState: TXBackground Check: YesFace to face interview required: NoCandidate must be local: NoCandidate must be your W2 Employee: Yes
Additional Information Location: Dallas, TX (hybrid- 2 days onsite)Duration: 6 Months (with possible extension)
About UsWe are specialized in recruiting and deliver the best professional talent of industry and we are committed to deliver best experience for our clients and job seekers. With over two decades of experience in the recruitment industry, we proudly help you to find the next job that matches your professional skills. Our team understands your needs or requirement before starting the recruitment, that enables to find the high quality of talent with high success rate of talent delivery, keeps us continue to be the best in the industry."
Data Engineer,NR Consulting,"New York, NY (Hybrid)",https://www.linkedin.com/jobs/view/3768016737/?eBP=JOB_SEARCH_ORGANIC&refId=sXFJza93p7CggsxCgSymqg%3D%3D&trackingId=UT3RbtTM25YOojdUJeRibw%3D%3D&trk=flagship3_search_srp_jobs,3768016737,"About the job
            
 
Data Engineer SMELocation: NYC / Charlotte (Candidate should be open for hybrid work model)Duration: 12 monthsResponsibilitiesA background in financial services, investment banking, regulated sectors and large enterpriseDevOps and Agile engineering practitioner with experience of test driven developmentAbility to work independently on specialized assignments within the context of project deliverablesTake ownership of providing solutions and tools that iteratively increase engineering efficiencies.Design should help embed standard processes, systems and operational models into the BAU approach for end-to-end execution of Data PipelinesExcellent communication skills & team player and collaboratorExpert knowledge of Industry Best Practice for ETL Design, Principles, Concepts.Working with Process Re-engineering and Operating ModelsStrong Stakeholder Management Skills And Stakeholder Governance ExperienceWorking with diverse, cross-functional teams located locally and globally, and the ability to quickly develop and nurture strong working relationshipsPreferredHands-on experience with or knowledge of Axiom data platformHave Snowflake SnowPro certifiedSnowflake Advanced TrainingAzure/AWS or GCP Data EngineerAzure/AWS or GCP Data Architect"
AWS Cloud Data Engineer,DMVTEK,"Jersey City, NJ (Hybrid)",https://www.linkedin.com/jobs/view/3721469195/?eBP=JOB_SEARCH_ORGANIC&refId=sXFJza93p7CggsxCgSymqg%3D%3D&trackingId=HatEJhy02cnAAn49sC1CDA%3D%3D&trk=flagship3_search_srp_jobs,3721469195,"About the job
            
 
Job Title: AWS Cloud Data Engineer Location : Jersey City, NJ (Hybrid)Duration: Long Term Responsibilities 12+ years of professional experience as Software Engineer, Applications Developer, Data Engineer or similar experienceEngage in Event streaming design development using, Apache Flink, AWS KDA, KafkaDevelop technical specification documents and generic/reusable frameworks Public cloud-AWS.Design and Develop stateless and stateful transformations using Apache FlinkDesign and Develop the streaming pipelines with a various sink points like MSK, S3, Rest API.Metric collection and Dash board integration with ,Prometheus, CloudWatch, datadog.Design and develop reconciliation and reprocessing the data and ensure zero data loss.Identify/troubleshoot application code-related issues and review and provide feedback to the final user.BS/BA degree or equivalent experience. Expertise in application, data, and infrastructure architecture disciplinesAdvanced knowledge of architecture and design across all systems. Proficiency in multiple modern programming languages"
"Data Engineer Databricks, Rest API & Pyspark - locals || Holmdel, NJ",Steneral Consulting,"Holmdel, NJ (Hybrid)",https://www.linkedin.com/jobs/view/3640735415/?eBP=JOB_SEARCH_ORGANIC&refId=sXFJza93p7CggsxCgSymqg%3D%3D&trackingId=T0Va45AZz%2B2kZLbRAv6YGg%3D%3D&trk=flagship3_search_srp_jobs,3640735415,"About the job
            
 
Data Engineer Databricks, Rest API & Pyspark - 6 month + contract to perm Published DescriptionDatabricks, Rest API & PysparkJob Title: Data EngineerLocation: This is a hybrid position. The resource will be required to work onsite at one of the Company offices two days per week. Office locations include New York, NY, Holmdel, NJ or Bethlehem, PA.Duration: This is a Contract to Hire role.Job DescriptionCompany is seeking an experienced Data Engineer to be part of our Data and Analytics organization. You will be playing a key role in building and delivering best-in-class data and analytics solutions aimed at creating value and impact for the organization and our customers. As a member of the data engineering team, you will help developing and delivery of Data Products with quality backed by best-in-class engineering. You will collaborate with analytics partners, business partners and IT partners to enable the solutions.You WillArchitect, build, and maintain scalable data & analytics pipelines for machine learning models, reports, dashboard, and other analytics solutions.Design, develop and implement low-latency, high-availability, and performant data applications and recommend & implement innovative engineering solutions.Design, develop, test and debug code in Java, Python, and other languages as per Company standards to improve the business processes.Design and develop code to create reliable and scalable data pipelines which can support various types of consumers including products like Customer Data Platform.Build and maintain integrations with different SaaS tools as required to activate customer centric data and insights.Apply and provide guidance on software engineering techniques like design patterns, code refactoring, framework design, code reusability, code versioning, performance optimization, and continuous build and Integration (CI/CD) to make the data analytics team robust and efficient.Performing all job functions consistent with Company policies and procedures, including those which govern handling PHI and PII.Collaborate with cross-functional teams to identify highest value activities for Advanced Data and Analytics group.Work closely with various IT and business teams to understand systems opportunities and constraints for maximally utilizing Company Enterprise Data Infrastructure.Develop relationships with business team members by being proactive, displaying an increasing understanding of the business processes and by recommending innovative solutions.Communicate project output in terms of customer value, business objectives, and product opportunity.You Have5+ years of experience with Bachelors / master's degree in computer science, Engineering, Applied mathematics or related field.Extensive hands-on development experience in one or more of programming languages like Java or Python and familiarity with SQL and bash.Extensive experience in all stages of software development and expertise in applying software engineering best practices.Familiarity with building and deploying scalable data pipelines to develop and deploy Data Solutions using Python, SQL, PySpark, Java.Familiar in designing and developing backend RESTful Webservices (APIs) using Microservices architecture to generate JSON/XML response using Java spring or Python.Experience in developing APIs using Python or Java to operationalize Machine Learning models and data assets.Experience with any of the backend frameworks like Spring Integration, Spring MVC etc.Experience in integrating database with backend layer using Spring DAO or Python equivalent JPA frameworks.Familiarity with API Gateways like APIGEE to secure webservice endpoints.Familiarity with concurrency and parallelism.Extensive Experience in SQL query and optimization.Familiarity with Data pipelines and ML development cycle.Experience in creating and configuring continuous integration/continuous deployment using pipelines to build and deploy applications in various environments and use best practices for DevOps to migrate code to Production environment.Familiarity with creating and modifying tables to store data in OLAP database like Hive and utilize data warehouse platforms like AWS Redshift, Databricks.Ability to investigate and repair application defects regardless of component: front-end, business logic, middleware, or database to improve code quality, consistency, delays and identify any bottlenecks or gaps in the implementation.Experience with unit testing frameworks like Mockito, Powermock and Junit for test driven development approach for logic implemented in Java. Ability to write unit tests in python using unit test library like pytest.Additional Qualifications (nice To Have)Have experience with building Customer 360 solution either in data warehouse or using products like Customer Data Platform.Experience in building data pipelines supporting customer centric marketing use cases and experience building integrations with marketing and engagement tools to activate the customer insights.Familiarity with identity resolution concept and familiarity with either building custom identity resolution solutions or using identity resolution product."
Founding Data Engineer,Parker,"New York, NY (Hybrid)",https://www.linkedin.com/jobs/view/3625993018/?eBP=JOB_SEARCH_ORGANIC&refId=sXFJza93p7CggsxCgSymqg%3D%3D&trackingId=bos1EteyXFqR6Y6hhqWHaQ%3D%3D&trk=flagship3_search_srp_jobs,3625993018,"About the job
            
 
Parker's mission is to increase the number of financially independent people. We believe we can achieve this goal by building tools that enable independent business owners to scale their businesses profitably. Our first product combines a virtual credit card system with dynamic spending limits and software tooling to help merchants grow and optimize their profitability.We are growing very fast -- in less than five months, we grew to millions in card volume. We have a significant waitlist of customers waiting to use our product. We are looking to expand our headcount quickly to support the demand. Our investors include Solomon Hykes (founder of Docker), Paul Buchheit (founder of Gmail), Paul Graham (founder of Y Combinator), Robert Leshner (founder of compound.finance), and many more. We have raised over $30M from top-tier fintech investors.What you'll be working onYou'll be involved in projects of varying scope and complexity: * Build credit risk models that segment merchants based on their revenue and spend patterns to offer dynamic credit limits that change with business performance given the high seasonality and fast pace of ecommerce * Use machine learning tools to build realtime credit underwriting models leveraging alternative and traditional data sourcesWhat You'll Need Passion for, or curiosity to learn, financial technologyTrack record of high-quality shipping products and features at scaleAbility to turn business and product ideas into engineering solutionsDesire to work in a fast-paced environment, continuously grow and master your craft
What we’d like to see Experience working with credit risk modelingExperience working with data analytics, algorithmic decision making, and real-time data systemsProven experience and subject matter expertise in e-commerce payments (nice to have) or financial services"
"Contract || Data Movement Systems Engineer || Jersey City, NJ/ Boston, MA (Local)",Steneral Consulting,"Boston, MA (Hybrid)",https://www.linkedin.com/jobs/view/3714145046/?eBP=JOB_SEARCH_ORGANIC&refId=sXFJza93p7CggsxCgSymqg%3D%3D&trackingId=i6dBsBI89ngakn4Rk992Ag%3D%3D&trk=flagship3_search_srp_jobs,3714145046,"About the job
            
 
Hi,Hope you are doing great,Job Title: Data Movement Systems EngineerLocation: Jersey City, NJ/ Boston, MA (Hybrid)Duration: 6-9 MonthsJob Description3 Days onsite Tues-Thurs2 Location: Jersey City, NJ or Boston, MAMust be local to either of the locationsKey skills FTP/SFTP connections/ Strong experience in Managed File Transfer Capabilities 8+ years experienceMomentum is the software package Client uses from- Broadriver Systems : Other like applications that you could look for in experience: MoveIT Sterling AxWay
The Data Movement Technology Specialist will work directly with the Data Movement Technology Department Head, Data Movement Technology Operations Manager and team members to provide technical expertise over the products that service our Managed File Transfer and Fax operations.This individual contributor position will provide strategic insight and execute improvements into the technology used to eliminate inefficiencies including process flow, surround technologies, monitoring of health of mission critical Managed File Transfer and Enterprise Faxing platforms, troubleshooting application and systems issues, assist in providing global 24x7 tech support, managing platform onboarding processes, reviewing technical hardware and software upgrades needed, and supporting annual BCP/DR test events.Principle Responsibilities Review and recommend effective work processes for the day to day of Data Movement Technology operationsDesign, build and maintenance of Enterprise Managed File Transfer middleware infrastructureCreate of Standards for infrastructure aspects of Enterprise Managed File transfer Middleware such as High Availability, performance tuning, troubleshooting.Responsible for Automation with Powershell, Linux to automate day to day tasks such as deployment,Utilize monitoring tools ( ie Splunk) to develop effective monitoring of incidents and failures and systems health checks.Provide documentation for both production and disaster recovery procedures as well as take part in regular disaster recovery and business continuity tests.Provides support to the operations teams on systems incidents and maintains written resolutions to frequent problems as they relate to Managed File Transfer processes in use within the firm
Required Knowledge, Skills & Abilities Bachelor's degree in Management of Information Systems, Computer Science, Information Technology, and/or equivalent work experience.At least 8+ years of experience specific to Middleware Managed File Transfer productsClear understandings on communication protocols like FTP/SFTP/HTTPS/AS2/PR4.Strong working knowledge of Networking and Infrastructure capabilities and interactions with Managed File Transfer ProcessesExperience with Configuring end to end setups installations, upgrades, patching and troubleshooting issues.Knowledge of Windows and MS Office product suites.Strong Financial services industry knowledge.Excellent written and communication skills.Experience in troubleshooting network connectivity issues.Understanding on Oracle database.Experience with Linux/Windows Operating systems.Experience on MFT application migration process.Experience with scripting and orchestration - Ansible / Python / Shell / PowershellAbility to multitask, manage high-priority initiatives and coordinate the activities with Client Service and Engineering teams to completion.Ability to work in a team setting with a diverse group: operations support resources, client services representatives, application and service delivery teams.Experience in handling SSL certificates renewal and troubleshooting certificate issues.Familiarity with observability tools like, Splunk, etc.Analyze application performance, perform turning and ensure high availability & stability of platform.Experience working on Job scheduling tools like Autosys.Must be able to communicate and effectively work with customers, clients, stakeholders, Internal line of business and IT teams.Must be able to convey both technical and non-technical concepts to wide audience.Must have strong sense of urgency and customer service focus.Provide 24/7 Level 3 support for file transfer issues and ensure all business critical files are transmitted within pre-defined SLA's.
Kumar HarshitAssociate Team Lead-Talent Acquisition -North America"
Data QA Engineer -locals,Steneral Consulting,"Spring, TX (Hybrid)",https://www.linkedin.com/jobs/view/3759316531/?eBP=JOB_SEARCH_ORGANIC&refId=sXFJza93p7CggsxCgSymqg%3D%3D&trackingId=RB7GvZDsXuLRDpvrodDjZA%3D%3D&trk=flagship3_search_srp_jobs,3759316531,"About the job
            
 
Spring TXUSC or GC Holder12 month contract Hybrid (onsite Tuesday – Thursday)Data QA Engineer The Data and Analytics team at is seeking a skilled and detail-oriented Quality Engineer with expertise in ETL/ELT data pipelines to join our dynamic team. As a Data QA Engineer, you will play a critical role in ensuring that data processed into our data lakehouse is of high quality and reliability, and that we avoid functional and regression defects. You will collaborate closely with cross-functional teams, including product managers, data engineers, BI developers and data scientists, to identify, report, and help resolve issues in our data-driven applications.Responsibilities  Utilize Azure DevOps for test case management and issue/defect tracking. Write clear and concise defect details describing actual versus expected behaviors. Identify and implement automated test cases required for Azure DevOps user stories and/or defects based on ticket descriptions, ensuring accuracy of D&A’s ETL/ELT pipelines. Conduct thorough manual and automated testing of data pipelines and transformations to identify defects and inconsistencies. Collaborate with the development and product management teams to understand complex data requirements, transformations, and analytical processes, providing early feedback on potential issues. Continuously improve testing processes, methodologies, and tools related to data quality, analytics testing, and Azure DevOps integration. Perform regression testing of data-related components to validate bug fixes and new features across different releases.
Qualifications  Strong understanding of software testing methodologies, tools, and processes, with a specific focus on data validation, analytics testing, and Azure DevOps integration. 5 years of experience as a QA Engineer in writing clear and concise test cases and test documentation for software applications"
Senior Data Engineer - W2 / 1099 only,Enexus Global Inc.,"Sunnyvale, CA (Hybrid)",https://www.linkedin.com/jobs/view/3747901038/?eBP=JOB_SEARCH_ORGANIC&refId=sXFJza93p7CggsxCgSymqg%3D%3D&trackingId=i%2B%2FyjQaKwb5f6EIOxHCj6A%3D%3D&trk=flagship3_search_srp_jobs,3747901038,"About the job
            
 
Role - Senior Data EngineerLocation - Sunnyvale, CA (hybrid)Contract Type - W2/1099 onlyMinimum Experience - 11+ YearsResponsibilities Develop and enhance data-processing, orchestration, monitoring, and more by leveraging popular open-source software, AWS, and GitLab automation.Collaborate with product and technology teams to design and validate the capabilities of the data platformIdentify, design, and implement process improvements: automating manual processes, optimizing for usability, re-designing for greater scalabilityProvide technical support and usage guidance to the users of our platform’s services.Drive the creation and refinement of metrics, monitoring, and alerting mechanisms to give us the visibility we need into our production services.
Qualifications Experience building and optimizing data pipelines in a distributed environmentExperience supporting and working with cross-functional teamsProficiency working in Linux environment8+ years of advanced working knowledge of SQL, Python, and PySpark5+ years of experience with using a broad range of AWS technologiesExperience using tools such as: Git/Bitbucket, Jenkins/CodeBuild, CodePipelineExperience with platform monitoring and alerts tools"
Data Engineer / FinTech,Motion Recruitment,"Phoenix, AZ (Hybrid)",https://www.linkedin.com/jobs/view/3762366253/?eBP=JOB_SEARCH_ORGANIC&refId=sXFJza93p7CggsxCgSymqg%3D%3D&trackingId=kG2ppfDgDjx4S%2FYSUJQ6hw%3D%3D&trk=flagship3_search_srp_jobs,3762366253,"About the job
            
 
Big Data EngineerA client of ours in the financial space is looking to hire a Big Data Engineer to join their team. You will be responsible for developing and designing software applications as well as modifying existing applications to meet business requirements. This will be a hybrid role with the expectation to go onsite 1-2 days a week in North Phoenix.Required Skills & Experience 5+ years of software development experience 3+ years of experience with Map-Reduce, Hive, Spark Hands-on experience writing and understanding complex SQL Experience in UNIX shell-scripting Bachelor’s degree in Engineering or Computer Science or equivalent 
What You Will Be DoingTech Breakdown 100% Data Engineering 
Daily Responsibilities 100% Hands On 
Applicants must be currently authorized to work in the US on a full-time basis now and in the future. This role cannot be done on a C2C basis.Posted By: Julie Bennett"
Data Engineer,Motion Recruitment,"Irving, TX (Hybrid)",https://www.linkedin.com/jobs/view/3755176171/?eBP=JOB_SEARCH_ORGANIC&refId=sXFJza93p7CggsxCgSymqg%3D%3D&trackingId=cGE9tuSXgyszrvC38ITosw%3D%3D&trk=flagship3_search_srp_jobs,3755176171,"About the job
            
 
Grow your career with an innovative global bank working in Irving, TX as a Data Analyst with AI exposure. Contract role with strong possibility of extension. Will require working a hybrid schedule 2-3 days onsite per week.Join one of the world's most renowned global banks and trusted brand with over 200 years of continuously evolving financial services worldwide. Will contribute to the development of new data analytic techniques, report generation and the improvement of processes and workflow for the area or function. You will work alongside some of the smartest minds in the industry who are excited to share their knowledge and to learn from you.Contract Duration: 4+ MonthsRequired Skills & Experience Experience architecting complex visualizations into Java applications. Bachelor's degree in Management Information Systems, Data Analytics, Data Science, Information Design, Computer Science. Techniques to extract and transform data from various sources such as MS Excel, SharePoint lists, MS SQL, Oracle, Service Now, Cognos, Ab Initio console, Autosys console. Ability to define and implement data querying expressions and techniques using Tableau, MS Excel Power Query, MS Excel Power Pivot, Power BI. Ability to organize and relate significant amounts of raw data (with attention to detail & accuracy) into data models. Ability to write data analytic expressions using formula languages. Experience in visual information design and interactive visualization development. Working to advanced knowledge in presentation frameworks such as MS PowerPoint. Analytical skills that allow for the development of data-driven reports. Experience in visual information design and interactive visualization development with expertise in representing complex information in an actionable and high impact way. Strong communications and presentation skills applicable to developing executive presentations, succinct storytelling, and ability to make the complex simple. Ability to rapidly iterate though conceptual ideas or designs. Strong analytical, problem solving and planning skills. Proficient in grasping complex technical concepts and articulate them to different audiences. 
Desired Skills & Experience Prefer any experience with integrating AI models to provide predictive and prescriptive analytics. 
What You Will Be Doing Apply AI and machine learning techniques (through collaboration with AI/ML team) to enhance data analysis and insights within Tableau dashboards. Integrate AI models to provide predictive and prescriptive analytics. Collaborate with Data Scientists to deploy AI models into Tableau dashboards. Work across functional teams to define and develop analytics and reporting requirements. Identify data sources that will help meet the analytic & reporting goals. Simplify and enhanced data by developing data transformation queries which normalize, combine, and standardize various data sets. Create data models that build relationships between various data sets. Define key metrics that measure strategic goals, provide strategic insights, interpret performance trends, and support data-based performance improvements. Develop data analytics expressions to produce key analytic metrics which will be used in data visualization. Create standard and custom visualizations while ensuring data validity, statistical integrity, and maximizing impact. Create dashboards using data expressions & visualizations which are aligned to strategic goals and operational initiatives. Develop and implement a robust reporting suite enabling management to make objective, data-driven business decisions. Develop and design presentations to convey complex information in clear graphics to tell the story through clean, concise, and well-organized dashboards. Distill information provided by subject matter experts and other teams into executive-level presentations and strategic insights. Revise and edit reports, presentation materials, procedures, and other documents. Provide agenda, meeting attendance, key meeting decisions and next steps/follow-ups for stakeholder meetings. Organize follow-ups and key actions for traceability and impact. Develop in Tableau to architect various complex visualization based reporting dashboards and integrate into Java applications. Collaborate with engineering/architecture practice team to innovate, develop and deploy Tableau/Java solutions. 
Posted By: Melissa Klein"
Senior Data Engineer,Nike,"Beaverton, OR (Hybrid)",https://www.linkedin.com/jobs/view/3693914347/?eBP=JOB_SEARCH_ORGANIC&refId=sXFJza93p7CggsxCgSymqg%3D%3D&trackingId=BAAYk8HH2frxjopATiAbUQ%3D%3D&trk=flagship3_search_srp_jobs,3693914347,"About the job
            
 
Work options: HybridHybridTitle: Business System Analyst (Tech)Location: Hybrid - Beaverton, ORDuration: 10 Month ContractBecome a Part of the NIKE, Inc. TeamNIKE, Inc. does more than outfit the world's best athletes. It is a place to explore potential, obliterate boundaries and push out the edges of what can be. The company looks for people who can grow, think, dream and create. Its culture thrives by embracing diversity and rewarding imagination. The brand seeks achievers, leaders and visionaries. At Nike, it’s about each person bringing skills and passion to a challenging and constantly evolving game.Nike has embraced big data technologies to enable data-driven decisions. We’re looking to expand our Data Engineering team to keep pace. As a Data Engineer, you will work with a variety of talented Nike teammates and be a driving force for building first-class solutions for Nike Technology and its business partners, working on development projects related to supply chain, commerce, consumer behavior and web analytics among others.Role Responsibilities Contribute to Design and implement data products and features in collaboration with product owners, data analysts, and business partners using Agile / Scrum methodologyContribute to overall architecture, frameworks, and patterns for processing and storing large data volumesContribute to the evaluation of new technologies/tools/frameworks centered around high-volume data processingTranslate product backlog items into logical units of work in engineeringImplement distributed data processing pipelines using tools and languages prevalent in the big data ecosystemBuild utilities, user-defined functions, libraries, and frameworks to better enable data flow patterns Work with engineering leads and other teams to ensure quality solutions are implemented, and engineering best practices are defined and followedBuild and incorporate automated unit tests and participate in integration testing efforts Utilize and advance continuous integration and deployment frameworksTroubleshoot data issues and perform root cause analysisWork across teams to resolve operational & performance issues
The following qualifications and technical skills will position you well for this role: Bachelor’s degree in Computer Science, or related technical discipline2+ years of experience in large-scale software development, 1+ years of big data experienceProgramming experience, Python or Scala preferred.Experience working with Hadoop and related processing frameworks such as Spark, Hive, etc.Experience with messaging/streaming/complex event processing tooling and frameworksExperience with data warehousing concepts, SQL and SQL Analytical functionsExperience with workflow orchestration tools like Apache AirflowExperience with source code control tools like Github or BitbucketAbility to communicate effectively, both verbally and written, with team membersInterest in and ability to quickly pick up new languages, technologies, and frameworksExperience in Agile/Scrum application development
The following skills and experience are also relevant to our overall environment, and nice to have: Experience with JavaExperience working in a public cloud environment, particularly AWSExperience with cloud warehouse tools like SnowflakeExperience working with NoSQL data stores such as HBase, DynamoDB, etc. Experience building RESTful API’s to enable data consumptionExperience with build tools such as Terraform or CloudFormation and automation tools such as Jenkins or Circle CIExperience with practices like Continuous Development, Continuous Integration and Automated Testing
These Are The Characteristics That We Strive For In Our Own Work. We Would Love To Hear From Candidates Who Embody The Same Desire to work collaboratively with your teammates to come up with the best solution to a problemDemonstrated experience and ability to deliver results on multiple projects in a fast-paced, agile environmentExcellent problem-solving and interpersonal communication skillsStrong desire to learn and share knowledge with others
NIKE, Inc. is a growth company that looks for team members to grow with it. Nike offers a generous total rewards package, casual work environment, a diverse and inclusive culture, and an electric atmosphere for professional development. No matter thelocation, or the role, every Nike employee shares one galvanizing mission: To bring inspiration and innovation to every athlete* in the world. NIKE, Inc. is committed to employing a diverse workforce. Qualified applicants will receive consideration without regard to race, color, religion, sex, national origin, age, sexual orientation, gender identity, gender expression, veteran status, or disability."
Job Opportunity - Data Engineer with Python Coding.,"Donato Technologies, Inc.","Austin, TX (Hybrid)",https://www.linkedin.com/jobs/view/3766669358/?eBP=JOB_SEARCH_ORGANIC&refId=sXFJza93p7CggsxCgSymqg%3D%3D&trackingId=N%2BOcw3YqDXTtvgD71F3JIQ%3D%3D&trk=flagship3_search_srp_jobs,3766669358,"About the job
            
 
Job Title: Data Engineer with Python Coding.Location: Austin, TX or Bay Area, CA (Hybrid 3 days onsite).Duration: 12+ Months.Ex-Facebook consultant is mandatoryJob Description 10+ years of professional work experience designing and implementing data pipelines in a cloud environment is required.5+ years of experience migrating/developing data solutions in the AWS cloud is required.5+ years of experience building/implementing data pipelines using Databricks or similar cloud database.Expert level knowledge of using SQL to write complex, highly optimized queries across large volumes of data.5+ years hands-on object-oriented programming experience using Python is required."
Urgent requirement of Data Engineer (Local to Ohio) - Hybrid,IntelliX Software Inc,"Cleveland, OH (Hybrid)",https://www.linkedin.com/jobs/view/3718990735/?eBP=JOB_SEARCH_ORGANIC&refId=sXFJza93p7CggsxCgSymqg%3D%3D&trackingId=OvOU0MZFpkD0rWZHjFVtBw%3D%3D&trk=flagship3_search_srp_jobs,3718990735,"About the job
            
 
Data Engineer (Local to Ohio)Duration: Long Term (W2 Prefer)The position is hybrid/remote- occasional check-in at a Lottery office located in the State of Ohio may be necessary 615 West Superior Avenue Cleveland, Ohio 44113Experience Required Strong knowledge of SQL to aid in data visualizationWorking knowledge of Hadoop tools, such as Spark, Impala, Hue, and Kafka, to create, query, and manage ETL data flowsworking knowledge of data connectors for embedding data visualizations from Tableau Server or PowerBI into SharePointKnowledge of Tableau Server and/or Microsoft PowerBI Python experience is a bonuspractical understanding and experience with data cleaning/cleansing.Conceptual understanding of networking schematics and data flows for ETL purposes.Ability to conceptualize development of data mart environments using batch data for agency business analyst usage.Some experience with newer data analysis tools, such as Tensorflow, is helpful.
RegardsRavi223-234-1750"
Data Engineer with Azure,Arnex Solutions LLC,"Dallas, TX (Hybrid)",https://www.linkedin.com/jobs/view/3712320738/?eBP=JOB_SEARCH_ORGANIC&refId=sXFJza93p7CggsxCgSymqg%3D%3D&trackingId=mQ8SqLRLvS6KVBB3Qzgb7A%3D%3D&trk=flagship3_search_srp_jobs,3712320738,"About the job
            
 
Position : Data Engineer with AzureLocation: Dallas,TX (2 days in week onsite)HybridPosition Type: W2 onlyExperience: 9+ yearsSkills & Experience RequiredScala/Python, Spark Structured Streaming, Spark Streaming, Kafka, micro-batchingAzure Event Hubs, Databricks, Stream Analytics, Data Explorer, MonitorsReading, writing, and optimizing streaming data at scale e.g. handling millions of connected devices sending billions of messages per dayReading from and writing to RDBMs and NoSQL DBStrong Understanding Of The FollowingEvent-Driven and Domain-Driven Architecture & ImplementationsBackend development of micro-service within the Cloud platform (in Azure)Essential | Working Knowledge of:Designing experience in High Available and Reliable SolutionsObserve, test, debug, and correct faults in a code baseDevelop and produce documentation, including technical specificationsHands-on experience in the unit test, integration tests, CI/CD process, and code coverageSecondary SkillsKubernetesFull data management skills data profiling, data analysis, data analytics, data transformation, data optimization, data governance, data anomaly handlingWorking knowledge in Yugabyte, Hadoop, HDFS, and relational databasesHands-on experience in Spark Streaming / Spark Structured Streaming DATA ENGINEER with Azure
Client: American AirlinesRate: $60/hr on W2Location: Dallas,TX (2 days in week onsite)Skills & Experience RequiredScala/Python, Spark Structured Streaming, Spark Streaming, Kafka, micro-batchingAzure Event Hubs, Databricks, Stream Analytics, Data Explorer, MonitorsReading, writing, and optimizing streaming data at scale e.g. handling millions of connected devices sending billions of messages per dayReading from and writing to RDBMs and NoSQL DBStrong Understanding Of The FollowingEvent-Driven and Domain-Driven Architecture & ImplementationsBackend development of micro-service within the Cloud platform (in Azure)Essential | Working Knowledge of:Designing experience in High Available and Reliable SolutionsObserve, test, debug, and correct faults in a code baseDevelop and produce documentation, including technical specificationsHands-on experience in the unit test, integration tests, CI/CD process, and code coverageSecondary Skills&a"
Azure Data Engineer,"The Dignify Solutions, LLC","Cary, NC (Hybrid)",https://www.linkedin.com/jobs/view/3768008562/?eBP=JOB_SEARCH_ORGANIC&refId=sXFJza93p7CggsxCgSymqg%3D%3D&trackingId=l0oC7u92XrG2g8ZGn2Eb3A%3D%3D&trk=flagship3_search_srp_jobs,3768008562,"About the job
            
 
Experience with Azure: Data Factory, Synapse, Blob Storage , ADLS , Azure SQL, Logic Apps Solid knowledge of data processing languages, such as SQL, Python, or Scala. Data Migration Azure DevOps ETL Experience with SSIS ,informatica"
"GCP Data Engineer || Dallas, TX",Steneral Consulting,"Dallas, TX (Hybrid)",https://www.linkedin.com/jobs/view/3710632765/?eBP=JOB_SEARCH_ORGANIC&refId=sXFJza93p7CggsxCgSymqg%3D%3D&trackingId=WoPBdREsQ57C3%2B39kseA8A%3D%3D&trk=flagship3_search_srp_jobs,3710632765,"About the job
            
 
GCP Data Engineer (must have 7 years with google cloud experience)  Dallas, TX - Hybrid / Onsite Position: Hybrid (2-3 days/week at the office) - Local candidates only under 40-50 Min commute of Dallas or Phoenix or PleasantonSpecific Location of the resource: Dallas TX, but open to Pleasanton CA and PhoenixRequired Qualifications 7+ years proven experience in developing and deploying data pipelines in GCP or Azure.5+ years of Snowflake, BigQuery and/or Databricks experience5+ years proven experience in building frameworks for data ingestion, processing, and consumption using GCP Data Flow, GCP Data Composer, Big Query.4+ years of strong experience with SQL, Python, Java, API development2+ years of proven expertise in creating real-time pipelines using Kafka, Pub/sub.gcpBuilding high quality data pipelines with monitoring and observability2+ years of experience building dashboards and reports with PowerBI and/or Thoughtspot
Preferred Qualifications Extensive experience in data transformations for Retail and e-commerce business use cases will be a plusBachelor's or Master's in computer engineering, computer science or related area.Knowledge of Github Actions for CICDKnowledge of building machine learning modelsMandatory Certifications Required: GCP Professional Architect or GCP Data Engineer"
"Hybrid Work - Data Engineer/Senior Data Engineering (GCP, Python, Hadoop)-locals in Irving TX",Steneral Consulting,"Irving, TX (Hybrid)",https://www.linkedin.com/jobs/view/3675562418/?eBP=JOB_SEARCH_ORGANIC&refId=sXFJza93p7CggsxCgSymqg%3D%3D&trackingId=0qqsBTO7EKKNgOaN5%2Bh0%2FA%3D%3D&trk=flagship3_search_srp_jobs,3675562418,"About the job
            
 
Job: Data Engineer/Senior Data Engineering (GCP, Python, Hadoop)Location: Hybrid- Irving, TXTerm: to end of Year + long term requirementInterview Process: one and done screen Face to face/ ONSITE in Irving, TX office Position Summary Analyzes complex data structures from disparate data sources and design large scale data engineering pipelineDevelops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needsCollaborates with product business and data science team to collect user stories and translate into technical specificationsUses knowledge in Cloud & Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelinesUses strong programming skills in PySpark, Python, Java or any of the major languages to build robust data pipelines and dynamic systemsBuilds highly scalable and extensible data marts and data models to support Data Science and other internal customers on Cloud. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards.Analyzes current information technology environments to identify and assess critical capabilities and recommend solutionsExperiments with available tools and advice on new tools to determine optimal solution given the requirements dictated by the model/use case
Required Qualifications 3+ years of progressively complex related experience in cloud data engineering and data analysis 2+ years GCP experience- must be working with GCP in current role5+ years of US Based work experienceKnowledge in programing languages such as PySpark, Java, Python, Hive, SQLKnowledge in Cloud Technology, Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environmentStrong knowledge of large-scale search applications and building high volume data pipelines, preferably using Dataproc, composer services on GCP o
Preferred Qualifications Ability to leverage multiple tools and programming languages to analyze and manipulate datasets from disparate data sourcesAbility to understand complex systems and solve challenging analytical problemsExperience with bash shell scripts, UNIX utilities & UNIX Commands"
Big Data Engineer,TMS,"St Louis, MO (Hybrid)",https://www.linkedin.com/jobs/view/3728846367/?eBP=JOB_SEARCH_ORGANIC&refId=sXFJza93p7CggsxCgSymqg%3D%3D&trackingId=aZsmS9fAdzjagqcB1FMb3A%3D%3D&trk=flagship3_search_srp_jobs,3728846367,"About the job
            
 
A Data Engineer WillThe Data Engineer will participate on data management aspects of client engagements to deliver Test & Learn solutions, as well as contribute to and foster a high performance collaborative workplace.  Independently executes projects through design, implementation, automation, and maintenance of large scale enterprise ETL processes for a global client base Act as an expert data resource within the team Deliver on-time, accurate, high-value, robust data solutions across multiple clients, solutions and industry sectors Build trust-based working relationships with peers and clients across local and global teams
All About You  Good understanding of Python – Pandas, Numpy, PySpark and Impala. Hands-On experience on visualization tools i.e. Power BI, Tableau, Qlik Sense etc.. Experience in doing data analysis and extraction on Hadoop. Experience with Enterprise Business Intelligence Platform/Data platform. Strong SQL and higher-level programming languages with solid knowledge of data mining, machine learning algorithms and tools Experience with data integration tools – ETL/ELT tools (i.e. Apache NiFi, Azure Data Factory, Pentaho, Talend) Experience with Databrick, Snowflake, Graph Database is a plus. Experience in hands-on data modeling, programming, querying, data mining and report development using large volumes of granular data to deliver business intelligence and custom reporting solutions. Exposure to collecting and/or working with data including standardizing, summarizing, offering initial observations and highlighting inconsistencies. Strong understanding of the application of analytical methods and data visualization to support business decisions. Ability to understand complex operational systems and analytics/business intelligence tools for the delivery of information products and analytical offerings to a large, global user base."
Onsite work - Need Sr. Data Engineer Jersey City NJ,Steneral Consulting,"Jersey City, NJ (Hybrid)",https://www.linkedin.com/jobs/view/3681091324/?eBP=JOB_SEARCH_ORGANIC&refId=sXFJza93p7CggsxCgSymqg%3D%3D&trackingId=2m9UlMdop%2FCgFxrya0Zzag%3D%3D&trk=flagship3_search_srp_jobs,3681091324,"About the job
            
 
Sr. Data Engineer Jersey City, NJ - Onsite - Local candidates only under 60 Min commute A minimum of ten (10) years of IT experience in data engineering or data management field8 - 10 years of experience in application development using SQL, PLSQL, Python, and shell scriptingStrong working knowledge and experience in Oracle, SQL Server, Snowflake, or any SQL databaseRecent experience as a Senior Data Engineer in any public cloud, preferably on Azure as well as on a cloud warehouse like Snowflake or Azure Synapse is required5+ years of experience in data warehousing and building data pipelines using various ETL and ELT toolsHands-on experience with Snowflake, DBT, and Airflow is highly preferredExperience in building cloud-native solutions, preferably on Azure including the use of DevOps CI / CD tools and containers.Familiarity with Familiarity with data modeling tools/techniques is a plusAzure or Snowflake training/certification is a plusStrong analytical, quantitative, problem-solving, communication, and organizational skillsSelf-driven, self-directed, passionate analytical, and focused on delivering the right results"
"Direct Client – Sr. Data Platform Engineer (Python, NoSql, CI/CD, Hadoop/Spark)",The AES Group,"Seattle, WA (Hybrid)",https://www.linkedin.com/jobs/view/3697323154/?eBP=JOB_SEARCH_ORGANIC&refId=sXFJza93p7CggsxCgSymqg%3D%3D&trackingId=HfSVf4jDmmBPB7bmZyqtVw%3D%3D&trk=flagship3_search_srp_jobs,3697323154,"About the job
            
 
Location: Hybrid (3 days in a week from office/ Seattle, WA)Independently design and develop programs and tools to support data pipelines including ingestion, curation, and provisioning of data for analytics, reporting and data science. Deliver key, complex business capabilities and platform features. Deliver high quality software using Agile/Scrum development methodologies. Identify and drive operational improvements for the team's data platform services.Tops 3 Skills Needed1Software development using Python4+ years2Experience with data technologies2+ years3CI/ CD2+ yearsYears Of Experience 4+ years 
Basic Qualifications/ Experience  Bachelor's degree in computer science or engineering. Of the required experience, must have the years of experience in each of the following: 4 years of software development experience using Python.4 years of experience using software engineering fundamentals, including algorithm theory and data structures. 2 years of experience using CI/CD tools to build and deploy software systems.2 years of experience using relational and non-relational, NoSQL databases. 1 years of experience using Hadoop or Spark. 1 years of experience using data hygiene routines and models 

Key Responsibilities Collaborate with development team, other Information Technology (IT) teams' developer leads.Initiate process improvements for new and existing systems. Design, develop, implement, document, and test changes to an application subsystem.Code, test, debugs, documents, and implement complex software applications.Create more complex prototypes and ensure deliverables are high quality and meet user expectations.Support system and integration testing activities.Perform root cause analysis to identify permanent resolutions to software or business process issuesInitiate design reviews for new applications and adhere to software development standards. Coach, and mentor other team members.Perform cross-training and facilitate information sharing among team members."
Lead Data bricks Engineer,SoftTrak Technologies,New York City Metropolitan Area (Hybrid),https://www.linkedin.com/jobs/view/3779160631/?eBP=JOB_SEARCH_ORGANIC&refId=xp%2BdiaF9xpdvJWSLpA9clw%3D%3D&trackingId=KOqIXOdWdX1kxcLsydB%2B7w%3D%3D&trk=flagship3_search_srp_jobs,3779160631,"About the job
            
 
Inperson interview is must Major Responsibilities:Work on Finance data related to Collaterals, ETD, OTD, Settlement market, Cash product, Repo, Duos repoDesign, develop, and deploy Databricks jobs to process and analyze large volumes of data. Collaborate with data engineers and data scientists to understand data requirements and implement appropriate data processing pipelines. Optimize Databricks jobs for performance and scalability to handle big data workloads. Monitor and troubleshoot Databricks jobs, identify and resolve issues or bottlenecks. Implement best practices for data management, security, and governance within the Databricks environment. Experience designing and developing Enterprise Data Warehouse solutions.Demonstrated proficiency with Data Analytics, Data InsightsProficient writing SQL queries and programming including stored procedures and reverse engineering existing processAzure Synapse/Bigquery/Redshift is good to have.Perform code reviews to ensure fit to requirements, optimal execution patterns and adherence to established standards. Skills:5+ years’ – Strong experience in Finance / Banking industry – Capital markets, investment banking - Collaterals, ETD, OTD, Settlement market, Cash product, Repo, Duos repo10+ years - Enterprise Data Management10+ years - SQL Server based development of large datasets5+ years with Data Warehouse Architecture, hands-on experience with Databricks platform. Extensive experience in PySpark coding.Azure Synapse/Bigquery/Redshift experience is good to have3+ years Python(numpy, pandas) coding experienceExperience with Snowflake utilities such as SnowSQL and SnowPipe - good to haveExperience in Data warehousing - OLTP, OLAP, Dimensions, Facts, and Data modeling.Previous experience leading an enterprise-wide Cloud Data Platform migration with strong architectural and design skillsCapable of discussing enterprise level services independent of technology stackExperience with Cloud based data architectures, messaging, and analyticsSuperior communication skillsCloud certification(s)Any experience with Regulatory Reporting is a PlusLead Databricks Engineer with Pyspark & Python Exp (Financial background is must)"
Senior Data Engineer,Applicantz,"Fremont, CA (Hybrid)",https://www.linkedin.com/jobs/view/3779975095/?eBP=JOB_SEARCH_ORGANIC&refId=xp%2BdiaF9xpdvJWSLpA9clw%3D%3D&trackingId=%2BtnprCNo6F3LGP4sWYkF0g%3D%3D&trk=flagship3_search_srp_jobs,3779975095,"About the job
            
 
VISA SPONSORSHIP IS NOT AVAILABLE. Hybrid work from Fremont Office.Our Client is a Fortune 350 company that engages in the design, manufacturing, marketing, and service of semiconductor processing equipment.Looking for a Data Engineer / Data Analyst with experience in Power BI, Sharepoint and SAP PLM.Required Qualifications: 5+ years of related experience with a bachelor’s degree in Computer Science, Data Science, Statistics, Supply Chain Management, Operations Research, or a related field.Expertise with SharePoint, Power BIPrefer experience in ENOVIA PLM and SAPProven experience in data analysis, data modeling, and statistical analysis.Proficient in data visualization tools such as Power BIExperience managing IT programsStrong analytical and problem-solving skillsAbility to develop front end applications with custom workflows and automations. (PowerApps, SharePoint Classic and Online, QuickBase Applications)Ability to integrate front end applications with databases (SAP/SILK) to show real time data (Custom APIs, Webservices)
Responsibilities: Data mining in PLM/SAPEnsure data quality and integrity by performing data cleansing, validation, and error handling.Optimize data infrastructure to support data processing, storage, and retrieval efficiently.Perform exploratory data analysis to uncover trends, patterns, and insights from the data.Conduct statistical analysis and data modeling to support business decision-making.Develop and maintain data documentation, including data dictionaries and data lineage.Create visually appealing and interactive dashboards that effectively communicate complex data insights to stakeholders.Leverage best practices to present data in a clear, concise, and compelling manner.Collaborate with business users & cross functional teams to understand their data requirements, analytical needs and translate them into actionable visualizations.Monitor and maintain existing dashboards, making improvements and updates as needed
Additional Qualifications: Experience with programming languages such as SQL.Positive attitude in a rapidly changing environment with ambiguous information.Concise communication skills.Intellectual curiosity with an ability to learn quickly.Passion for technology and operations.Experience with Databricks, Snowflake, AWS, and other cloud systemsExperience with Excel and GsheetsStrong understanding of supply chain processes and terminology.Strong analytical and problem-solving skills.Excellent communication and presentation skills.Ability to work independently and as part of a team."
Lead Big Data Engineer,Vimerse InfoTech Inc,"Atlanta, GA (Hybrid)",https://www.linkedin.com/jobs/view/3779159732/?eBP=JOB_SEARCH_ORGANIC&refId=xp%2BdiaF9xpdvJWSLpA9clw%3D%3D&trackingId=8F2Q3KQN8wc1f%2FxggPUu8A%3D%3D&trk=flagship3_search_srp_jobs,3779159732,"About the job
            
 
Role: Lead Big Data EngineerLocation: Atlanta, GAHybrid: Twice every fortnight or 4 times a month.Job Description: Must have Skills: Cloud architecture (Strong), Cloud development (Strong), Python (Strong), Snowflake, AWS Lambda. We are looking for a strong Data Engineer cum Architect, to create new modern data ingestion pipelines using latest technologies like AWS Athena, Lambdas, Python, Spark. You'll be working on data pipelines and tools to provide the underlying data ingestion framework."" 
 Technical Skills:  Frontrunner Be inclined towards process automations & improvements, Identifying & automating repetitive things. Must be able to handle Data engineering operations / enhancement project with a technical consultant bend. SQL, Python, PySpark , S3, Lambda, EMR, Glue, Athena, EC2, IAM, Redshift, DMS, Airflow, Jenkins, Snowflake. End-to-end data solutions (ingest, storage, integration, processing, access) on AWS. Migrate data from traditional relational database systems to AWS relational databases such as Amazon RDS, Aurora, and Redshift. 12+ years IT experience. Background and experience in data engineering/analytics. Should have a very good hands-on experience in Cloud DB platforms (Snowflake is preferable), Building data pipelines & SQL, Python for Data Engineering. Got experience to Perform, Support and Lead all aspects of Data Engineering strategy. Excellent root cause analysis skills. Ensure effective data pipeline engineering, deployment, ongoing operations, and continuous improvement. Manage and perform data operations and data engineering requirements including automation and optimization. Highly motivated, a self-starter, ability to work in a fast faced environment while managing competing priorities. Creative problem solver and highly collaborative teammate who is comfortable working as a key contributor. Certification in Data Engineering and/or Cloud Platforms are a plus.Good written and verbal communication skills, and comfortable presenting findings to Sr. Management."
"Data Engineer with strong experience with Stored procedures, SQL--Chicago, IL--Full Time",Visionary Innovative Technology Solutions LLC,"Chicago, IL (Hybrid)",https://www.linkedin.com/jobs/view/3779147847/?eBP=JOB_SEARCH_ORGANIC&refId=xp%2BdiaF9xpdvJWSLpA9clw%3D%3D&trackingId=7AMRJhANRg3%2FFgxi2LoLqg%3D%3D&trk=flagship3_search_srp_jobs,3779147847,"About the job
            
 
Position: Data Engineer with strong experience with Stored procedures, SQLLocation: Chicago, ILDuration: Contract  Job DescriptionExperience 13 + years · Extensive experience in design and development of Databases, SQL Server, stored procedures, Indexes, Views, and Triggers.· Database concepts, ability to write complex stored procedures, functions, triggers etc. with close view on database performance.· Strong SQL and performance improvement background.· Good with designing table structure considering performance in mind.· Good understanding of building data pipelines.· Good understanding of Snowflake and associated workflows."
Senior Data Engineer,Dice,"Commerce, CA (Hybrid)",https://www.linkedin.com/jobs/view/3779717207/?eBP=JOB_SEARCH_ORGANIC&refId=xp%2BdiaF9xpdvJWSLpA9clw%3D%3D&trackingId=qmBPuWqDxid%2FpAm%2F4OsGAQ%3D%3D&trk=flagship3_search_srp_jobs,3779717207,"About the job
            
 
Dice is the leading career destination for tech experts at every stage of their careers. Our client, Pyramid Technology Solutions, Inc., is seeking the following. Apply via Dice today!Job Role: Senior Data EngineerLocation: Los Angeles, CA (90012)Duration: 6 Months ContractSkills Preferred: Data engineering and data pipeline implementation experienceMicrosoft Transact-SQL development experiencePython development experiencePostgreSQL administration experienceData Lake environment experienceDatabase design - security - and architecture experienceDocker experienceGit and GitHub experienceApache Airflow configuration and management experienceApache Spark experienceMicrosoft SQL Server administration experienceStrong understanding of APIsGrafana experienceMachine Learning pipeline experienceExperience with Cloud-based database platforms (Azure - AWS - Google Cloud Platform - etc.)Linux Server or Linux subsystem experience such as YUM and DNF packages - implementing PostgreSQL – CentrifyCollaboration with data scientist teams - application teams - and project management teams 
Experience Preferred: Minimum of three (3) years of experience as a data engineerExperience in managing and maintaining Data Lakes - Data Warehouses - and public cloud or on-prem data environments
Education PreferredBachelor’s degree in an IT-related or engineering field - or relevant certifications (e.g. Microsoft SQL Server - PostgreSQL DBA - AWS - Azure)Additional Information:The work location is Commerce CA. with a hybrid of two days on-prem and three days’ work from home"
Hybrid Work - Need Senior Data Platform Engineer-Azure in Des Moines IA,Steneral Consulting,"Des Moines, IA (Hybrid)",https://www.linkedin.com/jobs/view/3750853112/?eBP=JOB_SEARCH_ORGANIC&refId=xp%2BdiaF9xpdvJWSLpA9clw%3D%3D&trackingId=hifxDEqnBZHoHuBU5B4ZZQ%3D%3D&trk=flagship3_search_srp_jobs,3750853112,"About the job
            
 
Senior Data Platform Engineer-AzureDes Moines, IA - Hybrid 3 days a week - locals are highly preferred but will consider someone from Midwest who will relocate from day one to work onsite.Communication and collaboration are key. They must be easily understood and able to speak well to their projects and technical experience. Must have valid LinkedIn and Photo ID required with submissionMust Have’s: Must have everything or please do not send them to me.   Azure Microsoft Fabric -- end to end lifecycle.Azure Azure SQL Database: Provisioning, performance tuning, scaling, and security.Azure Cosmos DB: Understanding of NoSQL databases, partitioning, consistency models.Azure Data Factory: Data integration and ETL processes.Azure Blob Storage and Data Lake Storage: Management, performance, security, and data lifecycle.Azure Stream Analytics: Real-time data streaming and analytics.Azure Databricks & HDInsight: Big data analytics solutions. (lower priority)Azure Synapse Analytics: Knowledge of data warehousing, data integration, and analytics.

   SQL: Writing, optimizing, and debugging SQL queries.Data modeling: Normalization, star schema, snowflake schemaFamiliarity with SDKs and APIs associated with Azure data services.Integration with other Azure services or third-party applications.Experience in one or more programming languages like C#, Python, or Java can be beneficial.Azure Monitor, Azure Log Analytics, and Application Insights.DP-203 certification Financial/Investment industry experience.

Job DescriptionHere are the skills sets for building out the Microsoft Azure Data Platform.  Azure Fundamentals: Understanding of Azure subscriptions, resources, and resource groups.Familiarity with Azure regions, availability zones, and the Azure portal.

  Azure Data Services Knowledge of tool set: Azure Microsoft Fabric -- end to end lifecycle.Azure Azure SQL Database: Provisioning, performance tuning, scaling, and security.Azure Cosmos DB: Understanding of NoSQL databases, partitioning, consistency models.Azure Data Factory: Data integration and ETL processes.Azure Blob Storage and Data Lake Storage: Management, performance, security, and data lifecycle.Azure Stream Analytics: Real-time data streaming and analytics.Azure Databricks & HDInsight: Big data analytics solutions. (lower priority)Azure Synapse Analytics: Knowledge of data warehousing, data integration, and analytics.

  Skills: SQL: Writing, optimizing, and debugging SQL queries.Data modeling: Normalization, star schema, snowflake schemaFamiliarity with SDKs and APIs associated with Azure data services.Integration with other Azure services or third-party applications.Experience in one or more programming languages like C#, Python, or Java can be beneficial.Azure Monitor, Azure Log Analytics, and Application Insights.DP-203 certification 

  Optional but helpful: Azure Active Directory and role-based access control (RBAC)Tools like Azure Data Migration Service, SSIS (SQL Server Integration Services).Strategies for migrating data from on-premises or other clouds to Azure."
"Data Engineer with Life Insurance industry experience || Des Moines, IA - Hybrid onsite from Day One",Steneral Consulting,"Des Moines, IA (Hybrid)",https://www.linkedin.com/jobs/view/3670708050/?eBP=JOB_SEARCH_ORGANIC&refId=xp%2BdiaF9xpdvJWSLpA9clw%3D%3D&trackingId=s5UO%2F16GELm6CnjsDFTirA%3D%3D&trk=flagship3_search_srp_jobs,3670708050,"About the job
            
 
Data Engineer With Life Insurance Industry Experience.Des Moines, IA - Hybrid onsite from Day One - Local candidates will be given priority.Candidates must be able to work onsite/hybrid in Des Moines, IA from day one. Prefer candidates local to Iowa.Please screen your candidates well to ensure they are an excellent technical fit, can speak to their experience well and are excellent communicators and collaborators that can be easily understood. This client is picky about that.Must Have's: (Please only send me candidates that have all the Must Have's.)   Experience in API Development and Data Governance tools like EDC.Proficient in developing APIs - using MuleSoft, to retrieve data from and send to cloud based applicationsAble to understand and modify C# code mainly for administration and configuration of toolsExpertise in using and administering Data Governance tools like Axon, EDC, IDQ, MDMGood communication skills Written and VerbalPreferably in Des Moines, open for remote if the candidate is extremely goodExperience working in Azure DevOpsLife Insurance industry experience

Job DescriptionIT Contractor RequisitionJob TitleData EngineerJob Description: Preferred Office Location: Des MoinesWe are looking for a Data Engineer with experience in API Development and Data Governance tools like EDC. This team member will work with our business stakeholders and other data team members to create APIs to provide data to and from Data Governance tools like EDC and Profisee.Desired Skill Years of Experience 5+ years   Proficient in developing APIs - using MuleSoft, to retrieve data from and send to cloud based applicationsAble to understand and modify C# code mainly for administration and configuration of toolsExpertise in using and administering Data Governance tools like Axon, EDC, IDQ, MDMGood communication skills Written and VerbalPreferably in Des Moines, open for remote if the candidate is extremely good

Additional Preferred Experience Technical background/aptitude to learn now tools and technologiesExperience working in Azure DevOpsFacilitating clear and effective communications across technical and non-technical individuals and teams at all levels of the organization."
Data Engineer,Motion Recruitment,"Philadelphia, PA (Hybrid)",https://www.linkedin.com/jobs/view/3761070836/?eBP=JOB_SEARCH_ORGANIC&refId=A2pTL530PT8RrsyeqTc7vA%3D%3D&trackingId=fZiYReZw6sc5EPKm0q4gjw%3D%3D&trk=flagship3_search_srp_jobs,3761070836,"About the job
            
 
A national pharmaceutical company is looking for a fulltime Data Engineer to join their team. This company, based in Philadlphia, is transforming lives and reinventing healthcare as they tackle society’s most pressing health issues.In this role, you will help build out a data platform, support clinical trials, and deliver patient and drug data. The ideal candidate will feel comfortable jumping into a fast moving and critical project. If you want to join an organization making a difference in the world, while also working on a fast-paced, exciting team, then this is an opportunity you do not want to miss! Required Skills & Experience Bachelor’s Degree in Computer Science/Engineering or a related field or similar experience 3+ years experience 
Desired Skills & Experience Experience working in the pharmaceutical industry Concepts of Microservices Spark 
What You Will Be Doing Tech Breakdown Python SQL AWS ETL 
Daily Responsibilities 100% Hands On 
Applicants must be currently authorized to work in the US now and in the future. This position cannot support any sponsorship or C2C requirements at this time.Posted By: Nicole McCafferty"
Senior Data Operations Engineer (R-15257),Dun & Bradstreet,"Austin, TX (Hybrid)",https://www.linkedin.com/jobs/view/3774852995/?eBP=JOB_SEARCH_ORGANIC&refId=A2pTL530PT8RrsyeqTc7vA%3D%3D&trackingId=oDjDH6mR5zIJW4LE9ZKLvQ%3D%3D&trk=flagship3_search_srp_jobs,3774852995,"About the job
            
 
Why We Work at Dun & BradstreetDun & Bradstreet unlocks the power of data through analytics, creating a better tomorrow. Each day, we are finding new ways to strengthen our award-winning culture and accelerate creativity, innovation and growth. Our 6,000+ global team members are passionate about what we do. We are dedicated to helping clients turn uncertainty into confidence, risk into opportunity and potential into prosperity. Bold and diverse thinkers are always welcome. Come join us!This position is a hybrid role and will need to be in Austin office 2 days a week - NO REMOTEThe Senior Data Operations Engineer will develop, maintain, and analyze datasets from diverse sources, including mobile and web, government agencies, web crawls, social media, and proprietary datasets, to create insights for our clients, power our platform, and create an innovative market understanding.The Senior Data Operations Engineer will create designs and share ideas for creating and improving data pipelines and tools.Key Requirements: Collaborate with the data, platform, QA, and DevOps teams to design and construct advanced systems for processing, analyzing, searching, and visualizing vast datasets.Architect resilient systems and write highly fault-tolerant software to consistently deliver high-quality results.Take initiative to become familiar with existing application code and achieve a complete understanding of how the applications function.Pioneering novel methods for extracting intelligence from a wide array of unique data sources.Generate fresh insights for our clients, provide novel perspectives on their markets.Co-create and document data processing systems that are easy to maintain, fostering collaborative and supportive team environment.Help maintain existing systems, including troubleshooting and resolving alerts.Be a good collaborator with your peers. Be easy to get ahold of and attend all required meetings.Share ideas across teams to spread awareness and use of frameworks and tooling.Share a friendly, supportive, and reliable attitude with a great team that hold each other accountable.
This position is a hybrid role and will need to be in Austin office 2 days a week - NO REMOTEKey Requirements: Extensive experience working with GCP services, including Big Query, Dataflow, Pub/Sub, Cloud Storage, Cloud Run, Cloud Functions and related technologies is required. Extensive experience with SQL and relational databases, including optimization and design.Expertise in containerized infrastructure and CI/CD systems, including CloudBuild, Docker, Kubernetes, and GitHub Actions.Testable and efficient Python coding for data processing and analysis.Experience with Amazon Web Services (EC2, RDS, S3, Redshift, EMR, and more).Experience with OS level scripting (bash, sed, awk, grep, etc.).Experience in AdTech, web cookies, and online advertising technologies.Familiarity with parallelization of applications on a single machine and across a network of machines.Experience with version control (GIT/Github/BitBucket) and Agile Project Management tools (Clickup/Jira/Confluence).Experience with object-oriented programming, functional programming a plusAnalytic tools and ETL/ELT/data pipeline frameworks a plus.
This position is a hybrid role and will need to be in Austin office 2 days a week - NO REMOTEBenefits We Offer  Generous paid time off in your first year, increasing with tenure. Up to 16 weeks 100% paid parental leave after one year of employment. Paid sick time to care for yourself or family members. Education assistance and extensive training resources. Do Good Program: Paid volunteer days & donation matching. Competitive 401k & Employee Stock Purchase Plan with company matching. Health & wellness benefits, including discounted Gympass membership rates. Medical, dental & vision insurance for you, spouse/partner & dependents. Learn more about our benefits: http://bit.ly/41Yyc3d .
All Dun & Bradstreet job postings can be found at https://www.dnb.com/about-us/careers-and-people/joblistings.html . Official communication from Dun & Bradstreet will come from an email address ending in @dnb.com.Notice to Applicants: Please be advised that this job posting page is hosted and powered by Lever. Your use of this page is subject to Lever's Privacy Notice and Cookie Policy , which governs the processing of visitor data on this platform.Equal Employment Opportunity (EEO): Dun & Bradstreet is an Equal Opportunity Employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, creed, sex, age, national origin, citizenship status, disability status, sexual orientation, gender identity or expression, pregnancy, genetic information, protected military and veteran status, ancestry, marital status, medical condition (cancer and genetic characteristics) or any other characteristic protected by law. View the EEO is the Law poster  here  and its supplement  here.  View the pay transparency policy  here ."
Sr. Data Engineer (Hybrid),Brady Corporation,"Milwaukee, WI (Hybrid)",https://www.linkedin.com/jobs/view/3658299090/?eBP=JOB_SEARCH_ORGANIC&refId=A2pTL530PT8RrsyeqTc7vA%3D%3D&trackingId=xe2qhDi4o%2FTVPgQXEvtMnQ%3D%3D&trk=flagship3_search_srp_jobs,3658299090,"About the job
            
 
Who we are:Brady makes products that make the world a safer and more productive place. We are a global leader in safety, identification and compliance solutions for a diverse range of workplaces. From the depths of the ocean to outer space, from the factory floor to the delivery room - we’re just about everywhere you look. Companies around the world trust Brady because of our deep expertise and knowledge across a wide range of industries and applications - powered by our world-class manufacturing capabilities.We have a diverse customer base in industries including electronics, telecommunications, manufacturing, electrical, construction, healthcare, aerospace and more. As of July 31, 2023, Brady employed approximately 5,600 people worldwide. Our fiscal 2023 sales were approximately $1.33 billion. Brady stock trades on the New York Stock Exchange under the symbol BRC. You can learn more about us at www.bradycorp.com.Why work at Brady:A career at Brady means working for a global company that has thrived for over 100 years, and whose innovative spirit drives our future growth.Brady offers competitive pay and great benefits, supported by a culture that encourages collaboration and innovation. We strive to foster an inclusive workplace where diverse talent can learn, grow, and succeed. And with deeply rooted values, no matter where you work at Brady, you’ll feel connected to the community through our charitable contributions and opportunities to give back.Our headquarters are in Milwaukee, Wisconsin, but we have more than 70 locations globally, giving our employees the opportunity to work with colleagues around the world.What we need:Brady is seeking a Senior BI Developer/Data Engineer who will design, development, and support of data pipelines from multiple applications and sources to facilitate data analytics needs. Our team partners with Sales & Marketing, Finance, and Operations business users and data scientists to help develop data driven insights and make data-driven decisions. The Senior BI Developer works with various data and BI tools and technologies, such as Python, SQL Server, Google Big Query, Google Cloud Composer, and etc.What you'll be doing: Develop data pipelines from multiple sources using Python with API calls.Develop SQL code or data flow to integrate and transform data from multiple sources into useful, consistent and easily consumable data elements.Work with business analysts and business users to understand data requirements and map into easy consumable data elements.Provide ongoing enhancement and support for data pipelines in the production environment. Understand critical needs of the business, identify and fix issues with a sense of urgency. Participate in discussions on design options and approaches, propose solutions and develop the new capabilities. Perform and lead the code reviews to identify coding, process and functionality improvements. Work on troubleshooting, identifying issues and performance bottlenecks, problem solving and impact analysis.Follow the Scrum process and take ownership of Jira stories.Lead the offshore developers, provide directions and guide them with development and support processes.
What you'll need to be successful: Bachelor of Science in Computer Science, Engineering or related fieldSelf-motivated with a strong work ethic, ability to work independently as well as ability to work within a teamStrong communication skills, ability to communicate well with technical and non-technical staff3+ years experience working with Python or .NET programming3+ years of experience working with ETL and SQL development3+ years of experience working with cloud data platform, GCP, Azure or AWSStrong analytical and problem solving skills including the ability to independently troubleshoot issues and determine and develop the best solution while considering all impacts to the production systemWilling to learn and able to adapt to new technologies as needed
Benefits: Complete insurance coverage starting on first day of employment – medical, dental, vision, life401(k) with company matchTuition reimbursementBonus opportunityVacation and holiday pay"
Data Engineer III (San Diego),ICW Group,"San Diego, CA (Hybrid)",https://www.linkedin.com/jobs/view/3745572364/?eBP=JOB_SEARCH_ORGANIC&refId=A2pTL530PT8RrsyeqTc7vA%3D%3D&trackingId=WAUbBt8LLP5sc5i1hRKOcQ%3D%3D&trk=flagship3_search_srp_jobs,3745572364,"About the job
            
 
Are you looking for more than just a job? Do you want to have a voice and feel a sense of belonging? At ICW Group, we hire innovative people who consistently adapt, grow and deliver. We believe in hard work, a fun work environment, and embracing creativity that only comes about when talented people collaborate to develop solutions. Our mission is to create the best insurance experience possible.Headquartered in San Diego with regional offices located throughout the United States, ICW Group has been named for seven consecutive years as a Top 50 performing P&C company offering the stability of a large, profitable and growing company combined with a small-company entrepreneurial spirit. Our purpose-driven ethos provides team members with opportunities to contribute, develop, and belong.The Data Engineer III will design, develop, and implement data pipelines, data integration and data storage solutions such as data warehouses, data lakes, relational and non-relational databases. The role will partner closely with IT Managers, Enterprise Business Intelligence, Data Governance and Data Science teams to solve business-significant data problems and enable data-driven decision-making, automation, and optimization. In the process, the role will have the opportunity to act as a Data Advisory member for the Actuarial, Business Intelligence (BI), Data Science, and Data Engineering teams, as well as a collaborator and contributor to Enterprise Data Architecture. Data is central to ICW Group’s business strategy and digital evolution, and the Data Engineer ensures optimal data delivery architecture is consistent throughout all projects.WHAT YOU WILL DOBuilds data solutions that ensure data integrity and information usability for enterprise-wide digital solutions and decision making. Leads the design and build of scalable data pipelines and solutions (batch and/or streaming) that make the best use of traditional and cloud platforms (AWS or similar) by understanding the business, technology, and data landscape including real time data processing.Provides analysis of complex data elements and systems, data flow and in development of conceptual, logical, and physical data models as well as verification and implementation of ETL/ELT mappings and transformation logic.Designs, supports and peer reviews the data models and schemas for new and existing data sources for the data warehouse.Conducts unit, integration, and system tests on our data sources to validate data against source systems, and continuously optimize performance to improve query speed and reduce cost.Builds the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using ‘big data’ technologies and tools.Leads data initiatives to ensure pipelines are reliable, efficient, testable, and maintainable.Maintains a strong understanding of the current landscape and proactively leads the analysis of the current environment to detect data deficiencies, gaps, and opportunities.Drives the improvement of engineering team processes via data architecture, engineering, test, and operational excellence best practices.Collaborates in implementing components of data strategy – Master Data Management (MDM), data virtualization etc.
Partners with various teams in delivering overall data solutions. Works closely with BI and Data Science teams in implementing various data streams.Partners with Enterprise Architecture, Technology, and Project teams to ensure consistency of solutions approach while maintaining data governance requirements.Contributes to data governance and data quality best practices including design reviews, unit testing, code reviews, and continuous integration and deployment.Collaborates with data architects to ensure the output of the physical models meet required needs— e.g., collaborates on data definition, data structure, data content and data usage.Evaluates and conducts POC on new technologies for fitment in next generation data platform ecosystem.Collaborates with the Enterprise Architecture team to drive tooling and standards to improve the productivity and quality of output for data engineers across the company.
WHAT YOU BRING TO THE ROLE Bachelor's degree in Computer Science, Applied Mathematics, Engineering, or any other technology related field required, or equivalent combination of education and experience.Minimum 6 years of experience in a data integration (Cloud/Traditional) engineering related role required.Experience with data practices (security, data management and governance) preferred. Experience in operations research, machine learning or optimization a plus.Insurance experience a plus.
CERTIFICATES, LICENSES, REGISTRATIONS Data architecture or data engineering related certification strongly desired.AWS Cloud Practitioner or more advanced AWS certification preferred.DAMA certifications preferred.
KNOWLEDGE AND SKILLS Expertise with database & data warehouse design, using MS-SQL, PostgreSQL etc.Knowledge of Model and Design of DB schemas for read and write performance.Working knowledge of API or stream-based data extraction processes.Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.Background in data modeling and performance tuning in relational and no-SQL databases.Experience with AWS technologies like Redshift, S3, EC2, Glue, EMR, Kinesis, Lambda, DynamoDB, etc.Experience with modern data architectures and modern data platforms like Snowflake, Databricks etc.Experience with data technologies: Hadoop, Spark, Kafka, Spark & Kafka Streaming, Python, Scala, Talend etc.Knowledge and experience with data movement tools – SSIS, Profisee, Alteryx, Informatica.Working knowledge of multiple data management domains such as data modeling, integration, warehousing, data quality, security, and governance.A self-starter mentality that thrives in a rapidly changing, fast-paced environment and tolerates ambiguity while demonstrating problem-solving with limited supervision.Strong analytical and time management skills. Self-motivated and able to handle tasks with minimal supervision.Must be organized, detail oriented, and able to multi-task. Ability to work well under pressure and deliver results with tight deadlines and under changing priorities.Ability to cross collaborate with multiple teams and offer value-added solutions to meet objectives.Ability to motivate and inspire team for maximum performance.Strong verbal and written communication skills. 
COMPENSATIONThe salary range listed for this position, $113,668 to $203,468, is exclusive of fringe benefits and potential bonuses. If hired at ICW Group, your final base salary compensation will be determined by factors such as skills, experience and/or education. In addition, we believe in the importance of pay equity and consider internal equity of our current team members as part of any final offer. We typically do not hire towards the maximum of the range in order to allow for future and continued salary growth. We also offer a substantial benefits package outlined below.WHY JOIN ICW GROUP?  Challenging work and the ability to make a differenceYou will have a voice and feel a sense of belongingWe offer a competitive benefits package, with generous medical, dental, and vision plans as well as 401K retirement plans and company matchBonus potential for all positionsPaid Time Off with an accrual rate of 5.23 hours per pay period (equal to 17 days per year)10 paid holidays throughout the calendar yearWant to continue learning? We’ll support you 100%
ICW Group is committed to creating a diverse environment and is proud to be an Equal Opportunity Employer. ICW Group will not discriminate against an applicant or employee on the basis of race, color, religion, national origin, ancestry, sex/gender, age, physical or mental disability, military or veteran status, genetic information, sexual orientation, gender identity, gender expression, marital status, or any other characteristic protected by applicable federal, state or local law."
Sr. Data Engineer,MHK TECH INC,"Jersey City, NJ (Hybrid)",https://www.linkedin.com/jobs/view/3767590420/?eBP=JOB_SEARCH_ORGANIC&refId=A2pTL530PT8RrsyeqTc7vA%3D%3D&trackingId=LTZRle1mD6yuHSkvRPXGvg%3D%3D&trk=flagship3_search_srp_jobs,3767590420,"About the job
            
 
ResponsibilitiesDesigning and implementing large-scale, distributed data processing systems using technologies such as Apache Hadoop, Apache Spark, or Apache Flink.Developing and optimizing data pipelines and workflows for ingesting, storing, processing, and analyzing large volumes of structured and unstructured data.Collaborating with data scientists, data analysts, and other stakeholders to understand data requirements and translate them into technical solutions.Building and maintaining data infrastructure, including data lakes, data warehouses, and real-time streaming platforms.Designing and implementing data models and schemas for efficient data storage and retrieval.Ensuring the scalability, availability, and fault-tolerance of big data systems through proper configuration, monitoring, and performance tuning.Identifying and evaluating new technologies, tools, and frameworks to improve the efficiency and effectiveness of big data processing.Implementing data security and privacy measures to protect sensitive information throughout the data lifecycle.Collaborating with cross-functional teams to integrate data from various sources, including structured databases, unstructured files, APIs, and streaming data.Developing and maintaining documentation, including data flow diagrams, system architecture, and technical specifications.RequirementsBachelor's or higher degree in Computer Science, Engineering, or a related field.Proven experience as a big data engineer or a similar role, with a deep understanding of big data technologies, frameworks, and best practices.Strong programming skills in languages such as Java, Scala, or Python for developing big data solutions.Experience with big data processing frameworks like Apache Hadoop, Apache Spark, Apache Flink, or similar.Proficiency in SQL and NoSQL databases, as well as data modeling and database design principles.Familiarity with cloud platforms and services, such as Amazon Web Services (AWS), Microsoft Azure, or Google Cloud Platform (GCP).Knowledge of distributed computing principles and technologies, such as HDFS, YARN, and containerization (e.g., Docker, Kubernetes).Understanding of real-time streaming technologies and frameworks, such as Apache Kafka or Apache Pulsar.Strong problem-solving skills and ability to optimize and tune big data processing systems for performance and scalability.Excellent communication and teamwork skills to collaborate with cross-functional teams and stakeholders."
Senior Data Engineer,Nayya,"New York, NY (Hybrid)",https://www.linkedin.com/jobs/view/3748332832/?eBP=JOB_SEARCH_ORGANIC&refId=A2pTL530PT8RrsyeqTc7vA%3D%3D&trackingId=xMyQMiCI0tzqx%2FZiJ4LxXQ%3D%3D&trk=flagship3_search_srp_jobs,3748332832,"About the job
            
 
About NayyaAt Nayya, we believe there’s a better way to choose benefits. A more transparent, less confusing way for employees to control their health and financial potential. Powered by billions of data points and machine learning, our benefits experience platform delivers personalized decision support and guidance during open enrollment, new employee onboarding, qualifying life events, and in the moments that matter all year round. This is one of the most stressful and challenging situations consumers face – and we see that as an opportunity to build an innovative response that can help millions of Americans possess the control and understanding they deserve.Your RoleWork directly with the Nayya health data infrastructure team to power the claims data extraction and integration platform. In addition to maintaining and working on claims file feeds and API integrations, you will join the team as we build a high volume, batch file claims data pipeline.The team maintains an internal facing API to power end user applications that leverage claims to enhance user experiences. You will help with maintaining that infrastructure, building new endpoints to power new use cases as well as working on an emerging, centralized data strategy for the organization. We will be working as a team to enhance data validation, governance, and access to organizational data from a single source of truth while building new infrastructure to exceed product requirements for our 2024 product goals and beyond. This is an excellent opportunity to get hands-on experience solidifying and scaling data infrastructure at a fast-growing startup.Responsibilities Build and maintain our Interoperability API and claims file feed integrations.Help design and implement new batch processing infrastructure.Enhance our data enrichment service to interface with external/third party data sources.Help design and implement a de-identified reporting and analytics platform.Maintain and improve an existing Python tech stack with a focus on security and scalability for data storage and API endpoints.Work with the internal stakeholders to build and maintain data and pipelines for various use cases.Recommend monitoring and analytics tools to automate common data needs and visibility.
Requirements Strong SQL skills including query performance and optimization techniques.Experience building and maintaining APIs to serve data to internal stakeholders.Familiarity with common warehousing and data lake services.Data pipeline development and maintenance experience.Familiarity with batch processing pipelines.Familiarity with data pipeline tuning and performance testing.Familiarity with building data processing infrastructure from the ground up.Familiarity with one or more RDBMS.Ability to identify tradeoffs for warehousing vs data lake infrastructure and applying solutions to the appropriate use case.Familiar with common pitfalls in high volume, partitioned data ingestion pipelines such as orphaned records and table locks.
Nice To Haves Experience with Terraform.Experience with data streaming pipelines and sources.Experience building and maintaining APIs (Flask, FastAPI, etc).Experience with AWS managed services and data engineering solutions.Experience with other languages such as Go, Ruby, and Javascript.
The salary range for New York based candidates for this role is $140,000-$175,000. We use a location factor to adjust this range for candidates that are located outside of geographic region of our New York office. Placement within the salary band is determined based on experience.#BI-HybridWhy Join Nayya?  Be an early employee of a quickly growing, VC-backed start-up - grow with us! Have a meaningful impact on a platform that is scaling very rapidlyContribute to a values-based culture with an emphasis on empowerment and autonomyWork in a highly collaborative, cross-functional environmentBenefits include: Competitive pay, employer-paid healthcare, stock options
Nayya is proud to be an Equal Employment Opportunity employer. We do not discriminate based upon race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Charlotte, NC (Hybrid)",https://www.linkedin.com/jobs/view/3773086802/?eBP=JOB_SEARCH_ORGANIC&refId=A2pTL530PT8RrsyeqTc7vA%3D%3D&trackingId=Jz6Des1TqbTd7BaQgXNjvw%3D%3D&trk=flagship3_search_srp_jobs,3773086802,"About the job
            
 
Who is Recruiting from Scratch : Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.https://www.recruitingfromscratch.com/This is a hybrid role based in our Palo Alto or San Francisco offices and will require you to be in office Tuesdays and Thursdays.What’s so interesting about this role?We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.What’s the job?We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.Responsibilities:  Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models  Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling  Be self-motivated in seeking solutions when the correct path isn’t always known  Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders  Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams  Build data processing streams for cleaning and modeling text data for LLMs  Research and evaluate new technologies in the big data space to guide our continuous improvement  Collaborate with multi-functional teams to help tune the performance of large data applications  Work with Privacy and Security team on data governance, risk and compliance initiatives  Work on initiatives to ensure stability, performance and reliability of our data infrastructure 
What We’ll Love About You  Bachelors in Computer Science, Mathematics, Physics, or a related fields  5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience  Experience in statistical analysis & visualization on datasets using Pandas or R  Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools  Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models  Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy  Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control  Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)  Experience with any public cloud environment - AWS, GCP or Azure  Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc  Experience building and maintaining ETL (managing high-quality reliable ETL pipelines) 
We’ll really swoon if you have   2+ years of experience of technical leadership in building data engineering pipelines for AI  Previous experience in building data pipeline for conversational AI APIs and recommender systems  Experience with distributed systems and microservices  Experience with Kubernetes and building Docker images  Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming  Strong understanding of applied machine learning topics  Be familiar with legal compliance (with data management tools) data classification, and retention  Consistent track record of managing and implementing complex data projects 
What You'll Love About Us  Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world  Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto  Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents  Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US  Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs  Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more  Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events 
Base Pay Range$160,000—$280,000 USDhttps://www.recruitingfromscratch.com/"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Elk Grove, CA (Hybrid)",https://www.linkedin.com/jobs/view/3773086811/?eBP=JOB_SEARCH_ORGANIC&refId=A2pTL530PT8RrsyeqTc7vA%3D%3D&trackingId=BxsS7tUHwiw6LdYfXC68XA%3D%3D&trk=flagship3_search_srp_jobs,3773086811,"About the job
            
 
Who is Recruiting from Scratch : Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.https://www.recruitingfromscratch.com/This is a hybrid role based in our Palo Alto or San Francisco offices and will require you to be in office Tuesdays and Thursdays.What’s so interesting about this role?We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.What’s the job?We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.Responsibilities:  Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models  Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling  Be self-motivated in seeking solutions when the correct path isn’t always known  Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders  Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams  Build data processing streams for cleaning and modeling text data for LLMs  Research and evaluate new technologies in the big data space to guide our continuous improvement  Collaborate with multi-functional teams to help tune the performance of large data applications  Work with Privacy and Security team on data governance, risk and compliance initiatives  Work on initiatives to ensure stability, performance and reliability of our data infrastructure 
What We’ll Love About You  Bachelors in Computer Science, Mathematics, Physics, or a related fields  5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience  Experience in statistical analysis & visualization on datasets using Pandas or R  Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools  Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models  Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy  Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control  Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)  Experience with any public cloud environment - AWS, GCP or Azure  Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc  Experience building and maintaining ETL (managing high-quality reliable ETL pipelines) 
We’ll really swoon if you have   2+ years of experience of technical leadership in building data engineering pipelines for AI  Previous experience in building data pipeline for conversational AI APIs and recommender systems  Experience with distributed systems and microservices  Experience with Kubernetes and building Docker images  Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming  Strong understanding of applied machine learning topics  Be familiar with legal compliance (with data management tools) data classification, and retention  Consistent track record of managing and implementing complex data projects 
What You'll Love About Us  Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world  Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto  Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents  Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US  Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs  Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more  Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events 
Base Pay Range$160,000—$280,000 USDhttps://www.recruitingfromscratch.com/"
GCP Cloud Data Engineer,NR Consulting,"Dallas, TX (Hybrid)",https://www.linkedin.com/jobs/view/3768019237/?eBP=JOB_SEARCH_ORGANIC&refId=A2pTL530PT8RrsyeqTc7vA%3D%3D&trackingId=D%2BHYX0DeyTkTdFUSdFT1XA%3D%3D&trk=flagship3_search_srp_jobs,3768019237,"About the job
            
 
Job DescriptionData engineer who can work on migrating the Spark (Pyspark / Scalaspark) jobs to Cloud based environments, Data pipeline building using Spark,Biquery, Deploy the jobs to multiple environments and monitor the Production jobs. Co-ordinate with offshore & onsite team, any reports generation. Migrating the scripts from Oozie to GCP Composer based scripts.Tech Stack"" Spark"" GCP Composer (Dags creation)"" GCP Dataproc"" GCP BigQuery, GCS"" Oozie"" HiveQL"" Terradata SQL"
Data Engineer - SME III (Remote),Akima,"Clarksburg, WV (Hybrid)",https://www.linkedin.com/jobs/view/3770477987/?eBP=JOB_SEARCH_ORGANIC&refId=A2pTL530PT8RrsyeqTc7vA%3D%3D&trackingId=CjSB5H9FSLAPdvhmscXW3Q%3D%3D&trk=flagship3_search_srp_jobs,3770477987,"About the job
            
 
Work Where it MattersTuva, an Akima company, is not just another federal IT contractor. As an Alaska Native Corporation (ANC), our mission and purpose extend beyond our exciting federal projects as we support our shareholder communities in Alaska.At Tuva, the work you do every day makes a difference in the lives of our 15,000 Iñupiat shareholders, a group of Alaska natives from one of the most remote and harshest environments in the United States.For our shareholders, Tuva provides support and employment opportunities and contributes to the survival of a culture that has thrived above the Arctic Circle for more than 10,000 years.For our government customers, Tuva helps optimize IT systems, tools, and methods, as well as mission support services and specialized technologies.As a Tuva employee, you will be surrounded by a challenging, yet supportive work environment that is committed to innovation and diversity, two of our most important values. You will also have access to our comprehensive benefits and competitive pay in addition to growth opportunities and excellent retirement options.Job Summary:The NCIC is a computerized database of documented criminal justice information consisting of 21 files. All of the existing NCIC and new N3G functionality will be migrated to, and developed on, an approved public cloud, such as Amazon Web Services (AWS) GovCloud, requiring real-time communication and data exchange between the mainframe and the public cloud for a transitional period of time.TUVA seeks to hire a Data Engineer to support SMEs services to be provided to our government client related to Data Engineering and to provide new test data to support large scale performance testing and the functionality of the NCIC database system being transitioned to the cloud.Job Responsibilities: The Data Engineer shall work under the direction of the Government Task Lead. The Data Engineer will work with their Government counterparts on Sprint Teams through the Agile development of the N3G system. The data engineer will be responsible for all aspects of data profiling, data design, data management, and production of test data. 
Minimum Qualifications: A minimum of six (6) years of data management, database administration or equivalent experience. A minimum of four (4) years of data engineering, data analysis, or equivalent experiencePossess a Bachelor of Arts (B.A.) or Bachelor of Science (B.S.) degree from an accredited institution or equivalent experience related to the scope and objectives of the contract. Top Secret Security Clearance. In addition to the education and experience requirements, candidates shall possess superior verbal and written communication skills with the ability to work without supervision. Candidates shall also possess excellent analytical, interpersonal, and presentation skills. 
Desired Qualifications: Prior work experience with the federal government. Prior work experience with either Amazon Web Services (AWS). Familiarity with Agile development. 
We are an equal opportunity employer. All applicants will receive consideration for employment, without regard to race, color, religion, creed, national origin, gender or gender-identity, age, marital status, sexual orientation, veteran status, disability, pregnancy or parental status, or any other basis prohibited by law. If you are an individual with a disability, and would like to request a reasonable accommodation for any part of the employment process, please contact us at job-assist@akima.com or 571-353-7053 (information about job applications status is not available at this contact information).\#CJP-1234Job: Information TechnologyTravel: NoOrganization: TUVAClearance: TSShift: Day JobWork Type: HybridReq ID: TUV03083"
"Member of Technical Staff, Data Engineer",Inflection AI,"Palo Alto, CA (Hybrid)",https://www.linkedin.com/jobs/view/3593512326/?eBP=JOB_SEARCH_ORGANIC&refId=A2pTL530PT8RrsyeqTc7vA%3D%3D&trackingId=ivyhX59Rqe0%2FvtmylcIZ4w%3D%3D&trk=flagship3_search_srp_jobs,3593512326,"About the job
            
 
About The RoleYou are an experienced data practitioner who will work closely with AI and infrastructure staff to improve our AI through better data. You will be responsible for collecting AI training data, optimizing how we use that data, and analyzing data to inform product direction. Data is key to Inflection’s success, so your work will have a massive impact and will directly improve our models and product.You are expected to have experience building large scale data processing pipelines, using tools like PySpark, Beam, or Flink. Expertise in ML is not necessary, but a familiarity with Machine Learning and NLP and a willingness to learn more on the job are important. A track record of adapting to new domains and a desire to use data to improve products are strong indicators for success in this role.You may have relevant experience as a Data Scientist, ML Engineer, Data Engineer or Software Engineer, but will be expected to apply your data skills to every step of building our large language model product.Employee Pay DisclosuresAt Inflection AI, we aim to attract and retain the best employees and compensate them in a way that appropriately and fairly values their individual contributions to the company. The pay range for this position in California, is estimated to fall in the base range of approximately $150,000 - $250,000. This estimate can vary based on the factors described above, so the actual starting annual base salary may be above or below this range.About InflectionWe are a small, friendly and multi-disciplinary AI studio creating a personal AI for everyone. Our first AI is called Pi, for personal intelligence, a supportive and empathetic conversational AI. Our studio is made up of the world's leading AI developers, creative designers, writers and innovators working together in a deeply multidisciplinary style to create a brand new class of digital experiences.Where We WorkWe work in a hybrid model. During the week we’re in the office on a flexible basis, often a few days per week. We are currently hiring in Palo Alto and London, although we do sometimes make exceptions.We work on a 6 week cycle. Every 7th week, we convene together in person as a company to brainstorm, bond and build.How We WorkWe value excellence and ownership. Our organizational structure focuses on individual responsibilities rather than management hierarchies. Everyone is expected to lead by doing. We are big believers in the unreasonable effectiveness of highly talented Individual Contributors who are given all the resources, space and ownership to move fast and deliver outstanding results.Teamwork and generosity are at our core. Our culture celebrates positive challenges, asking questions, learning and actively supporting one another. This mentality of shared respect and purposeful teamwork is key to our success. We equally value all technical and non-technical contributions.Constructive disagreement is essential. We appreciate when team members challenge assumptions, put forward new ideas, or encourage us to move faster or slower. Openness, honesty and kindness make us great.Feedback is our ground truth. We have a tight feedback loop between the user experience and our AI creation process. Quantitative and qualitative data drives our priorities. This goes for internal culture too. Everyone has ownership and visibility into key decisions and progress.Writing creates accountability. Whether on internal communication tools or in team memos, we are strong communicators with a special focus on the written word.We deeply value time to reset outside of work. We encourage one another to constantly take time to recharge and always focus on maintaining a healthy work-life balance.Engineering at InflectionWe are a vertically integrated AI studio. This means that our entire technology stack – from large foundational model pre-training to the user interface – is built in-house, with each of the components co-optimized to deliver the best AI experiences. We have built one of the most advanced large language models in the world, based on multiple novel and proprietary innovations.We believe in scale as the engine of progress in AI, and we are building one of the largest supercomputers in the world to develop and deploy the new generation of AIs.We wear multiple hats and don’t distinguish between engineering and research. We continuously explore and exploit, creating new and perfecting existing techniques and solutions. User feedback is our North Star.Our BenefitsWe offer generous benefits to ensure a positive, safe, inclusive and inspiring work environment for all Inflectioneers. Physical office space in Palo Alto and London, along with reimbursements for work from home expenses and co-working spacesUnlimited paid time off Parental leave and flexibility for all parents and caregiversGenerous medical, dental and vision plans for US employeesCompliance with country-specific benefits for non-US employeesVisa sponsorship for new hiresAvenues for personal growth such as coaching, conference attendance, or specific trainings
Diversity & InclusionWe are building personal AIs that we hope will serve everyone. We are deeply committed to representing the full extent of the human experience inside our AI Studio. This means that everyone from any walk of life is welcome if you have the right skills. We populate diverse candidate pools for all open roles."
Staff Data Engineer (AI/ML),Recruiting from Scratch,San Francisco Bay Area (Hybrid),https://www.linkedin.com/jobs/view/3774162073/?eBP=JOB_SEARCH_ORGANIC&refId=A2pTL530PT8RrsyeqTc7vA%3D%3D&trackingId=4%2FWsSFMD8JSkJvsM8oLC9g%3D%3D&trk=flagship3_search_srp_jobs,3774162073,"About the job
            
 
What’s so interesting about this role?Our Data Engineer lead is responsible for building high-quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize, and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety. What’s the job?We’re looking for an exceptional data engineer who is passionate about data for AI and the values it can bring to the company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes the identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment, and monitoring tools all for AI models. As a tech lead specializing in data engineering, you are expected to code and contribute to the stack.Responsibilities: Dive into our dataset and design, implement, and scale data pre/post-processing pipelines of ML modelsWork on applied ML solutions in the areas of data mining, cleaning, normalizing and modelingBe self-motivated in seeking solutions when the correct path isn’t always knownCollaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders Design and build data platforms & frameworks for processing high volumes of data, in real-time as well as batch, that will be used across engineering teams Build data processing streams for cleaning and modeling text data for LLMsResearch and evaluate new technologies in the big data space to guide our continuous improvement Collaborate with multi-functional teams to help tune the performance of large data applications Work with the Privacy and Security team on data governance, risk and compliance initiatives Work on initiatives to ensure the stability, performance and reliability of our data infrastructure 
What we’ll love about you  Bachelors in Computer Science, Mathematics, Physics, or a related fields5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experienceExperience in statistical analysis & visualization of datasets using Pandas or RExperience designing and building highly available, distributed systems of data extraction, ingestion, normalization, and processing of large data sets in real-time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow, or other pipeline toolsDemonstrated prior experience in creating data pipelines for text data sets NLP/ large language modelsAbility to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategyExcellent coding skills in Python, Java, bash, SQL, and expertise with Git version controlExperience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)Experience with any public cloud environment - AWS, GCP or AzureSignificant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etcExperience building and maintaining ETL (managing high-quality reliable ETL pipelines)
We’ll really swoon if you have  2+ years experience of technical leadership in building data engineering pipelines for AIPrevious experience in building data pipelines for conversational AI APIs and recommender systemsExperience with distributed systems and microservicesExperience with Kubernetes and building Docker imagesExperience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-StreamingStrong understanding of applied machine learning topicsBe familiar with legal compliance (with data management tools) data classification, and retentionConsistent track record of managing and implementing complex data projects
What you'll love about us Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the worldMultiple Locations: We are hiring someone for this role to be based ideally in San Francisco, Palo Alto or the Chicago Metro area. Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependentsRetirement Savings: Generous 401K plan with 6% match and immediate vest in the USCompensation: Industry-competitive compensation and eligibility for company bonus and equity programsQueer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and moreAdditional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events
About the CompanyWe are the world’s largest dating app for gay, bi, trans, and queer people. With around 13 million monthly active users, we've become a fundamental part of the global LGBTQ community, and we take pride in empowering our users to connect, express themselves, and discover the queer world around them.Our next evolution is underway as a newly public company looking to expand and continue to build meaningful experiences for our users. From social issues to original content, we’re blazing innovative paths with impact for our community, all while leveraging the latest tech stacks and striving for engineering excellence. At the heart of our work in this new chapter is a shared set of core values: openness and exploration, a bias for action, and strong support of the LGBTQ community. With a track record of strong financial performance and plans for continued headcount growth, we’re looking to build a team of talented, passionate, and open-minded people who believe in our mission, align with our values, and are excited to work at the intersection of innovative technology and social impact. Come be a part of this exciting journey with us.Who is Recruiting from Scratch:  Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more. Base salary: $160,000-$275,000 USD base"
Data Engineer,"IDR, Inc.","Nashville, TN (Hybrid)",https://www.linkedin.com/jobs/view/3724450615/?eBP=JOB_SEARCH_ORGANIC&refId=A2pTL530PT8RrsyeqTc7vA%3D%3D&trackingId=4w0nOAJsS77qPyeOYJACdA%3D%3D&trk=flagship3_search_srp_jobs,3724450615,"About the job
            
 
IDR is seeking a Data Engineer to join one of our top healthcare clients in Nashville, TN! This role offers extreme growth opportunities and the opportunity to work with new technologies! This role requires onsite work for the first 1-2 weeks and will then transition to remote work. This is a 6-month contract to hire opportunity. If you are ready to take you career to the next step, apply today! No C2C**
Position Overview For The Data Engineer Assist in the design of data structures and solutionsDevelop and modify SQL queries for IT and other departmentsAssist in the transfer of various data to a client master databaseCreate data models per client requirementsDocumentation of system and data process flows
Required Skills For The Data Engineer 4-6 years of experience in data engineering4+ years of hands-on experience with SQL Queries, Microsoft 365, Azure Data Factory, and PowerbiHealthcare industry experience preferredKnowledge of Data Warehousing patternsAgile Environment experience
What’s in it for the Data Engineer? Full Benefits! (Medical, Dental, Vision)Competitive compensationGreat work/life balanceAbility to work for a well-established, stable company
Why IDR? 20+ Years of Proven Industry Experience in 4 major markets.Employee Stock Ownership Program.Dedicated Engagement Manager who is committed to you and your success.Medical, Dental, Vision, and Life Insurance.ClearlyRated’s Best of Staffing"
Principal Data Engineer,Geode Capital Management,"Boston, MA (Hybrid)",https://www.linkedin.com/jobs/view/3758204534/?eBP=JOB_SEARCH_ORGANIC&refId=A2pTL530PT8RrsyeqTc7vA%3D%3D&trackingId=Z5PQyPvKKIejx4wuYGgMPg%3D%3D&trk=flagship3_search_srp_jobs,3758204534,"About the job
            
 
Geode Capital Management is actively searching for a full-time Principal Data Engineer to become an integral part of our Data team. The Principal Data Engineer will play a very critical role within our data team by providing technical leadership, implementation expertise, design best practices and guiding other highly skilled and motivated data engineers. In this capacity, you will work closely with the analytics team, architecture, and data analysts to gather requirements and contribute to the development of a best-in-class analytics platform centered around Snowflake and Tableau.This is a hybrid work environment with a weekly in-office schedule of Tuesdays, Wednesdays, and Thursdays with remote work availability on Mondays and Fridays.Responsibilities Build best in class analytics platform for our business teams to gain insights from our dataCollaborate with Product owners, Analytics Lead, Scrum Master, and Development teams to build effective data integration pipelines and Rest APIs on hosted dataMentor new or junior team members and participate in code reviewsCollaborate in design, development, and implementation of software and data solutionsCollaborate with development team in building innovative, event driven, API-first cloud native solutions using AWS platform, Snowflake, Snaplogic, Tableau, Python and Rest APIsiteratively deliver high quality products to enable our investment business, operations, analytics, and Client reporting
Skills You Bring A Bachelor’s or Master’s degree in computer science or a similar field5+ years of software and or data engineering experienceStrong implementation experience with snowflake ecosystem, knowledge of Snowflake architecture and conceptsStrong background in modeling and building data warehousesStrong working experience in Snowflake using warehouses, stored procedures, streams, snow pipes, tasks, stages, storage integration, ingestion frameworks and tools etc.Ability to develop ELT/ETL pipelines to move data to and from Snowflake data store using combination of Python, dbt and SnowSQLStrong experience in ETL/ELT technologies such as informatica, Snaplogic etc.Hands on experience building cloud native applications using AWS or AzureGood hands-on experience with scheduling tools such as Airflow, Control M, AutosysExcellent SQL skills, writing SQL stored procedures and functionsExperience in leading teams, propose design, establish best practices and engineering disciplineExperience building reports, dashboards, and visualizations with business intelligence tools such as Tableau or Power BI is preferredExperience with build/deploy automation & DevOps frameworksFamiliarity in building and consuming APIs (REST, API Gateway, etc.)Possessing outstanding technical and analytical skills with strong business knowledge in the financial services domainGood project management, experience in Agile methodologies (Kanban and SCRUM), communication, and interpersonal skillsStrong hands-on experience with requirements gathering analysis, coding, testing, implementation, maintenance, and review.
Company OverviewFounded in 2001, Geode is headquartered in Boston’s financial district, the center of one of the world’s most vibrant finance and technology hubs and employs approximately 170 employees.Geode is a systematic asset manager providing core beta exposures across a range of equity and niche asset classes, with over $1 trillion in AUM as of September 30, 2023. With a robust infrastructure and experienced investment professionals, Geode offers the scale of a large asset management firm with the benefits of a smaller organization.Geode is proud to be an equal opportunity employer and support a diversified work environment. Learn more about Geode at www.geodecapital.com/careers."
Data Engineer,Motion Recruitment,"New York, NY (Hybrid)",https://www.linkedin.com/jobs/view/3742725823/?eBP=JOB_SEARCH_ORGANIC&refId=A2pTL530PT8RrsyeqTc7vA%3D%3D&trackingId=WIbV3A3Whop1vFOR0DZX4A%3D%3D&trk=flagship3_search_srp_jobs,3742725823,"About the job
            
 
My client is looking for a Data Engineer to join their growing team, this is a large and established media company here in NYC. They are looking for a Data Engineer who will be responsible for expanding and optimizing their data and data pipeline architecture, as well as optimizing data flow and collection.The team functions in a hybrid working capacity, with an office in Manhattan. They are offering a competitive base salary, with a generous equity package.Qualifications Professional Data Engineering Experience, 3+ yrs Strong experience with their current stack. Python, SQL, AWS. Experience working with Snowflake is a plus. 
Offer/Benefits Competitive base salary and generous equity Strong healthcare coverage for you and your dependents Hybrid in their Manhattan office 401(k) Program 
Posted By: Max Singer"
Data Transmission Engineer,"The Dignify Solutions, LLC","Raleigh, NC (Hybrid)",https://www.linkedin.com/jobs/view/3768016054/?eBP=JOB_SEARCH_ORGANIC&refId=A2pTL530PT8RrsyeqTc7vA%3D%3D&trackingId=7IkRi4Hg2DHJ9N8YW2YFSg%3D%3D&trk=flagship3_search_srp_jobs,3768016054,"About the job
            
 
Should have strong experience setting up file transmission jobs using Connect:Direct Tool and Sterling File Gateway Should be flexible and available for support on a roster rotation basis Experience and competency with scripting languages like powershell, bash, etc Extensive knowledge, skills and background in various development and methodologies waterfall, agile, scrum, etc Should have very good communication skills to coordinate efforts for implementation with application stakeholders Bachelor's degree in Computer Science or related discipline or equivalent education and related training Connect:Direct, Sterling File Gateway, Changeman, ServiceNow Experience working with CA Workload Automation (ESP) Experience deploying /installation of connect direct agent"
Senior Data Engineer,MITRE,"McLean, VA (Hybrid)",https://www.linkedin.com/jobs/view/3774835962/?eBP=JOB_SEARCH_ORGANIC&refId=A2pTL530PT8RrsyeqTc7vA%3D%3D&trackingId=PPk8%2Bh2hp35kksj9UqcWjA%3D%3D&trk=flagship3_search_srp_jobs,3774835962,"About the job
            
 
Why choose between doing meaningful work and having a fulfilling life? At MITRE, you can have both. That's because MITRE people are committed to tackling our nation's toughest challenges—and we're committed to the long-term well-being of our employees. MITRE is different from most technology companies. We are a not-for-profit corporation chartered to work for the public interest, with no commercial conflicts to influence what we do. The R&D centers we operate for the government create lasting impact in fields as diverse as cybersecurity, healthcare, aviation, defense, and enterprise transformation. We're making a difference every day—working for a safer, healthier, and more secure nation and world. Our workplace reflects our values. We offer competitive benefits, exceptional professional development opportunities, and a culture of innovation that embraces diversity, inclusion, flexibility, collaboration, and career growth. If this sounds like the choice you want to make, then choose MITRE—and make a difference with us.Department Summary:The Data Environments and Engineering Department (L176) in the Data and Human Centered Solutions Innovation Center is seeking highly motivated, adaptive, and enthusiastic candidates with a wide variety of interests and a foundation of skills in systems engineering, software engineering, information systems, or data engineering to join our department as a Senior Data Engineer. Candidates should have a strong interest in addressing and solving challenges related to capturing, managing, and sharing data supporting decision making and enabling mission processes.Roles And ResponsibilitiesWorking with experienced project teams and data management experts, project tasks may include: Designing and building prototypes for ingesting, curating, and analyzing data at scale.Collecting and preparing data for use by data scientists and analystsSupporting stakeholder interviews to determine data requirements and define data flows.Documenting data transformation rules to meet the requirements for system interfaces and data integration projectsPerforming data profiling to identify quality gaps and determine if data is fit for use for its intended purposeResearching and evaluating data engineering tools and technologiesLeading or supporting sponsor work conducted in a lab environment
Basic Qualifications: Bachelor’s degree and 5 years of related experience.Be able to obtain and maintaining a DoD clearance.Strong developer skills using SQL and PythonHands-on experience with one or more database technologies (e.g., PostgreSQL, Oracle, MySQL, SQL Server, DB2, MongoDB, Neo4j, Couchbase, Cassandra)Excellent written and verbal communications skills with ability to describe and present complex technical concepts in relatable termsStrong knowledge of ETL/ELT processes, tools, and frameworksExperience with large scale data architectures. Activities may include analysis of alternatives, technical product evaluations, requirements analysis, design, data systems prototyping, and process automation.Experience building conceptual, logical, and physical data modelsKnowledge of cloud platforms and technologies such as Amazon Web Services, Microsoft Azure, and Google Cloud Platform.Proficiency in one or more modern programming languages such as Python, R, Scala, C++, Java, C#, and JavaScriptData Engineering skills to include: Expertise in preparing data for analysisStrong understanding of data science conceptsExperience with BI toolsUnderstanding ML frameworks, algorithms, and libraries
Applied mathematics: probability and statisticsStrong innovation, problem-solving, systems analysis, planning, organizational, and interpersonal skillsKnowledge of data management practices that align with modern/emerging analytics techniquesDesire to continuously learn new techniques and evolve approaches by regularly engaging in structured educational pursuits, personal research, and outreach to subject matter experts across the corporation
Preferred Qualifications: Master’s degree in computer science, data science, systems engineering, software engineering, or information systemsExperience migrating legacy data systems to new/modernized platformsHands-on experience with data flow and pipeline frameworks such as Kafka, NiFi, Spark, Airflow, and Hadoop.Familiarity with microservice architectures, data warehousing, data fabric, data lakes, and data meshUnderstand the use of data in AI/ML and LLMKnowledge and experience across data management domains, including data science, data governance, metadata management, data quality, business intelligence, data security & privacy, and data strategyExperience performing source to target data mappingKnowledge of natural language processing techniques
This requisition requires the candidate to have a minimum of the following clearance(s):NoneThis requisition requires the hired candidate to have or obtain, within one year from the date of hire, the following clearance(s):SecretWork Location Type:HybridMITRE is proud to be an equal opportunity employer. MITRE recruits, employs, trains, compensates, and promotes regardless of age; ancestry; color; family medical or genetic information; gender identity and expression; marital, military, or veteran status; national and ethnic origin; physical or mental disability; political affiliation; pregnancy; race; religion; sex; sexual orientation; and any other protected characteristics. For further information please visit the Equal Employment Opportunity Commission website EEO is the Law Poster and Pay Transparency.MITRE intends to maintain a website that is fully accessible to all individuals. If you are unable to search or apply for jobs and would like to request a reasonable accommodation for any part of MITRE’s employment process, please email recruitinghelp@mitre.org.Copyright © 1997-2023, The MITRE Corporation. All rights reserved. MITRE is a registered trademark of The MITRE Corporation. Material on this site may be copied and distributed with permission only.Benefits information may be found here"
AWS Data Engineer,Extend Information Systems Inc.,"Newark, NJ (Hybrid)",https://www.linkedin.com/jobs/view/3742890667/?eBP=JOB_SEARCH_ORGANIC&refId=A2pTL530PT8RrsyeqTc7vA%3D%3D&trackingId=qLbvbFrFuig0rRZDbrkEMA%3D%3D&trk=flagship3_search_srp_jobs,3742890667,"About the job
            
 
Job Title: AWS Data EngineerLocation: Newark, NJ (1 time in a week)Duration: Long TermExperience level : 9+ yrs Job Description Designing solutions using AWS services.Building proof of concepts for new tools, services.Working, coordinating with, and guiding offshore team.
Skills \ Competencies we are looking for: Overall experience 8-12 years 3+ years of Healthcare Payer experience within last 4 years. (Must)ETL experience.Experience with building Data Ingestion, Pipelines for high volume data.In depth experience with AWS services like Redshift, Glue, Athena, Pyspark, EMR, S3 Python experience. Advanced SQL experience.Familiarity with Dev Ops.
Thanks & Regards !!Anoop TiwariExtend Information SystemsCell: - Email: Anoop@extendinfosys.com"
Senior Data Engineer,Mastercard,"Missouri, United States (Hybrid)",https://www.linkedin.com/jobs/view/3776397212/?eBP=JOB_SEARCH_ORGANIC&refId=A2pTL530PT8RrsyeqTc7vA%3D%3D&trackingId=f5Mdkcgkv7Ei4n6jHSOEOQ%3D%3D&trk=flagship3_search_srp_jobs,3776397212,"About the job
            
 
Our PurposeWe work to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments and businesses realize their greatest potential. Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. We cultivate a culture of inclusion for all employees that respects their individual strengths, views, and experiences. We believe that our differences enable us to be a better team – one that makes better decisions, drives innovation and delivers better business results.Title And SummarySenior Data EngineerOverviewWe are seeking a talented and motivated Senior Data Engineer to join our data engineering team. In this role, you will play a critical part in designing, developing, and optimizing data pipelines and solutions that enable efficient data processing and analysis. As a Senior Data Engineer, you will collaborate with cross-functional teams to drive data-driven decision-making and contribute to the continuous improvement of our data infrastructure.Role  Design, develop, and maintain new data capabilities and infrastructure for utilizing Mastercard, third-party, and partner data to enhance Mastercard's data products and solutions Create new data pipelines, data transfers, and compliance-oriented infrastructure to facilitate seamless data utilization within cloud environments Identify existing data capability and infrastructure gaps or opportunities within and across initiatives and provide subject matter expertise in support of remediation Collaborate with technical team and business stakeholders to understand data requirements and translate them into technical solutions Work with large datasets, ensuring data quality, accuracy, and performance Implement data transformation, integration, and validation processes to support analytics and reporting needs Optimize and fine-tune data pipelines for improved speed, reliability, and efficiency Implement best practices for data storage, retrieval, and archival to ensure data accessibility and security Troubleshoot and resolve data-related issues, collaborating with the team to identify root causes Document data processes, data lineage, and technical specifications for future reference Participate in code reviews, ensuring adherence to coding standards and best practices Collaborate with DevOps teams to automate deployment and monitoring of data pipelines Additional tasks as required
All About You  Bachelor's or Master's degree in Computer Science, Engineering, or a related field Proven experience in data engineering, with a strong track record of designing and implementing data solutions Proficiency in programming languages such as Python, Java, or Scala, and experience with data processing frameworks (Spark, Hadoop, etc.) In-depth understanding of data warehousing concepts, cloud platforms (AWS, Azure, GCP), and data modeling techniques Strong knowledge of SQL and NoSQL databases, as well as data integration and transformation tools Passion for and engagement with emerging trends in data, AI/ML, analytics, and digital experiences Experience in data product development, analytical models, and model governance Experience in anonymizing data and managing the use of data Experience in data hygiene procedures, identity resolution capabilities or data management a plus Ability to create strategies and plans that define how information can be utilized to support an organization's overall business strategy, and how that information and data is organized, and governed inside an organization Strong project management skills and a demonstrated ability to understand complex information product constructs Familiarity with industry best practices for collection and use of data Outstanding problem-solving skills and the ability to navigate complex data challenges Effective communication and collaboration skills to work with both technical and non-technical stakeholders Experience with agile methodologies and DevOps practices
What is Data & Services?The Data & Services Team (D&S) is a key differentiator for Mastercard, providing the cutting-edge services that help our customers grow. Focused on thinking big and scaling fast around the globe, this team is responsible for end-to-end solutions for a diverse global customer base. We combine traditional management consulting with our rich data assets and in-house technology to provide our clients with powerful insights and tools to drive fact-based decision making. Centered on data-driven technologies and innovation, our services include consulting, loyalty and marketing programs, test-and-learn business experimentation, and data-driven information and risk management. While specializing in the payments industry, Mastercard Data & Services also works closely with major retailers, airlines, and other enterprises, leveraging data and insights garnered from within and beyond its network.D&S is continuously looking for passionate and talented technologists, who share our vision for empowering our customers to make better fact-based decisions, to join us and shape the growth of our team.In the US, Mastercard is an inclusive Equal Employment Opportunity employer that considers applicants without regard to gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law. If you require accommodations or assistance to complete the online application process, please contact reasonable_accommodation@mastercard.com and identify the type of accommodation or assistance you are requesting. Do not include any medical or health information in this email. The Reasonable Accommodations team will respond to your email promptly.Corporate Security Responsibility All activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must: Abide by Mastercard’s security policies and practices;Ensure the confidentiality and integrity of the information being accessed;Report any suspected information security violation or breach, andComplete all periodic mandatory security trainings in accordance with Mastercard’s guidelines.
In line with Mastercard’s total compensation philosophy and assuming that the job will be performed in the US, the successful candidate will be offered a competitive base salary based on location, experience and other qualifications for the role and may be eligible for an annual bonus or commissions depending on the role. Mastercard benefits for full time (and certain part time) employees generally include: insurance (including medical, prescription drug, dental, vision, disability, life insurance), flexible spending account and health savings account, paid leaves (including 16 weeks new parent leave, up to 20 paid days bereavement leave), 10 annual paid sick days, 10 or more annual paid vacation days based on level, 5 personal days, 10 annual paid U.S. observed holidays, 401k with a best-in-class company match, deferred compensation for eligible roles, fitness reimbursement or on-site fitness facilities, eligibility for tuition reimbursement, gender-inclusive benefits and many more.Pay RangesO'Fallon, Missouri: $115,000 - $184,000 USD"
Senior Data Engineer,FloSports,"Austin, Texas Metropolitan Area (Hybrid)",https://www.linkedin.com/jobs/view/3770053735/?eBP=JOB_SEARCH_ORGANIC&refId=A2pTL530PT8RrsyeqTc7vA%3D%3D&trackingId=Qi3SSLgs3%2BMoru1NTqX8gg%3D%3D&trk=flagship3_search_srp_jobs,3770053735,"About the job
            
 
DescriptionPosition at FloSportsFloSports is a world-class sports media company strategically positioned to be the essential destination for passionate sports fans, delighting them with live event coverage, breaking news, highlights, stats, rankings, and team and player profiles. We are growing Our Sports every day by continuing to invest in our ever-expanding ecosystem, which consists of over a dozen sport verticals and hundreds of streaming partners. FloSports is creating the home for sports like hockey, track & field, racing, grappling, wrestling, cheer and more, and we are looking for innovative and passionate people like you to help us!THE ROLE:We are on the hunt for a seasoned Sr. Data Engineer with a profound specialization in data transformations. As a linchpin in our data team, you'll work alongside other data professionals to ensure our data is efficiently structured and transformed, facilitating insightful analytics, business intelligence, and product requirements.RESPONSIBILITIES:  Data Transformation Development by:  Crafting and optimizing data transformation processes, converting raw datasets into structured useful formats.  Working in tandem with data platform developers and Product Managers to understand specific transformation necessities.  Validation & Quality by:  Incorporating rigorous data validation checks within the transformation processes to assure accuracy and consistency.  Collaborating with data ingestion teams to resolve discrepancies in source data and ensuring high data quality standards.  Metadata & Lineage by:  Designing and implementing mechanisms to capture, store, and manage metadata, ensuring data's context, and source are clear.  Documenting and managing data lineage, allowing traceability of data from its source through all its transformations.  Performance & Cost Optimization by:  Continuously fine-tuning transformation processes for optimal performance and efficiency, especially for large datasets.  Ensuring cost-effective data storage and processing, optimizing configurations, and resource utilization.  Continuous Learning & Knowledge Sharing by:  Keeping updated with the latest trends and technologies in data transformations, modeling, and pipeline development.  Leading knowledge-sharing sessions and mentoring junior engineers, fostering a culture of continuous learning. 
KNOWLEDGE, SKILLS AND ABILITIES:  Transformation Tools & Platforms: Proficiency in tools designed for data transformations on a large scale.  Database Expertise: Deep understanding of SQL/NoSQL databases, their structures, and optimization strategies.  ETL & ELT Processes: Comprehensive understanding of ETL/ELT processes, with a particular focus on data transformations.  Data Modeling: Rich experience in logical and physical data modeling tailored to transformed data structures.  Communication: Exceptional ability to convey complex technical details lucidly to diverse audiences.  Analytical Skills: Innate capability to identify patterns, nuances, and opportunities within datasets.  Scalability: Vision to create transformation processes adaptive to burgeoning data volumes and intricacies.  Innovative Problem Solving: Prowess in addressing challenges with novel, effective solutions. 
OUR COMMITMENT TO DIVERSITY:At FloSports, we are bonded by our passion for sports and our purpose to unite communities around experiences that finally give underserved sports the love they deserve. We recognize the need to build a company that seeks out, embraces, and celebrates our individual differences, ideas, and talent. FloSports is committed to the pursuit of a fair, equal and inclusive workplace where everyone is given the opportunity to grow to their fullest potential.OUR BENEFITS:  Recognized three years in a row as a Top Workplace by the Austin-American Statesman  Flexibility at work - you can take control of your profession and personal schedule  All-hands events hosted twice a year in beautiful Austin, Texas  Annual equity awards for all top performers  Competitive and comprehensive medical, dental and vision plans  Peace of mind through company-paid short-term disability, long-term disability and life insurance  Generous 401(K) company match vested immediately  Progressive parental leave policies  Unlimited paid time off  Hack-a-thons and a full calendar of team-building and social events  Free laundry service for all positions that require travel  Company donation to youth teams and leagues that our employees coach  Stocked snack bar, catered lunch and breakfast tacos every week"
AWS Data Engineer,TMS,"Florence, NJ (Hybrid)",https://www.linkedin.com/jobs/view/3742841796/?eBP=JOB_SEARCH_ORGANIC&refId=A2pTL530PT8RrsyeqTc7vA%3D%3D&trackingId=YA9j0TBpACIX4ECGwE7LKg%3D%3D&trk=flagship3_search_srp_jobs,3742841796,"About the job
            
 
Job Title : AWS Data EngineerLocation : Newark, NJ ( 1 Day in a week is must)Duration : Long Term.Job Description Designing solutions using AWS services.Building proof of concepts for new tools, services.Working, coordinating with, and guiding offshore team.
Skills \ Competencies we are looking for - Overall experience 8-12 years3+ years of Healthcare Payer experience within last 4 years. (Must)ETL experience.Experience with building Data Ingestion, Pipelines for high volume data.In depth experience with AWS services like Redshift, Glue, Athena, Pyspark, EMR, S3Python experience.Advanced SQL experience.Familiarity with Dev Ops."
"Principal Data Engineer (Boston, MA)",Huxley,"Boston, MA (Hybrid)",https://www.linkedin.com/jobs/view/3780198526/?eBP=JOB_SEARCH_ORGANIC&refId=A2pTL530PT8RrsyeqTc7vA%3D%3D&trackingId=8FaIzYARbeXR6oeP%2Bf0AlQ%3D%3D&trk=flagship3_search_srp_jobs,3780198526,"About the job
            
 
Principal Data EngineerBoston, MA (Hybrid - 3x/week onsite) An industry-leading GreenTech company is looking to bring on a Principal Data Engineer to the team. They offer a product suite of SaaS and AI tools used by companies around the globe to assess and improve their carbon footprint. In this role, you will be responsible for development, design and architecture of greenfield Big Data solutions within a cloud-native environment. This person will be highly visible across the firm, being a critical thought-leader to ensure delivery on highly-performant, scalable data solutions.  Qualifications:  6-8+ years of professional data engineering experienceExpertise developing Big Data systems centered on AWSHighly skilled using Snowflake, Redshift, S3, Lambda in a data engineering contextProficient with Python programming, PostgreSQL database and REST APIsPassionate about complex problem solving, delivering innovative solutions and working on meaningful projectsBS or MS in Computer Science (or related STEM field) 
Desired Skills and ExperiencePython, SQL, Postgres, Big Data, AWS, Snowflake, Redshift, S3, Lambda, Spark, Hadoop"
Stratascale Sr. Data Engineer,Stratascale – An SHI Company,"Charlotte, NC (Hybrid)",https://www.linkedin.com/jobs/view/3775627737/?eBP=JOB_SEARCH_ORGANIC&refId=A2pTL530PT8RrsyeqTc7vA%3D%3D&trackingId=KUyaY1foYeBciJ8tubTfNg%3D%3D&trk=flagship3_search_srp_jobs,3775627737,"About the job
            
 
Job SummaryIf you are interested in working in a startup like environment where you can directly influence the future of cloud-based solutions and services, then Stratascale is the place for you.The Stratascale engineering team is at the forefront of cybersecurity innovation, providing cutting-edge solutions to protect global enterprises against evolving digital threats. Our commitment to excellence and innovation is matched by our dedication to creating an inclusive and collaborative work environment where each contribution is valued and every individual can thrive.We are seeking a highly skilled and motivated Senior Data Engineer to join our software engineering organization. The ideal candidate will play a pivotal role in building data intelligence capabilities on top of our rich cybersecurity product data. Your work will directly contribute to enhancing our product's value by deriving actionable insights and enabling data-driven decision-making across the company.This position will report to Charlotte, NC on a hybrid work schedule as determined by Stratascale management.About UsStratascale, an SHI company, brings together the benefits of 31 years' experience delivering the very best technologies with a fresh consultative approach to designing, delivering and supporting the technology our customers need to transform their business. We call it Digital Agility.To learn more about Stratascale visit our website: https://stratascale.com/ResponsibilitiesInclude but not limited to:  Design, build, and maintain scalable and robust data pipelines to support analytics and intelligence gathering from various cybersecurity product data sources.  Collaborate with cross-functional teams to understand data needs and implement systems for collecting, storing, processing, and analyzing large datasets.  Ensure data quality and integrity throughout all designed systems and processes.  Develop and optimize data models to improve storage efficiency and data retrieval performance.  Implement secure data handling practices to maintain the confidentiality and integrity of sensitive information in compliance with industry standards.  Stay abreast of emerging technologies and industry trends in data engineering and cybersecurity to drive continuous innovation.  Craft and maintain documentation regarding data engineering processes, systems, and data dictionaries to ensure transparency and knowledge sharing within the team.  Provide mentorship and guidance to junior data engineering staff.  Be on-call for services that the team owns. 
Qualifications  Bachelor’s degree in Computer Science, Engineering, Data Science, or related field.  5+ years of experience in a data engineering role, with a proven track record of building high-volume data pipelines. 
Required Skills  Experience with machine learning algorithms and data science techniques.  Strong programming skills in languages such as Python, Go, or Rust.  Excellent communication and teamwork abilities to effectively collaborate with both technical and non-technical teams.  Experience with Compute, Database, Storage, Networking, and Security services in at least one of the major cloud providers; AWS, Azure, or GCP.  Excellent written and verbal communication skills.  Excellent organizational and time management skills.  Ability to collaborative with other internal departments.  Ability to work independently as well as in a team environment.  Ability to adapt to changing technology environment.  Expertise in SQL and NoSQL databases, data warehousing solutions, and ETL tools.  Experience with big data technologies such as Hadoop, Spark, Kafka, etc.  Knowledge of cloud services (AWS, Azure, GCP) and their data-related offerings.  Familiarity with cybersecurity principles and experience working with cybersecurity data is a plus.  Strong analytical and problem-solving skills with an attention to detail. 
Preferred  A master's degree or higher in a relevant field.  Certifications in cloud technologies or data engineering.  Experience supporting mission critical, 24x7 systems.  Experience with automation tools like CircleCI, GitLab, GitHub, or Jenkins.  Experience working within Agile/Scrum frameworks. 
Certifications Required  Preferred: Experience with multiple data intelligence integrated platforms, AWS, Azure, Google data and analytics certifications 
Additional Information  The estimated annual pay range for this position is $175,000 - $218,750 which includes a base salary and bonus. The compensation for this position is dependent on job-related knowledge, skills, experience, and market location and, therefore, will vary from individual to individual. Benefits may include, but are not limited to, medical, vision, dental, 401K, and flexible spending.  Equal Employment Opportunity – M/F/Disability/Protected Veteran Status 
Compensation StructureBase Plus Bonus Approved Min (Total Target Comp) USD $175,000.00/Yr. Approved Max (Total Target Comp) USD $218,750.00/Yr. Job Wrapping 1"
Lead Big Data Engineer,"Orpine Inc, an Inc 5000 Company","Atlanta, GA (Hybrid)",https://www.linkedin.com/jobs/view/3770171886/?eBP=JOB_SEARCH_ORGANIC&refId=A2pTL530PT8RrsyeqTc7vA%3D%3D&trackingId=iP4UYaMhEF0eLw12JWps7Q%3D%3D&trk=flagship3_search_srp_jobs,3770171886,"About the job
            
 
Job Description:· Must have Skills: Cloud architecture (Strong), Cloud development (Strong), Python (Strong), Snowflake, AWS Lambda.  ·       We are looking for a strong Data Engineer cum Architect, to create new modern data ingestion pipelines using latest technologies like AWS Athena, Lambdas, Python, Spark. ·       You'll be working on data pipelines and tools to provide the underlying data ingestion framework.""  Technical Skills: ·       Frontrunner ·       Be inclined towards process automations & improvements, Identifying & automating repetitive things. ·       Must be able to handle Data engineering operations / enhancement project with a technical consultant bend. · SQL, Python, PySpark, S3, Lambda, EMR, Glue, Athena, EC2, IAM, Redshift, DMS, Airflow, Jenkins, Snowflake. · End-to-end data solutions (ingest, storage, integration, processing, access) on AWS. ·       Migrate data from traditional relational database systems to AWS relational databases such as Amazon RDS, Aurora, and Redshift. ·       12+ years IT experience. Background and experience in data engineering/analytics. ·       Should have very good hands-on experience in Cloud DB platforms (Snowflake is preferable), Building data pipelines & SQL, Python for Data Engineering. ·       Got experience to Perform, Support and Lead all aspects of Data Engineering strategy. ·       Excellent root cause analysis skills. ·       Ensure effective data pipeline engineering, deployment, ongoing operations, and continuous improvement. ·       Manage and perform data operations and data engineering requirements including automation and optimization. ·       Highly motivated, a self-starter, ability to work in a fast faced environment while managing competing priorities. ·       Creative problem solver and highly collaborative teammate who is comfortable working as a key contributor. ·       Certification in Data Engineering and/or Cloud Platforms are a plus. ·       Good written and verbal communication skills, and comfortable presenting findings to Sr. Management."
Data Manager/Engineer,The Garrett Group,"Bellevue, NE (Hybrid)",https://www.linkedin.com/jobs/view/3763983339/?eBP=JOB_SEARCH_ORGANIC&refId=A2pTL530PT8RrsyeqTc7vA%3D%3D&trackingId=5doLe%2BDMPDjgQCsx1qIK2w%3D%3D&trk=flagship3_search_srp_jobs,3763983339,"About the job
            
 
Are you ready to make a significant impact on critical decision-making processes, drive operational efficiency, and enhance performance in a dynamic environment? The Garrett Group (TGG) invites you to join our team as a Data Manager/Engineer, where you'll play a pivotal role in supporting USSTRATCOM's mission by applying advanced analytical techniques to address complex challenges.Required Experience Includes 5 Years Of Experience In Managing the effective development and use of data systems to include: Collection, organization, storage, protection, and analysis of data systems; Implementation of backup and restoral procedures, and working practices;Ensuring compliance with data architecture and data management standards and procedures defined in the Data Management Framework
Developing, constructing, testing, and maintaining architectures, such as databases and large-scale processing systems Deriving trends in data sets and developing algorithms to help make raw data more useful to the enterprise Experience with SQL database design and multiple programming language to prepare data for analytical or operational use Capability to communicate requirements and data trends across departments and the enterprise 
Additional Experience Desired Strong background in Information Technology and systems, encompassing aspects like networking and cloud storage Familiarity with Microsoft Access and Oracle databases Competence with Microsoft Excel Power BI Understanding of Data Ontology, particularly Basic Formal Ontology (BFO), are additional assets. Experience with Informatica or other data Extract, Transform, Load (ETL) tools and processes is also beneficial.
At The Garrett Group, we prioritize the well-being of our team members and offer a comprehensive benefits package, including healthcare coverage, financial security through employer-paid life insurance, educational support with tuition assistance, and financial planning opportunities through our 401K plan. We value your dedication and talent, and our benefits package reflects our commitment to your overall well-being and professional success.The Garrett Group is an Equal Opportunity/Affirmative Action Employer, and we encourage applications from all qualified candidates, regardless of various factors such as race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.Job Posted by ApplicantPro"
DataOS Data Engineer,HP,"Corvallis, OR (Hybrid)",https://www.linkedin.com/jobs/view/3755219684/?eBP=JOB_SEARCH_ORGANIC&refId=hsSkwim35BCTf7aaW4iQ9w%3D%3D&trackingId=n2Tg5P74j3jUcBlHBGMipQ%3D%3D&trk=flagship3_search_srp_jobs,3755219684,"About the job
            
 
DataOS Data Engineer collaborating on cutting edge advances in scalable solutions for data ingestion, manipulation and end to end integration of data building blocks to support business driven data products.Applies developed subject matter knowledge to solve common and complex business issues within established guidelines and recommends appropriate alternatives. Works on problems of diverse complexity and scope. May act as a team or project coach providing direction to team activities and facilitates information validation and team decision making process. Exercises judgment within generally defined policies and practices to identify and select a solution. Ability to handle most unique situations. May seek advice in order to support complex business issues.Responsibilities Designs and establishes secure and performant data architectures, enhancements, updates, and programming changes for portions and subsystems of data product pipelines, repositories or models for structured/unstructured data. Focusing on developing sharable libraries that reduce development time and maintenance,Analyzes design and determines coding, programming, and integration activities required based on general objectives and knowledge of overall architecture of product or solution.Writes and executes complete testing plans, protocols, and documentation for assigned portion of data system or component; identifies and debugs, and creates solutions for issues with code and integration into data system architecture.Leads a project team of other data engineers to develop reliable, cost effective and high-quality solutions for assigned data system, model, or component.Collaborates and communicates with project team regarding project progress and issue resolution. Supports co-development processes and tools to promote common approaches to solve complex problems.Represents the data engineering team for all phases of larger and more-complex development projects.Provides guidance and mentoring to less experienced staff members. Strong technical Leadership is required.
Knowledge & Skills Using data engineering tools, languages (Python is a must. Java Scala is a plus), frameworks to mine, cleanse and explore Large Data Sets.Fluent in SQL & Cloud based data systems. Relational data modeling. Fluent in complex, distributed and massively parallel cloud systems (AWS, GCP, AZURE).Strong analytical and problem-solving skills with ability to represent complex algorithms in software.Designing data systems/solutions to manage complex data that are highly scalable and performant.Ability to performance tune Spark code.Strong understanding of database technologies and management systems.Strong understanding of cloud-based systems/services. Differentiate benefits for big data Lake House vs. Warehouse.Experience with workflow orchestration tools (Airflow, Jenkins)Strong Notebooks environment experience (Jupyter, DataBricks)Experience collecting requirements from Partners and choosing the right technologies to meet end to end data flow requirements that are required. (Data size, delivery times: hourly, daily monthly or real-time approaches)Database architecture testing methodology, including execution of test plans, debugging, and testing scripts and tools.Excellent written and verbal communication skills; mastery in English and local language.Ability to effectively communicate product architectures, design proposals and negotiate options at management levels.
Scope & Impact Collaborates with peers, junior engineers, data scientists and project team.Typically interacts with high-level Individual Contributors, Managers and Program Teams.Leads a project requiring data engineering solutions development.
Education & Experience Bachelor's or Master's degree in Computer Science, Data Engineering, Information Systems, Engineering or equivalent.Typically 4-6 years’ experience.
About HPYou’re out to reimagine and reinvent what’s possible—in your career as well as the world around you.So are we. We love taking on tough challenges, disrupting the status quo, and creating what’s next. We’re in search of talented people who are inspired by big challenges, driven to learn and grow, and dedicated to making a meaningful difference.HP is a technology company that operates in more than 170 countries around the world united in creating technology that makes life better for everyone, everywhere.Our history: HP’s commitment to diversity, equity and inclusion – it's just who we are.From the boardroom to factory floor, we create a culture where everyone is respected and where people can be themselves, while being a part of something bigger than themselves. We celebrate the notion that you can belong at HP and bring your authentic self to work each and every day. When you do that, you’re more innovative and that helps grow our bottom line. Come to HP and thrive!"
Lead Azure Data Engineer with Strong T-SQL experience,"RIT Solutions, Inc.","Austin, TX (Hybrid)",https://www.linkedin.com/jobs/view/3768005784/?eBP=JOB_SEARCH_ORGANIC&refId=hsSkwim35BCTf7aaW4iQ9w%3D%3D&trackingId=MM4uGtRr6fLfFIprjOrnog%3D%3D&trk=flagship3_search_srp_jobs,3768005784,"About the job
            
 
Job Description Title: Lead Azure Data Engineer with Strong T-SQL experience (Someone who previously worked as a SQL Developer or SQL BI Developer)- High Priority Location: Dallas, TX, OR Portland, OR and Fremont, CA HYBRID (3 days in a week onsite) Required experience: 12+ years minimum. Job Description: Must Have: Candidate must be comfortable for T-SQL hands on coding.Enterprise Data modelling / DesignAzure SQL Data Warehouse (Synapse)T-SQL (Hands on experience, writing quires, building stored procs, performance optimization, etc.)ADFADLSShould be able to understand the technical specifications and able to work independently with minimal or no supervision. Must Have:8+ Years of MS SQL Server.3+ Years of Azure."
Senior Data Engineer (Contract),tlnt,"Dallas, TX (Hybrid)",https://www.linkedin.com/jobs/view/3756665310/?eBP=JOB_SEARCH_ORGANIC&refId=hsSkwim35BCTf7aaW4iQ9w%3D%3D&trackingId=soxIThcRMExddlrT3%2BR8Bw%3D%3D&trk=flagship3_search_srp_jobs,3756665310,"About the job
            
 
The RoleRequirementsWe are seeking a highly experienced Senior Data Engineer with 5+ years of expertise in Azure data engineering. This role is responsible for the development and management of data pipelines, data modeling, and supporting organizational data analytics and insights initiatives. The ideal candidate will be proficient in tools and technologies like Pyspark, Azure Data Factory (ADF), Databricks, big data technologies, SQL, and Python. Additionally, the Senior Data Engineer will play a critical role in stakeholder management, leading development teams or project teams, guiding design discussions, and taking ownership of deliverables for larger projects. Effective communication skills are essential for understanding and translating functional and technical requirements into practical solutions. The ideal person will: Design, build, and maintain data pipelines in the Azure environment, ensuring smooth operation in both batch and real-time processing. Optimize Azure data infrastructure to facilitate accurate data extraction, transformation, and loading from a variety of data sources. Develop and automate ETL processes using tools like Pyspark, Azure Data Factory, and Databricks to facilitate data extraction and manipulation from diverse sources. Transform raw data stored in Azure Data Warehouses into accessible datasets suitable for both technical and non-technical stakeholders. Demonstrate expertise in Azure services and big data technologies, including but not limited to Azure Data Factory, Databricks, and other relevant tools. Proficiency in SQL and Python is also crucial. Engage in business discussions, collaborate with stakeholders, and take ownership of deliverables, ensuring that the data engineering solutions align with business objectives. Lead development teams or project teams, providing guidance and expertise to ensure the successful execution of data engineering projects. Function as an independent developer who can guide design discussions and make informed decisions to drive project success. Leverage extensive experience to handle larger projects and manage multiple teams effectively. Exhibit excellent communication skills, translating functional and technical requirements into actionable solutions that meet business needs. 
The Person Minimum of 4 years of hands-on experience in Azure data engineering. Proficiency in a range of key technologies is essential, including Pyspark, Azure Data Factory (ADF), Databricks, big data tools, SQL, and Python. Should be adept at handling business discussions and be capable of taking ownership of deliverables. Effective stakeholder management is crucial, including leading development teams or project teams to ensure successful project outcomes. Self-sufficient developer who can provide guidance in design discussions and make informed decisions. Demonstrated experience in successfully managing larger projects and multiple teams is a key requirement. Excellent communication skills are necessary to understand and bridge the gap between functional and technical requirements, providing practical and effective solutions. 
Note: Preferred candidates will be in the Dallas or willing to relocate to Dallas, Texas Our client is an equal opportunity employer, and we celebrate diversity and are committed to creating an inclusive environment for all.Note: This job description is intended to convey information essential to understanding the scope of the position and is not an exhaustive list of skills, efforts, duties, responsibilities, or working conditions associated with it.Skills: azure data factory (adf),azure data engineering,python,big data technologies,databricks,azure,pyspark,sql"
Data Engineer Level III (IT10000548),"Synectics for Management Decisions, Inc","Virginia, United States (Hybrid)",https://www.linkedin.com/jobs/view/3723931237/?eBP=JOB_SEARCH_ORGANIC&refId=hsSkwim35BCTf7aaW4iQ9w%3D%3D&trackingId=oq181tx1ChnI%2BqP9PFoeIg%3D%3D&trk=flagship3_search_srp_jobs,3723931237,"About the job
            
 
We are seeking a Data Engineers to join our growing team. The qualified applicant will play a key role in Identifying trends, patterns, and anomalies found in big data sets by performing extensive data analysis to develop insights. Hybrid work options are available.A Data Engineer is responsible for performig data mining, cleaning, and aggregation processes to prepare data, implement data models, conduct analysis, and develop databases. Interpreting results from multiple structured and unstructured data sources using programming, statistical, and analytical techniques and tools. The person in this role will collaborate and work as a team to ensure the success of the team. If cutting-edge data projects resonate with you and you care deeply about joining a mission-driven agency, we'd love to learn more about you.Responsibilities Leading design and development of data collection tools.Advising on and directly configuring data extraction, loading, and transformation pipelines.Advising on and building data models that fit programmatic and operational needs, combining various datasets, enhancing transactional data as relevant.Defining and running sustainable data cleaning and quality testing pipelines.Designing, developing, and implementing the most valuable data-driven solutions for the organization.
Experience And Qualifications Min. 7 years of experience.
Education Requirements Bachelor's degree in computer science, mathematics, engineering or related field of study. 
Clearance Requirements Ability to attain and retain a Public Trust Clearance.
Job Type Full TimeHybrid work schedule Monday-Friday9p-5p
Our hiring process is 100% online.About SynecticsSynectics is a solutions specialist company focused on federal data - gathering, storing, cleaning, analyzing, visualizing, reporting on, and ultimately making it meaningful for stakeholders and their constituents. We call ourselves ""Synecticians"" because we fuse our ideas and disciplines in every deliverable. We assemble experienced SMEs and solution architects - from different walks of life -to deliver excellence in information technology and data management. It is a privilege to play a behind-the-scenes role in applications that improve the quality of life in America and promote the health and welfare of our citizens.Our motto? Making Data Meaningful!Employee BenefitsSynectics is an Equal Opportunity Employer. We offer a competitive salary and an impressive full benefits package that includes:Professional DevelopmentHealth and WellbeingFinancial WellnessTuition & Training AssistanceProfessional Membership SubscriptionTechnical e-Learning TrainingsComputer Purchase ProgramMedical, Dental, and VisionPaid Time Off (PTO)11 Paid HolidaysParental LeaveLife InsuranceEmployee Assistance Program (EAP)401 (k) Retirement Plan w/Company MatchDiscretionary Profit SharingPre-tax Transportation BenefitsHealth Savings & Flexible Spending OfferingsFinancial CounselingWe pride ourselves on fostering an environment that supports professional development, growth, and diversity. Learn more about our benefits and perks at smdi.com/careers. Be sure to follow our social pages to be the first to hear about new positions at Synectics!"
Data QA Engineer,"Tech Talent Link, Inc","Vancouver, WA (Hybrid)",https://www.linkedin.com/jobs/view/3756663480/?eBP=JOB_SEARCH_ORGANIC&refId=hsSkwim35BCTf7aaW4iQ9w%3D%3D&trackingId=UAh9v1CvWuYAZGG8mGDCfw%3D%3D&trk=flagship3_search_srp_jobs,3756663480,"About the job
            
 
***We are unable to work with 3rd-party or corp-to-corp candidates for this position***   Overview  :  Our client will be hiring a Data QA Engineer to support two internal teams. The DNA of the employees who work out well includes folks who take ownership over their role, value process-driven work and have a solution-focused mentality. This position is the sole QA Engineer on the team.   This is a Hybrid position, 3 days in the Vancouver office and 2 days remote.   This position reports to the Manager of Insurance and Accounting and indirectly to the Senior IT/Data Manager.    Responsibilities:   Write complex SQL queries and test Power BI reports. Focus on manual testing and extracting, transferring, loading and validating data.  The team is currently connecting the data warehouse to D365.  Design test plans, scenarios, scripts, and procedures to validate complete functionality of software Detect any bugs or defects throughout the development process, recording what went wrong and how it was circumvented. Document the testing process to ensure the program runs effectively and that any results can be repeatedly replicated.  Work as part of a team to develop new testing tools/procedures to make the troubleshooting process more efficient.  Maintain catalog of tests and testing procedures including smoke, functional, nonfunctional, integration, end-to-end, performance, regression, UAT and post deploy tests. Leverages best practices, innovation, research in emerging solutions and business process improvement tools to develop long term solutions.  Collaborate with Internal Audit and Information Security Teams to ensure adherence to security and compliance requirements.  Create documentation and submit Change Requests for weekly Change Advisory Board approval.  Run SQL queries to generate reports for Insurance and Accounting.  Familiarity with the software development lifecycle (SLDC).  Solid hands-on experience querying data from SQL Server Databases or similar.  Experienced in Agile Scrum, waterfall and/or hybrid project delivery methodologies.  Proven experience developing test plans, scripts and other related documentation.    Screening Questions  : Tell me about your experience taking ownership over your role and being process-driven? Have you ever let a bug go into production? What is your knowledge of change mgmt./release mgmt?    Qualifications  : 5+ years of experience as a QA Engineer. Experience testing Power BI reports/dashboards (or other BI solutions). Strong ETL and data validation experience. Experience testing financial, accounting, or insurance data.   Preferred:  Experience with Microsoft Dynamics 365 related testing such as patches, upgrades, customizations, and integrations"
Senior Data Engineer - Vice President,Deutsche Bank,"Cary, NC (Hybrid)",https://www.linkedin.com/jobs/view/3739287072/?eBP=JOB_SEARCH_ORGANIC&refId=hsSkwim35BCTf7aaW4iQ9w%3D%3D&trackingId=GeEnDCPBWr2yy7BfOKL81w%3D%3D&trk=flagship3_search_srp_jobs,3739287072,"About the job
            
 
Position OverviewJob Title Senior Data EngineerCorporate Title Vice PresidentLocation Cary, NCWho We AreIn short – an essential part of Deutsche Bank’s technology solution, developing applications for key business areas.Our Technologists drive Cloud, Cyber and business technology strategy while transforming it within a robust, hands-on engineering culture. Learning is a key element of our people strategy, and we have a variety of options for you to develop professionally. Our approach to the future of work champions flexibility and is rooted in the understanding that there have been dramatic shifts in the ways we work.Having first established a presence in the Americas in the 19th century, Deutsche Bank opened its US technology center in Cary, North Carolina in 2009. Learn more about us here .OverviewOur Technology, Data, and Innovation (TDI) strategy is focused on strengthening engineering expertise, introducing an agile delivery model, as well as modernizing the Bank's Information Technology (IT) infrastructure with long-term investments and taking advantage of cloud computing. We continue to invest and build a team of visionary tech talent who will ensure we thrive in this period of unprecedented change for the industry, so we are seeking a Lead Engineer to work in the Transaction Monitoring and Data Controls team. You will be hands on technical engineer within our delivery pods and deliver software solutions. As lead engineer lead you will design software architecture and implement complex solutions, driving re-use and best practices. You will contribute to strategic design decisions and define engineering approaches that can be disruptive, with the goals of simplifying architectures, reducing technical debt.What We Offer You A diverse and inclusive environment that embraces change, innovation, and collaborationA hybrid working model with up to 60% work from home, allowing for in-office / work from home flexibility, generous vacation, personal and volunteer days, a commitment to Corporate Social Responsibility Employee Resource Groups support an inclusive workplace for everyone and promote community engagementAccess to a strong network of Communities of Practice connecting you to colleagues with shared interests and valuesCompetitive compensation packages including health and wellbeing benefits, retirement savings plans, parental leave, and family building benefits, educational resources, matching gift and volunteer programs
What You’ll Do Build the Infrastructure required for optimal ETL and ELT tools from a variety of data sources using batch processingConsult on improving data reliability, efficiency, and qualityEnsure architecture supports business requirementsCollaborate with stakeholders to understand requirements, evaluate, and refine storiesDesign, implement, and test solutions, providing support through the production processBuild reliable pipe-lines that is easy to support in production and design and develop high-quality and easily maintainable code
How You’ll Lead Act as a leader and contact person for all Data-related topics and questions - both from the Data Engineers (by actively coaching them) and from the other team membersProvide capable end to end solutions by taking complete ownershipShould provide active support to rest of the team when needed
Skills You’ll Need Degree in Computer Sciences, Math or engineeringExperience building enterprise applications using Python and/or Scala and operating systems such as UnixProficiency in Structure Query Language (SQL)Experience with Apache Ecosystem, including Spark, Hive, and HadoopKnowledge of Continuous Integration (CI) and Continuous Deployment (CD) tools, as well as Kubernetes, Apache Airflow and monitoring tools
Skills That Will Help You Excel Excellent communication skills, both written and spokenStrong problem solving and analytical skillsYou enjoy supporting our community of engineers and creates opportunities for progression, promoting continuous learning and skills development
ExpectationsIt is the Bank’s expectation that employees hired into this role will work in the Cary office in accordance with the Bank’s hybrid working model.Deutsche Bank provides reasonable accommodations to candidates and employees with a substantiated need based on disability and/or religion.The salary range for this position in Cary is $125k to $203k. Actual salaries may be based on a number of factors including, but not limited to, a candidate’s skill set, experience, education, work location and other qualifications. Posted salary ranges do not include incentive compensation or any other type of renumeration.Deutsche Bank Values & DiversityWe believe talent is found in all cultures, countries, races, ethnicities, genders, sexual orientations, disabilities, beliefs, generations, backgrounds, and experiences. We pursue a working environment where everyone can be authentic and feel a sense of belonging. Click here to find out more about our diversity and inclusion efforts.We are an Equal Opportunity Employer - Veterans/Disabled and other protected categories.Click these links to view the following notices: EEO is the Law poster and supplement ; Employee Rights and Responsibilities under the Family and Medical Leave Act ; Employee Polygraph Protection Act and Pay Transparency Nondiscrimination ProvisionLearn more about your life at Deutsche Bank through the eyes of our current employees: https://careers.db.com/lifeThe California Consumer Privacy Act outlines how companies can use personal information. If you are interested in receiving a copy of Deutsche Bank’s California Privacy Notice, please email HR.Direct@DB.com .Deutsche Bank BenefitsAt Deutsche Bank, we recognize that our benefit programs have a profound impact on our colleagues. That’s why we are focused on providing benefits and perks that enable our colleagues to live authenti­cally and be their whole selves, at every stage of life. We provide access to physical, emotional, and financial wellness benefits that allow our colleagues to stay financially secure and strike balance between work and home. Click here to learn more!Our values define the working environment we strive to create – diverse, supportive and welcoming of different views. We embrace a culture reflecting a variety of perspectives, insights and backgrounds to drive innovation. We build talented and diverse teams to drive business results and encourage our people to develop to their full potential. Talk to us about flexible work arrangements and other initiatives we offer.We promote good working relationships and encourage high standards of conduct and work performance. We welcome applications from talented people from all cultures, countries, races, genders, sexual orientations, disabilities, beliefs and generations and are committed to providing a working environment free from harassment, discrimination and retaliation.Visit Inside Deutsche Bank to discover more about the culture of Deutsche Bank including Diversity, Equity & Inclusion, Leadership, Learning, Future of Work and more besides.We are an Equal Opportunity Employer - Veterans/Disabled and other protected categories. Click these links to view the following notices: ""EEO is the Law poster"" and supplement ; Employee Rights and Responsibilities under the Family and Medical Leave Act ; Employee Polygraph Protection Act and Pay Transparency Nondiscrimination Provision ."
Data Science Engineer (Senior),"Integral Federal, Inc.","Annapolis Junction, MD (Hybrid)",https://www.linkedin.com/jobs/view/3580061411/?eBP=JOB_SEARCH_ORGANIC&refId=hsSkwim35BCTf7aaW4iQ9w%3D%3D&trackingId=jyHTM8gPSl0SP3nfdXx4kw%3D%3D&trk=flagship3_search_srp_jobs,3580061411,"About the job
            
 
Integral Federal is looking to hire a Data Science Engineer to support the Transportation Security Administration (TSA) Performance Engineering Analytics (PEA) program. The PEA program will support TSA as they continue to look for ways to enhance its layered approach to security through new state-of-the-art technologies, expanded use of existing and proven technologies, improved passenger identification techniques, and other developments that will continue to strengthen anti-terrorism capabilities. Analyze and organize raw data, build data systems and pipelines, and develop code/toolsto Extract, Transform, and Load (ETL) data into the appropriate structures for storageCombine raw information from different sources and explore ways to enhance data quality, performance, and reliabilityCreate performance metrics for data ingest, storage, and quality to track performance indicators for communication to relevant stakeholders.Apply continuous improvement methodologies, such as ITIL (Information Technology Infrastructure Library) and/or Six Sigma, towards analyzing, measuring, and documenting system improvements, and assessing data quality.Create and present consumable summaries of findings through effective communication and visualizations.Perform data processing, cleansing, mining and analysis utilizing data sources having relevant context that are internal and external to the business’ domain data repositories/data warehouses.Interpret, understand, and assess customer requirements to provide custom tool development, query optimization, aggregation, categorization, and interpretation of data.Monitors for data quality events such as missing data, non-conformant data, and unexpected values and appropriately responds and resolves data quality issuesFollows appropriate protocols and procedures to handle purging and masking information deemed to be sensitive in nature by privacy stakeholdersCapture data lineage and produce lineage reports to inform end users and customers how the data was sourced, transformed, and where it resides Extending data with third party sources of information when neededImplement data governance practices and procedures that serve as the guides for data science projects
Required 10+ years experience and a Bachelor's DegreeExpertise developing interrogating databases with Structure Query Language (SQL) to develop complex queries involving joins, subqueries, aggregation, indices, common table expressions, and query optimization/plans. The ability to code and debug stored procedures is desirable.Skills in applying pattern recognition techniques to large structured, semi-structured, and unstructured data sets to automate and optimize data wrangling (extract, transform, clean, evaluate, normalize/standardize, and organize) from disparate sources.Experience with developing ETL capabilities using tools such as IBM Infosphere/Datastage, Informatica, or similar toolsPrevious experience as a data engineer or in a similar roleTechnical expertise with data models, data mining, data cleansing, and segmentation techniquesKnowledge of programming languages (e.g. Java, Python, SQL)Hands-on experience with SQL database designGreat numerical and analytical skills
Preferred Experience with TSA systems and environment
Integral Federal is united by a shared passion of excellence in service. We are an Equal Opportunity Employer that cultivates a culture of diversity, equity, and inclusion, and are headquartered in Rockville, MD with offices in Charlottesville, Fredericksburg, DC and Aberdeen.We offer a comprehensive total rewards package including paid parental leave and immediate vesting in our 401K. That means you don’t have to wait an entire year to earn matching funds for your retirement contributions! Give us a try and become part of a curated group of intelligence professionals at Integral Federal Inc.Our package also includes Medical, Dental & Vision InsuranceFlexible Spending AccountsShort-Term and Long-Term Disability InsuranceLife InsurancePaid Time Off – Holidays, Vacation & Sick DaysEarned bonuses and awardsProfessional Training ReimbursementPaid ParkingEmployee Assistance Program"
Senior Data Engineer,University of Maryland,"College Park, MD (Hybrid)",https://www.linkedin.com/jobs/view/3776949011/?eBP=JOB_SEARCH_ORGANIC&refId=hsSkwim35BCTf7aaW4iQ9w%3D%3D&trackingId=jZUMc4Nd16SPRiPLEo5RJw%3D%3D&trk=flagship3_search_srp_jobs,3776949011,"About the job
            
 



      This job is sourced from a job board.
      Learn More



The Business Intelligence unit within University Relations is on a mission to modernize its data warehousing infrastructure. We are actively seeking a Senior Data Engineer with expertise in Apache Airflow to support this transformation. The Data Engineer will play a pivotal role in enhancing our ELT/ETL processes, ensuring seamless data flow from various in-premise and cloud sources into our MS Warehouse. This role is crucial in supporting the University of Maryland’s advancement operations, which encompasses development, alumni and donor relations, gift processing, and more. The Data Engineer will collaborate closely with business stakeholders, understanding their data requirements, and implementing scalable, Airflow-based solutions. They will also be responsible for supporting the Python data analysis stack and implementing best practices in data governance.Responsibilities Design, build, and maintain scalable cloud-based data pipelines using Python, Apache Airflow, and PySpark.Implement real-time data streaming solutions and batch processing using modern technologies.Work closely with data scientists and analysts to turn data into critical information and knowledge that can be used to make sound business decisions.Implement containerization solutions using Docker to ensure a consistent and scalable environment.Ability to learn and leverage current data visualization tools to create dynamic data applications and dashboards for various business units.Develop complex SQL queries for data analysis and reporting.Ensure data quality and integrity through robust testing frameworks.Collaborate with a cross-functional team on ms in an agile environment.Have working knowledge of ML concepts for data modeling"
Data Automation Engineer,NR Consulting,"Orlando, FL (Hybrid)",https://www.linkedin.com/jobs/view/3768020194/?eBP=JOB_SEARCH_ORGANIC&refId=hsSkwim35BCTf7aaW4iQ9w%3D%3D&trackingId=Uam1mATQPRtAJG1cOPY1uQ%3D%3D&trk=flagship3_search_srp_jobs,3768020194,"About the job
            
 
Job DescriptionProficiency in UI automation ( Selenium, Robot, Watir)Experience in Gherkin ( BDD /TDD )Proficiency with Python or other OO languageAgile testingExperience building or improving test automation frameworks.Proficiency CICD integration and pipeline development in Jenkins, Spinnaker or other similar tools"
Hybrid Work - Need Senior Data Platform Engineer-Azure in Des Moines IA,Steneral Consulting,"Des Moines, IA (Hybrid)",https://www.linkedin.com/jobs/view/3751750122/?eBP=JOB_SEARCH_ORGANIC&refId=hsSkwim35BCTf7aaW4iQ9w%3D%3D&trackingId=Wd9hX%2FEVXxdcc9KEc7nO4Q%3D%3D&trk=flagship3_search_srp_jobs,3751750122,"About the job
            
 
Senior Data Platform Engineer-AzureDes Moines, IA - Hybrid 3 days a week - locals are highly preferred but will consider someone from Midwest who will relocate from day one to work onsite.Communication and collaboration are key. They must be easily understood and able to speak well to their projects and technical experience. Must have valid LinkedIn and Photo ID required with submissionMust Have’s: Must have everything or please do not send them to me.   Azure Microsoft Fabric -- end to end lifecycle.Azure Azure SQL Database: Provisioning, performance tuning, scaling, and security.Azure Cosmos DB: Understanding of NoSQL databases, partitioning, consistency models.Azure Data Factory: Data integration and ETL processes.Azure Blob Storage and Data Lake Storage: Management, performance, security, and data lifecycle.Azure Stream Analytics: Real-time data streaming and analytics.Azure Databricks & HDInsight: Big data analytics solutions. (lower priority)Azure Synapse Analytics: Knowledge of data warehousing, data integration, and analytics.

   SQL: Writing, optimizing, and debugging SQL queries.Data modeling: Normalization, star schema, snowflake schemaFamiliarity with SDKs and APIs associated with Azure data services.Integration with other Azure services or third-party applications.Experience in one or more programming languages like C#, Python, or Java can be beneficial.Azure Monitor, Azure Log Analytics, and Application Insights.DP-203 certification Financial/Investment industry experience.

Job DescriptionHere are the skills sets for building out the Microsoft Azure Data Platform.  Azure Fundamentals: Understanding of Azure subscriptions, resources, and resource groups.Familiarity with Azure regions, availability zones, and the Azure portal.

  Azure Data Services Knowledge of tool set: Azure Microsoft Fabric -- end to end lifecycle.Azure Azure SQL Database: Provisioning, performance tuning, scaling, and security.Azure Cosmos DB: Understanding of NoSQL databases, partitioning, consistency models.Azure Data Factory: Data integration and ETL processes.Azure Blob Storage and Data Lake Storage: Management, performance, security, and data lifecycle.Azure Stream Analytics: Real-time data streaming and analytics.Azure Databricks & HDInsight: Big data analytics solutions. (lower priority)Azure Synapse Analytics: Knowledge of data warehousing, data integration, and analytics.

  Skills: SQL: Writing, optimizing, and debugging SQL queries.Data modeling: Normalization, star schema, snowflake schemaFamiliarity with SDKs and APIs associated with Azure data services.Integration with other Azure services or third-party applications.Experience in one or more programming languages like C#, Python, or Java can be beneficial.Azure Monitor, Azure Log Analytics, and Application Insights.DP-203 certification 

  Optional but helpful: Azure Active Directory and role-based access control (RBAC)Tools like Azure Data Migration Service, SSIS (SQL Server Integration Services).Strategies for migrating data from on-premises or other clouds to Azure."
Senior Data Engineer (Senior Data Modeler),CVS Health,"Irving, TX (Hybrid)",https://www.linkedin.com/jobs/view/3780173323/?eBP=JOB_SEARCH_ORGANIC&refId=hsSkwim35BCTf7aaW4iQ9w%3D%3D&trackingId=4KFg84u2RSZQ7SpYzMGHtg%3D%3D&trk=flagship3_search_srp_jobs,3780173323,"About the job
            
 
Bring your heart to CVS Health. Every one of us at CVS Health shares a single, clear purpose: Bringing our heart to every moment of your health. This purpose guides our commitment to deliver enhanced human-centric health care for a rapidly changing world. Anchored in our brand — with heart at its center — our purpose sends a personal message that how we deliver our services is just as important as what we deliver.Our Heart At Work Behaviors™ support this purpose. We want everyone who works at CVS Health to feel empowered by the role they play in transforming our culture and accelerating our ability to innovate and deliver solutions to make health care more personal, convenient and affordable.Position Summary Creates physical data models in Erwin and implements DDL in google cloud platform , performs data profiling on large data sets, works on metadata management in Alation.Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needsWrites ETL (Extract / Transform / Load) processes, designs database systems and develops tools for real-time and offline analytic processingCollaborates with data science team to transform data and integrate algorithms and models into automated processesUses knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries to build data pipelinesUses strong programming skills in Python, Java or any of the major languages to build robust data pipelines and dynamic systemsBuilds data marts and data models to support Data Science and other internal customers. Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standardsAnalyzes current information technology environments to identify and assess critical capabilities and recommend solutionsExperiments with available tools and advice on new tools in order to determine optimal solution given the requirements dictated by the model/use case
Required Qualifications 5+ years of data modeling experience, including experience in 3NF modeling and Dimensional modeling5+ years of ETL background in Cloud Development3+ years of experience in Data Governance, Metadata Management, SQLKnowledge in Java, Python, Hive, Cassandra, Pig, MySQL or NoSQL or similarKnowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment
Preferred Qualifications Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sourcesAbility to understand complex systems and solve challenging analytical problemsExperience with bash shell scripts, UNIX utilities & UNIX CommandsStrong knowledge of large-scale search applications and building high volume data pipelines
Education Bachelor’s Degree in Computer Science, Engineering, Statistics, Physics, Math, or related field or Advanced Degree
Pay RangeThe typical pay range for this role is:$90,000.00 – $180,000.00This pay range represents the base hourly rate or base annual full-time salary for all positions in the job grade within which this position falls. The actual base salary offer will depend on a variety of factors including experience, education, geography and other relevant factors. This position is eligible for a CVS Health bonus, commission or short-term incentive program in addition to the base pay range listed above.In addition to your compensation, enjoy the rewards of an organization that puts our heart into caring for our colleagues and our communities. The Company offers a full range of medical, dental, and vision benefits. Eligible employees may enroll in the Company’s 401(k) retirement savings plan, and an Employee Stock Purchase Plan is also available for eligible employees. The Company provides a fully-paid term life insurance plan to eligible employees, and short-term and long term disability benefits. CVS Health also offers numerous well-being programs, education assistance, free development courses, a CVS store discount, and discount programs with participating partners. As for time off, Company employees enjoy Paid Time Off (“PTO”) or vacation pay, as well as paid holidays throughout the calendar year. Number of paid holidays, sick time and other time off are provided consistent with relevant state law and Company policies.For more detailed information on available benefits, please visit jobs.CVSHealth.com/benefitsCVS Health requires certain colleagues to be fully vaccinated against COVID-19 (including any booster shots if required), where allowable under the law, unless they are approved for a reasonable accommodation based on disability, medical condition, religious belief, or other legally recognized reasons that prevents them from being vaccinated.You are required to have received at least one COVID-19 shot prior to your first day of employment and to provide proof of your vaccination status or apply for a reasonable accommodation within the first 10 days of your employment. Please note that in some states and roles, you may be required to provide proof of full vaccination or an approved reasonable accommodation before you can begin to actively work.CVS Health is committed to recruiting, hiring, developing, advancing, and retaining individuals with disabilities. As such, we strive to provide equal access to the benefits and privileges of employment, including the provision of a reasonable accommodation to perform essential job functions. CVS Health can provide a request for a reasonable accommodation, including a qualified interpreter, written information in other formats, translation or other services through ColleagueRelations@CVSHealth.com If you have a speech or hearing disability, please call 7-1-1 to utilize Telecommunications Relay Services (TRS). We will make every effort to respond to your request within 48 business hours and do everything we can to work towards a solution."
Integrated Supply Chain Data Engineer,Rockwell Automation,"Mayfield Heights, OH (Hybrid)",https://www.linkedin.com/jobs/view/3777402810/?eBP=JOB_SEARCH_ORGANIC&refId=hsSkwim35BCTf7aaW4iQ9w%3D%3D&trackingId=Y5IlwVa%2Fvs6Xi4sKe9CNdg%3D%3D&trk=flagship3_search_srp_jobs,3777402810,"About the job
            
 
Rockwell Automation is a global technology leader focused on helping the world’s manufacturers be more productive, sustainable, and agile. With more than 28,000 employees who make the world better every day, we know we have something special. Behind our customers - amazing companies that help feed the world, provide life-saving medicine on a global scale, and focus on clean water and green mobility - our people are energized problem solvers that take pride in how the work we do changes the world for the better.We welcome all makers, forward thinkers, and problem solvers who are looking for a place to do their best work. And if that’s you we would love to have you join us!Job Description Objective of position The Supply Chain Information Designer role is a highly collaborative role working with the Global Supply Chain (GSC) content delivery teams and the DA&I organization to establish reliable data foundation for the organization. The information designer must understand how the business applications support the processes, how the information transforms as it goes through the business process, and the information architecture required to support the business analytics for the supply chain functional domains. This position is expected to provide thought leadership in information design and the DA&I technology, along with educating members of the organization on how to best use the DA&I environment. Data Perspective.  Balanced perspective. Understands the intersection of Data, Process, and Function. Information Architecture.  Architecture modeling along with the ability to translate the model into the physical data design. Data Technology  – Interest and aptitude for working with a range of data technologies such as SQL, PSQL, Hadoop, along with ERP environments. Problem Solving.  Able to work on a large problem and not be derailed by the complexity and ambiguity. Thought Leadership.  Provide thought leadership for information design and architecture to meet current needs and anticipate future requirements. Design and Deployment  – Detail data definition to drive the realization of the data infrastructure supporting data consumers. Data Quality and Operation Stability  – Ensuring confidence in the data integrity and the reliability of the data environmentMajor Responsibilities The translation of business data into data structures that will support data presentation.Demonstrated ability to take current system deployments and translate the existing data structures into a future state DA&I data design.Facilitate discussions with cross functional groups.Decompose a process, identify the supporting data, and define supporting data structures.Support Data Foundation technology life cycle planning from selection, release management, version management, to replacementTechnical curiosity to sort through the range of data technologies available to arrive at a solution and drive the deployment of the right technical fit.Management of enterprise information model across business areas to enable the delivery of an e2e data model.Thought leadership in the data design agility required to support future use cases and future points of data consumption.Definition of data layer required to provide the consistent delivery of information to the content layers and realize value. Eco system model that includes source and target system identification along with data augmentation requirements.Definition of the capability and technology stack that supports the delivery of data to the data consumption points. Data security policy adherence in information foundation designData reliability program - proactive data validation of data sources to ensure the confidence of analytic content.
Basic Qualifications Bachelor's degree in IT, Business, Supply Chain or Industrial Engineering preferred; other Bachelors level degrees will be considered.Legal authorization to work in country is required. We will not offer sponsorship for this position now or in the future for this job opening
Preferred Qualifications SAP Ecosystem Data Structure and Navigation knowledgeSQL AdvancedAlternative programming language basics (Python, Scala, R)Power BI Skills and technical capability knowledgeSAP Ecosystem programming knowledgeDomain experience supporting two of the following functional areasPurchasing / SourcingMaterials (supply /demand planning / inventory management)Logistics (transportation / warehousing)Global Trade Management (import/export, classification)Care for Customer (Order Management / Invoicing)
We are an Equal Opportunity Employer including disability and veterans.If you are an individual with a disability and you need assistance or a reasonable accommodation during the application process, please contact our services team at +1 (844) 404-7247."
Data Engineer,BlueWater Federal Solutions,"Fayetteville, North Carolina Metropolitan Area (Hybrid)",https://www.linkedin.com/jobs/view/3759798899/?eBP=JOB_SEARCH_ORGANIC&refId=hsSkwim35BCTf7aaW4iQ9w%3D%3D&trackingId=LcvBpX5NdMc2ppI6ktP7SA%3D%3D&trk=flagship3_search_srp_jobs,3759798899,"About the job
            
 
BlueWater Federal is looking for a Data Engineer to support the United States Army Special Operations Command’s (USASOC). Their core responsibility is to provide trained and ready forces that are prepared to meet Geographic Combatant Command (GCC) and Theater Special Operations Command (TSOC) requirements around the world.US Citizenship is required,This project is rooted in agile best practices, executing the tenants of DevSecOps and implement continuous process improvement in process and tools/technology to facilitate USASOC’s transition to a data-centric organization, support ongoing AI/ML, Software, and application development across the enterprise, and enable modernization efforts that support the ARSOF mission. The project actively supports the Artificial Intelligence Division (AI DIV) with a focus on project Zeus and application development.Responsibilities· Adapt data to specific data science requirements and make raw data usable on the backend.· Assemble large, complex sets of data that meet non-functional and functional business requirements.· Work with stakeholders to assist them with data-related technical issues.Qualifications· Bachelor’s degree is required, MS Degree is preferred· 3+ years of data engineering experience· Must have an active Secret clearance. · Strong DB background. Proficiency in data modeling, SQL, and ETL processes.· Strong scripting skills and is familiar with data warehousing solutions (e.g., Amazon Redshift, Google BigQuery, Azure).· Experience with big data and open source technologies (e.g., Hadoop, Spark) is a plus.· SSCP or CCISO or CISM or CISSP or GSLC is desired.At BlueWater Federal Solutions, a Tetra Tech company, health and safety play a vital role in our success. BlueWater’s employees work together to comply with all applicable health & safety practices and protocols, including health orders and regulations related to COVID-19 that are mandated by local, state and federal authorities.BlueWater Federal Solutions is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, or protected veteran status and will not be discriminated against on the basis of disability."
Sr Data Engineer - (Small Molecules/Protein),"Career Developers, Inc.","New York, NY (Hybrid)",https://www.linkedin.com/jobs/view/3773578945/?eBP=JOB_SEARCH_ORGANIC&refId=hsSkwim35BCTf7aaW4iQ9w%3D%3D&trackingId=Q2e2yA0kJh9HDNyA1enr3w%3D%3D&trk=flagship3_search_srp_jobs,3773578945,"About the job
            
 
Refer a friend: Referral fee programCareer Developers Inc, a well-established staffing agency/consulting firm, celebrating 29 years in operation. Headquartered in West Palm Beach, FL, we offer comprehensive staffing services nationwide. With a portfolio of carefully chosen clients to represent, we ensure a productive partnership that exceeds most others. For our candidates, our commitment and goal lie in efficiently managing your expectations through business intelligence, spending time for interview preparations, providing open communication, and delivering exceptional feedback throughout the process. We look forward to helping advance your career!Sr.Data Engineer (Must have a strong background in Molecular Dynamics/Small Molecules)Location: Manhattan (on-site)Salary: 300-550K + BonusRelocation is availableSponsorship is available.Ideal candidate will have extensive experience with large-scale data management, strong Python programming skills, experience with Client systems, frameworks, and data lifecycles; and a background in data structures and algorithms. Relevant areas of expertise include engineering of large-scale chemical databases, experience with architecting infrastructure for the handling of life science data, fluency with UNIX/Linux command-line tools like awk and sed, and familiarity writing scientific software, but specific knowledge of any of these areas is less critical than intellectual curiosity, versatility, and a track record of achievement.Relevant experience:  Demonstrated experience with large-scale data management in the life sciences (ideally dealing with chemistry databases), data structures and algorithmsExperience architecting infrastructure for the handling of life science dataWriting scientific softwareIndustry experience required (above three years minimum)Python programming skills a must; C++ is a nice to have
Responsibilities: creating systems and infrastructure for modeling, curating, and indexing the petabytes of data generated by our special-purpose supercomputers and our large Linux HPC and GPU clusters; to implementing pipelines for processing computational and experimental data sets; and to developing and maintaining data management policies and toolkits for drug discovery data processing

Desired Skills and Experience
                DATA%20ENGINEER"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Hilliard, OH (Hybrid)",https://www.linkedin.com/jobs/view/3773092090/?eBP=JOB_SEARCH_ORGANIC&refId=hsSkwim35BCTf7aaW4iQ9w%3D%3D&trackingId=oxpCFMFcfy4Kr3sigH5YuA%3D%3D&trk=flagship3_search_srp_jobs,3773092090,"About the job
            
 
Who is Recruiting from Scratch : Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.https://www.recruitingfromscratch.com/This is a hybrid role based in our Palo Alto or San Francisco offices and will require you to be in office Tuesdays and Thursdays.What’s so interesting about this role?We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.What’s the job?We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.Responsibilities:  Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models  Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling  Be self-motivated in seeking solutions when the correct path isn’t always known  Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders  Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams  Build data processing streams for cleaning and modeling text data for LLMs  Research and evaluate new technologies in the big data space to guide our continuous improvement  Collaborate with multi-functional teams to help tune the performance of large data applications  Work with Privacy and Security team on data governance, risk and compliance initiatives  Work on initiatives to ensure stability, performance and reliability of our data infrastructure 
What We’ll Love About You  Bachelors in Computer Science, Mathematics, Physics, or a related fields  5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience  Experience in statistical analysis & visualization on datasets using Pandas or R  Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools  Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models  Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy  Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control  Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)  Experience with any public cloud environment - AWS, GCP or Azure  Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc  Experience building and maintaining ETL (managing high-quality reliable ETL pipelines) 
We’ll really swoon if you have   2+ years of experience of technical leadership in building data engineering pipelines for AI  Previous experience in building data pipeline for conversational AI APIs and recommender systems  Experience with distributed systems and microservices  Experience with Kubernetes and building Docker images  Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming  Strong understanding of applied machine learning topics  Be familiar with legal compliance (with data management tools) data classification, and retention  Consistent track record of managing and implementing complex data projects 
What You'll Love About Us  Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world  Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto  Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents  Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US  Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs  Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more  Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events 
Base Pay Range$160,000—$280,000 USDhttps://www.recruitingfromscratch.com/"
Senior Data Engineer,Motion Recruitment,"Irving, TX (Hybrid)",https://www.linkedin.com/jobs/view/3779714778/?eBP=JOB_SEARCH_ORGANIC&refId=hsSkwim35BCTf7aaW4iQ9w%3D%3D&trackingId=1laZPF2NUWjf%2FYflzgD%2BRQ%3D%3D&trk=flagship3_search_srp_jobs,3779714778,"About the job
            
 
Job Description For close to thirty years, this organization has stood at the forefront of the industry, leading the way in safeguarding and rejuvenating motor vehicles through specialized programs designed to optimize partner performance. They have revolutionized the field by introducing the first paintless dent repair (PDR) service contract, setting a new industry standard and remains dedicated to discovering innovative ways to collaborate with top-tier partners across the automotive, recreational, powersports, and marine markets we cater to. Drawing on the extensive expertise of our leadership team, they persist in cultivating enduring partnerships—a defining characteristic of their business ethos. As they continue to grow, this team is in search of a Senior Data Engineer. As a member of the Data and Analytics team your role will contribute to the success in both the daily operations and long-term strategy of the organization’s data insights. The optimal candidate brings a business-centric mindset to data consumption and governance efforts and has a history of being a growth enabler. As the Senior Data Engineer you’ll be primarily responsible for leading projects to automate manual processes and produce data driven insights for the team and other stakeholders across the organization, managing Data Consumption and Data Governance areas of DNA, and actively contribute to both data & application governance initiatives by designing and implementing policies, procedures, and best practices. You’ll collaborate with Business Intelligence developers/analysts and business teams to improve data models that feed Power BI and increase data accessibility. The optimal candidate brings a business-centric mindset to solve business problems though data analysis, advanced skills within SQL with the ability to retrieve data from systems, create integrated datasets, update, and create data tables and views, excellent problem solving and troubleshooting skills and any additional experience in financial, automotive, or insurance industries is a nice plus. This role local to Irving, TX and in office 4-5 days per week with some work from home flexibility. This role does not offer sponsorship or the opportunity for candidates to work C2C. Required Skills & Experience ETL processing Building data pipelines Python SQL Data Migration Data Warehousing Data Architecture Data Modeling Azure Databricks 
Desired Skills & Experience SQL Server Stored Procedures (Views, functions, triggers) PowerBI or other BI tools Enterprise Data Governance Enterprise Data Management Data science tooling and statistical modeling NetSuite, Great Plains, GAAP 
What You Will Be Doing Daily Responsibilities Data Engineering 90% Data visualization: 5% Data analysis/SQL querying: 5% 
The Offer $135,000 DOE Additional performance-based bonuses 
You Will Receive The Following Benefits Medical, Dental, and Vision insurance Paid Time Off (PTO) 401(k) + match Work from home flexibility 
Applicants must be currently authorized to work in the US on a full-time basis now and in the future.Posted By: Kelly Ethridge"
Lead Data Engineer-US,Zortech Solutions,"Jersey City, NJ (Hybrid)",https://www.linkedin.com/jobs/view/3667472992/?eBP=JOB_SEARCH_ORGANIC&refId=hsSkwim35BCTf7aaW4iQ9w%3D%3D&trackingId=HtaSG1nnkoS1atH47lR5UA%3D%3D&trk=flagship3_search_srp_jobs,3667472992,"About the job
            
 
Role: Cloud EngineerLocation: Remote/USDuration: 6+ MonthsJob DescriptionPrimary Job Function: The cloud engineering function provides in-depth technical expertise of cloud services, including cloud architecture, provisioning, and technical support of cloud platforms. Strong understanding of identifying optimal cloud-based solutions in support of business needs and deploying and maintaining cloud platforms in accordance with best practices and company policies. Responsible for designing, securing, and deploying cloud platforms primarily through infrastructure as code and automation. Regularly peer review small to mid-sized Infrastructure as Code scripts.Provides technical support on cloud platform issues. Architectural design knowledge of cloud platforms, best practices, and industry trends. Create small to mid-sized cloud solution architectural designs incorporating industry best practices. May assist senior engineer or team lead in larger scale more complex cloud designs.
Requirements Minimum of five years' experience in cloud computing, specifically in the design, building, deploying and system administration of cloud services. Experience with CI/CD processes. Experience working in an Agile environment. Experience coding infrastructure as code. Requires excellent analytical, communication and consultative skills, as well as sound judgment and the ability to work effectively with business stakeholders and other IT management and staff. Experience establishing guidelines, procedures and standards for platform automation is a plus. Strong OS systems knowledge is a plus. Cloud Platform certifications strongly preferred.
Responsibilities Daily interaction with divisional clients and peer resources in other technical support organizations (e.g. hardware, storage, etc.). Monthly interaction with vendors.Level 3+ infrastructure support for cloud platforms.Determine cloud platform design project requirements and the architectural requirements in support of business needs.Independently conceives and develops general approaches to a task/technology design project.Exercises latitude in approach to problem and solution.Adheres to established policies."
Cyber Associate Data Engineer,New York City Office of Technology & Innovation,"Brooklyn, NY (Hybrid)",https://www.linkedin.com/jobs/view/3747044845/?eBP=JOB_SEARCH_ORGANIC&refId=hsSkwim35BCTf7aaW4iQ9w%3D%3D&trackingId=AGQI0v2YDMiSZwHAm0Rqcg%3D%3D&trk=flagship3_search_srp_jobs,3747044845,"About the job
            
 
The Office of Technology and Innovation (OTI) leverages technology to drive opportunity, improve public safety, and help government run better across New York City. From delivering affordable broadband to protecting against cybersecurity threats and building digital government services, OTI is at the forefront of how the City delivers for New Yorkers in the 21st century. Watch our welcome video to see our work in action, follow us on social media @NYCOfficeofTech, and visit oti.nyc.gov to learn more.At OTI, we offer great benefits, and the chance to work on projects that have a meaningful impact on millions of people. You'll have the opportunity to work with cutting-edge technology and collaborate with other passionate professionals who share your drive and commitment to making a difference through technology.About New York City Cyber CommandOTI / NYC Cyber Command is charged with protecting all City systems against cyber threats, including systems that deliver vital services to New Yorkers. Headed by the Chief Information Security Officer of the City of New York, we provide in-depth support to over 100 agencies and offices to protect, detect, identify, respond to, and recover from cyber threats.The Security Sciences teams provide highly functional, available, trusted solutions that enable the City government to prevent, detect, respond, and recover from cyber threats. Security Sciences is responsible for security architecture and engineering, with an emphasis on big data and emerging technology. This includes evaluating security tools, building a highly resilient and defensible security architecture, and supporting Cyber Command’s software engineering and development lifecycle to rapidly meet the mission needs.About The PositionWe are seeking a motivated Associate Data Engineer to join our Data Science Team. Our Data Science Team strives to make security data a strategic asset by providing a platform to structure, manage, integrate, control, analyze, and support threat management activities. As an Associate Data Engineer, you will help build a secure, scalable, and cloud native data processing framework that will support OTI / Cyber Command's cybersecurity mission.Responsibilities will include:  Develop and maintain our data pipeline using Apache Beam, Java, Python, and other data processing technologies Identify and implement performance improvements across all pipelines Engage with data consumers and producers to design appropriate models to suit all needs. Maintain information exchanges through publish, subscribe, and alert functions that enable users to send and receive critical information as required. Support incident management, service-level management, change management, release management, continuity management, and availability management for databases and data management systems. Administer databases and/or data management systems that allow for the secure storage, query, protection, and utilization of data.
HOURS/SHIFTDay - Due to the necessary technical duties of this position in a 24/7 operation, candidate may be required to work various shifts such as weekends and/or nights/evenings.WORK LOCATIONBrooklyn, NYTO APPLYSpecial Note: Taking and passing civil service exams are necessary to maintain employment with the City of New York. Please check the Department of Citywide Administrative Services (DCAS) website (http://www.nyc.gov/html/dcas/html/work/exam_monthly.shtml) for important exam filing information. Please ensure that you are either a permanent employee in the civil service title listed on this posting, or, that you file for the examination when there is an open filing period. For more information regarding the civil service process, please visit the DCAS website at: http://www.nyc.gov/html/dcas/html/work/work.shtmlInterested applicants with other civil service titles who meet the preferred requirements should also submit a resume for considerationPlease go to www.cityjobs.nyc.gov and search for Job ID # 613352SUBMISSION OF A RESUME IS NOT A GUARANTEE THAT YOU WILL RECEIVE AN INTERVIEWAPPOINTMENTS ARE SUBJECT TO OVERSIGHT APPROVALOTI participates in E-Verify.Minimum Qualifications  A baccalaureate degree, from an accredited college including or supplemented by twenty-four (24) semester credits in cyber security, network security, computer science, computer programming, computer engineering, information technology, information science, information systems management, network administration, or a pertinent scientific, technical or related area; or A four-year high school diploma or its equivalent approved by a State’s department of education or a recognized accrediting organization and three years of satisfactory experience in any of the areas described in “1” above; or Education and/or experience equivalent to “1” or “2”, above. College education may be substituted for up to two years of the required experience in “2” above on the basis that sixty (60) semester credits from an accredited college is equated to one year of experience. In addition, twenty-four (24) credits from an accredited college or graduate school in cyber security, network security, computer science, computer programming, computer engineering, information technology, information science, information systems management, network administration, or a pertinent scientific, technical or related area; or a certificate of at least 625 hours in computer programming from an accredited technical school (post high school), may be substituted for one year of experience.
Preferred SkillsThe successful candidate should possess the following: - Experience with the Agile Development Methodology - Practical knowledge of both Java and Python - Familiarity with Unix scripting, Web development, and automated testing - Familiarity with machine learning techniques and machine learning toolkits such as R, scikit-learn, etc. - Experience working with Terraform. - Familiarity with the CI/CD process - At least one year professional, academic, or personal experience with software development or data engineering experience (includes internship experience). - At least 1 year professional, academic, or personal experience with object-oriented/object function scripting languages preferably java or python. - Familiarity with or exposure to cloud application development. - Familiarity with distributed data processing frameworks.55a ProgramThis position is also open to qualified persons with a disability who are eligible for the 55-a Program. Please indicate at the top of your resume and cover letter that you would like to be considered for the position through the 55-a Program.Public Service Loan ForgivenessAs a prospective employee of the City of New York, you may be eligible for federal loan forgiveness programs and state repayment assistance programs. For more information, please visit the U.S. Department of Education’s website at https://studentaid.gov/pslf/.Residency RequirementNew York City residency is generally required within 90 days of appointment. However, City Employees in certain titles who have worked for the City for 2 continuous years may also be eligible to reside in Nassau, Suffolk, Putnam, Westchester, Rockland, or Orange County. To determine if the residency requirement applies to you, please discuss with the agency representative at the time of interview.Additional InformationThe City of New York is an inclusive equal opportunity employer committed to recruiting and retaining a diverse workforce and providing a work environment that is free from discrimination and harassment based upon any legally protected status or protected characteristic, including but not limited to an individual's sex, race, color, ethnicity, national origin, age, religion, disability, sexual orientation, veteran status, gender identity, or pregnancy."
Senior Data Engineer,Carrier,"East Syracuse, NY (Hybrid)",https://www.linkedin.com/jobs/view/3644630178/?eBP=JOB_SEARCH_ORGANIC&refId=hsSkwim35BCTf7aaW4iQ9w%3D%3D&trackingId=lgqdeCDznDRjTIMVaY%2B2EQ%3D%3D&trk=flagship3_search_srp_jobs,3644630178,"About the job
            
 
Date Posted: 2022-11-24-08:00Country: United States of AmericaLocation: CANTR: Refrigeration-Syracuse, NY 6304 Thompson Rd, Syracuse, NY, 13221 USACarrier is the leading global provider of healthy, safe and sustainable building and cold chain solutions with a world-class, diverse workforce with business segments covering HVAC, refrigeration, and fire and security. We make modern life possible by delivering safer, smarter and more sustainable services that make a difference to people and our planet while revolutionizing industry trends. This is why we come to work every day. Join us and we can make a difference together.About This RoleThe data engineer will work with global cross-disciplinary teams to develop the next generation of digital analytics solutions for Carrier Refrigeration businesses.Carrier Transicold & Refrigeration Systems (Refrigeration) helps improve the transport and shipping of temperature-controlled cargoes with a complete line of equipment and services for refrigerated transport, industrial cooling, retail display and cold chain visibility. For 50 years, the Refrigeration business has been an industry leader, providing customers around the world with the most advanced, energy efficient and environmentally sustainable container refrigeration systems and generator sets, direct-drive and diesel truck units, trailer refrigeration systems, mechanical systems and refrigerated display cases.The data engineer will work with global cross-disciplinary teams to develop the next generation of digital analytics solutions for Carrier Refrigeration businesses.Key Responsibilities Coordinate with cross-functional teams to develop technical requirements for addressing business problemsWork within engineering teams to understand the data needs for developing analytics solutions to address business problemsDeveloping highly-efficient data pipelines that facilitate the ability to build data productsBuild the infrastructure for the optimal extraction, transformation and loading of data from a wide variety of data sources (including external data sources and services)Collaborate with data scientists and architects to deliver high quality data productsExplore mechanisms to enhance data quality and reliabilityEnsures that information is in compliance with the regulatory and security policies in placeEnsure strong commitment to milestones, quality and deliverablesStrong documentation of integration pipelinesWork with business to obtain an understanding of complex business problems and propose technical solutions
Basic Qualifications BS in Engineering/Computer Science3+ years of software development experience
Preferred Qualifications:Other qualifications you may have that would be beneficial in this role include: MS in Engineering, Data Engineering or another quantitative fieldAdvanced experience programming with SQL (ETL, performance tuning, stored procedures)Technical expertise in data modelingExperience with both large-scale batch and real-time data pipelinesExperience with large scale data analytics, replicated data storage, and operating services on prem or in the cloudPrior experience in development of or implementation of big data architecturesExperience creating and managing integration pipelines from scratchExperience with large datasets with missing and noisy dataExcellent communication and leadership skillsMust have experience with designing relational SQL and NoSQL databasesExperience with assimilating data through varied sources is valuedOverall passion and curiosity for technology, ability to research new technologiesPenchant for identifying and clarifying ambiguous requirementsMust be self-motivated and detail-orientedA “can do” attitude with strong work ethic and a history of producing quality applicationsExperience with iterative development and agile methodologies
RSRCARCarrier is An Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or veteran status, age or any other federally protected class.Job Applicant's Privacy Notice:Click on this link to read the Job Applicant's Privacy NoticeThis position may not be performed remotely in Westchester County, Ithaca or New York City at this time.COVID-19 vaccines are required for all newly hired Carrier U.S. Salaried employee, except as prohibited by law. Candidates residing in or for positions located in any of the following jurisdictions are not subject to this requirement: Alabama, Arkansas, Florida, Kansas, Indiana, Iowa, Mississippi, Montana, Nebraska, North Dakota, Tennessee, Texas, Utah, and West Virginia. If you have questions about applicability of this requirement to you, please contact Global People Services at: +1-833-819-1257"
Senior Data Engineer,"Steampunk, Inc.","McLean, VA (Hybrid)",https://www.linkedin.com/jobs/view/3734165558/?eBP=JOB_SEARCH_ORGANIC&refId=hsSkwim35BCTf7aaW4iQ9w%3D%3D&trackingId=CFgB1ikG%2BBRFsIt8aJl5Zw%3D%3D&trk=flagship3_search_srp_jobs,3734165558,"About the job
            
 
In today’s rapidly evolving technology landscape, an organization’s data has never been a more important aspect in achieving mission and business goals. Our data exploitation experts work with our clients to support their mission and business goals by creating and executing a comprehensive data strategy using the best technology and techniques, given the challenge.At Steampunk, our goal is to build and execute a data strategy for our clients to coordinate data collection and generation, to align the organization and its data assets in support of the mission, and ultimately to realize mission goals with the strongest effectiveness possible.For our clients, data is a strategic asset. They are looking to become a facts-based, data-driven, customer-focused organization. To help realize this goal, they are leveraging visual analytics platforms to analyze, visualize, and share information. At Steampunk you will design and develop solutions to high-impact, complex data problems, working with the best and data practitioners around. Our data exploitation approach is tightly integrated with Human-Centered Design and DevSecOps.We are looking for seasoned Senior Data Engineer to work with our team and our clients to develop enterprise grade data platforms, services, and pipelines. We are looking for more than just a ""Senior Data Engineer"", but a technologist with excellent communication and customer service skills and a passion for data and problem solving. Lead and architect migration of data environments with performance and reliability.Assess and understand the ETL jobs, workflows, BI tools, and reportsAddress technical inquiries concerning customization, integration, enterprise architecture and general feature / functionality of data productsExperience in crafting database / data warehouse solutions in cloud (Preferably AWS. Alternatively Azure, GCP).Key must have skill sets – Python, AWSSupport an Agile software development lifecycleYou will contribute to the growth of our Data Exploitation Practice!
US Citizen OnlyAbility to hold a position of public trust with the US government.8+ years experience with a Bachelor's Dergee or 6+ years of experience with a Master's Degree5-7 years industry experience coding commercial software and a passion for solving complex problems. 5-7 years direct experience in Data Engineering with experience in tools such as Big data tools DataBricks, Confluent Kafka, Collibra, Spark, etc.Relational SQL and NoSQL databases, including SQL, Postgres and Oracle.Data pipeline and workflow management tools Azkaban, Luigi, Airflow, etc.AWS cloud services EC2, EMR, RDS, S3 (or Azure equivalents)Object-oriented/object function scripting languages Python, Java, C++, Scala, etc.
Advanced working SQL knowledge and experience working with relational databases, query authoring and optimization (SQL) as well as working familiarity with a variety of databases.Experience with message queuing, stream processing, and highly scalable ‘big data’ data stores.Experience manipulating, processing, and extracting value from large, disconnected datasets.Experience manipulating structured and unstructured data for analysisExperience constructing complex queries to analyze results using databases or in a data processing development environmentExperience with data modeling tools and processExperience architecting data systems (transactional and warehouses)Experience aggregating results and/or compiling information for reporting from multiple datasetsExperience working in an Agile environment Experience supporting project teams of developers and data scientists who build web-based interfaces, dashboards, reports, and analytics/machine learning models
Steampunk is a Change Agent in the Federal contracting industry, bringing new thinking to clients in the Homeland, Federal Civilian, Health and DoD sectors. Through our Human-Centered delivery methodology, we are fundamentally changing the expectations our Federal clients have for true shared accountability in solving their toughest mission challenges. As an employee owned company, we focus on investing in our employees to enable them to do the greatest work of their careers – and rewarding them for outstanding contributions to our growth. If you want to learn more about our story, visit http//www.steampunk.com.We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law. Steampunk participates in the E-Verify program."
Sr. Data Engineer,Furniture.com,"Atlanta, GA (Hybrid)",https://www.linkedin.com/jobs/view/3745120051/?eBP=JOB_SEARCH_ORGANIC&refId=hsSkwim35BCTf7aaW4iQ9w%3D%3D&trackingId=QYoSFwinS3yUmLj943rYGA%3D%3D&trk=flagship3_search_srp_jobs,3745120051,"About the job
            
 
We are a high growth startup backed by one of America’s top furniture retailers. We are looking for a Sr. Data Engineer to execute our Data Strategy roadmap. This role will be part of a new team accountable for building our critical data ecosystem. You will bring together several diverse data payloads from external partners and customers into specialized data stores. You will work in close collaboration with data science, platform engineering, and product teams to build ETL pipelines, MDM solutions, and access control mechanisms to enable effective governance of the data for product delivery. This is a unique opportunity to be the first in a new department at a rapidly scaling startup.What you will do:  Build and maintain scalable ETL pipelines effectively transform data from disparate data sources into a canonical data store  Design data architecture and partner with our infrastructure team to execute the build out of the data foundation  Operate current data ingest services while building the next generation of ETL pipelines  Define a common data vocabulary across the organization  Develop audit reports to provide insights into overall system health and resiliency  Contribute to the technology roadmap 
What we’re looking for:  Bachelor’s degree in Information Technology, Computer Science, or related field  5+ years as a Data Engineer working on large scale ETL systems  AWS experience developing Integrations and analytical services  Demonstrated proficiency in Python and SQL  Experience with one or more commercial or open-source technologies like Databricks, Informatica, Talend, Airflow, Kafka is essential  Experience collaborating with data scientists and data analyst on projects related to forecasting, dashboards, and reports  Passionate about mentoring other team members and foster a culture of open feedback and communication  Able to communicate effectively with a technical and non-technical audience  Experience building web-based reporting solutions with data visualization technologies like Power BI, Tableau etc. is a plus  Able to contribute to prototypes and architecture by contributing code and designs  Experience working with Product Management in an Agile environment and break down complex features into manageable user stories  Demonstrated track record building data solutions based on a data lake is a plus 
The company is a high growth consumer tech start up that is addressing fundamental challenges in the$200B US furniture space. The company is a B2B and B2C digital aggregator with a simple mission: Toconnect furniture customers with the products they desire. For consumers, we make it easy to find theright furniture by enhancing search and streamlining their end-to-end customer experience. For retailpartners, we deliver a digital platform allowing them to expand their reach with a high-intent furnitureaudience. The site experience and discovery tools will differentiate us. The site design and content willdrive brand recognition and loyalty allowing us to capitalize on this market.We are backed by a top-5 furniture retailer in the US. The business is unique in that it will operate with the agility and innovation of a startup, yet with the robust resources and industry knowledgeset of an established brand.The company is an equal opportunity employer. We do not discriminate in hiring or employment against any individual based on race, color, gender, national origin, ancestry, religion, physical or mental disability, age, veteran status, sexual orientation, gender identity or expression, marital status, pregnancy, citizenship, or any other factor protected by anti-discrimination lawsApplicants must be authorized to work in the U.S. This role is hybrid work model and candidates must be able to commute to the NE Atlanta/Brookhaven area."
Data Engineer/Senior Data Engineer,UVA Health,"Charlottesville, VA (Hybrid)",https://www.linkedin.com/jobs/view/3758988122/?eBP=JOB_SEARCH_ORGANIC&refId=hsSkwim35BCTf7aaW4iQ9w%3D%3D&trackingId=8wo5nAEm4jKy4z5l48IIuA%3D%3D&trk=flagship3_search_srp_jobs,3758988122,"About the job
            
 
The University of Virginia Darden School of Business, one of the world's leading business schools, seeks a highly skilled and motivated Data Engineer/Senior Data Engineer to join its Strategic IT Data & Analytics team. This role will play a crucial role in building and maintaining our data infrastructure, ensuring the efficient extraction, loading and transformation (ELT) of data from various sources into our cloud-based data environments. The ideal candidate should have hands-on experience with ELT processes in cloud data platforms such as Databricks, Snowflake, or Microsoft Synapse. If you are passionate about data engineering, have strong analytical skills, a passion for higher education and enjoy working in a collaborative environment, we would love to hear from you. While the position is based in Charlottesville, VA, we offer the opportunity for the employee to work remotely for the majority of the year. However, we do require that the employee be onsite for one week every quarter.To be considered for a data engineer role, responsibilities include: Assist in designing and implementing efficient and scalable data pipelines using cloud-based ELT technologies (Microsoft Data Factory, Databricks, etc.) to support our data processing and analytics needs. Collaborating with cross-functional teams, including data analytics developers, functional analysts, and business stakeholders, to understand data requirements and ensure the smooth integration of data sources. Assist in building models, data integration workflows, and data transformation processes to support data analysis, reporting, and visualization. Developing, maintaining and orchestrating data pipelines and data processing workflows, ensuring data quality, reliability, and performance. Identifying and addressing performance bottlenecks and data quality issues in collaboration with the data operations team. Monitoring and troubleshooting data pipelines to ensure high availability, scalability, and optimal performance. Assist in implementing data governance and security measures to ensure compliance with data protection regulations and industry best practices. Keeping up-to-date with emerging trends, design patterns and technologies in data engineering and cloud-based data processing to drive continuous improvement and innovation within the organization. 
In addition to the above, Senior Data Engineer responsibilities include: Utilizing more fully developed technical skills and emerging advanced skills, to take on data engineering projects that are more complex.Designing scalable and efficient data architectures. Optimizing data pipelines, data integration workflows, and data transformation processes Leading the implementation of data governance practices, ensuring compliance with data protection regulations and industry standards. Providing guidance and mentorship to junior team members 
Education And Professional Experience Data Engineer - Bachelor's degree in Computer Science, MIS, Computer Engineering or related discipline with at least five years of directly related experience. Relevant experience may be considered in lieu of a degree.Senior Data Engineer - Bachelor's degree in Computer Science, MIS, Computer Engineering or a related discipline with at least seven years of directly related experience. Relevant experience may be considered in lieu of a degree.Certification in Databricks and/or Snowlake strongly preferred, or demonstrated progress to complete certification. Proven work experience as a Data Engineer or in a similar role, with a focus on developing and implementing data pipelines and ELT processes. Strong proficiency in working with cloud-based data platforms such as Microsoft Synapse, Databricks, or Snowflake, and experience with ELT processes. Hands-on experience with programming languages such as SQL, Python, and Scala for data manipulation, scripting, and automation. Proficiency in data modeling, database design, and SQL optimization techniques. Solid understanding of data warehousing concepts, dimensional modeling, and data integration techniques. Familiarity with data governance, data security, and privacy practices. Strong analytical and problem-solving skills, with the ability to work with large and complex datasets. Knowledge and experience working with APIs and web services. Familiarity with incorporating continuous integration/continuous deployment (CI/CD) development practices into data engineering workflows. Experience with big data technologies (e.g., Hadoop, Spark) and distributed computing frameworks is a plus. Excellent communication and collaboration skills, with the ability to work effectively in a team-oriented environment. 
To ApplyPlease apply through Workday, and search for R0051124. Internal applicants must apply through their UVA Workday profile by searching 'Find Jobs'.Complete The Application, And Upload The Following Required Materials CV/ResumeCover letter
Please note: Multiple documents can be submitted into the CV/Resume field. Alternatively, merge all documents into one PDF for submission. Applications that do not contain all required documents will not receive full consideration.For questions about the application process, please contact Jon Freeman, Academic Recruiter jf2sw@virginia.edu.For more information about UVA and the Charlottesville community please see http://www.virginia.edu/life/charlottesville and https://embarkcva.com/.The University of Virginia, including the UVA Health System which represents the UVA Medical Center, Schools of Medicine and Nursing, UVA Physician’s Group and the Claude Moore Health Sciences Library, are fundamentally committed to the diversity of our faculty and staff. We believe diversity is excellence expressing itself through every person's perspectives and lived experiences. We are equal opportunity and affirmative action employers. All qualified applicants will receive consideration for employment without regard to age, color, disability, gender identity or expression, marital status, national or ethnic origin, political affiliation, race, religion, sex (including pregnancy), sexual orientation, veteran status, and family medical or genetic information."
Sr. Data Engineer,Chubb,"Whitehouse Station, NJ (Hybrid)",https://www.linkedin.com/jobs/view/3756339027/?eBP=JOB_SEARCH_ORGANIC&refId=hsSkwim35BCTf7aaW4iQ9w%3D%3D&trackingId=ltJBfHomZyx7MaB3cT5xFA%3D%3D&trk=flagship3_search_srp_jobs,3756339027,"About the job
            
 
Job DescriptionAs a Sr. Data Engineer on our team, you will work on implementing complex data projects focusing on collecting, parsing, managing, analyzing, and visualizing large sets of data to turn information into insights using multiple platforms. We will look to you to be able to decide on hardware and software design needs and act according to the decisions as well as develop a proofs of concept for the selected solutions.We are looking for someone with a strong background in computer programming, data analysis, and visual analytics who is eager to tackle problems with large, complex datasets using the latest skills. You are a self-starter who will take ownership of your projects and deliver high-quality data-driven analytics solutions. You can solve diverse business problems using various tools, strategies, algorithms, and programming languages.In This Role, You Will Utilize the data engineering skills within and outside of the developing Chubb information ecosystem for discovery, analytics, and data managementWork with the data SME, Business Analyst team to design the solutionsYou will be using strong SQL skills to convert one 'raw' data from XML to a structured formatWork with various relational and non-relational data sources with the target being Azure-based SQL Data WarehouseSourcing data from underwriting applications, profiling, cleansing, and conforming to create master data sets for analytics useClean, unify, and organize messy and complex data sets for easy access, different levels of abstractions, and analysisHands-on data preparation activities using the SSIS package developmentPartner with the Global Data Analytics team to navigate the discovery solutions for data ingestionWork closely with the Data Science team to perform complex analytics and data preparation tasksBuild, deploy, and support applications, services, and APIs that meet business requirements in a highly complex technical environmentCoordinate with business stakeholders, business analysts, and project stakeholders to ensure proper assignments of a user story that meets business needs and SLAsInterfaces with business areas and other Chubb IT teams to coordinate work, manage cross-functional dependencies, and foster collaborationEnsuring application stability, and participating in resolving incidents and problems.Leverage your technical insights to stay on top of technical trends, evaluate new technologies, and develop proofs of concept for incorporation into our infrastructure
Qualifications Bachelor’s degree in Computer Science or any other IT-related field, or equivalent work experienceMinimum of 8-10 years of IT experience in a comparable role as a Data Engineer/Data AnalystNice to have experience in the P&C insurance/Life Insurance/ Investment/Banking domainExperience working in agile environments with application teamKnowledge of DMBS’ with the ability to write SQL queriesHands experience in Microsoft SQL server Database management and SSIS package developmentNice to have experience with R, Python, and data visualization tools like Qlik Sense/Power BI/Tableau/AlteryxExperience with DevOps tools, Jenkins and Jira, or equivalent Agile toolsNice to have experience in .NET or any equivalent technology stacksStrong analytical skills and coordination skills to achieve resultsAbility to identify, understand, and communicate business needsModels company values; demonstrates high integrity; meets commitmentsDemonstrates a sense of urgency and accountability; sets priorities and acts on key issues
About UsChubb is the world’s largest publicly traded property and casualty insurer. With operations in 54 countries, Chubb provides commercial and personal property and casualty insurance, personal accident and supplemental health insurance, reinsurance, and life insurance to a diverse group of clients. The company is distinguished by its extensive product and service offerings, broad distribution capabilities, exceptional financial strength, underwriting excellence, superior claims handling expertise and local operations globally.At Chubb, we are committed to equal employment opportunity and compliance with all laws and regulations pertaining to it. Our policy is to provide employment, training, compensation, promotion, and other conditions or opportunities of employment, without regard to race, color, religious creed, sex, gender, gender identity, gender expression, sexual orientation, marital status, national origin, ancestry, mental and physical disability, medical condition, genetic information, military and veteran status, age, and pregnancy or any other characteristic protected by law. Performance and qualifications are the only basis upon which we hire, assign, promote, compensate, develop and retain employees. Chubb prohibits all unlawful discrimination, harassment and retaliation against any individual who reports discrimination or harassment."
Senior Data Engineer,PamTen Inc,"Atlanta, GA (Hybrid)",https://www.linkedin.com/jobs/view/3767587585/?eBP=JOB_SEARCH_ORGANIC&refId=hsSkwim35BCTf7aaW4iQ9w%3D%3D&trackingId=MPZrtqrh4giir%2Fy2SwGnPA%3D%3D&trk=flagship3_search_srp_jobs,3767587585,"About the job
            
 
Job DescriptionAs a Data Architect, you will play a pivotal role in shaping our data infrastructure and ensuring that data flows seamlessly through our organization. Your primary responsibilities will include:Designing, developing, and maintaining end-to-end master data pipelines that efficiently collect, process, transform, and load data from diverse sources into our data ecosystem.Collaborating with cross-functional teams to understand business requirements and translate them into effective data architecture solutions.Ensuring data quality, integrity, and security throughout the data lifecycle by implementing best practices and standards.Optimizing data pipelines for performance, scalability, and reliability, and identifying opportunities for automation and process improvement.Utilizing your expertise in SQL, Unix/Shell scripting, Python, and data processing frameworks to create and manage data transformations and integrations.Utilizing concepts like Data warehouse, Data Lake house, Data Mess to share enterprise data strategies.Working closely with DevOps and Engineering teams to implement continuous integration and continuous deployment (CI/CD) pipelines for data-related processes.Staying current with industry trends and emerging technologies in data architecture and applying that knowledge to drive innovation within the organization.Bachelor's degree in computer science, Information Technology, or a related field (Master's degree preferred).A minimum of 5 years of experience in data architecture, with a proven track record of designing and deploying master data pipelines.Proficiency in SQL for data manipulation and retrieval from relational databases.Strong scripting skills in Unix/Shell and Python for automating data processes and transformations.Experience with data processing frameworks such as Apache Spark, Apache Flink, or similar technologies.Familiarity with CI/CD tools and practices for automating deployment and monitoring of data pipelines.Excellent problem-solving skills and the ability to optimize data workflows for performance and efficiency.Solid understanding of data modeling, data warehousing concepts, and ETL processes.Strong communication skills to collaborate effectively with cross-functional teams and articulate complex technical concepts to non-technical stakeholders.Knowledge, Skills, And AbilitiesIn-depth experience of designing and implementing information solutions.Experience in architecture practice, tools, and methodologies.Demonstrate the ability to work well with others and exhibit leadership.Have a track record of remaining unbiased toward specific technologies or vendors.Be an excellent communicator and collaborator, engaging with multiple technical and business stakeholders and leaders.Be able to translate the information architecture contribution to business outcomes into simple briefings for use by various data-and-analytics-related roles.Organizationally savvy, with a good understanding of the enterprise's political climate and how to navigate, influence and persuade political waters.Ability to communicate, influence and persuade peers and leadership.Ability to understand the long-term ( ""big picture "") and short-term perspectives of situations.Ability to quickly comprehend the functions and capabilities of new technologies.Displays intellectual curiosity and integrity.Education & Experience7+ years to 15 years A minimum of 12+ years of experience in IT, majority in information system design.3+ years to 7 years A minimum of 5 years as a Data Architect with progressively increasing responsibility.Bachelor's Degree Computer and Information Science Bachelor's in computer science Required.Master's Degree Computer and Information Science MS preferred."
Data Integration Engineer/ Developer,Nine Mind Solutions,"Dulles, VA (Hybrid)",https://www.linkedin.com/jobs/view/3768021546/?eBP=JOB_SEARCH_ORGANIC&refId=hsSkwim35BCTf7aaW4iQ9w%3D%3D&trackingId=A7k7a97%2BvN3NKqOgwxk%2B%2FA%3D%3D&trk=flagship3_search_srp_jobs,3768021546,"About the job
            
 
We are seeking a qualified Cyber Security Data Integration Engineer/ Developer to support the design, development, and deployment of advanced cybersecurity capabilities.Eligibility  Must be a US Citizen  Must have an active TS/SCI clearance.  Must be able to obtain Client Suitability prior to starting employment  6+ years of directly relevant experience  4+ years of experience with administration of enterprise SIEM technologies ( Splunk primarily) 
ResponsibilitiesResponsibilities :The Security Engineer is to play a key role in supporting a statewide program providing cyber assessment services and management that will protect 20+ affiliates from growing and evolving cyber threats. The engineering effort will focus on cloud security, SIEM and log management, and endpoint detection/response protecting customers from the ever growing and evolving cyber threats. This person will also work with customers to ensure the organization's compliance standards are met and maintained while also driving solid customer relationships to the next level.This position requires a thorough understanding of network architecture fundamentals, protocols, routing, firewalls, cloud, and DevOps. This position is part of a larger team; however, the candidate is expected to work well on his or her own under general supervision, be self-directed, able to multi-task, and prioritize work.Required Skills  Splunk Cloud experience: Architect, design, engineer, support, configure, administer content and maintain infrastructure for a highly available and disaster recovery configuration  Splunk experience: Administer Splunk and Splunk Application for Enterprise Security log or event management  Expertise with EDR toolsets administration, analysis, and integrations preferably CrowdStrike  Familiarity with SOAR Products include Phantom and ThreatConnect  Experience with scripting (e.g., PowerShell, bash/ksh/sh,python)  Ability to assist team with Incident response and handling  Excellent demonstrated experience in communicating technical information to non-technical and technical audiences.  Experience working directly with senior leadership and management. 
Desired Skills  Automation: Experience related to Ansible for performing administration using code and Git/Gitlab for workflow management  Familiarity with Windows and Linux integration, SQL database technologies, troubleshooting, deployment, patching, and administration  Experience with Logstash and ability to collect, parse, and transform logs  Experience with the standards compliance process (e.g., NIST) and writing network security documentation 
Desired Certifications : Splunk IT Service Intelligence Certified Admin, Splunk Enterprise Security Certified Admin, Splunk Cloud Certified Admin, CCNA, CCNP), NCSP 800-53 PractitionerRequired Education : Bachelor's degree in Information Security, Cyber Engineering or a related discipline is required. Two years of related work experience may be substituted for each year of degree-level education."
Data Engineer IV,Navy Federal Credit Union,"Pensacola, FL (Hybrid)",https://www.linkedin.com/jobs/view/3768692870/?eBP=JOB_SEARCH_ORGANIC&refId=Dkum4n5CE4r0Ge8XgQQBEg%3D%3D&trackingId=wa6sC0Yd1qXlV8h%2BPyqlMA%3D%3D&trk=flagship3_search_srp_jobs,3768692870,"About the job
            
 
OverviewDevelop strategies for data acquisition, archive recovery, and database implementation. Responsible for designing, building, integrating data from various resources, and managing big data. Develop data consumption patterns to share data with internal/external channels, while ensuring they are easily accessible, work smoothly, with the goal of optimizing the performance of Navy Federal’s big data ecosystem. Recognized as an expert with a specialized depth and/or breadth of expertise in discipline. Solves highly complex problems; takes a broad perspective to identify solutions. Leads functional teams or projects. Works independently.Responsibilities Define and build data integration processes to be used across the organizationBuild channel contracts and data consumption patterns for customer facing (On-line/Mobile) channelsAnalyze and validate data sharing requirements within and outside data partners Recognize potential issues and risks during the project implementation and suggest mitigation strategiesCommunicate and own the processes related to contracts and data consumption patternsExpert and key point of contact between the operational data hubs and the channel contracts for On-line/MobileApply engineering principles into the design and enhancement of new and existing data management systemsCoach and mentor project team members in carrying out project implementation activities Work directly with business leadership to understand data requirements; propose and develop solutions that enable effective decision-making and drives business objectives Prepare advanced project implementation plans which highlight major milestones and deliverables, leveraging standard methods and work planning tools Lead the preparation of high-quality project deliverables that are valued by the business and present them in such a manner that they are easily understood by project stakeholdersEnsure the security and integrity of system and product solutions including compliance with Navy Federal, industry engineering and Information Security principles and practicesPresent clear, organized and concise information to all audiences through a variety of media to enable effective business decisionsPerform other duties as assigned
Qualifications Master’s degree in Information Systems, Computer Science, Engineering, or related field, or the equivalent combination of education, training and experienceAdvanced skills in systems and application integration in a large, distributed architecture environmentsProficient skill level in .Net/C#, Python, Agile Frameworks (SAFE), Microsoft Databricks, Azure Data FactoryProficient skills in developing and operationalizing various data distribution patterns like, APIs, event based, pub/sub models Proficient in Data Architecture, Web Services, REST APIs, Event and Pub/Sub messaging architectureProficient in Mobile and Web application technologiesAbility to understand the business problem and determine what aspects of it require optimization; articulate those aspects in a clear and concise mannerProficient skills in understanding SQL and NoSQL and JSON structure Ability to understand other projects or functional areas to consolidate analytical and operational needs and processesDemonstrates change management and/or excellent communication skills Working knowledge of various data structures and the ability to extract data from various data sources Understands the concepts and application of data mapping and building requirementsUnderstands data models, large datasets, business/technical requirementsSkilled in managing the process between updating and maintaining data source systems and implementing data related requirements
Desired Qualifications Knowledge of Navy Federal Credit Union instructions, standards, and procedures
Hours: Monday - Friday, 8:00AM - 4:30PMLocation: 820 Follin Lane, Vienna, VA 22180 | 5550 Heritage Oaks Dr. Pensacola, FL 32526 | 141 Security Dr. Winchester, VA 22602About UsYou have goals, dreams, hobbies, and things you're passionate about—what's important to you is important to us. We're looking for people who not only want to do meaningful, challenging work, keep their skills sharp and move ahead, but who also take time for the things that matter to them—friends, family, and passions. And we're looking for team members who are passionate about our mission—making a difference in military members' and their families' lives. Together, we can make it happen. Don't take our word for it:  Military Times 2022 Best for Vets Employers WayUp Top 100 Internship Programs Forbes® 2022 The Best Employers for New Grads Fortune Best Workplaces for Women Fortune 100 Best Companies to Work For® Computerworld® Best Places to Work in IT Ripplematch Campus Forward Award - Excellence in Early Career Hiring Fortune Best Place to Work for Financial and Insurance Services
Equal Employment Opportunity: Navy Federal values, celebrates, and enacts diversity in the workplace. Navy Federal takes affirmative action to employ and advance in employment qualified individuals with disabilities, disabled veterans, Armed Forces service medal veterans, recently separated veterans, and other protected veterans. EOE/AA/M/F/Veteran/Disability EOE/AA/M/F/Veteran/DisabilityDisclaimers: Navy Federal reserves the right to fill this role at a higher/lower grade level based on business need. An assessment may be required to compete for this position. Job postings are subject to close early or extend out longer than the anticipated closing date at the hiring team’s discretion based on qualified applicant volume. Navy Federal Credit Union assesses market data to establish salary ranges that enable us to remain competitive. You are paid within the salary range, based on your experience, location and market positionBank Secrecy Act: Remains cognizant of and adheres to Navy Federal policies and procedures, and regulations pertaining to the Bank Secrecy Act."
"Data Engineer, Data Platform",Grammarly,"Nevada, United States (Hybrid)",https://www.linkedin.com/jobs/view/3656892825/?eBP=JOB_SEARCH_ORGANIC&refId=Dkum4n5CE4r0Ge8XgQQBEg%3D%3D&trackingId=txmyBbybYGLZDyyipjhiHA%3D%3D&trk=flagship3_search_srp_jobs,3656892825,"About the job
            
 
Grammarly is excited to offer a remote-first hybrid working model. Team members work primarily remotely in the United States, Canada, Ukraine, Germany, or Poland. Certain roles have specific location requirements to facilitate collaboration at a particular Grammarly hub.All roles have an in-person component: Conditions permitting, teams meet 2–4 weeks every quarter at one of Grammarly’s hubs in San Francisco, Kyiv, New York, Vancouver, and Berlin, or in a workspace in Kraków. This flexible approach gives team members the best of both worlds: plenty of focus time along with in-person collaboration that fosters trust and unlocks creativity.Grammarly team members in this role must be based in the United States or Canada, and they must be able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub(s) where the team is based.The opportunity Grammarly is the world’s leading AI writing assistance company trusted by over 30 million people and 70,000 professional teams every day. From instantly creating a first draft to perfecting every message, Grammarly’s product offerings help people at 96% of the Fortune 500 get their point across—and get results. Grammarly has been profitable for over a decade because we’ve stayed true to our values and built an enterprise-grade product that’s secure, reliable, and helps people do their best work—without selling their data. We’re proud to be one of Inc.’s best workplaces, a Glassdoor Best Place to Work, one of TIME’s 100 Most Influential Companies, and one of Fast Company’s Most Innovative Companies in AI.To achieve our ambitious goals, we’re looking for a Data Engineer to join our Data Engineering Platform team. This person will build highly automated, low latency core datasets that will help data engineers and end users across Grammarly to work with analytical data at scale.Grammarly’s engineers and researchers have the freedom to innovate and uncover breakthroughs—and, in turn, influence our product roadmap. The complexity of our technical challenges is growing rapidly as we scale our interfaces, algorithms, and infrastructure. Read more about our stack or hear from our team on our technical blog.Your impactAs a Data Engineer on our Data Engineering Platform team, you will: Drive improvements to make our analytics effortless by creating and adjusting core data models and storage structures, all while understanding the needs of our users. Make analytical data and metrics usable within a few minutes of real world events occuring, and build streaming processes for the output derived events and aggregate data.Model structure, storage, and access of data at very high volumes for our data lakehouse.Improve developer productivity and self-serve solutions by contributing components to our stream data processing framework(s).Own data engineering's infrastructure-as-code for provisioning services that allow our engineers to deploy mature software installations within a few hours.Build a world-class process that will allow our systems to scale.Mentor other back-end engineers on the team and help them grow.Build and contribute to AWS high-scale distributed systems on the back-end.
We’re Looking For Someone Who Embodies our EAGER values—is ethical, adaptable, gritty, empathetic, and remarkable.Is inspired by our MOVE principles, which are the blueprint for how things get done at Grammarly: move fast and learn faster, obsess about creating customer value, value impact over activity, and embrace healthy disagreement rooted in trust.Is able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub where the team is based.Has experience with Python, Scala, or Java.Has experience with designing database objects and writing relational queriesHas experience designing and standing up APIs and services.Has experience with system design and building internal tools.Has experience handling applications that work with data from data lakes.Has at least some experience building internal Admin sites.Has good knowledge of and at least some experience with AWS (or, alternatively, has deep expertise in Azure or GCE and is willing to learn AWS in a short time frame).Can knowledgeably choose an open source or third-party service to accomplish what they need or, alternatively, can devise a quick and simple solution on their own.
Support for you, professionally and personally Professional growth: We believe that autonomy and trust are key to empowering our team members to do their best, most innovative work in a way that aligns with their interests, talents, and well-being. We support professional development and advancement with training, coaching, and regular feedback.A connected team: Grammarly builds a product that helps people connect, and we apply this mindset to our own team. Our remote-first hybrid model enables a highly collaborative culture supported by our EAGER (ethical, adaptable, gritty, empathetic, and remarkable) values. We work to foster belonging among team members in a variety of ways. This includes our employee resource groups, Grammarly Circles, which promote connection among those with shared identities, such as BIPOC and LGBTQIA+ team members, women, and parents. We also celebrate our colleagues and accomplishments with global, local, and team-specific programs. 
Compensation And BenefitsGrammarly offers all team members competitive pay along with a benefits package encompassing the following and more: Excellent health care (including a wide range of medical, dental, vision, mental health, and fertility benefits)Disability and life insurance options401(k) and RRSP matching Paid parental leaveTwenty days of paid time off per year, eleven days of paid holidays per year, and unlimited sick days Home office stipendsCaregiver and pet care stipendsWellness stipendsAdmission discountsLearning and development opportunities
Grammarly takes a market-based approach to compensation, which means base pay may vary depending on your location. Our US and Canada locations are categorized into compensation zones based on each geographic region’s cost of labor index. For more information about our compensation zones and locations where we currently support employment, please refer to this page. If a location of interest is not listed, please speak with a recruiter for additional information.Base pay may vary considerably depending on job-related knowledge, skills, and experience. The expected salary ranges for this position are outlined below by compensation zone and may be modified in the future.United StatesZone 1: $167,000 - $242,000/year (USD)Zone 2: $150,000 – $218,000/year (USD)Zone 3: $142,000 – $206,000/year (USD)Zone 4: $134,000 – $194,000/year (USD)We encourage you to applyAt Grammarly, we value our differences, and we encourage all—especially those whose identities are traditionally underrepresented in tech organizations—to apply. We do not discriminate on the basis of race, religion, color, gender expression or identity, sexual orientation, ancestry, national origin, citizenship, age, marital status, veteran status, disability status, political belief, or any other characteristic protected by law. Grammarly is an equal opportunity employer and a participant in the US federal E-Verify program (US). We also abide by the Employment Equity Act (Canada).Please note that EEOC is optional and specific to US-based candidates.#NAAll team members meeting in person for official Grammarly business or working from a hub location are strongly encouraged to be vaccinated against COVID-19."
Principle Data Engineer,Clairvoyant,"Phoenix, AZ (Hybrid)",https://www.linkedin.com/jobs/view/3753045320/?eBP=JOB_SEARCH_ORGANIC&refId=Dkum4n5CE4r0Ge8XgQQBEg%3D%3D&trackingId=1429ikqwm2I7lyOZrZty9Q%3D%3D&trk=flagship3_search_srp_jobs,3753045320,"About the job
            
 
Company Overview And CultureEXL (NASDAQ: EXLS) is a global analytics and digital solutions company that partners with clients to improve business outcomes and unlock growth. Bringing together deep domain expertise with robust data, powerful analytics, cloud, and AI, we create agile, scalable solutions and execute complex operations for the world’s leading corporations in industries including insurance, healthcare, banking and financial services, media, and retail, among others. Focused on creating value from data for driving faster decision-making and transforming operating models, EXL was founded on the core values of innovation, collaboration, excellence, integrity and respect. Headquartered in New York, our team is over 40,000 strong, with more than 50 offices spanning six continents. For information, visit www.exlservice.com .For the past 20 years, EXL has worked as a strategic partner and won awards in its approach to helping its clients solve business challenges such as digital transformation, improving customer experience, streamlining business operations, taking products to market faster, improving corporate finance, building models to become compliant more quickly with new regulations, turning volumes of data into business opportunities, creating new channels for growth and better adapting to change. The business operates within four business units: Insurance, Health, Analytics, and Emerging businesses.Required SkillsPython, Pyspark/Spark, SQL, Data Lake (ON Prem or cloud) AWS, Snowflake.Job SummaryThe Data Engineer’s role is to play a lead role in developing a high-performance data platform, integrating data from a variety of internal and external sources, in order to support data and analytics activities. This is a technical role that involves defining changes to the warehouse data model and building scalable and efficient processes to populate or modify warehouse data. The successful candidate will have hands-on data processing and data modeling experience in cloud and on-prem environments.Responsibilities  Be a technical lead in the development of high-volume platforms to drive decisions and have a tangible beneficial impact on our clients and on business results.  Design and implement efficient data pipelines (ETLs) in order to integrate data from a variety of sources into Data Warehouse.  Design and implement data model changes that align with warehouse standards.  Design and implement backfill or other warehouse data management processes.  Develop and execute testing strategies to ensure high quality warehouse data.  Provide documentation, training, and consulting for data warehouse users.  Perform requirement and data analysis in order to support warehouse project definition.  Provide input and feedback to support continuous improvement in team processes.  Experience in working both, On-Prem and Cloud (AWS preferred).  Responsible for leading the team onsite and off shore with technical leadership and guidance. 
Qualifications  5+ years in a Data Engineering role  7+ years hands on experience with SQL:  Ability to write/ interpret SQL and Complex Joins/ Queries  Execution plan and SQL optimization (Oracle SQL Profiler)  o 3+ years coding experience (Python and/or PySpark).  o 3+ years hands on experience with big data and cloud technologies (Snowflake, EMR, Redshift, or similar technologies) is highly preferred  Schema design and architecture on snowflake  Architecture and design experience with AWS cloud  AWS services expertise: S3, RDS, EC2, ETL services (Glue, Kinesis)  Consumption layer design experience for reporting and dashboard  Expert level understanding and experience of ETL fundamentals and building efficient data pipelines.  3+ years at least - Enterprise GitHub – branch, release, DevOps, CI/CD pipeline.  Team player, Strong communication and collaboration skills.  Experience with Agile methodologies.  Master’s Degree (or a B.S. degree with relevant industry experience) in math, statistics, computer science, or equivalent technical field. 
EEO/Minorities/Females/Vets/Disabilities To view our total rewards offered click here —>  https://www.exlservice.com/us-careers-and-benefits Base Salary Range Disclaimer:  The base salary range represents the low and high end of the EXL base salary range for this position. Actual salaries will vary depending on factors including but not limited to: location and experience. The base salary range listed is just one component of EXL's total compensation package for employees. Other rewards may include bonuses, as well as a Paid Time Off policy, and many region specific benefits.Please also note that the data shared through the job application will be stored and processed by EXL in accordance with the EXL Privacy Policy.Application & Interview Impersonation Warning – Purposely impersonating another individual when applying and / or participating in an interview in order to obtain employment with EXL Service Holdings, Inc. (the “Company”) for yourself or for the other individual is a crime. We have implemented measures to deter and to uncover such unlawful conduct. If the Company identifies such fraudulent conduct, it will result in, as applicable, the application being rejected, an offer (if made) being rescinded, or termination of employment as well as possible legal action against the impersonator(s).EXL may use artificial intelligence to create insights on how your candidate information matches the requirements of the job for which you applied. While AI may be used in the recruiting process, all final decisions in the recruiting and hiring process will be taken by the recruiting and hiring teams after considering a candidate’s full profile. As a candidate, you can choose to opt out of this artificial intelligence screening process. Your decision to opt out will not negatively impact your opportunity for employment with EXL."
(5602) Data Engineer,"Merit321, Launching Careers","Columbia, MD (Hybrid)",https://www.linkedin.com/jobs/view/3700017595/?eBP=JOB_SEARCH_ORGANIC&refId=Dkum4n5CE4r0Ge8XgQQBEg%3D%3D&trackingId=aulhjudB9rFvgVtKTdPavA%3D%3D&trk=flagship3_search_srp_jobs,3700017595,"About the job
            
 
Position: (5602) Data Engineer Location: Columbia, MD (Hybrid)Clearance: Active Secret Clearance Our client is seeking a talented Data Engineer to support the acquisition of mission-critical and mission-support data sets. The preferred candidate will have a background in supporting cyber and/or network-related missions within military space, as either a developer, analyst, or engineer. Work is performed mostly on customer site in Columbia, MD with some flexibility to work from home.Essential Job Responsibilities The ideal candidate will have experience with big data systems, complex structured and unstructured data sets, and have supported government data acquisition, analysis, and/or sharing efforts in the past. To excel in the position, the candidate shall have a strong attention to detail, be able to understand technical complexities, and have the willingness to learn and adapt to the situation. The candidate will work both independently and as part of a large team to accomplish client objectives. 
Minimum Qualifications Security Clearance - Must have a current Secret level security clearance and be willing to get PLACEMENT MANAGER/SCI and CI Poly and therefore all candidates must be a U.S. Citizen. 5 years’ experience as a developer, analyst, or engineer with a Bachelors in related field; OR 3 years relevant experience with Masters in related field; OR High School Diploma or equivalent and 9 years relevant experience.Experience with programming languages such as Python and Java.Proficiency with acquisition and understanding of network data and the associated metadata.Fluency with data extraction, translation, and loading including data prep and labeling to enable data analytics.Experience with Kibana and Elasticsearch.Familiarity with various log formats such as JSON, XML, and others.Experience with data flow, management, and storage solutions (i.e. Kafka, NiFi, and AWS S3 and SQS solutions).Ability to decompose technical problems and troubleshoot system and dataflow issues.Must have a Security+ or similar certification or the ability to obtain it immediately. Must be able to work on customer site in Columbia, MD at least 4 days a week. (Subject to change).
Desired Skills (Optional) Experience with NOSQL databases such as Accumulo desired.Prior Experience supporting cyber and/or network security operations within a large enterprise, as either an analyst, engineer, architect, or developer.
EEOIt is the policy of Merit321 to provide equal opportunity in recruiting, hiring, training, and promoting individuals in all job categories without regard to race, color, religion, national origin, gender, age, disability, genetic information, veteran status, sexual orientation, gender identity, or any other protected class or category as may be defined by federal, state, or local laws or regulations."
Sr. Data Engineer,"Steampunk, Inc.","McLean, VA (Hybrid)",https://www.linkedin.com/jobs/view/3770815197/?eBP=JOB_SEARCH_ORGANIC&refId=Dkum4n5CE4r0Ge8XgQQBEg%3D%3D&trackingId=U%2BKfuAF9ViDo%2BKqkmBEWjw%3D%3D&trk=flagship3_search_srp_jobs,3770815197,"About the job
            
 
In today’s rapidly evolving technology landscape, an organization’s data has never been a more important aspect in achieving mission and business goals. Our data exploitation experts work with our clients to support their mission and business goals by creating and executing a comprehensive data strategy using the best technology and techniques, given the challenge.At Steampunk, our goal is to build and execute a data strategy for our clients to coordinate data collection and generation, to align the organization and its data assets in support of the mission, and ultimately to realize mission goals with the strongest effectiveness possible.For our clients, data is a strategic asset. They are looking to become a facts-based, data-driven, customer-focused organization. To help realize this goal, they are leveraging visual analytics platforms to analyze, visualize, and share information. At steampunk you will design and develop solutions to high-impact, complex data problems, working with the best and data practitioners around.We are looking for seasoned Senior Data Engineer to work with our team and our clients to develop enterprise grade data platforms, services, and pipelines. We are looking for more than just a ""Senior Data Engineer"", but a technologist with excellent communication and customer service skills and a passion for data and problem solving. Lead and architect migration of data environments with performance and reliability.Assess and understand the ETL jobs, workflows, BI tools, and reportsAddress technical inquiries concerning customization, integration, enterprise architecture and general feature / functionality of data productsExperience in crafting database / data warehouse solutions in cloud (Preferably AWS. Alternatively Azure, GCP).Key must have skill sets – Python, AWSSupport an Agile software development lifecycleYou will contribute to the growth of our Data Exploitation Practice!
US Citizen OnlyAbility to hold a position of public trust with the US government.5-7 years industry experience coding commercial software and a passion for solving complex problems. 5-7 years direct experience in Data Engineering with experience in tools such as Big data tools Hadoop, Spark, Kafka, etc.Relational SQL and NoSQL databases, including Postgres and Cassandra.Data pipeline and workflow management tools Azkaban, Luigi, Airflow, etc.AWS cloud services EC2, EMR, RDS, Redshift (or Azure equivalents)Data streaming systems Storm, Spark-Streaming, etc.Search tools Solr, Lucene, ElasticsearchObject-oriented/object function scripting languages Python, Java, C++, Scala, etc.
Advanced working SQL knowledge and experience working with relational databases, query authoring and optimization (SQL) as well as working familiarity with a variety of databases.Experience with message queuing, stream processing, and highly scalable ‘big data’ data stores.Experience manipulating, processing, and extracting value from large, disconnected datasets.Experience manipulating structured and unstructured data for analysisExperience constructing complex queries to analyze results using databases or in a data processing development environmentExperience with data modeling tools and processExperience architecting data systems (transactional and warehouses)Experience aggregating results and/or compiling information for reporting from multiple datasetsExperience working in an Agile environment Experience supporting project teams of developers and data scientists who build web-based interfaces, dashboards, reports, and analytics/machine learning models
Steampunk is a Change Agent in the Federal contracting industry, bringing new thinking to clients in the Homeland, Federal Civilian, Health and DoD sectors. Through our Human-Centered delivery methodology, we are fundamentally changing the expectations our Federal clients have for true shared accountability in solving their toughest mission challenges. As an employee owned company, we focus on investing in our employees to enable them to do the greatest work of their careers – and rewarding them for outstanding contributions to our growth. If you want to learn more about our story, visit http//www.steampunk.com.We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law. Steampunk participates in the E-Verify program."
"Data Engineer, Data Platform",Grammarly,"Illinois, United States (Hybrid)",https://www.linkedin.com/jobs/view/3656893890/?eBP=JOB_SEARCH_ORGANIC&refId=Dkum4n5CE4r0Ge8XgQQBEg%3D%3D&trackingId=hvknkCphJPJFeQUbPbh4yQ%3D%3D&trk=flagship3_search_srp_jobs,3656893890,"About the job
            
 
Grammarly is excited to offer a remote-first hybrid working model. Team members work primarily remotely in the United States, Canada, Ukraine, Germany, or Poland. Certain roles have specific location requirements to facilitate collaboration at a particular Grammarly hub.All roles have an in-person component: Conditions permitting, teams meet 2–4 weeks every quarter at one of Grammarly’s hubs in San Francisco, Kyiv, New York, Vancouver, and Berlin, or in a workspace in Kraków. This flexible approach gives team members the best of both worlds: plenty of focus time along with in-person collaboration that fosters trust and unlocks creativity.Grammarly team members in this role must be based in the United States or Canada, and they must be able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub(s) where the team is based.The opportunity Grammarly is the world’s leading AI writing assistance company trusted by over 30 million people and 70,000 professional teams every day. From instantly creating a first draft to perfecting every message, Grammarly’s product offerings help people at 96% of the Fortune 500 get their point across—and get results. Grammarly has been profitable for over a decade because we’ve stayed true to our values and built an enterprise-grade product that’s secure, reliable, and helps people do their best work—without selling their data. We’re proud to be one of Inc.’s best workplaces, a Glassdoor Best Place to Work, one of TIME’s 100 Most Influential Companies, and one of Fast Company’s Most Innovative Companies in AI.To achieve our ambitious goals, we’re looking for a Data Engineer to join our Data Engineering Platform team. This person will build highly automated, low latency core datasets that will help data engineers and end users across Grammarly to work with analytical data at scale.Grammarly’s engineers and researchers have the freedom to innovate and uncover breakthroughs—and, in turn, influence our product roadmap. The complexity of our technical challenges is growing rapidly as we scale our interfaces, algorithms, and infrastructure. Read more about our stack or hear from our team on our technical blog.Your impactAs a Data Engineer on our Data Engineering Platform team, you will: Drive improvements to make our analytics effortless by creating and adjusting core data models and storage structures, all while understanding the needs of our users. Make analytical data and metrics usable within a few minutes of real world events occuring, and build streaming processes for the output derived events and aggregate data.Model structure, storage, and access of data at very high volumes for our data lakehouse.Improve developer productivity and self-serve solutions by contributing components to our stream data processing framework(s).Own data engineering's infrastructure-as-code for provisioning services that allow our engineers to deploy mature software installations within a few hours.Build a world-class process that will allow our systems to scale.Mentor other back-end engineers on the team and help them grow.Build and contribute to AWS high-scale distributed systems on the back-end.
We’re Looking For Someone Who Embodies our EAGER values—is ethical, adaptable, gritty, empathetic, and remarkable.Is inspired by our MOVE principles, which are the blueprint for how things get done at Grammarly: move fast and learn faster, obsess about creating customer value, value impact over activity, and embrace healthy disagreement rooted in trust.Is able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub where the team is based.Has experience with Python, Scala, or Java.Has experience with designing database objects and writing relational queriesHas experience designing and standing up APIs and services.Has experience with system design and building internal tools.Has experience handling applications that work with data from data lakes.Has at least some experience building internal Admin sites.Has good knowledge of and at least some experience with AWS (or, alternatively, has deep expertise in Azure or GCE and is willing to learn AWS in a short time frame).Can knowledgeably choose an open source or third-party service to accomplish what they need or, alternatively, can devise a quick and simple solution on their own.
Support for you, professionally and personally Professional growth: We believe that autonomy and trust are key to empowering our team members to do their best, most innovative work in a way that aligns with their interests, talents, and well-being. We support professional development and advancement with training, coaching, and regular feedback.A connected team: Grammarly builds a product that helps people connect, and we apply this mindset to our own team. Our remote-first hybrid model enables a highly collaborative culture supported by our EAGER (ethical, adaptable, gritty, empathetic, and remarkable) values. We work to foster belonging among team members in a variety of ways. This includes our employee resource groups, Grammarly Circles, which promote connection among those with shared identities, such as BIPOC and LGBTQIA+ team members, women, and parents. We also celebrate our colleagues and accomplishments with global, local, and team-specific programs. 
Compensation And BenefitsGrammarly offers all team members competitive pay along with a benefits package encompassing the following and more: Excellent health care (including a wide range of medical, dental, vision, mental health, and fertility benefits)Disability and life insurance options401(k) and RRSP matching Paid parental leaveTwenty days of paid time off per year, eleven days of paid holidays per year, and unlimited sick days Home office stipendsCaregiver and pet care stipendsWellness stipendsAdmission discountsLearning and development opportunities
Grammarly takes a market-based approach to compensation, which means base pay may vary depending on your location. Our US and Canada locations are categorized into compensation zones based on each geographic region’s cost of labor index. For more information about our compensation zones and locations where we currently support employment, please refer to this page. If a location of interest is not listed, please speak with a recruiter for additional information.Base pay may vary considerably depending on job-related knowledge, skills, and experience. The expected salary ranges for this position are outlined below by compensation zone and may be modified in the future.United StatesZone 1: $167,000 - $242,000/year (USD)Zone 2: $150,000 – $218,000/year (USD)Zone 3: $142,000 – $206,000/year (USD)Zone 4: $134,000 – $194,000/year (USD)We encourage you to applyAt Grammarly, we value our differences, and we encourage all—especially those whose identities are traditionally underrepresented in tech organizations—to apply. We do not discriminate on the basis of race, religion, color, gender expression or identity, sexual orientation, ancestry, national origin, citizenship, age, marital status, veteran status, disability status, political belief, or any other characteristic protected by law. Grammarly is an equal opportunity employer and a participant in the US federal E-Verify program (US). We also abide by the Employment Equity Act (Canada).Please note that EEOC is optional and specific to US-based candidates.#NAAll team members meeting in person for official Grammarly business or working from a hub location are strongly encouraged to be vaccinated against COVID-19."
IT Data Engineer | 23-00961,ESPO Corporation,"Lake Forest, IL (Hybrid)",https://www.linkedin.com/jobs/view/3319254144/?eBP=JOB_SEARCH_ORGANIC&refId=Dkum4n5CE4r0Ge8XgQQBEg%3D%3D&trackingId=fADMhpH23p90c5%2BCELoj9A%3D%3D&trk=flagship3_search_srp_jobs,3319254144,"About the job
            
 
Job Title: IT Data EngineerLocation: Lake Forest, IL (Hybrid 3 days in office - Tues thru Thurs, Remote Mon & Fri)Job Type and Duration: Permanent salaried role, full timeGlobal consumer goods manufacturer looking to add an IT Data Engineer to their team in their corporate offices - hybrid role! Great team culture and bonuses!The position reports to the Director of Data Warehouse and Data LakeResponsibilities: Build data pipelines: Managed data pipelines consist of a series of stages through which data flows (for example, from data sources or endpoints of acquisition to integration to consumption for specific use cases). These data pipelines must be created, maintained and optimized as workloads move from development to production for specific use cases. Architecting, creating and maintaining data pipelines will be the primary responsibility of the data engineerData Integration and Data Lake: Bring data from disparate systems and applications into a centralized data lake on the Synapse platform. Design and develop robust ETL processes to ensure smooth and efficient data extraction, transformation, and loading.Drive Automation through effective metadata management: The data engineer will be responsible for using innovative and modern tools, techniques and architectures to partially or completely automate the most-common, repeatable and tedious data preparation and integration tasks in order to minimize manual and error-prone processes and improve productivity. The data engineer will also need to assist with renovating the data management infrastructure to drive automation in data integration and management and automate data pipelines to ensure the timely and accurate flow of data from source systems to the data lake. Implement scheduling, monitoring, and error handling mechanisms to maintain data pipeline reliability.Data Visualization: Collaborate with data visualization teams to ensure that the data stored in the data lake is readily available for dashboard development and reporting purposes. Optimize data structures and formats to facilitate efficient data retrieval for visualization tools such as Tableau, Power BI, and BOBJ.Data Cataloging and Documentation: Develop and maintain data cataloging processes to enable easy discovery and understanding of available data assets. Document data sources, data definitions, and data lineage to ensure transparency and data governance.  Synapse and  Purview experience highly desired.
QUALIFICATIONS Bachelor's degree and 5+ years of management experience; or, an advanced degree and 3+ years of management experience with Data Integration and platform build; Management experience a plusProven experience in ETL practices using tools such as SAP Data Services, SAP SLT, etc.Deep technical knowledge of data architecture, cloud platforms, modern data ETL tools and languages, and data governanceExperience with Azure Data Factory, and Synapse is mustStrong knowledge of data governance frameworks, practices, and industry standards.Familiarity with the consumer packaging industry is a plus.Experience working with multiple business units and applications such as SAP, Blue Yonder, One Stream, etc.At least three years of experience working in cross-functional teams and collaborating with business stakeholders in support of a departmental and/or multi-departmental data management and analytics initiativeProficiency in data visualization tools such as Tableau, Power BI, and BOBJ.Excellent communication and interpersonal skills to collaborate effectively with business stakeholders.Ability to adapt to a fast-paced and evolving environment.
ESPO CorporationWillowbrook, IL 60527(630) 789-2525View all open jobs at: www.espocorp.com/jobsLeaders in Technical Recruiting & Staffing since 1965We are an Equal Opportunity Employer and value the benefits of diversity in our workforce. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity and expression, national origin, disability, protected Veteran status or any other attribute or protected characteristic by law. If you need assistance applying please contact us at 630-789-2525."
Data Mining and Analytics Engineer (Junior),ICF,"Pensacola, FL (Hybrid)",https://www.linkedin.com/jobs/view/3726716396/?eBP=JOB_SEARCH_ORGANIC&refId=Dkum4n5CE4r0Ge8XgQQBEg%3D%3D&trackingId=D2ha%2BxPnjcNHFPbrZBL1Qw%3D%3D&trk=flagship3_search_srp_jobs,3726716396,"About the job
            
 
ICF International seeks a Junior Data Mining and Analytics Engineer to support the research and development of new cyber analytic capabilities that will help the US protect and defend its networks and critical information systems. The successful cleared candidate will act as a Data Mining and Analytics Engineer to support a large federal cyber security analytic program. Your work will contribute to the knowledge of how cyber-attacks work, how vulnerabilities are exploited, and the way hostile cyber actors operate. Utilize your skills to help experiment and prototype future cyber capabilities for implementation at large-scale.As the Junior Data Mining and Analytics Engineer, your skillset will create useful and actionable insight for the customer through the development of analytic solutions (hardware, analytics, tools, techniques, practices, deployment, standards, performance specifications, etc.) for analytic use cases developed during the performance of this project. You will work closely with the Analytics Research team to identify platform enhancements that support the forward-looking analytics under consideration.The ideal candidate has extensive knowledge of a wide variety of systems and networks to include high-volume/high-availability systems. You are focused on results, a self-starter, and have demonstrated success for using analytics to drive the understanding, growth, and success of the analysis. This is an opportunity to contribute to an important project from its beginning, work with the latest and emerging technologies, and all while building a great career at ICF!This role is primarily telework-based with occasional meetings at client locations (Arlington, VA or Pensacola, FL) or ICF facilities within the National Capital Region.What You Will Be Doing  Perform knowledge elicitation from customer subject matter experts and convert that to build analytic solutions Design, engineer, and optimize sustainment of large-scale distributed computation platforms and supporting environment (ecosystems) for various stakeholders, business owners, and industry partners Oversee the transition of services from third-party vendors to the analytic environment and be responsible for ad hoc and formal end-user training Identify applicable data to perform analytics and create solutions to acquire, transform, and load or correlate data components to and from the analytic environment Develop custom data modeling procedures to assist with data mining, modeling, and production Assess the effectiveness and accuracy of new data sources and data gathering techniques Develop processes and tools to monitor and analyze model performance and data accuracy Interpret and communicate results to non-technical customers
What You Must Have  Active high-level security clearance required as part of client contract requirements Bachelor’s degree in Computer Science, Mathematics, Engineering, or related field US Citizenship required as part of client contract requirements Practical working experience and advanced knowledge of cyber threats, tools, techniques, and processes. Experience in data modeling and working with datasets of all sizes using a variety of data mining and data analysis methods/tools
Preferred Skills/Experience  Master’s degree in Computer Science, Mathematics, Engineering, or related field Interpersonal skills and the ability to communicate effectively with various clients in order to explain and elaborate on technical details Experience in developing analytic tools, processes, and governance for storing, modeling, capturing, and delivering data to the client’s enterprise Experience with computational notebook software such as Zeppelin or Jupyter Experience with the application of visual analytics to computational analytic results Fluency in one or more programming languages (e.g., Python, JavaScript, R, etc.) Experience with database querying like SQL Readiness to collaborate with engineering teams, product teams, and customers to develop prototypes and software products Scaled Agile Framework (SAFe) experience Amazon Web Services (AWS) Certified Cloud Practitioner or higher desired CompTIA Security+ or higher cybersecurity certification preferred
#cybsr1Working at ICFICF is a global advisory and technology services provider, but we’re not your typical consultants. We combine unmatched expertise with cutting-edge technology to help clients solve their most complex challenges, navigate change, and shape the future.We can only solve the world's toughest challenges by building an inclusive workplace that allows everyone to thrive. We are an equal opportunity employer, committed to hiring regardless of any protected characteristic, such as race, ethnicity, national origin, color, sex, gender identity/expression, sexual orientation, religion, age, disability status, or military/veteran status. Together, our employees are empowered to share their expertise and collaborate with others to achieve personal and professional goals. For more information, please read our EEO & AA policy.Reasonable Accommodations are available, including, but not limited to, for disabled veterans, individuals with disabilities, and individuals with sincerely held religious beliefs, in all phases of the application and employment process. To request an accommodation please email icfcareercenter@icf.com and we will be happy to assist. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. Read more about non-discrimination: Know Your Rights and Pay Transparency Statement.Pay Range - There are multiple factors that are considered in determining final pay for a position, including, but not limited to, relevant work experience, skills, certifications and competencies that align to the specified role, geographic location, education and certifications as well as contract provisions regarding labor categories that are specific to the position. The pay range for this position is:$64,372.00 - $109,432.00Arlington, VA (VA31)"
Senior Data Engineer,PPL Corporation,"Louisville, KY (Hybrid)",https://www.linkedin.com/jobs/view/3778931368/?eBP=JOB_SEARCH_ORGANIC&refId=Dkum4n5CE4r0Ge8XgQQBEg%3D%3D&trackingId=d3VBQGrX4sIgut5q8EXATw%3D%3D&trk=flagship3_search_srp_jobs,3778931368,"About the job
            
 
Company Summary StatementAs one of the largest investor-owned utility companies in the United States, PPL Corporation (NYSE: PPL), is committed to creating long-term, sustainable value for our 3.5 million customers, our shareowners and the communities we serve. Our high-performing regulated utilities — PPL Electric Utilities, Louisville Gas and Electric, Kentucky Utilities and Rhode Island Energy — provide an outstanding experience for our customers, consistently ranking among the best utilities in the nation. PPL’s companies are also addressing challenges head-on by investing in new infrastructure and technology that is creating a smarter, more reliable and resilient energy grid. We are committed to doing our part to advance a cleaner energy future and drive innovation that enables us to achieve net-zero carbon emissions by 2050 while maintaining energy reliability and affordability for the customers and communities we serve. PPL is a positive force in the cities and towns where we do business, providing support for programs and organizations that empower the success of future generations by helping to build and maintain strong, diverse communities today.OverviewData Analysis: The Data Engineer designs and implements processes and layouts for complex, large-scale data sets used for modeling, data mining, and research purposes. This person is expected to maintain a data knowledgebase (e.g., data dictionary).Business Insights: The Data Engineer empowers business partners to turn information into action by building tools, developing protocols, and defining metrics for forward-looking data collection and analysis. The Senior level performs assigned tasks under minimal guidance from supervisor.ResponsibilitiesPerforms a variety of complex assignments (may lead some projects) and analyzes and solves complex problems in the below areas. Data Design Assist data architect in translating business requirements into design specificationsCreate source mappings, business definitions, measures, dimensions, visualization designs, test plans, and test casesConduct data studies and data discovery around new data sources or new uses for existing data sourcesInterpret and analyze data and generate reports as needed to understand flow of dataApply quantitative methods to derive actionable insights and outcomes from data
Data Management: Conduct data studies and data discovery around new data sources or new uses for existing data sourcesAssist in documenting meta data information including data types, lengths, and conversion functionsAssist in analyzing and researching new and existing products, procedures, and/or workflows for data managementExecute activities pertaining to end-to-end data set identification, development, and implementation according to the guidance from Data ScientistsSupport data collection, integration and retention requirements based on the input collected with the businessGather and process data from the Transmission, Distribution, Customer Service, and EU Service functions, and transform them into data productsImplement statistical data quality procedures around new data sources
Data Integration: Develop and modify functions, programs, routines, and stored procedures to export, transform, and load dataResolve integration and data mapping issues and discrepancies as objectives changeAssist in resolving data quality problems through the appropriate choice of error detection and correction, process control and improvement, or process design strategies collaborating with subject matter experts (SMEs)
Master Data Management: Execute on all aspects of MDM architecture (MDM data model, SAP MDM, MDM tools, MDM data integration, master data quality, MDM operations, master data security, )Maintain data warehouse performance by identifying and resolving data conflictsAdhere to data / information quality practicesConduct data cleaning to rid the system of old, unused data, or duplicate data for better management and quicker accessEnsures quality of master data in key systems, as well as development and documentation of processes with other functional data owners to support ongoing maintenance and data integrity
Data Governance: Adhere to data strategy, policies, controls, and programs to ensure the enterprise data is accurate, complete, secure, and reliable
May be assigned an Electric Utilities emergency and storm role. This is a special assignment that comes into play during storms and other emergencies when the company needs to restore power or respond to other issues affecting customer service. This role may necessitate the need to work after-hours, outside of your normal schedule.The company reserves the right to determine if this position will be assigned to work on-site, remotely, or a combination of both. Assigned work location may change. In the case of remote work, physical presence in the office/on-site may be required to engage in face-to-face interaction and coordination of work among direct reports and co-workers.
Qualifications Bachelor's degree and 5 years of related work experience OR 8 years of related work experienceAwareness of industry leading data quality and data protection management practicesAwareness of data governance practices, business and technology issues related to management of enterprise information assets and approaches related to data protectionAwareness of data related government regulatory requirements and emerging trends and issuesProven ability to prioritize and execute tasksHighly self-motivated and directed with attention to detailEasily adapts to changing circumstancesUnderstands business goals and strategic priorities
Preferred Qualifications: Experience supporting fast-changing business organizations"
Staff Data Engineer,SiriusXM,"Oakland, CA (Hybrid)",https://www.linkedin.com/jobs/view/3690120412/?eBP=JOB_SEARCH_ORGANIC&refId=Dkum4n5CE4r0Ge8XgQQBEg%3D%3D&trackingId=5Rcm4oQhhSq6pSirE1kTEA%3D%3D&trk=flagship3_search_srp_jobs,3690120412,"About the job
            
 
Who We AreSiriusXM and its brands (Pandora, SXM Media, AdsWizz, Simplecast, and SiriusXM Connected Vehicle Services) are leading a new era of audio entertainment and services by delivering the most compelling subscription and ad-supported audio entertainment experience for listeners -- in the car, at home, and anywhere on the go with connected devices. Our vision is to shape the future of audio, where everyone can be effortlessly connected to the voices, stories and music they love wherever they are. This is the place where a diverse group of emerging talent and legends alike come to share authentic and purposeful songs, stories, sounds and insights through some of the best programming and technology in the world. Our critically-acclaimed, industry-leading audio entertainment encompasses music, sports, comedy, news, talk, live events, and podcasting. No matter their individual role, each of our employees plays a vital part in bringing SiriusXM’s vision to life every day.SiriusXM is the leading audio entertainment company in North America, and the premier programmer and platform for subscription and digital advertising-supported audio products. SiriusXM’s platforms collectively reach approximately 150million listeners, the largest digital audio audience across paid and free tiers in North America, and deliver music, sports, talk, news, comedy, entertainment and podcasts. Pandora, a subsidiary of SiriusXM, is the largest ad-supported audio entertainment streaming service in the U.S. SiriusXM's subsidiaries Simplecast and AdsWizz make it a leader in podcast hosting, production, distribution, analytics and monetization. The Company’s advertising sales organization, which operates as SXM Media, leverages its scale, cross-platform sales organization and ad tech capabilities to deliver results for audio creators and advertisers. SiriusXM, through Sirius XM Canada Holdings, Inc., also offers satellite radio and audio entertainment in Canada. In addition to its audio entertainment businesses, SiriusXM offers connected vehicle services to automakers.How You’ll Make An ImpactSiriusXM is seeking a highly motivated and experienced Staff Data Engineer to join our Data Org. In this role, you will be responsible for designing and developing data processing pipelines, data warehousing solutions, and real-time data ingestion systems. You will work closely with data scientists, analysts, and other stakeholders to develop solutions that support the business needs of SiriusXM, Pandora, and AdsWizz.What You’ll Do Develop and maintain high-performance, scalable, and fault-tolerant data processing systems using Spark, Flink, Druid, or the like.Collaborate with cross-functional teams to integrate data from multiple sources and ensure data accuracy and consistency.Write complex SQL queries to support data analysis and reporting.Develop and maintain data models and ETL processes.Optimize data pipelines and system performance to ensure efficient data processing and analysis.Build and maintain data infrastructure, including data storage and data access systems.Stay up-to-date with the latest developments in data engineering technologies and best practices.
What You’ll Need Bachelor's degree in Computer Science, Engineering, or a related field7+ years of experience in Data EngineeringExpertise in Spark and PythonExperience with Scala is a bonusStrong understanding of distributed systems, data modeling, and ETL processesHands-on experience with cloud-based data warehousing solutions (e.g., AWS Redshift, Snowflake, Teradata)Excellent problem-solving skills and attention to detailStrong written and verbal communication skillsAbility to work independently and as part of a team
At SiriusXM, we carefully consider a wide range of factors when determining compensation, including your background and experience. These considerations can cause your compensation to vary. We expect the base salary for this position to be in the range of $120,000 to $166,000 and will depend on your skills, qualifications, and experience. Additionally, this role might be eligible for discretionary short-term and long-term incentives. We encourage all interested candidates to apply.Our goal at SiriusXM is to provide and maintain a work environment that fosters mutual respect, professionalism and cooperation. SiriusXM is an equal opportunity employer that does not discriminate on the basis of actual or perceived race, creed, color, religion, national origin, ancestry, alienage or citizenship status, age, disability or handicap, sex, gender identity, marital status, familial status, veteran status, sexual orientation or any other characteristic protected by applicable federal, state or local laws.The requirements and duties described above may be modified or waived by the Company in its sole discretion without notice.R-2023-08-59"
Data Engineer Lead,"Princeton IT Services, Inc","Raleigh, NC (Hybrid)",https://www.linkedin.com/jobs/view/3687326765/?eBP=JOB_SEARCH_ORGANIC&refId=Dkum4n5CE4r0Ge8XgQQBEg%3D%3D&trackingId=w3xskO0OK7oUVr8t0A64dg%3D%3D&trk=flagship3_search_srp_jobs,3687326765,"About the job
            
 
Position: Data Engineer LeadLocation: Raleigh, NC or Boston, MAJob Length: Long termPosition Type: C2C/W2Qualifications 9+ years Experience in Alation, Collibra, Snowflake9+ years Experience in Java , Spring boot , spark , Scala.Stays current with technology trends in order to provide best options for solutions Self-directed and is able to decompose work into problem sets for self and project team.Equally capable working as part of a team or independently.
Responsibilities Designs, develops, tests, and delivers software solutions using one or more commercial languages as well as, open-source tools. Data processing and analysis using Snowflake.Data management and Stewardship using Collibra.AlationData warehouse using Data Pipelines along with data transformation and optimization.Comfortable working within a culture of accountability and experimentationWork closely with internal stakeholders to implement solutions and generate reporting to meet business goals.Demonstrate critical thinking for potential roadblocks; comprehends bigger picture of the business and effectively communicates these issues to greater news digital organization.Collaborates with reporting teams and business owners to turn data into actionable business insights using self-service analytics and reporting tools.
Skills Required :Alation, Collibra, SnowflakeWe offer a competitive salary, benefits, and a dynamic work environment. If you have a passion for technology and are looking for an exciting opportunity to work with a talented team, please apply today."
"Data Engineer, Data Platform",Grammarly,"Arizona, United States (Hybrid)",https://www.linkedin.com/jobs/view/3689964226/?eBP=JOB_SEARCH_ORGANIC&refId=Dkum4n5CE4r0Ge8XgQQBEg%3D%3D&trackingId=y%2B4MHzE739k%2BRMSeFiBuzg%3D%3D&trk=flagship3_search_srp_jobs,3689964226,"About the job
            
 
Grammarly is excited to offer a remote-first hybrid working model. Team members work primarily remotely in the United States, Canada, Ukraine, Germany, or Poland. Certain roles have specific location requirements to facilitate collaboration at a particular Grammarly hub.All roles have an in-person component: Conditions permitting, teams meet 2–4 weeks every quarter at one of Grammarly’s hubs in San Francisco, Kyiv, New York, Vancouver, and Berlin, or in a workspace in Kraków. This flexible approach gives team members the best of both worlds: plenty of focus time along with in-person collaboration that fosters trust and unlocks creativity.Grammarly team members in this role must be based in the United States or Canada, and they must be able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub(s) where the team is based.The opportunity Grammarly is the world’s leading AI writing assistance company trusted by over 30 million people and 70,000 professional teams every day. From instantly creating a first draft to perfecting every message, Grammarly’s product offerings help people at 96% of the Fortune 500 get their point across—and get results. Grammarly has been profitable for over a decade because we’ve stayed true to our values and built an enterprise-grade product that’s secure, reliable, and helps people do their best work—without selling their data. We’re proud to be one of Inc.’s best workplaces, a Glassdoor Best Place to Work, one of TIME’s 100 Most Influential Companies, and one of Fast Company’s Most Innovative Companies in AI.To achieve our ambitious goals, we’re looking for a Data Engineer to join our Data Engineering Platform team. This person will build highly automated, low latency core datasets that will help data engineers and end users across Grammarly to work with analytical data at scale.Grammarly’s engineers and researchers have the freedom to innovate and uncover breakthroughs—and, in turn, influence our product roadmap. The complexity of our technical challenges is growing rapidly as we scale our interfaces, algorithms, and infrastructure. Read more about our stack or hear from our team on our technical blog.Your impactAs a Data Engineer on our Data Engineering Platform team, you will: Drive improvements to make our analytics effortless by creating and adjusting core data models and storage structures, all while understanding the needs of our users. Make analytical data and metrics usable within a few minutes of real world events occuring, and build streaming processes for the output derived events and aggregate data.Model structure, storage, and access of data at very high volumes for our data lakehouse.Improve developer productivity and self-serve solutions by contributing components to our stream data processing framework(s).Own data engineering's infrastructure-as-code for provisioning services that allow our engineers to deploy mature software installations within a few hours.Build a world-class process that will allow our systems to scale.Mentor other back-end engineers on the team and help them grow.Build and contribute to AWS high-scale distributed systems on the back-end.
We’re Looking For Someone Who Embodies our EAGER values—is ethical, adaptable, gritty, empathetic, and remarkable.Is inspired by our MOVE principles, which are the blueprint for how things get done at Grammarly: move fast and learn faster, obsess about creating customer value, value impact over activity, and embrace healthy disagreement rooted in trust.Is able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub where the team is based.Has experience with Python, Scala, or Java.Has experience with designing database objects and writing relational queriesHas experience designing and standing up APIs and services.Has experience with system design and building internal tools.Has experience handling applications that work with data from data lakes.Has at least some experience building internal Admin sites.Has good knowledge of and at least some experience with AWS (or, alternatively, has deep expertise in Azure or GCE and is willing to learn AWS in a short time frame).Can knowledgeably choose an open source or third-party service to accomplish what they need or, alternatively, can devise a quick and simple solution on their own.
Support for you, professionally and personally Professional growth: We believe that autonomy and trust are key to empowering our team members to do their best, most innovative work in a way that aligns with their interests, talents, and well-being. We support professional development and advancement with training, coaching, and regular feedback.A connected team: Grammarly builds a product that helps people connect, and we apply this mindset to our own team. Our remote-first hybrid model enables a highly collaborative culture supported by our EAGER (ethical, adaptable, gritty, empathetic, and remarkable) values. We work to foster belonging among team members in a variety of ways. This includes our employee resource groups, Grammarly Circles, which promote connection among those with shared identities, such as BIPOC and LGBTQIA+ team members, women, and parents. We also celebrate our colleagues and accomplishments with global, local, and team-specific programs. 
Compensation And BenefitsGrammarly offers all team members competitive pay along with a benefits package encompassing the following and more: Excellent health care (including a wide range of medical, dental, vision, mental health, and fertility benefits)Disability and life insurance options401(k) and RRSP matching Paid parental leaveTwenty days of paid time off per year, eleven days of paid holidays per year, and unlimited sick days Home office stipendsCaregiver and pet care stipendsWellness stipendsAdmission discountsLearning and development opportunities
Grammarly takes a market-based approach to compensation, which means base pay may vary depending on your location. Our US and Canada locations are categorized into compensation zones based on each geographic region’s cost of labor index. For more information about our compensation zones and locations where we currently support employment, please refer to this page. If a location of interest is not listed, please speak with a recruiter for additional information.Base pay may vary considerably depending on job-related knowledge, skills, and experience. The expected salary ranges for this position are outlined below by compensation zone and may be modified in the future.United StatesZone 1: $167,000 - $242,000/year (USD)Zone 2: $150,000 – $218,000/year (USD)Zone 3: $142,000 – $206,000/year (USD)Zone 4: $134,000 – $194,000/year (USD)We encourage you to applyAt Grammarly, we value our differences, and we encourage all—especially those whose identities are traditionally underrepresented in tech organizations—to apply. We do not discriminate on the basis of race, religion, color, gender expression or identity, sexual orientation, ancestry, national origin, citizenship, age, marital status, veteran status, disability status, political belief, or any other characteristic protected by law. Grammarly is an equal opportunity employer and a participant in the US federal E-Verify program (US). We also abide by the Employment Equity Act (Canada).Please note that EEOC is optional and specific to US-based candidates.#NAAll team members meeting in person for official Grammarly business or working from a hub location are strongly encouraged to be vaccinated against COVID-19."
Data Engineer IV,Navy Federal Credit Union,"Vienna, VA (Hybrid)",https://www.linkedin.com/jobs/view/3768699032/?eBP=JOB_SEARCH_ORGANIC&refId=Dkum4n5CE4r0Ge8XgQQBEg%3D%3D&trackingId=gl1IPpfCWwXwzPHcZcn6dQ%3D%3D&trk=flagship3_search_srp_jobs,3768699032,"About the job
            
 
OverviewDevelop strategies for data acquisition, archive recovery, and database implementation. Responsible for designing, building, integrating data from various resources, and managing big data. Develop data consumption patterns to share data with internal/external channels, while ensuring they are easily accessible, work smoothly, with the goal of optimizing the performance of Navy Federal’s big data ecosystem. Recognized as an expert with a specialized depth and/or breadth of expertise in discipline. Solves highly complex problems; takes a broad perspective to identify solutions. Leads functional teams or projects. Works independently.Responsibilities Define and build data integration processes to be used across the organizationBuild channel contracts and data consumption patterns for customer facing (On-line/Mobile) channelsAnalyze and validate data sharing requirements within and outside data partners Recognize potential issues and risks during the project implementation and suggest mitigation strategiesCommunicate and own the processes related to contracts and data consumption patternsExpert and key point of contact between the operational data hubs and the channel contracts for On-line/MobileApply engineering principles into the design and enhancement of new and existing data management systemsCoach and mentor project team members in carrying out project implementation activities Work directly with business leadership to understand data requirements; propose and develop solutions that enable effective decision-making and drives business objectives Prepare advanced project implementation plans which highlight major milestones and deliverables, leveraging standard methods and work planning tools Lead the preparation of high-quality project deliverables that are valued by the business and present them in such a manner that they are easily understood by project stakeholdersEnsure the security and integrity of system and product solutions including compliance with Navy Federal, industry engineering and Information Security principles and practicesPresent clear, organized and concise information to all audiences through a variety of media to enable effective business decisionsPerform other duties as assigned
Qualifications Master’s degree in Information Systems, Computer Science, Engineering, or related field, or the equivalent combination of education, training and experienceAdvanced skills in systems and application integration in a large, distributed architecture environmentsProficient skill level in .Net/C#, Python, Agile Frameworks (SAFE), Microsoft Databricks, Azure Data FactoryProficient skills in developing and operationalizing various data distribution patterns like, APIs, event based, pub/sub models Proficient in Data Architecture, Web Services, REST APIs, Event and Pub/Sub messaging architectureProficient in Mobile and Web application technologiesAbility to understand the business problem and determine what aspects of it require optimization; articulate those aspects in a clear and concise mannerProficient skills in understanding SQL and NoSQL and JSON structure Ability to understand other projects or functional areas to consolidate analytical and operational needs and processesDemonstrates change management and/or excellent communication skills Working knowledge of various data structures and the ability to extract data from various data sources Understands the concepts and application of data mapping and building requirementsUnderstands data models, large datasets, business/technical requirementsSkilled in managing the process between updating and maintaining data source systems and implementing data related requirements
Desired Qualifications Knowledge of Navy Federal Credit Union instructions, standards, and procedures
Hours: Monday - Friday, 8:00AM - 4:30PMLocation: 820 Follin Lane, Vienna, VA 22180 | 5550 Heritage Oaks Dr. Pensacola, FL 32526 | 141 Security Dr. Winchester, VA 22602About UsYou have goals, dreams, hobbies, and things you're passionate about—what's important to you is important to us. We're looking for people who not only want to do meaningful, challenging work, keep their skills sharp and move ahead, but who also take time for the things that matter to them—friends, family, and passions. And we're looking for team members who are passionate about our mission—making a difference in military members' and their families' lives. Together, we can make it happen. Don't take our word for it:  Military Times 2022 Best for Vets Employers WayUp Top 100 Internship Programs Forbes® 2022 The Best Employers for New Grads Fortune Best Workplaces for Women Fortune 100 Best Companies to Work For® Computerworld® Best Places to Work in IT Ripplematch Campus Forward Award - Excellence in Early Career Hiring Fortune Best Place to Work for Financial and Insurance Services
Equal Employment Opportunity: Navy Federal values, celebrates, and enacts diversity in the workplace. Navy Federal takes affirmative action to employ and advance in employment qualified individuals with disabilities, disabled veterans, Armed Forces service medal veterans, recently separated veterans, and other protected veterans. EOE/AA/M/F/Veteran/Disability EOE/AA/M/F/Veteran/DisabilityDisclaimers: Navy Federal reserves the right to fill this role at a higher/lower grade level based on business need. An assessment may be required to compete for this position. Job postings are subject to close early or extend out longer than the anticipated closing date at the hiring team’s discretion based on qualified applicant volume. Navy Federal Credit Union assesses market data to establish salary ranges that enable us to remain competitive. You are paid within the salary range, based on your experience, location and market positionBank Secrecy Act: Remains cognizant of and adheres to Navy Federal policies and procedures, and regulations pertaining to the Bank Secrecy Act."
"Principal Data Engineer, MDM",CarMax,"Richmond, VA (Hybrid)",https://www.linkedin.com/jobs/view/3640459729/?eBP=JOB_SEARCH_ORGANIC&refId=Dkum4n5CE4r0Ge8XgQQBEg%3D%3D&trackingId=PGE7pz6ApUoXY9A6EjJf5Q%3D%3D&trk=flagship3_search_srp_jobs,3640459729,"About the job
            
 
8901 - Corp Office West Crk - 12800 Tuckahoe Creek Parkway, Richmond, Virginia, 23238CarMax, the way your career should be!About This JobCarMax is disrupting the industry by empowering customers to buy a car on their own terms, providing an iconic customer experience. As a Principal Data Engineer on the Carmax Master Data Management team. You will be working with and collaborating with Product Managers, Delivery managers ann the technology team to build data integrations and processes that consolidate, standardize, and enrich vehicle data to provide customers with an iconic experience of buying and selling vehicles.What You Will Do - Essential Responsibilities Partners with Product Manager, Technology Manager and Architect to understand business objectives and outcomesFacilitates discussions between Architect, Developers, and business partners to create new data productsHands-on development of data integrations with the MDM platform and analytical environmentsGuides and mentors the teamLeads technical execution within an agile environment including discovery, analysis, design, development, and testingStays on top of industry trends and best practices to continuously improve what we do and improve customer experience with trusted vehicle dataProvides on-time and accurate estimation of work, ensures a high level of quality of team solutions, and is driven to meet commitments.
What Technologies You'll Be Working WithThis role requires hands-on work in Azure Data Factory, Databricks, Azure DevOps for CI/CD, Reltio, and load testing / automated testing tools.Qualifications And Requirements At least eight years of hands on ETL development experienceAt least four years of hands-on experience developing in Azure Data Factory, Databricks Python and SparkAt least 2 years leading a technical team, providing leadership, design documentation and diagramsAt least 2 years of hands-on experience working with configuration of Azure subscriptions, resource groups, resources and provisioningAt least 2 years leading the end-to-end design and development of ETL integrations to be consumed by the enterprise, including monitoring and production supportExperience with data driven initiatives, with background in MDM or other data management and governance platformsProven ability to learn and fully understand new technology, becoming the SME for the platforms and services your team interacts withSolid understanding of DevOps capabilities such as automated testing, continuous integration, and continuous deliveryExperience working with a Master Data Management platform tool (pref. Reltio)Proven track record of exhibiting strong critical thinking by analyzing facts in order to understand a business request or requirement thoroughly.Proven ability to mentor and develop others. Experience positively influencing team norms, culture, and technical vision.Experience with agile methodologiesVery strong communication both written and verbal
Work Location and Arrangement: This role will be based out of one of the following locations and have a Hybrid work arrangement: Richmond, VA Technology Innovation CenterDallas, TX Technology HubAtlanta, GA CarMax Auto Finance Office
Work Authorization : Candidates must be legally authorized to work for any U.S. employer on a full-time basis. Sponsorship will be considered for this specific role.About CarMaxCarMax disrupted the auto industry by delivering the honest, transparent and high-integrity experience customers want and deserve. This innovative thinking around the way cars are bought and sold has helped us become the nation's largest retailer of used cars, with over 200 locations nationwide.Our amazing team of more than 25,000 associates work together to deliver iconic customer experiences. Along the way, we help every associate grow their career and achieve their best, at work and in their community. We are recognized for our commitment to training and diversity and are one of the FORTUNE 100 Best Companies to Work For®.CarMax is an equal opportunity employer, and all qualified candidates will receive consideration for employment without regard to age, race, color, religion, sex, sexual orientation, gender identity, genetic information, national origin, protected veteran status, disability status, or any other characteristic protected by law.Upon an applicant's request, CarMax will consider reasonable accommodation to complete the CarMax Job Application."
DataOS Data Engineer,HP,"Boise, ID (Hybrid)",https://www.linkedin.com/jobs/view/3755224240/?eBP=JOB_SEARCH_ORGANIC&refId=Dkum4n5CE4r0Ge8XgQQBEg%3D%3D&trackingId=vuzSSwlL4VwPzhy%2FOyuY3g%3D%3D&trk=flagship3_search_srp_jobs,3755224240,"About the job
            
 
DataOS Data Engineer collaborating on cutting edge advances in scalable solutions for data ingestion, manipulation and end to end integration of data building blocks to support business driven data products.Applies developed subject matter knowledge to solve common and complex business issues within established guidelines and recommends appropriate alternatives. Works on problems of diverse complexity and scope. May act as a team or project coach providing direction to team activities and facilitates information validation and team decision making process. Exercises judgment within generally defined policies and practices to identify and select a solution. Ability to handle most unique situations. May seek advice in order to support complex business issues.Responsibilities Designs and establishes secure and performant data architectures, enhancements, updates, and programming changes for portions and subsystems of data product pipelines, repositories or models for structured/unstructured data. Focusing on developing sharable libraries that reduce development time and maintenance,Analyzes design and determines coding, programming, and integration activities required based on general objectives and knowledge of overall architecture of product or solution.Writes and executes complete testing plans, protocols, and documentation for assigned portion of data system or component; identifies and debugs, and creates solutions for issues with code and integration into data system architecture.Leads a project team of other data engineers to develop reliable, cost effective and high-quality solutions for assigned data system, model, or component.Collaborates and communicates with project team regarding project progress and issue resolution. Supports co-development processes and tools to promote common approaches to solve complex problems.Represents the data engineering team for all phases of larger and more-complex development projects.Provides guidance and mentoring to less experienced staff members. Strong technical Leadership is required.
Knowledge & Skills Using data engineering tools, languages (Python is a must. Java Scala is a plus), frameworks to mine, cleanse and explore Large Data Sets.Fluent in SQL & Cloud based data systems. Relational data modeling. Fluent in complex, distributed and massively parallel cloud systems (AWS, GCP, AZURE).Strong analytical and problem-solving skills with ability to represent complex algorithms in software.Designing data systems/solutions to manage complex data that are highly scalable and performant.Ability to performance tune Spark code.Strong understanding of database technologies and management systems.Strong understanding of cloud-based systems/services. Differentiate benefits for big data Lake House vs. Warehouse.Experience with workflow orchestration tools (Airflow, Jenkins)Strong Notebooks environment experience (Jupyter, DataBricks)Experience collecting requirements from Partners and choosing the right technologies to meet end to end data flow requirements that are required. (Data size, delivery times: hourly, daily monthly or real-time approaches)Database architecture testing methodology, including execution of test plans, debugging, and testing scripts and tools.Excellent written and verbal communication skills; mastery in English and local language.Ability to effectively communicate product architectures, design proposals and negotiate options at management levels.
Scope & Impact Collaborates with peers, junior engineers, data scientists and project team.Typically interacts with high-level Individual Contributors, Managers and Program Teams.Leads a project requiring data engineering solutions development.
Education & Experience Bachelor's or Master's degree in Computer Science, Data Engineering, Information Systems, Engineering or equivalent.Typically 4-6 years’ experience.
About HPYou’re out to reimagine and reinvent what’s possible—in your career as well as the world around you.So are we. We love taking on tough challenges, disrupting the status quo, and creating what’s next. We’re in search of talented people who are inspired by big challenges, driven to learn and grow, and dedicated to making a meaningful difference.HP is a technology company that operates in more than 170 countries around the world united in creating technology that makes life better for everyone, everywhere.Our history: HP’s commitment to diversity, equity and inclusion – it's just who we are.From the boardroom to factory floor, we create a culture where everyone is respected and where people can be themselves, while being a part of something bigger than themselves. We celebrate the notion that you can belong at HP and bring your authentic self to work each and every day. When you do that, you’re more innovative and that helps grow our bottom line. Come to HP and thrive!"
Sr. Data Engineer,AvalonBay Communities,"Arlington, VA (Hybrid)",https://www.linkedin.com/jobs/view/3748043653/?eBP=JOB_SEARCH_ORGANIC&refId=Dkum4n5CE4r0Ge8XgQQBEg%3D%3D&trackingId=Jh4UUUwshFSQef5NVR%2F0Ow%3D%3D&trk=flagship3_search_srp_jobs,3748043653,"About the job
            
 
OverviewAvalonBay Communities, Inc., an equity REIT, has a long-term track record of developing, redeveloping, acquiring and managing distinctive apartment homes in some of the best U.S. markets, and delivering outsized, risk-adjusted returns to shareholders. With equal parts experience and vision, we’ve established a leadership position rooted in our purpose of creating a better way to live and that is always focused on building value for the long term.Creating a better way to live is the purpose that binds AvalonBay associates. We take that purpose seriously and expect you will as well. By focusing on collaboration, innovation, and taking ownership of our choices and actions, we act in ways that focus on creating value for our customers, investors and associates. Your positive, professional, and consistent personal interactions make AvalonBay a great place to work.The RoleRoleAt AvalonBay we are building the industry’s most advanced Data Analytics capability. Join a green-field opportunity to take responsibility for developing Data Lakehouse in AWS at one of the largest REITs in the country. You will need strong data engineering capability particularly with Python, SQL, AWS tools including Glue, Lambda and Apache Spark. The role includes: Developing a comprehensive Data Acquisition solution and meta-data for our Data Lakehouse to support Business Intelligence and Data Science solutions.Working closely with our Digital Development team to support Customer Experience Applications development. Working closely with our Data Science team to implement machine learning, forecasting and simulation models.Delivering relational solutions in Snowflake to support Business Intelligence.Implementing Data Governance best practicesImplementing automated quality assurance best practices
You Have...Qualifications Required A Bachelor’s degree in Computer Science, or other technical field, and 7 years experience developing data pipelines in AWS.Deep knowledge of Python, SQL, Glue, Lambda and Apache Spark.Experience with Infrastructure as Code (IaC) tools such as AWS CloudFormation or AWS CDK preferred.Understanding of data warehousing methodologies and data lake concepts and experience with Snowflake a strong plus.Experience developing ETL processes and leveraging tools such as Talend.The ability to explain complex technical material to non-technical audiences.
How AvalonBay Supports YouWe know that our teams are the beating heart of our success and we’re committed to showing our appreciation.We Offer Comprehensive benefits – health, dental & vision, 401(k) with company match, paid vacation and holidays, tuition reimbursement, an employee stock purchase plan and more!Growth based on achievement and promotion from within.Associate recognition (a company-wide recognition program that celebrates associate efforts and successes in contributing to the overall success of the organization – including destination awards, ‘AvalonBay’s Very Best’ recognition program and others!).A 20% discount on our incredible apartment homes.A culture built on purpose and our core values - A Commitment to Integrity, A Spirit of Caring, and A Focus on Continuous Improvement.
Additional InfoAvalonBay is proud to be an equal opportunity employer and is committed to an inclusive and diverse work environment free of discrimination and harassment.  We believe that in order to achieve our purpose of creating a better way to live, we must recruit, develop and retain associates with a wide range of backgrounds, experiences and perspectives and create an environment that encourages all voices to be heard, understood and appreciated. With this we know we can do great things.AvalonBay makes employment decisions without regard to a person’s race, ethnicity, color, religion, sex, national origin, sexual orientation, gender identity, pregnancy (including childbirth, lactation or related medical conditions), age, physical or mental disability, genetic information (including characteristics or testing), citizenship status, military or veteran status, or any other status protected by the law.AvalonBay will consider for employment qualified applicants with criminal histories in a manner consistent with requirements under the law.For California residents, if you elect to apply to AvalonBay you accept the AvalonBay California Personnel Privacy Notice"
Data Movement Systems Engineer - NJ/MA,TekIntegral,"Jersey City, NJ (Hybrid)",https://www.linkedin.com/jobs/view/3733200650/?eBP=JOB_SEARCH_ORGANIC&refId=Dkum4n5CE4r0Ge8XgQQBEg%3D%3D&trackingId=6skgB5O%2FNv3ggRpwjo%2BdWQ%3D%3D&trk=flagship3_search_srp_jobs,3733200650,"About the job
            
 
Data Movement Systems EngineerLocation: Hybrid in Jersey City or BostonDuration: Contract to Hire (6-9 Months)Visa: USC GCKey Skills  FTP/SFTP connections/ Strong experience in Managed File Transfer Capabilities  8+ years’ experience  Momentum is the software package uses from- Broadriver Systems : Other like applications that you could look for in experience: MoveIT Sterling AxWay
Principle Responsibilities  Review and recommend effective work processes for the day to day of Data Movement Technology operations Design, build and maintenance of Enterprise Managed File Transfer middleware infrastructure Create of Standards for infrastructure aspects of Enterprise Managed File transfer Middleware such as High Availability, performance tuning, troubleshooting. Responsible for Automation with Powershell, Linux to automate day to day tasks such as deployment, Utilize monitoring tools ( ie Splunk) to develop effective monitoring of incidents and failures and systems health checks. Provide documentation for both production and disaster recovery procedures as well as take part in regular disaster recovery and business continuity tests. Provides support to the operations teams on systems incidents and maintains written resolutions to frequent problems as they relate to Managed File Transfer processes in use within the firm
Required Knowledge, Skills & Abilities  Bachelor's degree in Management of Information Systems, Computer Science, Information Technology, and/or equivalent work experience. At least 8+ years of experience specific to Middleware Managed File Transfer products Clear understandings on communication protocols like FTP/SFTP/HTTPS/AS2/PR4. Strong working knowledge of Networking and Infrastructure capabilities and interactions with Managed File Transfer Processes Experience with Configuring end to end setups installations, upgrades, patching and troubleshooting issues. Knowledge of Windows and MS Office product suites. Strong Financial services industry knowledge. Excellent written and communication skills. Experience in troubleshooting network connectivity issues. Understanding on Oracle database. Experience with Linux/Windows Operating systems. Experience on MFT application migration process. Experience with scripting and orchestration - Ansible / Python / Shell / Powershell Ability to multitask, manage high-priority initiatives and coordinate the activities with Client Service and Engineering teams to completion. Ability to work in a team setting with a diverse group: operations support resources, client services representatives, application and service delivery teams. Experience in handling SSL certificates renewal and troubleshooting certificate issues. Familiarity with observability tools like, Splunk, etc. Analyze application performance, perform turning and ensure high availability & stability of platform. Experience working on Job scheduling tools like Autosys. Must be able to communicate and effectively work with customers, clients, stakeholders, Internal line of business and IT teams. Must be able to convey both technical and non-technical concepts to wide audience. Must have strong sense of urgency and customer service focus. Provide 24/7 Level 3 support for file transfer issues and ensure all business critical files are transmitted within pre-defined SLA's."
"Data Engineer, Data Platform",Grammarly,"Colorado, United States (Hybrid)",https://www.linkedin.com/jobs/view/3689961989/?eBP=JOB_SEARCH_ORGANIC&refId=Dkum4n5CE4r0Ge8XgQQBEg%3D%3D&trackingId=xP3bZ%2Fzr7XmxOnlZPmx5DQ%3D%3D&trk=flagship3_search_srp_jobs,3689961989,"About the job
            
 
Grammarly is excited to offer a remote-first hybrid working model. Team members work primarily remotely in the United States, Canada, Ukraine, Germany, or Poland. Certain roles have specific location requirements to facilitate collaboration at a particular Grammarly hub.All roles have an in-person component: Conditions permitting, teams meet 2–4 weeks every quarter at one of Grammarly’s hubs in San Francisco, Kyiv, New York, Vancouver, and Berlin, or in a workspace in Kraków. This flexible approach gives team members the best of both worlds: plenty of focus time along with in-person collaboration that fosters trust and unlocks creativity.Grammarly team members in this role must be based in the United States or Canada, and they must be able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub(s) where the team is based.The opportunity Grammarly is the world’s leading AI writing assistance company trusted by over 30 million people and 70,000 professional teams every day. From instantly creating a first draft to perfecting every message, Grammarly’s product offerings help people at 96% of the Fortune 500 get their point across—and get results. Grammarly has been profitable for over a decade because we’ve stayed true to our values and built an enterprise-grade product that’s secure, reliable, and helps people do their best work—without selling their data. We’re proud to be one of Inc.’s best workplaces, a Glassdoor Best Place to Work, one of TIME’s 100 Most Influential Companies, and one of Fast Company’s Most Innovative Companies in AI.To achieve our ambitious goals, we’re looking for a Data Engineer to join our Data Engineering Platform team. This person will build highly automated, low latency core datasets that will help data engineers and end users across Grammarly to work with analytical data at scale.Grammarly’s engineers and researchers have the freedom to innovate and uncover breakthroughs—and, in turn, influence our product roadmap. The complexity of our technical challenges is growing rapidly as we scale our interfaces, algorithms, and infrastructure. Read more about our stack or hear from our team on our technical blog.Your impactAs a Data Engineer on our Data Engineering Platform team, you will: Drive improvements to make our analytics effortless by creating and adjusting core data models and storage structures, all while understanding the needs of our users. Make analytical data and metrics usable within a few minutes of real world events occuring, and build streaming processes for the output derived events and aggregate data.Model structure, storage, and access of data at very high volumes for our data lakehouse.Improve developer productivity and self-serve solutions by contributing components to our stream data processing framework(s).Own data engineering's infrastructure-as-code for provisioning services that allow our engineers to deploy mature software installations within a few hours.Build a world-class process that will allow our systems to scale.Mentor other back-end engineers on the team and help them grow.Build and contribute to AWS high-scale distributed systems on the back-end.
We’re Looking For Someone Who Embodies our EAGER values—is ethical, adaptable, gritty, empathetic, and remarkable.Is inspired by our MOVE principles, which are the blueprint for how things get done at Grammarly: move fast and learn faster, obsess about creating customer value, value impact over activity, and embrace healthy disagreement rooted in trust.Is able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub where the team is based.Has experience with Python, Scala, or Java.Has experience with designing database objects and writing relational queriesHas experience designing and standing up APIs and services.Has experience with system design and building internal tools.Has experience handling applications that work with data from data lakes.Has at least some experience building internal Admin sites.Has good knowledge of and at least some experience with AWS (or, alternatively, has deep expertise in Azure or GCE and is willing to learn AWS in a short time frame).Can knowledgeably choose an open source or third-party service to accomplish what they need or, alternatively, can devise a quick and simple solution on their own.
Support for you, professionally and personally Professional growth: We believe that autonomy and trust are key to empowering our team members to do their best, most innovative work in a way that aligns with their interests, talents, and well-being. We support professional development and advancement with training, coaching, and regular feedback.A connected team: Grammarly builds a product that helps people connect, and we apply this mindset to our own team. Our remote-first hybrid model enables a highly collaborative culture supported by our EAGER (ethical, adaptable, gritty, empathetic, and remarkable) values. We work to foster belonging among team members in a variety of ways. This includes our employee resource groups, Grammarly Circles, which promote connection among those with shared identities, such as BIPOC and LGBTQIA+ team members, women, and parents. We also celebrate our colleagues and accomplishments with global, local, and team-specific programs. 
Compensation And BenefitsGrammarly offers all team members competitive pay along with a benefits package encompassing the following and more: Excellent health care (including a wide range of medical, dental, vision, mental health, and fertility benefits)Disability and life insurance options401(k) and RRSP matching Paid parental leaveTwenty days of paid time off per year, eleven days of paid holidays per year, and unlimited sick days Home office stipendsCaregiver and pet care stipendsWellness stipendsAdmission discountsLearning and development opportunities
Grammarly takes a market-based approach to compensation, which means base pay may vary depending on your location. Our US and Canada locations are categorized into compensation zones based on each geographic region’s cost of labor index. For more information about our compensation zones and locations where we currently support employment, please refer to this page. If a location of interest is not listed, please speak with a recruiter for additional information.Base pay may vary considerably depending on job-related knowledge, skills, and experience. The expected salary ranges for this position are outlined below by compensation zone and may be modified in the future.United StatesZone 1: $167,000 - $242,000/year (USD)Zone 2: $150,000 – $218,000/year (USD)Zone 3: $142,000 – $206,000/year (USD)Zone 4: $134,000 – $194,000/year (USD)We encourage you to applyAt Grammarly, we value our differences, and we encourage all—especially those whose identities are traditionally underrepresented in tech organizations—to apply. We do not discriminate on the basis of race, religion, color, gender expression or identity, sexual orientation, ancestry, national origin, citizenship, age, marital status, veteran status, disability status, political belief, or any other characteristic protected by law. Grammarly is an equal opportunity employer and a participant in the US federal E-Verify program (US). We also abide by the Employment Equity Act (Canada).Please note that EEOC is optional and specific to US-based candidates.#NAAll team members meeting in person for official Grammarly business or working from a hub location are strongly encouraged to be vaccinated against COVID-19."
"Data Engineer, Data Platform",Grammarly,"Utah, United States (Hybrid)",https://www.linkedin.com/jobs/view/3689963287/?eBP=JOB_SEARCH_ORGANIC&refId=Dkum4n5CE4r0Ge8XgQQBEg%3D%3D&trackingId=YMTWyTH8VDIpI4WeYTkJFg%3D%3D&trk=flagship3_search_srp_jobs,3689963287,"About the job
            
 
Grammarly is excited to offer a remote-first hybrid working model. Team members work primarily remotely in the United States, Canada, Ukraine, Germany, or Poland. Certain roles have specific location requirements to facilitate collaboration at a particular Grammarly hub.All roles have an in-person component: Conditions permitting, teams meet 2–4 weeks every quarter at one of Grammarly’s hubs in San Francisco, Kyiv, New York, Vancouver, and Berlin, or in a workspace in Kraków. This flexible approach gives team members the best of both worlds: plenty of focus time along with in-person collaboration that fosters trust and unlocks creativity.Grammarly team members in this role must be based in the United States or Canada, and they must be able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub(s) where the team is based.The opportunity Grammarly is the world’s leading AI writing assistance company trusted by over 30 million people and 70,000 professional teams every day. From instantly creating a first draft to perfecting every message, Grammarly’s product offerings help people at 96% of the Fortune 500 get their point across—and get results. Grammarly has been profitable for over a decade because we’ve stayed true to our values and built an enterprise-grade product that’s secure, reliable, and helps people do their best work—without selling their data. We’re proud to be one of Inc.’s best workplaces, a Glassdoor Best Place to Work, one of TIME’s 100 Most Influential Companies, and one of Fast Company’s Most Innovative Companies in AI.To achieve our ambitious goals, we’re looking for a Data Engineer to join our Data Engineering Platform team. This person will build highly automated, low latency core datasets that will help data engineers and end users across Grammarly to work with analytical data at scale.Grammarly’s engineers and researchers have the freedom to innovate and uncover breakthroughs—and, in turn, influence our product roadmap. The complexity of our technical challenges is growing rapidly as we scale our interfaces, algorithms, and infrastructure. Read more about our stack or hear from our team on our technical blog.Your impactAs a Data Engineer on our Data Engineering Platform team, you will: Drive improvements to make our analytics effortless by creating and adjusting core data models and storage structures, all while understanding the needs of our users. Make analytical data and metrics usable within a few minutes of real world events occuring, and build streaming processes for the output derived events and aggregate data.Model structure, storage, and access of data at very high volumes for our data lakehouse.Improve developer productivity and self-serve solutions by contributing components to our stream data processing framework(s).Own data engineering's infrastructure-as-code for provisioning services that allow our engineers to deploy mature software installations within a few hours.Build a world-class process that will allow our systems to scale.Mentor other back-end engineers on the team and help them grow.Build and contribute to AWS high-scale distributed systems on the back-end.
We’re Looking For Someone Who Embodies our EAGER values—is ethical, adaptable, gritty, empathetic, and remarkable.Is inspired by our MOVE principles, which are the blueprint for how things get done at Grammarly: move fast and learn faster, obsess about creating customer value, value impact over activity, and embrace healthy disagreement rooted in trust.Is able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub where the team is based.Has experience with Python, Scala, or Java.Has experience with designing database objects and writing relational queriesHas experience designing and standing up APIs and services.Has experience with system design and building internal tools.Has experience handling applications that work with data from data lakes.Has at least some experience building internal Admin sites.Has good knowledge of and at least some experience with AWS (or, alternatively, has deep expertise in Azure or GCE and is willing to learn AWS in a short time frame).Can knowledgeably choose an open source or third-party service to accomplish what they need or, alternatively, can devise a quick and simple solution on their own.
Support for you, professionally and personally Professional growth: We believe that autonomy and trust are key to empowering our team members to do their best, most innovative work in a way that aligns with their interests, talents, and well-being. We support professional development and advancement with training, coaching, and regular feedback.A connected team: Grammarly builds a product that helps people connect, and we apply this mindset to our own team. Our remote-first hybrid model enables a highly collaborative culture supported by our EAGER (ethical, adaptable, gritty, empathetic, and remarkable) values. We work to foster belonging among team members in a variety of ways. This includes our employee resource groups, Grammarly Circles, which promote connection among those with shared identities, such as BIPOC and LGBTQIA+ team members, women, and parents. We also celebrate our colleagues and accomplishments with global, local, and team-specific programs. 
Compensation And BenefitsGrammarly offers all team members competitive pay along with a benefits package encompassing the following and more: Excellent health care (including a wide range of medical, dental, vision, mental health, and fertility benefits)Disability and life insurance options401(k) and RRSP matching Paid parental leaveTwenty days of paid time off per year, eleven days of paid holidays per year, and unlimited sick days Home office stipendsCaregiver and pet care stipendsWellness stipendsAdmission discountsLearning and development opportunities
Grammarly takes a market-based approach to compensation, which means base pay may vary depending on your location. Our US and Canada locations are categorized into compensation zones based on each geographic region’s cost of labor index. For more information about our compensation zones and locations where we currently support employment, please refer to this page. If a location of interest is not listed, please speak with a recruiter for additional information.Base pay may vary considerably depending on job-related knowledge, skills, and experience. The expected salary ranges for this position are outlined below by compensation zone and may be modified in the future.United StatesZone 1: $167,000 - $242,000/year (USD)Zone 2: $150,000 – $218,000/year (USD)Zone 3: $142,000 – $206,000/year (USD)Zone 4: $134,000 – $194,000/year (USD)We encourage you to applyAt Grammarly, we value our differences, and we encourage all—especially those whose identities are traditionally underrepresented in tech organizations—to apply. We do not discriminate on the basis of race, religion, color, gender expression or identity, sexual orientation, ancestry, national origin, citizenship, age, marital status, veteran status, disability status, political belief, or any other characteristic protected by law. Grammarly is an equal opportunity employer and a participant in the US federal E-Verify program (US). We also abide by the Employment Equity Act (Canada).Please note that EEOC is optional and specific to US-based candidates.#NAAll team members meeting in person for official Grammarly business or working from a hub location are strongly encouraged to be vaccinated against COVID-19."
Data Engineer IV,Navy Federal Credit Union,"Winchester, VA (Hybrid)",https://www.linkedin.com/jobs/view/3768696561/?eBP=JOB_SEARCH_ORGANIC&refId=Dkum4n5CE4r0Ge8XgQQBEg%3D%3D&trackingId=wIzAff3K4ZHMxuR%2FjREWJQ%3D%3D&trk=flagship3_search_srp_jobs,3768696561,"About the job
            
 
OverviewDevelop strategies for data acquisition, archive recovery, and database implementation. Responsible for designing, building, integrating data from various resources, and managing big data. Develop data consumption patterns to share data with internal/external channels, while ensuring they are easily accessible, work smoothly, with the goal of optimizing the performance of Navy Federal’s big data ecosystem. Recognized as an expert with a specialized depth and/or breadth of expertise in discipline. Solves highly complex problems; takes a broad perspective to identify solutions. Leads functional teams or projects. Works independently.Responsibilities Define and build data integration processes to be used across the organizationBuild channel contracts and data consumption patterns for customer facing (On-line/Mobile) channelsAnalyze and validate data sharing requirements within and outside data partners Recognize potential issues and risks during the project implementation and suggest mitigation strategiesCommunicate and own the processes related to contracts and data consumption patternsExpert and key point of contact between the operational data hubs and the channel contracts for On-line/MobileApply engineering principles into the design and enhancement of new and existing data management systemsCoach and mentor project team members in carrying out project implementation activities Work directly with business leadership to understand data requirements; propose and develop solutions that enable effective decision-making and drives business objectives Prepare advanced project implementation plans which highlight major milestones and deliverables, leveraging standard methods and work planning tools Lead the preparation of high-quality project deliverables that are valued by the business and present them in such a manner that they are easily understood by project stakeholdersEnsure the security and integrity of system and product solutions including compliance with Navy Federal, industry engineering and Information Security principles and practicesPresent clear, organized and concise information to all audiences through a variety of media to enable effective business decisionsPerform other duties as assigned
Qualifications Master’s degree in Information Systems, Computer Science, Engineering, or related field, or the equivalent combination of education, training and experienceAdvanced skills in systems and application integration in a large, distributed architecture environmentsProficient skill level in .Net/C#, Python, Agile Frameworks (SAFE), Microsoft Databricks, Azure Data FactoryProficient skills in developing and operationalizing various data distribution patterns like, APIs, event based, pub/sub models Proficient in Data Architecture, Web Services, REST APIs, Event and Pub/Sub messaging architectureProficient in Mobile and Web application technologiesAbility to understand the business problem and determine what aspects of it require optimization; articulate those aspects in a clear and concise mannerProficient skills in understanding SQL and NoSQL and JSON structure Ability to understand other projects or functional areas to consolidate analytical and operational needs and processesDemonstrates change management and/or excellent communication skills Working knowledge of various data structures and the ability to extract data from various data sources Understands the concepts and application of data mapping and building requirementsUnderstands data models, large datasets, business/technical requirementsSkilled in managing the process between updating and maintaining data source systems and implementing data related requirements
Desired Qualifications Knowledge of Navy Federal Credit Union instructions, standards, and procedures
Hours: Monday - Friday, 8:00AM - 4:30PMLocation: 820 Follin Lane, Vienna, VA 22180 | 5550 Heritage Oaks Dr. Pensacola, FL 32526 | 141 Security Dr. Winchester, VA 22602About UsYou have goals, dreams, hobbies, and things you're passionate about—what's important to you is important to us. We're looking for people who not only want to do meaningful, challenging work, keep their skills sharp and move ahead, but who also take time for the things that matter to them—friends, family, and passions. And we're looking for team members who are passionate about our mission—making a difference in military members' and their families' lives. Together, we can make it happen. Don't take our word for it:  Military Times 2022 Best for Vets Employers WayUp Top 100 Internship Programs Forbes® 2022 The Best Employers for New Grads Fortune Best Workplaces for Women Fortune 100 Best Companies to Work For® Computerworld® Best Places to Work in IT Ripplematch Campus Forward Award - Excellence in Early Career Hiring Fortune Best Place to Work for Financial and Insurance Services
Equal Employment Opportunity: Navy Federal values, celebrates, and enacts diversity in the workplace. Navy Federal takes affirmative action to employ and advance in employment qualified individuals with disabilities, disabled veterans, Armed Forces service medal veterans, recently separated veterans, and other protected veterans. EOE/AA/M/F/Veteran/Disability EOE/AA/M/F/Veteran/DisabilityDisclaimers: Navy Federal reserves the right to fill this role at a higher/lower grade level based on business need. An assessment may be required to compete for this position. Job postings are subject to close early or extend out longer than the anticipated closing date at the hiring team’s discretion based on qualified applicant volume. Navy Federal Credit Union assesses market data to establish salary ranges that enable us to remain competitive. You are paid within the salary range, based on your experience, location and market positionBank Secrecy Act: Remains cognizant of and adheres to Navy Federal policies and procedures, and regulations pertaining to the Bank Secrecy Act."
"Data Engineer, Data Platform",Grammarly,"Virginia, United States (Hybrid)",https://www.linkedin.com/jobs/view/3689965170/?eBP=JOB_SEARCH_ORGANIC&refId=Dkum4n5CE4r0Ge8XgQQBEg%3D%3D&trackingId=48La58L%2Fhs127mp%2B03YXVQ%3D%3D&trk=flagship3_search_srp_jobs,3689965170,"About the job
            
 
Grammarly is excited to offer a remote-first hybrid working model. Team members work primarily remotely in the United States, Canada, Ukraine, Germany, or Poland. Certain roles have specific location requirements to facilitate collaboration at a particular Grammarly hub.All roles have an in-person component: Conditions permitting, teams meet 2–4 weeks every quarter at one of Grammarly’s hubs in San Francisco, Kyiv, New York, Vancouver, and Berlin, or in a workspace in Kraków. This flexible approach gives team members the best of both worlds: plenty of focus time along with in-person collaboration that fosters trust and unlocks creativity.Grammarly team members in this role must be based in the United States or Canada, and they must be able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub(s) where the team is based.The opportunity Grammarly is the world’s leading AI writing assistance company trusted by over 30 million people and 70,000 professional teams every day. From instantly creating a first draft to perfecting every message, Grammarly’s product offerings help people at 96% of the Fortune 500 get their point across—and get results. Grammarly has been profitable for over a decade because we’ve stayed true to our values and built an enterprise-grade product that’s secure, reliable, and helps people do their best work—without selling their data. We’re proud to be one of Inc.’s best workplaces, a Glassdoor Best Place to Work, one of TIME’s 100 Most Influential Companies, and one of Fast Company’s Most Innovative Companies in AI.To achieve our ambitious goals, we’re looking for a Data Engineer to join our Data Engineering Platform team. This person will build highly automated, low latency core datasets that will help data engineers and end users across Grammarly to work with analytical data at scale.Grammarly’s engineers and researchers have the freedom to innovate and uncover breakthroughs—and, in turn, influence our product roadmap. The complexity of our technical challenges is growing rapidly as we scale our interfaces, algorithms, and infrastructure. Read more about our stack or hear from our team on our technical blog.Your impactAs a Data Engineer on our Data Engineering Platform team, you will: Drive improvements to make our analytics effortless by creating and adjusting core data models and storage structures, all while understanding the needs of our users. Make analytical data and metrics usable within a few minutes of real world events occuring, and build streaming processes for the output derived events and aggregate data.Model structure, storage, and access of data at very high volumes for our data lakehouse.Improve developer productivity and self-serve solutions by contributing components to our stream data processing framework(s).Own data engineering's infrastructure-as-code for provisioning services that allow our engineers to deploy mature software installations within a few hours.Build a world-class process that will allow our systems to scale.Mentor other back-end engineers on the team and help them grow.Build and contribute to AWS high-scale distributed systems on the back-end.
We’re Looking For Someone Who Embodies our EAGER values—is ethical, adaptable, gritty, empathetic, and remarkable.Is inspired by our MOVE principles, which are the blueprint for how things get done at Grammarly: move fast and learn faster, obsess about creating customer value, value impact over activity, and embrace healthy disagreement rooted in trust.Is able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub where the team is based.Has experience with Python, Scala, or Java.Has experience with designing database objects and writing relational queriesHas experience designing and standing up APIs and services.Has experience with system design and building internal tools.Has experience handling applications that work with data from data lakes.Has at least some experience building internal Admin sites.Has good knowledge of and at least some experience with AWS (or, alternatively, has deep expertise in Azure or GCE and is willing to learn AWS in a short time frame).Can knowledgeably choose an open source or third-party service to accomplish what they need or, alternatively, can devise a quick and simple solution on their own.
Support for you, professionally and personally Professional growth: We believe that autonomy and trust are key to empowering our team members to do their best, most innovative work in a way that aligns with their interests, talents, and well-being. We support professional development and advancement with training, coaching, and regular feedback.A connected team: Grammarly builds a product that helps people connect, and we apply this mindset to our own team. Our remote-first hybrid model enables a highly collaborative culture supported by our EAGER (ethical, adaptable, gritty, empathetic, and remarkable) values. We work to foster belonging among team members in a variety of ways. This includes our employee resource groups, Grammarly Circles, which promote connection among those with shared identities, such as BIPOC and LGBTQIA+ team members, women, and parents. We also celebrate our colleagues and accomplishments with global, local, and team-specific programs. 
Compensation And BenefitsGrammarly offers all team members competitive pay along with a benefits package encompassing the following and more: Excellent health care (including a wide range of medical, dental, vision, mental health, and fertility benefits)Disability and life insurance options401(k) and RRSP matching Paid parental leaveTwenty days of paid time off per year, eleven days of paid holidays per year, and unlimited sick days Home office stipendsCaregiver and pet care stipendsWellness stipendsAdmission discountsLearning and development opportunities
Grammarly takes a market-based approach to compensation, which means base pay may vary depending on your location. Our US and Canada locations are categorized into compensation zones based on each geographic region’s cost of labor index. For more information about our compensation zones and locations where we currently support employment, please refer to this page. If a location of interest is not listed, please speak with a recruiter for additional information.Base pay may vary considerably depending on job-related knowledge, skills, and experience. The expected salary ranges for this position are outlined below by compensation zone and may be modified in the future.United StatesZone 1: $167,000 - $242,000/year (USD)Zone 2: $150,000 – $218,000/year (USD)Zone 3: $142,000 – $206,000/year (USD)Zone 4: $134,000 – $194,000/year (USD)We encourage you to applyAt Grammarly, we value our differences, and we encourage all—especially those whose identities are traditionally underrepresented in tech organizations—to apply. We do not discriminate on the basis of race, religion, color, gender expression or identity, sexual orientation, ancestry, national origin, citizenship, age, marital status, veteran status, disability status, political belief, or any other characteristic protected by law. Grammarly is an equal opportunity employer and a participant in the US federal E-Verify program (US). We also abide by the Employment Equity Act (Canada).Please note that EEOC is optional and specific to US-based candidates.#NAAll team members meeting in person for official Grammarly business or working from a hub location are strongly encouraged to be vaccinated against COVID-19."
Senior Data Engineer - Hybrid,WorkatHome-JobBoard,"Irving, TX (Hybrid)",https://www.linkedin.com/jobs/view/3778650048/?eBP=JOB_SEARCH_ORGANIC&refId=Dkum4n5CE4r0Ge8XgQQBEg%3D%3D&trackingId=zIotG1QAzutbgBdns6nmBQ%3D%3D&trk=flagship3_search_srp_jobs,3778650048,"About the job
            
 
Brief Description Of The OrganizationCiti, the leading global bank, has approximately 200 million customer accounts and does business in more than 160 countries and jurisdictions. Our mission is to serve as a trusted partner to our clients by responsibly providing financial services that enable growth and economic progress. We strive to earn and maintain our clients' and the public's trust by constantly adhering to the highest ethical standards and making a positive impact on the communities we serve.Citi's Personal Banking and Wealth Management (PBWM) division is building a purpose-driven team to serve its globally diverse, digitally forward customers in top cities around the world. We're looking for exceptional candidates who think digitally and differently, resolve problems in unconventional ways and strive to provide a remarkable experience for our clients. We define success by our Citi Leadership Principles: We Take Ownership, We Deliver with Pride and We Succeed Together.Overview Of The RoleCiti's PBWM Technology (PBWMT) organization serves the PBWM global suite of products providing a full range of innovative and comprehensive set of services. Our businesses also offer industry-leading advanced technology, a strong worldwide presence, and a powerful global franchise.The Senior Data Engineer is a position responsible for leading a variety of engineering activities including the design, acquisition and deployment of hardware, software and network infrastructure in coordination with the Technology team. The overall objective of this role is to lead efforts to ensure quality standards are being met within existing and planned framework.Responsibilities Serve as a technology subject matter expert for internal and external stakeholders and provide direction for all firm mandated controls and compliance initiatives, all projects within the group and in creating a technology domain roadmapEnsure that all integration of functions meet business goalsDefine necessary system enhancements to deploy new products and process enhancementsRecommend product customization for system integrationIdentify problem causality, business impact and root causesExhibit knowledge of how own specialty area contributes to the business and apply knowledge of competitors, products and servicesAdvise or mentor junior team membersImpact the engineering function by influencing decisions through advice, counsel or facilitating servicesAppropriately assess risk when business decisions are made, demonstrating particular consideration for the firm's reputation and safeguarding Citigroup, its clients and assets, by driving compliance with applicable laws, rules and regulations, adhering to Policy, applying sound ethical judgment regarding personal behavior, conduct and business practices, and escalating, managing and reporting control issues with transparency.
Qualifications 6-10 years of relevant experience in an Engineering roleExperience working in Financial Services or a large complex and/or global environmentProject Management experienceConsistently demonstrates clear and concise written and verbal communicationComprehensive knowledge of design metrics, analytics tools, benchmarking activities and related reporting to identify best practicesDemonstrated analytic/diagnostic skillsAbility to work in a matrix environment and partner with virtual teamsAbility to work independently, multi-task, and take ownership of various parts of a project or initiativeAbility to work under pressure and manage to tight deadlines or unexpected changes in expectations or requirementsProven track record of operational process change and improvement
Education Bachelor's degree/University degree or equivalent experience
Job Family GroupTechnologyJob FamilySystems & EngineeringTime TypeFull time Primary Location: Irving Texas United States Primary Location Salary Range: $125,760.00 - $188,640.00Citi is an equal opportunity and affirmative action employer.Qualified applicants will receive consideration without regard to their race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.Citigroup Inc. and its subsidiaries (""Citi"") invite all qualified interested applicants to apply for career opportunities. If you are a person with a disability and need a reasonable accommodation to use our search tools and/or apply for a career opportunity review  Accessibility at Citi .View the "" EEO is the Law "" poster. View the EEO is the Law Supplement .View the EEO Policy Statement .View the Pay Transparency Posting"
Lead Data Engineer,Mastercard,"Missouri, United States (Hybrid)",https://www.linkedin.com/jobs/view/3765721300/?eBP=JOB_SEARCH_ORGANIC&refId=snJeDosRAZcNxNUnHdZOmA%3D%3D&trackingId=C75vjuP%2BViAPoSCu6P%2FhvQ%3D%3D&trk=flagship3_search_srp_jobs,3765721300,"About the job
            
 
Our PurposeWe work to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments and businesses realize their greatest potential. Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. We cultivate a culture of inclusion for all employees that respects their individual strengths, views, and experiences. We believe that our differences enable us to be a better team – one that makes better decisions, drives innovation and delivers better business results.Title And SummaryLead Data EngineerAs the Lead Data Engineer, you will lead design and development of enhanced data services and cloud enablement for Data and Services. You will push the limits of the state of the art with best-in-class data product enablement in the cloud and will design the roadmap develop data & analytics solutions that sit atop vast datasets. Your will enable a frictionless customer and developer experience, exposing data components for direct to customer use or as components for new product constructs. You will have the opportunity to create high performance analytic and ML solutions based on data sets measured in the billions of transactions and front-end visualizations to unleash the value of big data.OverviewAs the Lead Data Engineer, you will:  Lead the design of new data capabilities and infrastructure related to accessing and using MA, third party, and partner data to power Mastercard data products and solutions. Design required new data pipes, data transfers, and compliance related infrastructure to enable that use of the data in the cloud. Identify data-related capabilities and infrastructure requirements resulting from new and evolving product constructs and how those requirements can be developed cloud natively. Understand the Data and Services business strategy, information infrastructure and data needs for both new and existing products and services across various product initiatives. Collaborate with relevant data management, data governance and technical teams to enable long term viability of information assets and enterprise requirements. Develop a horizontal technology roadmap in conjunction with Data Strategy & Management team input to enable technology solutions for managing data that meet the needs of multiple business stakeholder groups. Identify existing data capability and infrastructure gaps or opportunities within and across initiatives and provide subject matter expertise in support of remediation.
All About You  Hands-on experience with technologies such as Python, Databricks, AWS, Java, Pyspark, Spark, Kafka, and SQL. You are curious about and motivated by the future trends in data, AI/ML, analytics, and digital experience Experience in data product development, analytical models, and model governance Experience in anonymizing data and managing the use of data Experience in data hygiene procedures, identity resolution capabilities or data management a plus Exceptional interpersonal skills with proven experience in relationship building and partnering. Must work well in both team and individual settings and must be able to work with a geographically dispersed team. Strong analytic capabilities and written and oral communication skills. Attention to detail is a must. Strong project management skills and a demonstrated ability to understand complex information product constructs Familiarity with industry best practices for collection and use of data Motivated self-starter with ability to excel at multi-tasking in a fast-paced environment, outstanding English written and verbal communication skills
In the US, Mastercard is an inclusive Equal Employment Opportunity employer that considers applicants without regard to gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law. If you require accommodations or assistance to complete the online application process, please contact reasonable_accommodation@mastercard.com and identify the type of accommodation or assistance you are requesting. Do not include any medical or health information in this email. The Reasonable Accommodations team will respond to your email promptly.Corporate Security Responsibility All activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must: Abide by Mastercard’s security policies and practices;Ensure the confidentiality and integrity of the information being accessed;Report any suspected information security violation or breach, andComplete all periodic mandatory security trainings in accordance with Mastercard’s guidelines.
In line with Mastercard’s total compensation philosophy and assuming that the job will be performed in the US, the successful candidate will be offered a competitive base salary based on location, experience and other qualifications for the role and may be eligible for an annual bonus or commissions depending on the role. Mastercard benefits for full time (and certain part time) employees generally include: insurance (including medical, prescription drug, dental, vision, disability, life insurance), flexible spending account and health savings account, paid leaves (including 16 weeks new parent leave, up to 20 paid days bereavement leave), 10 annual paid sick days, 10 or more annual paid vacation days based on level, 5 personal days, 10 annual paid U.S. observed holidays, 401k with a best-in-class company match, deferred compensation for eligible roles, fitness reimbursement or on-site fitness facilities, eligibility for tuition reimbursement, gender-inclusive benefits and many more.Pay RangesO'Fallon, Missouri: $138,000 - $221,000 USD"
Data Engineer,Motion Recruitment,"Arizona, United States (Hybrid)",https://www.linkedin.com/jobs/view/3739105864/?eBP=JOB_SEARCH_ORGANIC&refId=snJeDosRAZcNxNUnHdZOmA%3D%3D&trackingId=pLLvAj8GwEeC0C9GB1FJnQ%3D%3D&trk=flagship3_search_srp_jobs,3739105864,"About the job
            
 
Data Engineer We are working with a client in Scottsdale, AZ to hire a data engineer. You will be building data pipelines and doing data manipulations using BigQuery, DBT and MySQL. Raw data will be coming in from various sources and it is your job to bring the data together onto a single source.This role requires 3 days on-site in South Scottsdale. Not offering relocation assistance. Required Skills & Experience 2-3 of professional data engineering experience Proficiency using Python MySQL skills 
Desired Skills & Experience Experience working with DBT and FiveTran 
What You Will Be Doing Tech Breakdown 100% Data Engineering 
Daily Responsibilities 100% Hands on 
Applicants must be currently authorized to work in the US on a full-time basis now and in the future.Posted By: Natalie Daub"
Senior Data Engineer,"Transport Enterprise Leasing, LLC","Chattanooga, TN (Hybrid)",https://www.linkedin.com/jobs/view/3727092863/?eBP=JOB_SEARCH_ORGANIC&refId=snJeDosRAZcNxNUnHdZOmA%3D%3D&trackingId=W710ucAra460ufSZfaBqGA%3D%3D&trk=flagship3_search_srp_jobs,3727092863,"About the job
            
 
Share with a friend by copying and using this link: https://tel360.com/careers/openpositions/?gnk=job&gni=8a78859e8a48e57e018a8f81127d18e3&gns=Company+WebsitePosition PurposeWe are seeking a highly skilled Senior Data Engineer to join our dynamic team. In this role, you will be responsible for developing and maintaining both back-end ETL integrations and front-end user-facing metrics and dashboards. You will work closely with cross-functional teams, including other data engineers, business analysts, and software engineers, to ensure efficient data integration, processing, and visualization. You will play a crucial role in transforming raw data into actionable insights, enabling data-driven decision-making across the organization.(THIS POSITION IS ONSITE IN CHATTANOOGA, TN)Position Responsibilities Continuously learn, share, and implement improvements in all processes and responsibilities as needed to enhance effectiveness of providing world class service and support.Design, develop, and maintain robust ETL pipelines from various data sources. Implement scalable and optimized solutions for data processing, storage, and retrieval. Collaborate with stakeholders to understand data requirements and translate them into technical specifications. Coordinate with multi-platform development team to support a unified strategy for data governance and architecture. Build user-facing dashboards and visualizations to effectively present data insights and key metrics. Ensure data quality and integrity throughout the data pipeline and create tools to better identify and resolve any issues or bottlenecks. Optimize performance and scalability of data processing and storage systems. Support data validation efforts with stakeholders and development team as needed. Stay up-to-date with emerging technologies and industry trends in data engineering and analytics. Mentor and provide guidance to other team members, fostering knowledge sharing and professional development. 
Knowledge, Skills, And Abilities Bachelor’s or higher degree in Computer Science, Engineering or a related field. Proven Experience as a Data Engineer, preferably in a lead role. Strong expertise in developing and maintaining ETL pipelines using tools such as Apache Spark, Airflow, or similar frameworks. Proficiency in programming languages such as Python, Java, or Scala for data processing and integration. Solid understanding of data warehousing concepts, relational and NoSQL databases, and data modeling. Experience with front-end development and visualization tools such as Domo, Tableau, Power BI, or D3.js. Familiarity with cloud platforms such as AWS, Azure, or GCP and their data services (e.g., S3, Redshift, BigQuery). Comfortable working with a variety of APIs to push and pull data from various data systems and platforms. Excellent problem-solving and analytical skills with a keen attention to detail. Strong communication skills and the ability to collaborate effectively with cross-functional teams. 
Wage$110,000 to $130,000K based on experience.Benefits 100% employer paid medical and dental (single and family coverage) premiums through BlueCross BlueShield of TN.HSA with $800 annual employer contributionVoluntary Life, Short- and Long-Term Disability8-week paid family leavePTO10 Holidays (including birthday)Paid day off on Veterans Day for Veterans401(k) with up to 4% employer matchProfit Sharing (some exclusions apply)Retirement Pay ProgramYears of Service Cash Incentive Smart Dollar financial wellness program"
Staff Data Engineer,SiriusXM,"New York, NY (Hybrid)",https://www.linkedin.com/jobs/view/3690118696/?eBP=JOB_SEARCH_ORGANIC&refId=snJeDosRAZcNxNUnHdZOmA%3D%3D&trackingId=KDA72Ow%2BW8%2F77mDtkvipIQ%3D%3D&trk=flagship3_search_srp_jobs,3690118696,"About the job
            
 
Who We AreSiriusXM and its brands (Pandora, SXM Media, AdsWizz, Simplecast, and SiriusXM Connected Vehicle Services) are leading a new era of audio entertainment and services by delivering the most compelling subscription and ad-supported audio entertainment experience for listeners -- in the car, at home, and anywhere on the go with connected devices. Our vision is to shape the future of audio, where everyone can be effortlessly connected to the voices, stories and music they love wherever they are. This is the place where a diverse group of emerging talent and legends alike come to share authentic and purposeful songs, stories, sounds and insights through some of the best programming and technology in the world. Our critically-acclaimed, industry-leading audio entertainment encompasses music, sports, comedy, news, talk, live events, and podcasting. No matter their individual role, each of our employees plays a vital part in bringing SiriusXM’s vision to life every day.SiriusXM is the leading audio entertainment company in North America, and the premier programmer and platform for subscription and digital advertising-supported audio products. SiriusXM’s platforms collectively reach approximately 150million listeners, the largest digital audio audience across paid and free tiers in North America, and deliver music, sports, talk, news, comedy, entertainment and podcasts. Pandora, a subsidiary of SiriusXM, is the largest ad-supported audio entertainment streaming service in the U.S. SiriusXM's subsidiaries Simplecast and AdsWizz make it a leader in podcast hosting, production, distribution, analytics and monetization. The Company’s advertising sales organization, which operates as SXM Media, leverages its scale, cross-platform sales organization and ad tech capabilities to deliver results for audio creators and advertisers. SiriusXM, through Sirius XM Canada Holdings, Inc., also offers satellite radio and audio entertainment in Canada. In addition to its audio entertainment businesses, SiriusXM offers connected vehicle services to automakers.How You’ll Make An ImpactSiriusXM is seeking a highly motivated and experienced Staff Data Engineer to join our Data Org. In this role, you will be responsible for designing and developing data processing pipelines, data warehousing solutions, and real-time data ingestion systems. You will work closely with data scientists, analysts, and other stakeholders to develop solutions that support the business needs of SiriusXM, Pandora, and AdsWizz.What You’ll Do Develop and maintain high-performance, scalable, and fault-tolerant data processing systems using Spark, Flink, Druid, or the like.Collaborate with cross-functional teams to integrate data from multiple sources and ensure data accuracy and consistency.Write complex SQL queries to support data analysis and reporting.Develop and maintain data models and ETL processes.Optimize data pipelines and system performance to ensure efficient data processing and analysis.Build and maintain data infrastructure, including data storage and data access systems.Stay up-to-date with the latest developments in data engineering technologies and best practices.
What You’ll Need Bachelor's degree in Computer Science, Engineering, or a related field7+ years of experience in Data EngineeringExpertise in Spark and PythonExperience with Scala is a bonusStrong understanding of distributed systems, data modeling, and ETL processesHands-on experience with cloud-based data warehousing solutions (e.g., AWS Redshift, Snowflake, Teradata)Excellent problem-solving skills and attention to detailStrong written and verbal communication skillsAbility to work independently and as part of a team
At SiriusXM, we carefully consider a wide range of factors when determining compensation, including your background and experience. These considerations can cause your compensation to vary. We expect the base salary for this position to be in the range of $120,000 to $166,000 and will depend on your skills, qualifications, and experience. Additionally, this role might be eligible for discretionary short-term and long-term incentives. We encourage all interested candidates to apply.Our goal at SiriusXM is to provide and maintain a work environment that fosters mutual respect, professionalism and cooperation. SiriusXM is an equal opportunity employer that does not discriminate on the basis of actual or perceived race, creed, color, religion, national origin, ancestry, alienage or citizenship status, age, disability or handicap, sex, gender identity, marital status, familial status, veteran status, sexual orientation or any other characteristic protected by applicable federal, state or local laws.The requirements and duties described above may be modified or waived by the Company in its sole discretion without notice.R-2023-08-59"
Sr. Principal Data Engineer (TS/SCI) with Security Clearance,ClearanceJobs,"Colorado Springs, CO (Hybrid)",https://www.linkedin.com/jobs/view/3779710575/?eBP=JOB_SEARCH_ORGANIC&refId=snJeDosRAZcNxNUnHdZOmA%3D%3D&trackingId=6yDiZzNFiPSUUAL4rPMsug%3D%3D&trk=flagship3_search_srp_jobs,3779710575,"About the job
            
 
Zachary Piper Solutions is currently seeking a TS/SCI cleared Sr. Principal Data Engineer to support the United States Space Force's National Space Test & Training Complex (NSTTC) program in Colorado Springs, CO. The Data Engineer will coordinate with the government and partner data stewards across the program and U.S. Space Force data enterprise to manage data strategies, cross-cutting infrastructure, and processes to drive consistent data governance for test and training purposes. Responsibilities: * Evaluate, select, adapt, and modify data strategies, plans, provide potential solutions, and conduct engineering assessments for integration into large-scale enterprise data governance * Engage with government technical data stewards to aid in conducting data lineage, deriving business rules (attributes, calculations, derivations, etc) and understanding data attribute's purpose and utilization * Direct the definition, aggregation, and maintenance of critical data elements needed to populate modeling, simulation, and analysis software * Coordinate with government data stewards to create enterprise level data quality metrics and a quality data model, consistent across enterprise solutions  Support on-boarding and integration of data sources and serve as a liaison between government organizations in support of a federated governance model Analyze complex data problems and apply principles and practices of the data disciplines while devising new approaches and solutions to data problem sets Partner with the government customer data administrators to ensure that standards and governance are understood and implemented Create capabilities to perform data quality audits, find data collection issues, and suggest improvements while acting on findings as necessary and implementing fixes
Qualifications: * Masters in IT, Computer Science or related field with 14 years of experience in Data/Systems Engineering  Must have an active TS/SCI CISSP certified Experience with Agile DevSecOps Experience with AWS and cloud systems Experience with SQL Server and SQL Server Integration Services Experience with enterprise level data infrastructure and architecture Experience with Machine Learning and Artificial Intelligence
Compensation: * Pay Range: $175,000 - $200,000  Benefits: Cigna health, dental, vision, 401k, 20 days paid time off, and 11 federal holidays"
Senior Data Engineer,Zencon Group,"Dorchester, MA (Hybrid)",https://www.linkedin.com/jobs/view/3752310299/?eBP=JOB_SEARCH_ORGANIC&refId=snJeDosRAZcNxNUnHdZOmA%3D%3D&trackingId=NcdQmiMKwPLIz61DvjA1EA%3D%3D&trk=flagship3_search_srp_jobs,3752310299,"About the job
            
 
Senior Data Engineer   Hybrid – remote and onsite in Quincy, MA   Long term contract   The  Senior Data Engineer (SDE) will lead the new Snowflake & Informatica implementation of the agency. The individual in this role will collaborate with business users and other engineers to build a new data warehouse solution using Snowflake. The SDE will help with the overall Data Strategy, including infrastructure, software, utilities/tools, Public Cloud Solutions, and Business Intelligence Solutions for the organization.    DETAILED LIST OF JOB DUTIES AND RESPONSIBILITIES:   Provide an understanding of Snowflake Data Warehouse, Data Movement, Data Curation, Data Staging, and Transformation best practices.  Build processes supporting data transformation, data structures, metadata, dependency, and workload management.  Create and contribute to the automation of essential processes related to infrastructure solution deployment, creating repeatable and robust deployments.  Perform advanced data transformation activities using Informatica, PL/SQL, Oracle Database tuning, SQL tuning, and Informatica Server administration.  Provide hands-on solutions, including platform ownership and supporting essential platform capabilities.  Responsible for Performance and performance tuning, Data Quality, Protection, and Availability  Responsible for developing and implementing solutions related to Snowflake and preparing data for Business Intelligence  Fully document all solution work, including designs and configurations  Play an essential role in troubleshooting data system problems and providing viable solution options 
    QUALIFICATIONS:   Minimum eight years of experience in IT with a focus on Enterprise Data Solutions  5 years of experience in:   Snowflake Data Solutions, hands-on Data warehousing methodologies and modelling techniques  Data migration methods of on-prem to cloud data solutions, including ELT/ETL Tools and concepts  Working with Batch and Stream data SQL, preferable Snowflake SQL Massively Parallel Processing (MPP) Analytical Datastores Education Requirements 

   Bachelor's degree in computer science, engineering, or Management Information Systems, or related IT field."
Senior Data Engineer,Mastercard,"Missouri, United States (Hybrid)",https://www.linkedin.com/jobs/view/3776395298/?eBP=JOB_SEARCH_ORGANIC&refId=snJeDosRAZcNxNUnHdZOmA%3D%3D&trackingId=eHZD6LzRZHSKvESasyBQ4w%3D%3D&trk=flagship3_search_srp_jobs,3776395298,"About the job
            
 
Our PurposeWe work to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments and businesses realize their greatest potential. Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. We cultivate a culture of inclusion for all employees that respects their individual strengths, views, and experiences. We believe that our differences enable us to be a better team – one that makes better decisions, drives innovation and delivers better business results.Title And SummarySenior Data EngineerOverviewWe are seeking a talented and motivated Senior Data Engineer to join our data engineering team. In this role, you will play a critical part in designing, developing, and optimizing data pipelines and solutions that enable efficient data processing and analysis. As a Senior Data Engineer, you will collaborate with cross-functional teams to drive data-driven decision-making and contribute to the continuous improvement of our data infrastructure.Role  Design, develop, and maintain new data capabilities and infrastructure for utilizing Mastercard, third-party, and partner data to enhance Mastercard's data products and solutions Create new data pipelines, data transfers, and compliance-oriented infrastructure to facilitate seamless data utilization within cloud environments Identify existing data capability and infrastructure gaps or opportunities within and across initiatives and provide subject matter expertise in support of remediation Collaborate with technical team and business stakeholders to understand data requirements and translate them into technical solutions Work with large datasets, ensuring data quality, accuracy, and performance Implement data transformation, integration, and validation processes to support analytics and reporting needs Optimize and fine-tune data pipelines for improved speed, reliability, and efficiency Implement best practices for data storage, retrieval, and archival to ensure data accessibility and security Troubleshoot and resolve data-related issues, collaborating with the team to identify root causes Document data processes, data lineage, and technical specifications for future reference Participate in code reviews, ensuring adherence to coding standards and best practices Collaborate with DevOps teams to automate deployment and monitoring of data pipelines Additional tasks as required
All About You  Bachelor's or Master's degree in Computer Science, Engineering, or a related field Proven experience in data engineering, with a strong track record of designing and implementing data solutions Proficiency in programming languages such as Python, Java, or Scala, and experience with data processing frameworks (Spark, Hadoop, etc.) In-depth understanding of data warehousing concepts, cloud platforms (AWS, Azure, GCP), and data modeling techniques Strong knowledge of SQL and NoSQL databases, as well as data integration and transformation tools Passion for and engagement with emerging trends in data, AI/ML, analytics, and digital experiences Experience in data product development, analytical models, and model governance Experience in anonymizing data and managing the use of data Experience in data hygiene procedures, identity resolution capabilities or data management a plus Ability to create strategies and plans that define how information can be utilized to support an organization's overall business strategy, and how that information and data is organized, and governed inside an organization Strong project management skills and a demonstrated ability to understand complex information product constructs Familiarity with industry best practices for collection and use of data Outstanding problem-solving skills and the ability to navigate complex data challenges Effective communication and collaboration skills to work with both technical and non-technical stakeholders Experience with agile methodologies and DevOps practices
What is Data & Services?The Data & Services Team (D&S) is a key differentiator for Mastercard, providing the cutting-edge services that help our customers grow. Focused on thinking big and scaling fast around the globe, this team is responsible for end-to-end solutions for a diverse global customer base. We combine traditional management consulting with our rich data assets and in-house technology to provide our clients with powerful insights and tools to drive fact-based decision making. Centered on data-driven technologies and innovation, our services include consulting, loyalty and marketing programs, test-and-learn business experimentation, and data-driven information and risk management. While specializing in the payments industry, Mastercard Data & Services also works closely with major retailers, airlines, and other enterprises, leveraging data and insights garnered from within and beyond its network.D&S is continuously looking for passionate and talented technologists, who share our vision for empowering our customers to make better fact-based decisions, to join us and shape the growth of our team.In the US, Mastercard is an inclusive Equal Employment Opportunity employer that considers applicants without regard to gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law. If you require accommodations or assistance to complete the online application process, please contact reasonable_accommodation@mastercard.com and identify the type of accommodation or assistance you are requesting. Do not include any medical or health information in this email. The Reasonable Accommodations team will respond to your email promptly.Corporate Security Responsibility All activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must: Abide by Mastercard’s security policies and practices;Ensure the confidentiality and integrity of the information being accessed;Report any suspected information security violation or breach, andComplete all periodic mandatory security trainings in accordance with Mastercard’s guidelines.
In line with Mastercard’s total compensation philosophy and assuming that the job will be performed in the US, the successful candidate will be offered a competitive base salary based on location, experience and other qualifications for the role and may be eligible for an annual bonus or commissions depending on the role. Mastercard benefits for full time (and certain part time) employees generally include: insurance (including medical, prescription drug, dental, vision, disability, life insurance), flexible spending account and health savings account, paid leaves (including 16 weeks new parent leave, up to 20 paid days bereavement leave), 10 annual paid sick days, 10 or more annual paid vacation days based on level, 5 personal days, 10 annual paid U.S. observed holidays, 401k with a best-in-class company match, deferred compensation for eligible roles, fitness reimbursement or on-site fitness facilities, eligibility for tuition reimbursement, gender-inclusive benefits and many more.Pay RangesO'Fallon, Missouri: $115,000 - $184,000 USD"
"Azure Data Engineer, Tech Lead (Contract)",tlnt,"Dallas, TX (Hybrid)",https://www.linkedin.com/jobs/view/3756663633/?eBP=JOB_SEARCH_ORGANIC&refId=snJeDosRAZcNxNUnHdZOmA%3D%3D&trackingId=QmVNQ7ybf8HZnm4ssT3OzQ%3D%3D&trk=flagship3_search_srp_jobs,3756663633,"About the job
            
 
The RoleRequirementsWe are seeking a highly experienced Senior Data Engineer with 10+ years of expertise in Azure data engineering. This role is responsible for the development and management of data pipelines, data modeling, and supporting organizational data analytics and insights initiatives. The ideal candidate will be proficient in tools and technologies like Pyspark, Azure Data Factory (ADF), Databricks, big data technologies, SQL, and Python. Additionally, the Senior Data Engineer will play a critical role in stakeholder management, leading development teams or project teams, guiding design discussions, and taking ownership of deliverables for larger projects. Effective communication skills are essential for understanding and translating functional and technical requirements into practical solutions. The ideal person will: Design, build, and maintain data pipelines in the Azure environment, ensuring smooth operation in both batch and real-time processing. Optimize Azure data infrastructure to facilitate accurate data extraction, transformation, and loading from a variety of data sources. Develop and automate ETL processes using tools like Pyspark, Azure Data Factory, and Databricks to facilitate data extraction and manipulation from diverse sources. Transform raw data stored in Azure Data Warehouses into accessible datasets suitable for both technical and non-technical stakeholders. Demonstrate expertise in Azure services and big data technologies, including but not limited to Azure Data Factory, Databricks, and other relevant tools. Proficiency in SQL and Python is also crucial. Engage in business discussions, collaborate with stakeholders, and take ownership of deliverables, ensuring that the data engineering solutions align with business objectives. Lead development teams or project teams, providing guidance and expertise to ensure the successful execution of data engineering projects. Function as an independent developer who can guide design discussions and make informed decisions to drive project success. Leverage extensive experience to handle larger projects and manage multiple teams effectively. Exhibit excellent communication skills, translating functional and technical requirements into actionable solutions that meet business needs. 
The Person Minimum of 7 years of hands-on experience in Azure data engineering. Proficiency in a range of key technologies is essential, including Pyspark, Azure Data Factory (ADF), Databricks, big data tools, SQL, and Python. Should be adept at handling business discussions and be capable of taking ownership of deliverables. Effective stakeholder management is crucial, including leading development teams or project teams to ensure successful project outcomes. Self-sufficient developer who can provide guidance in design discussions and make informed decisions. Demonstrated experience in successfully managing larger projects and multiple teams is a key requirement. Excellent communication skills are necessary to understand and bridge the gap between functional and technical requirements, providing practical and effective solutions. 
Note: Preferred candidates will be in the Dallas or willing to relocate to Dallas, Texas Our client is an equal opportunity employer, and we celebrate diversity and are committed to creating an inclusive environment for all.Note: This job description is intended to convey information essential to understanding the scope of the position and is not an exhaustive list of skills, efforts, duties, responsibilities, or working conditions associated with it.Skills: pyspark,sql,python,azure,azure data engineer,adf (azure data factory),azure data factory,databricks"
Sr. Data Engineer,Chubb,"Whitehouse Station, NJ (Hybrid)",https://www.linkedin.com/jobs/view/3756335093/?eBP=JOB_SEARCH_ORGANIC&refId=snJeDosRAZcNxNUnHdZOmA%3D%3D&trackingId=rq6lhaTuEbUF3x4SPnkrig%3D%3D&trk=flagship3_search_srp_jobs,3756335093,"About the job
            
 
Job DescriptionWe are looking for an experienced and motivated Senior ETL Developer to join our dynamic team. In this role, you will lead the delivery of key projects in support of North America financial reporting from the enterprise data warehouse and associated data marts, including the Business Analytics Repository (BAR). BAR is a strategic application within the business, feeding into multiple systems and applications both up and downstream with data that directly supports business decisions being made each and every day. You will be responsible for leading ETL development projects, coordinating with cross-functional teams to ensure project success, and creating and maintaining data integration solutions to meet business requirements. The ideal candidate will have experience with ETL development solutions such as AWS Glue, Google Dataflow, Azure Data Factory, Snowflake, Informatica/IICS and be able to identify and resolve data quality issues, performance bottlenecks, and other ETL-related problems.Responsibilities Lead ETL development projects and coordinate with cross-functional teams to ensure project successCreate and maintain data integration solutions to meet business requirementsIdentify and resolve data quality issues, performance bottlenecks, and other ETL-related problemsDesign and develop scalable ETL workflows and data pipelines using ETL tools such as Informatica/IICSEnsure compliance with data governance and security policiesDevelop and maintain documentation such as technical design documents, data lineage, and ETL runbooksMentor junior ETL developers and provide technical guidance to the teamEvaluate modern technologies and tools, and recommend solutions to improve ETL processes and performanceContribute to the architecture, design, and development of data warehousing and business intelligence solutionsUnderstands data mapping and data modeling methodologies including normal form, star, and snowflake to reduce data redundancy and improve data integrity.Participate in analysis, design, and ETL development as part of Agile development methodologies and provide status updates to the managementMaintains knowledge on current and emerging developments/trends for assigned area(s) of responsibility, assesses the impact, and collaborates with Scrum Team and Leadership to incorporate current trends and developments in current and future solutions
Qualifications 5 Year/bachelor’s degree or equivalent work experience (4 years of experience in lieu of Bachelors)_Minimum Required in Computer Science, Computer Information Systems, Information Systems, Information Technology or Computer Engineering or equivalent work experienceAt least 5+ years of Strong understanding of ETL development concepts and tools such as ETL development solutions (e.g., AWS Glue, Google Dataflow, Azure Data Factory, Snowflake, Informatica/IICS) Experience with Data Warehousing and Business Intelligence concepts and technologiesStrong knowledge of SQL and advanced programming languages such as Python and JavaDemonstrated critical thinking skills and the ability to identify and resolve data quality issues, performance bottlenecks, and other ETL-related problemsExperience with Agile methodologies and project-management skillsExcellent communication and interpersonal skillsAbility to mentor and provide technical guidance to junior ETL developersExperience with cloud based environment required.2+ years of experience in scheduling jobs using Autosys (or comparable distributed scheduler)3+ years of experience writing Unix/Linux or Windows Scripts in tools such as PERL, Shell script, Python, etc.3+ years of experience in creating complex technical specifications from business requirements/specifications
About UsChubb is the world’s largest publicly traded property and casualty insurer. With operations in 54 countries, Chubb provides commercial and personal property and casualty insurance, personal accident and supplemental health insurance, reinsurance, and life insurance to a diverse group of clients. The company is distinguished by its extensive product and service offerings, broad distribution capabilities, exceptional financial strength, underwriting excellence, superior claims handling expertise and local operations globally.At Chubb, we are committed to equal employment opportunity and compliance with all laws and regulations pertaining to it. Our policy is to provide employment, training, compensation, promotion, and other conditions or opportunities of employment, without regard to race, color, religious creed, sex, gender, gender identity, gender expression, sexual orientation, marital status, national origin, ancestry, mental and physical disability, medical condition, genetic information, military and veteran status, age, and pregnancy or any other characteristic protected by law. Performance and qualifications are the only basis upon which we hire, assign, promote, compensate, develop and retain employees. Chubb prohibits all unlawful discrimination, harassment and retaliation against any individual who reports discrimination or harassment."
Data Center and Cloud Solutions Engineer,"QuadraNet Enterprises, LLC.","Los Angeles, CA (Hybrid)",https://www.linkedin.com/jobs/view/3778493374/?eBP=JOB_SEARCH_ORGANIC&refId=snJeDosRAZcNxNUnHdZOmA%3D%3D&trackingId=XHH6ohGQbh0%2Bw175Tmxq4w%3D%3D&trk=flagship3_search_srp_jobs,3778493374,"About the job
            
 
Challenges You Will SolveWe are looking for a skilled Data Center and Cloud Sales Engineer to join our team and help drive our cloud solutions business. As a Cloud Sales Engineer, you will be responsible for providing technical expertise to sales teams, helping to identify and qualify opportunities, and delivering technical presentations and demos to prospective clients. You will also work closely with our product and engineering teams to ensure that our cloud solutions meet customer requirements and stay competitive in the market.What You’ll Do  Collaborate with the sales team to understand client needs and identify opportunities to sell our cloud solutions Deliver technical presentations and demonstrations to clients, showcasing the benefits and features of our cloud solutions Develop and maintain strong relationships with clients, serving as a trusted technical advisor Conduct product demonstrations and proof-of-concept (POC) projects to showcase our cloud solutions Work with product and engineering teams to understand the technical details of our cloud solutions and how they fit into the market Participate in trade shows, conferences, and other industry events to promote our cloud solutions and generate leads Provide feedback from clients to product and engineering teams to improve our cloud solutions Continuously stay up to date on industry trends and emerging technologies to stay competitive in the market
What You’ll Bring  Bachelor’s degree in computer science, Electrical Engineering, or a related field. 3-5 years of experience in technical sales, preferably in the data center or cloud solutions industry. Strong technical knowledge of data center and cloud solutions, including virtualization, storage, networking, and security. Excellent communication and interpersonal skills, with the ability to explain technical concepts to non-technical audiences. Strong analytical and problem-solving skills. Ability to work independently and as part of a team. Willingness to travel as needed to meet with customers and attend industry events. Experience with CRM systems, such as Salesforce, is a plus.
Flexible work from home options available."
Load Research Data Engineer,Salt River Project,Greater Phoenix Area (Hybrid),https://www.linkedin.com/jobs/view/3773566584/?eBP=JOB_SEARCH_ORGANIC&refId=snJeDosRAZcNxNUnHdZOmA%3D%3D&trackingId=P8T9o3yXTJM9UJo6sObwzw%3D%3D&trk=flagship3_search_srp_jobs,3773566584,"About the job
            
 
Join us in building a better future for Arizona!SRP is one of the largest public power and water utilities in the U.S. providing electricity to approximately one million customers in the greater metropolitan Phoenix area. Since its founding in 1903, SRP has fostered a culture of stewardship and customer service consistently ranking as an industry leader in customer service according to J.D. Power and named one of Arizona's best employers by Forbes. SRP continues to adapt to its changing business environment by seeking innovative ways to reimagine utility service and the provision of critical resources essential to the life and economy of Arizona.Why Work at SRPSRP's success is rooted in our employees' happiness, health, and safety. That's why we offer a comprehensive benefits package to meet the needs of our employees and enhance their well-being. In addition to competitive pay and performance incentives, eligible employees can take advantage of the following benefits: Pension Plan (at no cost to the employee)401(k) plan with employer matchingAvailable your first day: Medical, vision, dental, and life insuranceOver 200+ hours of PTO (includes vacation days, holidays, floating holidays, and sick leave)Parental leave (up to 4 weeks) and adoption assistanceWellness programs (including access to a recreation and fitness facility)Short and long-term disability plansTuition assistance for both undergraduate and graduate programs10 Employee Resource Groups for career development, community service, and networking
SummaryLoad Research is one of the primary departments at SRP that interfaces with the advanced metering infrastructure data stream. The high frequency data provided by the advanced meters is used in planning customer programs, predicting customer demand and understanding customer preferences and trends in electricity usage. Current and future metering infrastructure require the high-level data management and analysis capabilities that are offered by Load Research. These needs will continue to grow in the future as advanced distribution/load management concepts are put into practice.This position serves as the primary database, server, and data process architect for the business unit. Additionally provides IT systems administration, software, and information custodian support.What You'll Do:  Maintain and develop databases for customer meter data along with the tools SRP employees use to access and manipulate these data.Developing, deploying and monitoring recurring database jobs using Windows job scheduler, Batch scripting, SQL queries, SAS scripts, and similar toolsDevelop subject matter expertise related to SRP’s existing database infrastructure, which includes SQL servers, SAS servers, cloud computing servers, Hadoop, Snowflake, and other similar technologies. Innovate, plan, and execute process improvements for database structure and data process workflows, including robotic process automation. Customer usage reports for internal and external clients.Support Forecasting and Load Research with database and user administration, including administration of Itron’s MV-90 software product.Manage large data sets in an organized manner with effective quality control and supporting documentation.Interfaces with other internal groups on process improvements and resolving issues, including root cause analysis. Occasional training of staff and quality control of outputs.Exhibits high-level data security, an understanding of sensitive data protocols and service as an information custodian for the department.
What it Takes to Succeed:Promotion to Level 2 requires a minimum of two years experience at Level 1 and demonstrated capability to perform advanced and more difficult work as determined by the supervisor. Promotion to Senior Level requires a minimum of 3 years experience at Level 2, is fully competent in all aspects of functional area of assignment and as such would be recognized as a specialist in area of assignment and may have periodic or occasional lead responsibilities.Professional Qualifications:  T-SQL expertise is strongly preferred. Experience using Python, Excel, Visual Basic, Batch scripting, Windows job scheduler, SAS statistical software, MS SQL Server database design and performance tuning, and working knowledge with Oracle and DB2 is preferred. Knowledge and experience in the electric utility industry is preferred, but not required.A degree in a related field is preferred, but not required.
EducationCompletion of a Bachelor's Degree from an accredited institution that prepares the employee for the assignment.Hybrid WorkplaceSRP currently offers a hybrid workplace, which allows employees whose jobs can be performed remotely, and who have sufficient technical capability, to telework up to three days per week. Although teleworking is available, all employees must live and work in Arizona. We are taking steps to protect the health and well-being of all team members, and by following a number of health and safety protocols, to reduce the risk of the coronavirus (COVID-19).Equal Opportunity Employer StatementSalt River Project (SRP) recognizes diversity and inclusion as key drivers of innovation and growth and seeks to attract a diverse employee base that reflects our community. We are committed to equal employment opportunity regardless of race, color, religion, sex (including pregnancy), gender identity, sexual orientation, national origin, age, disability, genetic information, military status or any other protected status under applicable federal, state or local law. Ultimately, SRP aspires to fully apply the power of diversity and inclusion to build a more equitable and sustainable future for our customers, employees and community. Drug/Alcohol Policy StatementIn order to promote the safety and well-being of our employees, customers and the communities we serve, SRP is committed to maintaining a drug/alcohol free work environment. Although marijuana may now be legal in Arizona, except as otherwise specified under Arizona law, SRP considers it to be an illegal drug for the purpose of our drug/alcohol policy because marijuana remains illegal at the federal level. Any candidate found to be impaired during the hiring process or who has the presence of an illegal drug or unauthorized substance in their system during the pre-employment drug/alcohol test may be disqualified from further consideration in the hiring process.Work AuthorizationAll candidates must be legally authorized to work in the United States.Currently, SRP does not sponsor H1B visas."
Senior Data Engineer Informatica,"CTC Group, Inc","Lanham, MD (Hybrid)",https://www.linkedin.com/jobs/view/3699649066/?eBP=JOB_SEARCH_ORGANIC&refId=snJeDosRAZcNxNUnHdZOmA%3D%3D&trackingId=geq5GrEyI09iyjyk7e0gNw%3D%3D&trk=flagship3_search_srp_jobs,3699649066,"About the job
            
 
Must be US Citizen to be consideredPosition Overview: As a Senior Informatica Data Engineer, you will take charge of designing, developing, and maintaining our ETL processes, ensuring the seamless flow of data across various systems. You will collaborate with cross-functional teams to shape data solutions that support our strategic goals, mentor junior developers, and drive data integrity and quality.Responsibilities Lead the design and architecture of complex ETL solutions to transform and load data from diverse sources into target data warehouses.Support the analysis, collection, transformation and ingestion of complex datasets in support of Data Migration of mission critical systems of our clients.Design, develop, monitor, and maintain solutions on the Informatica platform.Integrate with broader technology architecture used across the organization.Evaluate, develop, test and build ETL solutions on Informatica PowerCenter based on project requirements.Analyze data issues, understand requirements, create specifications, design data workflows, provide estimates.Support environment setup, monitoring and support change management in a multi-vendor environment.Collaborate with cross-functional teams to deliver data solutions that align with business objectives.
Qualifications 7+ years of experience with Informatica PowerCenter, workflows, mappings, complex queries, scripting and tuning5+ years of experience working with Data and the Cloud environmentExperience with enterprise-level design and implementation of data migration from legacy repositories, including Access DB, SQL Server, Oracle, or PostgreSQL Experience developing technical design documentation in collaboration with functional and integration teamsExperience in bulk importing Flat files such as CSV, XML. JSONExperience with performance tuning, deployment scripts, or reusable frameworksExcellent problem-solving skills and attention to detail.Effective communication skills to collaborate with technical and non-technical stakeholders.Demonstrated ability to lead projects and mentor junior team members.Applicants selected will be subject to a government background investigation"
"Data Engineer, Data Platform",Grammarly,"South Carolina, United States (Hybrid)",https://www.linkedin.com/jobs/view/3689966167/?eBP=JOB_SEARCH_ORGANIC&refId=snJeDosRAZcNxNUnHdZOmA%3D%3D&trackingId=4%2B6iVrgK%2FmKMYVEJeCoTjA%3D%3D&trk=flagship3_search_srp_jobs,3689966167,"About the job
            
 
Grammarly is excited to offer a remote-first hybrid working model. Team members work primarily remotely in the United States, Canada, Ukraine, Germany, or Poland. Certain roles have specific location requirements to facilitate collaboration at a particular Grammarly hub.All roles have an in-person component: Conditions permitting, teams meet 2–4 weeks every quarter at one of Grammarly’s hubs in San Francisco, Kyiv, New York, Vancouver, and Berlin, or in a workspace in Kraków. This flexible approach gives team members the best of both worlds: plenty of focus time along with in-person collaboration that fosters trust and unlocks creativity.Grammarly team members in this role must be based in the United States or Canada, and they must be able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub(s) where the team is based.The opportunity Grammarly is the world’s leading AI writing assistance company trusted by over 30 million people and 70,000 professional teams every day. From instantly creating a first draft to perfecting every message, Grammarly’s product offerings help people at 96% of the Fortune 500 get their point across—and get results. Grammarly has been profitable for over a decade because we’ve stayed true to our values and built an enterprise-grade product that’s secure, reliable, and helps people do their best work—without selling their data. We’re proud to be one of Inc.’s best workplaces, a Glassdoor Best Place to Work, one of TIME’s 100 Most Influential Companies, and one of Fast Company’s Most Innovative Companies in AI.To achieve our ambitious goals, we’re looking for a Data Engineer to join our Data Engineering Platform team. This person will build highly automated, low latency core datasets that will help data engineers and end users across Grammarly to work with analytical data at scale.Grammarly’s engineers and researchers have the freedom to innovate and uncover breakthroughs—and, in turn, influence our product roadmap. The complexity of our technical challenges is growing rapidly as we scale our interfaces, algorithms, and infrastructure. Read more about our stack or hear from our team on our technical blog.Your impactAs a Data Engineer on our Data Engineering Platform team, you will: Drive improvements to make our analytics effortless by creating and adjusting core data models and storage structures, all while understanding the needs of our users. Make analytical data and metrics usable within a few minutes of real world events occuring, and build streaming processes for the output derived events and aggregate data.Model structure, storage, and access of data at very high volumes for our data lakehouse.Improve developer productivity and self-serve solutions by contributing components to our stream data processing framework(s).Own data engineering's infrastructure-as-code for provisioning services that allow our engineers to deploy mature software installations within a few hours.Build a world-class process that will allow our systems to scale.Mentor other back-end engineers on the team and help them grow.Build and contribute to AWS high-scale distributed systems on the back-end.
We’re Looking For Someone Who Embodies our EAGER values—is ethical, adaptable, gritty, empathetic, and remarkable.Is inspired by our MOVE principles, which are the blueprint for how things get done at Grammarly: move fast and learn faster, obsess about creating customer value, value impact over activity, and embrace healthy disagreement rooted in trust.Is able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub where the team is based.Has experience with Python, Scala, or Java.Has experience with designing database objects and writing relational queriesHas experience designing and standing up APIs and services.Has experience with system design and building internal tools.Has experience handling applications that work with data from data lakes.Has at least some experience building internal Admin sites.Has good knowledge of and at least some experience with AWS (or, alternatively, has deep expertise in Azure or GCE and is willing to learn AWS in a short time frame).Can knowledgeably choose an open source or third-party service to accomplish what they need or, alternatively, can devise a quick and simple solution on their own.
Support for you, professionally and personally Professional growth: We believe that autonomy and trust are key to empowering our team members to do their best, most innovative work in a way that aligns with their interests, talents, and well-being. We support professional development and advancement with training, coaching, and regular feedback.A connected team: Grammarly builds a product that helps people connect, and we apply this mindset to our own team. Our remote-first hybrid model enables a highly collaborative culture supported by our EAGER (ethical, adaptable, gritty, empathetic, and remarkable) values. We work to foster belonging among team members in a variety of ways. This includes our employee resource groups, Grammarly Circles, which promote connection among those with shared identities, such as BIPOC and LGBTQIA+ team members, women, and parents. We also celebrate our colleagues and accomplishments with global, local, and team-specific programs. 
Compensation And BenefitsGrammarly offers all team members competitive pay along with a benefits package encompassing the following and more: Excellent health care (including a wide range of medical, dental, vision, mental health, and fertility benefits)Disability and life insurance options401(k) and RRSP matching Paid parental leaveTwenty days of paid time off per year, eleven days of paid holidays per year, and unlimited sick days Home office stipendsCaregiver and pet care stipendsWellness stipendsAdmission discountsLearning and development opportunities
Grammarly takes a market-based approach to compensation, which means base pay may vary depending on your location. Our US and Canada locations are categorized into compensation zones based on each geographic region’s cost of labor index. For more information about our compensation zones and locations where we currently support employment, please refer to this page. If a location of interest is not listed, please speak with a recruiter for additional information.Base pay may vary considerably depending on job-related knowledge, skills, and experience. The expected salary ranges for this position are outlined below by compensation zone and may be modified in the future.United StatesZone 1: $167,000 - $242,000/year (USD)Zone 2: $150,000 – $218,000/year (USD)Zone 3: $142,000 – $206,000/year (USD)Zone 4: $134,000 – $194,000/year (USD)We encourage you to applyAt Grammarly, we value our differences, and we encourage all—especially those whose identities are traditionally underrepresented in tech organizations—to apply. We do not discriminate on the basis of race, religion, color, gender expression or identity, sexual orientation, ancestry, national origin, citizenship, age, marital status, veteran status, disability status, political belief, or any other characteristic protected by law. Grammarly is an equal opportunity employer and a participant in the US federal E-Verify program (US). We also abide by the Employment Equity Act (Canada).Please note that EEOC is optional and specific to US-based candidates.#NAAll team members meeting in person for official Grammarly business or working from a hub location are strongly encouraged to be vaccinated against COVID-19."
"Senior Data Engineer // Dallas, TX",Motion Recruitment,"Irving, TX (Hybrid)",https://www.linkedin.com/jobs/view/3767070916/?eBP=JOB_SEARCH_ORGANIC&refId=snJeDosRAZcNxNUnHdZOmA%3D%3D&trackingId=2Hm7yKA3Swx1pyiMtuVgzQ%3D%3D&trk=flagship3_search_srp_jobs,3767070916,"About the job
            
 
We are searching for a Senior Data Engineer for our client. This opportunity is local to Irving, TX. Ideally, they prefer this candidate go in 5-days per-week; however, for the right candidate hybrid could be considered. This is a full-time direct-hire opportunity.As for what the client does, they are an innovative industry leader within automotive restoration. They collaborate with well-known partners within both land and marine industries (sports, leisure, etc.). This company cares about their employees and offers stellar growth opportunities within.Required Skills & Experience Lengthy experience with ETLStrong abilities related to PythonUsage of Scala in recent positionsConfident in SQL abilities
Desired Skills & Experience Data WarehousingData Modeling and SQL QueryingDatabase architectureAzure- DatabricksPowerBI
What You Will Be DoingTech Breakdown 100% Data Engineering
Daily Responsibilities 80% Hands-On10% Mentorship10% Team Collaboration
The Offer Bonus eligible
You Will Receive The Following Benefits Medical, Dental, and Vision Insurance PTOStock Options
Applicants must be currently authorized to work in the US on a full-time basis now and in the future.Posted By: Katherine Spalding"
"Lead Snowflake Data Engineer - Dallas, TX",Software Technology Inc.,"Dallas, TX (Hybrid)",https://www.linkedin.com/jobs/view/3675553920/?eBP=JOB_SEARCH_ORGANIC&refId=snJeDosRAZcNxNUnHdZOmA%3D%3D&trackingId=L5OaXtfr7a2XxXvtC2bAqw%3D%3D&trk=flagship3_search_srp_jobs,3675553920,"About the job
            
 
Must Haves 12+ years of experience within data engineeringPython experienceSnowflake experience (developing ETL pipelines)AWSFluent in English (written and verbal)
Job DescriptionThis role will be part of a team focused on cloud transformation, modernizing analytics platforms and improving agility. The role requires hands-on experience in building and managing analytics solutions in Snowflake, Provide direction on adoption of Cloud technologies (Snowflake) and industry best practices in the field of Data Engineering architecture and Development.Primary Duties And Responsibilities Establish Data Engineering architecture strategy, best practices, standards, and roadmapExperience developing ETL Pipeline using Python, Snowflake and IDMC.Experience with loading batch data and streaming data via KafkaBuild Data Flows mapping Source systems and Process flows.Assemble large, complex data sets that meet non-functional and functional business requirementsPerform code reviews and assist developers in optimization and troubleshooting.Expertise in Snowflake advanced concepts like setting up resource monitors, RBAC controls, virtual warehouse sizing, query performance tuning, zero copy clone, time travel and understand how to use these featuresKnowledge in AWS and management technologies such as S3.Strong written communication skillsIs effective and persuasive in both written and oral communication
Good To Have Kafka experience (loading batch and streaming data)"
Lead Data Engineer,Zortech Solutions,"Tarrytown, NY (Hybrid)",https://www.linkedin.com/jobs/view/3667476705/?eBP=JOB_SEARCH_ORGANIC&refId=snJeDosRAZcNxNUnHdZOmA%3D%3D&trackingId=fDF277%2FQ6j9gtBEWabfO7w%3D%3D&trk=flagship3_search_srp_jobs,3667476705,"About the job
            
 
Role : Lead Data EngineerLocation : Tarrytown NY 10591 (Hybrid role 3 days onsite 2 days WFH)Duration: 6-12+ MonthsJob DescriptionMust have AWS , Apache Airflow , Pyspark , Redshift Candidate should have 12+ years of experience in Data EngineeringDesigning, creating, testing and maintaining the complete data management & processing systems.Working closely with the stakeholders & solution architect.Ensuring architecture meets the business requirements.Building highly scalable, robust & fault-tolerant systems.Taking care of the complete ETL process.Knowledge of Hadoop ecosystem and different frameworks inside it HDFS, YARN, MapReduce, Apache Pig, Hive, Flume, Sqoop, ZooKeeper, Oozie, Impala and KafkaMust have knowledge and working experience in Real-time processing Framework (Apache Spark), PySpark and in AWS RedshiftMust have experience on SQL-based technologies (e.g. MySQL/ Oracle DB) and NoSQL technologies (e.g. Cassandra and MongoDB)Should have Python/Scala/Java Programming skillsDiscovering data acquisitions opportunitiesFinding ways & methods to find value out of existing data.Improving data quality, reliability & efficiency of the individual components & the complete system.Creating a complete solution by integrating a variety of programming languages & tools together.Creating data models to reduce system complexities and hence increase efficiency & reduce cost.Introducing new data management tools & technologies into the existing system to make it more efficient.Setting & achieving individual as well as the team goal.Problem solving mindset working in agile environment"
Java UI / Data Engineer,"The Dignify Solutions, LLC","Boston, MA (Hybrid)",https://www.linkedin.com/jobs/view/3768013820/?eBP=JOB_SEARCH_ORGANIC&refId=snJeDosRAZcNxNUnHdZOmA%3D%3D&trackingId=Wyg8yWbHsYJ%2BnFXMkqdElg%3D%3D&trk=flagship3_search_srp_jobs,3768013820,"About the job
            
 
PrISM, Landing Page, DA Framework UI Java SQL Oracle or Postgres React.js/JavaScript Additional Skills - SSC specific Cloud (CDT)"
Data Integration Engineer,The Wills Group,"La Plata, MD (Hybrid)",https://www.linkedin.com/jobs/view/3774785402/?eBP=JOB_SEARCH_ORGANIC&refId=snJeDosRAZcNxNUnHdZOmA%3D%3D&trackingId=Z%2FR4f8mIerinHOzotpylag%3D%3D&trk=flagship3_search_srp_jobs,3774785402,"About the job
            
 
Join our dynamic IT team at The Wills Group, as a Data Integration Engineer. You'll play a pivotal role in operationalizing data and analytics for our digital initiatives. Responsibilities include building and optimizing data pipelines, internal analytics, and system integrations. We're seeking self-motivated candidates who thrive in a fast-paced, small team environment and enjoy utilizing data to develop creative reporting and integrations that solve business challenges.Key Responsibilities: Develop and maintain data pipelines for reporting.Create and manage system integrations using Azure Data Factory and ERP-specific tools.Collaborate with IT Solutions team to gather requirements for reporting and integrations.Maintain and enhance algorithms using Azure Machine Learning.Document system integrations.Create and maintain Power BI datasets and SQL Server Reporting Services reports.Collaborate with IT leadership to implement self-service automation and analytics.Act as an escalation point for support desk requests related to reporting and integrations.
Qualifications and Experience: Bachelor's degree in computer science, Data Management, or equivalent experience.3 years of experience in data management, integration, and optimization.3 years of hands-on experience with relational SQL databases.2 years of experience with object-oriented/object function scripting languages such as Python or R.2 years of experience with Azure data management services (Azure Data Factory or Azure Synapse); comparable AWS experience accepted.1 year of experience with Power BI.Proficiency with SQL Server Reporting Services.Working knowledge of Regular Expressions (REGEX).Strong data analysis skills and attention to detail.Excellent written and verbal communication skills.
Preferred Qualifications: Experience supporting a retail, food service, or consumer goods company in an IT role.
Availability and Travel: Core hours: Monday – Friday, 8:00 AM - 4:30 PM ETHybrid schedule: 2 days per week in-office, with the remainder remote.Occasional travel required for company events, industry events, training, and conferences.
Competencies: Proven ability to deliver results and meet customer expectations.Collaborative team player.Effective adaptation to change.Strong technological expertise.Strong analytical skills.
Why You Should Join Wills Group As a thriving, family-owned, $1.5 billion company headquartered in scenic La Plata, Maryland, (a 45-minute commute from Washington, DC), we take pride in our strong presence across the Mid-Atlantic region. Featuring nearly 300 retail locations of our family of brands including Dash In, Splash In ECO Car Wash, and SMO Motor Fuels, we are shaping the future of convenience retailing, fuels marketing, and commercial real estate.Since 1926, our work-hard, play-hard mentality propels us to serve the communities that have supported us throughout the years. Keeping lives in motion is more than our mission--it's our way of life! We're dedicated to empowering individuals to embrace new possibilities and chart their own paths to success. Discover the fulfillment of working alongside passionate professionals, where your ideas are valued, and your potential is nurtured. Become part of something bigger when you join the Wills Group!Benefits and Perks Embark on a rewarding journey where your growth, future, and well-being take center stage! As a certified Great Place to Work™, the Wills Group understands today's professionals desire meaningful careers with a culture that's as authentic as possible. We pride ourselves in fostering an environment that supports your overall development. Look forward to joining a company that celebrates your wins whether big or small. You can count on us to provide industry-leading total rewards packages that include a range of benefits and perks that contribute to your overall well-being: Financial Well-being – Employer 401(k) match (currently at 7%), health savings plan, and financial planning.Physical Well-being – Comprehensive health, vision, and dental plans tailored to meet the needs of our people and their families, even their pets!Paid Time Off – Vacation, sick, personal, community engagement, and parental leave for new parents.Work/Life Balance – Hybrid and Flexible work environment, Employee Assistance Program, travel assistance, family life planning.Education and Development Opportunities – 100% tuition reimbursement to support our people’s education goals, robust development programs, and certificate program assistance (up to 100% employer-paid).Competitive Salary - Competitive pay matched to DC Metro area.Bonus Opportunity - Up to 12.5% annually
Wills Group is an equal opportunity employer. Wills Group does not discriminate on the basis of race, religion, color, sex, gender identity, sexual orientation, age, non-disqualifying physical or mental disability, national origin, veteran status or any other basis covered by appropriate law. All employment is decided on the basis of qualifications, merit, and business need."
Data Engineer IV - Max Digital (Data Engineering),ACV Auctions,"Michigan, United States (Hybrid)",https://www.linkedin.com/jobs/view/3762875655/?eBP=JOB_SEARCH_ORGANIC&refId=snJeDosRAZcNxNUnHdZOmA%3D%3D&trackingId=ghBzjm%2BNtOT0rG%2FEJ8fXYw%3D%3D&trk=flagship3_search_srp_jobs,3762875655,"About the job
            
 
If you are looking for a career at a dynamic company with a people-first mindset and a deep culture of growth and autonomy, ACV is the right place for you! Competitive compensation packages and learning and development opportunities, ACV has what you need to advance to the next level in your career. We will continue to raise the bar every day by investing in our people and technology to help our customers succeed. We hire people who share our passion, bring innovative ideas to the table, and enjoy a collaborative atmosphere.Who we are:ACV is a technology company that has revolutionized how dealers buy and sell cars online. We are transforming the automotive industry. ACV Auctions Inc. (ACV), has applied innovation and user-designed, data driven applications and solutions. We are building the most trusted and efficient digital marketplace with data solutions for sourcing, selling and managing used vehicles with transparency and comprehensive insights that were once unimaginable. We are disruptors of the industry and we want you to join us on our journey. ACV’s network of brands includes ACV Auctions, ACV Transportation, ClearCar, MAX Digital and ACV Capital within its Marketplace Products, as well as, True360 and Data Services.At ACV we focus on the Health, Physical, Financial, Social and Emotional Wellness of our Teammates and to support this we offer:  Multiple medical plans including a high deductible health plan that costs $0 out of your paycheck  Company-sponsored (paid) Short-Term Disability, Long-Term Disability, and Life Insurance  Comprehensive optional benefits such as Dental, Vision, Supplemental Life/AD&D, Legal/ID Protection, and Accident and Critical Illness Insurance  Generous paid time off options, including vacation time, sick days, Company holidays, floating holidays, parental leave, bereavement leave, jury duty leave, voting leave, and other forms of paid leave as required by applicable law or regulation  Employee Stock Purchase Program with additional opportunities to earn stock in the Company  Retirement planning through the Company’s 401(k) 
 Who we are looking for: We are seeking a highly skilled Engineer IV in Data Engineering with a strong foundation in computer science and excellent problem-solving skills. You will be responsible for maintaining and extending our database operations, optimizing SQL queries, and designing scalable data services. What you will do:   Actively and consistently support all efforts to simplify and enhance the customer experience.  Maintain and extend (as required) existing database operations solution for backups, index defragmentation, data retention, etc.  Troubleshoot any SQL Server or ETL stack outages during our operational support window.  Triage any issues with data stack (SSIS, C#, Web APIs).  Support development, integration, and stage SQL Server environments for application development and data science teams.  Ensure that new database development meets company standards for readability, reliability, and performance. Work with internal teams on transactional and analytical schema design.  Collaborate with software and DevOps engineers to design scalable services, plan feature roll-out, and ensure high reliability and performance of our products.  Architect and build entire services including but not limited to; data modeling, storage, message brokers, protocols and interfaces.  Design, build and maintain complex systems that can scale rapidly with little maintenance.  Conduct code reviews, develop high-quality documentation, and build robust test suites.  Own the overall performance of products and services within a defined area of focus.  Be empowered to lead and complete software projects with minimal guidance from managers.  Lead team discussions to define technical requirements on new and current products.  Respond-to and troubleshoot highly complex problems quickly, efficiently, and effectively.  Mentor junior engineers.  Perform additional duties as assigned. 
 What you will need:   Bachelor's degree in Computer Science, Information Technology, Computer Information Systems, Management Information Systems, or similar  5 years' building & supporting the database-tier of SaaS web applications.  Ability to read, write, speak, and understand English. 
 Expert understanding of SQL query execution fundamentals and query optimization principles.  Experience maintaining and extending an existing codebase, adapting to pre-existing patterns and tracing the code’s path of execution.  ETL workflow implementation (SSIS, Airflow, C#, Python)  Experience working with Cloud Services (AWS RDS, S3, SQS, SNS)  Experience working with NoSQL data stores (MongoDB)  Experience writing unit and integration testing (DBT, C#)  Expert SQL and data-layer development experience; OLTP schema design.  Experience integrating 3rd-party APIs, implementing authentication & authorization and developing asynchronous data flows.  Nice to Have  OLAP schema design experience.  Experience with Airflow, Snowflake, etc.  Experience with DBT 

Our ValuesTrust & Transparency | People First | Positive Experiences | Calm Persistence | Never SettlingAt ACV, we are committed to an inclusive culture in which every individual is welcomed and empowered to celebrate their true selves. We achieve this by fostering a work environment of acceptance and understanding that is free from discrimination. ACV is committed to being an equal opportunity employer regardless of sex, race, creed, color, religion, marital status, national origin, age, pregnancy, sexual orientation, gender, gender identity, gender expression, genetic information, disability, military status, status as a veteran, or any other protected characteristic. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you have a disability or special need that requires reasonable accommodation, please let us know.For information on our collection and use of your personal information, please see our Privacy Notice.Apply Now"
Staff Data Engineer,Metropolis Technologies,"Seattle, WA (Hybrid)",https://www.linkedin.com/jobs/view/3775454911/?eBP=JOB_SEARCH_ORGANIC&refId=snJeDosRAZcNxNUnHdZOmA%3D%3D&trackingId=RpsvLU6heLKsTdgIb6Xjuw%3D%3D&trk=flagship3_search_srp_jobs,3775454911,"About the job
            
 
Location: Seattle, WA, New York, NY OR Los Angeles, CA (Hybrid)  The CompanyMetropolis is an artificial intelligence company for the real world. Metropolis' computer vision platform enables people to transact in the physical world with even greater ease than we experience online. Today, we are reimagining parking. Because it's important, it's everywhere, and impacts everyone – enabling millions of consumers to just ""drive in and drive out"" – that's it. Tomorrow, we will power ""checkout-free"" experiences anywhere you go.Position Overview and Responsibilities At Metropolis, we are building the next-gen end-to-end data ecosystem including Data Ingestion, Data Warehousing, Data Integration, Analytic Data Products and Machine Learning capabilities. We are looking for a hands-on thought leader to drive the technical vision, design and development of the data systems. If you are excited about working with Data APIs, Data Vault and Dimensional data modeling, Serverless Cloud platforms, Data Pipeline, Data Lake, ML Ops, etc. you might be interested in this opportunity. In this role, you will help define and build the data tech stack including Snowflake, AWS Redshift, Databricks, Python, Scala, SQL, SparkSQL, Talend, Airflow, AWS Glue, Tableau, AWS Sagemaker. You will play a key and significant role in enabling the Metropolis business teams to leverage data to meet business goals. Key Responsibilities Collaborate with Application and Engineering teams to build a strong understanding of the source data systemsLeverage your understanding of the source data systems to design and build conceptual, logical and physical data models for the Data Warehouse, Data Marts and MDM repositories.Collaborate with the analytics and business teams to architect and implement performant data solutions for Analytics and ML business use cases.Design and lead the implementation of frameworks and best practices for data ingestion, data integration and ETL processes.Design and lead the implementation of data quality framework including data quality metrics and continuous data quality monitoring, assessment and resolution.Create business data catalog and/or data dictionaries to document data lineages, data definitions and metadata for data domains.Assist with establishing best practices and standards for data privacy and data security.Work closely with data engineering, analytics, business and offshore/onshore consulting teams to ensure alignment on architecture and design.Provide technical oversight and mentor development teamsHands-on development to support DW and ETL initiatives.
Requirements and Qualifications Bachelor’s degree in a STEM discipline or related field10+ years of relevant hands-on experience in data and analytics domain / teams.Expert level proficiency in SQL5+ years of demonstrated experience in building relational and dimensional data warehouse data models. Experience with Data Vault methodology would be a plus. 5+ years experience in data ingestion (batch, streaming, API, etc.) and data integration (ETL) development using Informatica, Talend, AWS Glue or similar.3+ years experience working with cloud data warehouses such as Snowflake, AWS Redshift, Azure, BigQuery or similar. Proficiency in Snowflake is strongly preferred. 2+ programming experience developing data solutions in Python, Java, Scala or similar2+ years Agile / Scrum experience including participating in daily sprints, backlog grooming and program increments Demonstrated ability to adapt to new data technologies and learn quickly Ability to communicate across all levels of the organization and work with diverse project teams.Preferred local to Santa Monica, CA, Seattle, WA, or New York City, NY. Hybrid working environment (3 days in office per week). Other locations considered on a case-by-case basis.
When you join Metropolis, you’ll join a team of world-class product leaders and engineers, building an ecosystem of technologies at the intersection of parking, mobility, and real estate. Our goal is to build an inclusive culture where everyone has a voice and the best idea wins. You will play a key role in building and maintaining this culture as our organization grows. The anticipated base salary for this position is $180,000.00 to 220,000.00 annually. The actual base pay offered is determined by a number of variables, including, as appropriate, the applicant's qualifications for the position, years of relevant experience, distinctive skills, level of education attained, certifications or other professional licenses held, and the location of residence and/or place of employment. Base pay is one component of Metropolis’s total compensation package, which may also include access to or eligibility for healthcare benefits, a 401(k) plan, short-term and long-term disability coverage, basic life insurance, a lucrative stock option plan, bonus plans and more."
Sr. Data Engineer (Remote),Chamberlain Group,"Oak Brook, IL (Hybrid)",https://www.linkedin.com/jobs/view/3699221246/?eBP=JOB_SEARCH_ORGANIC&refId=snJeDosRAZcNxNUnHdZOmA%3D%3D&trackingId=3oHKuChAre80neHIm%2Fo3eQ%3D%3D&trk=flagship3_search_srp_jobs,3699221246,"About the job
            
 
If you are a current Chamberlain Group employee, please click here to apply through your Workday account.Chamberlain Group is a global leader in access solutions with top brands, such as LiftMaster and Chamberlain, found in millions of homes, businesses, and communities worldwide.As a leader in the Smart Home industry, we boast one of the largest IoT install bases, with innovative products consisting of cameras, locks, card readers, garage door openers, gates and more, all powered by our myQ digital ecosystem.This role is responsible for providing technical expertise and leadership to design and deliver end-to-end data engineering solutions to support advanced analytics capabilities and drive innovation and decision-makingacross Chamberlain.Essential Duties And Responsibilities Build and maintain real-time and batch data pipelines across the advanced analytics platform.Design, develop and orchestrate highly robust and scalable ETL pipelines.Design and implement Dimensional and NoSQL data modelling as per the business requirements.Develop highly optimal codebase and perform Spark optimizations for Big Data use cases.Design, develop and deploy optimal monitoring and testing strategy for the data products.Collaborate with stakeholders and advanced analytics business partners to understand business needs and translate requirements into scalable data engineering solutions.Collaborate with data scientists to prepare data for model development and production.Collaborate with data visualization and reporting application developers to ensure the sustainability of production applications and reports.Collaborate with data architects on the enhancement of Chamberlain’s enterprise data architecture and platforms.Provide leadership to third-party contractors.Comply with health and safety guidelines and rules.Protect CGI’s reputation by keeping information confidential.Maintain professional and technical knowledge by attending educational workshops, professional publications, establishing personal networks, and participating in professional societies.
Minimum QualificationsEducation/Certifications: Bachelor’s degree in computer science or related quantitative field of study
Experience: 4+ years of professional experience
Knowledge, Skills, and Abilities: Natural sense of urgency, teamwork, and collaboration reflected in daily work ethic.Proficient in Spark or Databricks, Cloud Data Engineering Services preferably Azure, Streaming frameworks like Event Hubs or Kafka.Proficient in Microsoft Office.Familiarity with modern Machine Learning Operationalization techniques.Agile methodologies.Familiarity with Data visualization tools, such as Qlik or Power BI.
Preferred QualificationsEducation/Certifications: Master’s degree in computer science or related quantitative field of study
Experience: 4+ years of professional experience2+ years of professional experience delivering engineering for advanced analytics or data science solutions
Knowledge, Skills, and Abilities: Agile methodologiesExperience with IoT Data Architecture.Machine Learning Operationalization (MLOps) proficiency.REST API design and development.Proficiency with streaming design patterns.
Chamberlain Group wants all of its employees to succeed and encourages people of all backgrounds to apply. We’re proud to be an Equal Opportunity Employer, and you’ll be considered for this role regardless of race, color, religion, sex, national origin, age, sexual orientation, ancestry; marital, disabled or veteran status. We’re committed to fostering an environment where people of all lived experiences feel welcome.Persons with disabilities who anticipate needing accommodations for any part of the application process may contact, in confidence Recruiting@Chamberlain.com.NOTE: Staffing agencies, headhunters, recruiters, and/or placement agencies, please do not contact our hiring managers directly."
"Sr Data Center Network Engineer || Hampton, NH (Hybrid Role)",Steneral Consulting,"Hampton, NH (Hybrid)",https://www.linkedin.com/jobs/view/3639594226/?eBP=JOB_SEARCH_ORGANIC&refId=snJeDosRAZcNxNUnHdZOmA%3D%3D&trackingId=AURh0sVogYCLmYCUZDeyWw%3D%3D&trk=flagship3_search_srp_jobs,3639594226,"About the job
            
 
Onsite 3 days a week, must be under 60 mins commuteNeed LinkedIn  Must have extensive experience within data center environmentsOther core skills Next Generation FabricsCisco ACI Experience. This is a Must have requirement. They have to have experience working with it not just studying it or using it in a lab.JuniperRouting Protocol Knowledge (OSPF, BG and MP-BGP)VAR experience"
Sr. Data Engineer,Alliant Credit Union,"Chicago, IL (Hybrid)",https://www.linkedin.com/jobs/view/3772773871/?eBP=JOB_SEARCH_ORGANIC&refId=snJeDosRAZcNxNUnHdZOmA%3D%3D&trackingId=RkxDWDINPc%2FNT9OLcHicEA%3D%3D&trk=flagship3_search_srp_jobs,3772773871,"About the job
            
 
In this role, you will provide continuous build-up and operationalization of an enterprise-class modern data environment, which may include various components within the Azure, Hadoop, SQL server, and Informatic stack. Works with AI/ML, LLM technologies encompassed within the cloud and on-prem data management technology stack while also having knowledge and capabilities as a systems developer. Coordinates, designs, builds, and integrates complex application technology solutions, aligned to architectural standards and definitions and ensures IT services are delivered effectively and efficiently.Responsibilities Development, operation and support of modern data environments (AI/ML, LLM) such as new concepts, practices, and approaches.Design, build, deploy and maintain data pipelines using modern data tools on batch and streaming data in relational and non-relational databases in cloud and on-prem.Collaborate with different teams on infrastructure setup, testing, monitoring, tuning/optimizing, troubleshooting and maintenance.Collaborate with development and strategy teams on component and software vendor services in the cloud and on-prem, recommendation, installation and management of cloud-native and on prem jobs.Collaborate with the data teams in technical investigations, development, and prototypes and corporate IT function around integrating data management ecosystem(s) with critical enterprise systems.Develop and manage data management testing activities and create roadmaps for ongoing data management technology and growth.Perform capacity monitoring and capacity planning on infrastructure and cloud resources.Manage activities through the implementation and maintenance of security and governance components across various data protocols.Participate in the design and implementation of a disaster recovery strategy for modern data components and the alignment activities with pertinent audit and compliance activities.Provide input and develop new processes and standards in support of the organization's business/functional short-term strategies.Address data-related problems in regard to systems integration, compatibility, and multiple-platform integration.Support existing data pipelines on-prem, cloud and business engineering extract.Provide technical support to meet application service level agreements. 
 Education Bachelors Degree - Computer Science or Related - Minimum 
Years Of Experience 5 Years - Data warehouse, data lake, cloud technology or related - Minimum3 Years - AI/ML, LLM - Preferred 
In Lieu Of Education 8 years - Data warehouse, data lake, cloud technology 
You Will Benefit From Competitive medical, dental, and free vision benefitsPaid parental leaveCompetitive compensation planGym memberships discountsGenerous PTO and banking holidays offTuition reimbursement401k with immediate employer match and vesting 
Adhere to and ensure compliance of all business transactions with policy and process of the Bank Secrecy Act. Ensures compliance with all applicable state and federal laws, company procedures and policies. Maintains integrity and ethics in all actions and conversations with or regarding credit union members and their accounts; complies with Privacy Act directives.The responsibilities listed do not contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this position. Duties, responsibilities and activities may change at any time with or without notice."
Senior Data Engineer,Lineage,"Novi, MI (Hybrid)",https://www.linkedin.com/jobs/view/3772244884/?eBP=JOB_SEARCH_ORGANIC&refId=snJeDosRAZcNxNUnHdZOmA%3D%3D&trackingId=%2F1DML0e%2BbeC8JM500zVNyw%3D%3D&trk=flagship3_search_srp_jobs,3772244884,"About the job
            
 
A Senior Data Engineer is accountable for the design and implementation of Lineage’s data assets. Specifically, this includes being responsible for developing normalized, dimensional, and hybrid data models to support reporting, analytics, and APIs used by other internal and external applications.The Data Engineer will focus on the following:  Data Ingestion - moving source system data to central data assets Data Modeling - determining field-level data usage Data Wrangling - identifying queries/algorithms to efficiently transform data from source system formats to a unified RDBMS schema Data Quality - ensuring complete and uninterrupted data mappings from source to central data assets
Job Description:A Senior Data Engineer is accountable for the design and implementation of Lineage’s data assets. Specifically, this includes being responsible for developing normalized, dimensional, and hybrid data models to support reporting, analytics, and APIs used by other internal and external applications. The Data Engineer will focus on the following:Data Ingestion - moving source system data to central data assets Data Modeling - determining field-level data usage Data Wrangling - identifying queries/algorithms to efficiently transform data from source system formats to a unified RDBMS schema Data Quality - ensuring complete and uninterrupted data mappings from source to central data assetsAt Lineage, we leverage cloud services, best-of-breed technologies, and external teams to augment our capabilities with sufficient capacity. This means that in addition to being a great technologist and individual contributor, you need to be able to review and collaborate with partners to ensure that we get the same quality from them too.As a company, Lineage Logistics leverages state-of-the-art technology to meet our customers’ needs, assist in our international growth, and create an infrastructure that enables Lineage to lead the industry.As the ideal candidate for this position, you will exhibit the technical expertise, business acumen and flexibility required to successfully work with all levels of Lineage’s organization.Essential Job Functions: Perform detailed data analysis and translate business requirements into logical and physical data modelsLead complex technical data discussions on modeling, integration, and overall technology solutions and designPossess the ability to a create robust, scalable, and sustainable data architecture that supports requirements and provides for future expansionIntegrate additional data sets into the Operational Data Store to support organizational needsRe-engineer existing physical data structures and recommend efficiencies as it relates to data storage and retrievalAutomate, monitor, and support data mappings, and re-design if required to make them faster and/or more reliableDesign and implement system components which reconcile and audit the results of the extract/transform or conversion of data from source systems to the target structuresManage and provide guidance to employee and contract resourcesWork closely with others to help ensure the physical design meets the functional and performance requirements of the applications utilizing the dataCollaborate with report and API developers on data mapping and report / application design
Requirements:Possess prior experience in designing and building data ingestion and storage solutionsExhibit knowledge related to full life cycle experience in Data Modeling, Data Integration, and Master Data ManagementArchitect and implement data structures for reporting solutionsMastery of SQL with ability to perform analytical queries as well as updates to data setsGrow and maintain a broad and deep knowledge of data technologies and techniquesPossess a willingness to be coached and proactively seek feedbackHandle multiple assignments/projects simultaneouslyProperly set priority and order for workWork in a fast-paced, agile environmentAsk for help and provide it to othersUnderstand your skills and your talents and apply them appropriatelyLearn and apply new technologies and business conceptsMaintain personal accountability for getting things done on time and with quality resultsDemonstrate excellent verbal and written communication skills including the ability to explain technical concepts to business leadersBe willing and able to travel on occasion (Education and/or Experience: 5+ years - Data Engineering / Modeling experience; delivering both logical models and physical designs with experience translating business needs into data requirementsA degree in Computer Science, a related discipline, or equivalent work experience of 6+ years overall IT or the equivalent combination of education, training and experience from which comparable skills can be acquiredExperience with cloud-based data platformsExperience with a mix of relational database technologies like Microsoft SQL-Server, Oracle, PostgreSQL, MySQL, etc.
Preferred Skills: Previous Supply Chain or Logistics industry experience strongly preferredExperience working with data from the following technologies is a plus: ERP, WMS, TMS, CRM, HRISPrior software development experience is also preferred
Why Lineage?This is an excellent position to begin your career path within Lineage! Success in this role enables greater responsibilities and promotions! A career at Lineage starts with learning about our business and how each team member plays a part each and every day to satisfy our customers’ requirements. Beyond that, you’ll help us grow and learn on our journey to be the very best employer in our industry. We’ll ask you for your opinion and ensure we do our part to keep you developing and engaged as we grow our business. Working at Lineage is energizing and enjoyable. We value respect and care about our team members.Lineage is an Equal Employment Opportunity Employer and is committed to compliance with all federal, state, and local laws that prohibit workplace discrimination and unlawful harassment and retaliation. Lineage will not discriminate against any applicant on the basis of race, color, age, national origin, religion, physical or mental disability or any other protected status under federal, state and local law.BenefitsLineage provides safe, stable, reliable work environments, medical, dental, and basic life and disability insurance benefits, 401 retirement plan, paid time off, annual bonus eligibility, and a minimum of 7 holidays throughout the calendar year."
"Data Engineer, Data Platform",Grammarly,"North Carolina, United States (Hybrid)",https://www.linkedin.com/jobs/view/3689961982/?eBP=JOB_SEARCH_ORGANIC&refId=snJeDosRAZcNxNUnHdZOmA%3D%3D&trackingId=sk28jzEK4DFWZHZvH%2B1wOQ%3D%3D&trk=flagship3_search_srp_jobs,3689961982,"About the job
            
 
Grammarly is excited to offer a remote-first hybrid working model. Team members work primarily remotely in the United States, Canada, Ukraine, Germany, or Poland. Certain roles have specific location requirements to facilitate collaboration at a particular Grammarly hub.All roles have an in-person component: Conditions permitting, teams meet 2–4 weeks every quarter at one of Grammarly’s hubs in San Francisco, Kyiv, New York, Vancouver, and Berlin, or in a workspace in Kraków. This flexible approach gives team members the best of both worlds: plenty of focus time along with in-person collaboration that fosters trust and unlocks creativity.Grammarly team members in this role must be based in the United States or Canada, and they must be able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub(s) where the team is based.The opportunity Grammarly is the world’s leading AI writing assistance company trusted by over 30 million people and 70,000 professional teams every day. From instantly creating a first draft to perfecting every message, Grammarly’s product offerings help people at 96% of the Fortune 500 get their point across—and get results. Grammarly has been profitable for over a decade because we’ve stayed true to our values and built an enterprise-grade product that’s secure, reliable, and helps people do their best work—without selling their data. We’re proud to be one of Inc.’s best workplaces, a Glassdoor Best Place to Work, one of TIME’s 100 Most Influential Companies, and one of Fast Company’s Most Innovative Companies in AI.To achieve our ambitious goals, we’re looking for a Data Engineer to join our Data Engineering Platform team. This person will build highly automated, low latency core datasets that will help data engineers and end users across Grammarly to work with analytical data at scale.Grammarly’s engineers and researchers have the freedom to innovate and uncover breakthroughs—and, in turn, influence our product roadmap. The complexity of our technical challenges is growing rapidly as we scale our interfaces, algorithms, and infrastructure. Read more about our stack or hear from our team on our technical blog.Your impactAs a Data Engineer on our Data Engineering Platform team, you will: Drive improvements to make our analytics effortless by creating and adjusting core data models and storage structures, all while understanding the needs of our users. Make analytical data and metrics usable within a few minutes of real world events occuring, and build streaming processes for the output derived events and aggregate data.Model structure, storage, and access of data at very high volumes for our data lakehouse.Improve developer productivity and self-serve solutions by contributing components to our stream data processing framework(s).Own data engineering's infrastructure-as-code for provisioning services that allow our engineers to deploy mature software installations within a few hours.Build a world-class process that will allow our systems to scale.Mentor other back-end engineers on the team and help them grow.Build and contribute to AWS high-scale distributed systems on the back-end.
We’re Looking For Someone Who Embodies our EAGER values—is ethical, adaptable, gritty, empathetic, and remarkable.Is inspired by our MOVE principles, which are the blueprint for how things get done at Grammarly: move fast and learn faster, obsess about creating customer value, value impact over activity, and embrace healthy disagreement rooted in trust.Is able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub where the team is based.Has experience with Python, Scala, or Java.Has experience with designing database objects and writing relational queriesHas experience designing and standing up APIs and services.Has experience with system design and building internal tools.Has experience handling applications that work with data from data lakes.Has at least some experience building internal Admin sites.Has good knowledge of and at least some experience with AWS (or, alternatively, has deep expertise in Azure or GCE and is willing to learn AWS in a short time frame).Can knowledgeably choose an open source or third-party service to accomplish what they need or, alternatively, can devise a quick and simple solution on their own.
Support for you, professionally and personally Professional growth: We believe that autonomy and trust are key to empowering our team members to do their best, most innovative work in a way that aligns with their interests, talents, and well-being. We support professional development and advancement with training, coaching, and regular feedback.A connected team: Grammarly builds a product that helps people connect, and we apply this mindset to our own team. Our remote-first hybrid model enables a highly collaborative culture supported by our EAGER (ethical, adaptable, gritty, empathetic, and remarkable) values. We work to foster belonging among team members in a variety of ways. This includes our employee resource groups, Grammarly Circles, which promote connection among those with shared identities, such as BIPOC and LGBTQIA+ team members, women, and parents. We also celebrate our colleagues and accomplishments with global, local, and team-specific programs. 
Compensation And BenefitsGrammarly offers all team members competitive pay along with a benefits package encompassing the following and more: Excellent health care (including a wide range of medical, dental, vision, mental health, and fertility benefits)Disability and life insurance options401(k) and RRSP matching Paid parental leaveTwenty days of paid time off per year, eleven days of paid holidays per year, and unlimited sick days Home office stipendsCaregiver and pet care stipendsWellness stipendsAdmission discountsLearning and development opportunities
Grammarly takes a market-based approach to compensation, which means base pay may vary depending on your location. Our US and Canada locations are categorized into compensation zones based on each geographic region’s cost of labor index. For more information about our compensation zones and locations where we currently support employment, please refer to this page. If a location of interest is not listed, please speak with a recruiter for additional information.Base pay may vary considerably depending on job-related knowledge, skills, and experience. The expected salary ranges for this position are outlined below by compensation zone and may be modified in the future.United StatesZone 1: $167,000 - $242,000/year (USD)Zone 2: $150,000 – $218,000/year (USD)Zone 3: $142,000 – $206,000/year (USD)Zone 4: $134,000 – $194,000/year (USD)We encourage you to applyAt Grammarly, we value our differences, and we encourage all—especially those whose identities are traditionally underrepresented in tech organizations—to apply. We do not discriminate on the basis of race, religion, color, gender expression or identity, sexual orientation, ancestry, national origin, citizenship, age, marital status, veteran status, disability status, political belief, or any other characteristic protected by law. Grammarly is an equal opportunity employer and a participant in the US federal E-Verify program (US). We also abide by the Employment Equity Act (Canada).Please note that EEOC is optional and specific to US-based candidates.#NAAll team members meeting in person for official Grammarly business or working from a hub location are strongly encouraged to be vaccinated against COVID-19."
Senior Data Engineer,Aspen Dental,"Chicago, IL (Hybrid)",https://www.linkedin.com/jobs/view/3764793121/?eBP=JOB_SEARCH_ORGANIC&refId=UGLS5oOcuSte7Lbn4dE1nQ%3D%3D&trackingId=97HV01uQbJVL0UENZNAtWg%3D%3D&trk=flagship3_search_srp_jobs,3764793121,"About the job
            
 
The Aspen Group (TAG) is one of the largest and most trusted retail healthcare business support organizations in the U.S., supporting 15,000 healthcare professionals and team members at more than 1,000 health and wellness offices across 46 states in three distinct categories: Dental care, urgent care, and medical aesthetics. Working in partnership with independent practice owners and clinicians, the team is united by a single purpose: to prove that healthcare can be better and smarter for everyone. ADMI provides a comprehensive suite of centralized business support services that power the impact of four consumer-facing businesses: Aspen Dental, ClearChoice Dental Implant Centers, WellNow Urgent Care, and Chapter Aesthetic Studio. Each brand has access to a deep community of experts, tools, and resources to grow its practices and an unwavering commitment to delivering high-quality consumer healthcare experiences at scale. As a part of the data engineering team, you will join one of our teams focused on a core business domain – clinical operation that generates revenue and claim. You will work alongside developers modernizing our legacy systems by developing new software and services using cloud-native GCP technologies. Our stack uses MS-SQL, SSIS, GCP Big Query, and Python, using advanced SQL and NoSQL data layers on Kubernetes operating as an active-active multi-region solution.R2023-017821"
"Data Engineer, Data Platform",Grammarly,"Minnesota, United States (Hybrid)",https://www.linkedin.com/jobs/view/3689966136/?eBP=JOB_SEARCH_ORGANIC&refId=UGLS5oOcuSte7Lbn4dE1nQ%3D%3D&trackingId=6sLD38%2Bw9vBMBma8AAXqSQ%3D%3D&trk=flagship3_search_srp_jobs,3689966136,"About the job
            
 
Grammarly is excited to offer a remote-first hybrid working model. Team members work primarily remotely in the United States, Canada, Ukraine, Germany, or Poland. Certain roles have specific location requirements to facilitate collaboration at a particular Grammarly hub.All roles have an in-person component: Conditions permitting, teams meet 2–4 weeks every quarter at one of Grammarly’s hubs in San Francisco, Kyiv, New York, Vancouver, and Berlin, or in a workspace in Kraków. This flexible approach gives team members the best of both worlds: plenty of focus time along with in-person collaboration that fosters trust and unlocks creativity.Grammarly team members in this role must be based in the United States or Canada, and they must be able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub(s) where the team is based.The opportunity Grammarly is the world’s leading AI writing assistance company trusted by over 30 million people and 70,000 professional teams every day. From instantly creating a first draft to perfecting every message, Grammarly’s product offerings help people at 96% of the Fortune 500 get their point across—and get results. Grammarly has been profitable for over a decade because we’ve stayed true to our values and built an enterprise-grade product that’s secure, reliable, and helps people do their best work—without selling their data. We’re proud to be one of Inc.’s best workplaces, a Glassdoor Best Place to Work, one of TIME’s 100 Most Influential Companies, and one of Fast Company’s Most Innovative Companies in AI.To achieve our ambitious goals, we’re looking for a Data Engineer to join our Data Engineering Platform team. This person will build highly automated, low latency core datasets that will help data engineers and end users across Grammarly to work with analytical data at scale.Grammarly’s engineers and researchers have the freedom to innovate and uncover breakthroughs—and, in turn, influence our product roadmap. The complexity of our technical challenges is growing rapidly as we scale our interfaces, algorithms, and infrastructure. Read more about our stack or hear from our team on our technical blog.Your impactAs a Data Engineer on our Data Engineering Platform team, you will: Drive improvements to make our analytics effortless by creating and adjusting core data models and storage structures, all while understanding the needs of our users. Make analytical data and metrics usable within a few minutes of real world events occuring, and build streaming processes for the output derived events and aggregate data.Model structure, storage, and access of data at very high volumes for our data lakehouse.Improve developer productivity and self-serve solutions by contributing components to our stream data processing framework(s).Own data engineering's infrastructure-as-code for provisioning services that allow our engineers to deploy mature software installations within a few hours.Build a world-class process that will allow our systems to scale.Mentor other back-end engineers on the team and help them grow.Build and contribute to AWS high-scale distributed systems on the back-end.
We’re Looking For Someone Who Embodies our EAGER values—is ethical, adaptable, gritty, empathetic, and remarkable.Is inspired by our MOVE principles, which are the blueprint for how things get done at Grammarly: move fast and learn faster, obsess about creating customer value, value impact over activity, and embrace healthy disagreement rooted in trust.Is able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub where the team is based.Has experience with Python, Scala, or Java.Has experience with designing database objects and writing relational queriesHas experience designing and standing up APIs and services.Has experience with system design and building internal tools.Has experience handling applications that work with data from data lakes.Has at least some experience building internal Admin sites.Has good knowledge of and at least some experience with AWS (or, alternatively, has deep expertise in Azure or GCE and is willing to learn AWS in a short time frame).Can knowledgeably choose an open source or third-party service to accomplish what they need or, alternatively, can devise a quick and simple solution on their own.
Support for you, professionally and personally Professional growth: We believe that autonomy and trust are key to empowering our team members to do their best, most innovative work in a way that aligns with their interests, talents, and well-being. We support professional development and advancement with training, coaching, and regular feedback.A connected team: Grammarly builds a product that helps people connect, and we apply this mindset to our own team. Our remote-first hybrid model enables a highly collaborative culture supported by our EAGER (ethical, adaptable, gritty, empathetic, and remarkable) values. We work to foster belonging among team members in a variety of ways. This includes our employee resource groups, Grammarly Circles, which promote connection among those with shared identities, such as BIPOC and LGBTQIA+ team members, women, and parents. We also celebrate our colleagues and accomplishments with global, local, and team-specific programs. 
Compensation And BenefitsGrammarly offers all team members competitive pay along with a benefits package encompassing the following and more: Excellent health care (including a wide range of medical, dental, vision, mental health, and fertility benefits)Disability and life insurance options401(k) and RRSP matching Paid parental leaveTwenty days of paid time off per year, eleven days of paid holidays per year, and unlimited sick days Home office stipendsCaregiver and pet care stipendsWellness stipendsAdmission discountsLearning and development opportunities
Grammarly takes a market-based approach to compensation, which means base pay may vary depending on your location. Our US and Canada locations are categorized into compensation zones based on each geographic region’s cost of labor index. For more information about our compensation zones and locations where we currently support employment, please refer to this page. If a location of interest is not listed, please speak with a recruiter for additional information.Base pay may vary considerably depending on job-related knowledge, skills, and experience. The expected salary ranges for this position are outlined below by compensation zone and may be modified in the future.United StatesZone 1: $167,000 - $242,000/year (USD)Zone 2: $150,000 – $218,000/year (USD)Zone 3: $142,000 – $206,000/year (USD)Zone 4: $134,000 – $194,000/year (USD)We encourage you to applyAt Grammarly, we value our differences, and we encourage all—especially those whose identities are traditionally underrepresented in tech organizations—to apply. We do not discriminate on the basis of race, religion, color, gender expression or identity, sexual orientation, ancestry, national origin, citizenship, age, marital status, veteran status, disability status, political belief, or any other characteristic protected by law. Grammarly is an equal opportunity employer and a participant in the US federal E-Verify program (US). We also abide by the Employment Equity Act (Canada).Please note that EEOC is optional and specific to US-based candidates.#NAAll team members meeting in person for official Grammarly business or working from a hub location are strongly encouraged to be vaccinated against COVID-19."
Data Engineer IV - Max Digital (Data Engineering),ACV Auctions,"Texas, United States (Hybrid)",https://www.linkedin.com/jobs/view/3762880105/?eBP=JOB_SEARCH_ORGANIC&refId=UGLS5oOcuSte7Lbn4dE1nQ%3D%3D&trackingId=wngf3tdPvbUqxD%2BsfbBtGw%3D%3D&trk=flagship3_search_srp_jobs,3762880105,"About the job
            
 
If you are looking for a career at a dynamic company with a people-first mindset and a deep culture of growth and autonomy, ACV is the right place for you! Competitive compensation packages and learning and development opportunities, ACV has what you need to advance to the next level in your career. We will continue to raise the bar every day by investing in our people and technology to help our customers succeed. We hire people who share our passion, bring innovative ideas to the table, and enjoy a collaborative atmosphere.Who we are:ACV is a technology company that has revolutionized how dealers buy and sell cars online. We are transforming the automotive industry. ACV Auctions Inc. (ACV), has applied innovation and user-designed, data driven applications and solutions. We are building the most trusted and efficient digital marketplace with data solutions for sourcing, selling and managing used vehicles with transparency and comprehensive insights that were once unimaginable. We are disruptors of the industry and we want you to join us on our journey. ACV’s network of brands includes ACV Auctions, ACV Transportation, ClearCar, MAX Digital and ACV Capital within its Marketplace Products, as well as, True360 and Data Services.At ACV we focus on the Health, Physical, Financial, Social and Emotional Wellness of our Teammates and to support this we offer:  Multiple medical plans including a high deductible health plan that costs $0 out of your paycheck  Company-sponsored (paid) Short-Term Disability, Long-Term Disability, and Life Insurance  Comprehensive optional benefits such as Dental, Vision, Supplemental Life/AD&D, Legal/ID Protection, and Accident and Critical Illness Insurance  Generous paid time off options, including vacation time, sick days, Company holidays, floating holidays, parental leave, bereavement leave, jury duty leave, voting leave, and other forms of paid leave as required by applicable law or regulation  Employee Stock Purchase Program with additional opportunities to earn stock in the Company  Retirement planning through the Company’s 401(k) 
 Who we are looking for: We are seeking a highly skilled Engineer IV in Data Engineering with a strong foundation in computer science and excellent problem-solving skills. You will be responsible for maintaining and extending our database operations, optimizing SQL queries, and designing scalable data services. What you will do:   Actively and consistently support all efforts to simplify and enhance the customer experience.  Maintain and extend (as required) existing database operations solution for backups, index defragmentation, data retention, etc.  Troubleshoot any SQL Server or ETL stack outages during our operational support window.  Triage any issues with data stack (SSIS, C#, Web APIs).  Support development, integration, and stage SQL Server environments for application development and data science teams.  Ensure that new database development meets company standards for readability, reliability, and performance. Work with internal teams on transactional and analytical schema design.  Collaborate with software and DevOps engineers to design scalable services, plan feature roll-out, and ensure high reliability and performance of our products.  Architect and build entire services including but not limited to; data modeling, storage, message brokers, protocols and interfaces.  Design, build and maintain complex systems that can scale rapidly with little maintenance.  Conduct code reviews, develop high-quality documentation, and build robust test suites.  Own the overall performance of products and services within a defined area of focus.  Be empowered to lead and complete software projects with minimal guidance from managers.  Lead team discussions to define technical requirements on new and current products.  Respond-to and troubleshoot highly complex problems quickly, efficiently, and effectively.  Mentor junior engineers.  Perform additional duties as assigned. 
 What you will need:   Bachelor's degree in Computer Science, Information Technology, Computer Information Systems, Management Information Systems, or similar  5 years' building & supporting the database-tier of SaaS web applications.  Ability to read, write, speak, and understand English. 
 Expert understanding of SQL query execution fundamentals and query optimization principles.  Experience maintaining and extending an existing codebase, adapting to pre-existing patterns and tracing the code’s path of execution.  ETL workflow implementation (SSIS, Airflow, C#, Python)  Experience working with Cloud Services (AWS RDS, S3, SQS, SNS)  Experience working with NoSQL data stores (MongoDB)  Experience writing unit and integration testing (DBT, C#)  Expert SQL and data-layer development experience; OLTP schema design.  Experience integrating 3rd-party APIs, implementing authentication & authorization and developing asynchronous data flows.  Nice to Have  OLAP schema design experience.  Experience with Airflow, Snowflake, etc.  Experience with DBT 

Our ValuesTrust & Transparency | People First | Positive Experiences | Calm Persistence | Never SettlingAt ACV, we are committed to an inclusive culture in which every individual is welcomed and empowered to celebrate their true selves. We achieve this by fostering a work environment of acceptance and understanding that is free from discrimination. ACV is committed to being an equal opportunity employer regardless of sex, race, creed, color, religion, marital status, national origin, age, pregnancy, sexual orientation, gender, gender identity, gender expression, genetic information, disability, military status, status as a veteran, or any other protected characteristic. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you have a disability or special need that requires reasonable accommodation, please let us know.For information on our collection and use of your personal information, please see our Privacy Notice.Apply Now"
Sr. Data Engineer,Ekodus INC.,"Jersey City, NJ (Hybrid)",https://www.linkedin.com/jobs/view/3682464088/?eBP=JOB_SEARCH_ORGANIC&refId=UGLS5oOcuSte7Lbn4dE1nQ%3D%3D&trackingId=Z6Po0wDmO4Mcw%2Fa%2B4g7OOw%3D%3D&trk=flagship3_search_srp_jobs,3682464088,"About the job
            
 
Only GC or US Citizen needed for thisSr. Data Engineer (Contract)Onsite - New Jersy City  A minimum of ten (10) years of IT experience in data engineering or data management field8 - 10 years of experience in application development using SQL, PLSQL, Python, and shell scriptingStrong working knowledge and experience in Oracle, SQL Server, Snowflake, or any SQL databaseRecent experience as a Senior Data Engineer in any public cloud, preferably on Azure as well as on a cloud warehouse like Snowflake or Azure Synapse is required5+ years of experience in data warehousing and building data pipelines using various ETL and ELT toolsHands-on experience with Snowflake, DBT (Data build tool), and Airflow is highly preferredExperience in building cloud-native solutions, preferably on Azure including the use of DevOps CI / CD tools and containers.Familiarity with Familiarity with data modeling tools/techniques is a plusAzure or Snowflake training/certification is a plusStrong analytical, quantitative, problem-solving, communication, and organizational skillsSelf-driven, self-directed, passionate analytical, and focused on delivering the right results
Please share resume at msharma@ekodusinc.com or career@ekodusinc.com."
Principal Data Engineer,Alexander Technology Group,"Boston, MA (Hybrid)",https://www.linkedin.com/jobs/view/3774327626/?eBP=JOB_SEARCH_ORGANIC&refId=UGLS5oOcuSte7Lbn4dE1nQ%3D%3D&trackingId=zX%2BXl4IuLzl9g5xP4yiTpw%3D%3D&trk=flagship3_search_srp_jobs,3774327626,"About the job
            
 
If interested, please send a copy of your most updated resume to iostberg@alexandertg.com along with your best availability for a phone call.Principal Data EngineerDowntown Boston: Hybrid (3 days/week onsite)Perm FTEOur client is actively searching for a full-time Principal Data Engineer to become an integral part of their Data team. The Principal Data Engineer will play a very critical role within the data team by providing technical leadership, implementation expertise, design best practices and guiding other highly skilled and motivated data engineers. In this capacity, you will work closely with the analytics team, architecture, and data analysts to gather requirements and contribute to the development of a best-in-class analytics platform centered around Snowflake and Tableau.This is a hybrid work environment with a weekly in-office schedule of Tuesdays, Wednesdays, and Thursdays with remote work availability on Mondays and Fridays.Responsibilities Build best in class analytics platform for our business teams to gain insights from our dataCollaborate with Product owners, Analytics Lead, Scrum Master, and Development teams to build effective data integration pipelines and Rest APIs on hosted dataMentor new or junior team members and participate in code reviewsCollaborate in design, development, and implementation of software and data solutionsCollaborate with development team in building innovative, event driven, API-first cloud native solutions using AWS platform, Snowflake, Snaplogic, Tableau, Python and Rest APIsiteratively deliver high quality products to enable our investment business, operations, analytics, and Client reporting
Skills You Bring: A Bachelor’s or Master’s degree in computer science or a similar field5+ years of software and or data engineering experienceStrong implementation experience with snowflake ecosystem, knowledge of Snowflake architecture and conceptsStrong background in modeling and building data warehousesStrong working experience in Snowflake using warehouses, stored procedures, streams, snow pipes, tasks, stages, storage integration, ingestion frameworks and tools etc.Ability to develop ELT/ETL pipelines to move data to and from Snowflake data store using combination of Python, dbt and SnowSQLStrong experience in ETL/ELT technologies such as informatica, Snaplogic etc.Hands on experience building cloud native applications using AWS or AzureGood hands-on experience with scheduling tools such as Airflow, Control M, AutosysExcellent SQL skills, writing SQL stored procedures and functionsExperience in leading teams, propose design, establish best practices and engineering disciplineExperience building reports, dashboards, and visualizations with business intelligence tools such as Tableau or Power BI is preferredExperience with build/deploy automation & DevOps frameworksFamiliarity in building and consuming APIs (REST, API Gateway, etc.)Possessing outstanding technical and analytical skills with strong business knowledge in the financial services domainGood project management, experience in Agile methodologies (Kanban and SCRUM), communication, and interpersonal skillsStrong hands-on experience with requirements gathering analysis, coding, testing, implementation, maintenance, and review
Candidates coming out of Investment Services or Capital Management are highly encouraged to apply!If interested, please send a copy of your most updated resume to iostberg@alexandertg.com along with your best availability for a phone call.ATG456 MONATG"
Data Engineer,Motion Recruitment,"Pittsburgh, PA (Hybrid)",https://www.linkedin.com/jobs/view/3737590602/?eBP=JOB_SEARCH_ORGANIC&refId=UGLS5oOcuSte7Lbn4dE1nQ%3D%3D&trackingId=hG4F2mQoKbGFLpaRKii3rQ%3D%3D&trk=flagship3_search_srp_jobs,3737590602,"About the job
            
 
Based out of Pittsburgh, PA this global investment firm is helping their diverse customer base achieve their entrepreneur dreams. With one on one mentorships, global network connections and customized planning they can advance innovative team and ideas no matter the industry.They have already helped fund over 3,300 start ups and are hiring Data Engineers to scale their pipelines in order to support their increased customer base.Requirements 6+ years of relevant Data Engineering experience 5+ years working with Python Must have streaming data experience (Kafka is preferred) AWS Snowflake Airflow 
The Offer Competitive Salary & Bonus Eligible 
Benefits Hybrid office set up (3 days a week in Pittsburgh office) 401K Health Benefits Paid Time Off 90 days of parental leave 
Posted By: Caroline Stranieri"
Data Engineer/Senior Data Engineer,University of Virginia,"Charlottesville, VA (Hybrid)",https://www.linkedin.com/jobs/view/3760214767/?eBP=JOB_SEARCH_ORGANIC&refId=UGLS5oOcuSte7Lbn4dE1nQ%3D%3D&trackingId=keBRwSYd6MkpQ5NOOr%2F8cg%3D%3D&trk=flagship3_search_srp_jobs,3760214767,"About the job
            
 
The University of Virginia Darden School of Business, one of the world's leading business schools, seeks a highly skilled and motivated Data Engineer/Senior Data Engineer to join its Strategic IT Data & Analytics team. This role will play a crucial role in building and maintaining our data infrastructure, ensuring the efficient extraction, loading and transformation (ELT) of data from various sources into our cloud-based data environments. The ideal candidate should have hands-on experience with ELT processes in cloud data platforms such as Databricks, Snowflake, or Microsoft Synapse. If you are passionate about data engineering, have strong analytical skills, a passion for higher education and enjoy working in a collaborative environment, we would love to hear from you. While the position is based in Charlottesville, VA, we offer the opportunity for the employee to work remotely for the majority of the year. However, we do require that the employee be onsite for one week every quarter.To be considered for a data engineer role, responsibilities include: Assist in designing and implementing efficient and scalable data pipelines using cloud-based ELT technologies (Microsoft Data Factory, Databricks, etc.) to support our data processing and analytics needs. Collaborating with cross-functional teams, including data analytics developers, functional analysts, and business stakeholders, to understand data requirements and ensure the smooth integration of data sources. Assist in building models, data integration workflows, and data transformation processes to support data analysis, reporting, and visualization. Developing, maintaining and orchestrating data pipelines and data processing workflows, ensuring data quality, reliability, and performance. Identifying and addressing performance bottlenecks and data quality issues in collaboration with the data operations team. Monitoring and troubleshooting data pipelines to ensure high availability, scalability, and optimal performance. Assist in implementing data governance and security measures to ensure compliance with data protection regulations and industry best practices. Keeping up-to-date with emerging trends, design patterns and technologies in data engineering and cloud-based data processing to drive continuous improvement and innovation within the organization. 
In addition to the above, Senior Data Engineer responsibilities include: Utilizing more fully developed technical skills and emerging advanced skills, to take on data engineering projects that are more complex.Designing scalable and efficient data architectures. Optimizing data pipelines, data integration workflows, and data transformation processes Leading the implementation of data governance practices, ensuring compliance with data protection regulations and industry standards. Providing guidance and mentorship to junior team members 
Education And Professional Experience Data Engineer - Bachelor's degree in Computer Science, MIS, Computer Engineering or related discipline with at least five years of directly related experience. Relevant experience may be considered in lieu of a degree.Senior Data Engineer - Bachelor's degree in Computer Science, MIS, Computer Engineering or a related discipline with at least seven years of directly related experience. Relevant experience may be considered in lieu of a degree.Certification in Databricks and/or Snowlake strongly preferred, or demonstrated progress to complete certification. Proven work experience as a Data Engineer or in a similar role, with a focus on developing and implementing data pipelines and ELT processes. Strong proficiency in working with cloud-based data platforms such as Microsoft Synapse, Databricks, or Snowflake, and experience with ELT processes. Hands-on experience with programming languages such as SQL, Python, and Scala for data manipulation, scripting, and automation. Proficiency in data modeling, database design, and SQL optimization techniques. Solid understanding of data warehousing concepts, dimensional modeling, and data integration techniques. Familiarity with data governance, data security, and privacy practices. Strong analytical and problem-solving skills, with the ability to work with large and complex datasets. Knowledge and experience working with APIs and web services. Familiarity with incorporating continuous integration/continuous deployment (CI/CD) development practices into data engineering workflows. Experience with big data technologies (e.g., Hadoop, Spark) and distributed computing frameworks is a plus. Excellent communication and collaboration skills, with the ability to work effectively in a team-oriented environment. 
To ApplyPlease apply through Workday, and search for R0051124. Internal applicants must apply through their UVA Workday profile by searching 'Find Jobs'.Complete The Application, And Upload The Following Required Materials CV/ResumeCover letter
Please note: Multiple documents can be submitted into the CV/Resume field. Alternatively, merge all documents into one PDF for submission. Applications that do not contain all required documents will not receive full consideration.For questions about the application process, please contact Jon Freeman, Academic Recruiter jf2sw@virginia.edu.For more information about UVA and the Charlottesville community please see http://www.virginia.edu/life/charlottesville and https://embarkcva.com/.The University of Virginia, including the UVA Health System which represents the UVA Medical Center, Schools of Medicine and Nursing, UVA Physician’s Group and the Claude Moore Health Sciences Library, are fundamentally committed to the diversity of our faculty and staff. We believe diversity is excellence expressing itself through every person's perspectives and lived experiences. We are equal opportunity and affirmative action employers. All qualified applicants will receive consideration for employment without regard to age, color, disability, gender identity or expression, marital status, national or ethnic origin, political affiliation, race, religion, sex (including pregnancy), sexual orientation, veteran status, and family medical or genetic information."
Senior Data Engineer,Fusion HCR,"Las Vegas, NV (Hybrid)",https://www.linkedin.com/jobs/view/3694583600/?eBP=JOB_SEARCH_ORGANIC&refId=UGLS5oOcuSte7Lbn4dE1nQ%3D%3D&trackingId=vRTERvWQqynMzEcVirNMrA%3D%3D&trk=flagship3_search_srp_jobs,3694583600,"About the job
            
 
Fusion HCR is Hiring! We are looking for a Senior Data Engineer to support our client working with the latest Microsoft Business Intelligence stack and Google Computing Platform (Informix is a huge plus). Full-time role with 401k, bonuses, paid benefits for you and your family and much more! If you meet the qualifications, please apply or reach out to me directly!Position OverviewThe primary responsibility of the Senior Data Engineer is to create and support data centric solutions, providing business value from corporate data assets. The Data Engineer is expected to maintain expertise across an application technology stack (Microsoft SQL Server BI) as well as a technology domain (BI/DW and Data Analytics) and have the ability to embrace and leverage new technologies while working effectively with other information technology professionals and business users to ensure that the data solutions are stable, efficient and responsive to business needs.ResponsibilitiesDesign, build, deliver and support Data Warehouse and ETL structures and solutions Act as an internal consultant, providing architecture, vision, problem anticipation, and problem solving for the assigned project(s), as well as a Subject Matter Expertise for data and analytic users. Provide senior level application and on-call support for data and integration solutions. Participate and at times lead project teams within an agile environment. Successfully engage in multiple initiatives concurrently, including application and on call support, minor projects, major projects, functional requirements, systems specifications and subject matter expertise. Prepare clear and concise documentation for delivered solutions and processes, integrating documentation with corporate knowledgebase.Additional ResponsibilitiesIn partnership with the Product Management team, provide subject matter expertise for determining business requirements to address business opportunities or issues across business functions for data centric solutions. Maintain familiarity with technological trends and innovations in the data warehousing and data management domains. Interpret and satisfy business needs by building and enhancing systems to transform, cleanse and provision corporate data assets in a governed, secure and high-performance manner. Provide subject matter expertise for data and analytics. Participate in the ongoing Data Management maturation process. Safety is an essential function of this job. Consistent and regular attendance, including on-call availability on a rotational basis, is an essential function of this job.Minimum Qualifications 21 years of age. Proof of authorization/eligibility to work in the United States. Bachelor’s degree or equivalent in relevant discipline.Must be able to obtain and maintain Nevada Gaming Control Board Registration and any other certification or license, as required by law or policy. 4 years ETL experience using SQL Server SSIS 2008r2-2014. 5 years with T-SQL programming. Must understand Relational and Dimensional database modeling. Must exhibit a high level of mastery of the Microsoft BI Stack, including: SSIS, SSRS, SSAS, SharePoint, PowerPivot, etc. Should be experienced in automated file handling with ETL tools via FTP and other means. Should have administrative experience with SQL Server 2008-2012. 
Additional Qualifications Prefer experience with key concepts of Data Management such as Data Quality and MDM. Experience with Data Preparation for Data Science a plus. Experience with Big Data, Advanced Analytic techniques and Real Time data a plus. Must be willing and capable of adopting new technologies and paradigms such as Big Data technologies (Hadoop, hive, etc.), event based analytics, etc. Experience with analytic tools such as Spotfire, Tableau, SAS, Cognos a plus. Prefer TFS and Agile experience (SCRUM). Prefer gaming and hospitality experience. Must be able to adhere to SOPs and methodologies, and be willing to improve them when necessary."
Sr. Data Engineer,Chubb,"Philadelphia, PA (Hybrid)",https://www.linkedin.com/jobs/view/3756333790/?eBP=JOB_SEARCH_ORGANIC&refId=UGLS5oOcuSte7Lbn4dE1nQ%3D%3D&trackingId=Q9IiFI%2FI8NF7qe0O%2FUbdcQ%3D%3D&trk=flagship3_search_srp_jobs,3756333790,"About the job
            
 
Job DescriptionWe are looking for an experienced and motivated Senior ETL Developer to join our dynamic team. In this role, you will lead the delivery of key projects in support of North America financial reporting from the enterprise data warehouse and associated data marts, including the Business Analytics Repository (BAR). BAR is a strategic application within the business, feeding into multiple systems and applications both up and downstream with data that directly supports business decisions being made each and every day. You will be responsible for leading ETL development projects, coordinating with cross-functional teams to ensure project success, and creating and maintaining data integration solutions to meet business requirements. The ideal candidate will have experience with ETL development solutions such as AWS Glue, Google Dataflow, Azure Data Factory, Snowflake, Informatica/IICS and be able to identify and resolve data quality issues, performance bottlenecks, and other ETL-related problems.Responsibilities Lead ETL development projects and coordinate with cross-functional teams to ensure project successCreate and maintain data integration solutions to meet business requirementsIdentify and resolve data quality issues, performance bottlenecks, and other ETL-related problemsDesign and develop scalable ETL workflows and data pipelines using ETL tools such as Informatica/IICSEnsure compliance with data governance and security policiesDevelop and maintain documentation such as technical design documents, data lineage, and ETL runbooksMentor junior ETL developers and provide technical guidance to the teamEvaluate modern technologies and tools, and recommend solutions to improve ETL processes and performanceContribute to the architecture, design, and development of data warehousing and business intelligence solutionsUnderstands data mapping and data modeling methodologies including normal form, star, and snowflake to reduce data redundancy and improve data integrity.Participate in analysis, design, and ETL development as part of Agile development methodologies and provide status updates to the managementMaintains knowledge on current and emerging developments/trends for assigned area(s) of responsibility, assesses the impact, and collaborates with Scrum Team and Leadership to incorporate current trends and developments in current and future solutions
Qualifications 5 Year/bachelor’s degree or equivalent work experience (4 years of experience in lieu of Bachelors)_Minimum Required in Computer Science, Computer Information Systems, Information Systems, Information Technology or Computer Engineering or equivalent work experienceAt least 5+ years of Strong understanding of ETL development concepts and tools such as ETL development solutions (e.g., AWS Glue, Google Dataflow, Azure Data Factory, Snowflake, Informatica/IICS) Experience with Data Warehousing and Business Intelligence concepts and technologiesStrong knowledge of SQL and advanced programming languages such as Python and JavaDemonstrated critical thinking skills and the ability to identify and resolve data quality issues, performance bottlenecks, and other ETL-related problemsExperience with Agile methodologies and project-management skillsExcellent communication and interpersonal skillsAbility to mentor and provide technical guidance to junior ETL developersExperience with cloud based environment required.2+ years of experience in scheduling jobs using Autosys (or comparable distributed scheduler)3+ years of experience writing Unix/Linux or Windows Scripts in tools such as PERL, Shell script, Python, etc.3+ years of experience in creating complex technical specifications from business requirements/specifications
About UsChubb is the world’s largest publicly traded property and casualty insurer. With operations in 54 countries, Chubb provides commercial and personal property and casualty insurance, personal accident and supplemental health insurance, reinsurance, and life insurance to a diverse group of clients. The company is distinguished by its extensive product and service offerings, broad distribution capabilities, exceptional financial strength, underwriting excellence, superior claims handling expertise and local operations globally.At Chubb, we are committed to equal employment opportunity and compliance with all laws and regulations pertaining to it. Our policy is to provide employment, training, compensation, promotion, and other conditions or opportunities of employment, without regard to race, color, religious creed, sex, gender, gender identity, gender expression, sexual orientation, marital status, national origin, ancestry, mental and physical disability, medical condition, genetic information, military and veteran status, age, and pregnancy or any other characteristic protected by law. Performance and qualifications are the only basis upon which we hire, assign, promote, compensate, develop and retain employees. Chubb prohibits all unlawful discrimination, harassment and retaliation against any individual who reports discrimination or harassment."
Sr Streaming Data Engineer,Discount Tire,"Scottsdale, AZ (Hybrid)",https://www.linkedin.com/jobs/view/3731267961/?eBP=JOB_SEARCH_ORGANIC&refId=UGLS5oOcuSte7Lbn4dE1nQ%3D%3D&trackingId=2qzd0hcfB4Oa%2BVNn1XZlYQ%3D%3D&trk=flagship3_search_srp_jobs,3731267961,"About the job
            
 
OverviewHere at Discount Tire, we celebrate the spirit of our people with extraordinary pride and enthusiasm. Our business has been growing for more than 60 years and now is the best time in our history to join us. We recognize that to remain the industry leader we must continue to grow and evolve our business in a rapidly changing industry. We are achieving this, not only by opening new stores, but by transforming our technological landscape and making data a central component of our strategy. The Business Analytics team, one of the fastest growing teams in the company, is leading this change. We are responsible for driving the insights, recommendations, and developing the decision support tools that influence the strategic direction of the company.Under minimal supervision, the Analytics Senior Streaming Data Engineer provides hands-on technical expertise in designing, building, and implementing analytics solutions that support scalable data solutions in a real-time environment. This role is essential to drive the company's data initiatives, ensuring the successful design and execution of high-performing and robust streaming data systems.Essential Duties and Responsibilities: Drives Discount Tire’s real time analytics to support operational insights and decision-making leveraging established engineering disciplines. Collaborate with source data owners, data engineers, data scientists, and other stakeholders to implement high-velocity data pipelines. Work with other product team(s) to define data dictionary, data lineage and relationship metrics. Contributes to the strategic direction and decision making related to Data Warehousing, Big Data and Data analytics in close collaboration with Technical Leadership.Designs and develops high performance distributed data warehouse, distributed analytic systems and cloud architecture.Develops, launches and maintains efficient and fault tolerant, batch and streaming, data pipelines (ETL/ELT) to populate databases and object stores from multiple disparate data sources.Develops complex data calculations through data integration tools and scripting languages.Designs and implements data quality metrics, standards, guidelines; automates data quality checks / routines as part of data processing frameworks; validates flow of information.Determines Data Warehousing and Big Data infrastructure needs, including but not limited to, automation of system builds, security requirements, performance requirements and logging/monitoring in collaboration with DevOps engineers.Troubleshoots complex data and performance related issues; implements adjustments, documents root cause and corrective measure; transfers knowledge to operations support team.Documents standards, best practices and technical specifications. Facilitates in design and code review sessions and provides feedback and mentorship to peers.Provides technical assistance to junior data engineers.Collaborates with broader analytics team and internal IT partners.Assists employees, vendors or other customers by answering questions related to Data Warehousing and Big Data processes, procedures and services.Participates in the development of complex cross application architectures in collaboration with cross functional teams.Implement advanced analytics and data science models and automates complex analysis at scale.Stays current on the latest industry technologies, trends and strategies.Completes work in a timely and accurate manner while providing exceptional customer service.Provides Tier 3 support and create run books to mentor Tier 1 and Tier 2 support staff.Other duties as assigned.
Qualifications: This position requires a minimum of five years of progressive streaming data development and integration experience. Proven understanding of logical and physical data modeling is imperative. Ability to translate a logical data model into a relational or non-relational solution is necessary. Understanding of multiple relational (RDMS) and non-relational (NoSQL) data platforms is needed. Expert level SQL experience is required. Advanced scripting knowledge with SQL, Python, Java or R is necessary. Advanced experience in SQL tuning, indexing, distribution, partitioning, data access patterns and scaling strategies is needed. Hands-on / in-depth experience in the following tools, technologies & Concepts such as: SQL (advanced level), Apache Kafka, Apache Flink, Kinesis Data Analytics or related streaming technologies. Expert experience with data integrations and data processing for business intelligence and analytics workloads is required. Advanced experience with AWS S3 or other distributed object stores, MPP Data Warehousing, (e.g. AWS Redshift), Elastic MapReduce a plus. Hands-on experience in database development using views, SQL scripts and transformations is needed. Proficient with Microsoft office, including skills with Word and Excel, is necessary. Experience working with large complex data sets. Understanding of Software Development Life Cycle (SDLC) methodologies such as Agile and Waterfall is needed. Proven analytical problem solving and decision making skills is critical. Proven ability to communicate across all levels of the organization is necessary; must be able to clearly articulate technical ideas to a non-technical audience both verbally and in writing. Ability to work independently and in a team is vital. Customer service skills including the ability to manage and respond to different customer situations while maintaining a positive and friendly attitude is essential. The ability to multi-task, and manage multiple projects to meet various deadlines simultaneously is required. The ability to work efficiently and accurately under pressure, meet deadlines and present a professional demeanor is essential. In addition troubleshooting and organizational skills with a can-do attitude and the ability to adjust to changing requirements are essential. 
Educational Requirements: This position requires a Bachelors Degree in Computer Science, Computer Information Systems or related or equivalent experience.Data or cloud related certifications are a plus.
Discount Tire provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state, or local law."
Sr Data Science / Sr AI Engineer,Maarut Inc,"New York, NY (Hybrid)",https://www.linkedin.com/jobs/view/3769973451/?eBP=JOB_SEARCH_ORGANIC&refId=UGLS5oOcuSte7Lbn4dE1nQ%3D%3D&trackingId=UMSqrEaPjQQaSGKSbaK3LA%3D%3D&trk=flagship3_search_srp_jobs,3769973451,"About the job
            
 
Overall 12+ Years into Information Technology or Data science.Specific Skills: Prompting LLMs, API programming, automating/orchestrating LLM-applications, preprocessing/embedding text, retrieval augmented generation (RAG)Specific Technologies: Python, Jupyter Notebook, GitHub, vector databases, Docker containers, LangChainResponsibilities Develop on-premise GenAI applications, leveraging open-source or proprietary large language models (LLMs) and following responsible AI practices Write Python code in Jupyter Notebook for data preprocessing, feature extraction, API calls, and application orchestration Integrate GenAI application components like vector databases, API endpoints, document databases, and other endpoints for full end-to-end solution Configure LLM libraries for use on on-premise servers Coordinate with IT Technology Team to troubleshoot and optimize application components Maintaining code repositories in GitHub, ensuring proper versioning and collaboration Deploy and evaluate AI models and related infrastructure Actively participate in Agile ceremonies and collaborate with cross-functional teams, providing regular updates to the Product Owner 
RequirementsQualifications: Degree in computer science, data science, analytics, or similar discipline Working knowledge of Python NLP/LLM libraries, GitHub, Jupyter Notebook, vector databases, API endpoints, and containerization Previous experience delivering AI or GenAI applications within a large financial services organization (i.e., banking or insurance) Basic understanding of NLP models, including a key concepts like embeddings and the inner workings of LLMs Demonstrated ability to debug and evaluate the performance of GenAI models Strong familiarity with Agile methodologies and practices for efficient project delivery"
Sr. Data Engineer,Chubb,"Philadelphia, PA (Hybrid)",https://www.linkedin.com/jobs/view/3756338059/?eBP=JOB_SEARCH_ORGANIC&refId=UGLS5oOcuSte7Lbn4dE1nQ%3D%3D&trackingId=d8qma23qOccaBBpMqHaQxA%3D%3D&trk=flagship3_search_srp_jobs,3756338059,"About the job
            
 
Job DescriptionAs a Sr. Data Engineer on our team, you will work on implementing complex data projects focusing on collecting, parsing, managing, analyzing, and visualizing large sets of data to turn information into insights using multiple platforms. We will look to you to be able to decide on hardware and software design needs and act according to the decisions as well as develop a proofs of concept for the selected solutions.We are looking for someone with a strong background in computer programming, data analysis, and visual analytics who is eager to tackle problems with large, complex datasets using the latest skills. You are a self-starter who will take ownership of your projects and deliver high-quality data-driven analytics solutions. You can solve diverse business problems using various tools, strategies, algorithms, and programming languages.In This Role, You Will Utilize the data engineering skills within and outside of the developing Chubb information ecosystem for discovery, analytics, and data managementWork with the data SME, Business Analyst team to design the solutionsYou will be using strong SQL skills to convert one 'raw' data from XML to a structured formatWork with various relational and non-relational data sources with the target being Azure-based SQL Data WarehouseSourcing data from underwriting applications, profiling, cleansing, and conforming to create master data sets for analytics useClean, unify, and organize messy and complex data sets for easy access, different levels of abstractions, and analysisHands-on data preparation activities using the SSIS package developmentPartner with the Global Data Analytics team to navigate the discovery solutions for data ingestionWork closely with the Data Science team to perform complex analytics and data preparation tasksBuild, deploy, and support applications, services, and APIs that meet business requirements in a highly complex technical environmentCoordinate with business stakeholders, business analysts, and project stakeholders to ensure proper assignments of a user story that meets business needs and SLAsInterfaces with business areas and other Chubb IT teams to coordinate work, manage cross-functional dependencies, and foster collaborationEnsuring application stability, and participating in resolving incidents and problems.Leverage your technical insights to stay on top of technical trends, evaluate new technologies, and develop proofs of concept for incorporation into our infrastructure
Qualifications Bachelor’s degree in Computer Science or any other IT-related field, or equivalent work experienceMinimum of 8-10 years of IT experience in a comparable role as a Data Engineer/Data AnalystNice to have experience in the P&C insurance/Life Insurance/ Investment/Banking domainExperience working in agile environments with application teamKnowledge of DMBS’ with the ability to write SQL queriesHands experience in Microsoft SQL server Database management and SSIS package developmentNice to have experience with R, Python, and data visualization tools like Qlik Sense/Power BI/Tableau/AlteryxExperience with DevOps tools, Jenkins and Jira, or equivalent Agile toolsNice to have experience in .NET or any equivalent technology stacksStrong analytical skills and coordination skills to achieve resultsAbility to identify, understand, and communicate business needsModels company values; demonstrates high integrity; meets commitmentsDemonstrates a sense of urgency and accountability; sets priorities and acts on key issues
About UsChubb is the world’s largest publicly traded property and casualty insurer. With operations in 54 countries, Chubb provides commercial and personal property and casualty insurance, personal accident and supplemental health insurance, reinsurance, and life insurance to a diverse group of clients. The company is distinguished by its extensive product and service offerings, broad distribution capabilities, exceptional financial strength, underwriting excellence, superior claims handling expertise and local operations globally.At Chubb, we are committed to equal employment opportunity and compliance with all laws and regulations pertaining to it. Our policy is to provide employment, training, compensation, promotion, and other conditions or opportunities of employment, without regard to race, color, religious creed, sex, gender, gender identity, gender expression, sexual orientation, marital status, national origin, ancestry, mental and physical disability, medical condition, genetic information, military and veteran status, age, and pregnancy or any other characteristic protected by law. Performance and qualifications are the only basis upon which we hire, assign, promote, compensate, develop and retain employees. Chubb prohibits all unlawful discrimination, harassment and retaliation against any individual who reports discrimination or harassment."
Sr Data Engineer (contract),Tundra Technical Solutions,"Dallas, TX (Hybrid)",https://www.linkedin.com/jobs/view/3718229270/?eBP=JOB_SEARCH_ORGANIC&refId=UGLS5oOcuSte7Lbn4dE1nQ%3D%3D&trackingId=ivv3vkLfUBLN4A2TYnF%2Fyg%3D%3D&trk=flagship3_search_srp_jobs,3718229270,"About the job
            
 
Position: Sr Data EngineerLocation: Irving, Dallas (onsite role)Duration: 4 Months (extendable)Project/Role Overview: Our client is currently onboarding North America based OPCO’ s (Operating companies) to their new digital platform (Spark) which is built on Kubernetes running in AZURE, the customer and associate experience (CX/AX) on their new SPARK platform in EMEA. They are planning to migrate North American operating companies (OPCOs) to the platform, beginning in 2024.With this integration work the engineer will be responsible for designing, configuring, and implementing data validation tools which will validate the data transferred between. You will ensure best practices are followed through the implementation lifecycle and help construct data sets for use by the team and other business stakeholders to power both reporting and validation.Specific Job DescriptionJob responsibilities Highly skilled and motivated Data EngineerAs a Data Engineer, you will be responsible for validating the data between two systems, ensuring accuracy and consistency.Implement data quality checks and monitoring processes.Design and Develop data entity validation scripts using python.Validate and reconcile data between two systems, ensuring accuracy and consistency.Collaborate with cross-functional teams to identify and resolve data discrepancies and issues.Develop and implement data quality checks and monitoring processes to ensure data integrity.Design and maintain data pipelines and ETL processes to extract, transform, and load data from various sources.Optimize data storage and retrieval processes to improve performance and efficiency.Perform data analysis and profiling to identify data quality issues and recommend solutions.Develop and maintain documentation for data validation processes, data mappings, and data lineage.Stay up to date with industry trends and best practices in data engineering and data validation.Support migration of North American OPCOs, enabling data validation on the new platform.Gather business requirements from key client stakeholders and translate these into technical tracking specifications based on standards and industry best practice.
Required Skills Strong knowledge of IPAAS and data validation for migration of applications.Demonstrable experience in delivering end to end data validation.Confident understanding of web development and its languages and possess Python proficiency.Proactive and highly organized with strong time management and planning skills.Able to change direction and work on multiple projects across a wide range of topics.[Desired] Knowledge of Python/Google BigQuery architecture.
Job 67248"
Senior Staff AI Data Engineer,Recruiting from Scratch,"Phoenix, AZ (Hybrid)",https://www.linkedin.com/jobs/view/3773087763/?eBP=JOB_SEARCH_ORGANIC&refId=UGLS5oOcuSte7Lbn4dE1nQ%3D%3D&trackingId=0ed6pqLyvDEO88Vnhtv9TQ%3D%3D&trk=flagship3_search_srp_jobs,3773087763,"About the job
            
 
Who is Recruiting from Scratch : Recruiting from Scratch is a premier talent firm that focuses on placing the best product managers, software, and hardware talent at innovative companies. Our team is 100% remote and we work with teams across the United States to help them hire. We work with companies funded by the best investors including Sequoia Capital, Lightspeed Ventures, Tiger Global Management, A16Z, Accel, DFJ, and more.https://www.recruitingfromscratch.com/This is a hybrid role based in our Palo Alto or San Francisco offices and will require you to be in office Tuesdays and Thursdays.What’s so interesting about this role?We believe that AI can revolutionize the dating industry. Our Data Engineer lead is responsible for building high quality ML datasets at scale, used to train ML models that power AI-centric features. In this pivotal role, you will have the opportunity to build foundational tools and data pipelines to ingest, normalize and clean the valuable data that would be fundamental for our ML engineers to build AI tools including recommendations, LLMs, ads, visual search, growth/notifications, trust and safety.What’s the job?We’re looking for an exceptional data engineer who is passionate about data for AI and values it can bring to our company, Who loves working with data ops at scale; and who is committed to the hard work necessary to continuously improve our ML data pipelines.In this position, you will be responsible for establishing and executing the strategy for our organization’s ML Data Engine, with an initial focus on agile ML Data OPs. This includes identification of infrastructure components and data stack to be used, design and implementation of pipelines between data systems and teams, automation workflows, data enrichment and monitoring tools all for AI models. As a tech lead specialized in data engineering, you are expected to code and contribute to the stack.Responsibilities:  Dive into our dataset and design, implement and scale data pre/post processing pipelines of ML models  Work on applied ML solutions in the areas of data mining, cleaning, normalizing and modeling  Be self-motivated in seeking solutions when the correct path isn’t always known  Collaborate with engineers in conceptualizing, planning and implementing data engineering initiatives working with different stakeholders  Design and build data platforms & frameworks for processing high volumes of data, in real time as well as batch, that will be used across engineering teams  Build data processing streams for cleaning and modeling text data for LLMs  Research and evaluate new technologies in the big data space to guide our continuous improvement  Collaborate with multi-functional teams to help tune the performance of large data applications  Work with Privacy and Security team on data governance, risk and compliance initiatives  Work on initiatives to ensure stability, performance and reliability of our data infrastructure 
What We’ll Love About You  Bachelors in Computer Science, Mathematics, Physics, or a related fields  5+ years of experience as a data engineer building production-level pre/post-processing data pipelines for ML/DL models, including 2+ years of technical leadership experience  Experience in statistical analysis & visualization on datasets using Pandas or R  Experience designing and building highly available, distributed systems of data extraction, ingestion, normalization and processing of large data sets in real time as well as batch, that will be used across engineering teams using orchestration frameworks like Airflow, KubeFlow or other pipeline tools  Demonstrated prior experience in creating data pipelines for text data sets NLP/ large language models  Ability to produce well-engineered software, including appropriate automated test suites, technical documentation, and operational strategy  Excellent coding skills in Python, Java, bash, SQL, and expertise with Git version control  Experience using big data technologies (Snowflake, Airflow, Kubernetes, Docker, Helm, Spark, pySpark)  Experience with any public cloud environment - AWS, GCP or Azure  Significant experience with relational databases and query authoring (SQL) as well as NoSQL databases like DynamoDB etc  Experience building and maintaining ETL (managing high-quality reliable ETL pipelines) 
We’ll really swoon if you have   2+ years of experience of technical leadership in building data engineering pipelines for AI  Previous experience in building data pipeline for conversational AI APIs and recommender systems  Experience with distributed systems and microservices  Experience with Kubernetes and building Docker images  Experience with building stream-processing systems, using solutions such as Kafka, Storm or Spark-Streaming  Strong understanding of applied machine learning topics  Be familiar with legal compliance (with data management tools) data classification, and retention  Consistent track record of managing and implementing complex data projects 
What You'll Love About Us  Mission and Impact: We are the world-leading LGBTQ social networking service. Your role will impact the lives of millions of LGBTQ people around the world  Multiple Locations: We are hiring someone for this role to be based ideally in San Francisco or Palo Alto  Family Insurance: Insurance premium coverage for health, dental, and vision for you and partial coverage for your dependents  Retirement Savings: Generous 401K plan with 6% match and immediate vest in the US  Compensation: Industry-competitive compensation and eligibility for company bonus and equity programs  Queer-Inclusive Benefits: Industry-leading gender-affirming offerings with up to 90% cost coverage, access to Included Health, monthly stipends for HRT, and more  Additional Benefits: Flexible vacation policy, monthly stipends for cell phone, internet, wellness, and food, one-time home-office setup stipend, and company-sponsored events 
Base Pay Range$160,000—$280,000 USDhttps://www.recruitingfromscratch.com/"
Sr. Data Engineer,Diverse Lynx,"Plano, TX (Hybrid)",https://www.linkedin.com/jobs/view/3764419990/?eBP=JOB_SEARCH_ORGANIC&refId=UGLS5oOcuSte7Lbn4dE1nQ%3D%3D&trackingId=hx4xVtEzyc1u%2B%2BvgbLIKIA%3D%3D&trk=flagship3_search_srp_jobs,3764419990,"About the job
            
 
Technical/Functional Skills Primary – Cloud Based ETL Tool, Snowflake (Cloud DB), Snaplogic , DBT, Strong SQL, Unix/Python, Control-M Experience Required 10&plus; Years Roles & Responsibilities Role DescriptionAnalyze requirements and existing resources to Propose, create ETL designs and database objectsWork with project and business analyst leads in order to develop and clarify in-depth technical requirements including logical and physical data modeling activitiesDesign and implement ETL processes for data transactions related to Enterprise Data Warehouse, Operational Data Store (ODS), and other data structures to support our Business Intelligence operationsDevelops, enhances, debugs, supports, maintains and tests software applications that support business units or supporting functions using IBM Infosphere Data Stage ETL or any other cloud based ETL tool both ETL and ELT approaches. These application program solutions may involve diverse development platforms, software, hardware, technologies and tools.Must have hands-on on Snowflake development environment with all SQL operations. Must be aware of ELT approach as well.Participates in the design, development and implementation of complex applications, often using IBM Infosphere Information Server (IIS) products like Data Stage, Quality Stage on a Linux Grid environment. Control-M/Scheduling tools.Required Skills:10&plus; Yrs Relevant IT software experience (Technical) in ETL Datastage or any other cloud based ETL Tool development Experience with databases like Snowflake (Cloud DB), Oracle, Netezza, MS SQL Server 2012&plus;, DB2 and MS AccessExperience with job automation & scheduling software (Control-M) Strong ability to write SQL queriesDesired Skills:  Familiar with Java API’s, Snowflake (Cloud DB), Snaplogic, Python, UNIX, Windows, File transfer utilities, process flow creation, ETL technologies, HadoopGood to have Skills: Java Springboot, Snow-Pro Certified, Snaplogic, Strong SQL, Strong conceptual understanding of core DW Concepts including different approaches/methodologies. dbt (data build tool) experience is an added advantageDiverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company."
Senior Data Quality Engineer,Crystal Equation Corporation,"Atlanta, GA (Hybrid)",https://www.linkedin.com/jobs/view/3748603486/?eBP=JOB_SEARCH_ORGANIC&refId=UGLS5oOcuSte7Lbn4dE1nQ%3D%3D&trackingId=bMrBYyd%2BvO0F7fTLzxCKIw%3D%3D&trk=flagship3_search_srp_jobs,3748603486,"About the job
            
 
Job DescriptionJob PurposeThe Senior Data Quality Engineer will be responsible for developing, maintaining and automating tests to ensure all compliance reports, surveillance systems are accurate and complete. This role requires frequent interaction with software developers, testers, product managers and compliance professionals to ensure delivery of quality reports to internal and external users for business decision making and government compliance needs.Responsibilities Strong background in functional testing with the ability to design and develop test strategy and test cases. Gather information from disparate systems to aid problem resolution and perform integration testing Work with Business Analysts, Software developers, ETL and Data warehouse engineers to review technical requirements and produce test strategies, test scenarios, and test cases for the data warehouse team to validate ETL scripts and confirm correct data is being extracted from the source. Work with trading systems for trade entry in order test surveillance systems for monitoring Testing skills to validate the transformation logic; diagnostic and debugging skills. Performing source-to-target-validation and data quality testing experience with programming skills/python coding/automation efforts and exposure to fixml/xml parsing Ability to lead a distributed testing effort. Excellent attention to detail. Ability to work in an agile environment with an iterative approach to development. 
Knowledge And ExperienceRequires experience/knowledge of the following functional areas: 5+ experience in software application testing, with a 3 year experience in enterprise data warehouse environments. Proficient in SQL and database systems Experience Unix to analyze logs and ability to parse XML files Track record of completing assignments on time with a high degree of quality. Excellent analytical, problem-solving, communication and interpersonal skills. Ability to work independently and productively under pressure. Bachelor’s Degree or equivalent 
ScheduleThis role offers work from home flexibility of up to 2 days per week."
Data Engineer IV - Max Digital (Data Operations),ACV Auctions,"Austin, TX (Hybrid)",https://www.linkedin.com/jobs/view/3762878251/?eBP=JOB_SEARCH_ORGANIC&refId=UGLS5oOcuSte7Lbn4dE1nQ%3D%3D&trackingId=tZl357pHug8dqE7HLT8LYQ%3D%3D&trk=flagship3_search_srp_jobs,3762878251,"About the job
            
 
If you are looking for a career at a dynamic company with a people-first mindset and a deep culture of growth and autonomy, ACV is the right place for you! Competitive compensation packages and learning and development opportunities, ACV has what you need to advance to the next level in your career. We will continue to raise the bar every day by investing in our people and technology to help our customers succeed. We hire people who share our passion, bring innovative ideas to the table, and enjoy a collaborative atmosphere.Who we are:ACV is a technology company that has revolutionized how dealers buy and sell cars online. We are transforming the automotive industry. ACV Auctions Inc. (ACV), has applied innovation and user-designed, data driven applications and solutions. We are building the most trusted and efficient digital marketplace with data solutions for sourcing, selling and managing used vehicles with transparency and comprehensive insights that were once unimaginable. We are disruptors of the industry and we want you to join us on our journey. ACV’s network of brands includes ACV Auctions, ACV Transportation, ClearCar, MAX Digital and ACV Capital within its Marketplace Products, as well as, True360 and Data Services.At ACV we focus on the Health, Physical, Financial, Social and Emotional Wellness of our Teammates and to support this we offer:  Multiple medical plans including a high deductible health plan that costs $0 out of your paycheck  Company-sponsored (paid) Short-Term Disability, Long-Term Disability, and Life Insurance  Comprehensive optional benefits such as Dental, Vision, Supplemental Life/AD&D, Legal/ID Protection, and Accident and Critical Illness Insurance  Generous paid time off options, including vacation time, sick days, Company holidays, floating holidays, parental leave, bereavement leave, jury duty leave, voting leave, and other forms of paid leave as required by applicable law or regulation  Employee Stock Purchase Program with additional opportunities to earn stock in the Company  Retirement planning through the Company’s 401(k) 
 Who we are looking for: We are seeking a highly skilled Engineer IV in Data Engineering with a strong foundation in computer science and excellent problem-solving skills. You will be responsible for maintaining and extending our database operations, optimizing SQL queries, and designing scalable data services. What you will do:   Actively and consistently support all efforts to simplify and enhance the customer experience.  Maintain and extend (as required) existing database operations solution for backups, index defragmentation, data retention, etc.  Troubleshoot any SQL Server or ETL stack (SSIS, C#, Web APIs) outages during our operational support window.  Leverage monitoring tools to ensure high performance and availability; work with operations and engineering to improve as required.  Leverage DMVs and monitoring tools to ensure system performance; work with data operations and engineering to improve as required.  Ensure existing HADR (availability groups) solution is functional and meets requirements.  Support development, integration, and stage SQL Server environments for application development and data science teams.  Ensure that new database development meets company standards for readability, reliability, and performance. Work with internal teams on transactional and analytical schema design.  Collaborate with software and DevOps engineers to design scalable services, plan feature roll-out, and ensure high reliability and performance of our products.  Conduct code reviews, develop high-quality documentation, and build robust test suites.  Own the overall performance of products and services within a defined area of focus.  Respond-to and troubleshoot highly complex problems quickly, efficiently, and effectively. This may include being part of the emergency after-hours on-call rotation.  Mentor junior data engineers.  Perform additional duties as assigned. 
 What you will need:    Bachelor's degree in Computer Science, Information Technology, Computer Information Systems, Management Information Systems, or similar  5 years' building & supporting the database-tier of SaaS web applications.  Ability to read, write, speak, and understand English.  Expert in SQL Query optimization  ETL workflow implementation (SSIS, Airflow, C#, Python)  Experience working with Cloud Services (AWS RDS, S3, SQS, SNS)  Experience working with NoSQL data stores (e.g., MongoDB)  Experience developing Windows services in C#  Experience writing unit and integration testing  Expert SQL and data-layer development experience; OLTP schema design.  Experience using and integrating with cloud services, specifically: AWS RDS, S3, SQS, SNS.  Nice to Have  Experience with Airflow  Experience with DBT 

Our ValuesTrust & Transparency | People First | Positive Experiences | Calm Persistence | Never SettlingAt ACV, we are committed to an inclusive culture in which every individual is welcomed and empowered to celebrate their true selves. We achieve this by fostering a work environment of acceptance and understanding that is free from discrimination. ACV is committed to being an equal opportunity employer regardless of sex, race, creed, color, religion, marital status, national origin, age, pregnancy, sexual orientation, gender, gender identity, gender expression, genetic information, disability, military status, status as a veteran, or any other protected characteristic. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you have a disability or special need that requires reasonable accommodation, please let us know.For information on our collection and use of your personal information, please see our Privacy Notice.Apply Now"
"Data Engineer, Data Platform",Grammarly,"Maine, United States (Hybrid)",https://www.linkedin.com/jobs/view/3689960992/?eBP=JOB_SEARCH_ORGANIC&refId=UGLS5oOcuSte7Lbn4dE1nQ%3D%3D&trackingId=jflz3Huzfs%2F%2FNryZWdlFrw%3D%3D&trk=flagship3_search_srp_jobs,3689960992,"About the job
            
 
Grammarly is excited to offer a remote-first hybrid working model. Team members work primarily remotely in the United States, Canada, Ukraine, Germany, or Poland. Certain roles have specific location requirements to facilitate collaboration at a particular Grammarly hub.All roles have an in-person component: Conditions permitting, teams meet 2–4 weeks every quarter at one of Grammarly’s hubs in San Francisco, Kyiv, New York, Vancouver, and Berlin, or in a workspace in Kraków. This flexible approach gives team members the best of both worlds: plenty of focus time along with in-person collaboration that fosters trust and unlocks creativity.Grammarly team members in this role must be based in the United States or Canada, and they must be able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub(s) where the team is based.The opportunity Grammarly is the world’s leading AI writing assistance company trusted by over 30 million people and 70,000 professional teams every day. From instantly creating a first draft to perfecting every message, Grammarly’s product offerings help people at 96% of the Fortune 500 get their point across—and get results. Grammarly has been profitable for over a decade because we’ve stayed true to our values and built an enterprise-grade product that’s secure, reliable, and helps people do their best work—without selling their data. We’re proud to be one of Inc.’s best workplaces, a Glassdoor Best Place to Work, one of TIME’s 100 Most Influential Companies, and one of Fast Company’s Most Innovative Companies in AI.To achieve our ambitious goals, we’re looking for a Data Engineer to join our Data Engineering Platform team. This person will build highly automated, low latency core datasets that will help data engineers and end users across Grammarly to work with analytical data at scale.Grammarly’s engineers and researchers have the freedom to innovate and uncover breakthroughs—and, in turn, influence our product roadmap. The complexity of our technical challenges is growing rapidly as we scale our interfaces, algorithms, and infrastructure. Read more about our stack or hear from our team on our technical blog.Your impactAs a Data Engineer on our Data Engineering Platform team, you will: Drive improvements to make our analytics effortless by creating and adjusting core data models and storage structures, all while understanding the needs of our users. Make analytical data and metrics usable within a few minutes of real world events occuring, and build streaming processes for the output derived events and aggregate data.Model structure, storage, and access of data at very high volumes for our data lakehouse.Improve developer productivity and self-serve solutions by contributing components to our stream data processing framework(s).Own data engineering's infrastructure-as-code for provisioning services that allow our engineers to deploy mature software installations within a few hours.Build a world-class process that will allow our systems to scale.Mentor other back-end engineers on the team and help them grow.Build and contribute to AWS high-scale distributed systems on the back-end.
We’re Looking For Someone Who Embodies our EAGER values—is ethical, adaptable, gritty, empathetic, and remarkable.Is inspired by our MOVE principles, which are the blueprint for how things get done at Grammarly: move fast and learn faster, obsess about creating customer value, value impact over activity, and embrace healthy disagreement rooted in trust.Is able to collaborate in person 2 weeks per quarter, traveling if necessary to the hub where the team is based.Has experience with Python, Scala, or Java.Has experience with designing database objects and writing relational queriesHas experience designing and standing up APIs and services.Has experience with system design and building internal tools.Has experience handling applications that work with data from data lakes.Has at least some experience building internal Admin sites.Has good knowledge of and at least some experience with AWS (or, alternatively, has deep expertise in Azure or GCE and is willing to learn AWS in a short time frame).Can knowledgeably choose an open source or third-party service to accomplish what they need or, alternatively, can devise a quick and simple solution on their own.
Support for you, professionally and personally Professional growth: We believe that autonomy and trust are key to empowering our team members to do their best, most innovative work in a way that aligns with their interests, talents, and well-being. We support professional development and advancement with training, coaching, and regular feedback.A connected team: Grammarly builds a product that helps people connect, and we apply this mindset to our own team. Our remote-first hybrid model enables a highly collaborative culture supported by our EAGER (ethical, adaptable, gritty, empathetic, and remarkable) values. We work to foster belonging among team members in a variety of ways. This includes our employee resource groups, Grammarly Circles, which promote connection among those with shared identities, such as BIPOC and LGBTQIA+ team members, women, and parents. We also celebrate our colleagues and accomplishments with global, local, and team-specific programs. 
Compensation And BenefitsGrammarly offers all team members competitive pay along with a benefits package encompassing the following and more: Excellent health care (including a wide range of medical, dental, vision, mental health, and fertility benefits)Disability and life insurance options401(k) and RRSP matching Paid parental leaveTwenty days of paid time off per year, eleven days of paid holidays per year, and unlimited sick days Home office stipendsCaregiver and pet care stipendsWellness stipendsAdmission discountsLearning and development opportunities
Grammarly takes a market-based approach to compensation, which means base pay may vary depending on your location. Our US and Canada locations are categorized into compensation zones based on each geographic region’s cost of labor index. For more information about our compensation zones and locations where we currently support employment, please refer to this page. If a location of interest is not listed, please speak with a recruiter for additional information.Base pay may vary considerably depending on job-related knowledge, skills, and experience. The expected salary ranges for this position are outlined below by compensation zone and may be modified in the future.United StatesZone 1: $167,000 - $242,000/year (USD)Zone 2: $150,000 – $218,000/year (USD)Zone 3: $142,000 – $206,000/year (USD)Zone 4: $134,000 – $194,000/year (USD)We encourage you to applyAt Grammarly, we value our differences, and we encourage all—especially those whose identities are traditionally underrepresented in tech organizations—to apply. We do not discriminate on the basis of race, religion, color, gender expression or identity, sexual orientation, ancestry, national origin, citizenship, age, marital status, veteran status, disability status, political belief, or any other characteristic protected by law. Grammarly is an equal opportunity employer and a participant in the US federal E-Verify program (US). We also abide by the Employment Equity Act (Canada).Please note that EEOC is optional and specific to US-based candidates.#NAAll team members meeting in person for official Grammarly business or working from a hub location are strongly encouraged to be vaccinated against COVID-19."
"Data Engineer-Senior -Northern, VA","iSenpai, LLC","Northern, VA (Hybrid)",https://www.linkedin.com/jobs/view/3750122411/?eBP=JOB_SEARCH_ORGANIC&refId=UGLS5oOcuSte7Lbn4dE1nQ%3D%3D&trackingId=3q04IYppHxE7TDRozWtD2A%3D%3D&trk=flagship3_search_srp_jobs,3750122411,"About the job
            
 
iSenpai is a Woman-Owned Small Business (WOSB) that provides enterprise IT and cyber security services, cloud technology, and data analytics solutions for US Government and commercial customers. We specialize in cloud-based solutions with cyber security integrated into the design, delivered using efficient Agile DevSecOps. Engaging across industry and academia to develop innovative approaches, we support our customers’ changing mission needs and data. Our iSenpai designs are industry-proven to handle enormous data streams from open source, Internet of Things (IOT), cyber tools, and social media. We bring expertise with the National Institute of Standards and Technology (NIST) Special Publication (SP) 800-53 and the Risk Management Framework (RMF) to maintain secure, scalable, and highly available systems to meet the mission. Our goal is to provide expert services to our clients while cultivating knowledge among all employees for the advancement of our services. We offer a comprehensive package of 100% employer-paid benefits, including medical, dental, vision, HSA Contribution, 401K match, and more.ISenpai Offers An Extremely Competitive Benefits Package To Include Health, Dental, and Vision Insurance Premiums are 100% provided by iSenpai for employees and eligible dependentsPersonal Accident Insurance provided by iSenpaiLife Insurance provided by iSenpaiShort and Long Term Disability Insurance provided by iSenpai401K Contribution Matching - 5% dollar for dollarReimbursement for any wellness programs and/or work-life balance programsReimbursement for cell phone plansReimbursement for home internetReimbursement every 2 years for a cell phone upgradeThousands of discounts on everything from your cell phone bill to NFL tickets, Movie Tickets, Live Performances, etcRewards for obtaining new IT certificationsComputer-based training (CBT) library on IT and information security topics and certificationsRemote access to a virtual lab for testing/learning opportunitiesFlexible / Alternative Work Schedules (based on customer requirements)
iSenpai is an equal opportunity / affirmative action employer. We give equal consideration to all qualified candidates without regard to race, color, gender, nationality, disability, or protected veteran status.NOTE: To all recruitment and staffing agencies: iSenpai does not accept agency resumes or soliciting of your services. Please do not forward resumes to our jobs alias, iSenpai employees, or any other company location. iSenpai is not responsible for any fees related to unsolicited resumes or staffing services. Do not attempt to solicit your services.Single owner corp to corp OR 1099 are encouraged to apply!Data Engineer Senior-Northern, VARequired Degree: Bachelor’s Degree Required Experience: 7+ years of experience with application supportWork Location: Northern Virginia-Hybrid (As needed)Clearance Required: Top SecretDescriptionAs a Data Engineer, you will play a crucial role in designing, developing, and maintaining our Advana data infrastructure and systems. Your expertise in ETL, Databricks, Python, Spark, Scala, JavaScript/JSON, SQL, and Jupyter Notebooks will be essential in ensuring efficient data processing and analysis.Responsibilities Design, develop, and implement end-to-end data pipelines, utilizing ETL processes and technologies such as Databricks, Python, Spark, Scala, JavaScript/JSON, SQL, and Jupyter Notebooks.Create and optimize data pipelines from scratch, ensuring scalability, reliability, and high-performance processing.Perform data cleansing, data integration, and data quality assurance activities to maintain the accuracy and integrity of large datasets.Leverage big data technologies to efficiently process and analyze large datasets, particularly those encountered in a federal agency.Troubleshoot data-related problems and provide innovative solutions to address complex data challenges.Implement and enforce data governance policies and procedures, ensuring compliance with regulatory requirements and industry best practices.Work closely with cross-functional teams to understand data requirements and design optimal data models and architectures.Collaborate with data scientists, analysts, and stakeholders to provide timely and accurate data insights and support decision-making processes.Maintain documentation for software applications, workflows, and processes.Stay updated with emerging trends and advancements in data engineering and recommend suitable tools and technologies for continuous improvement.
Requirements Secret clearance is required; TS/SCI clearance is preferred.Minimum of 7 years of experience as a Data Engineer, with demonstrated experience creating data pipelines from scratch.High level of proficiency in ETL processes and demonstrated hands-on experience with technologies such as Databricks, Python, Spark, Scala, JavaScript/JSON, SQL, and Jupyter Notebooks.Strong problem-solving skills and ability to solve complex data-related issues.Demonstrated experience working with large datasets and leveraging big data technologies to process and analyze data efficiently.Understanding of data modeling/visualization, database design principles, and data governance practices.Excellent communication and collaboration skills, with the ability to work effectively with cross-functional teams.Detail-oriented mindset with a commitment to delivering high-quality results.Must be in the DC Metro area and available to work onsite (Crystal City, VA, and Alexandria, VA) 2-3 days per week.
Nice To Have Knowledge of Qlik/Qlik Sense, QVD/QlikView, and Qlik Production Application Standards (QPAS) is a significant plus.Recent DoD or IC-related experience.Previous experience with Advana is a plus."
Data Engineer,LaSalle Network,"Chicago, IL (Hybrid)",https://www.linkedin.com/jobs/view/3730251742/?eBP=JOB_SEARCH_ORGANIC&refId=UGLS5oOcuSte7Lbn4dE1nQ%3D%3D&trackingId=bW%2FfD3KagERrBvPr3%2FkHsw%3D%3D&trk=flagship3_search_srp_jobs,3730251742,"About the job
            
 
Want to grow your career with one of the fastest growing industries? LaSalle Network is partnered with a top Chicago-based client looking to hire a Data Engineer their team.  The Data Engineer will be responsible for delivering strategic and tangible solutions through data. We are looking for self-starters that have the ability to work cross-functionally across various teams.  Data Engineer Responsibilities:  Architect, implement and maintain the information data lake and leverage various reporting platforms to enable decision making across the portfolio companiesBuild ETLs to ingest data into the data warehouse and data lake, as well as end-user facing reporting applicationsWork to deliver flexible, scalable, end-to-end solutionsWork with big data and emerging technologies while driving business intelligence solutions end-to-end: business requirements, data modeling, ETL, reporting and dashboardingClose coordination with the senior leadership team on a day-to-day basis; ability to clearly communicate complicated data infrastructure concepts is a mustSupport for other ad hoc strategic data projects and initiatives
 Data Engineer Requirements: 4+ years of experience with Data Modeling, SQL, ETL, Data Warehousing, (Snowflake, Redshift, Azure, AWS)4+ years of experience with enterprise-class Business Intelligence tools such as Looker, Power BI, Tableau, Oracle BI, MicroStrategyPython knowledge or experience an added plusBachelor's degree in computer science, information systems, mathematics, statistics or related fieldExpertise in the design, creation, management and business use of large datasets Machine Learning experience a plusPassionate about working with large unstructured datasets and helping set the vision for the overall data strategy across the growing portfolio
 Thank you,  Kelsey Gonzalez Team Lead LaSalle NetworkLaSalle Network is an Equal Opportunity Employer m/f/d/v. LaSalle Network is the leading provider of direct hire and temporary staffing services. For over two decades, LaSalle has helped organizations hire faster and connect top talent with opportunities, from entry-level positions to the C-suite. With units specializing in Accounting and Finance, Administrative, Marketing, Technology, Supply chain, Healthcare Revenue Cycle, Call Center, Human Resources and Executive Search. LaSalle offers staffing and recruiting solutions to companies of all sizes and across all industries. LaSalle Network is the premier staffing and recruiting firm, earning over 100 culture, revenue and industry-based awards from major publications and having its company experts regularly contribute insights on retention strategies, hiring trends and hiring challenges, and more to national news outlets."
Senior Data Engineer (Remote),Braintrust,Los Angeles Metropolitan Area (Hybrid),https://www.linkedin.com/jobs/view/3775183536/?eBP=JOB_SEARCH_ORGANIC&refId=UGLS5oOcuSte7Lbn4dE1nQ%3D%3D&trackingId=zOvPzvLm3OLO526tQQo%2Bdw%3D%3D&trk=flagship3_search_srp_jobs,3775183536,"About the job
            
 
About UsBraintrust is a user-owned talent network that connects top-tier professionals with the world's leading enterprises. We prioritize transparency, eliminating middlemen and high markups, ensuring job-seekers are matched swiftly to innovative roles while clients benefit from unparalleled efficiency and quality.About The Hiring ProcessThe hiring process for this role involves completing your Braintrust profile, applying directly to the role on Braintrust, and undergoing a one-time screening to ensure you meet our vetted talent specifications. After this, the hiring team will contact you directly if they believe you are a suitable match.Our process isn't for everyone, that's intentional. If you believe that you are a top candidate for this job, please join our network to give yourself the opportunity to work with top companies.JOB TYPE: Direct Hire/ FTE Position (no agencies/C2C - see notes below)LOCATION: Work from anywhere - Anytime | No timezone overlap requiredSALARY RANGE $153,000 – $210,000 /yrESTIMATED DURATION: 40/week - long termEXPERIENCE: 5-9 yearsBRAINTRUST JOB ID: 11426The OpportunityWhat We’re Looking For 4+ years of proven experience as a Data Engineer, with a focus on building robust data pipelines and maintaining data infrastructure. Strong programming skills in languages like Python, Scala or Javascript, with a solid understanding of software engineering principles. Hands-on Experience building or maintaining data pipelines with a modern orchestration tools like Airflorw/Prefect/DagsterStrong expertise with SQL and non-SQL DBs, cloud-based data platforms (e.g., AWS, GCP) and their related services (S3, BigQuery, etc.). Expert level writing of SQL for data manipulation, transformation and for analytics. Experience building and maintaining interactive and intuitive dashboards using tools like Looker. Comfortable with tool development, including building and enhancing internal data tools and frameworks. Excellent problem-solving and analytical skills, with a strong attention to detail. Effective communication skills, with the ability to collaborate with cross-functional teams and present complex ideas to both technical and non-technical stakeholders. 
What You'll Be Working OnWho We Are ✨Snackpass’s mission is to unify the physical and digital world for local commerce.We power mobile order pickup and social commerce for restaurants, modernizing the customer experience while making restaurant operators successful.Opportunity ✨Snackpass is one of the fastest growing marketplaces (a16z top marketplaces), and a top 100 YC company. We are backed by Andreeson Horowitz, Y Combinator, General Catalyst, First Round Capital, Craft Ventures and many others. We are hiring people who are humble and hungry to join us in any of our hubs (NYC, SF, LA) or remotely.Our vision is to be the dominant platform for pickup, a $750B market globally.About The RoleWe are seeking a talented and experienced Senior Data Engineer to join our dynamic team. As a Senior Data Engineer, you will play a crucial role in building and maintaining data pipelines, developing and managing dashboards, and contributing to tool development. Your expertise will enable us to efficiently process and analyze large volumes of data, providing valuable insights to drive business decisions.What You’ll Be Working On Build scalable and efficient data pipelines to collect, process, and transform large volumes of data from diverse sources. Develop and maintain data models, ensuring data integrity and optimizing query performance. Collaborate with stakeholders to understand their data requirements and provide the necessary infrastructure and support. Build and maintain interactive dashboards and visualizations to enable data-driven decision-making across the organization. Contribute to the development of internal data tools and frameworks, automating repetitive tasks and improving data accessibility and usability. Stay up to date with the latest advancements in data engineering technologies and best practices, and proactively recommend improvements to our data infrastructure. Mentor and provide guidance to junior data engineers, fostering a culture of learning and growth. 
What You Will Get From UsYou will receive competitive compensation, a generous equity grant in a high-growth start-up, and benefits like healthcare, medical & dental coverage, unlimited PTO, a home office budget, wellness budget and more.Importantly, you will also receive an unparalleled amount of ownership over the work you do here. We are a small team, so the opportunity to make a large impact, work on a broad spectrum of challenges and grow your personal skill-set awaits you here.Finally, you will get a diverse and inclusive work environment where you will be surrounded by hungry and humble colleagues.Snackpass is an equal opportunity employer and we value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. In fact, we are confident that the most inclusive and diverse teams accomplish the most extraordinary results.Apply Now!NotesOur employers all have varying legal and geographic requirements for their roles, they trust Braintrust to find them the talent that meet their unique specifications. For that reason, this role is not available to C2C candidates working with an agency. If you are a professional contractor who has created an LLC/corp around their consulting practice, this is well aligned with Braintrust and we’d welcome your application.Braintrust values the multitude of talents and perspectives that a diverse workforce brings. All qualified applicants will receive consideration for employment without regard to race, national origin, religion, age, color, sex, sexual orientation, gender identity, disability, or protected veteran status."
Principal Data Engineer (32719-JCEN),Professional Diversity Network,"Natick, MA (Hybrid)",https://www.linkedin.com/jobs/view/3776916571/?eBP=JOB_SEARCH_ORGANIC&refId=UGLS5oOcuSte7Lbn4dE1nQ%3D%3D&trackingId=kbf%2F2tM96Kb1C3VRp%2FfxrA%3D%3D&trk=flagship3_search_srp_jobs,3776916571,"About the job
            
 
SummaryMathWorks has a hybrid work model that enables staff members to split their time between office and home. The hybrid model provides the advantage of having both in-person time with colleagues and flexible at-home life optimizations. Learn More: https://www.mathworks.com/company/jobs/resources/applying-and-interviewing.html#onboarding.We are seeking a self-motivated and self-directed data engineer / software engineer to develop and maintain our license usage data system.This position plays a key role in a business-critical initiative and requires excellent hands-on software development skills as well as a big-picture perspective.You will develop requirements, implement features, and analyze data to further our understanding of customers and their usage of MathWorks products.Strong interpersonal and communication skills are a must as this is a highly visible position that involves collaboration with teams across the organization.MathWorks nurtures growth, appreciates diversity, encourages initiative, values teamwork, shares success, and rewards excellence.ResponsibilitiesAs part of this team, you will... Evaluate and employ appropriate state of the art solutions from Amazon Web Services / EC2 and other providers.Extract and analyze customer provided data to accurately determine usage and identify patterns.Gather business use cases, develop requirements, enhance our data pipeline, and build reports in support of Sales, Marketing, and Development teams tasked with utilizing usage data to better understand our customers and their use of our products.Manage the workflow and on-time delivery of requests for enhancements, reports, ad hoc analysis, and modeling for targeting and segmentation strategies.Establish and maintain the data pipeline and infrastructure. Collaborate with Quality Engineers to develop data validation strategies.Develop a strong working knowledge of MathWorks development practices, business processes, and enterprise data warehouse and reporting environment.
Qualifications Excellent problem-solving and communication skillsExpert level of proficiency in SQL, Excel, and at least one of the following (MATLAB, SAS, SPSS or R).Experience with scripting in one or more of the following (Perl, Python, Ruby, PHP)Proficient with reporting tools such as; PowerBI or Tableau is a plus.Experience with enterprise system applicationsAbility to clearly explain technical and analytical information (verbally, written & in presentation format) and apply/summarize for business use.Excellent project management skills with attention to detailAbility to handle multiple projects simultaneously
Required Qualifications A bachelor's degree and 10 years of professional work experience (or equivalent experience) is required.Expertise with SQLExperience with data securityExperience with scripting
The MathWorks, Inc. is an equal opportunity employer. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, veteran status, and other protected characteristics. The EEO is the Law poster is available here.MathWorks participates in E-Verify. View the E-Verify posters here.PDN-9a9d6bd2-f366-4866-baf4-acdbc0fe2fac"
Senior Data Engineer : Irving - TX,Diverse Lynx,"Irving, TX (Hybrid)",https://www.linkedin.com/jobs/view/3764422633/?eBP=JOB_SEARCH_ORGANIC&refId=UGLS5oOcuSte7Lbn4dE1nQ%3D%3D&trackingId=bH460IG4YBxyOsn8CnEQ%2Bw%3D%3D&trk=flagship3_search_srp_jobs,3764422633,"About the job
            
 
5&plus; Years experience in DW/BI or Bigdata Project 4 &plus; years experience in ETL / Data ingestion Expert in creating in data pipeline using python Strong knowledge and working experience in SPARK Proficient in SQL , PL/SQL Experiece in BigQuery Good hands on experience in Cloud Composer Good experience in Cloud SQL & GCP platform Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company."
Senior Data Engineer,Mastech Digital,"Austin, MN (Hybrid)",https://www.linkedin.com/jobs/view/3756014140/?eBP=JOB_SEARCH_ORGANIC&refId=UGLS5oOcuSte7Lbn4dE1nQ%3D%3D&trackingId=lInBBFlV3cSlpwgosDwytQ%3D%3D&trk=flagship3_search_srp_jobs,3756014140,"About the job
            
 
DescriptionMastech Digital provides digital and mainstream technology staff as well as Digital Transformation Services for all American Corporations. We are currently seeking a Senior Data Engineer for our client in the Manufacturing domain. We value our professionals, providing comprehensive benefits and the opportunity for growth. This is a Permanent position, and the client is looking for someone to start immediately.Duration: Full-timeLocation: Austin, MN/Minneapolis, MN/Willmar, MN/Bentonville, AR/Bridgewater, NJ/Chicago, IL/Naperville, IL (Hybrid (2-3 Days Onsite))Role: Senior Data EngineerPrimary Skills: Data Engineering/Data WarehouseRole Description: The Senior Data Engineer must have at least 7+ years of experience.Required Experience And Skills  A Bachelor’s degree in Computer Science, MIS, or related area and significant experience with business intelligence design and development. Good experience with reading and writing SQL. Good experience in designing and developing within a business intelligence/reporting tool like Oracle Business Intelligence, Tableau or Google Cloud Platform. Good experience engineering within a data warehouse or related experience with dimensional data modeling. Good experience designing and developing ETLs/pipelines in Python, Google BigQuery Dataprocs and/or Informatica ETL. Excellent written and verbal communication skills. Proven ability to gather detailed technical requirements to design and develop business intelligence report solutions from beginning to end. Excellent organizational and time management skills. Tested problem-solving and decision-making skills. A strong pattern of initiative. Highly developed interpersonal and leadership skills.
Education: Bachelor’s degree in Computer Science, Electrical/Electronic Engineering, Information Technology or another related field or EquivalentExperience: Minimum 7+ years of experienceRelocation: This position will not cover relocation expensesTravel: NoLocal Preferred: YesNote: Must be able to work on a W2 basis (No C2C)Recruiter Name: Sahil JoshiRecruiter Phone: 412-684-6378Equal Employment Opportunity"
Sr. Data Engineer,Theorem,"San Mateo, CA (Hybrid)",https://www.linkedin.com/jobs/view/3778902139/?eBP=JOB_SEARCH_ORGANIC&refId=UGLS5oOcuSte7Lbn4dE1nQ%3D%3D&trackingId=04swxsw1qnoSY0Z%2BTQIgiA%3D%3D&trk=flagship3_search_srp_jobs,3778902139,"About the job
            
 
About UsPursuit of truth in credit. By using machine learning to anticipate and manage risk in credit, we’re empowering our partners and lenders to unlock opportunity and access for more borrowers, everywhere.We strive to be the preferred partner to lending platforms, providing not only access to capital but also underwriting technology capabilities to allow innovative lending platforms to grow their business.Our firm is made up of 60+ professionals working in San Mateo (HQ) and New York, working in-office on Tuesdays and Thursdays. We are passionate, hard-working, relentlessly-resourceful, impact-focused individuals. We deeply value intellectual curiosity, independence of thought, creative idea generation, empathy, and close collaboration.The RoleAs a Data Engineer, you will work alongside quantitative researchers, finance & operations, investor relations & sales, and capital markets & partnerships.Your role is to develop the systems and data pipelines that enable the shared data assets informing every decision of the firm.What You'll Do Partner with stakeholders and senior leaders across Theorem to understand needs and set priorities for data-driven workflowsDesign new capabilities for Theorem's data infrastructureDevelop robust software to support new and existing data initiatives across the firmContribute to the creation of an industry-leading loan modeling platform that will generate data around which the business will coalesceBuild complex data pipelines with high demands for correctness, speed, robustness, and availabilityGrow as an engineer by working with and receiving mentorship from senior engineers at the firm
You Will Own Availability and freshness of data through clear ETL ownership and automated alerting strategiesIntegration of system operational data sources into the Theorem data warehouseQuantification and tracking of data quality across all pipelines and build technologyEnd-user discovery and accessibility of data used across the firm by normalizing, standardizing, and cataloging all assetsDeprecation and removal of legacy data pipelines & reporting systems
What We're Looking For A minimum of 4 years of Data Engineering experience at a technology companyAn ability to partner with non-technical colleagues to transform business requirements into technical solutionsExperience building automated reports, dashboards, and visualizations of curated data, e.g. SQL, Jupyter, LookerExperience assessing, implementing, and monitoring data validation and quality (correctness, completeness, availability, etc)Fluency in SQL and PythonDeep expertise in relational data modeling, schema design, and normalization
Expertise in 2 or more of the following areas Distributed computation/query frameworks, e.g. Apache Spark, Databricks, PrestoDistributed columnar data warehouses, e.g. AWS, Google BigQuery, SnowflakeETL and data cataloging frameworks, e.g. Hive/AWS Glue, Fivetran, dbtDAG/workflow management tools, e.g. Argo, AirflowStreams/queues/event sourcing, e.g. Kafka, AWS MSK
Bonus Experience with containerized environments, e.g. Kubernetes, DockerExperience using Bazel Previous financial experience in ABS (Asset Backed Securities) 
Characteristics To Thrive Hardworking and grittyEthical, intellectually honest, and transparentDetail-orientedProactive with communicationCollaborative and team success-orientedExcited to learn and grow from feedback and experienceAt home on small, high-impact teamsThorough in your end-to-end ownership of outcomesBiased towards action to solve problems
Additional Information Expected full-time salary range between $150,000 to $220,000 + bonus + equity + benefitsAdvertised and actual salary ranges may differ by geographic area, work experience, education, and/or skill level
Our CommitmentWe foster an environment that welcomes professionals with a diversity of backgrounds and ideas. We value professionals who are thoughtful, innovative, tenacious, and mission-driven. Every member of the team has a major impact on the company's success with visible contributions to the business. We encourage and reward growth, learning, and a solutions-seeking mindset. We offer a competitive salary and opportunity for equity ownership, generous benefits, and an inclusive and collaborative work environment. If you’re excited by the opportunities to create outsized impact as part of a world-class team, we strongly encourage you to apply.We provide reasonable accommodation for qualified individuals with disabilities and disabled veterans in job application procedures. If you have any difficulty using our online system and you need an accommodation due to a disability, you may use the following alternative email address to contact us about your interest in employment: careers@theoremlp.com. Alternatively, you can contact us at 415-489-0457."
