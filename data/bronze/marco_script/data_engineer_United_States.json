[{"title": "Junior Data Engineer", "company_name": "HireMeFast LLC - Career Accelerator - Land A Job", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3786249797/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D&trk=flagship3_search_srp_jobs&lipi=urn%3Ali%3Apage%3Ad_flagship3_search_srp_jobs%3BQExogrHBQoSF9zZm%2B9kufw%3D%3D&lici=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D", "id": "3786249797", "description": "About the job\nThis is a remote position.\n\n Job Title: Junior Data Engineer\n\n Employment Type: Full-Time\n\n Salary: $64,000 - $76,000 per annum\n\n Experience Required:  Minimum 1 year of project experience\n\nAbout us: HireMeFast is a leading staffing and recruitment agency specializing in connecting businesses with top-tier talent across various industries. Our mission is to bridge the gap between exceptional candidates and organizations needing their skills, expertise, and unique qualities. Our team of experienced and dedicated recruitment specialists utilizes innovative sourcing strategies, and a vast network to identify and attract top talent. We conduct comprehensive procedures to ensure that only the most qualified candidates are presented to our clients.\n\nPosition Summary\n\nJoin the fast-paced, innovative, and collaborative environment focused on providing an AIOps platform that enhances the intelligence of the CVS Health infrastructure. Work closely with subject matter experts and colleagues to build and scale out machine learning and AI solutions that will detect, predict, and recommend solutions to correct issues before system impact and enhance efficiency, reliability, and performance of CVS Health\u2019s IT operations.\n\nKey Responsibilities include:\n\nData pipeline development: Designed, implemented, and managed data pipelines for extracting, transforming, and loading data from various sources into data lakes for processing, analytics, and correlation. \nData modeling: Create and maintain data models ensuring data quality, scalability, and efficiency \nDevelop and automate processes to clean, transform, and prepare data for analytics, ensuring data accuracy and consistency \nData Integration: Integrate data from disparate sources, both structured and unstructured to provide a unified view of key infrastructure platform and application data \nUtilize big data technologies such as Kafka to process and analyze large volumes of data efficiently \nImplement data security measures to protect sensitive information and ensure compliance with data and privacy regulation \nCreate/maintain documentation for data processes, data flows, and system configurations \nPerformance Optimization- Monitor and optimize data pipelines and systems for performance, scalability and cost-effectiveness \n\nCharacteristics of this role:\n\nTeam Player: Willing to teach, share knowledge, and work with others to make the team successful. \nCommunication: Exceptional verbal, written, organizational, presentation, and communication skills. \nCreativity: Ability to take written and verbal requirements and come up with other innovative ideas. \nAttention to detail: Systematically and accurately research future solutions and current problems. \nStrong work ethic: The innate drive to do work extremely well. \nPassion: A drive to deliver better products and services than expected to customers. \n\nRequired Qualifications\n\n2+ years of programming experience in languages such as Python, Java, SQL \n2+ years of experience with ETL tools and database management (relational, non-relational) \n2+ years of experience in data modeling techniques and tools to design efficient scalable data structures \nSkills in data quality assessment, data cleansing, and data validation \n\nQualifications\n\nKnowledge of big data technologies and cloud platforms \nExperience with technologies like PySpark, Databricks, and Azure Synapse. \n\nEducation\n\nBachelor\u2019s degree in Computer Science, Information Technology, or related field, or equivalent working experience\n\nWhy HireMeFast LLC?\n\nAt HireMeFast, we understand that finding the right individuals to join your team is crucial for success and growth of your organization. We are committed to streamlining the hiring process for our clients, ensuring they have access to a diverse pool of highly qualified candidates who are a perfect fit for their specific needs."}, {"title": "Data Engineer  - Need candidates from Texas ( REMOTE )", "company_name": "Pulivarthi Group (PG)", "location": "United States (Remote)", "link": "/jobs/view/3770685212/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=9gdMwuif%2BpB%2BCQYvQi3imQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3770685212", "description": "About the job\nFollow us on Linkedin: https://www.linkedin.com/company/pulivarthigroup/\n\nPulivarthi Group LLC is a Global Staffing & IT Technology Solutions company, with our prime focus of providing world class solutions to our customers with the right talent. We combine the expertise of our team and the culture of your company to help you with the solution that is affordable and innovative using high quality standards and technologies.\n\nWe\u2019ve served some of the largest healthcare, financial services, and government entities in the U.S.\n\nData Engineer - FT only - Azure SQL with data lakes experience\n\nData Engineer -\n\nthe resource requirement for this Data Engineer role needed more clarification.\n\nOur customer is requesting a resource who knows Azure CI/CD (Continuous Integration / Continuous Deployment) to build and integrate their DevOps pipeline.\n\nOnce the DevOps pipelines are mapped, they need the Data Engineer to connect to all the Data Sources, primarily their Data Lake.\n\nThe Customer (based in Houston) is interested in a resource in a time zone not too far removed from the Central Time Zone to allow real time collaboration."}, {"title": "Data Engineer", "company_name": "Otter Products", "location": "Fort Collins, CO (Remote)", "link": "/jobs/view/3766728302/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=77U3U7CAi1PxgDmpI3%2BiNQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3766728302", "description": "About the job\nOverview \n\nOtter Products is currently recruiting for a Data Engineer to join our Data team! You can be based in our Fort Collins, CO office with flexibility to work a portion of your time remotely, or 100% Remote in the U.S.\n\nThe Data Engineer applies professional experience, concepts, and company objectives toward building and operationalizing the minimally inclusive data necessary for the enterprise data initiatives following industry standard practices and tools. The bulk of the Data Engineer's work would be in designing, managing and optimizing data pipelines, then moving these data pipelines effectively into production for key data and analytics consumers like business/data analysts, data scientists or other stakeholder that needs curated data for data and analytics use cases across the enterprise. The Data Engineer also ensures compliance with data governance and data security requirements while creating, improving and operationalizing these integrated and reusable data pipelines to enable faster data access, integrated data reuse and vastly improved time-to-solution for data and analytics initiatives. Additionally, the Data Engineer will be expected to collaborate with other data team members and data consumers working on the models and algorithms developed by them toward optimization for data quality, security, and governance to put them into production leading to potentially large productivity gains.\n\n About Otter Products \n\nOtter Products, we grow to give. From our founder's garage in 1998 to the global technology leader we are today, Otter Products continues to drive growth through innovation.\n\nThrough our industry-leading brands - OtterBox, Liviri and OtterCares - we provide our partners the number-one selling and most trusted products in our categories. Our philanthropic spirit is the foundation on which we foster our partner relationships, allowing us to grow and to give - together.\n\nBy way of our charitable arm, the OtterCares Foundation, we support our communities and invest in our future through education that inspires kids to change the world.\n\nAnd even as our global community of Otters continues to grow, our founder's core values are still at the heart of everything we do. We measure our success by our ability to give back to our communities and strengthen opportunities for all.\n\nFor more information visit otterproducts.com\n\n Responsibilities \n\nComplete analysis, design and development of BI solutions using Azure SQL Server\nFamiliarity with Data Warehouse concepts\nExperience coding the extraction, transformation, and loading (ETL) processes\nDatabase development primarily in SSIS, Data Factory and SQL\nPartner with the business to determine end user database/reporting needs and requirements\nGenerate ad hoc reports using Power BI or SSRS\nCollaborate with other developers to create and implement best approach solution(s)\nTroubleshoot Azure tools, systems, and software\nReview queries for performance issues, making changes as needed\nCollaborate with team to performance-tune Azure application as necessary\nAssist with the analysis and extraction of relevant information from large amounts of historical business data to feed data science initiatives\nParticipate and support the design and documentation of processes for large scale data analyses, model development, model validation and model implementation\nSupport and maintain a positive safety culture by following all safety policies and procedures and actively contributing to a safe working environment\nOther duties as required\n\n Qualifications \n\nBachelor's degree required. Degree in Computer Science or Mathematics preferred\nMinimum of four years of experience in an IT or analytical role required. Experience in database development, report writing and/or statistics preferred.\n\n EEO \n\nOtter Products, LLC is an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, age, marital status, pregnancy, sex, sexual orientation, gender, gender identity or expression, national origin, disability, veteran status, or any other characteristic or status protected by law.\n\n For US Based Roles Only - Compensation Range Minimum \n\nUSD $90,000.00/Yr.\n\n For US Based Roles Only - Compensation Range Maximum \n\nUSD $120,000.00/Yr.\n\n Additional Total Rewards \n\nProfit Sharing Program Eligible, Benefits Eligible - Full Time- check out otterproducts.com/careers/why for more info"}, {"title": "Software Engineer, Distributed Systems", "company_name": "Hightouch", "location": "United States (Remote)", "link": "/jobs/view/3628572539/?eBP=CwEAAAGMd4QO8APODyE_-mMOPhYyQGg5ZjVbPwq_3V7DK0B2bLAf10pcedM9S2T71X5i4TiJuEwOzWBbSiTKbLuk-VD-_Y-dN5uyf8YpYkHYqEYFxySMS9zr2hRjNrxX9gXbxssh35TFK9YTNV2WeH2Ncb675JgFbJTgNWSZySFC6rrdJGmdSDTr2q5d3VoxORz3GjsDbzjeyZQCvQ07RqYw7oujWSgslycs2LMCY_-N0NEj6e0PvbaYKiZ_5IW34bGVSYLXpHwqa8oOcNttYZ90UA3E_7sDSFzABdIVlfrp5zzERrIYy5sh2MIInxHdDoKgQw8VmOn5uM4S0tXUl-i7LnN0_W0NfPBmLiVDNuveIOm4MfDNPxfe5QKPk6FUAVpvgbcoJn9yRyE&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=rph%2FT6Q6Vkp6YoggJf9cJQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3628572539", "description": "About the job\nAbout Hightouch\n\nHightouch\u2019s mission is to empower everyone to take action on their data. Through our Reverse ETL platform, business and data users can seamlessly sync data from where it resides, such as warehouses and databases, to where it is needed, including operational systems and SaaS tools. Traditionally, acting on data has required engineering time and bandwidth, and left most business users stuck with charts and reports that are unable to take automated action on their data. With Hightouch, every business user, without writing any code, can activate data to streamline critical processes, improve marketing performance, and scale operations.\n\nOur team operates with a focus on making a meaningful impact for our customers. We believe in approaching challenges with a first principles thinking mindset, moving quickly and embracing our value of efficient execution, and treating each other with compassion and kindness. We look for team members that are strong communicators, have a growth mindset, and are motivated and persistent in achieving our goals.\n\nHundreds of companies use Hightouch, including Spotify, Ramp, Retool, NBA, Plaid, and Betterment. We\u2019re based in San Francisco, are remote-friendly, and backed by leading investors such as Amplify Partners, ICONIQ Growth, Bain Capital Ventures, Y-Combinator, and Afore Capital.\n\nAbout The Role\n\nWe are looking for a distributed systems engineer to work on the systems that power the syncing engine our customers and other engineering teams rely on. This presents an exciting challenge where you can apply your expertise in distributed systems, performance optimization, and troubleshooting to push the boundaries of what is possible and meaningful improve our ability to move massive amounts of customer data. This role also provides a unique opportunity to work on a multi-cloud and multi-region infrastructure that supports a global customer base.\n\nWe believe in enabling our engineers to do their best work for our customers by giving them extremely high levels of ownership and autonomy. This comes in different forms: you will own and deliver projects from start to finish, you will work directly with customers to solve their hardest scaling problems, and you will have a lot of influence over what we work on as a team and company.\n\nSome of the problems we\u2019ll be working on include:\n\nSync Speed: Customers want to sync a lot of data to important destinations like Facebook and Snapchat, which requires us to analyze every part of our syncing process and find where we can optimize to sync data more quickly\nStreaming Syncing: We currently sync data from sources in batch only, but envision a world where we also do syncs in real-time and support streaming sources like webhooks and queues\nScalability and Reliability: As part of our rapid growth, we\u2019re always evaluating current future bottlenecks in scaling and reliability, and architecting for the next order of magnitude of growth\nPersonalization API: Our Personalization API product provides a low latency caching layer on top of data warehouses that enable customers to use their data warehouse for real-time personalization use cases with \nMulti-Region and Multi-Cloud: Supporting our multi-region and multi-cloud backend, including extending it to launch Hightouch on in new regions to support data residency requirements of our global customer base\n\nWe are looking for talented, intellectually curious, and motivated individuals who are interested in tackling the problems above. This is a senior role, but we focus on impact and potential for growth more than years of experience. The salary range for this position is $170,000 - $240,000 USD per year, which is location independent in accordance with our remote-first policy.\n\nAbout You\n\nYou are an engineer with a passion for solving hard technical problems that generate real value for customers. You\u2019re motivated by high ownership and are comfortable in a fast-paced, startup environment.\n\nYou have experience and comfort with distributed systems and high-scale systems or the ability and desire to build this experience quickly. This experience can take different forms:\n\nYou\u2019ve built syncing engines that process and move terabytes of data per day in a scalable way\nYou\u2019ve created event collection or data streaming services that power company critical systems\nYou\u2019ve owned systems that have undergone orders of magnitude of growth and helped them scale accordingly\nYou have relevant skills and are a fast learner who is excited to take on the challenges we face\n\nInterview Process\n\nOur goal with the interview process is to balance speed with giving both parties opportunities to assess whether there is a strong mutual fit. We will ask you questions, but we want you to ask us questions! Our technical interviews focus on how you design systems because we believe this is the best way for us to see how you work and for you to see how we collaborate. We don\u2019t ask you to write code to solve technical brainteasers that don\u2019t appear in your day to day job.\n\nRecruiter screen [30m]: Introductory call with our recruiting team to get to know each other and see if the role could be a good mutual fit.\nSystems design screen [45m]: Designing a data processing feature end-to-end.\nHiring manager interview [30m]: Chat with hiring manager about past experiences and future operating preferences to assess fit on company values and operating principles.\nSystems design interview [90m]: Work with the interviewer to architect a system at a conceptual level. The problem will be at a pretty high level - and have both product and customer requirements as well as technical."}, {"title": "Entry-Level Data Scientist Engineer", "company_name": "Phoenix Recruitment LLC", "location": "Texas City, TX (Remote)", "link": "/jobs/view/3788727625/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=8iwtytHsCu4wMwJ4fj3pxw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3788727625", "description": "About the job\nThis is a remote position.\n\nTitle - Entry-Level Data Scientist Engineer, 1 year of project experience\n\nEmployment Type: Full-time\n\nBase Salary: $60K-$70K\n\nPhoenix Recruitment offers a variety of recruiting services to assist both employers and employees. They specialize in marketing open positions, recruiting, and helping employers to find qualified candidates across various industries. Phoenix Recruitment has expertise in streamlining the hiring process. They can help ensure that the process is efficient, well organized, and compliant with relevant regulations.\n\nSkills and Abilities:\n\nStrong knowledge of R or Python for data analysis and modeling. \nProficiency in statistical programs such as R, SAS, MATLAB, or Python. \nFamiliarity with spreadsheets (VBA) and database applications (Access, Oracle, SQL, or equivalent technology). \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \nAbility to learn new methods quickly and work under deadlines. \nExcellent teamwork and communication skills. \nStrong analytical and problem-solving abilities. \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \n\nPreferred:\n\nKnowledge of actuarial concepts and life, health, and/or annuity products. \nExperience with statistical modeling techniques such as GLM, Decision Trees, Time Series, Regression, etc. \nFamiliarity with Microsoft DeployR. \nExposure to insurance risk analysis. \nBasic experience in computational finance, econometrics, statistics, and math. \nKnowledge of SQL and VBA. \nFamiliarity with R or Python for predictive modeling \n\nWhy Phoenix Recruitment LLC?\n\nPhoenix Recruitment often has an extensive network of employers and candidates. This network allows them to tap into a pool of qualified candidates and connect them with suitable job opportunities. They can also leverage their connections to help employers find the right talent efficiently. Outsourcing the recruitment process to a specialized agency can save your time and resources, avoid delays, reduce administrative burdens, and increase the chances of finding the right fit for your organization."}, {"title": "Data Engineer - Remote Work", "company_name": "BairesDev", "location": "United States (Remote)", "link": "/jobs/view/3783162297/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=OtoCrAAHZFomVup04fy35g%3D%3D&trk=flagship3_search_srp_jobs", "id": "3783162297", "description": "About the job\nWho We are\n\nBairesDev is proud to be the fastest-growing company in America. With people in five continents and world-class clients, we are only as strong as the multicultural teams at the heart of our business. To consistently deliver the highest quality solutions to our clients, we only hire the Top 1% of the best talents and nurture their professional growth on exciting projects.\n\nWe are looking for a Data Engineer to join our Development team and participate in different projects made up of multicultural teams distributed throughout the world.\n\nWhat You Will Do\n\n Interview other Data Scientists and Data engineers to vet their technical abilities.\n Input data from those interviews into the product and help develop a predictive, fair, and enjoyable solution.\n Work together with developers, tech leads, and solution architects to build applications.\n Improve existing structures by adding new functionalities or proposing technological updates so that as a result, the impact of your contribution is significant in the core of each business.\n Improve the user experience on scalable and high availability platforms, contributing to the key differential of each business.\n\nHere\u2019s What We Are Looking For\n\n 5+ years of experience working as a developer.\n 3+ years of experience working as a Data Scientist / Data Engineer.\n Proficient with analysis, troubleshooting, and problem-solving.\n Hands-on experience with managing data loads and data quality.\n Advanced English level.\n\nHow we do make your work (and your life) easier:\n\n 100% remote work.\n Hardware setup for you to work from home.\n Flexible hours - make your schedule.\n Paid parental leave, vacation & holidays.\n Diverse and multicultural work environment.\n An innovative environment with the structure and resources of a leading multinational.\n Excellent compensation \u2014 well above the market average.\n Here you can grow at the speed of your learning curve.\n\nOur people work remotely but with a consistent and robust culture that promotes diversity and teamwork. To continue being the leading software development company in Latin America, we want to ensure that every BairesDev member gets the best growth and professional development opportunities in a diverse, welcoming, and innovative environment.\n\nEvery BairesDev team member brings something unique to our company.\n\nWe want to hear your story. Apply now!"}, {"title": "Data Engineer (Remote)", "company_name": "Cascade Debt", "location": "United States (Remote)", "link": "/jobs/view/3755336571/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=pmUtn1j2zyPr25CNPh2RNw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3755336571", "description": "About the job\nDo you want to work in a small team where you can make a real difference in a company? Would you embrace the challenge of building a high impact platform that is used globally? Then we want to speak with you!\n\nAbout Cascade\n\nCascade is a fintech startup backed by Canadian and US investors that empowers companies to grow by democratizing access to institutional debt. We are building tools that modernize how companies raise and manage debt, lowering the barriers to entry to the $7 trillion specialty finance and alternative credit market for companies around the world. Founded just last year, we are gearing up for our next phase of technical development and are seeking a talented Data Engineer to lead the way.\n\nAbout The Role\n\nAs a Data Engineer you\u2019ll be an early member of a growing team building a pioneering platform in the debt infrastructure space. You will design, build, and launch efficient, scalable, and reliable data pipelines to move and transform data .\n\nWhat You\u2019ll Do\n\n Building databases, data lakes and data ingestion pipelines to integrate customer databases and datasets to our systems \n Ingesting financial datasets from external customers, then u pdating and maintaining accurate and complete data mappings to ensure that our product s are displaying high quality data \n M onitoring and alerting across our data pipelines in order to make sure that our data ingests are reliable and correct \n Translat ing business goals and stakeholder requirements into new data flows and deliver analytical insights ensuring that the data flows create real business value continuously \n\nSkills And Experience\n\n 2 + years experience in a data engineering or data analyst role \n E xperience with relational SQL and NoSQL databases, including PostgreSQL , MySQL, MariaDB, MongoDB, Bigtable, etc. \n E xperience working with ETL technologies , such as Databricks, Fivetran , or dbt \n Proficiency in writing SQL queries and knowledge of analytical data warehouses such as RedShift, BigQuery , and Snowflake \n Some experience with CI/CD automation such as GitHub Actions \n Developing APIs and integrating with 3rd party APIs \n Bonus: experience in fintech / SaaS / credit analysis \n\nPeople Who Thrive At Cascade Are\n\n Self-starters, who take the initiative to tackle challenges in a remote work environment \n Good communicators, who can operate without ego to discuss, learn, grow, and help others do the same \n Passionate about creating great digital experiences for users \n Problem solvers who are not satisfied with the status quo \n\nBenefits\n\nRemote: we are a remote-first company and are very flexible on hours as long as things get done \nHome-office: there\u2019s a $1000 USD home office allowance to set yourself up \nEquity: we expect you to have an owner-mentality, and have the equity plan to match \nBenefits: health, dental, vision, and more \nPerks: we offer free lunches weekly and off-site trips \nJob satisfaction: we offer autonomy, ample opportunities for mastery, and an opportunity to make a difference for companies around the world"}, {"title": "Data Engineer", "company_name": "ISC (Integrated Specialty Coverages, LLC)", "location": "Carlsbad, CA (Remote)", "link": "/jobs/view/3778293528/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ew7%2FfEuXSujs5IGabdqUyw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3778293528", "description": "About the job\nAbout Integrated Specialty Coverages\n\nIntegrated Specialty Coverages LLC (ISC) is a growth stage technology and data-driven insurance company leading innovation in the market.\n\nBacked by one of the leading private equity firms, KKR, and led by a forward-thinking management team, ISC is combining the worlds of insurance and technology to create an Insurtech powerhouse. As a leading online distributor of insurance products for a range of industries and \"Main Street USA\", we are looking for the right people to help us in our mission of achieving exponential growth. We strive to be the number one place to go for brokers and agents to source insurance. To accomplish this, we're building a digitally focused team that deeply understands the intersection between user experience, data, and AI/ML to optimize the way we engage with our customers and partners.\n\nJob Summary\n\nThe Data Engineer role will be the technical liaison between multiple groups including a data science team, the engineering team, product management, and business stakeholders. You do not need any insurance knowledge prior, however, you must quickly dive deep into the insurance world and ask questions to become a subject matter expert. You will be responsible for building a data platform to facilitate the data science team. You must be a self-starter that can build out features such as a data pipeline from scratch. There will be support from both engineering and data science for any buildout.\n\nJob Responsibilities\n\nCreate and maintain optimal data pipeline architecture. \nAssemble large, complex data sets that meet functional / non-functional business requirements. \nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. \nBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using AWS technologies, SQL, and Python. \nWork with stakeholders including the Executive, Product, Data Science, and Engineering teams to assist with data-related technical issues and support their data infrastructure needs. \nWork with data science and analytics teams to strive for greater functionality in our data systems. \n\nMinimum Qualifications:\n\nAdvanced working SQL knowledge and experience working with relational databases, strong query authoring (SQL) as well as working familiarity with a variety of databases (Snowflake, Redshift, MySQL, MSSQL, etc.)\nExperience building and optimizing data pipelines, architecture and data sets. \nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. \nBuild processes supporting data transformation, data structures, metadata, dependency, and workload management. \nA successful history of transforming, processing and extracting value from large disconnected, datasets from a variety of data sources (Flat files, Excel, databases, etc.)\nStrong analytic skills related to working with unstructured datasets. \nStrong project management and organizational skills. \nExperience supporting and working with cross-functional teams in a dynamic environment. \nAbility to mentor/guide/collaborate with other team members. \n\nWe are looking for a candidate with 3-5 years of experience in a Data Engineer role. They should also have experience using the following software/tools:\n\nExperience with relational and MPP databases, including Snowflake and MySQL. \nExperience developing software in an agile environment from the requirements stage to production. \nExperience with version control: git\nExperience with container technologies: Docker\nExperience with data pipeline and workflow management tools: Airflow, Jenkins, AWS Glue, Azkaban, Luigi, etc. \nExperience with AWS cloud services: EC2, ECS, Batch, S3, EMR, RDS, Redshift\nExperience with other cloud services: Snowflake, Airflow\nExperience with object-oriented/object function scripting languages: Python, Java, C++, etc. \nExperience with data modeling and data warehouse design\nExperience with data visualization tools (PowerBI, QuickSight)\n\nThe starting annual pay scale for this position is listed below. Actual starting pay will be based on factors such as skills, qualifications, training, and experience. In addition, the company offers comprehensive benefits including medical, dental and vision insurance, 401(k) plan with match, paid time off, and other benefits.\n\nISC's salary ranges are determined by role and level. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations and could be higher or lower based on a multitude of factors, including job-related skills, experience, and relevant education or training.\n\nNational Pay Range\n\n$100,000\u2014$110,000 USD\n\nBenefits of Working at ISC\n\nCompetitive vacation and flexible working arrangements\nComprehensive and inclusive health benefits\nA variety of professional development and mentorship opportunities\nChoice of technology whether at home or in the office\n\nISC's Ownership Behaviors\n\nDo the Right Thing *Be Relentless. Pursue Excellence. *Personal Responsibility & Own Outcomes\nTry Fast. Learn Fast. Fail Fast. Think Big. *Build Extraordinary Owners & Join Forces *Grit & Determination\n\nApplicants with disabilities may contact ISC HR department via e-mail, and other means to request and arrange accommodation. If you need assistance to accommodate a disability, you may request accommodation at any time. Please contact ISC at HR@ISCMGA.com.\n\nISC believes in creating long-term relationships by being responsive and relevant and by consistently delivering value to our community of customers. Specifically, with our employees, we focus on attracting, developing, and retaining the best talent for our business, challenging our people, demonstrating a \"can-do\" attitude, and fostering a collaborative and mutually supportive environment. ISC is an equal opportunity employer and a participant in the US federal E-Verify program (US).\n\nDiversity creates a healthier atmosphere: All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, gender, gender identity, sexual orientation, marital status, medical condition, genetic information, mental or physical disability, military or veteran status, or any other characteristic protected by local, state, or Federal law."}, {"title": "Data Engineer -Remote", "company_name": "RIT Solutions, Inc.", "location": "Denver, CO (Remote)", "link": "/jobs/view/3768007496/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=1hQH8EDaII1eJ7nQ9BACQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3768007496", "description": "About the job\nDuration: 6+ months CTH\n\nMust have a valid LinkedIn profile\n\nRequired Skills\n\n 3-6 years of experience (no more than 6 years)\n Start up environment experience\n Python, SQL, and DBT"}, {"title": "Big Data Engineer - W2", "company_name": "Megan Soft Inc", "location": "United States (Remote)", "link": "/jobs/view/3674845652/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fCAxh4N%2F4YZJ5aEIASaeeA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3674845652", "description": "About the job\nTitle: Big Data Engineer - W2\n\nLocation: Dearborn, MI (Remote)\n\nDuration: 12+ Months\n\nDirect Client: Ford Motors\n\nMust have skills: Should be able to handle GCP requirements\n\n\"Translates business requirements to build repeatable, sustainable, efficient, coded processes that can be productionized by Software Engineers and readily modified by other Data Engineer Creates POC processes in Dev/QA and works with Software Engineers to productionize and align best coding and process practices that maximize efficiency, speed, stability, system resources and capabilities Awareness of and compliance with data privacy, security, legal and contractual guidelines Incorporates data quality and privacy checks/alerts to minimize bad data being consumed by end users, models and dashboards, and to protect customer data; revises checks for new data issues Validates data products and pipelines are functioning as expected following system or application upgrades, source changes, etc. Works closely with Architects if required to align systems, tools and applications being utilized with business use case and performance requirements Provides data product support and maintenance Establishes, tracks and monitors KPIs related to specific data products and deliverable Participate in iteration cadence activities (Iteration Planning, task estimation, daily scrum, demos, etc.). Will be a part of on-call team to resolve critical production support tickets during business hours and off hours.\n\nSkills Required\n\nExperience with Big Data Technologies (Hadoop , ETL Tool, Spark. Hive etc..) Someone who understands Cloud and has experience working with Cloud Platforms (preferably GCP) Should be able to handle GCP requirements - ingesting into GCS/ Big Query, working with GCP components Hands-on in Data Flow, Cloud Storage (GCS), Big Query and GitHub. Proficient in Python and SQL.\n\nSkills Preferred\n\nManages ETL: uses programming and tools for data ingestion, configures pipelines, applies transformations and decoding, integrates and fuses data, moves and securely deliver Experience NoSQL databases\n\nAdditional Information\n\nIf the candidate is remote only, please indicate \"\"100% Remote\"\" under candidate's name on resume. Other similar schedule notations can be \"\"Local Candidate\"\" or \"\"Hybrid Candidate.\"\" ***HRA Test Required***\""}, {"title": "Remote Opportunity: Quantexa Data Engineer", "company_name": "SPAR Information Systems LLC", "location": "United States (Remote)", "link": "/jobs/view/3680073191/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ioykVCZSOQ7BK5%2FwRQHOWg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3680073191", "description": "About the job\nHello All, Hope you are doing great!! Please go through the job descriptions and let me know your interest. Role: Quantexa Data engineer  Location: Remote Duration: Long Term Contract Job Description: 1. End to End usage of Quantexa including guiding our client team on how to use from both front end and data ingestion Reason they are asking for Certification 2. Resolve Entity issues with data 3. Knowledge of Scala and Hive which requires coding experience using parquet 4. Advanced SQL knowledge requiring writing SQL queries 5. Experience having worked with GCP data lake 6. Quantexa certification required\n\nThanks & Regards,\n\nSatnam Singh\n\nDirect: 201 623 3660\n\nEmail : Satnam.singh@sparinfosys.com"}, {"title": "Remote Work - Need Staff Data Engineer - USC only", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3746283980/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=CvIuLBNO%2BPMGi4IXVRkP0A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3746283980", "description": "About the job\n100% Remote, client is in NY\n\nLook for strong job history, Python, Machine Learning, SQL, 10/10 comms, etc. \n\nMUST be an Engineer \n\nSummary\n\nWe're looking for an experienced Data Engineer to join our client's growing team of top notched IT professionals. You will work closely across multiple groups including Editorial and Audio, Marketing, Advertising, etc. to achieve their goals and objectives. The Data Engineering team builds tools throughout the stack while sharing knowledge across these departments.\n\nAs the company grows, they're looking for Data Engineer who will help solidify and expand our pipelines and maintain their data warehouse. Their business model is based on performing complex and very detailed analyses of how our products perform with their subscribers. You'll be responsible for working alongside analysts, product, marketing, and other teams to define new data collection components and measurement schemes that support each new product and feature.\n\nHow Can Add Value To Their Mission\n\nCreate and maintain data pipelines to provide insights and drive business decisions\nEstablish data warehousing strategy (ex. Kimball, Data Vault, etc.)\nMaintain data infrastructure on our AWS accounts\nCollaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.\nWrite unit/integration tests, contributes to engineering wiki, and documents work\nImplement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it\n\nWhat You'll Need To Succeed\n\n7+ years of industry experience measuring product performance and user behavior\nExperience working with a variety of data management technologies, including RedShift, Kinesis/Kafka, Glue, Spark, Postgres, Airflow, dbt, and others.\nExperience implementing BI reporting tools such as Looker\nExperience interfacing with engineers, product managers and analysts to understand data needs\nKnowledge of a variety of measurement beacons, SDKs, APIs including Google Analytics, Amplitude, Braze, Stripe, email service providers\nA commitment to building secure, resilient, fault-tolerant architectures, with clear documentation and procedures in place for support\nA focus on accuracy and detail as you build, ensuring data pipelines produce clear, predictable results\nUnderstanding of the typical metrics a subscription and advertising-supported business needs to measure success\nExperience with Client techniques as applied to behavioral segmentation or anomaly identification is a definite plus\nFamiliarity using developer tools that increase productivity and facilitate the development of resilient code (eg. Docker, CircleCI, Serverless) is a plus"}, {"title": "ENTRY LEVEL DATA ENGINEER (REMOTE)", "company_name": "SynergisticIT", "location": "Jacksonville, FL (Remote)", "link": "/jobs/view/3767587909/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=om7L3CnowiC4PLIcpZD70A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3767587909", "description": "About the job\nSYNERGISTICIT wants every candidate to know that the Job Market is Challenging and to stand out, you need to have exceptional skills and technologies and that's where we come in to make sure you get the attention which you need\n\nSynergisticIT understands the complex nature of the job market and how difficult it can be to secure a position, especially for fresh graduates. Therefore, we assist and help tech-savvies to convert their passions into professions. We go above and beyond to keep you working in your niche.\n\nAs we focus on long-term success, we provide complete career development solutions. From job search to upskilling portfolio and interview preparation, we can guide you at every step of your career.\n\nSynergisticIT spares no efforts to connect you with a large network of tech giants, including Google, Apple, PayPal, Dell, Cisco, Client, etc. Presently, we are actively looking for ENTRY LEVEL DATA ENGINEER a driven mindset. Get the right opportunity and gain experience in building web-centric solutions on Java.\n\nWho Should Apply : Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or anyone looking to make their career in IT Industry\n\nWe also assist in filing for STEM extension and H1b and Green card filing.\n\nCandidates who are serious about their future in the IT Industry and have set big goals for themselves.\n\nCandidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. We also offer Skill Enhancement Programs if the candidates are missing skills or experience which our clients need with great outcomes\n\nCandidates can benefit from skill enhancement if they fall into the below categories. If they are qualified with enough skills then no need for skill enhancement\n\nCandidates Who Lack Experience\n\nHave had a break in careers\n\nLack Technical Competency\n\ncandidates who want to get employed and make a career in the Tech Industry \n\nPlease Also Check The Below Links\n\nhttps://www.synergisticit.com/candidate-outcomes/\n\nhttps://www.synergisticit.com/java-track/\n\nhttps://www.synergisticit.com/data-science-track/\n\nhttps://www.synergisticit.com/which-is-the-best-option-for-tech-job-seekers-staffing-companies-consulting-companies-bootcamps-or-synergisticit/\n\nhttps://www.synergisticit.com/contact-us/\n\nIf the skills are not a match candidates can opt for Skill enhancement. Or their resume can be sent out to clients to see if responses are achievable\n\nREQUIRED SKILLS For Java/Software Programmers\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Core Java , javascript , C++ or software programming\nSpring boot, Microservices and REST API's experience\nExcellent written and verbal communication skills\n\nFor data Science/Machine learning\n\nRequired Skills\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Statistics, Python, data visualization tools\nExcellent written and verbal communication skills\n\nPreferred skills: NLP, Text mining, Tableau, Time series analysis\n\nTechnical skills are required by clients for selection even if its Junior or entry level position each additional Technical skill helps a candidate's resume to be picked by clients over other job seekers.\n\nClients hire candidates with the right technical skills which they need and reject candidates who lack the required technical skills.\n\nNo third party candidates or c2c candidates\n\nPlease apply to the posting\n\nNo phone calls please. Shortlisted candidates would be reached out"}, {"title": "Data Engineer - Remote | WFH", "company_name": "Get It Recruit - Information Technology", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3782241984/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=BgBf1kSAAX%2BSuw6inJsIVw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3782241984", "description": "About the job\nJoin an innovative team at a leading company in the engineering and manufacturing industry, where we are committed to leveraging data for continuous improvement and operational excellence. This hybrid position allows you to enjoy the flexibility of remote work on Mondays and Fridays.\n\nWhy Join Us\n\nAt our company, we prioritize your well-being and professional growth. We offer a comprehensive benefits package, including excellent health benefits, a 6% 401(k) match, generous paid vacation and sick time, 12 paid holidays, and more.\n\nRole Overview\n\nAs a Data Engineer, you will play a pivotal role in transforming raw data into actionable insights, driving improvements in engineering and manufacturing processes, enhancing product quality, and optimizing operational efficiency. Collaborate with dynamic cross-functional teams to extract, analyze, and visualize data, influencing strategic decisions through your valuable insights.\n\nKey Responsibilities\n\nData Collection and Integration:\n\nGather, clean, and integrate data from diverse sources, including production databases, sensor data, and external datasets.\n\nData Analysis\n\nConduct exploratory data analysis to identify patterns, trends, and anomalies.\n\nPerform statistical analysis and hypothesis testing to derive meaningful insights.\n\nData Visualization\n\nCreate informative and visually appealing dashboards and reports using tools like Tableau, Power BI, or Python libraries.\n\nPredictive Modeling\n\nDevelop predictive models to forecast manufacturing performance, quality, and operational outcomes.\n\nRoot Cause Analysis\n\nInvestigate production issues and quality concerns by analyzing data to identify root causes and recommend corrective actions.\n\nPerformance Monitoring\n\nEstablish and maintain key performance indicators (KPIs) to track manufacturing and engineering metrics.\n\nGenerate regular reports to communicate performance trends to stakeholders.\n\nCross-functional Collaboration\n\nCollaborate with engineering, manufacturing, and quality assurance teams to understand their data needs and provide analytical support.\n\nContinuous Improvement\n\nSuggest process improvements and optimizations based on data analysis.\n\nData Security And Compliance\n\nEnsure data security and compliance with relevant regulations, such as GDPR or industry-specific standards.\n\nInfrastructure And Development\n\nBuild and maintain the infrastructure for collecting, storing, and processing data.\n\nDesign, develop, and manage data pipelines, ETL processes, and data warehouses.\n\nRequired Experience\n\nProven experience as a Sr. Data Analyst and/or Data Engineer in an engineering or manufacturing environment.\n\nProficiency in data analysis tools and programming languages, such as Python, R, SQL, and Excel.\n\nStrong data visualization skills with tools like Tableau or Power BI.\n\nExcellent knowledge of statistical analysis and machine learning techniques.\n\nAbility to communicate complex findings and insights effectively to both technical and non-technical stakeholders.\n\nStrong problem-solving skills and attention to detail.\n\nFamiliarity with manufacturing processes and quality control is a plus.\n\nExperience with enterprise ERPs (SAP S4HANA, Oracle EBS), CRMs (Salesforce, Hubspot), and IoT.\n\nProficient in working with cloud computing platforms including AWS, GCP, and Azure.\n\nExpertise in configuring and optimizing cloud-based data solutions for scalability, reliability, and performance.\n\nBachelor's Degree in Data Science, Statistics, Engineering, or Computer Science Required.\n\nApply Now and be part of a team shaping the future of data-driven innovation in engineering and manufacturing!\n\nEmployment Type: Full-Time"}, {"title": "Entry Level Software Engineer - Data Backend Engineer (Remote - Canada)", "company_name": "Yelp", "location": "United States (Remote)", "link": "/jobs/view/3752881757/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fankeoxifvhpTzOYvowHYQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3752881757", "description": "About the job\nYelp engineering culture is driven by our values: we\u2019re a cooperative team that values individual authenticity and encourages creative solutions to problems. All new engineers deploy working code their first week, and we strive to broaden individual impact with support from managers, mentors, and teams. At the end of the day, we\u2019re all about helping our users, growing as engineers, and having fun in a collaborative environment.\n Interested in exploring data and finding innovative ways to collect it, curate it, and make it easily accessible for others to use? How about doing that at an enormous scale? Yelp\u2019s datasets contain billions of interactions between users and local businesses around the globe. If working with our vast amount of data sounds exciting, come and join us!\n\nThe Marketplace Data Observability team is responsible for managing, governing, and monitoring all of Yelp\u2019s Ads data to protect our revenue stream and also support offline analysis that aids in making long term company decisions. We build tools that help us to better understand our data quality, and we strive to uncover data issues before they impact our consumers.\n\nAs a Data Backend Engineer on this team, you will be responsible for building elegant and scalable data products that serve critical, up-to-date, structured information to support different types of analytics within Yelp. As a core contributor to our growing data modeling and data warehousing engineering efforts, you will help design and own mission-critical data flow pipelines and datastores to enable decisions including effective A/B testing and company investments.\n This opportunity is fully remote and does not require you to be located in any particular area in Canada. We welcome applicants from throughout Canada. We\u2019d love to have you apply, even if you don\u2019t feel you meet every single requirement in this posting. At Yelp, we\u2019re looking for great people, not just those who simply check off all the boxes.\n  Build systems that can effectively store and crunch terabytes of data.\nDesign and develop data models for efficient data storage, retrieval, and reporting.\nCreate and maintain conceptual, logical, and physical data models using industry-standard modeling tools.\nCollaborate with cross-functional teams, including engineers, data analysts, business analysts, and data scientists, to understand data requirements and translate them into effective data models.\nParticipate in data integration efforts, including ETL processes and data migration.\nStay up-to-date with industry best practices, emerging technologies, and trends related to data warehousing.\nSupport on-call rotations as needed to operate the team.\n  Understanding of high performing and scalable data systems.\nExperience in building and orchestrating ETL pipelines.\nExperience with Data Lake or Data Warehouse landscape.\nA hunger for tracking down root causes and fixing them in systematic ways.\nAbility to communicate effectively to technical and non-technical cohorts alike. \nExposure to some of the following technologies: Python, AWS Redshift, AWS Athena / Apache Presto, Big Data technologies (e.g S3, Hadoop, Hive, Spark, Flink, Kafka etc), NoSQL systems like Cassandra, DBT."}, {"title": "Data Engineer (Associate Consultant), June 2024", "company_name": "UDig", "location": "Nashville, TN (Remote)", "link": "/jobs/view/3716039960/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=XUjo40kU55HnDCqJ%2F3FdPg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3716039960", "description": "About the job\nCan UDig It? \n\nUDig designs, builds, and implements technology solutions that deliver on business objectives. People join our team because they are excited to have an impact on the future of our organization, passionate about delivering meaningful results, and energized by solving complex problems. Oh, and they like working with other driven, smart, and authentic people!\n\nUDig is proud to be recognized as a \"Best Places to Work\" in Virginia and Tennessee. We're committed to fostering a transparent environment where leadership is accessible, professional development opportunities are abundant, and you are empowered to achieve your career goals. You'll collaborate, develop deep relationships with teammates, and accelerate your success while having fun!\n\nIf you're graduating in May 2024 and excited by the idea of impacting organizations by building data solutions to achieve business goals, you should consider joining our growing technology team.\n\nHow Things Go Down\n\nAs a new, entry-level teammate, you'll take part in our \"Breaking Ground\" training program designed to help you quickly get up-to-speed on UDig's delivery approach within a project team. The program covers:\n\nUDig delivery process and methodologies such as gathering requirements and delivering projects using agile\nTechnical guidance and instruction on the best practices from Data leaders at UDig\nDelivering executive and technical presentations\nCoaching to prepare you to for success on your first client project at UDig\n\nA typical day might entail:\n\nProject standups and check-ins\nDesign and development of data pipelines, BI reports, and more\nPlanning and participating in project demos\nContributing to internal UDig projects\nCareer and job growth through UDig-provided training\nCreating long-lasting friendships with teammates through fun activities as well as your projects (including social events, winning a prize for guessing the weight of pumpkin, or sharing experience on cool tech side projects)\n\nUDig believes it is important for Associate Consultants going through our Breaking Ground program to reside in one of our major markets (Richmond, VA or Nashville, TN metro area) for at least the first year of employment.\n\nWant to learn more? Hear directly from our teammates by checking out our 'A Day in the Life' blog series: https://www.udig.com/digging-in/breaking-ground-2023/\n\nSound Interesting? Here's the Technical Piece\n\nBelow are a few boxes you should be able to check if\u202fyou're interested: Don't check every box? Go ahead and apply!\n\n1+ year of experience with relational database management systems such as Microsoft SQL Server, MySQL, or Oracle\nUnderstanding of the full lifecycle of design, development, and implementation of data solutions including architecture design, development, testing\nSome experience in ETL development or reporting is a plus\nAbility to present technical ideas and high-level concepts and solutions to internal and external team members with varying degrees of technical knowledge\nBachelor's Degree in in computer science, information technology, or any science or business discipline\nAbility to take your work seriously but also seriously enjoy a good time with your UDig colleagues\n\nWhat's in It For You? \n\nAt UDig, you will build your career alongside talented and experienced developers while gaining experience and having fun! We provide continuous challenges and professional development opportunities. Additionally, we offer: \n\nCompetitive salary, merit reviews, and career advancement paths\nFlexible, hybrid environment\nIndividual $1500 Training Budget\nRegular team building and social activities (virtual and/or in-person)\nTransparent culture with strong communication and access to leadership\nGreat benefits like\nGenerous Paid Time Off, company holidays, and parental leave\nMultiple Single and Family Health Insurance plans to choose from\nDental & Vision coverage\nShort-Term and Long-Term disability\nOptional accident and critical illness coverage\nMatching 401(k)\nand more!"}, {"title": "Expression of Interest: Data Engineer", "company_name": "Fingerprint for Success (F4S)", "location": "New York, NY (Remote)", "link": "/jobs/view/3787769880/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=l%2FEDpQqUNmCttGdIyTk7Cg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787769880", "description": "About the job\nWe are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.\n\nThe F4S Talent Pool is a pilot project designed to:\n\nHelp job seekers get discovered by our partners based on their anticipated hiring needs.\nProvide optional support and resources for job seekers in their career endeavors.\nHelp individuals understand, and bring out the best in themselves and each other.\n\nThe F4S Talent Pool process:\n\nOnce you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.\n\nAbout Fingerprint For Success (F4S)\n\nBacked by 20+ years of research, F4S\u2019s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high performing teams.\n\nKeep in mind, joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your F4S work style assessment to match the hiring needs of our partners.\n\nYour feedback is a gift! Write to us via:\n\nPowered by JazzHR\n\noWuPJ9lwq9"}, {"title": "Data Engineer Seattle, WA (remote)", "company_name": "Conch Technologies, Inc", "location": "United States (Remote)", "link": "/jobs/view/3765052211/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=SBETI78UMutCHuRp5FnP%2FA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3765052211", "description": "About the job\nHi,\n\nGreetings from Conch Technologies Inc\n\nData Engineering Lead \u2013 With Databricks and Unity Catalog\n\nLocation \u2013 Seattle, WA (remote)\n\nDuration \u2013 12 months\n\nNeed: Design and implement data architecture and strategy, leveraging technologies such as Databricks, Unity Catalog, Privacera, and Collibra.\n\nAs a Data Engineering Lead, you will play a pivotal role in defining and executing technical deliverables while assessing complexities and levels of effort. You'll engage with both vendor and internal teams to resolve technical challenges and ensure seamless collaboration. Your leadership will extend to planning and prioritization with the SCRUM master, facilitating effective coordination between onsite and offshore teams, and driving feature prioritization through collaboration with product managers.\n\nKey Responsibilities\n\n Lead the definition and breakdown of technical deliverables, assessing complexities and levels of effort.\n Collaborate with vendor and internal teams to address technical issues.\n Drive planning and prioritization in conjunction with the SCRUM master.\n Coordinate with onsite and offshore teams for timely project delivery.\n Collaborate with product managers to prioritize features, communicate with stakeholders, and clarify requirements.\n Collect and manage user-reported issues, ensuring prompt resolution and communication.\n Define comprehensive test plans, oversee their execution, and document results.\n\n--\n\nWith Regards,\n\nNagesh G\n\nMobile: 408-381-5645\n\nDesk: 901-313-3066\n\nEmail: nagesh@conchtech.com \n\nWeb: www.conchtech.com"}, {"title": "Remote Work - Need Azure Fabric Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3759325599/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=C3iVjmcR9wjUip1nNjUiCg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3759325599", "description": "About the job\nThe client is placing heavy emphasis on Microsoft and Azure Fabric experience\n\nInformation Management Engineer \u2013 Take the identified information management sources and assist migration to the Client Data Lake, Azure Fabric. The engineer will transform information management (de-dup, clean and enrich).\n\nTake the identified data sources and bring them into our Data Lake, Azure Fabric. Once the data is in, the engineer will need to be able to transform data (de-dup, clean and enrich the data). We are focused on using a medallion data model for data architecture. The tools we are working with are: Delta Parquet files, python, spark, notebooks, T-SQL, Fabric lakehouse/warehouse, Dataflows Gen2, Data Pipelines, Direct Lake, shortcuts, ADF."}, {"title": "Data Engineer", "company_name": "Catalist", "location": "Washington, DC (Remote)", "link": "/jobs/view/3744951279/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ui1Fk6ICmnPNqAlZDZAcwA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3744951279", "description": "About the job\nFor over 17 years, Catalist has been a leader in civic data and data science innovation. Our mission is to provide progressive organizations with the data, software, and services needed to better identify, understand, and communicate with the people they need to engage and mobilize. Our clients include the largest, most influential organizations in the U.S. active in civic engagement, advocacy, and political campaigns.\n\nCatalist is home to a dedicated, creative team of technologists, data scientists, and campaign experts committed to using our talents and technology to nurture a vibrant and growing progressive community.\n\nAs a Data Engineer at Catalist, you will work closely with the Analytics and Technology departments to design, build, support, and maintain various data pipelines and processes with an end goal of providing data and intelligence to the progressive community. This role primarily involves translating data architecture designs into functional processes, code, and systems.\n\nThe ideal candidate will be a highly motivated individual with excellent technical skills, a strong desire to learn new skills, and an interest in progressive politics. Catalist values creativity and problem-solving. Our work is on the cutting edge of data-driven politics, and your support will help Democratic candidates and progressive organizations conduct successful advocacy and electoral campaigns.\n\nThis position reports to the Deputy Chief Data Officer. The Data Engineer is a part of a growing Data team that supports all underlying work at Catalist.\n\nThis position is included in our CWA bargaining unit.\n\nPrinciple Duties & Responsibilities\n\nDevelop scalable, production processes, code, and systems for data ingest, transformation, modeling, and reporting across a variety of platforms\nTranslate mock-ups and designs into functional processes, code, and systems\nCreate architecture designs when needed\nProvide quality assurance and testing on processes, code, systems, and products\nBecome an internal subject matter expert on various datasets and support other Catalist departments/teams on usage of those datasets\nExecute ad-hoc data and database maintenance tasks as requiredProject manage cross-departmental efforts, with direct responsibility for stakeholder engagement, management, and execution of technical elements\n\n\nRequirements\n\nBS or BA in a technical field, or relevant experience\n1-2 years of experience working with SQL databases\nExperience with data cleaning or ETL processes\nExperience with distributed computing systems and/or distributed datastores (particularly the Hadoop ecosystem)\nExperience managing projects\nFamiliarity with Catalist data, progressive politics, voter files, and/or commercial data\nBackground check required\n\n\nPreferred Skills & Abilities\n\nWillingness to be a problem solver and produce results in a fast paced environment\nAbility to be creative and personable, and articulate ideas clearly\nExperience working with SQL databases\nProficiency with Python or another object-oriented programming language (R, Java, Scala, etc\u2026)\nExperience working in cloud environments (AWS, GCP, etc.)\nExperience working in command line environments such as Bash\nExperience with a version control tool such as git or github\n\n\nBenefits\n\nMedical, Dental, Vision, Prescription Drug\n\nCatalist offers Medical, Dental, Vision, and Prescription Drug coverage for eligible staff and their eligible dependents. Catalist\u2019s Medical plan is a comprehensive PPO program including Prescription Drug coverage with 85% of the premium paid by Catalist. Dental and Vision coverage is provided at no cost to employees.\n\nGroup Term Life Insurance and Long-Term & Short-Term Disability Coverage\n\nGroup Term Life Insurance and Long-Term and Short-Term Disability coverage is available for eligible staff. These benefits are provided at no cost to Catalist employees.\n\n401(k) Safe Harbor Plan\n\nA 401(k) Safe Harbor Plan is available to eligible staff with a 3% contribution from Catalist from the date of hire. Employees may contribute pre-tax or post-tax from their salary up to the legal limits set forth by the IRS.\n\nMedical and Dependent Care Flexible Spending Accounts (FSAs)\n\nCatalist offers an FSA Program that gives eligible staff the ability to pay out-of-pocket medical/dental/vision/child care expenses from pre-tax earnings.\n\nTransit Benefits\n\nCatalist also makes available a Transit benefit FSA program to eligible employees using pre-tax contributions with a company match.\n\nProfessional Development and Remote Work Expenses\n\nEligible employees may be reimbursed up to $750 each year for professional development / education and remote work expenses.\n\nStudent Loan PayDown or SaveUp\n\nCatalist offers a Student Loan PayDown and College SaveUp benefit for eligible staff.\n\nVacation, Personal Leave, Sick Leave Benefits\n\nCatalist offers generous vacation benefits to all eligible staff. Eligible employees also receive:\n\n 14 Paid Holidays\n Personal Days\n Sick Leave\n Parental Leave\n\n\nHybrid Office/Remote Work\n\nCertain positions at Catalist are eligible for Office/Remote Hybrid or full Remote status."}, {"title": "Remote Entry Level Data Analyst/Engineer", "company_name": "SynergisticIT", "location": "White Plains, NY (Remote)", "link": "/jobs/view/3774171423/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=q54sk1%2FRww0z%2BC%2BE8e0jQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774171423", "description": "About the job\nAt SynergisticIT, we're all about making connections. Whatever IT goals you have, our software programmers can help achieve those. Our Software development teams can take up turnkey projects and execute them in an effective and efficient manner. If you are looking to source talent our recruiters will find the ideal IT talent for your company. What's the secret to our success? Well, it all starts with taking quality time to listen to each client's specific needs. After we have a thorough grasp of your IT goals, we can better customize our Developments as per your specific needs. We can also tailor make recruiting programs to exceed your expectations. Since our founding in 2010, SynergisticIT's strategies have earned the company an enviable position in the software development, IT staffing and IT skill enhancement fields. SynergisticIT continues to work with hundreds of satisfied American clients with our software programmers working on our projects and after gaining hands on experience on cutting edge technologies moving to contribute their skills to great clients like Apple, Google, Client, Ebay, Paypal, Kroger, the Walt Disney Company and hundreds more. If you are tired of working with inefficient programmers who take a lot of time to ramp up we want you to try us. Our software programmers can hit the ground running and get you the maximum return on your investment. You have already tried the rest its time you tried the best. SynergisticIT - Home of the Best Data Scientists and Software Programmers in the Bay Area.\n\n Why Us ? \n\nSynergisticIT has a proven track record of successfully skill enhancement and staffing IT employees for some of the world's most iconic brands. Our team takes the time to fully understand every client's needs so we could best meet your IT staffing requirements. The knowledgeable staff at SynergisticIT is always more than happy to work with clients to ensure they reach their software development goals. Besides staffing, SynergisticIT is also committed to helping young IT professionals advance their career with a robust upskill program . Everyone who goes through SynergisticIT's program learns all the skills necessary to succeed in many IT fields ranging from Java to Machine Learning. Additionally, everyone trained at SynergisticIT has been through extensive mock and technical interview screenings to bolster their career prospects. Last, but certainly not least, SynergisticIT takes great care to respect the privacy considerations for every client. All companies who work with SynergisticIT can rest assured their confidential data is protected using the most up-to-date encryption technologies. SynergisticIT also complies with all the latest NDA agreements.\n\n REQUIRED SKILLS For Java /Software Programmers \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Time series analysis \n\n We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2022 and at Gartner Data Analytics Summit (Florida)-2023 \n\nOracle CloudWorld Event (OCW) Las Vegas 2022 | SynergisticIT - YouTube\n\n https://www.youtube.com/watch?v=OAFOhcGy9Z8 \n\n https://www.youtube.com/watch?v=EmO7NrWHkLM \n\n https://www.youtube.com/watch?v=NVBU9RYZ6UI \n\n https://www.youtube.com/watch?v=Yy74yvjatVg \n\nSynergisticIT at Gartner Data and Analytics Summit 2023 - YouTube\n\nFor preparing for interviews please visit  https://www.synergisticit.com/interview-questions/ \n\n We are looking for the right matching candidates for our clients \n\n Please apply via the job posting \n\n REQUIRED SKILLS For Java /Full Stack/Software Programmer \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning Positions \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Tensorflow \n\n If you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team. \n\n No phone calls please.  Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates"}, {"title": "Data Engineer", "company_name": "Anika Systems", "location": "Leesburg, VA (Remote)", "link": "/jobs/view/3787766893/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=mApL9JnN%2B%2Fr2%2BYUPlxRYOA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787766893", "description": "About the job\nAnika Systems is a fast growing, woman-owned small business that specializes in providing innovative IT solutions for federal government agencies. Our expertise lies in accelerating delivery in Data and Analytics, Intelligent Automation, Application Development, and IT Modernization. We are currently expanding our Federal team and are seeking a passionate and talented Data Engineer. This opportunity is 100% remote.\n\nMust be a U.S. Citizen with the ability to obtain and maintain a government suitability clearance\n\nResponsibilities\n\nDesigns, develops, builds, analyzes, evaluates, and installs database management systems to include database modeling and design, relational database architecture, metadata, and repository creation and configuration management.\nDefines and oversees database organizations, standards, controls, procedures, and documentation. Provides technical consulting in the definition, design, and creation of a database environment.\nAdvises applications development staff and users on data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nDesigns and implements databases with respect to access methods, access time, batch processes, device allocation, validation checks, organization, protection and security, documentation, and statistical methods.\nUses data mapping, data mining, and data transformational analysis tools to design and develop databases.\nDetermines data storage and optimum storage requirements.\nPrepares system requirements, source analysis, and process analyses and designs throughout the database implementation.\n\nRequired Skills And Experience\n\nBA/BS and 1 year of relevant experience\nExperience with DataBricks, SQL, Python\nApplications development experience with data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nExcellent communication skills (both oral and written) with process-oriented organizational skills to ensure project success\n\nDesired Skills And Experience\n\nDemonstrated successful development experience with the full life cycle of government database management systems including database modeling and design, relational database architecture, metadata and repository creation, and configuration management.\n\nPowered by JazzHR\n\nTlKWIorucJ"}, {"title": "Remote Work - Need Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3774712978/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=r3Bp%2F0qcOD2OpC5jpbUOcA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774712978", "description": "About the job\nTitle : Data Engineer\n\nLocation : Remote\n\nDuration : 6+ months to start (long term contract (work order are extended 2x year))\n\nPlease Fill out the skill matrix: Technology Some Awareness (1) Novice (2) Intermediate (3) Advanced (4) Expert (5) SQL Tableau Desktop /Creator Tableau API Integration AWS - S3 Glue AWS - Athena Redshift AWS - Lambda AWS - SNS/SQS msExcel\n\nProfile: Data Analyst will work closely with Architects and leads in the Interflow team, key business stakeholders including program staff to support the organizations effort in the areas of data Analysis and information management. Role should have strong experience in writing SQL queries to extract data for reports. Candidate should have experience working with large set of data. As these reports are very critical for the company and we are looking for an expert who is capable of handling, consuming and extracting large sets of data for analysis and reports.\n\nMust Haves\n\nAWS - Athena\nAWS - S3\nGlue\nhands-on development of reporting applications\nMS Excel\nRedShift\nSQL\nTableau Desktop /Creator\n\nNice\n\nAWS - Lambda\nAWS - SNS/SQS\nTableau API Integration\n\nSkills\n\nBA/BS required (major in an analytical field desired) \nA minimum 10+ years work-related experience. \nExperience in hands-on development of reporting applications for the Web in a professional environment \nThe Ethos of continuous improvement and interest in learning new things.\nStrong analytical thinking and structured problem-solving ability\nAbility to handle multiple projects and assignments simultaneously and effectively in a cross-functional team environment. \nVersed on the agile methodology and best practices. \nExcellent interpersonal and collaboration skill with the ability to work with a diverse set of colleagues, across functions, from different organizations, disciplines, etc. \nSelf-starter, ability to set priorities, work independently and attain goals"}, {"title": "Data Engineer", "company_name": "Team Remotely Inc", "location": "Houston, TX (Remote)", "link": "/jobs/view/3786249829/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=78lY5Jo%2FfyRPC6TQYCdvww%3D%3D&trk=flagship3_search_srp_jobs", "id": "3786249829", "description": "About the job\nThis is a remote position.\n\n Data Engineer (US/Canada Residents Only, 1 year experience, remote)\n\nTeam Remotely Inc. is a staffing and recruitment agency that offers a comprehensive solution for talent acquisition, including sourcing, vetting, pay rolling, and managing talent. Whether you need contract staffing, direct hire, direct sourcing, talent pools, or diversity initiatives, our model can support your hiring strategy.\n\n Hiring Type: Full-Time\n\n Base Salary: $55K-$70K Per Annum.\n\nResponsibilities\n\nDesigning user interface changes for web-based DB applications. \nReviewing application requirements and interface designs. \nDeveloping and implementing highly responsive user interface components using react concepts. \nWriting application interface codes using JavaScript following react.js workflows. \nTroubleshooting interface software and debugging application codes. \nDeveloping and implementing front-end architecture to support user interface concepts. \nMonitoring and improving front-end performance. \n\nQualifications And Experience\n\nBachelor\u2019s degree in computer science, information technology, or a similar field. \nPrevious experience working as a react.js developer. \nIn-depth knowledge of JavaScript, CSS, HTML, jQuery, and front-end languages. \nKnowledge of REACT tools including React.js, Webpack, Enzyme, Redux, and Flux. \nExperience with user interface design. \nKnowledge of performance testing frameworks, including Mocha and Jest. \nExperience with browser-based debugging and performance testing software. \nTroubleshooting skills. \nProject management skills. \nProblem-solving skills. \nVerbal communication skills. \n\n Why work with Team Remotely?\n\nTeam Remotely Inc. is a staffing platform offering a seamless experience for employers and candidates. Employers can post job openings and specify their requirements, while candidates can create profiles and upload resumes.\n\nThe team of Team Remotely continuously learns and adapts based on previous successful placements, constantly improving its matching capabilities. This ensures that the recommendations provided by Team Remotely are tailored and accurate, increasing the likelihood of a successful match between employers and candidates. By providing intelligent and data-driven solutions, they strive to enhance the efficiency and effectiveness of the hiring process, ultimately helping companies find the best talent and individuals find their dream jobs."}, {"title": "Data Engineer", "company_name": "ZAG Technical Services", "location": "United States (Remote)", "link": "/jobs/view/3771403464/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=Hf0VefrKXBFIL5MtHmY0Pw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3771403464", "description": "About the job\nWe are looking for a tech savvy Data Engineer to contribute to the building, operating, and scaling of next generation data platforms and tools that will power data-driven analytics capabilities throughout the ZAG products and services portfolio, spanning areas such as business intelligence, reporting, and data analytics.\n\nThe ideal team member is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. Must be comfortable supporting the data needs of multiple clients and teams, systems, and products within the modernization of ecosystems. The right-fit person will be excited by the prospect of optimizing or even re-designing data processes to support next generation of advanced analytics capabilities, which in-turn translates to implementing strategic enhancements to maintain competitive advantage in the industry.\n\nThe successful team member will leverage their proficiency to\u2026\n\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\nBuild the analytics processes required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Data Management and cloud based \u2018big data\u2019 technologies.\nBuild analytics tools that utilize technologies to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.\nCollaborate with client teams to identify and solution regional and local requirements within the context of the standard global business model for enterprise analytics.\nParticipates in meetings, discussions, strategy sessions where changes, improvements, and enhancements are proposed, evaluated, and approved. \n\n\nThe exceptional team member will have or demonstrate progressive experiences in\u2026 \n\nBachelor\u2019s degree or higher in Computer Science or related field\nExperience working with BI and data warehousing tools\nExperience working with enterprise data, where security is paramount and data governance is critical\nAbility to work independently and in a team environment, and effectively engage all levels of the organization\nUnderstanding of distributed systems driving large-scale data processing and analytics with a successful history of manipulating, processing, and extracting value from large, disconnected datasets.\nAbility to communicate effectively (listening, presenting, and questioning)\nStrong organizational, written, and communication skills\n\n\nAbout ZAG\n\nZAG Technical Services empowers our clients\u2019 success through intelligent business solutions that increase employee productivity, network security and team collaboration. We aim to ensure that our client\u2019s IT investments drive ROI, provide a competitive advantage and help businesses grow. We do this by living our core set of values: Integrity Foremost, Client-Centered, Accountable Always, Teamwork Throughout, and Exceptional is the Goal.\n\nWhat ZAG Has To Offer\n\nCompetitive compensation package \nBenefits including medical, dental, vision, 401K and life insurance \n\n\nZAG Technical Services is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all of our employees.\n\nZAG Technical Services is an E-Verify participant and will provide the Social Security Administration (SSA) and, if necessary, the Department of Homeland Security (DHS), with information from each new employee\u2019s Form I-9 to confirm work authorization."}, {"title": "Junior Data Engineer", "company_name": "HireMeFast LLC - Career Accelerator - Land A Job", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3786249797/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D&trk=flagship3_search_srp_jobs&lipi=urn%3Ali%3Apage%3Ad_flagship3_search_srp_jobs%3BQExogrHBQoSF9zZm%2B9kufw%3D%3D&lici=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D", "id": "3786249797", "description": "About the job\nThis is a remote position.\n\n Job Title: Junior Data Engineer\n\n Employment Type: Full-Time\n\n Salary: $64,000 - $76,000 per annum\n\n Experience Required:  Minimum 1 year of project experience\n\nAbout us: HireMeFast is a leading staffing and recruitment agency specializing in connecting businesses with top-tier talent across various industries. Our mission is to bridge the gap between exceptional candidates and organizations needing their skills, expertise, and unique qualities. Our team of experienced and dedicated recruitment specialists utilizes innovative sourcing strategies, and a vast network to identify and attract top talent. We conduct comprehensive procedures to ensure that only the most qualified candidates are presented to our clients.\n\nPosition Summary\n\nJoin the fast-paced, innovative, and collaborative environment focused on providing an AIOps platform that enhances the intelligence of the CVS Health infrastructure. Work closely with subject matter experts and colleagues to build and scale out machine learning and AI solutions that will detect, predict, and recommend solutions to correct issues before system impact and enhance efficiency, reliability, and performance of CVS Health\u2019s IT operations.\n\nKey Responsibilities include:\n\nData pipeline development: Designed, implemented, and managed data pipelines for extracting, transforming, and loading data from various sources into data lakes for processing, analytics, and correlation. \nData modeling: Create and maintain data models ensuring data quality, scalability, and efficiency \nDevelop and automate processes to clean, transform, and prepare data for analytics, ensuring data accuracy and consistency \nData Integration: Integrate data from disparate sources, both structured and unstructured to provide a unified view of key infrastructure platform and application data \nUtilize big data technologies such as Kafka to process and analyze large volumes of data efficiently \nImplement data security measures to protect sensitive information and ensure compliance with data and privacy regulation \nCreate/maintain documentation for data processes, data flows, and system configurations \nPerformance Optimization- Monitor and optimize data pipelines and systems for performance, scalability and cost-effectiveness \n\nCharacteristics of this role:\n\nTeam Player: Willing to teach, share knowledge, and work with others to make the team successful. \nCommunication: Exceptional verbal, written, organizational, presentation, and communication skills. \nCreativity: Ability to take written and verbal requirements and come up with other innovative ideas. \nAttention to detail: Systematically and accurately research future solutions and current problems. \nStrong work ethic: The innate drive to do work extremely well. \nPassion: A drive to deliver better products and services than expected to customers. \n\nRequired Qualifications\n\n2+ years of programming experience in languages such as Python, Java, SQL \n2+ years of experience with ETL tools and database management (relational, non-relational) \n2+ years of experience in data modeling techniques and tools to design efficient scalable data structures \nSkills in data quality assessment, data cleansing, and data validation \n\nQualifications\n\nKnowledge of big data technologies and cloud platforms \nExperience with technologies like PySpark, Databricks, and Azure Synapse. \n\nEducation\n\nBachelor\u2019s degree in Computer Science, Information Technology, or related field, or equivalent working experience\n\nWhy HireMeFast LLC?\n\nAt HireMeFast, we understand that finding the right individuals to join your team is crucial for success and growth of your organization. We are committed to streamlining the hiring process for our clients, ensuring they have access to a diverse pool of highly qualified candidates who are a perfect fit for their specific needs."}, {"title": "Data Engineer  - Need candidates from Texas ( REMOTE )", "company_name": "Pulivarthi Group (PG)", "location": "United States (Remote)", "link": "/jobs/view/3770685212/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=9gdMwuif%2BpB%2BCQYvQi3imQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3770685212", "description": "About the job\nFollow us on Linkedin: https://www.linkedin.com/company/pulivarthigroup/\n\nPulivarthi Group LLC is a Global Staffing & IT Technology Solutions company, with our prime focus of providing world class solutions to our customers with the right talent. We combine the expertise of our team and the culture of your company to help you with the solution that is affordable and innovative using high quality standards and technologies.\n\nWe\u2019ve served some of the largest healthcare, financial services, and government entities in the U.S.\n\nData Engineer - FT only - Azure SQL with data lakes experience\n\nData Engineer -\n\nthe resource requirement for this Data Engineer role needed more clarification.\n\nOur customer is requesting a resource who knows Azure CI/CD (Continuous Integration / Continuous Deployment) to build and integrate their DevOps pipeline.\n\nOnce the DevOps pipelines are mapped, they need the Data Engineer to connect to all the Data Sources, primarily their Data Lake.\n\nThe Customer (based in Houston) is interested in a resource in a time zone not too far removed from the Central Time Zone to allow real time collaboration."}, {"title": "Data Engineer", "company_name": "Otter Products", "location": "Fort Collins, CO (Remote)", "link": "/jobs/view/3766728302/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=77U3U7CAi1PxgDmpI3%2BiNQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3766728302", "description": "About the job\nOverview \n\nOtter Products is currently recruiting for a Data Engineer to join our Data team! You can be based in our Fort Collins, CO office with flexibility to work a portion of your time remotely, or 100% Remote in the U.S.\n\nThe Data Engineer applies professional experience, concepts, and company objectives toward building and operationalizing the minimally inclusive data necessary for the enterprise data initiatives following industry standard practices and tools. The bulk of the Data Engineer's work would be in designing, managing and optimizing data pipelines, then moving these data pipelines effectively into production for key data and analytics consumers like business/data analysts, data scientists or other stakeholder that needs curated data for data and analytics use cases across the enterprise. The Data Engineer also ensures compliance with data governance and data security requirements while creating, improving and operationalizing these integrated and reusable data pipelines to enable faster data access, integrated data reuse and vastly improved time-to-solution for data and analytics initiatives. Additionally, the Data Engineer will be expected to collaborate with other data team members and data consumers working on the models and algorithms developed by them toward optimization for data quality, security, and governance to put them into production leading to potentially large productivity gains.\n\n About Otter Products \n\nOtter Products, we grow to give. From our founder's garage in 1998 to the global technology leader we are today, Otter Products continues to drive growth through innovation.\n\nThrough our industry-leading brands - OtterBox, Liviri and OtterCares - we provide our partners the number-one selling and most trusted products in our categories. Our philanthropic spirit is the foundation on which we foster our partner relationships, allowing us to grow and to give - together.\n\nBy way of our charitable arm, the OtterCares Foundation, we support our communities and invest in our future through education that inspires kids to change the world.\n\nAnd even as our global community of Otters continues to grow, our founder's core values are still at the heart of everything we do. We measure our success by our ability to give back to our communities and strengthen opportunities for all.\n\nFor more information visit otterproducts.com\n\n Responsibilities \n\nComplete analysis, design and development of BI solutions using Azure SQL Server\nFamiliarity with Data Warehouse concepts\nExperience coding the extraction, transformation, and loading (ETL) processes\nDatabase development primarily in SSIS, Data Factory and SQL\nPartner with the business to determine end user database/reporting needs and requirements\nGenerate ad hoc reports using Power BI or SSRS\nCollaborate with other developers to create and implement best approach solution(s)\nTroubleshoot Azure tools, systems, and software\nReview queries for performance issues, making changes as needed\nCollaborate with team to performance-tune Azure application as necessary\nAssist with the analysis and extraction of relevant information from large amounts of historical business data to feed data science initiatives\nParticipate and support the design and documentation of processes for large scale data analyses, model development, model validation and model implementation\nSupport and maintain a positive safety culture by following all safety policies and procedures and actively contributing to a safe working environment\nOther duties as required\n\n Qualifications \n\nBachelor's degree required. Degree in Computer Science or Mathematics preferred\nMinimum of four years of experience in an IT or analytical role required. Experience in database development, report writing and/or statistics preferred.\n\n EEO \n\nOtter Products, LLC is an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, age, marital status, pregnancy, sex, sexual orientation, gender, gender identity or expression, national origin, disability, veteran status, or any other characteristic or status protected by law.\n\n For US Based Roles Only - Compensation Range Minimum \n\nUSD $90,000.00/Yr.\n\n For US Based Roles Only - Compensation Range Maximum \n\nUSD $120,000.00/Yr.\n\n Additional Total Rewards \n\nProfit Sharing Program Eligible, Benefits Eligible - Full Time- check out otterproducts.com/careers/why for more info"}, {"title": "Software Engineer, Distributed Systems", "company_name": "Hightouch", "location": "United States (Remote)", "link": "/jobs/view/3628572539/?eBP=CwEAAAGMd4QO8APODyE_-mMOPhYyQGg5ZjVbPwq_3V7DK0B2bLAf10pcedM9S2T71X5i4TiJuEwOzWBbSiTKbLuk-VD-_Y-dN5uyf8YpYkHYqEYFxySMS9zr2hRjNrxX9gXbxssh35TFK9YTNV2WeH2Ncb675JgFbJTgNWSZySFC6rrdJGmdSDTr2q5d3VoxORz3GjsDbzjeyZQCvQ07RqYw7oujWSgslycs2LMCY_-N0NEj6e0PvbaYKiZ_5IW34bGVSYLXpHwqa8oOcNttYZ90UA3E_7sDSFzABdIVlfrp5zzERrIYy5sh2MIInxHdDoKgQw8VmOn5uM4S0tXUl-i7LnN0_W0NfPBmLiVDNuveIOm4MfDNPxfe5QKPk6FUAVpvgbcoJn9yRyE&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=rph%2FT6Q6Vkp6YoggJf9cJQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3628572539", "description": "About the job\nAbout Hightouch\n\nHightouch\u2019s mission is to empower everyone to take action on their data. Through our Reverse ETL platform, business and data users can seamlessly sync data from where it resides, such as warehouses and databases, to where it is needed, including operational systems and SaaS tools. Traditionally, acting on data has required engineering time and bandwidth, and left most business users stuck with charts and reports that are unable to take automated action on their data. With Hightouch, every business user, without writing any code, can activate data to streamline critical processes, improve marketing performance, and scale operations.\n\nOur team operates with a focus on making a meaningful impact for our customers. We believe in approaching challenges with a first principles thinking mindset, moving quickly and embracing our value of efficient execution, and treating each other with compassion and kindness. We look for team members that are strong communicators, have a growth mindset, and are motivated and persistent in achieving our goals.\n\nHundreds of companies use Hightouch, including Spotify, Ramp, Retool, NBA, Plaid, and Betterment. We\u2019re based in San Francisco, are remote-friendly, and backed by leading investors such as Amplify Partners, ICONIQ Growth, Bain Capital Ventures, Y-Combinator, and Afore Capital.\n\nAbout The Role\n\nWe are looking for a distributed systems engineer to work on the systems that power the syncing engine our customers and other engineering teams rely on. This presents an exciting challenge where you can apply your expertise in distributed systems, performance optimization, and troubleshooting to push the boundaries of what is possible and meaningful improve our ability to move massive amounts of customer data. This role also provides a unique opportunity to work on a multi-cloud and multi-region infrastructure that supports a global customer base.\n\nWe believe in enabling our engineers to do their best work for our customers by giving them extremely high levels of ownership and autonomy. This comes in different forms: you will own and deliver projects from start to finish, you will work directly with customers to solve their hardest scaling problems, and you will have a lot of influence over what we work on as a team and company.\n\nSome of the problems we\u2019ll be working on include:\n\nSync Speed: Customers want to sync a lot of data to important destinations like Facebook and Snapchat, which requires us to analyze every part of our syncing process and find where we can optimize to sync data more quickly\nStreaming Syncing: We currently sync data from sources in batch only, but envision a world where we also do syncs in real-time and support streaming sources like webhooks and queues\nScalability and Reliability: As part of our rapid growth, we\u2019re always evaluating current future bottlenecks in scaling and reliability, and architecting for the next order of magnitude of growth\nPersonalization API: Our Personalization API product provides a low latency caching layer on top of data warehouses that enable customers to use their data warehouse for real-time personalization use cases with \nMulti-Region and Multi-Cloud: Supporting our multi-region and multi-cloud backend, including extending it to launch Hightouch on in new regions to support data residency requirements of our global customer base\n\nWe are looking for talented, intellectually curious, and motivated individuals who are interested in tackling the problems above. This is a senior role, but we focus on impact and potential for growth more than years of experience. The salary range for this position is $170,000 - $240,000 USD per year, which is location independent in accordance with our remote-first policy.\n\nAbout You\n\nYou are an engineer with a passion for solving hard technical problems that generate real value for customers. You\u2019re motivated by high ownership and are comfortable in a fast-paced, startup environment.\n\nYou have experience and comfort with distributed systems and high-scale systems or the ability and desire to build this experience quickly. This experience can take different forms:\n\nYou\u2019ve built syncing engines that process and move terabytes of data per day in a scalable way\nYou\u2019ve created event collection or data streaming services that power company critical systems\nYou\u2019ve owned systems that have undergone orders of magnitude of growth and helped them scale accordingly\nYou have relevant skills and are a fast learner who is excited to take on the challenges we face\n\nInterview Process\n\nOur goal with the interview process is to balance speed with giving both parties opportunities to assess whether there is a strong mutual fit. We will ask you questions, but we want you to ask us questions! Our technical interviews focus on how you design systems because we believe this is the best way for us to see how you work and for you to see how we collaborate. We don\u2019t ask you to write code to solve technical brainteasers that don\u2019t appear in your day to day job.\n\nRecruiter screen [30m]: Introductory call with our recruiting team to get to know each other and see if the role could be a good mutual fit.\nSystems design screen [45m]: Designing a data processing feature end-to-end.\nHiring manager interview [30m]: Chat with hiring manager about past experiences and future operating preferences to assess fit on company values and operating principles.\nSystems design interview [90m]: Work with the interviewer to architect a system at a conceptual level. The problem will be at a pretty high level - and have both product and customer requirements as well as technical."}, {"title": "Entry-Level Data Scientist Engineer", "company_name": "Phoenix Recruitment LLC", "location": "Texas City, TX (Remote)", "link": "/jobs/view/3788727625/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=8iwtytHsCu4wMwJ4fj3pxw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3788727625", "description": "About the job\nThis is a remote position.\n\nTitle - Entry-Level Data Scientist Engineer, 1 year of project experience\n\nEmployment Type: Full-time\n\nBase Salary: $60K-$70K\n\nPhoenix Recruitment offers a variety of recruiting services to assist both employers and employees. They specialize in marketing open positions, recruiting, and helping employers to find qualified candidates across various industries. Phoenix Recruitment has expertise in streamlining the hiring process. They can help ensure that the process is efficient, well organized, and compliant with relevant regulations.\n\nSkills and Abilities:\n\nStrong knowledge of R or Python for data analysis and modeling. \nProficiency in statistical programs such as R, SAS, MATLAB, or Python. \nFamiliarity with spreadsheets (VBA) and database applications (Access, Oracle, SQL, or equivalent technology). \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \nAbility to learn new methods quickly and work under deadlines. \nExcellent teamwork and communication skills. \nStrong analytical and problem-solving abilities. \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \n\nPreferred:\n\nKnowledge of actuarial concepts and life, health, and/or annuity products. \nExperience with statistical modeling techniques such as GLM, Decision Trees, Time Series, Regression, etc. \nFamiliarity with Microsoft DeployR. \nExposure to insurance risk analysis. \nBasic experience in computational finance, econometrics, statistics, and math. \nKnowledge of SQL and VBA. \nFamiliarity with R or Python for predictive modeling \n\nWhy Phoenix Recruitment LLC?\n\nPhoenix Recruitment often has an extensive network of employers and candidates. This network allows them to tap into a pool of qualified candidates and connect them with suitable job opportunities. They can also leverage their connections to help employers find the right talent efficiently. Outsourcing the recruitment process to a specialized agency can save your time and resources, avoid delays, reduce administrative burdens, and increase the chances of finding the right fit for your organization."}, {"title": "Data Engineer - Remote Work", "company_name": "BairesDev", "location": "United States (Remote)", "link": "/jobs/view/3783162297/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=OtoCrAAHZFomVup04fy35g%3D%3D&trk=flagship3_search_srp_jobs", "id": "3783162297", "description": "About the job\nWho We are\n\nBairesDev is proud to be the fastest-growing company in America. With people in five continents and world-class clients, we are only as strong as the multicultural teams at the heart of our business. To consistently deliver the highest quality solutions to our clients, we only hire the Top 1% of the best talents and nurture their professional growth on exciting projects.\n\nWe are looking for a Data Engineer to join our Development team and participate in different projects made up of multicultural teams distributed throughout the world.\n\nWhat You Will Do\n\n Interview other Data Scientists and Data engineers to vet their technical abilities.\n Input data from those interviews into the product and help develop a predictive, fair, and enjoyable solution.\n Work together with developers, tech leads, and solution architects to build applications.\n Improve existing structures by adding new functionalities or proposing technological updates so that as a result, the impact of your contribution is significant in the core of each business.\n Improve the user experience on scalable and high availability platforms, contributing to the key differential of each business.\n\nHere\u2019s What We Are Looking For\n\n 5+ years of experience working as a developer.\n 3+ years of experience working as a Data Scientist / Data Engineer.\n Proficient with analysis, troubleshooting, and problem-solving.\n Hands-on experience with managing data loads and data quality.\n Advanced English level.\n\nHow we do make your work (and your life) easier:\n\n 100% remote work.\n Hardware setup for you to work from home.\n Flexible hours - make your schedule.\n Paid parental leave, vacation & holidays.\n Diverse and multicultural work environment.\n An innovative environment with the structure and resources of a leading multinational.\n Excellent compensation \u2014 well above the market average.\n Here you can grow at the speed of your learning curve.\n\nOur people work remotely but with a consistent and robust culture that promotes diversity and teamwork. To continue being the leading software development company in Latin America, we want to ensure that every BairesDev member gets the best growth and professional development opportunities in a diverse, welcoming, and innovative environment.\n\nEvery BairesDev team member brings something unique to our company.\n\nWe want to hear your story. Apply now!"}, {"title": "Data Engineer (Remote)", "company_name": "Cascade Debt", "location": "United States (Remote)", "link": "/jobs/view/3755336571/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=pmUtn1j2zyPr25CNPh2RNw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3755336571", "description": "About the job\nDo you want to work in a small team where you can make a real difference in a company? Would you embrace the challenge of building a high impact platform that is used globally? Then we want to speak with you!\n\nAbout Cascade\n\nCascade is a fintech startup backed by Canadian and US investors that empowers companies to grow by democratizing access to institutional debt. We are building tools that modernize how companies raise and manage debt, lowering the barriers to entry to the $7 trillion specialty finance and alternative credit market for companies around the world. Founded just last year, we are gearing up for our next phase of technical development and are seeking a talented Data Engineer to lead the way.\n\nAbout The Role\n\nAs a Data Engineer you\u2019ll be an early member of a growing team building a pioneering platform in the debt infrastructure space. You will design, build, and launch efficient, scalable, and reliable data pipelines to move and transform data .\n\nWhat You\u2019ll Do\n\n Building databases, data lakes and data ingestion pipelines to integrate customer databases and datasets to our systems \n Ingesting financial datasets from external customers, then u pdating and maintaining accurate and complete data mappings to ensure that our product s are displaying high quality data \n M onitoring and alerting across our data pipelines in order to make sure that our data ingests are reliable and correct \n Translat ing business goals and stakeholder requirements into new data flows and deliver analytical insights ensuring that the data flows create real business value continuously \n\nSkills And Experience\n\n 2 + years experience in a data engineering or data analyst role \n E xperience with relational SQL and NoSQL databases, including PostgreSQL , MySQL, MariaDB, MongoDB, Bigtable, etc. \n E xperience working with ETL technologies , such as Databricks, Fivetran , or dbt \n Proficiency in writing SQL queries and knowledge of analytical data warehouses such as RedShift, BigQuery , and Snowflake \n Some experience with CI/CD automation such as GitHub Actions \n Developing APIs and integrating with 3rd party APIs \n Bonus: experience in fintech / SaaS / credit analysis \n\nPeople Who Thrive At Cascade Are\n\n Self-starters, who take the initiative to tackle challenges in a remote work environment \n Good communicators, who can operate without ego to discuss, learn, grow, and help others do the same \n Passionate about creating great digital experiences for users \n Problem solvers who are not satisfied with the status quo \n\nBenefits\n\nRemote: we are a remote-first company and are very flexible on hours as long as things get done \nHome-office: there\u2019s a $1000 USD home office allowance to set yourself up \nEquity: we expect you to have an owner-mentality, and have the equity plan to match \nBenefits: health, dental, vision, and more \nPerks: we offer free lunches weekly and off-site trips \nJob satisfaction: we offer autonomy, ample opportunities for mastery, and an opportunity to make a difference for companies around the world"}, {"title": "Data Engineer", "company_name": "ISC (Integrated Specialty Coverages, LLC)", "location": "Carlsbad, CA (Remote)", "link": "/jobs/view/3778293528/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ew7%2FfEuXSujs5IGabdqUyw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3778293528", "description": "About the job\nAbout Integrated Specialty Coverages\n\nIntegrated Specialty Coverages LLC (ISC) is a growth stage technology and data-driven insurance company leading innovation in the market.\n\nBacked by one of the leading private equity firms, KKR, and led by a forward-thinking management team, ISC is combining the worlds of insurance and technology to create an Insurtech powerhouse. As a leading online distributor of insurance products for a range of industries and \"Main Street USA\", we are looking for the right people to help us in our mission of achieving exponential growth. We strive to be the number one place to go for brokers and agents to source insurance. To accomplish this, we're building a digitally focused team that deeply understands the intersection between user experience, data, and AI/ML to optimize the way we engage with our customers and partners.\n\nJob Summary\n\nThe Data Engineer role will be the technical liaison between multiple groups including a data science team, the engineering team, product management, and business stakeholders. You do not need any insurance knowledge prior, however, you must quickly dive deep into the insurance world and ask questions to become a subject matter expert. You will be responsible for building a data platform to facilitate the data science team. You must be a self-starter that can build out features such as a data pipeline from scratch. There will be support from both engineering and data science for any buildout.\n\nJob Responsibilities\n\nCreate and maintain optimal data pipeline architecture. \nAssemble large, complex data sets that meet functional / non-functional business requirements. \nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. \nBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using AWS technologies, SQL, and Python. \nWork with stakeholders including the Executive, Product, Data Science, and Engineering teams to assist with data-related technical issues and support their data infrastructure needs. \nWork with data science and analytics teams to strive for greater functionality in our data systems. \n\nMinimum Qualifications:\n\nAdvanced working SQL knowledge and experience working with relational databases, strong query authoring (SQL) as well as working familiarity with a variety of databases (Snowflake, Redshift, MySQL, MSSQL, etc.)\nExperience building and optimizing data pipelines, architecture and data sets. \nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. \nBuild processes supporting data transformation, data structures, metadata, dependency, and workload management. \nA successful history of transforming, processing and extracting value from large disconnected, datasets from a variety of data sources (Flat files, Excel, databases, etc.)\nStrong analytic skills related to working with unstructured datasets. \nStrong project management and organizational skills. \nExperience supporting and working with cross-functional teams in a dynamic environment. \nAbility to mentor/guide/collaborate with other team members. \n\nWe are looking for a candidate with 3-5 years of experience in a Data Engineer role. They should also have experience using the following software/tools:\n\nExperience with relational and MPP databases, including Snowflake and MySQL. \nExperience developing software in an agile environment from the requirements stage to production. \nExperience with version control: git\nExperience with container technologies: Docker\nExperience with data pipeline and workflow management tools: Airflow, Jenkins, AWS Glue, Azkaban, Luigi, etc. \nExperience with AWS cloud services: EC2, ECS, Batch, S3, EMR, RDS, Redshift\nExperience with other cloud services: Snowflake, Airflow\nExperience with object-oriented/object function scripting languages: Python, Java, C++, etc. \nExperience with data modeling and data warehouse design\nExperience with data visualization tools (PowerBI, QuickSight)\n\nThe starting annual pay scale for this position is listed below. Actual starting pay will be based on factors such as skills, qualifications, training, and experience. In addition, the company offers comprehensive benefits including medical, dental and vision insurance, 401(k) plan with match, paid time off, and other benefits.\n\nISC's salary ranges are determined by role and level. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations and could be higher or lower based on a multitude of factors, including job-related skills, experience, and relevant education or training.\n\nNational Pay Range\n\n$100,000\u2014$110,000 USD\n\nBenefits of Working at ISC\n\nCompetitive vacation and flexible working arrangements\nComprehensive and inclusive health benefits\nA variety of professional development and mentorship opportunities\nChoice of technology whether at home or in the office\n\nISC's Ownership Behaviors\n\nDo the Right Thing *Be Relentless. Pursue Excellence. *Personal Responsibility & Own Outcomes\nTry Fast. Learn Fast. Fail Fast. Think Big. *Build Extraordinary Owners & Join Forces *Grit & Determination\n\nApplicants with disabilities may contact ISC HR department via e-mail, and other means to request and arrange accommodation. If you need assistance to accommodate a disability, you may request accommodation at any time. Please contact ISC at HR@ISCMGA.com.\n\nISC believes in creating long-term relationships by being responsive and relevant and by consistently delivering value to our community of customers. Specifically, with our employees, we focus on attracting, developing, and retaining the best talent for our business, challenging our people, demonstrating a \"can-do\" attitude, and fostering a collaborative and mutually supportive environment. ISC is an equal opportunity employer and a participant in the US federal E-Verify program (US).\n\nDiversity creates a healthier atmosphere: All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, gender, gender identity, sexual orientation, marital status, medical condition, genetic information, mental or physical disability, military or veteran status, or any other characteristic protected by local, state, or Federal law."}, {"title": "Data Engineer -Remote", "company_name": "RIT Solutions, Inc.", "location": "Denver, CO (Remote)", "link": "/jobs/view/3768007496/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=1hQH8EDaII1eJ7nQ9BACQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3768007496", "description": "About the job\nDuration: 6+ months CTH\n\nMust have a valid LinkedIn profile\n\nRequired Skills\n\n 3-6 years of experience (no more than 6 years)\n Start up environment experience\n Python, SQL, and DBT"}, {"title": "Big Data Engineer - W2", "company_name": "Megan Soft Inc", "location": "United States (Remote)", "link": "/jobs/view/3674845652/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fCAxh4N%2F4YZJ5aEIASaeeA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3674845652", "description": "About the job\nTitle: Big Data Engineer - W2\n\nLocation: Dearborn, MI (Remote)\n\nDuration: 12+ Months\n\nDirect Client: Ford Motors\n\nMust have skills: Should be able to handle GCP requirements\n\n\"Translates business requirements to build repeatable, sustainable, efficient, coded processes that can be productionized by Software Engineers and readily modified by other Data Engineer Creates POC processes in Dev/QA and works with Software Engineers to productionize and align best coding and process practices that maximize efficiency, speed, stability, system resources and capabilities Awareness of and compliance with data privacy, security, legal and contractual guidelines Incorporates data quality and privacy checks/alerts to minimize bad data being consumed by end users, models and dashboards, and to protect customer data; revises checks for new data issues Validates data products and pipelines are functioning as expected following system or application upgrades, source changes, etc. Works closely with Architects if required to align systems, tools and applications being utilized with business use case and performance requirements Provides data product support and maintenance Establishes, tracks and monitors KPIs related to specific data products and deliverable Participate in iteration cadence activities (Iteration Planning, task estimation, daily scrum, demos, etc.). Will be a part of on-call team to resolve critical production support tickets during business hours and off hours.\n\nSkills Required\n\nExperience with Big Data Technologies (Hadoop , ETL Tool, Spark. Hive etc..) Someone who understands Cloud and has experience working with Cloud Platforms (preferably GCP) Should be able to handle GCP requirements - ingesting into GCS/ Big Query, working with GCP components Hands-on in Data Flow, Cloud Storage (GCS), Big Query and GitHub. Proficient in Python and SQL.\n\nSkills Preferred\n\nManages ETL: uses programming and tools for data ingestion, configures pipelines, applies transformations and decoding, integrates and fuses data, moves and securely deliver Experience NoSQL databases\n\nAdditional Information\n\nIf the candidate is remote only, please indicate \"\"100% Remote\"\" under candidate's name on resume. Other similar schedule notations can be \"\"Local Candidate\"\" or \"\"Hybrid Candidate.\"\" ***HRA Test Required***\""}, {"title": "Remote Opportunity: Quantexa Data Engineer", "company_name": "SPAR Information Systems LLC", "location": "United States (Remote)", "link": "/jobs/view/3680073191/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ioykVCZSOQ7BK5%2FwRQHOWg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3680073191", "description": "About the job\nHello All, Hope you are doing great!! Please go through the job descriptions and let me know your interest. Role: Quantexa Data engineer  Location: Remote Duration: Long Term Contract Job Description: 1. End to End usage of Quantexa including guiding our client team on how to use from both front end and data ingestion Reason they are asking for Certification 2. Resolve Entity issues with data 3. Knowledge of Scala and Hive which requires coding experience using parquet 4. Advanced SQL knowledge requiring writing SQL queries 5. Experience having worked with GCP data lake 6. Quantexa certification required\n\nThanks & Regards,\n\nSatnam Singh\n\nDirect: 201 623 3660\n\nEmail : Satnam.singh@sparinfosys.com"}, {"title": "Remote Work - Need Staff Data Engineer - USC only", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3746283980/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=CvIuLBNO%2BPMGi4IXVRkP0A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3746283980", "description": "About the job\n100% Remote, client is in NY\n\nLook for strong job history, Python, Machine Learning, SQL, 10/10 comms, etc. \n\nMUST be an Engineer \n\nSummary\n\nWe're looking for an experienced Data Engineer to join our client's growing team of top notched IT professionals. You will work closely across multiple groups including Editorial and Audio, Marketing, Advertising, etc. to achieve their goals and objectives. The Data Engineering team builds tools throughout the stack while sharing knowledge across these departments.\n\nAs the company grows, they're looking for Data Engineer who will help solidify and expand our pipelines and maintain their data warehouse. Their business model is based on performing complex and very detailed analyses of how our products perform with their subscribers. You'll be responsible for working alongside analysts, product, marketing, and other teams to define new data collection components and measurement schemes that support each new product and feature.\n\nHow Can Add Value To Their Mission\n\nCreate and maintain data pipelines to provide insights and drive business decisions\nEstablish data warehousing strategy (ex. Kimball, Data Vault, etc.)\nMaintain data infrastructure on our AWS accounts\nCollaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.\nWrite unit/integration tests, contributes to engineering wiki, and documents work\nImplement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it\n\nWhat You'll Need To Succeed\n\n7+ years of industry experience measuring product performance and user behavior\nExperience working with a variety of data management technologies, including RedShift, Kinesis/Kafka, Glue, Spark, Postgres, Airflow, dbt, and others.\nExperience implementing BI reporting tools such as Looker\nExperience interfacing with engineers, product managers and analysts to understand data needs\nKnowledge of a variety of measurement beacons, SDKs, APIs including Google Analytics, Amplitude, Braze, Stripe, email service providers\nA commitment to building secure, resilient, fault-tolerant architectures, with clear documentation and procedures in place for support\nA focus on accuracy and detail as you build, ensuring data pipelines produce clear, predictable results\nUnderstanding of the typical metrics a subscription and advertising-supported business needs to measure success\nExperience with Client techniques as applied to behavioral segmentation or anomaly identification is a definite plus\nFamiliarity using developer tools that increase productivity and facilitate the development of resilient code (eg. Docker, CircleCI, Serverless) is a plus"}, {"title": "ENTRY LEVEL DATA ENGINEER (REMOTE)", "company_name": "SynergisticIT", "location": "Jacksonville, FL (Remote)", "link": "/jobs/view/3767587909/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=om7L3CnowiC4PLIcpZD70A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3767587909", "description": "About the job\nSYNERGISTICIT wants every candidate to know that the Job Market is Challenging and to stand out, you need to have exceptional skills and technologies and that's where we come in to make sure you get the attention which you need\n\nSynergisticIT understands the complex nature of the job market and how difficult it can be to secure a position, especially for fresh graduates. Therefore, we assist and help tech-savvies to convert their passions into professions. We go above and beyond to keep you working in your niche.\n\nAs we focus on long-term success, we provide complete career development solutions. From job search to upskilling portfolio and interview preparation, we can guide you at every step of your career.\n\nSynergisticIT spares no efforts to connect you with a large network of tech giants, including Google, Apple, PayPal, Dell, Cisco, Client, etc. Presently, we are actively looking for ENTRY LEVEL DATA ENGINEER a driven mindset. Get the right opportunity and gain experience in building web-centric solutions on Java.\n\nWho Should Apply : Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or anyone looking to make their career in IT Industry\n\nWe also assist in filing for STEM extension and H1b and Green card filing.\n\nCandidates who are serious about their future in the IT Industry and have set big goals for themselves.\n\nCandidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. We also offer Skill Enhancement Programs if the candidates are missing skills or experience which our clients need with great outcomes\n\nCandidates can benefit from skill enhancement if they fall into the below categories. If they are qualified with enough skills then no need for skill enhancement\n\nCandidates Who Lack Experience\n\nHave had a break in careers\n\nLack Technical Competency\n\ncandidates who want to get employed and make a career in the Tech Industry \n\nPlease Also Check The Below Links\n\nhttps://www.synergisticit.com/candidate-outcomes/\n\nhttps://www.synergisticit.com/java-track/\n\nhttps://www.synergisticit.com/data-science-track/\n\nhttps://www.synergisticit.com/which-is-the-best-option-for-tech-job-seekers-staffing-companies-consulting-companies-bootcamps-or-synergisticit/\n\nhttps://www.synergisticit.com/contact-us/\n\nIf the skills are not a match candidates can opt for Skill enhancement. Or their resume can be sent out to clients to see if responses are achievable\n\nREQUIRED SKILLS For Java/Software Programmers\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Core Java , javascript , C++ or software programming\nSpring boot, Microservices and REST API's experience\nExcellent written and verbal communication skills\n\nFor data Science/Machine learning\n\nRequired Skills\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Statistics, Python, data visualization tools\nExcellent written and verbal communication skills\n\nPreferred skills: NLP, Text mining, Tableau, Time series analysis\n\nTechnical skills are required by clients for selection even if its Junior or entry level position each additional Technical skill helps a candidate's resume to be picked by clients over other job seekers.\n\nClients hire candidates with the right technical skills which they need and reject candidates who lack the required technical skills.\n\nNo third party candidates or c2c candidates\n\nPlease apply to the posting\n\nNo phone calls please. Shortlisted candidates would be reached out"}, {"title": "Data Engineer - Remote | WFH", "company_name": "Get It Recruit - Information Technology", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3782241984/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=BgBf1kSAAX%2BSuw6inJsIVw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3782241984", "description": "About the job\nJoin an innovative team at a leading company in the engineering and manufacturing industry, where we are committed to leveraging data for continuous improvement and operational excellence. This hybrid position allows you to enjoy the flexibility of remote work on Mondays and Fridays.\n\nWhy Join Us\n\nAt our company, we prioritize your well-being and professional growth. We offer a comprehensive benefits package, including excellent health benefits, a 6% 401(k) match, generous paid vacation and sick time, 12 paid holidays, and more.\n\nRole Overview\n\nAs a Data Engineer, you will play a pivotal role in transforming raw data into actionable insights, driving improvements in engineering and manufacturing processes, enhancing product quality, and optimizing operational efficiency. Collaborate with dynamic cross-functional teams to extract, analyze, and visualize data, influencing strategic decisions through your valuable insights.\n\nKey Responsibilities\n\nData Collection and Integration:\n\nGather, clean, and integrate data from diverse sources, including production databases, sensor data, and external datasets.\n\nData Analysis\n\nConduct exploratory data analysis to identify patterns, trends, and anomalies.\n\nPerform statistical analysis and hypothesis testing to derive meaningful insights.\n\nData Visualization\n\nCreate informative and visually appealing dashboards and reports using tools like Tableau, Power BI, or Python libraries.\n\nPredictive Modeling\n\nDevelop predictive models to forecast manufacturing performance, quality, and operational outcomes.\n\nRoot Cause Analysis\n\nInvestigate production issues and quality concerns by analyzing data to identify root causes and recommend corrective actions.\n\nPerformance Monitoring\n\nEstablish and maintain key performance indicators (KPIs) to track manufacturing and engineering metrics.\n\nGenerate regular reports to communicate performance trends to stakeholders.\n\nCross-functional Collaboration\n\nCollaborate with engineering, manufacturing, and quality assurance teams to understand their data needs and provide analytical support.\n\nContinuous Improvement\n\nSuggest process improvements and optimizations based on data analysis.\n\nData Security And Compliance\n\nEnsure data security and compliance with relevant regulations, such as GDPR or industry-specific standards.\n\nInfrastructure And Development\n\nBuild and maintain the infrastructure for collecting, storing, and processing data.\n\nDesign, develop, and manage data pipelines, ETL processes, and data warehouses.\n\nRequired Experience\n\nProven experience as a Sr. Data Analyst and/or Data Engineer in an engineering or manufacturing environment.\n\nProficiency in data analysis tools and programming languages, such as Python, R, SQL, and Excel.\n\nStrong data visualization skills with tools like Tableau or Power BI.\n\nExcellent knowledge of statistical analysis and machine learning techniques.\n\nAbility to communicate complex findings and insights effectively to both technical and non-technical stakeholders.\n\nStrong problem-solving skills and attention to detail.\n\nFamiliarity with manufacturing processes and quality control is a plus.\n\nExperience with enterprise ERPs (SAP S4HANA, Oracle EBS), CRMs (Salesforce, Hubspot), and IoT.\n\nProficient in working with cloud computing platforms including AWS, GCP, and Azure.\n\nExpertise in configuring and optimizing cloud-based data solutions for scalability, reliability, and performance.\n\nBachelor's Degree in Data Science, Statistics, Engineering, or Computer Science Required.\n\nApply Now and be part of a team shaping the future of data-driven innovation in engineering and manufacturing!\n\nEmployment Type: Full-Time"}, {"title": "Entry Level Software Engineer - Data Backend Engineer (Remote - Canada)", "company_name": "Yelp", "location": "United States (Remote)", "link": "/jobs/view/3752881757/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fankeoxifvhpTzOYvowHYQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3752881757", "description": "About the job\nYelp engineering culture is driven by our values: we\u2019re a cooperative team that values individual authenticity and encourages creative solutions to problems. All new engineers deploy working code their first week, and we strive to broaden individual impact with support from managers, mentors, and teams. At the end of the day, we\u2019re all about helping our users, growing as engineers, and having fun in a collaborative environment.\n Interested in exploring data and finding innovative ways to collect it, curate it, and make it easily accessible for others to use? How about doing that at an enormous scale? Yelp\u2019s datasets contain billions of interactions between users and local businesses around the globe. If working with our vast amount of data sounds exciting, come and join us!\n\nThe Marketplace Data Observability team is responsible for managing, governing, and monitoring all of Yelp\u2019s Ads data to protect our revenue stream and also support offline analysis that aids in making long term company decisions. We build tools that help us to better understand our data quality, and we strive to uncover data issues before they impact our consumers.\n\nAs a Data Backend Engineer on this team, you will be responsible for building elegant and scalable data products that serve critical, up-to-date, structured information to support different types of analytics within Yelp. As a core contributor to our growing data modeling and data warehousing engineering efforts, you will help design and own mission-critical data flow pipelines and datastores to enable decisions including effective A/B testing and company investments.\n This opportunity is fully remote and does not require you to be located in any particular area in Canada. We welcome applicants from throughout Canada. We\u2019d love to have you apply, even if you don\u2019t feel you meet every single requirement in this posting. At Yelp, we\u2019re looking for great people, not just those who simply check off all the boxes.\n  Build systems that can effectively store and crunch terabytes of data.\nDesign and develop data models for efficient data storage, retrieval, and reporting.\nCreate and maintain conceptual, logical, and physical data models using industry-standard modeling tools.\nCollaborate with cross-functional teams, including engineers, data analysts, business analysts, and data scientists, to understand data requirements and translate them into effective data models.\nParticipate in data integration efforts, including ETL processes and data migration.\nStay up-to-date with industry best practices, emerging technologies, and trends related to data warehousing.\nSupport on-call rotations as needed to operate the team.\n  Understanding of high performing and scalable data systems.\nExperience in building and orchestrating ETL pipelines.\nExperience with Data Lake or Data Warehouse landscape.\nA hunger for tracking down root causes and fixing them in systematic ways.\nAbility to communicate effectively to technical and non-technical cohorts alike. \nExposure to some of the following technologies: Python, AWS Redshift, AWS Athena / Apache Presto, Big Data technologies (e.g S3, Hadoop, Hive, Spark, Flink, Kafka etc), NoSQL systems like Cassandra, DBT."}, {"title": "Data Engineer (Associate Consultant), June 2024", "company_name": "UDig", "location": "Nashville, TN (Remote)", "link": "/jobs/view/3716039960/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=XUjo40kU55HnDCqJ%2F3FdPg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3716039960", "description": "About the job\nCan UDig It? \n\nUDig designs, builds, and implements technology solutions that deliver on business objectives. People join our team because they are excited to have an impact on the future of our organization, passionate about delivering meaningful results, and energized by solving complex problems. Oh, and they like working with other driven, smart, and authentic people!\n\nUDig is proud to be recognized as a \"Best Places to Work\" in Virginia and Tennessee. We're committed to fostering a transparent environment where leadership is accessible, professional development opportunities are abundant, and you are empowered to achieve your career goals. You'll collaborate, develop deep relationships with teammates, and accelerate your success while having fun!\n\nIf you're graduating in May 2024 and excited by the idea of impacting organizations by building data solutions to achieve business goals, you should consider joining our growing technology team.\n\nHow Things Go Down\n\nAs a new, entry-level teammate, you'll take part in our \"Breaking Ground\" training program designed to help you quickly get up-to-speed on UDig's delivery approach within a project team. The program covers:\n\nUDig delivery process and methodologies such as gathering requirements and delivering projects using agile\nTechnical guidance and instruction on the best practices from Data leaders at UDig\nDelivering executive and technical presentations\nCoaching to prepare you to for success on your first client project at UDig\n\nA typical day might entail:\n\nProject standups and check-ins\nDesign and development of data pipelines, BI reports, and more\nPlanning and participating in project demos\nContributing to internal UDig projects\nCareer and job growth through UDig-provided training\nCreating long-lasting friendships with teammates through fun activities as well as your projects (including social events, winning a prize for guessing the weight of pumpkin, or sharing experience on cool tech side projects)\n\nUDig believes it is important for Associate Consultants going through our Breaking Ground program to reside in one of our major markets (Richmond, VA or Nashville, TN metro area) for at least the first year of employment.\n\nWant to learn more? Hear directly from our teammates by checking out our 'A Day in the Life' blog series: https://www.udig.com/digging-in/breaking-ground-2023/\n\nSound Interesting? Here's the Technical Piece\n\nBelow are a few boxes you should be able to check if\u202fyou're interested: Don't check every box? Go ahead and apply!\n\n1+ year of experience with relational database management systems such as Microsoft SQL Server, MySQL, or Oracle\nUnderstanding of the full lifecycle of design, development, and implementation of data solutions including architecture design, development, testing\nSome experience in ETL development or reporting is a plus\nAbility to present technical ideas and high-level concepts and solutions to internal and external team members with varying degrees of technical knowledge\nBachelor's Degree in in computer science, information technology, or any science or business discipline\nAbility to take your work seriously but also seriously enjoy a good time with your UDig colleagues\n\nWhat's in It For You? \n\nAt UDig, you will build your career alongside talented and experienced developers while gaining experience and having fun! We provide continuous challenges and professional development opportunities. Additionally, we offer: \n\nCompetitive salary, merit reviews, and career advancement paths\nFlexible, hybrid environment\nIndividual $1500 Training Budget\nRegular team building and social activities (virtual and/or in-person)\nTransparent culture with strong communication and access to leadership\nGreat benefits like\nGenerous Paid Time Off, company holidays, and parental leave\nMultiple Single and Family Health Insurance plans to choose from\nDental & Vision coverage\nShort-Term and Long-Term disability\nOptional accident and critical illness coverage\nMatching 401(k)\nand more!"}, {"title": "Expression of Interest: Data Engineer", "company_name": "Fingerprint for Success (F4S)", "location": "New York, NY (Remote)", "link": "/jobs/view/3787769880/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=l%2FEDpQqUNmCttGdIyTk7Cg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787769880", "description": "About the job\nWe are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.\n\nThe F4S Talent Pool is a pilot project designed to:\n\nHelp job seekers get discovered by our partners based on their anticipated hiring needs.\nProvide optional support and resources for job seekers in their career endeavors.\nHelp individuals understand, and bring out the best in themselves and each other.\n\nThe F4S Talent Pool process:\n\nOnce you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.\n\nAbout Fingerprint For Success (F4S)\n\nBacked by 20+ years of research, F4S\u2019s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high performing teams.\n\nKeep in mind, joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your F4S work style assessment to match the hiring needs of our partners.\n\nYour feedback is a gift! Write to us via:\n\nPowered by JazzHR\n\noWuPJ9lwq9"}, {"title": "Data Engineer Seattle, WA (remote)", "company_name": "Conch Technologies, Inc", "location": "United States (Remote)", "link": "/jobs/view/3765052211/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=SBETI78UMutCHuRp5FnP%2FA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3765052211", "description": "About the job\nHi,\n\nGreetings from Conch Technologies Inc\n\nData Engineering Lead \u2013 With Databricks and Unity Catalog\n\nLocation \u2013 Seattle, WA (remote)\n\nDuration \u2013 12 months\n\nNeed: Design and implement data architecture and strategy, leveraging technologies such as Databricks, Unity Catalog, Privacera, and Collibra.\n\nAs a Data Engineering Lead, you will play a pivotal role in defining and executing technical deliverables while assessing complexities and levels of effort. You'll engage with both vendor and internal teams to resolve technical challenges and ensure seamless collaboration. Your leadership will extend to planning and prioritization with the SCRUM master, facilitating effective coordination between onsite and offshore teams, and driving feature prioritization through collaboration with product managers.\n\nKey Responsibilities\n\n Lead the definition and breakdown of technical deliverables, assessing complexities and levels of effort.\n Collaborate with vendor and internal teams to address technical issues.\n Drive planning and prioritization in conjunction with the SCRUM master.\n Coordinate with onsite and offshore teams for timely project delivery.\n Collaborate with product managers to prioritize features, communicate with stakeholders, and clarify requirements.\n Collect and manage user-reported issues, ensuring prompt resolution and communication.\n Define comprehensive test plans, oversee their execution, and document results.\n\n--\n\nWith Regards,\n\nNagesh G\n\nMobile: 408-381-5645\n\nDesk: 901-313-3066\n\nEmail: nagesh@conchtech.com \n\nWeb: www.conchtech.com"}, {"title": "Remote Work - Need Azure Fabric Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3759325599/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=C3iVjmcR9wjUip1nNjUiCg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3759325599", "description": "About the job\nThe client is placing heavy emphasis on Microsoft and Azure Fabric experience\n\nInformation Management Engineer \u2013 Take the identified information management sources and assist migration to the Client Data Lake, Azure Fabric. The engineer will transform information management (de-dup, clean and enrich).\n\nTake the identified data sources and bring them into our Data Lake, Azure Fabric. Once the data is in, the engineer will need to be able to transform data (de-dup, clean and enrich the data). We are focused on using a medallion data model for data architecture. The tools we are working with are: Delta Parquet files, python, spark, notebooks, T-SQL, Fabric lakehouse/warehouse, Dataflows Gen2, Data Pipelines, Direct Lake, shortcuts, ADF."}, {"title": "Data Engineer", "company_name": "Catalist", "location": "Washington, DC (Remote)", "link": "/jobs/view/3744951279/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ui1Fk6ICmnPNqAlZDZAcwA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3744951279", "description": "About the job\nFor over 17 years, Catalist has been a leader in civic data and data science innovation. Our mission is to provide progressive organizations with the data, software, and services needed to better identify, understand, and communicate with the people they need to engage and mobilize. Our clients include the largest, most influential organizations in the U.S. active in civic engagement, advocacy, and political campaigns.\n\nCatalist is home to a dedicated, creative team of technologists, data scientists, and campaign experts committed to using our talents and technology to nurture a vibrant and growing progressive community.\n\nAs a Data Engineer at Catalist, you will work closely with the Analytics and Technology departments to design, build, support, and maintain various data pipelines and processes with an end goal of providing data and intelligence to the progressive community. This role primarily involves translating data architecture designs into functional processes, code, and systems.\n\nThe ideal candidate will be a highly motivated individual with excellent technical skills, a strong desire to learn new skills, and an interest in progressive politics. Catalist values creativity and problem-solving. Our work is on the cutting edge of data-driven politics, and your support will help Democratic candidates and progressive organizations conduct successful advocacy and electoral campaigns.\n\nThis position reports to the Deputy Chief Data Officer. The Data Engineer is a part of a growing Data team that supports all underlying work at Catalist.\n\nThis position is included in our CWA bargaining unit.\n\nPrinciple Duties & Responsibilities\n\nDevelop scalable, production processes, code, and systems for data ingest, transformation, modeling, and reporting across a variety of platforms\nTranslate mock-ups and designs into functional processes, code, and systems\nCreate architecture designs when needed\nProvide quality assurance and testing on processes, code, systems, and products\nBecome an internal subject matter expert on various datasets and support other Catalist departments/teams on usage of those datasets\nExecute ad-hoc data and database maintenance tasks as requiredProject manage cross-departmental efforts, with direct responsibility for stakeholder engagement, management, and execution of technical elements\n\n\nRequirements\n\nBS or BA in a technical field, or relevant experience\n1-2 years of experience working with SQL databases\nExperience with data cleaning or ETL processes\nExperience with distributed computing systems and/or distributed datastores (particularly the Hadoop ecosystem)\nExperience managing projects\nFamiliarity with Catalist data, progressive politics, voter files, and/or commercial data\nBackground check required\n\n\nPreferred Skills & Abilities\n\nWillingness to be a problem solver and produce results in a fast paced environment\nAbility to be creative and personable, and articulate ideas clearly\nExperience working with SQL databases\nProficiency with Python or another object-oriented programming language (R, Java, Scala, etc\u2026)\nExperience working in cloud environments (AWS, GCP, etc.)\nExperience working in command line environments such as Bash\nExperience with a version control tool such as git or github\n\n\nBenefits\n\nMedical, Dental, Vision, Prescription Drug\n\nCatalist offers Medical, Dental, Vision, and Prescription Drug coverage for eligible staff and their eligible dependents. Catalist\u2019s Medical plan is a comprehensive PPO program including Prescription Drug coverage with 85% of the premium paid by Catalist. Dental and Vision coverage is provided at no cost to employees.\n\nGroup Term Life Insurance and Long-Term & Short-Term Disability Coverage\n\nGroup Term Life Insurance and Long-Term and Short-Term Disability coverage is available for eligible staff. These benefits are provided at no cost to Catalist employees.\n\n401(k) Safe Harbor Plan\n\nA 401(k) Safe Harbor Plan is available to eligible staff with a 3% contribution from Catalist from the date of hire. Employees may contribute pre-tax or post-tax from their salary up to the legal limits set forth by the IRS.\n\nMedical and Dependent Care Flexible Spending Accounts (FSAs)\n\nCatalist offers an FSA Program that gives eligible staff the ability to pay out-of-pocket medical/dental/vision/child care expenses from pre-tax earnings.\n\nTransit Benefits\n\nCatalist also makes available a Transit benefit FSA program to eligible employees using pre-tax contributions with a company match.\n\nProfessional Development and Remote Work Expenses\n\nEligible employees may be reimbursed up to $750 each year for professional development / education and remote work expenses.\n\nStudent Loan PayDown or SaveUp\n\nCatalist offers a Student Loan PayDown and College SaveUp benefit for eligible staff.\n\nVacation, Personal Leave, Sick Leave Benefits\n\nCatalist offers generous vacation benefits to all eligible staff. Eligible employees also receive:\n\n 14 Paid Holidays\n Personal Days\n Sick Leave\n Parental Leave\n\n\nHybrid Office/Remote Work\n\nCertain positions at Catalist are eligible for Office/Remote Hybrid or full Remote status."}, {"title": "Remote Entry Level Data Analyst/Engineer", "company_name": "SynergisticIT", "location": "White Plains, NY (Remote)", "link": "/jobs/view/3774171423/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=q54sk1%2FRww0z%2BC%2BE8e0jQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774171423", "description": "About the job\nAt SynergisticIT, we're all about making connections. Whatever IT goals you have, our software programmers can help achieve those. Our Software development teams can take up turnkey projects and execute them in an effective and efficient manner. If you are looking to source talent our recruiters will find the ideal IT talent for your company. What's the secret to our success? Well, it all starts with taking quality time to listen to each client's specific needs. After we have a thorough grasp of your IT goals, we can better customize our Developments as per your specific needs. We can also tailor make recruiting programs to exceed your expectations. Since our founding in 2010, SynergisticIT's strategies have earned the company an enviable position in the software development, IT staffing and IT skill enhancement fields. SynergisticIT continues to work with hundreds of satisfied American clients with our software programmers working on our projects and after gaining hands on experience on cutting edge technologies moving to contribute their skills to great clients like Apple, Google, Client, Ebay, Paypal, Kroger, the Walt Disney Company and hundreds more. If you are tired of working with inefficient programmers who take a lot of time to ramp up we want you to try us. Our software programmers can hit the ground running and get you the maximum return on your investment. You have already tried the rest its time you tried the best. SynergisticIT - Home of the Best Data Scientists and Software Programmers in the Bay Area.\n\n Why Us ? \n\nSynergisticIT has a proven track record of successfully skill enhancement and staffing IT employees for some of the world's most iconic brands. Our team takes the time to fully understand every client's needs so we could best meet your IT staffing requirements. The knowledgeable staff at SynergisticIT is always more than happy to work with clients to ensure they reach their software development goals. Besides staffing, SynergisticIT is also committed to helping young IT professionals advance their career with a robust upskill program . Everyone who goes through SynergisticIT's program learns all the skills necessary to succeed in many IT fields ranging from Java to Machine Learning. Additionally, everyone trained at SynergisticIT has been through extensive mock and technical interview screenings to bolster their career prospects. Last, but certainly not least, SynergisticIT takes great care to respect the privacy considerations for every client. All companies who work with SynergisticIT can rest assured their confidential data is protected using the most up-to-date encryption technologies. SynergisticIT also complies with all the latest NDA agreements.\n\n REQUIRED SKILLS For Java /Software Programmers \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Time series analysis \n\n We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2022 and at Gartner Data Analytics Summit (Florida)-2023 \n\nOracle CloudWorld Event (OCW) Las Vegas 2022 | SynergisticIT - YouTube\n\n https://www.youtube.com/watch?v=OAFOhcGy9Z8 \n\n https://www.youtube.com/watch?v=EmO7NrWHkLM \n\n https://www.youtube.com/watch?v=NVBU9RYZ6UI \n\n https://www.youtube.com/watch?v=Yy74yvjatVg \n\nSynergisticIT at Gartner Data and Analytics Summit 2023 - YouTube\n\nFor preparing for interviews please visit  https://www.synergisticit.com/interview-questions/ \n\n We are looking for the right matching candidates for our clients \n\n Please apply via the job posting \n\n REQUIRED SKILLS For Java /Full Stack/Software Programmer \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning Positions \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Tensorflow \n\n If you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team. \n\n No phone calls please.  Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates"}, {"title": "Data Engineer", "company_name": "Anika Systems", "location": "Leesburg, VA (Remote)", "link": "/jobs/view/3787766893/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=mApL9JnN%2B%2Fr2%2BYUPlxRYOA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787766893", "description": "About the job\nAnika Systems is a fast growing, woman-owned small business that specializes in providing innovative IT solutions for federal government agencies. Our expertise lies in accelerating delivery in Data and Analytics, Intelligent Automation, Application Development, and IT Modernization. We are currently expanding our Federal team and are seeking a passionate and talented Data Engineer. This opportunity is 100% remote.\n\nMust be a U.S. Citizen with the ability to obtain and maintain a government suitability clearance\n\nResponsibilities\n\nDesigns, develops, builds, analyzes, evaluates, and installs database management systems to include database modeling and design, relational database architecture, metadata, and repository creation and configuration management.\nDefines and oversees database organizations, standards, controls, procedures, and documentation. Provides technical consulting in the definition, design, and creation of a database environment.\nAdvises applications development staff and users on data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nDesigns and implements databases with respect to access methods, access time, batch processes, device allocation, validation checks, organization, protection and security, documentation, and statistical methods.\nUses data mapping, data mining, and data transformational analysis tools to design and develop databases.\nDetermines data storage and optimum storage requirements.\nPrepares system requirements, source analysis, and process analyses and designs throughout the database implementation.\n\nRequired Skills And Experience\n\nBA/BS and 1 year of relevant experience\nExperience with DataBricks, SQL, Python\nApplications development experience with data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nExcellent communication skills (both oral and written) with process-oriented organizational skills to ensure project success\n\nDesired Skills And Experience\n\nDemonstrated successful development experience with the full life cycle of government database management systems including database modeling and design, relational database architecture, metadata and repository creation, and configuration management.\n\nPowered by JazzHR\n\nTlKWIorucJ"}, {"title": "Remote Work - Need Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3774712978/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=r3Bp%2F0qcOD2OpC5jpbUOcA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774712978", "description": "About the job\nTitle : Data Engineer\n\nLocation : Remote\n\nDuration : 6+ months to start (long term contract (work order are extended 2x year))\n\nPlease Fill out the skill matrix: Technology Some Awareness (1) Novice (2) Intermediate (3) Advanced (4) Expert (5) SQL Tableau Desktop /Creator Tableau API Integration AWS - S3 Glue AWS - Athena Redshift AWS - Lambda AWS - SNS/SQS msExcel\n\nProfile: Data Analyst will work closely with Architects and leads in the Interflow team, key business stakeholders including program staff to support the organizations effort in the areas of data Analysis and information management. Role should have strong experience in writing SQL queries to extract data for reports. Candidate should have experience working with large set of data. As these reports are very critical for the company and we are looking for an expert who is capable of handling, consuming and extracting large sets of data for analysis and reports.\n\nMust Haves\n\nAWS - Athena\nAWS - S3\nGlue\nhands-on development of reporting applications\nMS Excel\nRedShift\nSQL\nTableau Desktop /Creator\n\nNice\n\nAWS - Lambda\nAWS - SNS/SQS\nTableau API Integration\n\nSkills\n\nBA/BS required (major in an analytical field desired) \nA minimum 10+ years work-related experience. \nExperience in hands-on development of reporting applications for the Web in a professional environment \nThe Ethos of continuous improvement and interest in learning new things.\nStrong analytical thinking and structured problem-solving ability\nAbility to handle multiple projects and assignments simultaneously and effectively in a cross-functional team environment. \nVersed on the agile methodology and best practices. \nExcellent interpersonal and collaboration skill with the ability to work with a diverse set of colleagues, across functions, from different organizations, disciplines, etc. \nSelf-starter, ability to set priorities, work independently and attain goals"}, {"title": "Data Engineer", "company_name": "Team Remotely Inc", "location": "Houston, TX (Remote)", "link": "/jobs/view/3786249829/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=78lY5Jo%2FfyRPC6TQYCdvww%3D%3D&trk=flagship3_search_srp_jobs", "id": "3786249829", "description": "About the job\nThis is a remote position.\n\n Data Engineer (US/Canada Residents Only, 1 year experience, remote)\n\nTeam Remotely Inc. is a staffing and recruitment agency that offers a comprehensive solution for talent acquisition, including sourcing, vetting, pay rolling, and managing talent. Whether you need contract staffing, direct hire, direct sourcing, talent pools, or diversity initiatives, our model can support your hiring strategy.\n\n Hiring Type: Full-Time\n\n Base Salary: $55K-$70K Per Annum.\n\nResponsibilities\n\nDesigning user interface changes for web-based DB applications. \nReviewing application requirements and interface designs. \nDeveloping and implementing highly responsive user interface components using react concepts. \nWriting application interface codes using JavaScript following react.js workflows. \nTroubleshooting interface software and debugging application codes. \nDeveloping and implementing front-end architecture to support user interface concepts. \nMonitoring and improving front-end performance. \n\nQualifications And Experience\n\nBachelor\u2019s degree in computer science, information technology, or a similar field. \nPrevious experience working as a react.js developer. \nIn-depth knowledge of JavaScript, CSS, HTML, jQuery, and front-end languages. \nKnowledge of REACT tools including React.js, Webpack, Enzyme, Redux, and Flux. \nExperience with user interface design. \nKnowledge of performance testing frameworks, including Mocha and Jest. \nExperience with browser-based debugging and performance testing software. \nTroubleshooting skills. \nProject management skills. \nProblem-solving skills. \nVerbal communication skills. \n\n Why work with Team Remotely?\n\nTeam Remotely Inc. is a staffing platform offering a seamless experience for employers and candidates. Employers can post job openings and specify their requirements, while candidates can create profiles and upload resumes.\n\nThe team of Team Remotely continuously learns and adapts based on previous successful placements, constantly improving its matching capabilities. This ensures that the recommendations provided by Team Remotely are tailored and accurate, increasing the likelihood of a successful match between employers and candidates. By providing intelligent and data-driven solutions, they strive to enhance the efficiency and effectiveness of the hiring process, ultimately helping companies find the best talent and individuals find their dream jobs."}, {"title": "Data Engineer", "company_name": "ZAG Technical Services", "location": "United States (Remote)", "link": "/jobs/view/3771403464/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=Hf0VefrKXBFIL5MtHmY0Pw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3771403464", "description": "About the job\nWe are looking for a tech savvy Data Engineer to contribute to the building, operating, and scaling of next generation data platforms and tools that will power data-driven analytics capabilities throughout the ZAG products and services portfolio, spanning areas such as business intelligence, reporting, and data analytics.\n\nThe ideal team member is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. Must be comfortable supporting the data needs of multiple clients and teams, systems, and products within the modernization of ecosystems. The right-fit person will be excited by the prospect of optimizing or even re-designing data processes to support next generation of advanced analytics capabilities, which in-turn translates to implementing strategic enhancements to maintain competitive advantage in the industry.\n\nThe successful team member will leverage their proficiency to\u2026\n\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\nBuild the analytics processes required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Data Management and cloud based \u2018big data\u2019 technologies.\nBuild analytics tools that utilize technologies to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.\nCollaborate with client teams to identify and solution regional and local requirements within the context of the standard global business model for enterprise analytics.\nParticipates in meetings, discussions, strategy sessions where changes, improvements, and enhancements are proposed, evaluated, and approved. \n\n\nThe exceptional team member will have or demonstrate progressive experiences in\u2026 \n\nBachelor\u2019s degree or higher in Computer Science or related field\nExperience working with BI and data warehousing tools\nExperience working with enterprise data, where security is paramount and data governance is critical\nAbility to work independently and in a team environment, and effectively engage all levels of the organization\nUnderstanding of distributed systems driving large-scale data processing and analytics with a successful history of manipulating, processing, and extracting value from large, disconnected datasets.\nAbility to communicate effectively (listening, presenting, and questioning)\nStrong organizational, written, and communication skills\n\n\nAbout ZAG\n\nZAG Technical Services empowers our clients\u2019 success through intelligent business solutions that increase employee productivity, network security and team collaboration. We aim to ensure that our client\u2019s IT investments drive ROI, provide a competitive advantage and help businesses grow. We do this by living our core set of values: Integrity Foremost, Client-Centered, Accountable Always, Teamwork Throughout, and Exceptional is the Goal.\n\nWhat ZAG Has To Offer\n\nCompetitive compensation package \nBenefits including medical, dental, vision, 401K and life insurance \n\n\nZAG Technical Services is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all of our employees.\n\nZAG Technical Services is an E-Verify participant and will provide the Social Security Administration (SSA) and, if necessary, the Department of Homeland Security (DHS), with information from each new employee\u2019s Form I-9 to confirm work authorization."}, {"title": "Junior Data Engineer", "company_name": "HireMeFast LLC - Career Accelerator - Land A Job", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3786249797/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D&trk=flagship3_search_srp_jobs&lipi=urn%3Ali%3Apage%3Ad_flagship3_search_srp_jobs%3BQExogrHBQoSF9zZm%2B9kufw%3D%3D&lici=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D", "id": "3786249797", "description": "About the job\nThis is a remote position.\n\n Job Title: Junior Data Engineer\n\n Employment Type: Full-Time\n\n Salary: $64,000 - $76,000 per annum\n\n Experience Required:  Minimum 1 year of project experience\n\nAbout us: HireMeFast is a leading staffing and recruitment agency specializing in connecting businesses with top-tier talent across various industries. Our mission is to bridge the gap between exceptional candidates and organizations needing their skills, expertise, and unique qualities. Our team of experienced and dedicated recruitment specialists utilizes innovative sourcing strategies, and a vast network to identify and attract top talent. We conduct comprehensive procedures to ensure that only the most qualified candidates are presented to our clients.\n\nPosition Summary\n\nJoin the fast-paced, innovative, and collaborative environment focused on providing an AIOps platform that enhances the intelligence of the CVS Health infrastructure. Work closely with subject matter experts and colleagues to build and scale out machine learning and AI solutions that will detect, predict, and recommend solutions to correct issues before system impact and enhance efficiency, reliability, and performance of CVS Health\u2019s IT operations.\n\nKey Responsibilities include:\n\nData pipeline development: Designed, implemented, and managed data pipelines for extracting, transforming, and loading data from various sources into data lakes for processing, analytics, and correlation. \nData modeling: Create and maintain data models ensuring data quality, scalability, and efficiency \nDevelop and automate processes to clean, transform, and prepare data for analytics, ensuring data accuracy and consistency \nData Integration: Integrate data from disparate sources, both structured and unstructured to provide a unified view of key infrastructure platform and application data \nUtilize big data technologies such as Kafka to process and analyze large volumes of data efficiently \nImplement data security measures to protect sensitive information and ensure compliance with data and privacy regulation \nCreate/maintain documentation for data processes, data flows, and system configurations \nPerformance Optimization- Monitor and optimize data pipelines and systems for performance, scalability and cost-effectiveness \n\nCharacteristics of this role:\n\nTeam Player: Willing to teach, share knowledge, and work with others to make the team successful. \nCommunication: Exceptional verbal, written, organizational, presentation, and communication skills. \nCreativity: Ability to take written and verbal requirements and come up with other innovative ideas. \nAttention to detail: Systematically and accurately research future solutions and current problems. \nStrong work ethic: The innate drive to do work extremely well. \nPassion: A drive to deliver better products and services than expected to customers. \n\nRequired Qualifications\n\n2+ years of programming experience in languages such as Python, Java, SQL \n2+ years of experience with ETL tools and database management (relational, non-relational) \n2+ years of experience in data modeling techniques and tools to design efficient scalable data structures \nSkills in data quality assessment, data cleansing, and data validation \n\nQualifications\n\nKnowledge of big data technologies and cloud platforms \nExperience with technologies like PySpark, Databricks, and Azure Synapse. \n\nEducation\n\nBachelor\u2019s degree in Computer Science, Information Technology, or related field, or equivalent working experience\n\nWhy HireMeFast LLC?\n\nAt HireMeFast, we understand that finding the right individuals to join your team is crucial for success and growth of your organization. We are committed to streamlining the hiring process for our clients, ensuring they have access to a diverse pool of highly qualified candidates who are a perfect fit for their specific needs."}, {"title": "Data Engineer  - Need candidates from Texas ( REMOTE )", "company_name": "Pulivarthi Group (PG)", "location": "United States (Remote)", "link": "/jobs/view/3770685212/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=9gdMwuif%2BpB%2BCQYvQi3imQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3770685212", "description": "About the job\nFollow us on Linkedin: https://www.linkedin.com/company/pulivarthigroup/\n\nPulivarthi Group LLC is a Global Staffing & IT Technology Solutions company, with our prime focus of providing world class solutions to our customers with the right talent. We combine the expertise of our team and the culture of your company to help you with the solution that is affordable and innovative using high quality standards and technologies.\n\nWe\u2019ve served some of the largest healthcare, financial services, and government entities in the U.S.\n\nData Engineer - FT only - Azure SQL with data lakes experience\n\nData Engineer -\n\nthe resource requirement for this Data Engineer role needed more clarification.\n\nOur customer is requesting a resource who knows Azure CI/CD (Continuous Integration / Continuous Deployment) to build and integrate their DevOps pipeline.\n\nOnce the DevOps pipelines are mapped, they need the Data Engineer to connect to all the Data Sources, primarily their Data Lake.\n\nThe Customer (based in Houston) is interested in a resource in a time zone not too far removed from the Central Time Zone to allow real time collaboration."}, {"title": "Data Engineer", "company_name": "Otter Products", "location": "Fort Collins, CO (Remote)", "link": "/jobs/view/3766728302/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=77U3U7CAi1PxgDmpI3%2BiNQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3766728302", "description": "About the job\nOverview \n\nOtter Products is currently recruiting for a Data Engineer to join our Data team! You can be based in our Fort Collins, CO office with flexibility to work a portion of your time remotely, or 100% Remote in the U.S.\n\nThe Data Engineer applies professional experience, concepts, and company objectives toward building and operationalizing the minimally inclusive data necessary for the enterprise data initiatives following industry standard practices and tools. The bulk of the Data Engineer's work would be in designing, managing and optimizing data pipelines, then moving these data pipelines effectively into production for key data and analytics consumers like business/data analysts, data scientists or other stakeholder that needs curated data for data and analytics use cases across the enterprise. The Data Engineer also ensures compliance with data governance and data security requirements while creating, improving and operationalizing these integrated and reusable data pipelines to enable faster data access, integrated data reuse and vastly improved time-to-solution for data and analytics initiatives. Additionally, the Data Engineer will be expected to collaborate with other data team members and data consumers working on the models and algorithms developed by them toward optimization for data quality, security, and governance to put them into production leading to potentially large productivity gains.\n\n About Otter Products \n\nOtter Products, we grow to give. From our founder's garage in 1998 to the global technology leader we are today, Otter Products continues to drive growth through innovation.\n\nThrough our industry-leading brands - OtterBox, Liviri and OtterCares - we provide our partners the number-one selling and most trusted products in our categories. Our philanthropic spirit is the foundation on which we foster our partner relationships, allowing us to grow and to give - together.\n\nBy way of our charitable arm, the OtterCares Foundation, we support our communities and invest in our future through education that inspires kids to change the world.\n\nAnd even as our global community of Otters continues to grow, our founder's core values are still at the heart of everything we do. We measure our success by our ability to give back to our communities and strengthen opportunities for all.\n\nFor more information visit otterproducts.com\n\n Responsibilities \n\nComplete analysis, design and development of BI solutions using Azure SQL Server\nFamiliarity with Data Warehouse concepts\nExperience coding the extraction, transformation, and loading (ETL) processes\nDatabase development primarily in SSIS, Data Factory and SQL\nPartner with the business to determine end user database/reporting needs and requirements\nGenerate ad hoc reports using Power BI or SSRS\nCollaborate with other developers to create and implement best approach solution(s)\nTroubleshoot Azure tools, systems, and software\nReview queries for performance issues, making changes as needed\nCollaborate with team to performance-tune Azure application as necessary\nAssist with the analysis and extraction of relevant information from large amounts of historical business data to feed data science initiatives\nParticipate and support the design and documentation of processes for large scale data analyses, model development, model validation and model implementation\nSupport and maintain a positive safety culture by following all safety policies and procedures and actively contributing to a safe working environment\nOther duties as required\n\n Qualifications \n\nBachelor's degree required. Degree in Computer Science or Mathematics preferred\nMinimum of four years of experience in an IT or analytical role required. Experience in database development, report writing and/or statistics preferred.\n\n EEO \n\nOtter Products, LLC is an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, age, marital status, pregnancy, sex, sexual orientation, gender, gender identity or expression, national origin, disability, veteran status, or any other characteristic or status protected by law.\n\n For US Based Roles Only - Compensation Range Minimum \n\nUSD $90,000.00/Yr.\n\n For US Based Roles Only - Compensation Range Maximum \n\nUSD $120,000.00/Yr.\n\n Additional Total Rewards \n\nProfit Sharing Program Eligible, Benefits Eligible - Full Time- check out otterproducts.com/careers/why for more info"}, {"title": "Software Engineer, Distributed Systems", "company_name": "Hightouch", "location": "United States (Remote)", "link": "/jobs/view/3628572539/?eBP=CwEAAAGMd4QO8APODyE_-mMOPhYyQGg5ZjVbPwq_3V7DK0B2bLAf10pcedM9S2T71X5i4TiJuEwOzWBbSiTKbLuk-VD-_Y-dN5uyf8YpYkHYqEYFxySMS9zr2hRjNrxX9gXbxssh35TFK9YTNV2WeH2Ncb675JgFbJTgNWSZySFC6rrdJGmdSDTr2q5d3VoxORz3GjsDbzjeyZQCvQ07RqYw7oujWSgslycs2LMCY_-N0NEj6e0PvbaYKiZ_5IW34bGVSYLXpHwqa8oOcNttYZ90UA3E_7sDSFzABdIVlfrp5zzERrIYy5sh2MIInxHdDoKgQw8VmOn5uM4S0tXUl-i7LnN0_W0NfPBmLiVDNuveIOm4MfDNPxfe5QKPk6FUAVpvgbcoJn9yRyE&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=rph%2FT6Q6Vkp6YoggJf9cJQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3628572539", "description": "About the job\nAbout Hightouch\n\nHightouch\u2019s mission is to empower everyone to take action on their data. Through our Reverse ETL platform, business and data users can seamlessly sync data from where it resides, such as warehouses and databases, to where it is needed, including operational systems and SaaS tools. Traditionally, acting on data has required engineering time and bandwidth, and left most business users stuck with charts and reports that are unable to take automated action on their data. With Hightouch, every business user, without writing any code, can activate data to streamline critical processes, improve marketing performance, and scale operations.\n\nOur team operates with a focus on making a meaningful impact for our customers. We believe in approaching challenges with a first principles thinking mindset, moving quickly and embracing our value of efficient execution, and treating each other with compassion and kindness. We look for team members that are strong communicators, have a growth mindset, and are motivated and persistent in achieving our goals.\n\nHundreds of companies use Hightouch, including Spotify, Ramp, Retool, NBA, Plaid, and Betterment. We\u2019re based in San Francisco, are remote-friendly, and backed by leading investors such as Amplify Partners, ICONIQ Growth, Bain Capital Ventures, Y-Combinator, and Afore Capital.\n\nAbout The Role\n\nWe are looking for a distributed systems engineer to work on the systems that power the syncing engine our customers and other engineering teams rely on. This presents an exciting challenge where you can apply your expertise in distributed systems, performance optimization, and troubleshooting to push the boundaries of what is possible and meaningful improve our ability to move massive amounts of customer data. This role also provides a unique opportunity to work on a multi-cloud and multi-region infrastructure that supports a global customer base.\n\nWe believe in enabling our engineers to do their best work for our customers by giving them extremely high levels of ownership and autonomy. This comes in different forms: you will own and deliver projects from start to finish, you will work directly with customers to solve their hardest scaling problems, and you will have a lot of influence over what we work on as a team and company.\n\nSome of the problems we\u2019ll be working on include:\n\nSync Speed: Customers want to sync a lot of data to important destinations like Facebook and Snapchat, which requires us to analyze every part of our syncing process and find where we can optimize to sync data more quickly\nStreaming Syncing: We currently sync data from sources in batch only, but envision a world where we also do syncs in real-time and support streaming sources like webhooks and queues\nScalability and Reliability: As part of our rapid growth, we\u2019re always evaluating current future bottlenecks in scaling and reliability, and architecting for the next order of magnitude of growth\nPersonalization API: Our Personalization API product provides a low latency caching layer on top of data warehouses that enable customers to use their data warehouse for real-time personalization use cases with \nMulti-Region and Multi-Cloud: Supporting our multi-region and multi-cloud backend, including extending it to launch Hightouch on in new regions to support data residency requirements of our global customer base\n\nWe are looking for talented, intellectually curious, and motivated individuals who are interested in tackling the problems above. This is a senior role, but we focus on impact and potential for growth more than years of experience. The salary range for this position is $170,000 - $240,000 USD per year, which is location independent in accordance with our remote-first policy.\n\nAbout You\n\nYou are an engineer with a passion for solving hard technical problems that generate real value for customers. You\u2019re motivated by high ownership and are comfortable in a fast-paced, startup environment.\n\nYou have experience and comfort with distributed systems and high-scale systems or the ability and desire to build this experience quickly. This experience can take different forms:\n\nYou\u2019ve built syncing engines that process and move terabytes of data per day in a scalable way\nYou\u2019ve created event collection or data streaming services that power company critical systems\nYou\u2019ve owned systems that have undergone orders of magnitude of growth and helped them scale accordingly\nYou have relevant skills and are a fast learner who is excited to take on the challenges we face\n\nInterview Process\n\nOur goal with the interview process is to balance speed with giving both parties opportunities to assess whether there is a strong mutual fit. We will ask you questions, but we want you to ask us questions! Our technical interviews focus on how you design systems because we believe this is the best way for us to see how you work and for you to see how we collaborate. We don\u2019t ask you to write code to solve technical brainteasers that don\u2019t appear in your day to day job.\n\nRecruiter screen [30m]: Introductory call with our recruiting team to get to know each other and see if the role could be a good mutual fit.\nSystems design screen [45m]: Designing a data processing feature end-to-end.\nHiring manager interview [30m]: Chat with hiring manager about past experiences and future operating preferences to assess fit on company values and operating principles.\nSystems design interview [90m]: Work with the interviewer to architect a system at a conceptual level. The problem will be at a pretty high level - and have both product and customer requirements as well as technical."}, {"title": "Entry-Level Data Scientist Engineer", "company_name": "Phoenix Recruitment LLC", "location": "Texas City, TX (Remote)", "link": "/jobs/view/3788727625/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=8iwtytHsCu4wMwJ4fj3pxw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3788727625", "description": "About the job\nThis is a remote position.\n\nTitle - Entry-Level Data Scientist Engineer, 1 year of project experience\n\nEmployment Type: Full-time\n\nBase Salary: $60K-$70K\n\nPhoenix Recruitment offers a variety of recruiting services to assist both employers and employees. They specialize in marketing open positions, recruiting, and helping employers to find qualified candidates across various industries. Phoenix Recruitment has expertise in streamlining the hiring process. They can help ensure that the process is efficient, well organized, and compliant with relevant regulations.\n\nSkills and Abilities:\n\nStrong knowledge of R or Python for data analysis and modeling. \nProficiency in statistical programs such as R, SAS, MATLAB, or Python. \nFamiliarity with spreadsheets (VBA) and database applications (Access, Oracle, SQL, or equivalent technology). \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \nAbility to learn new methods quickly and work under deadlines. \nExcellent teamwork and communication skills. \nStrong analytical and problem-solving abilities. \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \n\nPreferred:\n\nKnowledge of actuarial concepts and life, health, and/or annuity products. \nExperience with statistical modeling techniques such as GLM, Decision Trees, Time Series, Regression, etc. \nFamiliarity with Microsoft DeployR. \nExposure to insurance risk analysis. \nBasic experience in computational finance, econometrics, statistics, and math. \nKnowledge of SQL and VBA. \nFamiliarity with R or Python for predictive modeling \n\nWhy Phoenix Recruitment LLC?\n\nPhoenix Recruitment often has an extensive network of employers and candidates. This network allows them to tap into a pool of qualified candidates and connect them with suitable job opportunities. They can also leverage their connections to help employers find the right talent efficiently. Outsourcing the recruitment process to a specialized agency can save your time and resources, avoid delays, reduce administrative burdens, and increase the chances of finding the right fit for your organization."}, {"title": "Data Engineer - Remote Work", "company_name": "BairesDev", "location": "United States (Remote)", "link": "/jobs/view/3783162297/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=OtoCrAAHZFomVup04fy35g%3D%3D&trk=flagship3_search_srp_jobs", "id": "3783162297", "description": "About the job\nWho We are\n\nBairesDev is proud to be the fastest-growing company in America. With people in five continents and world-class clients, we are only as strong as the multicultural teams at the heart of our business. To consistently deliver the highest quality solutions to our clients, we only hire the Top 1% of the best talents and nurture their professional growth on exciting projects.\n\nWe are looking for a Data Engineer to join our Development team and participate in different projects made up of multicultural teams distributed throughout the world.\n\nWhat You Will Do\n\n Interview other Data Scientists and Data engineers to vet their technical abilities.\n Input data from those interviews into the product and help develop a predictive, fair, and enjoyable solution.\n Work together with developers, tech leads, and solution architects to build applications.\n Improve existing structures by adding new functionalities or proposing technological updates so that as a result, the impact of your contribution is significant in the core of each business.\n Improve the user experience on scalable and high availability platforms, contributing to the key differential of each business.\n\nHere\u2019s What We Are Looking For\n\n 5+ years of experience working as a developer.\n 3+ years of experience working as a Data Scientist / Data Engineer.\n Proficient with analysis, troubleshooting, and problem-solving.\n Hands-on experience with managing data loads and data quality.\n Advanced English level.\n\nHow we do make your work (and your life) easier:\n\n 100% remote work.\n Hardware setup for you to work from home.\n Flexible hours - make your schedule.\n Paid parental leave, vacation & holidays.\n Diverse and multicultural work environment.\n An innovative environment with the structure and resources of a leading multinational.\n Excellent compensation \u2014 well above the market average.\n Here you can grow at the speed of your learning curve.\n\nOur people work remotely but with a consistent and robust culture that promotes diversity and teamwork. To continue being the leading software development company in Latin America, we want to ensure that every BairesDev member gets the best growth and professional development opportunities in a diverse, welcoming, and innovative environment.\n\nEvery BairesDev team member brings something unique to our company.\n\nWe want to hear your story. Apply now!"}, {"title": "Data Engineer (Remote)", "company_name": "Cascade Debt", "location": "United States (Remote)", "link": "/jobs/view/3755336571/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=pmUtn1j2zyPr25CNPh2RNw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3755336571", "description": "About the job\nDo you want to work in a small team where you can make a real difference in a company? Would you embrace the challenge of building a high impact platform that is used globally? Then we want to speak with you!\n\nAbout Cascade\n\nCascade is a fintech startup backed by Canadian and US investors that empowers companies to grow by democratizing access to institutional debt. We are building tools that modernize how companies raise and manage debt, lowering the barriers to entry to the $7 trillion specialty finance and alternative credit market for companies around the world. Founded just last year, we are gearing up for our next phase of technical development and are seeking a talented Data Engineer to lead the way.\n\nAbout The Role\n\nAs a Data Engineer you\u2019ll be an early member of a growing team building a pioneering platform in the debt infrastructure space. You will design, build, and launch efficient, scalable, and reliable data pipelines to move and transform data .\n\nWhat You\u2019ll Do\n\n Building databases, data lakes and data ingestion pipelines to integrate customer databases and datasets to our systems \n Ingesting financial datasets from external customers, then u pdating and maintaining accurate and complete data mappings to ensure that our product s are displaying high quality data \n M onitoring and alerting across our data pipelines in order to make sure that our data ingests are reliable and correct \n Translat ing business goals and stakeholder requirements into new data flows and deliver analytical insights ensuring that the data flows create real business value continuously \n\nSkills And Experience\n\n 2 + years experience in a data engineering or data analyst role \n E xperience with relational SQL and NoSQL databases, including PostgreSQL , MySQL, MariaDB, MongoDB, Bigtable, etc. \n E xperience working with ETL technologies , such as Databricks, Fivetran , or dbt \n Proficiency in writing SQL queries and knowledge of analytical data warehouses such as RedShift, BigQuery , and Snowflake \n Some experience with CI/CD automation such as GitHub Actions \n Developing APIs and integrating with 3rd party APIs \n Bonus: experience in fintech / SaaS / credit analysis \n\nPeople Who Thrive At Cascade Are\n\n Self-starters, who take the initiative to tackle challenges in a remote work environment \n Good communicators, who can operate without ego to discuss, learn, grow, and help others do the same \n Passionate about creating great digital experiences for users \n Problem solvers who are not satisfied with the status quo \n\nBenefits\n\nRemote: we are a remote-first company and are very flexible on hours as long as things get done \nHome-office: there\u2019s a $1000 USD home office allowance to set yourself up \nEquity: we expect you to have an owner-mentality, and have the equity plan to match \nBenefits: health, dental, vision, and more \nPerks: we offer free lunches weekly and off-site trips \nJob satisfaction: we offer autonomy, ample opportunities for mastery, and an opportunity to make a difference for companies around the world"}, {"title": "Data Engineer", "company_name": "ISC (Integrated Specialty Coverages, LLC)", "location": "Carlsbad, CA (Remote)", "link": "/jobs/view/3778293528/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ew7%2FfEuXSujs5IGabdqUyw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3778293528", "description": "About the job\nAbout Integrated Specialty Coverages\n\nIntegrated Specialty Coverages LLC (ISC) is a growth stage technology and data-driven insurance company leading innovation in the market.\n\nBacked by one of the leading private equity firms, KKR, and led by a forward-thinking management team, ISC is combining the worlds of insurance and technology to create an Insurtech powerhouse. As a leading online distributor of insurance products for a range of industries and \"Main Street USA\", we are looking for the right people to help us in our mission of achieving exponential growth. We strive to be the number one place to go for brokers and agents to source insurance. To accomplish this, we're building a digitally focused team that deeply understands the intersection between user experience, data, and AI/ML to optimize the way we engage with our customers and partners.\n\nJob Summary\n\nThe Data Engineer role will be the technical liaison between multiple groups including a data science team, the engineering team, product management, and business stakeholders. You do not need any insurance knowledge prior, however, you must quickly dive deep into the insurance world and ask questions to become a subject matter expert. You will be responsible for building a data platform to facilitate the data science team. You must be a self-starter that can build out features such as a data pipeline from scratch. There will be support from both engineering and data science for any buildout.\n\nJob Responsibilities\n\nCreate and maintain optimal data pipeline architecture. \nAssemble large, complex data sets that meet functional / non-functional business requirements. \nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. \nBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using AWS technologies, SQL, and Python. \nWork with stakeholders including the Executive, Product, Data Science, and Engineering teams to assist with data-related technical issues and support their data infrastructure needs. \nWork with data science and analytics teams to strive for greater functionality in our data systems. \n\nMinimum Qualifications:\n\nAdvanced working SQL knowledge and experience working with relational databases, strong query authoring (SQL) as well as working familiarity with a variety of databases (Snowflake, Redshift, MySQL, MSSQL, etc.)\nExperience building and optimizing data pipelines, architecture and data sets. \nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. \nBuild processes supporting data transformation, data structures, metadata, dependency, and workload management. \nA successful history of transforming, processing and extracting value from large disconnected, datasets from a variety of data sources (Flat files, Excel, databases, etc.)\nStrong analytic skills related to working with unstructured datasets. \nStrong project management and organizational skills. \nExperience supporting and working with cross-functional teams in a dynamic environment. \nAbility to mentor/guide/collaborate with other team members. \n\nWe are looking for a candidate with 3-5 years of experience in a Data Engineer role. They should also have experience using the following software/tools:\n\nExperience with relational and MPP databases, including Snowflake and MySQL. \nExperience developing software in an agile environment from the requirements stage to production. \nExperience with version control: git\nExperience with container technologies: Docker\nExperience with data pipeline and workflow management tools: Airflow, Jenkins, AWS Glue, Azkaban, Luigi, etc. \nExperience with AWS cloud services: EC2, ECS, Batch, S3, EMR, RDS, Redshift\nExperience with other cloud services: Snowflake, Airflow\nExperience with object-oriented/object function scripting languages: Python, Java, C++, etc. \nExperience with data modeling and data warehouse design\nExperience with data visualization tools (PowerBI, QuickSight)\n\nThe starting annual pay scale for this position is listed below. Actual starting pay will be based on factors such as skills, qualifications, training, and experience. In addition, the company offers comprehensive benefits including medical, dental and vision insurance, 401(k) plan with match, paid time off, and other benefits.\n\nISC's salary ranges are determined by role and level. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations and could be higher or lower based on a multitude of factors, including job-related skills, experience, and relevant education or training.\n\nNational Pay Range\n\n$100,000\u2014$110,000 USD\n\nBenefits of Working at ISC\n\nCompetitive vacation and flexible working arrangements\nComprehensive and inclusive health benefits\nA variety of professional development and mentorship opportunities\nChoice of technology whether at home or in the office\n\nISC's Ownership Behaviors\n\nDo the Right Thing *Be Relentless. Pursue Excellence. *Personal Responsibility & Own Outcomes\nTry Fast. Learn Fast. Fail Fast. Think Big. *Build Extraordinary Owners & Join Forces *Grit & Determination\n\nApplicants with disabilities may contact ISC HR department via e-mail, and other means to request and arrange accommodation. If you need assistance to accommodate a disability, you may request accommodation at any time. Please contact ISC at HR@ISCMGA.com.\n\nISC believes in creating long-term relationships by being responsive and relevant and by consistently delivering value to our community of customers. Specifically, with our employees, we focus on attracting, developing, and retaining the best talent for our business, challenging our people, demonstrating a \"can-do\" attitude, and fostering a collaborative and mutually supportive environment. ISC is an equal opportunity employer and a participant in the US federal E-Verify program (US).\n\nDiversity creates a healthier atmosphere: All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, gender, gender identity, sexual orientation, marital status, medical condition, genetic information, mental or physical disability, military or veteran status, or any other characteristic protected by local, state, or Federal law."}, {"title": "Data Engineer -Remote", "company_name": "RIT Solutions, Inc.", "location": "Denver, CO (Remote)", "link": "/jobs/view/3768007496/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=1hQH8EDaII1eJ7nQ9BACQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3768007496", "description": "About the job\nDuration: 6+ months CTH\n\nMust have a valid LinkedIn profile\n\nRequired Skills\n\n 3-6 years of experience (no more than 6 years)\n Start up environment experience\n Python, SQL, and DBT"}, {"title": "Big Data Engineer - W2", "company_name": "Megan Soft Inc", "location": "United States (Remote)", "link": "/jobs/view/3674845652/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fCAxh4N%2F4YZJ5aEIASaeeA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3674845652", "description": "About the job\nTitle: Big Data Engineer - W2\n\nLocation: Dearborn, MI (Remote)\n\nDuration: 12+ Months\n\nDirect Client: Ford Motors\n\nMust have skills: Should be able to handle GCP requirements\n\n\"Translates business requirements to build repeatable, sustainable, efficient, coded processes that can be productionized by Software Engineers and readily modified by other Data Engineer Creates POC processes in Dev/QA and works with Software Engineers to productionize and align best coding and process practices that maximize efficiency, speed, stability, system resources and capabilities Awareness of and compliance with data privacy, security, legal and contractual guidelines Incorporates data quality and privacy checks/alerts to minimize bad data being consumed by end users, models and dashboards, and to protect customer data; revises checks for new data issues Validates data products and pipelines are functioning as expected following system or application upgrades, source changes, etc. Works closely with Architects if required to align systems, tools and applications being utilized with business use case and performance requirements Provides data product support and maintenance Establishes, tracks and monitors KPIs related to specific data products and deliverable Participate in iteration cadence activities (Iteration Planning, task estimation, daily scrum, demos, etc.). Will be a part of on-call team to resolve critical production support tickets during business hours and off hours.\n\nSkills Required\n\nExperience with Big Data Technologies (Hadoop , ETL Tool, Spark. Hive etc..) Someone who understands Cloud and has experience working with Cloud Platforms (preferably GCP) Should be able to handle GCP requirements - ingesting into GCS/ Big Query, working with GCP components Hands-on in Data Flow, Cloud Storage (GCS), Big Query and GitHub. Proficient in Python and SQL.\n\nSkills Preferred\n\nManages ETL: uses programming and tools for data ingestion, configures pipelines, applies transformations and decoding, integrates and fuses data, moves and securely deliver Experience NoSQL databases\n\nAdditional Information\n\nIf the candidate is remote only, please indicate \"\"100% Remote\"\" under candidate's name on resume. Other similar schedule notations can be \"\"Local Candidate\"\" or \"\"Hybrid Candidate.\"\" ***HRA Test Required***\""}, {"title": "Remote Opportunity: Quantexa Data Engineer", "company_name": "SPAR Information Systems LLC", "location": "United States (Remote)", "link": "/jobs/view/3680073191/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ioykVCZSOQ7BK5%2FwRQHOWg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3680073191", "description": "About the job\nHello All, Hope you are doing great!! Please go through the job descriptions and let me know your interest. Role: Quantexa Data engineer  Location: Remote Duration: Long Term Contract Job Description: 1. End to End usage of Quantexa including guiding our client team on how to use from both front end and data ingestion Reason they are asking for Certification 2. Resolve Entity issues with data 3. Knowledge of Scala and Hive which requires coding experience using parquet 4. Advanced SQL knowledge requiring writing SQL queries 5. Experience having worked with GCP data lake 6. Quantexa certification required\n\nThanks & Regards,\n\nSatnam Singh\n\nDirect: 201 623 3660\n\nEmail : Satnam.singh@sparinfosys.com"}, {"title": "Remote Work - Need Staff Data Engineer - USC only", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3746283980/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=CvIuLBNO%2BPMGi4IXVRkP0A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3746283980", "description": "About the job\n100% Remote, client is in NY\n\nLook for strong job history, Python, Machine Learning, SQL, 10/10 comms, etc. \n\nMUST be an Engineer \n\nSummary\n\nWe're looking for an experienced Data Engineer to join our client's growing team of top notched IT professionals. You will work closely across multiple groups including Editorial and Audio, Marketing, Advertising, etc. to achieve their goals and objectives. The Data Engineering team builds tools throughout the stack while sharing knowledge across these departments.\n\nAs the company grows, they're looking for Data Engineer who will help solidify and expand our pipelines and maintain their data warehouse. Their business model is based on performing complex and very detailed analyses of how our products perform with their subscribers. You'll be responsible for working alongside analysts, product, marketing, and other teams to define new data collection components and measurement schemes that support each new product and feature.\n\nHow Can Add Value To Their Mission\n\nCreate and maintain data pipelines to provide insights and drive business decisions\nEstablish data warehousing strategy (ex. Kimball, Data Vault, etc.)\nMaintain data infrastructure on our AWS accounts\nCollaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.\nWrite unit/integration tests, contributes to engineering wiki, and documents work\nImplement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it\n\nWhat You'll Need To Succeed\n\n7+ years of industry experience measuring product performance and user behavior\nExperience working with a variety of data management technologies, including RedShift, Kinesis/Kafka, Glue, Spark, Postgres, Airflow, dbt, and others.\nExperience implementing BI reporting tools such as Looker\nExperience interfacing with engineers, product managers and analysts to understand data needs\nKnowledge of a variety of measurement beacons, SDKs, APIs including Google Analytics, Amplitude, Braze, Stripe, email service providers\nA commitment to building secure, resilient, fault-tolerant architectures, with clear documentation and procedures in place for support\nA focus on accuracy and detail as you build, ensuring data pipelines produce clear, predictable results\nUnderstanding of the typical metrics a subscription and advertising-supported business needs to measure success\nExperience with Client techniques as applied to behavioral segmentation or anomaly identification is a definite plus\nFamiliarity using developer tools that increase productivity and facilitate the development of resilient code (eg. Docker, CircleCI, Serverless) is a plus"}, {"title": "ENTRY LEVEL DATA ENGINEER (REMOTE)", "company_name": "SynergisticIT", "location": "Jacksonville, FL (Remote)", "link": "/jobs/view/3767587909/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=om7L3CnowiC4PLIcpZD70A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3767587909", "description": "About the job\nSYNERGISTICIT wants every candidate to know that the Job Market is Challenging and to stand out, you need to have exceptional skills and technologies and that's where we come in to make sure you get the attention which you need\n\nSynergisticIT understands the complex nature of the job market and how difficult it can be to secure a position, especially for fresh graduates. Therefore, we assist and help tech-savvies to convert their passions into professions. We go above and beyond to keep you working in your niche.\n\nAs we focus on long-term success, we provide complete career development solutions. From job search to upskilling portfolio and interview preparation, we can guide you at every step of your career.\n\nSynergisticIT spares no efforts to connect you with a large network of tech giants, including Google, Apple, PayPal, Dell, Cisco, Client, etc. Presently, we are actively looking for ENTRY LEVEL DATA ENGINEER a driven mindset. Get the right opportunity and gain experience in building web-centric solutions on Java.\n\nWho Should Apply : Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or anyone looking to make their career in IT Industry\n\nWe also assist in filing for STEM extension and H1b and Green card filing.\n\nCandidates who are serious about their future in the IT Industry and have set big goals for themselves.\n\nCandidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. We also offer Skill Enhancement Programs if the candidates are missing skills or experience which our clients need with great outcomes\n\nCandidates can benefit from skill enhancement if they fall into the below categories. If they are qualified with enough skills then no need for skill enhancement\n\nCandidates Who Lack Experience\n\nHave had a break in careers\n\nLack Technical Competency\n\ncandidates who want to get employed and make a career in the Tech Industry \n\nPlease Also Check The Below Links\n\nhttps://www.synergisticit.com/candidate-outcomes/\n\nhttps://www.synergisticit.com/java-track/\n\nhttps://www.synergisticit.com/data-science-track/\n\nhttps://www.synergisticit.com/which-is-the-best-option-for-tech-job-seekers-staffing-companies-consulting-companies-bootcamps-or-synergisticit/\n\nhttps://www.synergisticit.com/contact-us/\n\nIf the skills are not a match candidates can opt for Skill enhancement. Or their resume can be sent out to clients to see if responses are achievable\n\nREQUIRED SKILLS For Java/Software Programmers\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Core Java , javascript , C++ or software programming\nSpring boot, Microservices and REST API's experience\nExcellent written and verbal communication skills\n\nFor data Science/Machine learning\n\nRequired Skills\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Statistics, Python, data visualization tools\nExcellent written and verbal communication skills\n\nPreferred skills: NLP, Text mining, Tableau, Time series analysis\n\nTechnical skills are required by clients for selection even if its Junior or entry level position each additional Technical skill helps a candidate's resume to be picked by clients over other job seekers.\n\nClients hire candidates with the right technical skills which they need and reject candidates who lack the required technical skills.\n\nNo third party candidates or c2c candidates\n\nPlease apply to the posting\n\nNo phone calls please. Shortlisted candidates would be reached out"}, {"title": "Data Engineer - Remote | WFH", "company_name": "Get It Recruit - Information Technology", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3782241984/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=BgBf1kSAAX%2BSuw6inJsIVw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3782241984", "description": "About the job\nJoin an innovative team at a leading company in the engineering and manufacturing industry, where we are committed to leveraging data for continuous improvement and operational excellence. This hybrid position allows you to enjoy the flexibility of remote work on Mondays and Fridays.\n\nWhy Join Us\n\nAt our company, we prioritize your well-being and professional growth. We offer a comprehensive benefits package, including excellent health benefits, a 6% 401(k) match, generous paid vacation and sick time, 12 paid holidays, and more.\n\nRole Overview\n\nAs a Data Engineer, you will play a pivotal role in transforming raw data into actionable insights, driving improvements in engineering and manufacturing processes, enhancing product quality, and optimizing operational efficiency. Collaborate with dynamic cross-functional teams to extract, analyze, and visualize data, influencing strategic decisions through your valuable insights.\n\nKey Responsibilities\n\nData Collection and Integration:\n\nGather, clean, and integrate data from diverse sources, including production databases, sensor data, and external datasets.\n\nData Analysis\n\nConduct exploratory data analysis to identify patterns, trends, and anomalies.\n\nPerform statistical analysis and hypothesis testing to derive meaningful insights.\n\nData Visualization\n\nCreate informative and visually appealing dashboards and reports using tools like Tableau, Power BI, or Python libraries.\n\nPredictive Modeling\n\nDevelop predictive models to forecast manufacturing performance, quality, and operational outcomes.\n\nRoot Cause Analysis\n\nInvestigate production issues and quality concerns by analyzing data to identify root causes and recommend corrective actions.\n\nPerformance Monitoring\n\nEstablish and maintain key performance indicators (KPIs) to track manufacturing and engineering metrics.\n\nGenerate regular reports to communicate performance trends to stakeholders.\n\nCross-functional Collaboration\n\nCollaborate with engineering, manufacturing, and quality assurance teams to understand their data needs and provide analytical support.\n\nContinuous Improvement\n\nSuggest process improvements and optimizations based on data analysis.\n\nData Security And Compliance\n\nEnsure data security and compliance with relevant regulations, such as GDPR or industry-specific standards.\n\nInfrastructure And Development\n\nBuild and maintain the infrastructure for collecting, storing, and processing data.\n\nDesign, develop, and manage data pipelines, ETL processes, and data warehouses.\n\nRequired Experience\n\nProven experience as a Sr. Data Analyst and/or Data Engineer in an engineering or manufacturing environment.\n\nProficiency in data analysis tools and programming languages, such as Python, R, SQL, and Excel.\n\nStrong data visualization skills with tools like Tableau or Power BI.\n\nExcellent knowledge of statistical analysis and machine learning techniques.\n\nAbility to communicate complex findings and insights effectively to both technical and non-technical stakeholders.\n\nStrong problem-solving skills and attention to detail.\n\nFamiliarity with manufacturing processes and quality control is a plus.\n\nExperience with enterprise ERPs (SAP S4HANA, Oracle EBS), CRMs (Salesforce, Hubspot), and IoT.\n\nProficient in working with cloud computing platforms including AWS, GCP, and Azure.\n\nExpertise in configuring and optimizing cloud-based data solutions for scalability, reliability, and performance.\n\nBachelor's Degree in Data Science, Statistics, Engineering, or Computer Science Required.\n\nApply Now and be part of a team shaping the future of data-driven innovation in engineering and manufacturing!\n\nEmployment Type: Full-Time"}, {"title": "Entry Level Software Engineer - Data Backend Engineer (Remote - Canada)", "company_name": "Yelp", "location": "United States (Remote)", "link": "/jobs/view/3752881757/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fankeoxifvhpTzOYvowHYQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3752881757", "description": "About the job\nYelp engineering culture is driven by our values: we\u2019re a cooperative team that values individual authenticity and encourages creative solutions to problems. All new engineers deploy working code their first week, and we strive to broaden individual impact with support from managers, mentors, and teams. At the end of the day, we\u2019re all about helping our users, growing as engineers, and having fun in a collaborative environment.\n Interested in exploring data and finding innovative ways to collect it, curate it, and make it easily accessible for others to use? How about doing that at an enormous scale? Yelp\u2019s datasets contain billions of interactions between users and local businesses around the globe. If working with our vast amount of data sounds exciting, come and join us!\n\nThe Marketplace Data Observability team is responsible for managing, governing, and monitoring all of Yelp\u2019s Ads data to protect our revenue stream and also support offline analysis that aids in making long term company decisions. We build tools that help us to better understand our data quality, and we strive to uncover data issues before they impact our consumers.\n\nAs a Data Backend Engineer on this team, you will be responsible for building elegant and scalable data products that serve critical, up-to-date, structured information to support different types of analytics within Yelp. As a core contributor to our growing data modeling and data warehousing engineering efforts, you will help design and own mission-critical data flow pipelines and datastores to enable decisions including effective A/B testing and company investments.\n This opportunity is fully remote and does not require you to be located in any particular area in Canada. We welcome applicants from throughout Canada. We\u2019d love to have you apply, even if you don\u2019t feel you meet every single requirement in this posting. At Yelp, we\u2019re looking for great people, not just those who simply check off all the boxes.\n  Build systems that can effectively store and crunch terabytes of data.\nDesign and develop data models for efficient data storage, retrieval, and reporting.\nCreate and maintain conceptual, logical, and physical data models using industry-standard modeling tools.\nCollaborate with cross-functional teams, including engineers, data analysts, business analysts, and data scientists, to understand data requirements and translate them into effective data models.\nParticipate in data integration efforts, including ETL processes and data migration.\nStay up-to-date with industry best practices, emerging technologies, and trends related to data warehousing.\nSupport on-call rotations as needed to operate the team.\n  Understanding of high performing and scalable data systems.\nExperience in building and orchestrating ETL pipelines.\nExperience with Data Lake or Data Warehouse landscape.\nA hunger for tracking down root causes and fixing them in systematic ways.\nAbility to communicate effectively to technical and non-technical cohorts alike. \nExposure to some of the following technologies: Python, AWS Redshift, AWS Athena / Apache Presto, Big Data technologies (e.g S3, Hadoop, Hive, Spark, Flink, Kafka etc), NoSQL systems like Cassandra, DBT."}, {"title": "Data Engineer (Associate Consultant), June 2024", "company_name": "UDig", "location": "Nashville, TN (Remote)", "link": "/jobs/view/3716039960/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=XUjo40kU55HnDCqJ%2F3FdPg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3716039960", "description": "About the job\nCan UDig It? \n\nUDig designs, builds, and implements technology solutions that deliver on business objectives. People join our team because they are excited to have an impact on the future of our organization, passionate about delivering meaningful results, and energized by solving complex problems. Oh, and they like working with other driven, smart, and authentic people!\n\nUDig is proud to be recognized as a \"Best Places to Work\" in Virginia and Tennessee. We're committed to fostering a transparent environment where leadership is accessible, professional development opportunities are abundant, and you are empowered to achieve your career goals. You'll collaborate, develop deep relationships with teammates, and accelerate your success while having fun!\n\nIf you're graduating in May 2024 and excited by the idea of impacting organizations by building data solutions to achieve business goals, you should consider joining our growing technology team.\n\nHow Things Go Down\n\nAs a new, entry-level teammate, you'll take part in our \"Breaking Ground\" training program designed to help you quickly get up-to-speed on UDig's delivery approach within a project team. The program covers:\n\nUDig delivery process and methodologies such as gathering requirements and delivering projects using agile\nTechnical guidance and instruction on the best practices from Data leaders at UDig\nDelivering executive and technical presentations\nCoaching to prepare you to for success on your first client project at UDig\n\nA typical day might entail:\n\nProject standups and check-ins\nDesign and development of data pipelines, BI reports, and more\nPlanning and participating in project demos\nContributing to internal UDig projects\nCareer and job growth through UDig-provided training\nCreating long-lasting friendships with teammates through fun activities as well as your projects (including social events, winning a prize for guessing the weight of pumpkin, or sharing experience on cool tech side projects)\n\nUDig believes it is important for Associate Consultants going through our Breaking Ground program to reside in one of our major markets (Richmond, VA or Nashville, TN metro area) for at least the first year of employment.\n\nWant to learn more? Hear directly from our teammates by checking out our 'A Day in the Life' blog series: https://www.udig.com/digging-in/breaking-ground-2023/\n\nSound Interesting? Here's the Technical Piece\n\nBelow are a few boxes you should be able to check if\u202fyou're interested: Don't check every box? Go ahead and apply!\n\n1+ year of experience with relational database management systems such as Microsoft SQL Server, MySQL, or Oracle\nUnderstanding of the full lifecycle of design, development, and implementation of data solutions including architecture design, development, testing\nSome experience in ETL development or reporting is a plus\nAbility to present technical ideas and high-level concepts and solutions to internal and external team members with varying degrees of technical knowledge\nBachelor's Degree in in computer science, information technology, or any science or business discipline\nAbility to take your work seriously but also seriously enjoy a good time with your UDig colleagues\n\nWhat's in It For You? \n\nAt UDig, you will build your career alongside talented and experienced developers while gaining experience and having fun! We provide continuous challenges and professional development opportunities. Additionally, we offer: \n\nCompetitive salary, merit reviews, and career advancement paths\nFlexible, hybrid environment\nIndividual $1500 Training Budget\nRegular team building and social activities (virtual and/or in-person)\nTransparent culture with strong communication and access to leadership\nGreat benefits like\nGenerous Paid Time Off, company holidays, and parental leave\nMultiple Single and Family Health Insurance plans to choose from\nDental & Vision coverage\nShort-Term and Long-Term disability\nOptional accident and critical illness coverage\nMatching 401(k)\nand more!"}, {"title": "Expression of Interest: Data Engineer", "company_name": "Fingerprint for Success (F4S)", "location": "New York, NY (Remote)", "link": "/jobs/view/3787769880/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=l%2FEDpQqUNmCttGdIyTk7Cg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787769880", "description": "About the job\nWe are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.\n\nThe F4S Talent Pool is a pilot project designed to:\n\nHelp job seekers get discovered by our partners based on their anticipated hiring needs.\nProvide optional support and resources for job seekers in their career endeavors.\nHelp individuals understand, and bring out the best in themselves and each other.\n\nThe F4S Talent Pool process:\n\nOnce you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.\n\nAbout Fingerprint For Success (F4S)\n\nBacked by 20+ years of research, F4S\u2019s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high performing teams.\n\nKeep in mind, joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your F4S work style assessment to match the hiring needs of our partners.\n\nYour feedback is a gift! Write to us via:\n\nPowered by JazzHR\n\noWuPJ9lwq9"}, {"title": "Data Engineer Seattle, WA (remote)", "company_name": "Conch Technologies, Inc", "location": "United States (Remote)", "link": "/jobs/view/3765052211/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=SBETI78UMutCHuRp5FnP%2FA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3765052211", "description": "About the job\nHi,\n\nGreetings from Conch Technologies Inc\n\nData Engineering Lead \u2013 With Databricks and Unity Catalog\n\nLocation \u2013 Seattle, WA (remote)\n\nDuration \u2013 12 months\n\nNeed: Design and implement data architecture and strategy, leveraging technologies such as Databricks, Unity Catalog, Privacera, and Collibra.\n\nAs a Data Engineering Lead, you will play a pivotal role in defining and executing technical deliverables while assessing complexities and levels of effort. You'll engage with both vendor and internal teams to resolve technical challenges and ensure seamless collaboration. Your leadership will extend to planning and prioritization with the SCRUM master, facilitating effective coordination between onsite and offshore teams, and driving feature prioritization through collaboration with product managers.\n\nKey Responsibilities\n\n Lead the definition and breakdown of technical deliverables, assessing complexities and levels of effort.\n Collaborate with vendor and internal teams to address technical issues.\n Drive planning and prioritization in conjunction with the SCRUM master.\n Coordinate with onsite and offshore teams for timely project delivery.\n Collaborate with product managers to prioritize features, communicate with stakeholders, and clarify requirements.\n Collect and manage user-reported issues, ensuring prompt resolution and communication.\n Define comprehensive test plans, oversee their execution, and document results.\n\n--\n\nWith Regards,\n\nNagesh G\n\nMobile: 408-381-5645\n\nDesk: 901-313-3066\n\nEmail: nagesh@conchtech.com \n\nWeb: www.conchtech.com"}, {"title": "Remote Work - Need Azure Fabric Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3759325599/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=C3iVjmcR9wjUip1nNjUiCg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3759325599", "description": "About the job\nThe client is placing heavy emphasis on Microsoft and Azure Fabric experience\n\nInformation Management Engineer \u2013 Take the identified information management sources and assist migration to the Client Data Lake, Azure Fabric. The engineer will transform information management (de-dup, clean and enrich).\n\nTake the identified data sources and bring them into our Data Lake, Azure Fabric. Once the data is in, the engineer will need to be able to transform data (de-dup, clean and enrich the data). We are focused on using a medallion data model for data architecture. The tools we are working with are: Delta Parquet files, python, spark, notebooks, T-SQL, Fabric lakehouse/warehouse, Dataflows Gen2, Data Pipelines, Direct Lake, shortcuts, ADF."}, {"title": "Data Engineer", "company_name": "Catalist", "location": "Washington, DC (Remote)", "link": "/jobs/view/3744951279/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ui1Fk6ICmnPNqAlZDZAcwA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3744951279", "description": "About the job\nFor over 17 years, Catalist has been a leader in civic data and data science innovation. Our mission is to provide progressive organizations with the data, software, and services needed to better identify, understand, and communicate with the people they need to engage and mobilize. Our clients include the largest, most influential organizations in the U.S. active in civic engagement, advocacy, and political campaigns.\n\nCatalist is home to a dedicated, creative team of technologists, data scientists, and campaign experts committed to using our talents and technology to nurture a vibrant and growing progressive community.\n\nAs a Data Engineer at Catalist, you will work closely with the Analytics and Technology departments to design, build, support, and maintain various data pipelines and processes with an end goal of providing data and intelligence to the progressive community. This role primarily involves translating data architecture designs into functional processes, code, and systems.\n\nThe ideal candidate will be a highly motivated individual with excellent technical skills, a strong desire to learn new skills, and an interest in progressive politics. Catalist values creativity and problem-solving. Our work is on the cutting edge of data-driven politics, and your support will help Democratic candidates and progressive organizations conduct successful advocacy and electoral campaigns.\n\nThis position reports to the Deputy Chief Data Officer. The Data Engineer is a part of a growing Data team that supports all underlying work at Catalist.\n\nThis position is included in our CWA bargaining unit.\n\nPrinciple Duties & Responsibilities\n\nDevelop scalable, production processes, code, and systems for data ingest, transformation, modeling, and reporting across a variety of platforms\nTranslate mock-ups and designs into functional processes, code, and systems\nCreate architecture designs when needed\nProvide quality assurance and testing on processes, code, systems, and products\nBecome an internal subject matter expert on various datasets and support other Catalist departments/teams on usage of those datasets\nExecute ad-hoc data and database maintenance tasks as requiredProject manage cross-departmental efforts, with direct responsibility for stakeholder engagement, management, and execution of technical elements\n\n\nRequirements\n\nBS or BA in a technical field, or relevant experience\n1-2 years of experience working with SQL databases\nExperience with data cleaning or ETL processes\nExperience with distributed computing systems and/or distributed datastores (particularly the Hadoop ecosystem)\nExperience managing projects\nFamiliarity with Catalist data, progressive politics, voter files, and/or commercial data\nBackground check required\n\n\nPreferred Skills & Abilities\n\nWillingness to be a problem solver and produce results in a fast paced environment\nAbility to be creative and personable, and articulate ideas clearly\nExperience working with SQL databases\nProficiency with Python or another object-oriented programming language (R, Java, Scala, etc\u2026)\nExperience working in cloud environments (AWS, GCP, etc.)\nExperience working in command line environments such as Bash\nExperience with a version control tool such as git or github\n\n\nBenefits\n\nMedical, Dental, Vision, Prescription Drug\n\nCatalist offers Medical, Dental, Vision, and Prescription Drug coverage for eligible staff and their eligible dependents. Catalist\u2019s Medical plan is a comprehensive PPO program including Prescription Drug coverage with 85% of the premium paid by Catalist. Dental and Vision coverage is provided at no cost to employees.\n\nGroup Term Life Insurance and Long-Term & Short-Term Disability Coverage\n\nGroup Term Life Insurance and Long-Term and Short-Term Disability coverage is available for eligible staff. These benefits are provided at no cost to Catalist employees.\n\n401(k) Safe Harbor Plan\n\nA 401(k) Safe Harbor Plan is available to eligible staff with a 3% contribution from Catalist from the date of hire. Employees may contribute pre-tax or post-tax from their salary up to the legal limits set forth by the IRS.\n\nMedical and Dependent Care Flexible Spending Accounts (FSAs)\n\nCatalist offers an FSA Program that gives eligible staff the ability to pay out-of-pocket medical/dental/vision/child care expenses from pre-tax earnings.\n\nTransit Benefits\n\nCatalist also makes available a Transit benefit FSA program to eligible employees using pre-tax contributions with a company match.\n\nProfessional Development and Remote Work Expenses\n\nEligible employees may be reimbursed up to $750 each year for professional development / education and remote work expenses.\n\nStudent Loan PayDown or SaveUp\n\nCatalist offers a Student Loan PayDown and College SaveUp benefit for eligible staff.\n\nVacation, Personal Leave, Sick Leave Benefits\n\nCatalist offers generous vacation benefits to all eligible staff. Eligible employees also receive:\n\n 14 Paid Holidays\n Personal Days\n Sick Leave\n Parental Leave\n\n\nHybrid Office/Remote Work\n\nCertain positions at Catalist are eligible for Office/Remote Hybrid or full Remote status."}, {"title": "Remote Entry Level Data Analyst/Engineer", "company_name": "SynergisticIT", "location": "White Plains, NY (Remote)", "link": "/jobs/view/3774171423/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=q54sk1%2FRww0z%2BC%2BE8e0jQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774171423", "description": "About the job\nAt SynergisticIT, we're all about making connections. Whatever IT goals you have, our software programmers can help achieve those. Our Software development teams can take up turnkey projects and execute them in an effective and efficient manner. If you are looking to source talent our recruiters will find the ideal IT talent for your company. What's the secret to our success? Well, it all starts with taking quality time to listen to each client's specific needs. After we have a thorough grasp of your IT goals, we can better customize our Developments as per your specific needs. We can also tailor make recruiting programs to exceed your expectations. Since our founding in 2010, SynergisticIT's strategies have earned the company an enviable position in the software development, IT staffing and IT skill enhancement fields. SynergisticIT continues to work with hundreds of satisfied American clients with our software programmers working on our projects and after gaining hands on experience on cutting edge technologies moving to contribute their skills to great clients like Apple, Google, Client, Ebay, Paypal, Kroger, the Walt Disney Company and hundreds more. If you are tired of working with inefficient programmers who take a lot of time to ramp up we want you to try us. Our software programmers can hit the ground running and get you the maximum return on your investment. You have already tried the rest its time you tried the best. SynergisticIT - Home of the Best Data Scientists and Software Programmers in the Bay Area.\n\n Why Us ? \n\nSynergisticIT has a proven track record of successfully skill enhancement and staffing IT employees for some of the world's most iconic brands. Our team takes the time to fully understand every client's needs so we could best meet your IT staffing requirements. The knowledgeable staff at SynergisticIT is always more than happy to work with clients to ensure they reach their software development goals. Besides staffing, SynergisticIT is also committed to helping young IT professionals advance their career with a robust upskill program . Everyone who goes through SynergisticIT's program learns all the skills necessary to succeed in many IT fields ranging from Java to Machine Learning. Additionally, everyone trained at SynergisticIT has been through extensive mock and technical interview screenings to bolster their career prospects. Last, but certainly not least, SynergisticIT takes great care to respect the privacy considerations for every client. All companies who work with SynergisticIT can rest assured their confidential data is protected using the most up-to-date encryption technologies. SynergisticIT also complies with all the latest NDA agreements.\n\n REQUIRED SKILLS For Java /Software Programmers \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Time series analysis \n\n We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2022 and at Gartner Data Analytics Summit (Florida)-2023 \n\nOracle CloudWorld Event (OCW) Las Vegas 2022 | SynergisticIT - YouTube\n\n https://www.youtube.com/watch?v=OAFOhcGy9Z8 \n\n https://www.youtube.com/watch?v=EmO7NrWHkLM \n\n https://www.youtube.com/watch?v=NVBU9RYZ6UI \n\n https://www.youtube.com/watch?v=Yy74yvjatVg \n\nSynergisticIT at Gartner Data and Analytics Summit 2023 - YouTube\n\nFor preparing for interviews please visit  https://www.synergisticit.com/interview-questions/ \n\n We are looking for the right matching candidates for our clients \n\n Please apply via the job posting \n\n REQUIRED SKILLS For Java /Full Stack/Software Programmer \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning Positions \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Tensorflow \n\n If you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team. \n\n No phone calls please.  Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates"}, {"title": "Data Engineer", "company_name": "Anika Systems", "location": "Leesburg, VA (Remote)", "link": "/jobs/view/3787766893/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=mApL9JnN%2B%2Fr2%2BYUPlxRYOA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787766893", "description": "About the job\nAnika Systems is a fast growing, woman-owned small business that specializes in providing innovative IT solutions for federal government agencies. Our expertise lies in accelerating delivery in Data and Analytics, Intelligent Automation, Application Development, and IT Modernization. We are currently expanding our Federal team and are seeking a passionate and talented Data Engineer. This opportunity is 100% remote.\n\nMust be a U.S. Citizen with the ability to obtain and maintain a government suitability clearance\n\nResponsibilities\n\nDesigns, develops, builds, analyzes, evaluates, and installs database management systems to include database modeling and design, relational database architecture, metadata, and repository creation and configuration management.\nDefines and oversees database organizations, standards, controls, procedures, and documentation. Provides technical consulting in the definition, design, and creation of a database environment.\nAdvises applications development staff and users on data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nDesigns and implements databases with respect to access methods, access time, batch processes, device allocation, validation checks, organization, protection and security, documentation, and statistical methods.\nUses data mapping, data mining, and data transformational analysis tools to design and develop databases.\nDetermines data storage and optimum storage requirements.\nPrepares system requirements, source analysis, and process analyses and designs throughout the database implementation.\n\nRequired Skills And Experience\n\nBA/BS and 1 year of relevant experience\nExperience with DataBricks, SQL, Python\nApplications development experience with data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nExcellent communication skills (both oral and written) with process-oriented organizational skills to ensure project success\n\nDesired Skills And Experience\n\nDemonstrated successful development experience with the full life cycle of government database management systems including database modeling and design, relational database architecture, metadata and repository creation, and configuration management.\n\nPowered by JazzHR\n\nTlKWIorucJ"}, {"title": "Remote Work - Need Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3774712978/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=r3Bp%2F0qcOD2OpC5jpbUOcA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774712978", "description": "About the job\nTitle : Data Engineer\n\nLocation : Remote\n\nDuration : 6+ months to start (long term contract (work order are extended 2x year))\n\nPlease Fill out the skill matrix: Technology Some Awareness (1) Novice (2) Intermediate (3) Advanced (4) Expert (5) SQL Tableau Desktop /Creator Tableau API Integration AWS - S3 Glue AWS - Athena Redshift AWS - Lambda AWS - SNS/SQS msExcel\n\nProfile: Data Analyst will work closely with Architects and leads in the Interflow team, key business stakeholders including program staff to support the organizations effort in the areas of data Analysis and information management. Role should have strong experience in writing SQL queries to extract data for reports. Candidate should have experience working with large set of data. As these reports are very critical for the company and we are looking for an expert who is capable of handling, consuming and extracting large sets of data for analysis and reports.\n\nMust Haves\n\nAWS - Athena\nAWS - S3\nGlue\nhands-on development of reporting applications\nMS Excel\nRedShift\nSQL\nTableau Desktop /Creator\n\nNice\n\nAWS - Lambda\nAWS - SNS/SQS\nTableau API Integration\n\nSkills\n\nBA/BS required (major in an analytical field desired) \nA minimum 10+ years work-related experience. \nExperience in hands-on development of reporting applications for the Web in a professional environment \nThe Ethos of continuous improvement and interest in learning new things.\nStrong analytical thinking and structured problem-solving ability\nAbility to handle multiple projects and assignments simultaneously and effectively in a cross-functional team environment. \nVersed on the agile methodology and best practices. \nExcellent interpersonal and collaboration skill with the ability to work with a diverse set of colleagues, across functions, from different organizations, disciplines, etc. \nSelf-starter, ability to set priorities, work independently and attain goals"}, {"title": "Data Engineer", "company_name": "Team Remotely Inc", "location": "Houston, TX (Remote)", "link": "/jobs/view/3786249829/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=78lY5Jo%2FfyRPC6TQYCdvww%3D%3D&trk=flagship3_search_srp_jobs", "id": "3786249829", "description": "About the job\nThis is a remote position.\n\n Data Engineer (US/Canada Residents Only, 1 year experience, remote)\n\nTeam Remotely Inc. is a staffing and recruitment agency that offers a comprehensive solution for talent acquisition, including sourcing, vetting, pay rolling, and managing talent. Whether you need contract staffing, direct hire, direct sourcing, talent pools, or diversity initiatives, our model can support your hiring strategy.\n\n Hiring Type: Full-Time\n\n Base Salary: $55K-$70K Per Annum.\n\nResponsibilities\n\nDesigning user interface changes for web-based DB applications. \nReviewing application requirements and interface designs. \nDeveloping and implementing highly responsive user interface components using react concepts. \nWriting application interface codes using JavaScript following react.js workflows. \nTroubleshooting interface software and debugging application codes. \nDeveloping and implementing front-end architecture to support user interface concepts. \nMonitoring and improving front-end performance. \n\nQualifications And Experience\n\nBachelor\u2019s degree in computer science, information technology, or a similar field. \nPrevious experience working as a react.js developer. \nIn-depth knowledge of JavaScript, CSS, HTML, jQuery, and front-end languages. \nKnowledge of REACT tools including React.js, Webpack, Enzyme, Redux, and Flux. \nExperience with user interface design. \nKnowledge of performance testing frameworks, including Mocha and Jest. \nExperience with browser-based debugging and performance testing software. \nTroubleshooting skills. \nProject management skills. \nProblem-solving skills. \nVerbal communication skills. \n\n Why work with Team Remotely?\n\nTeam Remotely Inc. is a staffing platform offering a seamless experience for employers and candidates. Employers can post job openings and specify their requirements, while candidates can create profiles and upload resumes.\n\nThe team of Team Remotely continuously learns and adapts based on previous successful placements, constantly improving its matching capabilities. This ensures that the recommendations provided by Team Remotely are tailored and accurate, increasing the likelihood of a successful match between employers and candidates. By providing intelligent and data-driven solutions, they strive to enhance the efficiency and effectiveness of the hiring process, ultimately helping companies find the best talent and individuals find their dream jobs."}, {"title": "Data Engineer", "company_name": "ZAG Technical Services", "location": "United States (Remote)", "link": "/jobs/view/3771403464/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=Hf0VefrKXBFIL5MtHmY0Pw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3771403464", "description": "About the job\nWe are looking for a tech savvy Data Engineer to contribute to the building, operating, and scaling of next generation data platforms and tools that will power data-driven analytics capabilities throughout the ZAG products and services portfolio, spanning areas such as business intelligence, reporting, and data analytics.\n\nThe ideal team member is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. Must be comfortable supporting the data needs of multiple clients and teams, systems, and products within the modernization of ecosystems. The right-fit person will be excited by the prospect of optimizing or even re-designing data processes to support next generation of advanced analytics capabilities, which in-turn translates to implementing strategic enhancements to maintain competitive advantage in the industry.\n\nThe successful team member will leverage their proficiency to\u2026\n\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\nBuild the analytics processes required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Data Management and cloud based \u2018big data\u2019 technologies.\nBuild analytics tools that utilize technologies to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.\nCollaborate with client teams to identify and solution regional and local requirements within the context of the standard global business model for enterprise analytics.\nParticipates in meetings, discussions, strategy sessions where changes, improvements, and enhancements are proposed, evaluated, and approved. \n\n\nThe exceptional team member will have or demonstrate progressive experiences in\u2026 \n\nBachelor\u2019s degree or higher in Computer Science or related field\nExperience working with BI and data warehousing tools\nExperience working with enterprise data, where security is paramount and data governance is critical\nAbility to work independently and in a team environment, and effectively engage all levels of the organization\nUnderstanding of distributed systems driving large-scale data processing and analytics with a successful history of manipulating, processing, and extracting value from large, disconnected datasets.\nAbility to communicate effectively (listening, presenting, and questioning)\nStrong organizational, written, and communication skills\n\n\nAbout ZAG\n\nZAG Technical Services empowers our clients\u2019 success through intelligent business solutions that increase employee productivity, network security and team collaboration. We aim to ensure that our client\u2019s IT investments drive ROI, provide a competitive advantage and help businesses grow. We do this by living our core set of values: Integrity Foremost, Client-Centered, Accountable Always, Teamwork Throughout, and Exceptional is the Goal.\n\nWhat ZAG Has To Offer\n\nCompetitive compensation package \nBenefits including medical, dental, vision, 401K and life insurance \n\n\nZAG Technical Services is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all of our employees.\n\nZAG Technical Services is an E-Verify participant and will provide the Social Security Administration (SSA) and, if necessary, the Department of Homeland Security (DHS), with information from each new employee\u2019s Form I-9 to confirm work authorization."}, {"title": "Junior Data Engineer", "company_name": "HireMeFast LLC - Career Accelerator - Land A Job", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3786249797/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D&trk=flagship3_search_srp_jobs&lipi=urn%3Ali%3Apage%3Ad_flagship3_search_srp_jobs%3BQExogrHBQoSF9zZm%2B9kufw%3D%3D&lici=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D", "id": "3786249797", "description": "About the job\nThis is a remote position.\n\n Job Title: Junior Data Engineer\n\n Employment Type: Full-Time\n\n Salary: $64,000 - $76,000 per annum\n\n Experience Required:  Minimum 1 year of project experience\n\nAbout us: HireMeFast is a leading staffing and recruitment agency specializing in connecting businesses with top-tier talent across various industries. Our mission is to bridge the gap between exceptional candidates and organizations needing their skills, expertise, and unique qualities. Our team of experienced and dedicated recruitment specialists utilizes innovative sourcing strategies, and a vast network to identify and attract top talent. We conduct comprehensive procedures to ensure that only the most qualified candidates are presented to our clients.\n\nPosition Summary\n\nJoin the fast-paced, innovative, and collaborative environment focused on providing an AIOps platform that enhances the intelligence of the CVS Health infrastructure. Work closely with subject matter experts and colleagues to build and scale out machine learning and AI solutions that will detect, predict, and recommend solutions to correct issues before system impact and enhance efficiency, reliability, and performance of CVS Health\u2019s IT operations.\n\nKey Responsibilities include:\n\nData pipeline development: Designed, implemented, and managed data pipelines for extracting, transforming, and loading data from various sources into data lakes for processing, analytics, and correlation. \nData modeling: Create and maintain data models ensuring data quality, scalability, and efficiency \nDevelop and automate processes to clean, transform, and prepare data for analytics, ensuring data accuracy and consistency \nData Integration: Integrate data from disparate sources, both structured and unstructured to provide a unified view of key infrastructure platform and application data \nUtilize big data technologies such as Kafka to process and analyze large volumes of data efficiently \nImplement data security measures to protect sensitive information and ensure compliance with data and privacy regulation \nCreate/maintain documentation for data processes, data flows, and system configurations \nPerformance Optimization- Monitor and optimize data pipelines and systems for performance, scalability and cost-effectiveness \n\nCharacteristics of this role:\n\nTeam Player: Willing to teach, share knowledge, and work with others to make the team successful. \nCommunication: Exceptional verbal, written, organizational, presentation, and communication skills. \nCreativity: Ability to take written and verbal requirements and come up with other innovative ideas. \nAttention to detail: Systematically and accurately research future solutions and current problems. \nStrong work ethic: The innate drive to do work extremely well. \nPassion: A drive to deliver better products and services than expected to customers. \n\nRequired Qualifications\n\n2+ years of programming experience in languages such as Python, Java, SQL \n2+ years of experience with ETL tools and database management (relational, non-relational) \n2+ years of experience in data modeling techniques and tools to design efficient scalable data structures \nSkills in data quality assessment, data cleansing, and data validation \n\nQualifications\n\nKnowledge of big data technologies and cloud platforms \nExperience with technologies like PySpark, Databricks, and Azure Synapse. \n\nEducation\n\nBachelor\u2019s degree in Computer Science, Information Technology, or related field, or equivalent working experience\n\nWhy HireMeFast LLC?\n\nAt HireMeFast, we understand that finding the right individuals to join your team is crucial for success and growth of your organization. We are committed to streamlining the hiring process for our clients, ensuring they have access to a diverse pool of highly qualified candidates who are a perfect fit for their specific needs."}, {"title": "Data Engineer  - Need candidates from Texas ( REMOTE )", "company_name": "Pulivarthi Group (PG)", "location": "United States (Remote)", "link": "/jobs/view/3770685212/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=9gdMwuif%2BpB%2BCQYvQi3imQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3770685212", "description": "About the job\nFollow us on Linkedin: https://www.linkedin.com/company/pulivarthigroup/\n\nPulivarthi Group LLC is a Global Staffing & IT Technology Solutions company, with our prime focus of providing world class solutions to our customers with the right talent. We combine the expertise of our team and the culture of your company to help you with the solution that is affordable and innovative using high quality standards and technologies.\n\nWe\u2019ve served some of the largest healthcare, financial services, and government entities in the U.S.\n\nData Engineer - FT only - Azure SQL with data lakes experience\n\nData Engineer -\n\nthe resource requirement for this Data Engineer role needed more clarification.\n\nOur customer is requesting a resource who knows Azure CI/CD (Continuous Integration / Continuous Deployment) to build and integrate their DevOps pipeline.\n\nOnce the DevOps pipelines are mapped, they need the Data Engineer to connect to all the Data Sources, primarily their Data Lake.\n\nThe Customer (based in Houston) is interested in a resource in a time zone not too far removed from the Central Time Zone to allow real time collaboration."}, {"title": "Data Engineer", "company_name": "Otter Products", "location": "Fort Collins, CO (Remote)", "link": "/jobs/view/3766728302/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=77U3U7CAi1PxgDmpI3%2BiNQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3766728302", "description": "About the job\nOverview \n\nOtter Products is currently recruiting for a Data Engineer to join our Data team! You can be based in our Fort Collins, CO office with flexibility to work a portion of your time remotely, or 100% Remote in the U.S.\n\nThe Data Engineer applies professional experience, concepts, and company objectives toward building and operationalizing the minimally inclusive data necessary for the enterprise data initiatives following industry standard practices and tools. The bulk of the Data Engineer's work would be in designing, managing and optimizing data pipelines, then moving these data pipelines effectively into production for key data and analytics consumers like business/data analysts, data scientists or other stakeholder that needs curated data for data and analytics use cases across the enterprise. The Data Engineer also ensures compliance with data governance and data security requirements while creating, improving and operationalizing these integrated and reusable data pipelines to enable faster data access, integrated data reuse and vastly improved time-to-solution for data and analytics initiatives. Additionally, the Data Engineer will be expected to collaborate with other data team members and data consumers working on the models and algorithms developed by them toward optimization for data quality, security, and governance to put them into production leading to potentially large productivity gains.\n\n About Otter Products \n\nOtter Products, we grow to give. From our founder's garage in 1998 to the global technology leader we are today, Otter Products continues to drive growth through innovation.\n\nThrough our industry-leading brands - OtterBox, Liviri and OtterCares - we provide our partners the number-one selling and most trusted products in our categories. Our philanthropic spirit is the foundation on which we foster our partner relationships, allowing us to grow and to give - together.\n\nBy way of our charitable arm, the OtterCares Foundation, we support our communities and invest in our future through education that inspires kids to change the world.\n\nAnd even as our global community of Otters continues to grow, our founder's core values are still at the heart of everything we do. We measure our success by our ability to give back to our communities and strengthen opportunities for all.\n\nFor more information visit otterproducts.com\n\n Responsibilities \n\nComplete analysis, design and development of BI solutions using Azure SQL Server\nFamiliarity with Data Warehouse concepts\nExperience coding the extraction, transformation, and loading (ETL) processes\nDatabase development primarily in SSIS, Data Factory and SQL\nPartner with the business to determine end user database/reporting needs and requirements\nGenerate ad hoc reports using Power BI or SSRS\nCollaborate with other developers to create and implement best approach solution(s)\nTroubleshoot Azure tools, systems, and software\nReview queries for performance issues, making changes as needed\nCollaborate with team to performance-tune Azure application as necessary\nAssist with the analysis and extraction of relevant information from large amounts of historical business data to feed data science initiatives\nParticipate and support the design and documentation of processes for large scale data analyses, model development, model validation and model implementation\nSupport and maintain a positive safety culture by following all safety policies and procedures and actively contributing to a safe working environment\nOther duties as required\n\n Qualifications \n\nBachelor's degree required. Degree in Computer Science or Mathematics preferred\nMinimum of four years of experience in an IT or analytical role required. Experience in database development, report writing and/or statistics preferred.\n\n EEO \n\nOtter Products, LLC is an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, age, marital status, pregnancy, sex, sexual orientation, gender, gender identity or expression, national origin, disability, veteran status, or any other characteristic or status protected by law.\n\n For US Based Roles Only - Compensation Range Minimum \n\nUSD $90,000.00/Yr.\n\n For US Based Roles Only - Compensation Range Maximum \n\nUSD $120,000.00/Yr.\n\n Additional Total Rewards \n\nProfit Sharing Program Eligible, Benefits Eligible - Full Time- check out otterproducts.com/careers/why for more info"}, {"title": "Software Engineer, Distributed Systems", "company_name": "Hightouch", "location": "United States (Remote)", "link": "/jobs/view/3628572539/?eBP=CwEAAAGMd4QO8APODyE_-mMOPhYyQGg5ZjVbPwq_3V7DK0B2bLAf10pcedM9S2T71X5i4TiJuEwOzWBbSiTKbLuk-VD-_Y-dN5uyf8YpYkHYqEYFxySMS9zr2hRjNrxX9gXbxssh35TFK9YTNV2WeH2Ncb675JgFbJTgNWSZySFC6rrdJGmdSDTr2q5d3VoxORz3GjsDbzjeyZQCvQ07RqYw7oujWSgslycs2LMCY_-N0NEj6e0PvbaYKiZ_5IW34bGVSYLXpHwqa8oOcNttYZ90UA3E_7sDSFzABdIVlfrp5zzERrIYy5sh2MIInxHdDoKgQw8VmOn5uM4S0tXUl-i7LnN0_W0NfPBmLiVDNuveIOm4MfDNPxfe5QKPk6FUAVpvgbcoJn9yRyE&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=rph%2FT6Q6Vkp6YoggJf9cJQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3628572539", "description": "About the job\nAbout Hightouch\n\nHightouch\u2019s mission is to empower everyone to take action on their data. Through our Reverse ETL platform, business and data users can seamlessly sync data from where it resides, such as warehouses and databases, to where it is needed, including operational systems and SaaS tools. Traditionally, acting on data has required engineering time and bandwidth, and left most business users stuck with charts and reports that are unable to take automated action on their data. With Hightouch, every business user, without writing any code, can activate data to streamline critical processes, improve marketing performance, and scale operations.\n\nOur team operates with a focus on making a meaningful impact for our customers. We believe in approaching challenges with a first principles thinking mindset, moving quickly and embracing our value of efficient execution, and treating each other with compassion and kindness. We look for team members that are strong communicators, have a growth mindset, and are motivated and persistent in achieving our goals.\n\nHundreds of companies use Hightouch, including Spotify, Ramp, Retool, NBA, Plaid, and Betterment. We\u2019re based in San Francisco, are remote-friendly, and backed by leading investors such as Amplify Partners, ICONIQ Growth, Bain Capital Ventures, Y-Combinator, and Afore Capital.\n\nAbout The Role\n\nWe are looking for a distributed systems engineer to work on the systems that power the syncing engine our customers and other engineering teams rely on. This presents an exciting challenge where you can apply your expertise in distributed systems, performance optimization, and troubleshooting to push the boundaries of what is possible and meaningful improve our ability to move massive amounts of customer data. This role also provides a unique opportunity to work on a multi-cloud and multi-region infrastructure that supports a global customer base.\n\nWe believe in enabling our engineers to do their best work for our customers by giving them extremely high levels of ownership and autonomy. This comes in different forms: you will own and deliver projects from start to finish, you will work directly with customers to solve their hardest scaling problems, and you will have a lot of influence over what we work on as a team and company.\n\nSome of the problems we\u2019ll be working on include:\n\nSync Speed: Customers want to sync a lot of data to important destinations like Facebook and Snapchat, which requires us to analyze every part of our syncing process and find where we can optimize to sync data more quickly\nStreaming Syncing: We currently sync data from sources in batch only, but envision a world where we also do syncs in real-time and support streaming sources like webhooks and queues\nScalability and Reliability: As part of our rapid growth, we\u2019re always evaluating current future bottlenecks in scaling and reliability, and architecting for the next order of magnitude of growth\nPersonalization API: Our Personalization API product provides a low latency caching layer on top of data warehouses that enable customers to use their data warehouse for real-time personalization use cases with \nMulti-Region and Multi-Cloud: Supporting our multi-region and multi-cloud backend, including extending it to launch Hightouch on in new regions to support data residency requirements of our global customer base\n\nWe are looking for talented, intellectually curious, and motivated individuals who are interested in tackling the problems above. This is a senior role, but we focus on impact and potential for growth more than years of experience. The salary range for this position is $170,000 - $240,000 USD per year, which is location independent in accordance with our remote-first policy.\n\nAbout You\n\nYou are an engineer with a passion for solving hard technical problems that generate real value for customers. You\u2019re motivated by high ownership and are comfortable in a fast-paced, startup environment.\n\nYou have experience and comfort with distributed systems and high-scale systems or the ability and desire to build this experience quickly. This experience can take different forms:\n\nYou\u2019ve built syncing engines that process and move terabytes of data per day in a scalable way\nYou\u2019ve created event collection or data streaming services that power company critical systems\nYou\u2019ve owned systems that have undergone orders of magnitude of growth and helped them scale accordingly\nYou have relevant skills and are a fast learner who is excited to take on the challenges we face\n\nInterview Process\n\nOur goal with the interview process is to balance speed with giving both parties opportunities to assess whether there is a strong mutual fit. We will ask you questions, but we want you to ask us questions! Our technical interviews focus on how you design systems because we believe this is the best way for us to see how you work and for you to see how we collaborate. We don\u2019t ask you to write code to solve technical brainteasers that don\u2019t appear in your day to day job.\n\nRecruiter screen [30m]: Introductory call with our recruiting team to get to know each other and see if the role could be a good mutual fit.\nSystems design screen [45m]: Designing a data processing feature end-to-end.\nHiring manager interview [30m]: Chat with hiring manager about past experiences and future operating preferences to assess fit on company values and operating principles.\nSystems design interview [90m]: Work with the interviewer to architect a system at a conceptual level. The problem will be at a pretty high level - and have both product and customer requirements as well as technical."}, {"title": "Entry-Level Data Scientist Engineer", "company_name": "Phoenix Recruitment LLC", "location": "Texas City, TX (Remote)", "link": "/jobs/view/3788727625/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=8iwtytHsCu4wMwJ4fj3pxw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3788727625", "description": "About the job\nThis is a remote position.\n\nTitle - Entry-Level Data Scientist Engineer, 1 year of project experience\n\nEmployment Type: Full-time\n\nBase Salary: $60K-$70K\n\nPhoenix Recruitment offers a variety of recruiting services to assist both employers and employees. They specialize in marketing open positions, recruiting, and helping employers to find qualified candidates across various industries. Phoenix Recruitment has expertise in streamlining the hiring process. They can help ensure that the process is efficient, well organized, and compliant with relevant regulations.\n\nSkills and Abilities:\n\nStrong knowledge of R or Python for data analysis and modeling. \nProficiency in statistical programs such as R, SAS, MATLAB, or Python. \nFamiliarity with spreadsheets (VBA) and database applications (Access, Oracle, SQL, or equivalent technology). \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \nAbility to learn new methods quickly and work under deadlines. \nExcellent teamwork and communication skills. \nStrong analytical and problem-solving abilities. \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \n\nPreferred:\n\nKnowledge of actuarial concepts and life, health, and/or annuity products. \nExperience with statistical modeling techniques such as GLM, Decision Trees, Time Series, Regression, etc. \nFamiliarity with Microsoft DeployR. \nExposure to insurance risk analysis. \nBasic experience in computational finance, econometrics, statistics, and math. \nKnowledge of SQL and VBA. \nFamiliarity with R or Python for predictive modeling \n\nWhy Phoenix Recruitment LLC?\n\nPhoenix Recruitment often has an extensive network of employers and candidates. This network allows them to tap into a pool of qualified candidates and connect them with suitable job opportunities. They can also leverage their connections to help employers find the right talent efficiently. Outsourcing the recruitment process to a specialized agency can save your time and resources, avoid delays, reduce administrative burdens, and increase the chances of finding the right fit for your organization."}, {"title": "Data Engineer - Remote Work", "company_name": "BairesDev", "location": "United States (Remote)", "link": "/jobs/view/3783162297/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=OtoCrAAHZFomVup04fy35g%3D%3D&trk=flagship3_search_srp_jobs", "id": "3783162297", "description": "About the job\nWho We are\n\nBairesDev is proud to be the fastest-growing company in America. With people in five continents and world-class clients, we are only as strong as the multicultural teams at the heart of our business. To consistently deliver the highest quality solutions to our clients, we only hire the Top 1% of the best talents and nurture their professional growth on exciting projects.\n\nWe are looking for a Data Engineer to join our Development team and participate in different projects made up of multicultural teams distributed throughout the world.\n\nWhat You Will Do\n\n Interview other Data Scientists and Data engineers to vet their technical abilities.\n Input data from those interviews into the product and help develop a predictive, fair, and enjoyable solution.\n Work together with developers, tech leads, and solution architects to build applications.\n Improve existing structures by adding new functionalities or proposing technological updates so that as a result, the impact of your contribution is significant in the core of each business.\n Improve the user experience on scalable and high availability platforms, contributing to the key differential of each business.\n\nHere\u2019s What We Are Looking For\n\n 5+ years of experience working as a developer.\n 3+ years of experience working as a Data Scientist / Data Engineer.\n Proficient with analysis, troubleshooting, and problem-solving.\n Hands-on experience with managing data loads and data quality.\n Advanced English level.\n\nHow we do make your work (and your life) easier:\n\n 100% remote work.\n Hardware setup for you to work from home.\n Flexible hours - make your schedule.\n Paid parental leave, vacation & holidays.\n Diverse and multicultural work environment.\n An innovative environment with the structure and resources of a leading multinational.\n Excellent compensation \u2014 well above the market average.\n Here you can grow at the speed of your learning curve.\n\nOur people work remotely but with a consistent and robust culture that promotes diversity and teamwork. To continue being the leading software development company in Latin America, we want to ensure that every BairesDev member gets the best growth and professional development opportunities in a diverse, welcoming, and innovative environment.\n\nEvery BairesDev team member brings something unique to our company.\n\nWe want to hear your story. Apply now!"}, {"title": "Data Engineer (Remote)", "company_name": "Cascade Debt", "location": "United States (Remote)", "link": "/jobs/view/3755336571/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=pmUtn1j2zyPr25CNPh2RNw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3755336571", "description": "About the job\nDo you want to work in a small team where you can make a real difference in a company? Would you embrace the challenge of building a high impact platform that is used globally? Then we want to speak with you!\n\nAbout Cascade\n\nCascade is a fintech startup backed by Canadian and US investors that empowers companies to grow by democratizing access to institutional debt. We are building tools that modernize how companies raise and manage debt, lowering the barriers to entry to the $7 trillion specialty finance and alternative credit market for companies around the world. Founded just last year, we are gearing up for our next phase of technical development and are seeking a talented Data Engineer to lead the way.\n\nAbout The Role\n\nAs a Data Engineer you\u2019ll be an early member of a growing team building a pioneering platform in the debt infrastructure space. You will design, build, and launch efficient, scalable, and reliable data pipelines to move and transform data .\n\nWhat You\u2019ll Do\n\n Building databases, data lakes and data ingestion pipelines to integrate customer databases and datasets to our systems \n Ingesting financial datasets from external customers, then u pdating and maintaining accurate and complete data mappings to ensure that our product s are displaying high quality data \n M onitoring and alerting across our data pipelines in order to make sure that our data ingests are reliable and correct \n Translat ing business goals and stakeholder requirements into new data flows and deliver analytical insights ensuring that the data flows create real business value continuously \n\nSkills And Experience\n\n 2 + years experience in a data engineering or data analyst role \n E xperience with relational SQL and NoSQL databases, including PostgreSQL , MySQL, MariaDB, MongoDB, Bigtable, etc. \n E xperience working with ETL technologies , such as Databricks, Fivetran , or dbt \n Proficiency in writing SQL queries and knowledge of analytical data warehouses such as RedShift, BigQuery , and Snowflake \n Some experience with CI/CD automation such as GitHub Actions \n Developing APIs and integrating with 3rd party APIs \n Bonus: experience in fintech / SaaS / credit analysis \n\nPeople Who Thrive At Cascade Are\n\n Self-starters, who take the initiative to tackle challenges in a remote work environment \n Good communicators, who can operate without ego to discuss, learn, grow, and help others do the same \n Passionate about creating great digital experiences for users \n Problem solvers who are not satisfied with the status quo \n\nBenefits\n\nRemote: we are a remote-first company and are very flexible on hours as long as things get done \nHome-office: there\u2019s a $1000 USD home office allowance to set yourself up \nEquity: we expect you to have an owner-mentality, and have the equity plan to match \nBenefits: health, dental, vision, and more \nPerks: we offer free lunches weekly and off-site trips \nJob satisfaction: we offer autonomy, ample opportunities for mastery, and an opportunity to make a difference for companies around the world"}, {"title": "Data Engineer", "company_name": "ISC (Integrated Specialty Coverages, LLC)", "location": "Carlsbad, CA (Remote)", "link": "/jobs/view/3778293528/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ew7%2FfEuXSujs5IGabdqUyw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3778293528", "description": "About the job\nAbout Integrated Specialty Coverages\n\nIntegrated Specialty Coverages LLC (ISC) is a growth stage technology and data-driven insurance company leading innovation in the market.\n\nBacked by one of the leading private equity firms, KKR, and led by a forward-thinking management team, ISC is combining the worlds of insurance and technology to create an Insurtech powerhouse. As a leading online distributor of insurance products for a range of industries and \"Main Street USA\", we are looking for the right people to help us in our mission of achieving exponential growth. We strive to be the number one place to go for brokers and agents to source insurance. To accomplish this, we're building a digitally focused team that deeply understands the intersection between user experience, data, and AI/ML to optimize the way we engage with our customers and partners.\n\nJob Summary\n\nThe Data Engineer role will be the technical liaison between multiple groups including a data science team, the engineering team, product management, and business stakeholders. You do not need any insurance knowledge prior, however, you must quickly dive deep into the insurance world and ask questions to become a subject matter expert. You will be responsible for building a data platform to facilitate the data science team. You must be a self-starter that can build out features such as a data pipeline from scratch. There will be support from both engineering and data science for any buildout.\n\nJob Responsibilities\n\nCreate and maintain optimal data pipeline architecture. \nAssemble large, complex data sets that meet functional / non-functional business requirements. \nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. \nBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using AWS technologies, SQL, and Python. \nWork with stakeholders including the Executive, Product, Data Science, and Engineering teams to assist with data-related technical issues and support their data infrastructure needs. \nWork with data science and analytics teams to strive for greater functionality in our data systems. \n\nMinimum Qualifications:\n\nAdvanced working SQL knowledge and experience working with relational databases, strong query authoring (SQL) as well as working familiarity with a variety of databases (Snowflake, Redshift, MySQL, MSSQL, etc.)\nExperience building and optimizing data pipelines, architecture and data sets. \nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. \nBuild processes supporting data transformation, data structures, metadata, dependency, and workload management. \nA successful history of transforming, processing and extracting value from large disconnected, datasets from a variety of data sources (Flat files, Excel, databases, etc.)\nStrong analytic skills related to working with unstructured datasets. \nStrong project management and organizational skills. \nExperience supporting and working with cross-functional teams in a dynamic environment. \nAbility to mentor/guide/collaborate with other team members. \n\nWe are looking for a candidate with 3-5 years of experience in a Data Engineer role. They should also have experience using the following software/tools:\n\nExperience with relational and MPP databases, including Snowflake and MySQL. \nExperience developing software in an agile environment from the requirements stage to production. \nExperience with version control: git\nExperience with container technologies: Docker\nExperience with data pipeline and workflow management tools: Airflow, Jenkins, AWS Glue, Azkaban, Luigi, etc. \nExperience with AWS cloud services: EC2, ECS, Batch, S3, EMR, RDS, Redshift\nExperience with other cloud services: Snowflake, Airflow\nExperience with object-oriented/object function scripting languages: Python, Java, C++, etc. \nExperience with data modeling and data warehouse design\nExperience with data visualization tools (PowerBI, QuickSight)\n\nThe starting annual pay scale for this position is listed below. Actual starting pay will be based on factors such as skills, qualifications, training, and experience. In addition, the company offers comprehensive benefits including medical, dental and vision insurance, 401(k) plan with match, paid time off, and other benefits.\n\nISC's salary ranges are determined by role and level. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations and could be higher or lower based on a multitude of factors, including job-related skills, experience, and relevant education or training.\n\nNational Pay Range\n\n$100,000\u2014$110,000 USD\n\nBenefits of Working at ISC\n\nCompetitive vacation and flexible working arrangements\nComprehensive and inclusive health benefits\nA variety of professional development and mentorship opportunities\nChoice of technology whether at home or in the office\n\nISC's Ownership Behaviors\n\nDo the Right Thing *Be Relentless. Pursue Excellence. *Personal Responsibility & Own Outcomes\nTry Fast. Learn Fast. Fail Fast. Think Big. *Build Extraordinary Owners & Join Forces *Grit & Determination\n\nApplicants with disabilities may contact ISC HR department via e-mail, and other means to request and arrange accommodation. If you need assistance to accommodate a disability, you may request accommodation at any time. Please contact ISC at HR@ISCMGA.com.\n\nISC believes in creating long-term relationships by being responsive and relevant and by consistently delivering value to our community of customers. Specifically, with our employees, we focus on attracting, developing, and retaining the best talent for our business, challenging our people, demonstrating a \"can-do\" attitude, and fostering a collaborative and mutually supportive environment. ISC is an equal opportunity employer and a participant in the US federal E-Verify program (US).\n\nDiversity creates a healthier atmosphere: All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, gender, gender identity, sexual orientation, marital status, medical condition, genetic information, mental or physical disability, military or veteran status, or any other characteristic protected by local, state, or Federal law."}, {"title": "Data Engineer -Remote", "company_name": "RIT Solutions, Inc.", "location": "Denver, CO (Remote)", "link": "/jobs/view/3768007496/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=1hQH8EDaII1eJ7nQ9BACQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3768007496", "description": "About the job\nDuration: 6+ months CTH\n\nMust have a valid LinkedIn profile\n\nRequired Skills\n\n 3-6 years of experience (no more than 6 years)\n Start up environment experience\n Python, SQL, and DBT"}, {"title": "Big Data Engineer - W2", "company_name": "Megan Soft Inc", "location": "United States (Remote)", "link": "/jobs/view/3674845652/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fCAxh4N%2F4YZJ5aEIASaeeA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3674845652", "description": "About the job\nTitle: Big Data Engineer - W2\n\nLocation: Dearborn, MI (Remote)\n\nDuration: 12+ Months\n\nDirect Client: Ford Motors\n\nMust have skills: Should be able to handle GCP requirements\n\n\"Translates business requirements to build repeatable, sustainable, efficient, coded processes that can be productionized by Software Engineers and readily modified by other Data Engineer Creates POC processes in Dev/QA and works with Software Engineers to productionize and align best coding and process practices that maximize efficiency, speed, stability, system resources and capabilities Awareness of and compliance with data privacy, security, legal and contractual guidelines Incorporates data quality and privacy checks/alerts to minimize bad data being consumed by end users, models and dashboards, and to protect customer data; revises checks for new data issues Validates data products and pipelines are functioning as expected following system or application upgrades, source changes, etc. Works closely with Architects if required to align systems, tools and applications being utilized with business use case and performance requirements Provides data product support and maintenance Establishes, tracks and monitors KPIs related to specific data products and deliverable Participate in iteration cadence activities (Iteration Planning, task estimation, daily scrum, demos, etc.). Will be a part of on-call team to resolve critical production support tickets during business hours and off hours.\n\nSkills Required\n\nExperience with Big Data Technologies (Hadoop , ETL Tool, Spark. Hive etc..) Someone who understands Cloud and has experience working with Cloud Platforms (preferably GCP) Should be able to handle GCP requirements - ingesting into GCS/ Big Query, working with GCP components Hands-on in Data Flow, Cloud Storage (GCS), Big Query and GitHub. Proficient in Python and SQL.\n\nSkills Preferred\n\nManages ETL: uses programming and tools for data ingestion, configures pipelines, applies transformations and decoding, integrates and fuses data, moves and securely deliver Experience NoSQL databases\n\nAdditional Information\n\nIf the candidate is remote only, please indicate \"\"100% Remote\"\" under candidate's name on resume. Other similar schedule notations can be \"\"Local Candidate\"\" or \"\"Hybrid Candidate.\"\" ***HRA Test Required***\""}, {"title": "Remote Opportunity: Quantexa Data Engineer", "company_name": "SPAR Information Systems LLC", "location": "United States (Remote)", "link": "/jobs/view/3680073191/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ioykVCZSOQ7BK5%2FwRQHOWg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3680073191", "description": "About the job\nHello All, Hope you are doing great!! Please go through the job descriptions and let me know your interest. Role: Quantexa Data engineer  Location: Remote Duration: Long Term Contract Job Description: 1. End to End usage of Quantexa including guiding our client team on how to use from both front end and data ingestion Reason they are asking for Certification 2. Resolve Entity issues with data 3. Knowledge of Scala and Hive which requires coding experience using parquet 4. Advanced SQL knowledge requiring writing SQL queries 5. Experience having worked with GCP data lake 6. Quantexa certification required\n\nThanks & Regards,\n\nSatnam Singh\n\nDirect: 201 623 3660\n\nEmail : Satnam.singh@sparinfosys.com"}, {"title": "Remote Work - Need Staff Data Engineer - USC only", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3746283980/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=CvIuLBNO%2BPMGi4IXVRkP0A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3746283980", "description": "About the job\n100% Remote, client is in NY\n\nLook for strong job history, Python, Machine Learning, SQL, 10/10 comms, etc. \n\nMUST be an Engineer \n\nSummary\n\nWe're looking for an experienced Data Engineer to join our client's growing team of top notched IT professionals. You will work closely across multiple groups including Editorial and Audio, Marketing, Advertising, etc. to achieve their goals and objectives. The Data Engineering team builds tools throughout the stack while sharing knowledge across these departments.\n\nAs the company grows, they're looking for Data Engineer who will help solidify and expand our pipelines and maintain their data warehouse. Their business model is based on performing complex and very detailed analyses of how our products perform with their subscribers. You'll be responsible for working alongside analysts, product, marketing, and other teams to define new data collection components and measurement schemes that support each new product and feature.\n\nHow Can Add Value To Their Mission\n\nCreate and maintain data pipelines to provide insights and drive business decisions\nEstablish data warehousing strategy (ex. Kimball, Data Vault, etc.)\nMaintain data infrastructure on our AWS accounts\nCollaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.\nWrite unit/integration tests, contributes to engineering wiki, and documents work\nImplement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it\n\nWhat You'll Need To Succeed\n\n7+ years of industry experience measuring product performance and user behavior\nExperience working with a variety of data management technologies, including RedShift, Kinesis/Kafka, Glue, Spark, Postgres, Airflow, dbt, and others.\nExperience implementing BI reporting tools such as Looker\nExperience interfacing with engineers, product managers and analysts to understand data needs\nKnowledge of a variety of measurement beacons, SDKs, APIs including Google Analytics, Amplitude, Braze, Stripe, email service providers\nA commitment to building secure, resilient, fault-tolerant architectures, with clear documentation and procedures in place for support\nA focus on accuracy and detail as you build, ensuring data pipelines produce clear, predictable results\nUnderstanding of the typical metrics a subscription and advertising-supported business needs to measure success\nExperience with Client techniques as applied to behavioral segmentation or anomaly identification is a definite plus\nFamiliarity using developer tools that increase productivity and facilitate the development of resilient code (eg. Docker, CircleCI, Serverless) is a plus"}, {"title": "ENTRY LEVEL DATA ENGINEER (REMOTE)", "company_name": "SynergisticIT", "location": "Jacksonville, FL (Remote)", "link": "/jobs/view/3767587909/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=om7L3CnowiC4PLIcpZD70A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3767587909", "description": "About the job\nSYNERGISTICIT wants every candidate to know that the Job Market is Challenging and to stand out, you need to have exceptional skills and technologies and that's where we come in to make sure you get the attention which you need\n\nSynergisticIT understands the complex nature of the job market and how difficult it can be to secure a position, especially for fresh graduates. Therefore, we assist and help tech-savvies to convert their passions into professions. We go above and beyond to keep you working in your niche.\n\nAs we focus on long-term success, we provide complete career development solutions. From job search to upskilling portfolio and interview preparation, we can guide you at every step of your career.\n\nSynergisticIT spares no efforts to connect you with a large network of tech giants, including Google, Apple, PayPal, Dell, Cisco, Client, etc. Presently, we are actively looking for ENTRY LEVEL DATA ENGINEER a driven mindset. Get the right opportunity and gain experience in building web-centric solutions on Java.\n\nWho Should Apply : Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or anyone looking to make their career in IT Industry\n\nWe also assist in filing for STEM extension and H1b and Green card filing.\n\nCandidates who are serious about their future in the IT Industry and have set big goals for themselves.\n\nCandidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. We also offer Skill Enhancement Programs if the candidates are missing skills or experience which our clients need with great outcomes\n\nCandidates can benefit from skill enhancement if they fall into the below categories. If they are qualified with enough skills then no need for skill enhancement\n\nCandidates Who Lack Experience\n\nHave had a break in careers\n\nLack Technical Competency\n\ncandidates who want to get employed and make a career in the Tech Industry \n\nPlease Also Check The Below Links\n\nhttps://www.synergisticit.com/candidate-outcomes/\n\nhttps://www.synergisticit.com/java-track/\n\nhttps://www.synergisticit.com/data-science-track/\n\nhttps://www.synergisticit.com/which-is-the-best-option-for-tech-job-seekers-staffing-companies-consulting-companies-bootcamps-or-synergisticit/\n\nhttps://www.synergisticit.com/contact-us/\n\nIf the skills are not a match candidates can opt for Skill enhancement. Or their resume can be sent out to clients to see if responses are achievable\n\nREQUIRED SKILLS For Java/Software Programmers\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Core Java , javascript , C++ or software programming\nSpring boot, Microservices and REST API's experience\nExcellent written and verbal communication skills\n\nFor data Science/Machine learning\n\nRequired Skills\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Statistics, Python, data visualization tools\nExcellent written and verbal communication skills\n\nPreferred skills: NLP, Text mining, Tableau, Time series analysis\n\nTechnical skills are required by clients for selection even if its Junior or entry level position each additional Technical skill helps a candidate's resume to be picked by clients over other job seekers.\n\nClients hire candidates with the right technical skills which they need and reject candidates who lack the required technical skills.\n\nNo third party candidates or c2c candidates\n\nPlease apply to the posting\n\nNo phone calls please. Shortlisted candidates would be reached out"}, {"title": "Data Engineer - Remote | WFH", "company_name": "Get It Recruit - Information Technology", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3782241984/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=BgBf1kSAAX%2BSuw6inJsIVw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3782241984", "description": "About the job\nJoin an innovative team at a leading company in the engineering and manufacturing industry, where we are committed to leveraging data for continuous improvement and operational excellence. This hybrid position allows you to enjoy the flexibility of remote work on Mondays and Fridays.\n\nWhy Join Us\n\nAt our company, we prioritize your well-being and professional growth. We offer a comprehensive benefits package, including excellent health benefits, a 6% 401(k) match, generous paid vacation and sick time, 12 paid holidays, and more.\n\nRole Overview\n\nAs a Data Engineer, you will play a pivotal role in transforming raw data into actionable insights, driving improvements in engineering and manufacturing processes, enhancing product quality, and optimizing operational efficiency. Collaborate with dynamic cross-functional teams to extract, analyze, and visualize data, influencing strategic decisions through your valuable insights.\n\nKey Responsibilities\n\nData Collection and Integration:\n\nGather, clean, and integrate data from diverse sources, including production databases, sensor data, and external datasets.\n\nData Analysis\n\nConduct exploratory data analysis to identify patterns, trends, and anomalies.\n\nPerform statistical analysis and hypothesis testing to derive meaningful insights.\n\nData Visualization\n\nCreate informative and visually appealing dashboards and reports using tools like Tableau, Power BI, or Python libraries.\n\nPredictive Modeling\n\nDevelop predictive models to forecast manufacturing performance, quality, and operational outcomes.\n\nRoot Cause Analysis\n\nInvestigate production issues and quality concerns by analyzing data to identify root causes and recommend corrective actions.\n\nPerformance Monitoring\n\nEstablish and maintain key performance indicators (KPIs) to track manufacturing and engineering metrics.\n\nGenerate regular reports to communicate performance trends to stakeholders.\n\nCross-functional Collaboration\n\nCollaborate with engineering, manufacturing, and quality assurance teams to understand their data needs and provide analytical support.\n\nContinuous Improvement\n\nSuggest process improvements and optimizations based on data analysis.\n\nData Security And Compliance\n\nEnsure data security and compliance with relevant regulations, such as GDPR or industry-specific standards.\n\nInfrastructure And Development\n\nBuild and maintain the infrastructure for collecting, storing, and processing data.\n\nDesign, develop, and manage data pipelines, ETL processes, and data warehouses.\n\nRequired Experience\n\nProven experience as a Sr. Data Analyst and/or Data Engineer in an engineering or manufacturing environment.\n\nProficiency in data analysis tools and programming languages, such as Python, R, SQL, and Excel.\n\nStrong data visualization skills with tools like Tableau or Power BI.\n\nExcellent knowledge of statistical analysis and machine learning techniques.\n\nAbility to communicate complex findings and insights effectively to both technical and non-technical stakeholders.\n\nStrong problem-solving skills and attention to detail.\n\nFamiliarity with manufacturing processes and quality control is a plus.\n\nExperience with enterprise ERPs (SAP S4HANA, Oracle EBS), CRMs (Salesforce, Hubspot), and IoT.\n\nProficient in working with cloud computing platforms including AWS, GCP, and Azure.\n\nExpertise in configuring and optimizing cloud-based data solutions for scalability, reliability, and performance.\n\nBachelor's Degree in Data Science, Statistics, Engineering, or Computer Science Required.\n\nApply Now and be part of a team shaping the future of data-driven innovation in engineering and manufacturing!\n\nEmployment Type: Full-Time"}, {"title": "Entry Level Software Engineer - Data Backend Engineer (Remote - Canada)", "company_name": "Yelp", "location": "United States (Remote)", "link": "/jobs/view/3752881757/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fankeoxifvhpTzOYvowHYQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3752881757", "description": "About the job\nYelp engineering culture is driven by our values: we\u2019re a cooperative team that values individual authenticity and encourages creative solutions to problems. All new engineers deploy working code their first week, and we strive to broaden individual impact with support from managers, mentors, and teams. At the end of the day, we\u2019re all about helping our users, growing as engineers, and having fun in a collaborative environment.\n Interested in exploring data and finding innovative ways to collect it, curate it, and make it easily accessible for others to use? How about doing that at an enormous scale? Yelp\u2019s datasets contain billions of interactions between users and local businesses around the globe. If working with our vast amount of data sounds exciting, come and join us!\n\nThe Marketplace Data Observability team is responsible for managing, governing, and monitoring all of Yelp\u2019s Ads data to protect our revenue stream and also support offline analysis that aids in making long term company decisions. We build tools that help us to better understand our data quality, and we strive to uncover data issues before they impact our consumers.\n\nAs a Data Backend Engineer on this team, you will be responsible for building elegant and scalable data products that serve critical, up-to-date, structured information to support different types of analytics within Yelp. As a core contributor to our growing data modeling and data warehousing engineering efforts, you will help design and own mission-critical data flow pipelines and datastores to enable decisions including effective A/B testing and company investments.\n This opportunity is fully remote and does not require you to be located in any particular area in Canada. We welcome applicants from throughout Canada. We\u2019d love to have you apply, even if you don\u2019t feel you meet every single requirement in this posting. At Yelp, we\u2019re looking for great people, not just those who simply check off all the boxes.\n  Build systems that can effectively store and crunch terabytes of data.\nDesign and develop data models for efficient data storage, retrieval, and reporting.\nCreate and maintain conceptual, logical, and physical data models using industry-standard modeling tools.\nCollaborate with cross-functional teams, including engineers, data analysts, business analysts, and data scientists, to understand data requirements and translate them into effective data models.\nParticipate in data integration efforts, including ETL processes and data migration.\nStay up-to-date with industry best practices, emerging technologies, and trends related to data warehousing.\nSupport on-call rotations as needed to operate the team.\n  Understanding of high performing and scalable data systems.\nExperience in building and orchestrating ETL pipelines.\nExperience with Data Lake or Data Warehouse landscape.\nA hunger for tracking down root causes and fixing them in systematic ways.\nAbility to communicate effectively to technical and non-technical cohorts alike. \nExposure to some of the following technologies: Python, AWS Redshift, AWS Athena / Apache Presto, Big Data technologies (e.g S3, Hadoop, Hive, Spark, Flink, Kafka etc), NoSQL systems like Cassandra, DBT."}, {"title": "Data Engineer (Associate Consultant), June 2024", "company_name": "UDig", "location": "Nashville, TN (Remote)", "link": "/jobs/view/3716039960/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=XUjo40kU55HnDCqJ%2F3FdPg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3716039960", "description": "About the job\nCan UDig It? \n\nUDig designs, builds, and implements technology solutions that deliver on business objectives. People join our team because they are excited to have an impact on the future of our organization, passionate about delivering meaningful results, and energized by solving complex problems. Oh, and they like working with other driven, smart, and authentic people!\n\nUDig is proud to be recognized as a \"Best Places to Work\" in Virginia and Tennessee. We're committed to fostering a transparent environment where leadership is accessible, professional development opportunities are abundant, and you are empowered to achieve your career goals. You'll collaborate, develop deep relationships with teammates, and accelerate your success while having fun!\n\nIf you're graduating in May 2024 and excited by the idea of impacting organizations by building data solutions to achieve business goals, you should consider joining our growing technology team.\n\nHow Things Go Down\n\nAs a new, entry-level teammate, you'll take part in our \"Breaking Ground\" training program designed to help you quickly get up-to-speed on UDig's delivery approach within a project team. The program covers:\n\nUDig delivery process and methodologies such as gathering requirements and delivering projects using agile\nTechnical guidance and instruction on the best practices from Data leaders at UDig\nDelivering executive and technical presentations\nCoaching to prepare you to for success on your first client project at UDig\n\nA typical day might entail:\n\nProject standups and check-ins\nDesign and development of data pipelines, BI reports, and more\nPlanning and participating in project demos\nContributing to internal UDig projects\nCareer and job growth through UDig-provided training\nCreating long-lasting friendships with teammates through fun activities as well as your projects (including social events, winning a prize for guessing the weight of pumpkin, or sharing experience on cool tech side projects)\n\nUDig believes it is important for Associate Consultants going through our Breaking Ground program to reside in one of our major markets (Richmond, VA or Nashville, TN metro area) for at least the first year of employment.\n\nWant to learn more? Hear directly from our teammates by checking out our 'A Day in the Life' blog series: https://www.udig.com/digging-in/breaking-ground-2023/\n\nSound Interesting? Here's the Technical Piece\n\nBelow are a few boxes you should be able to check if\u202fyou're interested: Don't check every box? Go ahead and apply!\n\n1+ year of experience with relational database management systems such as Microsoft SQL Server, MySQL, or Oracle\nUnderstanding of the full lifecycle of design, development, and implementation of data solutions including architecture design, development, testing\nSome experience in ETL development or reporting is a plus\nAbility to present technical ideas and high-level concepts and solutions to internal and external team members with varying degrees of technical knowledge\nBachelor's Degree in in computer science, information technology, or any science or business discipline\nAbility to take your work seriously but also seriously enjoy a good time with your UDig colleagues\n\nWhat's in It For You? \n\nAt UDig, you will build your career alongside talented and experienced developers while gaining experience and having fun! We provide continuous challenges and professional development opportunities. Additionally, we offer: \n\nCompetitive salary, merit reviews, and career advancement paths\nFlexible, hybrid environment\nIndividual $1500 Training Budget\nRegular team building and social activities (virtual and/or in-person)\nTransparent culture with strong communication and access to leadership\nGreat benefits like\nGenerous Paid Time Off, company holidays, and parental leave\nMultiple Single and Family Health Insurance plans to choose from\nDental & Vision coverage\nShort-Term and Long-Term disability\nOptional accident and critical illness coverage\nMatching 401(k)\nand more!"}, {"title": "Expression of Interest: Data Engineer", "company_name": "Fingerprint for Success (F4S)", "location": "New York, NY (Remote)", "link": "/jobs/view/3787769880/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=l%2FEDpQqUNmCttGdIyTk7Cg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787769880", "description": "About the job\nWe are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.\n\nThe F4S Talent Pool is a pilot project designed to:\n\nHelp job seekers get discovered by our partners based on their anticipated hiring needs.\nProvide optional support and resources for job seekers in their career endeavors.\nHelp individuals understand, and bring out the best in themselves and each other.\n\nThe F4S Talent Pool process:\n\nOnce you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.\n\nAbout Fingerprint For Success (F4S)\n\nBacked by 20+ years of research, F4S\u2019s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high performing teams.\n\nKeep in mind, joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your F4S work style assessment to match the hiring needs of our partners.\n\nYour feedback is a gift! Write to us via:\n\nPowered by JazzHR\n\noWuPJ9lwq9"}, {"title": "Data Engineer Seattle, WA (remote)", "company_name": "Conch Technologies, Inc", "location": "United States (Remote)", "link": "/jobs/view/3765052211/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=SBETI78UMutCHuRp5FnP%2FA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3765052211", "description": "About the job\nHi,\n\nGreetings from Conch Technologies Inc\n\nData Engineering Lead \u2013 With Databricks and Unity Catalog\n\nLocation \u2013 Seattle, WA (remote)\n\nDuration \u2013 12 months\n\nNeed: Design and implement data architecture and strategy, leveraging technologies such as Databricks, Unity Catalog, Privacera, and Collibra.\n\nAs a Data Engineering Lead, you will play a pivotal role in defining and executing technical deliverables while assessing complexities and levels of effort. You'll engage with both vendor and internal teams to resolve technical challenges and ensure seamless collaboration. Your leadership will extend to planning and prioritization with the SCRUM master, facilitating effective coordination between onsite and offshore teams, and driving feature prioritization through collaboration with product managers.\n\nKey Responsibilities\n\n Lead the definition and breakdown of technical deliverables, assessing complexities and levels of effort.\n Collaborate with vendor and internal teams to address technical issues.\n Drive planning and prioritization in conjunction with the SCRUM master.\n Coordinate with onsite and offshore teams for timely project delivery.\n Collaborate with product managers to prioritize features, communicate with stakeholders, and clarify requirements.\n Collect and manage user-reported issues, ensuring prompt resolution and communication.\n Define comprehensive test plans, oversee their execution, and document results.\n\n--\n\nWith Regards,\n\nNagesh G\n\nMobile: 408-381-5645\n\nDesk: 901-313-3066\n\nEmail: nagesh@conchtech.com \n\nWeb: www.conchtech.com"}, {"title": "Remote Work - Need Azure Fabric Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3759325599/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=C3iVjmcR9wjUip1nNjUiCg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3759325599", "description": "About the job\nThe client is placing heavy emphasis on Microsoft and Azure Fabric experience\n\nInformation Management Engineer \u2013 Take the identified information management sources and assist migration to the Client Data Lake, Azure Fabric. The engineer will transform information management (de-dup, clean and enrich).\n\nTake the identified data sources and bring them into our Data Lake, Azure Fabric. Once the data is in, the engineer will need to be able to transform data (de-dup, clean and enrich the data). We are focused on using a medallion data model for data architecture. The tools we are working with are: Delta Parquet files, python, spark, notebooks, T-SQL, Fabric lakehouse/warehouse, Dataflows Gen2, Data Pipelines, Direct Lake, shortcuts, ADF."}, {"title": "Data Engineer", "company_name": "Catalist", "location": "Washington, DC (Remote)", "link": "/jobs/view/3744951279/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ui1Fk6ICmnPNqAlZDZAcwA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3744951279", "description": "About the job\nFor over 17 years, Catalist has been a leader in civic data and data science innovation. Our mission is to provide progressive organizations with the data, software, and services needed to better identify, understand, and communicate with the people they need to engage and mobilize. Our clients include the largest, most influential organizations in the U.S. active in civic engagement, advocacy, and political campaigns.\n\nCatalist is home to a dedicated, creative team of technologists, data scientists, and campaign experts committed to using our talents and technology to nurture a vibrant and growing progressive community.\n\nAs a Data Engineer at Catalist, you will work closely with the Analytics and Technology departments to design, build, support, and maintain various data pipelines and processes with an end goal of providing data and intelligence to the progressive community. This role primarily involves translating data architecture designs into functional processes, code, and systems.\n\nThe ideal candidate will be a highly motivated individual with excellent technical skills, a strong desire to learn new skills, and an interest in progressive politics. Catalist values creativity and problem-solving. Our work is on the cutting edge of data-driven politics, and your support will help Democratic candidates and progressive organizations conduct successful advocacy and electoral campaigns.\n\nThis position reports to the Deputy Chief Data Officer. The Data Engineer is a part of a growing Data team that supports all underlying work at Catalist.\n\nThis position is included in our CWA bargaining unit.\n\nPrinciple Duties & Responsibilities\n\nDevelop scalable, production processes, code, and systems for data ingest, transformation, modeling, and reporting across a variety of platforms\nTranslate mock-ups and designs into functional processes, code, and systems\nCreate architecture designs when needed\nProvide quality assurance and testing on processes, code, systems, and products\nBecome an internal subject matter expert on various datasets and support other Catalist departments/teams on usage of those datasets\nExecute ad-hoc data and database maintenance tasks as requiredProject manage cross-departmental efforts, with direct responsibility for stakeholder engagement, management, and execution of technical elements\n\n\nRequirements\n\nBS or BA in a technical field, or relevant experience\n1-2 years of experience working with SQL databases\nExperience with data cleaning or ETL processes\nExperience with distributed computing systems and/or distributed datastores (particularly the Hadoop ecosystem)\nExperience managing projects\nFamiliarity with Catalist data, progressive politics, voter files, and/or commercial data\nBackground check required\n\n\nPreferred Skills & Abilities\n\nWillingness to be a problem solver and produce results in a fast paced environment\nAbility to be creative and personable, and articulate ideas clearly\nExperience working with SQL databases\nProficiency with Python or another object-oriented programming language (R, Java, Scala, etc\u2026)\nExperience working in cloud environments (AWS, GCP, etc.)\nExperience working in command line environments such as Bash\nExperience with a version control tool such as git or github\n\n\nBenefits\n\nMedical, Dental, Vision, Prescription Drug\n\nCatalist offers Medical, Dental, Vision, and Prescription Drug coverage for eligible staff and their eligible dependents. Catalist\u2019s Medical plan is a comprehensive PPO program including Prescription Drug coverage with 85% of the premium paid by Catalist. Dental and Vision coverage is provided at no cost to employees.\n\nGroup Term Life Insurance and Long-Term & Short-Term Disability Coverage\n\nGroup Term Life Insurance and Long-Term and Short-Term Disability coverage is available for eligible staff. These benefits are provided at no cost to Catalist employees.\n\n401(k) Safe Harbor Plan\n\nA 401(k) Safe Harbor Plan is available to eligible staff with a 3% contribution from Catalist from the date of hire. Employees may contribute pre-tax or post-tax from their salary up to the legal limits set forth by the IRS.\n\nMedical and Dependent Care Flexible Spending Accounts (FSAs)\n\nCatalist offers an FSA Program that gives eligible staff the ability to pay out-of-pocket medical/dental/vision/child care expenses from pre-tax earnings.\n\nTransit Benefits\n\nCatalist also makes available a Transit benefit FSA program to eligible employees using pre-tax contributions with a company match.\n\nProfessional Development and Remote Work Expenses\n\nEligible employees may be reimbursed up to $750 each year for professional development / education and remote work expenses.\n\nStudent Loan PayDown or SaveUp\n\nCatalist offers a Student Loan PayDown and College SaveUp benefit for eligible staff.\n\nVacation, Personal Leave, Sick Leave Benefits\n\nCatalist offers generous vacation benefits to all eligible staff. Eligible employees also receive:\n\n 14 Paid Holidays\n Personal Days\n Sick Leave\n Parental Leave\n\n\nHybrid Office/Remote Work\n\nCertain positions at Catalist are eligible for Office/Remote Hybrid or full Remote status."}, {"title": "Remote Entry Level Data Analyst/Engineer", "company_name": "SynergisticIT", "location": "White Plains, NY (Remote)", "link": "/jobs/view/3774171423/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=q54sk1%2FRww0z%2BC%2BE8e0jQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774171423", "description": "About the job\nAt SynergisticIT, we're all about making connections. Whatever IT goals you have, our software programmers can help achieve those. Our Software development teams can take up turnkey projects and execute them in an effective and efficient manner. If you are looking to source talent our recruiters will find the ideal IT talent for your company. What's the secret to our success? Well, it all starts with taking quality time to listen to each client's specific needs. After we have a thorough grasp of your IT goals, we can better customize our Developments as per your specific needs. We can also tailor make recruiting programs to exceed your expectations. Since our founding in 2010, SynergisticIT's strategies have earned the company an enviable position in the software development, IT staffing and IT skill enhancement fields. SynergisticIT continues to work with hundreds of satisfied American clients with our software programmers working on our projects and after gaining hands on experience on cutting edge technologies moving to contribute their skills to great clients like Apple, Google, Client, Ebay, Paypal, Kroger, the Walt Disney Company and hundreds more. If you are tired of working with inefficient programmers who take a lot of time to ramp up we want you to try us. Our software programmers can hit the ground running and get you the maximum return on your investment. You have already tried the rest its time you tried the best. SynergisticIT - Home of the Best Data Scientists and Software Programmers in the Bay Area.\n\n Why Us ? \n\nSynergisticIT has a proven track record of successfully skill enhancement and staffing IT employees for some of the world's most iconic brands. Our team takes the time to fully understand every client's needs so we could best meet your IT staffing requirements. The knowledgeable staff at SynergisticIT is always more than happy to work with clients to ensure they reach their software development goals. Besides staffing, SynergisticIT is also committed to helping young IT professionals advance their career with a robust upskill program . Everyone who goes through SynergisticIT's program learns all the skills necessary to succeed in many IT fields ranging from Java to Machine Learning. Additionally, everyone trained at SynergisticIT has been through extensive mock and technical interview screenings to bolster their career prospects. Last, but certainly not least, SynergisticIT takes great care to respect the privacy considerations for every client. All companies who work with SynergisticIT can rest assured their confidential data is protected using the most up-to-date encryption technologies. SynergisticIT also complies with all the latest NDA agreements.\n\n REQUIRED SKILLS For Java /Software Programmers \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Time series analysis \n\n We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2022 and at Gartner Data Analytics Summit (Florida)-2023 \n\nOracle CloudWorld Event (OCW) Las Vegas 2022 | SynergisticIT - YouTube\n\n https://www.youtube.com/watch?v=OAFOhcGy9Z8 \n\n https://www.youtube.com/watch?v=EmO7NrWHkLM \n\n https://www.youtube.com/watch?v=NVBU9RYZ6UI \n\n https://www.youtube.com/watch?v=Yy74yvjatVg \n\nSynergisticIT at Gartner Data and Analytics Summit 2023 - YouTube\n\nFor preparing for interviews please visit  https://www.synergisticit.com/interview-questions/ \n\n We are looking for the right matching candidates for our clients \n\n Please apply via the job posting \n\n REQUIRED SKILLS For Java /Full Stack/Software Programmer \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning Positions \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Tensorflow \n\n If you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team. \n\n No phone calls please.  Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates"}, {"title": "Data Engineer", "company_name": "Anika Systems", "location": "Leesburg, VA (Remote)", "link": "/jobs/view/3787766893/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=mApL9JnN%2B%2Fr2%2BYUPlxRYOA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787766893", "description": "About the job\nAnika Systems is a fast growing, woman-owned small business that specializes in providing innovative IT solutions for federal government agencies. Our expertise lies in accelerating delivery in Data and Analytics, Intelligent Automation, Application Development, and IT Modernization. We are currently expanding our Federal team and are seeking a passionate and talented Data Engineer. This opportunity is 100% remote.\n\nMust be a U.S. Citizen with the ability to obtain and maintain a government suitability clearance\n\nResponsibilities\n\nDesigns, develops, builds, analyzes, evaluates, and installs database management systems to include database modeling and design, relational database architecture, metadata, and repository creation and configuration management.\nDefines and oversees database organizations, standards, controls, procedures, and documentation. Provides technical consulting in the definition, design, and creation of a database environment.\nAdvises applications development staff and users on data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nDesigns and implements databases with respect to access methods, access time, batch processes, device allocation, validation checks, organization, protection and security, documentation, and statistical methods.\nUses data mapping, data mining, and data transformational analysis tools to design and develop databases.\nDetermines data storage and optimum storage requirements.\nPrepares system requirements, source analysis, and process analyses and designs throughout the database implementation.\n\nRequired Skills And Experience\n\nBA/BS and 1 year of relevant experience\nExperience with DataBricks, SQL, Python\nApplications development experience with data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nExcellent communication skills (both oral and written) with process-oriented organizational skills to ensure project success\n\nDesired Skills And Experience\n\nDemonstrated successful development experience with the full life cycle of government database management systems including database modeling and design, relational database architecture, metadata and repository creation, and configuration management.\n\nPowered by JazzHR\n\nTlKWIorucJ"}, {"title": "Remote Work - Need Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3774712978/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=r3Bp%2F0qcOD2OpC5jpbUOcA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774712978", "description": "About the job\nTitle : Data Engineer\n\nLocation : Remote\n\nDuration : 6+ months to start (long term contract (work order are extended 2x year))\n\nPlease Fill out the skill matrix: Technology Some Awareness (1) Novice (2) Intermediate (3) Advanced (4) Expert (5) SQL Tableau Desktop /Creator Tableau API Integration AWS - S3 Glue AWS - Athena Redshift AWS - Lambda AWS - SNS/SQS msExcel\n\nProfile: Data Analyst will work closely with Architects and leads in the Interflow team, key business stakeholders including program staff to support the organizations effort in the areas of data Analysis and information management. Role should have strong experience in writing SQL queries to extract data for reports. Candidate should have experience working with large set of data. As these reports are very critical for the company and we are looking for an expert who is capable of handling, consuming and extracting large sets of data for analysis and reports.\n\nMust Haves\n\nAWS - Athena\nAWS - S3\nGlue\nhands-on development of reporting applications\nMS Excel\nRedShift\nSQL\nTableau Desktop /Creator\n\nNice\n\nAWS - Lambda\nAWS - SNS/SQS\nTableau API Integration\n\nSkills\n\nBA/BS required (major in an analytical field desired) \nA minimum 10+ years work-related experience. \nExperience in hands-on development of reporting applications for the Web in a professional environment \nThe Ethos of continuous improvement and interest in learning new things.\nStrong analytical thinking and structured problem-solving ability\nAbility to handle multiple projects and assignments simultaneously and effectively in a cross-functional team environment. \nVersed on the agile methodology and best practices. \nExcellent interpersonal and collaboration skill with the ability to work with a diverse set of colleagues, across functions, from different organizations, disciplines, etc. \nSelf-starter, ability to set priorities, work independently and attain goals"}, {"title": "Data Engineer", "company_name": "Team Remotely Inc", "location": "Houston, TX (Remote)", "link": "/jobs/view/3786249829/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=78lY5Jo%2FfyRPC6TQYCdvww%3D%3D&trk=flagship3_search_srp_jobs", "id": "3786249829", "description": "About the job\nThis is a remote position.\n\n Data Engineer (US/Canada Residents Only, 1 year experience, remote)\n\nTeam Remotely Inc. is a staffing and recruitment agency that offers a comprehensive solution for talent acquisition, including sourcing, vetting, pay rolling, and managing talent. Whether you need contract staffing, direct hire, direct sourcing, talent pools, or diversity initiatives, our model can support your hiring strategy.\n\n Hiring Type: Full-Time\n\n Base Salary: $55K-$70K Per Annum.\n\nResponsibilities\n\nDesigning user interface changes for web-based DB applications. \nReviewing application requirements and interface designs. \nDeveloping and implementing highly responsive user interface components using react concepts. \nWriting application interface codes using JavaScript following react.js workflows. \nTroubleshooting interface software and debugging application codes. \nDeveloping and implementing front-end architecture to support user interface concepts. \nMonitoring and improving front-end performance. \n\nQualifications And Experience\n\nBachelor\u2019s degree in computer science, information technology, or a similar field. \nPrevious experience working as a react.js developer. \nIn-depth knowledge of JavaScript, CSS, HTML, jQuery, and front-end languages. \nKnowledge of REACT tools including React.js, Webpack, Enzyme, Redux, and Flux. \nExperience with user interface design. \nKnowledge of performance testing frameworks, including Mocha and Jest. \nExperience with browser-based debugging and performance testing software. \nTroubleshooting skills. \nProject management skills. \nProblem-solving skills. \nVerbal communication skills. \n\n Why work with Team Remotely?\n\nTeam Remotely Inc. is a staffing platform offering a seamless experience for employers and candidates. Employers can post job openings and specify their requirements, while candidates can create profiles and upload resumes.\n\nThe team of Team Remotely continuously learns and adapts based on previous successful placements, constantly improving its matching capabilities. This ensures that the recommendations provided by Team Remotely are tailored and accurate, increasing the likelihood of a successful match between employers and candidates. By providing intelligent and data-driven solutions, they strive to enhance the efficiency and effectiveness of the hiring process, ultimately helping companies find the best talent and individuals find their dream jobs."}, {"title": "Data Engineer", "company_name": "ZAG Technical Services", "location": "United States (Remote)", "link": "/jobs/view/3771403464/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=Hf0VefrKXBFIL5MtHmY0Pw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3771403464", "description": "About the job\nWe are looking for a tech savvy Data Engineer to contribute to the building, operating, and scaling of next generation data platforms and tools that will power data-driven analytics capabilities throughout the ZAG products and services portfolio, spanning areas such as business intelligence, reporting, and data analytics.\n\nThe ideal team member is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. Must be comfortable supporting the data needs of multiple clients and teams, systems, and products within the modernization of ecosystems. The right-fit person will be excited by the prospect of optimizing or even re-designing data processes to support next generation of advanced analytics capabilities, which in-turn translates to implementing strategic enhancements to maintain competitive advantage in the industry.\n\nThe successful team member will leverage their proficiency to\u2026\n\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\nBuild the analytics processes required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Data Management and cloud based \u2018big data\u2019 technologies.\nBuild analytics tools that utilize technologies to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.\nCollaborate with client teams to identify and solution regional and local requirements within the context of the standard global business model for enterprise analytics.\nParticipates in meetings, discussions, strategy sessions where changes, improvements, and enhancements are proposed, evaluated, and approved. \n\n\nThe exceptional team member will have or demonstrate progressive experiences in\u2026 \n\nBachelor\u2019s degree or higher in Computer Science or related field\nExperience working with BI and data warehousing tools\nExperience working with enterprise data, where security is paramount and data governance is critical\nAbility to work independently and in a team environment, and effectively engage all levels of the organization\nUnderstanding of distributed systems driving large-scale data processing and analytics with a successful history of manipulating, processing, and extracting value from large, disconnected datasets.\nAbility to communicate effectively (listening, presenting, and questioning)\nStrong organizational, written, and communication skills\n\n\nAbout ZAG\n\nZAG Technical Services empowers our clients\u2019 success through intelligent business solutions that increase employee productivity, network security and team collaboration. We aim to ensure that our client\u2019s IT investments drive ROI, provide a competitive advantage and help businesses grow. We do this by living our core set of values: Integrity Foremost, Client-Centered, Accountable Always, Teamwork Throughout, and Exceptional is the Goal.\n\nWhat ZAG Has To Offer\n\nCompetitive compensation package \nBenefits including medical, dental, vision, 401K and life insurance \n\n\nZAG Technical Services is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all of our employees.\n\nZAG Technical Services is an E-Verify participant and will provide the Social Security Administration (SSA) and, if necessary, the Department of Homeland Security (DHS), with information from each new employee\u2019s Form I-9 to confirm work authorization."}, {"title": "Junior Data Engineer", "company_name": "HireMeFast LLC - Career Accelerator - Land A Job", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3786249797/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D&trk=flagship3_search_srp_jobs&lipi=urn%3Ali%3Apage%3Ad_flagship3_search_srp_jobs%3BQExogrHBQoSF9zZm%2B9kufw%3D%3D&lici=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D", "id": "3786249797", "description": "About the job\nThis is a remote position.\n\n Job Title: Junior Data Engineer\n\n Employment Type: Full-Time\n\n Salary: $64,000 - $76,000 per annum\n\n Experience Required:  Minimum 1 year of project experience\n\nAbout us: HireMeFast is a leading staffing and recruitment agency specializing in connecting businesses with top-tier talent across various industries. Our mission is to bridge the gap between exceptional candidates and organizations needing their skills, expertise, and unique qualities. Our team of experienced and dedicated recruitment specialists utilizes innovative sourcing strategies, and a vast network to identify and attract top talent. We conduct comprehensive procedures to ensure that only the most qualified candidates are presented to our clients.\n\nPosition Summary\n\nJoin the fast-paced, innovative, and collaborative environment focused on providing an AIOps platform that enhances the intelligence of the CVS Health infrastructure. Work closely with subject matter experts and colleagues to build and scale out machine learning and AI solutions that will detect, predict, and recommend solutions to correct issues before system impact and enhance efficiency, reliability, and performance of CVS Health\u2019s IT operations.\n\nKey Responsibilities include:\n\nData pipeline development: Designed, implemented, and managed data pipelines for extracting, transforming, and loading data from various sources into data lakes for processing, analytics, and correlation. \nData modeling: Create and maintain data models ensuring data quality, scalability, and efficiency \nDevelop and automate processes to clean, transform, and prepare data for analytics, ensuring data accuracy and consistency \nData Integration: Integrate data from disparate sources, both structured and unstructured to provide a unified view of key infrastructure platform and application data \nUtilize big data technologies such as Kafka to process and analyze large volumes of data efficiently \nImplement data security measures to protect sensitive information and ensure compliance with data and privacy regulation \nCreate/maintain documentation for data processes, data flows, and system configurations \nPerformance Optimization- Monitor and optimize data pipelines and systems for performance, scalability and cost-effectiveness \n\nCharacteristics of this role:\n\nTeam Player: Willing to teach, share knowledge, and work with others to make the team successful. \nCommunication: Exceptional verbal, written, organizational, presentation, and communication skills. \nCreativity: Ability to take written and verbal requirements and come up with other innovative ideas. \nAttention to detail: Systematically and accurately research future solutions and current problems. \nStrong work ethic: The innate drive to do work extremely well. \nPassion: A drive to deliver better products and services than expected to customers. \n\nRequired Qualifications\n\n2+ years of programming experience in languages such as Python, Java, SQL \n2+ years of experience with ETL tools and database management (relational, non-relational) \n2+ years of experience in data modeling techniques and tools to design efficient scalable data structures \nSkills in data quality assessment, data cleansing, and data validation \n\nQualifications\n\nKnowledge of big data technologies and cloud platforms \nExperience with technologies like PySpark, Databricks, and Azure Synapse. \n\nEducation\n\nBachelor\u2019s degree in Computer Science, Information Technology, or related field, or equivalent working experience\n\nWhy HireMeFast LLC?\n\nAt HireMeFast, we understand that finding the right individuals to join your team is crucial for success and growth of your organization. We are committed to streamlining the hiring process for our clients, ensuring they have access to a diverse pool of highly qualified candidates who are a perfect fit for their specific needs."}, {"title": "Data Engineer  - Need candidates from Texas ( REMOTE )", "company_name": "Pulivarthi Group (PG)", "location": "United States (Remote)", "link": "/jobs/view/3770685212/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=9gdMwuif%2BpB%2BCQYvQi3imQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3770685212", "description": "About the job\nFollow us on Linkedin: https://www.linkedin.com/company/pulivarthigroup/\n\nPulivarthi Group LLC is a Global Staffing & IT Technology Solutions company, with our prime focus of providing world class solutions to our customers with the right talent. We combine the expertise of our team and the culture of your company to help you with the solution that is affordable and innovative using high quality standards and technologies.\n\nWe\u2019ve served some of the largest healthcare, financial services, and government entities in the U.S.\n\nData Engineer - FT only - Azure SQL with data lakes experience\n\nData Engineer -\n\nthe resource requirement for this Data Engineer role needed more clarification.\n\nOur customer is requesting a resource who knows Azure CI/CD (Continuous Integration / Continuous Deployment) to build and integrate their DevOps pipeline.\n\nOnce the DevOps pipelines are mapped, they need the Data Engineer to connect to all the Data Sources, primarily their Data Lake.\n\nThe Customer (based in Houston) is interested in a resource in a time zone not too far removed from the Central Time Zone to allow real time collaboration."}, {"title": "Data Engineer", "company_name": "Otter Products", "location": "Fort Collins, CO (Remote)", "link": "/jobs/view/3766728302/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=77U3U7CAi1PxgDmpI3%2BiNQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3766728302", "description": "About the job\nOverview \n\nOtter Products is currently recruiting for a Data Engineer to join our Data team! You can be based in our Fort Collins, CO office with flexibility to work a portion of your time remotely, or 100% Remote in the U.S.\n\nThe Data Engineer applies professional experience, concepts, and company objectives toward building and operationalizing the minimally inclusive data necessary for the enterprise data initiatives following industry standard practices and tools. The bulk of the Data Engineer's work would be in designing, managing and optimizing data pipelines, then moving these data pipelines effectively into production for key data and analytics consumers like business/data analysts, data scientists or other stakeholder that needs curated data for data and analytics use cases across the enterprise. The Data Engineer also ensures compliance with data governance and data security requirements while creating, improving and operationalizing these integrated and reusable data pipelines to enable faster data access, integrated data reuse and vastly improved time-to-solution for data and analytics initiatives. Additionally, the Data Engineer will be expected to collaborate with other data team members and data consumers working on the models and algorithms developed by them toward optimization for data quality, security, and governance to put them into production leading to potentially large productivity gains.\n\n About Otter Products \n\nOtter Products, we grow to give. From our founder's garage in 1998 to the global technology leader we are today, Otter Products continues to drive growth through innovation.\n\nThrough our industry-leading brands - OtterBox, Liviri and OtterCares - we provide our partners the number-one selling and most trusted products in our categories. Our philanthropic spirit is the foundation on which we foster our partner relationships, allowing us to grow and to give - together.\n\nBy way of our charitable arm, the OtterCares Foundation, we support our communities and invest in our future through education that inspires kids to change the world.\n\nAnd even as our global community of Otters continues to grow, our founder's core values are still at the heart of everything we do. We measure our success by our ability to give back to our communities and strengthen opportunities for all.\n\nFor more information visit otterproducts.com\n\n Responsibilities \n\nComplete analysis, design and development of BI solutions using Azure SQL Server\nFamiliarity with Data Warehouse concepts\nExperience coding the extraction, transformation, and loading (ETL) processes\nDatabase development primarily in SSIS, Data Factory and SQL\nPartner with the business to determine end user database/reporting needs and requirements\nGenerate ad hoc reports using Power BI or SSRS\nCollaborate with other developers to create and implement best approach solution(s)\nTroubleshoot Azure tools, systems, and software\nReview queries for performance issues, making changes as needed\nCollaborate with team to performance-tune Azure application as necessary\nAssist with the analysis and extraction of relevant information from large amounts of historical business data to feed data science initiatives\nParticipate and support the design and documentation of processes for large scale data analyses, model development, model validation and model implementation\nSupport and maintain a positive safety culture by following all safety policies and procedures and actively contributing to a safe working environment\nOther duties as required\n\n Qualifications \n\nBachelor's degree required. Degree in Computer Science or Mathematics preferred\nMinimum of four years of experience in an IT or analytical role required. Experience in database development, report writing and/or statistics preferred.\n\n EEO \n\nOtter Products, LLC is an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, age, marital status, pregnancy, sex, sexual orientation, gender, gender identity or expression, national origin, disability, veteran status, or any other characteristic or status protected by law.\n\n For US Based Roles Only - Compensation Range Minimum \n\nUSD $90,000.00/Yr.\n\n For US Based Roles Only - Compensation Range Maximum \n\nUSD $120,000.00/Yr.\n\n Additional Total Rewards \n\nProfit Sharing Program Eligible, Benefits Eligible - Full Time- check out otterproducts.com/careers/why for more info"}, {"title": "Software Engineer, Distributed Systems", "company_name": "Hightouch", "location": "United States (Remote)", "link": "/jobs/view/3628572539/?eBP=CwEAAAGMd4QO8APODyE_-mMOPhYyQGg5ZjVbPwq_3V7DK0B2bLAf10pcedM9S2T71X5i4TiJuEwOzWBbSiTKbLuk-VD-_Y-dN5uyf8YpYkHYqEYFxySMS9zr2hRjNrxX9gXbxssh35TFK9YTNV2WeH2Ncb675JgFbJTgNWSZySFC6rrdJGmdSDTr2q5d3VoxORz3GjsDbzjeyZQCvQ07RqYw7oujWSgslycs2LMCY_-N0NEj6e0PvbaYKiZ_5IW34bGVSYLXpHwqa8oOcNttYZ90UA3E_7sDSFzABdIVlfrp5zzERrIYy5sh2MIInxHdDoKgQw8VmOn5uM4S0tXUl-i7LnN0_W0NfPBmLiVDNuveIOm4MfDNPxfe5QKPk6FUAVpvgbcoJn9yRyE&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=rph%2FT6Q6Vkp6YoggJf9cJQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3628572539", "description": "About the job\nAbout Hightouch\n\nHightouch\u2019s mission is to empower everyone to take action on their data. Through our Reverse ETL platform, business and data users can seamlessly sync data from where it resides, such as warehouses and databases, to where it is needed, including operational systems and SaaS tools. Traditionally, acting on data has required engineering time and bandwidth, and left most business users stuck with charts and reports that are unable to take automated action on their data. With Hightouch, every business user, without writing any code, can activate data to streamline critical processes, improve marketing performance, and scale operations.\n\nOur team operates with a focus on making a meaningful impact for our customers. We believe in approaching challenges with a first principles thinking mindset, moving quickly and embracing our value of efficient execution, and treating each other with compassion and kindness. We look for team members that are strong communicators, have a growth mindset, and are motivated and persistent in achieving our goals.\n\nHundreds of companies use Hightouch, including Spotify, Ramp, Retool, NBA, Plaid, and Betterment. We\u2019re based in San Francisco, are remote-friendly, and backed by leading investors such as Amplify Partners, ICONIQ Growth, Bain Capital Ventures, Y-Combinator, and Afore Capital.\n\nAbout The Role\n\nWe are looking for a distributed systems engineer to work on the systems that power the syncing engine our customers and other engineering teams rely on. This presents an exciting challenge where you can apply your expertise in distributed systems, performance optimization, and troubleshooting to push the boundaries of what is possible and meaningful improve our ability to move massive amounts of customer data. This role also provides a unique opportunity to work on a multi-cloud and multi-region infrastructure that supports a global customer base.\n\nWe believe in enabling our engineers to do their best work for our customers by giving them extremely high levels of ownership and autonomy. This comes in different forms: you will own and deliver projects from start to finish, you will work directly with customers to solve their hardest scaling problems, and you will have a lot of influence over what we work on as a team and company.\n\nSome of the problems we\u2019ll be working on include:\n\nSync Speed: Customers want to sync a lot of data to important destinations like Facebook and Snapchat, which requires us to analyze every part of our syncing process and find where we can optimize to sync data more quickly\nStreaming Syncing: We currently sync data from sources in batch only, but envision a world where we also do syncs in real-time and support streaming sources like webhooks and queues\nScalability and Reliability: As part of our rapid growth, we\u2019re always evaluating current future bottlenecks in scaling and reliability, and architecting for the next order of magnitude of growth\nPersonalization API: Our Personalization API product provides a low latency caching layer on top of data warehouses that enable customers to use their data warehouse for real-time personalization use cases with \nMulti-Region and Multi-Cloud: Supporting our multi-region and multi-cloud backend, including extending it to launch Hightouch on in new regions to support data residency requirements of our global customer base\n\nWe are looking for talented, intellectually curious, and motivated individuals who are interested in tackling the problems above. This is a senior role, but we focus on impact and potential for growth more than years of experience. The salary range for this position is $170,000 - $240,000 USD per year, which is location independent in accordance with our remote-first policy.\n\nAbout You\n\nYou are an engineer with a passion for solving hard technical problems that generate real value for customers. You\u2019re motivated by high ownership and are comfortable in a fast-paced, startup environment.\n\nYou have experience and comfort with distributed systems and high-scale systems or the ability and desire to build this experience quickly. This experience can take different forms:\n\nYou\u2019ve built syncing engines that process and move terabytes of data per day in a scalable way\nYou\u2019ve created event collection or data streaming services that power company critical systems\nYou\u2019ve owned systems that have undergone orders of magnitude of growth and helped them scale accordingly\nYou have relevant skills and are a fast learner who is excited to take on the challenges we face\n\nInterview Process\n\nOur goal with the interview process is to balance speed with giving both parties opportunities to assess whether there is a strong mutual fit. We will ask you questions, but we want you to ask us questions! Our technical interviews focus on how you design systems because we believe this is the best way for us to see how you work and for you to see how we collaborate. We don\u2019t ask you to write code to solve technical brainteasers that don\u2019t appear in your day to day job.\n\nRecruiter screen [30m]: Introductory call with our recruiting team to get to know each other and see if the role could be a good mutual fit.\nSystems design screen [45m]: Designing a data processing feature end-to-end.\nHiring manager interview [30m]: Chat with hiring manager about past experiences and future operating preferences to assess fit on company values and operating principles.\nSystems design interview [90m]: Work with the interviewer to architect a system at a conceptual level. The problem will be at a pretty high level - and have both product and customer requirements as well as technical."}, {"title": "Entry-Level Data Scientist Engineer", "company_name": "Phoenix Recruitment LLC", "location": "Texas City, TX (Remote)", "link": "/jobs/view/3788727625/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=8iwtytHsCu4wMwJ4fj3pxw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3788727625", "description": "About the job\nThis is a remote position.\n\nTitle - Entry-Level Data Scientist Engineer, 1 year of project experience\n\nEmployment Type: Full-time\n\nBase Salary: $60K-$70K\n\nPhoenix Recruitment offers a variety of recruiting services to assist both employers and employees. They specialize in marketing open positions, recruiting, and helping employers to find qualified candidates across various industries. Phoenix Recruitment has expertise in streamlining the hiring process. They can help ensure that the process is efficient, well organized, and compliant with relevant regulations.\n\nSkills and Abilities:\n\nStrong knowledge of R or Python for data analysis and modeling. \nProficiency in statistical programs such as R, SAS, MATLAB, or Python. \nFamiliarity with spreadsheets (VBA) and database applications (Access, Oracle, SQL, or equivalent technology). \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \nAbility to learn new methods quickly and work under deadlines. \nExcellent teamwork and communication skills. \nStrong analytical and problem-solving abilities. \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \n\nPreferred:\n\nKnowledge of actuarial concepts and life, health, and/or annuity products. \nExperience with statistical modeling techniques such as GLM, Decision Trees, Time Series, Regression, etc. \nFamiliarity with Microsoft DeployR. \nExposure to insurance risk analysis. \nBasic experience in computational finance, econometrics, statistics, and math. \nKnowledge of SQL and VBA. \nFamiliarity with R or Python for predictive modeling \n\nWhy Phoenix Recruitment LLC?\n\nPhoenix Recruitment often has an extensive network of employers and candidates. This network allows them to tap into a pool of qualified candidates and connect them with suitable job opportunities. They can also leverage their connections to help employers find the right talent efficiently. Outsourcing the recruitment process to a specialized agency can save your time and resources, avoid delays, reduce administrative burdens, and increase the chances of finding the right fit for your organization."}, {"title": "Data Engineer - Remote Work", "company_name": "BairesDev", "location": "United States (Remote)", "link": "/jobs/view/3783162297/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=OtoCrAAHZFomVup04fy35g%3D%3D&trk=flagship3_search_srp_jobs", "id": "3783162297", "description": "About the job\nWho We are\n\nBairesDev is proud to be the fastest-growing company in America. With people in five continents and world-class clients, we are only as strong as the multicultural teams at the heart of our business. To consistently deliver the highest quality solutions to our clients, we only hire the Top 1% of the best talents and nurture their professional growth on exciting projects.\n\nWe are looking for a Data Engineer to join our Development team and participate in different projects made up of multicultural teams distributed throughout the world.\n\nWhat You Will Do\n\n Interview other Data Scientists and Data engineers to vet their technical abilities.\n Input data from those interviews into the product and help develop a predictive, fair, and enjoyable solution.\n Work together with developers, tech leads, and solution architects to build applications.\n Improve existing structures by adding new functionalities or proposing technological updates so that as a result, the impact of your contribution is significant in the core of each business.\n Improve the user experience on scalable and high availability platforms, contributing to the key differential of each business.\n\nHere\u2019s What We Are Looking For\n\n 5+ years of experience working as a developer.\n 3+ years of experience working as a Data Scientist / Data Engineer.\n Proficient with analysis, troubleshooting, and problem-solving.\n Hands-on experience with managing data loads and data quality.\n Advanced English level.\n\nHow we do make your work (and your life) easier:\n\n 100% remote work.\n Hardware setup for you to work from home.\n Flexible hours - make your schedule.\n Paid parental leave, vacation & holidays.\n Diverse and multicultural work environment.\n An innovative environment with the structure and resources of a leading multinational.\n Excellent compensation \u2014 well above the market average.\n Here you can grow at the speed of your learning curve.\n\nOur people work remotely but with a consistent and robust culture that promotes diversity and teamwork. To continue being the leading software development company in Latin America, we want to ensure that every BairesDev member gets the best growth and professional development opportunities in a diverse, welcoming, and innovative environment.\n\nEvery BairesDev team member brings something unique to our company.\n\nWe want to hear your story. Apply now!"}, {"title": "Data Engineer (Remote)", "company_name": "Cascade Debt", "location": "United States (Remote)", "link": "/jobs/view/3755336571/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=pmUtn1j2zyPr25CNPh2RNw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3755336571", "description": "About the job\nDo you want to work in a small team where you can make a real difference in a company? Would you embrace the challenge of building a high impact platform that is used globally? Then we want to speak with you!\n\nAbout Cascade\n\nCascade is a fintech startup backed by Canadian and US investors that empowers companies to grow by democratizing access to institutional debt. We are building tools that modernize how companies raise and manage debt, lowering the barriers to entry to the $7 trillion specialty finance and alternative credit market for companies around the world. Founded just last year, we are gearing up for our next phase of technical development and are seeking a talented Data Engineer to lead the way.\n\nAbout The Role\n\nAs a Data Engineer you\u2019ll be an early member of a growing team building a pioneering platform in the debt infrastructure space. You will design, build, and launch efficient, scalable, and reliable data pipelines to move and transform data .\n\nWhat You\u2019ll Do\n\n Building databases, data lakes and data ingestion pipelines to integrate customer databases and datasets to our systems \n Ingesting financial datasets from external customers, then u pdating and maintaining accurate and complete data mappings to ensure that our product s are displaying high quality data \n M onitoring and alerting across our data pipelines in order to make sure that our data ingests are reliable and correct \n Translat ing business goals and stakeholder requirements into new data flows and deliver analytical insights ensuring that the data flows create real business value continuously \n\nSkills And Experience\n\n 2 + years experience in a data engineering or data analyst role \n E xperience with relational SQL and NoSQL databases, including PostgreSQL , MySQL, MariaDB, MongoDB, Bigtable, etc. \n E xperience working with ETL technologies , such as Databricks, Fivetran , or dbt \n Proficiency in writing SQL queries and knowledge of analytical data warehouses such as RedShift, BigQuery , and Snowflake \n Some experience with CI/CD automation such as GitHub Actions \n Developing APIs and integrating with 3rd party APIs \n Bonus: experience in fintech / SaaS / credit analysis \n\nPeople Who Thrive At Cascade Are\n\n Self-starters, who take the initiative to tackle challenges in a remote work environment \n Good communicators, who can operate without ego to discuss, learn, grow, and help others do the same \n Passionate about creating great digital experiences for users \n Problem solvers who are not satisfied with the status quo \n\nBenefits\n\nRemote: we are a remote-first company and are very flexible on hours as long as things get done \nHome-office: there\u2019s a $1000 USD home office allowance to set yourself up \nEquity: we expect you to have an owner-mentality, and have the equity plan to match \nBenefits: health, dental, vision, and more \nPerks: we offer free lunches weekly and off-site trips \nJob satisfaction: we offer autonomy, ample opportunities for mastery, and an opportunity to make a difference for companies around the world"}, {"title": "Data Engineer", "company_name": "ISC (Integrated Specialty Coverages, LLC)", "location": "Carlsbad, CA (Remote)", "link": "/jobs/view/3778293528/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ew7%2FfEuXSujs5IGabdqUyw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3778293528", "description": "About the job\nAbout Integrated Specialty Coverages\n\nIntegrated Specialty Coverages LLC (ISC) is a growth stage technology and data-driven insurance company leading innovation in the market.\n\nBacked by one of the leading private equity firms, KKR, and led by a forward-thinking management team, ISC is combining the worlds of insurance and technology to create an Insurtech powerhouse. As a leading online distributor of insurance products for a range of industries and \"Main Street USA\", we are looking for the right people to help us in our mission of achieving exponential growth. We strive to be the number one place to go for brokers and agents to source insurance. To accomplish this, we're building a digitally focused team that deeply understands the intersection between user experience, data, and AI/ML to optimize the way we engage with our customers and partners.\n\nJob Summary\n\nThe Data Engineer role will be the technical liaison between multiple groups including a data science team, the engineering team, product management, and business stakeholders. You do not need any insurance knowledge prior, however, you must quickly dive deep into the insurance world and ask questions to become a subject matter expert. You will be responsible for building a data platform to facilitate the data science team. You must be a self-starter that can build out features such as a data pipeline from scratch. There will be support from both engineering and data science for any buildout.\n\nJob Responsibilities\n\nCreate and maintain optimal data pipeline architecture. \nAssemble large, complex data sets that meet functional / non-functional business requirements. \nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. \nBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using AWS technologies, SQL, and Python. \nWork with stakeholders including the Executive, Product, Data Science, and Engineering teams to assist with data-related technical issues and support their data infrastructure needs. \nWork with data science and analytics teams to strive for greater functionality in our data systems. \n\nMinimum Qualifications:\n\nAdvanced working SQL knowledge and experience working with relational databases, strong query authoring (SQL) as well as working familiarity with a variety of databases (Snowflake, Redshift, MySQL, MSSQL, etc.)\nExperience building and optimizing data pipelines, architecture and data sets. \nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. \nBuild processes supporting data transformation, data structures, metadata, dependency, and workload management. \nA successful history of transforming, processing and extracting value from large disconnected, datasets from a variety of data sources (Flat files, Excel, databases, etc.)\nStrong analytic skills related to working with unstructured datasets. \nStrong project management and organizational skills. \nExperience supporting and working with cross-functional teams in a dynamic environment. \nAbility to mentor/guide/collaborate with other team members. \n\nWe are looking for a candidate with 3-5 years of experience in a Data Engineer role. They should also have experience using the following software/tools:\n\nExperience with relational and MPP databases, including Snowflake and MySQL. \nExperience developing software in an agile environment from the requirements stage to production. \nExperience with version control: git\nExperience with container technologies: Docker\nExperience with data pipeline and workflow management tools: Airflow, Jenkins, AWS Glue, Azkaban, Luigi, etc. \nExperience with AWS cloud services: EC2, ECS, Batch, S3, EMR, RDS, Redshift\nExperience with other cloud services: Snowflake, Airflow\nExperience with object-oriented/object function scripting languages: Python, Java, C++, etc. \nExperience with data modeling and data warehouse design\nExperience with data visualization tools (PowerBI, QuickSight)\n\nThe starting annual pay scale for this position is listed below. Actual starting pay will be based on factors such as skills, qualifications, training, and experience. In addition, the company offers comprehensive benefits including medical, dental and vision insurance, 401(k) plan with match, paid time off, and other benefits.\n\nISC's salary ranges are determined by role and level. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations and could be higher or lower based on a multitude of factors, including job-related skills, experience, and relevant education or training.\n\nNational Pay Range\n\n$100,000\u2014$110,000 USD\n\nBenefits of Working at ISC\n\nCompetitive vacation and flexible working arrangements\nComprehensive and inclusive health benefits\nA variety of professional development and mentorship opportunities\nChoice of technology whether at home or in the office\n\nISC's Ownership Behaviors\n\nDo the Right Thing *Be Relentless. Pursue Excellence. *Personal Responsibility & Own Outcomes\nTry Fast. Learn Fast. Fail Fast. Think Big. *Build Extraordinary Owners & Join Forces *Grit & Determination\n\nApplicants with disabilities may contact ISC HR department via e-mail, and other means to request and arrange accommodation. If you need assistance to accommodate a disability, you may request accommodation at any time. Please contact ISC at HR@ISCMGA.com.\n\nISC believes in creating long-term relationships by being responsive and relevant and by consistently delivering value to our community of customers. Specifically, with our employees, we focus on attracting, developing, and retaining the best talent for our business, challenging our people, demonstrating a \"can-do\" attitude, and fostering a collaborative and mutually supportive environment. ISC is an equal opportunity employer and a participant in the US federal E-Verify program (US).\n\nDiversity creates a healthier atmosphere: All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, gender, gender identity, sexual orientation, marital status, medical condition, genetic information, mental or physical disability, military or veteran status, or any other characteristic protected by local, state, or Federal law."}, {"title": "Data Engineer -Remote", "company_name": "RIT Solutions, Inc.", "location": "Denver, CO (Remote)", "link": "/jobs/view/3768007496/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=1hQH8EDaII1eJ7nQ9BACQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3768007496", "description": "About the job\nDuration: 6+ months CTH\n\nMust have a valid LinkedIn profile\n\nRequired Skills\n\n 3-6 years of experience (no more than 6 years)\n Start up environment experience\n Python, SQL, and DBT"}, {"title": "Big Data Engineer - W2", "company_name": "Megan Soft Inc", "location": "United States (Remote)", "link": "/jobs/view/3674845652/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fCAxh4N%2F4YZJ5aEIASaeeA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3674845652", "description": "About the job\nTitle: Big Data Engineer - W2\n\nLocation: Dearborn, MI (Remote)\n\nDuration: 12+ Months\n\nDirect Client: Ford Motors\n\nMust have skills: Should be able to handle GCP requirements\n\n\"Translates business requirements to build repeatable, sustainable, efficient, coded processes that can be productionized by Software Engineers and readily modified by other Data Engineer Creates POC processes in Dev/QA and works with Software Engineers to productionize and align best coding and process practices that maximize efficiency, speed, stability, system resources and capabilities Awareness of and compliance with data privacy, security, legal and contractual guidelines Incorporates data quality and privacy checks/alerts to minimize bad data being consumed by end users, models and dashboards, and to protect customer data; revises checks for new data issues Validates data products and pipelines are functioning as expected following system or application upgrades, source changes, etc. Works closely with Architects if required to align systems, tools and applications being utilized with business use case and performance requirements Provides data product support and maintenance Establishes, tracks and monitors KPIs related to specific data products and deliverable Participate in iteration cadence activities (Iteration Planning, task estimation, daily scrum, demos, etc.). Will be a part of on-call team to resolve critical production support tickets during business hours and off hours.\n\nSkills Required\n\nExperience with Big Data Technologies (Hadoop , ETL Tool, Spark. Hive etc..) Someone who understands Cloud and has experience working with Cloud Platforms (preferably GCP) Should be able to handle GCP requirements - ingesting into GCS/ Big Query, working with GCP components Hands-on in Data Flow, Cloud Storage (GCS), Big Query and GitHub. Proficient in Python and SQL.\n\nSkills Preferred\n\nManages ETL: uses programming and tools for data ingestion, configures pipelines, applies transformations and decoding, integrates and fuses data, moves and securely deliver Experience NoSQL databases\n\nAdditional Information\n\nIf the candidate is remote only, please indicate \"\"100% Remote\"\" under candidate's name on resume. Other similar schedule notations can be \"\"Local Candidate\"\" or \"\"Hybrid Candidate.\"\" ***HRA Test Required***\""}, {"title": "Remote Opportunity: Quantexa Data Engineer", "company_name": "SPAR Information Systems LLC", "location": "United States (Remote)", "link": "/jobs/view/3680073191/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ioykVCZSOQ7BK5%2FwRQHOWg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3680073191", "description": "About the job\nHello All, Hope you are doing great!! Please go through the job descriptions and let me know your interest. Role: Quantexa Data engineer  Location: Remote Duration: Long Term Contract Job Description: 1. End to End usage of Quantexa including guiding our client team on how to use from both front end and data ingestion Reason they are asking for Certification 2. Resolve Entity issues with data 3. Knowledge of Scala and Hive which requires coding experience using parquet 4. Advanced SQL knowledge requiring writing SQL queries 5. Experience having worked with GCP data lake 6. Quantexa certification required\n\nThanks & Regards,\n\nSatnam Singh\n\nDirect: 201 623 3660\n\nEmail : Satnam.singh@sparinfosys.com"}, {"title": "Remote Work - Need Staff Data Engineer - USC only", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3746283980/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=CvIuLBNO%2BPMGi4IXVRkP0A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3746283980", "description": "About the job\n100% Remote, client is in NY\n\nLook for strong job history, Python, Machine Learning, SQL, 10/10 comms, etc. \n\nMUST be an Engineer \n\nSummary\n\nWe're looking for an experienced Data Engineer to join our client's growing team of top notched IT professionals. You will work closely across multiple groups including Editorial and Audio, Marketing, Advertising, etc. to achieve their goals and objectives. The Data Engineering team builds tools throughout the stack while sharing knowledge across these departments.\n\nAs the company grows, they're looking for Data Engineer who will help solidify and expand our pipelines and maintain their data warehouse. Their business model is based on performing complex and very detailed analyses of how our products perform with their subscribers. You'll be responsible for working alongside analysts, product, marketing, and other teams to define new data collection components and measurement schemes that support each new product and feature.\n\nHow Can Add Value To Their Mission\n\nCreate and maintain data pipelines to provide insights and drive business decisions\nEstablish data warehousing strategy (ex. Kimball, Data Vault, etc.)\nMaintain data infrastructure on our AWS accounts\nCollaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.\nWrite unit/integration tests, contributes to engineering wiki, and documents work\nImplement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it\n\nWhat You'll Need To Succeed\n\n7+ years of industry experience measuring product performance and user behavior\nExperience working with a variety of data management technologies, including RedShift, Kinesis/Kafka, Glue, Spark, Postgres, Airflow, dbt, and others.\nExperience implementing BI reporting tools such as Looker\nExperience interfacing with engineers, product managers and analysts to understand data needs\nKnowledge of a variety of measurement beacons, SDKs, APIs including Google Analytics, Amplitude, Braze, Stripe, email service providers\nA commitment to building secure, resilient, fault-tolerant architectures, with clear documentation and procedures in place for support\nA focus on accuracy and detail as you build, ensuring data pipelines produce clear, predictable results\nUnderstanding of the typical metrics a subscription and advertising-supported business needs to measure success\nExperience with Client techniques as applied to behavioral segmentation or anomaly identification is a definite plus\nFamiliarity using developer tools that increase productivity and facilitate the development of resilient code (eg. Docker, CircleCI, Serverless) is a plus"}, {"title": "ENTRY LEVEL DATA ENGINEER (REMOTE)", "company_name": "SynergisticIT", "location": "Jacksonville, FL (Remote)", "link": "/jobs/view/3767587909/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=om7L3CnowiC4PLIcpZD70A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3767587909", "description": "About the job\nSYNERGISTICIT wants every candidate to know that the Job Market is Challenging and to stand out, you need to have exceptional skills and technologies and that's where we come in to make sure you get the attention which you need\n\nSynergisticIT understands the complex nature of the job market and how difficult it can be to secure a position, especially for fresh graduates. Therefore, we assist and help tech-savvies to convert their passions into professions. We go above and beyond to keep you working in your niche.\n\nAs we focus on long-term success, we provide complete career development solutions. From job search to upskilling portfolio and interview preparation, we can guide you at every step of your career.\n\nSynergisticIT spares no efforts to connect you with a large network of tech giants, including Google, Apple, PayPal, Dell, Cisco, Client, etc. Presently, we are actively looking for ENTRY LEVEL DATA ENGINEER a driven mindset. Get the right opportunity and gain experience in building web-centric solutions on Java.\n\nWho Should Apply : Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or anyone looking to make their career in IT Industry\n\nWe also assist in filing for STEM extension and H1b and Green card filing.\n\nCandidates who are serious about their future in the IT Industry and have set big goals for themselves.\n\nCandidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. We also offer Skill Enhancement Programs if the candidates are missing skills or experience which our clients need with great outcomes\n\nCandidates can benefit from skill enhancement if they fall into the below categories. If they are qualified with enough skills then no need for skill enhancement\n\nCandidates Who Lack Experience\n\nHave had a break in careers\n\nLack Technical Competency\n\ncandidates who want to get employed and make a career in the Tech Industry \n\nPlease Also Check The Below Links\n\nhttps://www.synergisticit.com/candidate-outcomes/\n\nhttps://www.synergisticit.com/java-track/\n\nhttps://www.synergisticit.com/data-science-track/\n\nhttps://www.synergisticit.com/which-is-the-best-option-for-tech-job-seekers-staffing-companies-consulting-companies-bootcamps-or-synergisticit/\n\nhttps://www.synergisticit.com/contact-us/\n\nIf the skills are not a match candidates can opt for Skill enhancement. Or their resume can be sent out to clients to see if responses are achievable\n\nREQUIRED SKILLS For Java/Software Programmers\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Core Java , javascript , C++ or software programming\nSpring boot, Microservices and REST API's experience\nExcellent written and verbal communication skills\n\nFor data Science/Machine learning\n\nRequired Skills\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Statistics, Python, data visualization tools\nExcellent written and verbal communication skills\n\nPreferred skills: NLP, Text mining, Tableau, Time series analysis\n\nTechnical skills are required by clients for selection even if its Junior or entry level position each additional Technical skill helps a candidate's resume to be picked by clients over other job seekers.\n\nClients hire candidates with the right technical skills which they need and reject candidates who lack the required technical skills.\n\nNo third party candidates or c2c candidates\n\nPlease apply to the posting\n\nNo phone calls please. Shortlisted candidates would be reached out"}, {"title": "Data Engineer - Remote | WFH", "company_name": "Get It Recruit - Information Technology", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3782241984/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=BgBf1kSAAX%2BSuw6inJsIVw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3782241984", "description": "About the job\nJoin an innovative team at a leading company in the engineering and manufacturing industry, where we are committed to leveraging data for continuous improvement and operational excellence. This hybrid position allows you to enjoy the flexibility of remote work on Mondays and Fridays.\n\nWhy Join Us\n\nAt our company, we prioritize your well-being and professional growth. We offer a comprehensive benefits package, including excellent health benefits, a 6% 401(k) match, generous paid vacation and sick time, 12 paid holidays, and more.\n\nRole Overview\n\nAs a Data Engineer, you will play a pivotal role in transforming raw data into actionable insights, driving improvements in engineering and manufacturing processes, enhancing product quality, and optimizing operational efficiency. Collaborate with dynamic cross-functional teams to extract, analyze, and visualize data, influencing strategic decisions through your valuable insights.\n\nKey Responsibilities\n\nData Collection and Integration:\n\nGather, clean, and integrate data from diverse sources, including production databases, sensor data, and external datasets.\n\nData Analysis\n\nConduct exploratory data analysis to identify patterns, trends, and anomalies.\n\nPerform statistical analysis and hypothesis testing to derive meaningful insights.\n\nData Visualization\n\nCreate informative and visually appealing dashboards and reports using tools like Tableau, Power BI, or Python libraries.\n\nPredictive Modeling\n\nDevelop predictive models to forecast manufacturing performance, quality, and operational outcomes.\n\nRoot Cause Analysis\n\nInvestigate production issues and quality concerns by analyzing data to identify root causes and recommend corrective actions.\n\nPerformance Monitoring\n\nEstablish and maintain key performance indicators (KPIs) to track manufacturing and engineering metrics.\n\nGenerate regular reports to communicate performance trends to stakeholders.\n\nCross-functional Collaboration\n\nCollaborate with engineering, manufacturing, and quality assurance teams to understand their data needs and provide analytical support.\n\nContinuous Improvement\n\nSuggest process improvements and optimizations based on data analysis.\n\nData Security And Compliance\n\nEnsure data security and compliance with relevant regulations, such as GDPR or industry-specific standards.\n\nInfrastructure And Development\n\nBuild and maintain the infrastructure for collecting, storing, and processing data.\n\nDesign, develop, and manage data pipelines, ETL processes, and data warehouses.\n\nRequired Experience\n\nProven experience as a Sr. Data Analyst and/or Data Engineer in an engineering or manufacturing environment.\n\nProficiency in data analysis tools and programming languages, such as Python, R, SQL, and Excel.\n\nStrong data visualization skills with tools like Tableau or Power BI.\n\nExcellent knowledge of statistical analysis and machine learning techniques.\n\nAbility to communicate complex findings and insights effectively to both technical and non-technical stakeholders.\n\nStrong problem-solving skills and attention to detail.\n\nFamiliarity with manufacturing processes and quality control is a plus.\n\nExperience with enterprise ERPs (SAP S4HANA, Oracle EBS), CRMs (Salesforce, Hubspot), and IoT.\n\nProficient in working with cloud computing platforms including AWS, GCP, and Azure.\n\nExpertise in configuring and optimizing cloud-based data solutions for scalability, reliability, and performance.\n\nBachelor's Degree in Data Science, Statistics, Engineering, or Computer Science Required.\n\nApply Now and be part of a team shaping the future of data-driven innovation in engineering and manufacturing!\n\nEmployment Type: Full-Time"}, {"title": "Entry Level Software Engineer - Data Backend Engineer (Remote - Canada)", "company_name": "Yelp", "location": "United States (Remote)", "link": "/jobs/view/3752881757/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fankeoxifvhpTzOYvowHYQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3752881757", "description": "About the job\nYelp engineering culture is driven by our values: we\u2019re a cooperative team that values individual authenticity and encourages creative solutions to problems. All new engineers deploy working code their first week, and we strive to broaden individual impact with support from managers, mentors, and teams. At the end of the day, we\u2019re all about helping our users, growing as engineers, and having fun in a collaborative environment.\n Interested in exploring data and finding innovative ways to collect it, curate it, and make it easily accessible for others to use? How about doing that at an enormous scale? Yelp\u2019s datasets contain billions of interactions between users and local businesses around the globe. If working with our vast amount of data sounds exciting, come and join us!\n\nThe Marketplace Data Observability team is responsible for managing, governing, and monitoring all of Yelp\u2019s Ads data to protect our revenue stream and also support offline analysis that aids in making long term company decisions. We build tools that help us to better understand our data quality, and we strive to uncover data issues before they impact our consumers.\n\nAs a Data Backend Engineer on this team, you will be responsible for building elegant and scalable data products that serve critical, up-to-date, structured information to support different types of analytics within Yelp. As a core contributor to our growing data modeling and data warehousing engineering efforts, you will help design and own mission-critical data flow pipelines and datastores to enable decisions including effective A/B testing and company investments.\n This opportunity is fully remote and does not require you to be located in any particular area in Canada. We welcome applicants from throughout Canada. We\u2019d love to have you apply, even if you don\u2019t feel you meet every single requirement in this posting. At Yelp, we\u2019re looking for great people, not just those who simply check off all the boxes.\n  Build systems that can effectively store and crunch terabytes of data.\nDesign and develop data models for efficient data storage, retrieval, and reporting.\nCreate and maintain conceptual, logical, and physical data models using industry-standard modeling tools.\nCollaborate with cross-functional teams, including engineers, data analysts, business analysts, and data scientists, to understand data requirements and translate them into effective data models.\nParticipate in data integration efforts, including ETL processes and data migration.\nStay up-to-date with industry best practices, emerging technologies, and trends related to data warehousing.\nSupport on-call rotations as needed to operate the team.\n  Understanding of high performing and scalable data systems.\nExperience in building and orchestrating ETL pipelines.\nExperience with Data Lake or Data Warehouse landscape.\nA hunger for tracking down root causes and fixing them in systematic ways.\nAbility to communicate effectively to technical and non-technical cohorts alike. \nExposure to some of the following technologies: Python, AWS Redshift, AWS Athena / Apache Presto, Big Data technologies (e.g S3, Hadoop, Hive, Spark, Flink, Kafka etc), NoSQL systems like Cassandra, DBT."}, {"title": "Data Engineer (Associate Consultant), June 2024", "company_name": "UDig", "location": "Nashville, TN (Remote)", "link": "/jobs/view/3716039960/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=XUjo40kU55HnDCqJ%2F3FdPg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3716039960", "description": "About the job\nCan UDig It? \n\nUDig designs, builds, and implements technology solutions that deliver on business objectives. People join our team because they are excited to have an impact on the future of our organization, passionate about delivering meaningful results, and energized by solving complex problems. Oh, and they like working with other driven, smart, and authentic people!\n\nUDig is proud to be recognized as a \"Best Places to Work\" in Virginia and Tennessee. We're committed to fostering a transparent environment where leadership is accessible, professional development opportunities are abundant, and you are empowered to achieve your career goals. You'll collaborate, develop deep relationships with teammates, and accelerate your success while having fun!\n\nIf you're graduating in May 2024 and excited by the idea of impacting organizations by building data solutions to achieve business goals, you should consider joining our growing technology team.\n\nHow Things Go Down\n\nAs a new, entry-level teammate, you'll take part in our \"Breaking Ground\" training program designed to help you quickly get up-to-speed on UDig's delivery approach within a project team. The program covers:\n\nUDig delivery process and methodologies such as gathering requirements and delivering projects using agile\nTechnical guidance and instruction on the best practices from Data leaders at UDig\nDelivering executive and technical presentations\nCoaching to prepare you to for success on your first client project at UDig\n\nA typical day might entail:\n\nProject standups and check-ins\nDesign and development of data pipelines, BI reports, and more\nPlanning and participating in project demos\nContributing to internal UDig projects\nCareer and job growth through UDig-provided training\nCreating long-lasting friendships with teammates through fun activities as well as your projects (including social events, winning a prize for guessing the weight of pumpkin, or sharing experience on cool tech side projects)\n\nUDig believes it is important for Associate Consultants going through our Breaking Ground program to reside in one of our major markets (Richmond, VA or Nashville, TN metro area) for at least the first year of employment.\n\nWant to learn more? Hear directly from our teammates by checking out our 'A Day in the Life' blog series: https://www.udig.com/digging-in/breaking-ground-2023/\n\nSound Interesting? Here's the Technical Piece\n\nBelow are a few boxes you should be able to check if\u202fyou're interested: Don't check every box? Go ahead and apply!\n\n1+ year of experience with relational database management systems such as Microsoft SQL Server, MySQL, or Oracle\nUnderstanding of the full lifecycle of design, development, and implementation of data solutions including architecture design, development, testing\nSome experience in ETL development or reporting is a plus\nAbility to present technical ideas and high-level concepts and solutions to internal and external team members with varying degrees of technical knowledge\nBachelor's Degree in in computer science, information technology, or any science or business discipline\nAbility to take your work seriously but also seriously enjoy a good time with your UDig colleagues\n\nWhat's in It For You? \n\nAt UDig, you will build your career alongside talented and experienced developers while gaining experience and having fun! We provide continuous challenges and professional development opportunities. Additionally, we offer: \n\nCompetitive salary, merit reviews, and career advancement paths\nFlexible, hybrid environment\nIndividual $1500 Training Budget\nRegular team building and social activities (virtual and/or in-person)\nTransparent culture with strong communication and access to leadership\nGreat benefits like\nGenerous Paid Time Off, company holidays, and parental leave\nMultiple Single and Family Health Insurance plans to choose from\nDental & Vision coverage\nShort-Term and Long-Term disability\nOptional accident and critical illness coverage\nMatching 401(k)\nand more!"}, {"title": "Expression of Interest: Data Engineer", "company_name": "Fingerprint for Success (F4S)", "location": "New York, NY (Remote)", "link": "/jobs/view/3787769880/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=l%2FEDpQqUNmCttGdIyTk7Cg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787769880", "description": "About the job\nWe are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.\n\nThe F4S Talent Pool is a pilot project designed to:\n\nHelp job seekers get discovered by our partners based on their anticipated hiring needs.\nProvide optional support and resources for job seekers in their career endeavors.\nHelp individuals understand, and bring out the best in themselves and each other.\n\nThe F4S Talent Pool process:\n\nOnce you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.\n\nAbout Fingerprint For Success (F4S)\n\nBacked by 20+ years of research, F4S\u2019s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high performing teams.\n\nKeep in mind, joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your F4S work style assessment to match the hiring needs of our partners.\n\nYour feedback is a gift! Write to us via:\n\nPowered by JazzHR\n\noWuPJ9lwq9"}, {"title": "Data Engineer Seattle, WA (remote)", "company_name": "Conch Technologies, Inc", "location": "United States (Remote)", "link": "/jobs/view/3765052211/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=SBETI78UMutCHuRp5FnP%2FA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3765052211", "description": "About the job\nHi,\n\nGreetings from Conch Technologies Inc\n\nData Engineering Lead \u2013 With Databricks and Unity Catalog\n\nLocation \u2013 Seattle, WA (remote)\n\nDuration \u2013 12 months\n\nNeed: Design and implement data architecture and strategy, leveraging technologies such as Databricks, Unity Catalog, Privacera, and Collibra.\n\nAs a Data Engineering Lead, you will play a pivotal role in defining and executing technical deliverables while assessing complexities and levels of effort. You'll engage with both vendor and internal teams to resolve technical challenges and ensure seamless collaboration. Your leadership will extend to planning and prioritization with the SCRUM master, facilitating effective coordination between onsite and offshore teams, and driving feature prioritization through collaboration with product managers.\n\nKey Responsibilities\n\n Lead the definition and breakdown of technical deliverables, assessing complexities and levels of effort.\n Collaborate with vendor and internal teams to address technical issues.\n Drive planning and prioritization in conjunction with the SCRUM master.\n Coordinate with onsite and offshore teams for timely project delivery.\n Collaborate with product managers to prioritize features, communicate with stakeholders, and clarify requirements.\n Collect and manage user-reported issues, ensuring prompt resolution and communication.\n Define comprehensive test plans, oversee their execution, and document results.\n\n--\n\nWith Regards,\n\nNagesh G\n\nMobile: 408-381-5645\n\nDesk: 901-313-3066\n\nEmail: nagesh@conchtech.com \n\nWeb: www.conchtech.com"}, {"title": "Remote Work - Need Azure Fabric Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3759325599/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=C3iVjmcR9wjUip1nNjUiCg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3759325599", "description": "About the job\nThe client is placing heavy emphasis on Microsoft and Azure Fabric experience\n\nInformation Management Engineer \u2013 Take the identified information management sources and assist migration to the Client Data Lake, Azure Fabric. The engineer will transform information management (de-dup, clean and enrich).\n\nTake the identified data sources and bring them into our Data Lake, Azure Fabric. Once the data is in, the engineer will need to be able to transform data (de-dup, clean and enrich the data). We are focused on using a medallion data model for data architecture. The tools we are working with are: Delta Parquet files, python, spark, notebooks, T-SQL, Fabric lakehouse/warehouse, Dataflows Gen2, Data Pipelines, Direct Lake, shortcuts, ADF."}, {"title": "Data Engineer", "company_name": "Catalist", "location": "Washington, DC (Remote)", "link": "/jobs/view/3744951279/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ui1Fk6ICmnPNqAlZDZAcwA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3744951279", "description": "About the job\nFor over 17 years, Catalist has been a leader in civic data and data science innovation. Our mission is to provide progressive organizations with the data, software, and services needed to better identify, understand, and communicate with the people they need to engage and mobilize. Our clients include the largest, most influential organizations in the U.S. active in civic engagement, advocacy, and political campaigns.\n\nCatalist is home to a dedicated, creative team of technologists, data scientists, and campaign experts committed to using our talents and technology to nurture a vibrant and growing progressive community.\n\nAs a Data Engineer at Catalist, you will work closely with the Analytics and Technology departments to design, build, support, and maintain various data pipelines and processes with an end goal of providing data and intelligence to the progressive community. This role primarily involves translating data architecture designs into functional processes, code, and systems.\n\nThe ideal candidate will be a highly motivated individual with excellent technical skills, a strong desire to learn new skills, and an interest in progressive politics. Catalist values creativity and problem-solving. Our work is on the cutting edge of data-driven politics, and your support will help Democratic candidates and progressive organizations conduct successful advocacy and electoral campaigns.\n\nThis position reports to the Deputy Chief Data Officer. The Data Engineer is a part of a growing Data team that supports all underlying work at Catalist.\n\nThis position is included in our CWA bargaining unit.\n\nPrinciple Duties & Responsibilities\n\nDevelop scalable, production processes, code, and systems for data ingest, transformation, modeling, and reporting across a variety of platforms\nTranslate mock-ups and designs into functional processes, code, and systems\nCreate architecture designs when needed\nProvide quality assurance and testing on processes, code, systems, and products\nBecome an internal subject matter expert on various datasets and support other Catalist departments/teams on usage of those datasets\nExecute ad-hoc data and database maintenance tasks as requiredProject manage cross-departmental efforts, with direct responsibility for stakeholder engagement, management, and execution of technical elements\n\n\nRequirements\n\nBS or BA in a technical field, or relevant experience\n1-2 years of experience working with SQL databases\nExperience with data cleaning or ETL processes\nExperience with distributed computing systems and/or distributed datastores (particularly the Hadoop ecosystem)\nExperience managing projects\nFamiliarity with Catalist data, progressive politics, voter files, and/or commercial data\nBackground check required\n\n\nPreferred Skills & Abilities\n\nWillingness to be a problem solver and produce results in a fast paced environment\nAbility to be creative and personable, and articulate ideas clearly\nExperience working with SQL databases\nProficiency with Python or another object-oriented programming language (R, Java, Scala, etc\u2026)\nExperience working in cloud environments (AWS, GCP, etc.)\nExperience working in command line environments such as Bash\nExperience with a version control tool such as git or github\n\n\nBenefits\n\nMedical, Dental, Vision, Prescription Drug\n\nCatalist offers Medical, Dental, Vision, and Prescription Drug coverage for eligible staff and their eligible dependents. Catalist\u2019s Medical plan is a comprehensive PPO program including Prescription Drug coverage with 85% of the premium paid by Catalist. Dental and Vision coverage is provided at no cost to employees.\n\nGroup Term Life Insurance and Long-Term & Short-Term Disability Coverage\n\nGroup Term Life Insurance and Long-Term and Short-Term Disability coverage is available for eligible staff. These benefits are provided at no cost to Catalist employees.\n\n401(k) Safe Harbor Plan\n\nA 401(k) Safe Harbor Plan is available to eligible staff with a 3% contribution from Catalist from the date of hire. Employees may contribute pre-tax or post-tax from their salary up to the legal limits set forth by the IRS.\n\nMedical and Dependent Care Flexible Spending Accounts (FSAs)\n\nCatalist offers an FSA Program that gives eligible staff the ability to pay out-of-pocket medical/dental/vision/child care expenses from pre-tax earnings.\n\nTransit Benefits\n\nCatalist also makes available a Transit benefit FSA program to eligible employees using pre-tax contributions with a company match.\n\nProfessional Development and Remote Work Expenses\n\nEligible employees may be reimbursed up to $750 each year for professional development / education and remote work expenses.\n\nStudent Loan PayDown or SaveUp\n\nCatalist offers a Student Loan PayDown and College SaveUp benefit for eligible staff.\n\nVacation, Personal Leave, Sick Leave Benefits\n\nCatalist offers generous vacation benefits to all eligible staff. Eligible employees also receive:\n\n 14 Paid Holidays\n Personal Days\n Sick Leave\n Parental Leave\n\n\nHybrid Office/Remote Work\n\nCertain positions at Catalist are eligible for Office/Remote Hybrid or full Remote status."}, {"title": "Remote Entry Level Data Analyst/Engineer", "company_name": "SynergisticIT", "location": "White Plains, NY (Remote)", "link": "/jobs/view/3774171423/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=q54sk1%2FRww0z%2BC%2BE8e0jQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774171423", "description": "About the job\nAt SynergisticIT, we're all about making connections. Whatever IT goals you have, our software programmers can help achieve those. Our Software development teams can take up turnkey projects and execute them in an effective and efficient manner. If you are looking to source talent our recruiters will find the ideal IT talent for your company. What's the secret to our success? Well, it all starts with taking quality time to listen to each client's specific needs. After we have a thorough grasp of your IT goals, we can better customize our Developments as per your specific needs. We can also tailor make recruiting programs to exceed your expectations. Since our founding in 2010, SynergisticIT's strategies have earned the company an enviable position in the software development, IT staffing and IT skill enhancement fields. SynergisticIT continues to work with hundreds of satisfied American clients with our software programmers working on our projects and after gaining hands on experience on cutting edge technologies moving to contribute their skills to great clients like Apple, Google, Client, Ebay, Paypal, Kroger, the Walt Disney Company and hundreds more. If you are tired of working with inefficient programmers who take a lot of time to ramp up we want you to try us. Our software programmers can hit the ground running and get you the maximum return on your investment. You have already tried the rest its time you tried the best. SynergisticIT - Home of the Best Data Scientists and Software Programmers in the Bay Area.\n\n Why Us ? \n\nSynergisticIT has a proven track record of successfully skill enhancement and staffing IT employees for some of the world's most iconic brands. Our team takes the time to fully understand every client's needs so we could best meet your IT staffing requirements. The knowledgeable staff at SynergisticIT is always more than happy to work with clients to ensure they reach their software development goals. Besides staffing, SynergisticIT is also committed to helping young IT professionals advance their career with a robust upskill program . Everyone who goes through SynergisticIT's program learns all the skills necessary to succeed in many IT fields ranging from Java to Machine Learning. Additionally, everyone trained at SynergisticIT has been through extensive mock and technical interview screenings to bolster their career prospects. Last, but certainly not least, SynergisticIT takes great care to respect the privacy considerations for every client. All companies who work with SynergisticIT can rest assured their confidential data is protected using the most up-to-date encryption technologies. SynergisticIT also complies with all the latest NDA agreements.\n\n REQUIRED SKILLS For Java /Software Programmers \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Time series analysis \n\n We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2022 and at Gartner Data Analytics Summit (Florida)-2023 \n\nOracle CloudWorld Event (OCW) Las Vegas 2022 | SynergisticIT - YouTube\n\n https://www.youtube.com/watch?v=OAFOhcGy9Z8 \n\n https://www.youtube.com/watch?v=EmO7NrWHkLM \n\n https://www.youtube.com/watch?v=NVBU9RYZ6UI \n\n https://www.youtube.com/watch?v=Yy74yvjatVg \n\nSynergisticIT at Gartner Data and Analytics Summit 2023 - YouTube\n\nFor preparing for interviews please visit  https://www.synergisticit.com/interview-questions/ \n\n We are looking for the right matching candidates for our clients \n\n Please apply via the job posting \n\n REQUIRED SKILLS For Java /Full Stack/Software Programmer \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning Positions \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Tensorflow \n\n If you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team. \n\n No phone calls please.  Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates"}, {"title": "Data Engineer", "company_name": "Anika Systems", "location": "Leesburg, VA (Remote)", "link": "/jobs/view/3787766893/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=mApL9JnN%2B%2Fr2%2BYUPlxRYOA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787766893", "description": "About the job\nAnika Systems is a fast growing, woman-owned small business that specializes in providing innovative IT solutions for federal government agencies. Our expertise lies in accelerating delivery in Data and Analytics, Intelligent Automation, Application Development, and IT Modernization. We are currently expanding our Federal team and are seeking a passionate and talented Data Engineer. This opportunity is 100% remote.\n\nMust be a U.S. Citizen with the ability to obtain and maintain a government suitability clearance\n\nResponsibilities\n\nDesigns, develops, builds, analyzes, evaluates, and installs database management systems to include database modeling and design, relational database architecture, metadata, and repository creation and configuration management.\nDefines and oversees database organizations, standards, controls, procedures, and documentation. Provides technical consulting in the definition, design, and creation of a database environment.\nAdvises applications development staff and users on data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nDesigns and implements databases with respect to access methods, access time, batch processes, device allocation, validation checks, organization, protection and security, documentation, and statistical methods.\nUses data mapping, data mining, and data transformational analysis tools to design and develop databases.\nDetermines data storage and optimum storage requirements.\nPrepares system requirements, source analysis, and process analyses and designs throughout the database implementation.\n\nRequired Skills And Experience\n\nBA/BS and 1 year of relevant experience\nExperience with DataBricks, SQL, Python\nApplications development experience with data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nExcellent communication skills (both oral and written) with process-oriented organizational skills to ensure project success\n\nDesired Skills And Experience\n\nDemonstrated successful development experience with the full life cycle of government database management systems including database modeling and design, relational database architecture, metadata and repository creation, and configuration management.\n\nPowered by JazzHR\n\nTlKWIorucJ"}, {"title": "Remote Work - Need Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3774712978/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=r3Bp%2F0qcOD2OpC5jpbUOcA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774712978", "description": "About the job\nTitle : Data Engineer\n\nLocation : Remote\n\nDuration : 6+ months to start (long term contract (work order are extended 2x year))\n\nPlease Fill out the skill matrix: Technology Some Awareness (1) Novice (2) Intermediate (3) Advanced (4) Expert (5) SQL Tableau Desktop /Creator Tableau API Integration AWS - S3 Glue AWS - Athena Redshift AWS - Lambda AWS - SNS/SQS msExcel\n\nProfile: Data Analyst will work closely with Architects and leads in the Interflow team, key business stakeholders including program staff to support the organizations effort in the areas of data Analysis and information management. Role should have strong experience in writing SQL queries to extract data for reports. Candidate should have experience working with large set of data. As these reports are very critical for the company and we are looking for an expert who is capable of handling, consuming and extracting large sets of data for analysis and reports.\n\nMust Haves\n\nAWS - Athena\nAWS - S3\nGlue\nhands-on development of reporting applications\nMS Excel\nRedShift\nSQL\nTableau Desktop /Creator\n\nNice\n\nAWS - Lambda\nAWS - SNS/SQS\nTableau API Integration\n\nSkills\n\nBA/BS required (major in an analytical field desired) \nA minimum 10+ years work-related experience. \nExperience in hands-on development of reporting applications for the Web in a professional environment \nThe Ethos of continuous improvement and interest in learning new things.\nStrong analytical thinking and structured problem-solving ability\nAbility to handle multiple projects and assignments simultaneously and effectively in a cross-functional team environment. \nVersed on the agile methodology and best practices. \nExcellent interpersonal and collaboration skill with the ability to work with a diverse set of colleagues, across functions, from different organizations, disciplines, etc. \nSelf-starter, ability to set priorities, work independently and attain goals"}, {"title": "Data Engineer", "company_name": "Team Remotely Inc", "location": "Houston, TX (Remote)", "link": "/jobs/view/3786249829/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=78lY5Jo%2FfyRPC6TQYCdvww%3D%3D&trk=flagship3_search_srp_jobs", "id": "3786249829", "description": "About the job\nThis is a remote position.\n\n Data Engineer (US/Canada Residents Only, 1 year experience, remote)\n\nTeam Remotely Inc. is a staffing and recruitment agency that offers a comprehensive solution for talent acquisition, including sourcing, vetting, pay rolling, and managing talent. Whether you need contract staffing, direct hire, direct sourcing, talent pools, or diversity initiatives, our model can support your hiring strategy.\n\n Hiring Type: Full-Time\n\n Base Salary: $55K-$70K Per Annum.\n\nResponsibilities\n\nDesigning user interface changes for web-based DB applications. \nReviewing application requirements and interface designs. \nDeveloping and implementing highly responsive user interface components using react concepts. \nWriting application interface codes using JavaScript following react.js workflows. \nTroubleshooting interface software and debugging application codes. \nDeveloping and implementing front-end architecture to support user interface concepts. \nMonitoring and improving front-end performance. \n\nQualifications And Experience\n\nBachelor\u2019s degree in computer science, information technology, or a similar field. \nPrevious experience working as a react.js developer. \nIn-depth knowledge of JavaScript, CSS, HTML, jQuery, and front-end languages. \nKnowledge of REACT tools including React.js, Webpack, Enzyme, Redux, and Flux. \nExperience with user interface design. \nKnowledge of performance testing frameworks, including Mocha and Jest. \nExperience with browser-based debugging and performance testing software. \nTroubleshooting skills. \nProject management skills. \nProblem-solving skills. \nVerbal communication skills. \n\n Why work with Team Remotely?\n\nTeam Remotely Inc. is a staffing platform offering a seamless experience for employers and candidates. Employers can post job openings and specify their requirements, while candidates can create profiles and upload resumes.\n\nThe team of Team Remotely continuously learns and adapts based on previous successful placements, constantly improving its matching capabilities. This ensures that the recommendations provided by Team Remotely are tailored and accurate, increasing the likelihood of a successful match between employers and candidates. By providing intelligent and data-driven solutions, they strive to enhance the efficiency and effectiveness of the hiring process, ultimately helping companies find the best talent and individuals find their dream jobs."}, {"title": "Data Engineer", "company_name": "ZAG Technical Services", "location": "United States (Remote)", "link": "/jobs/view/3771403464/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=Hf0VefrKXBFIL5MtHmY0Pw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3771403464", "description": "About the job\nWe are looking for a tech savvy Data Engineer to contribute to the building, operating, and scaling of next generation data platforms and tools that will power data-driven analytics capabilities throughout the ZAG products and services portfolio, spanning areas such as business intelligence, reporting, and data analytics.\n\nThe ideal team member is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. Must be comfortable supporting the data needs of multiple clients and teams, systems, and products within the modernization of ecosystems. The right-fit person will be excited by the prospect of optimizing or even re-designing data processes to support next generation of advanced analytics capabilities, which in-turn translates to implementing strategic enhancements to maintain competitive advantage in the industry.\n\nThe successful team member will leverage their proficiency to\u2026\n\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\nBuild the analytics processes required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Data Management and cloud based \u2018big data\u2019 technologies.\nBuild analytics tools that utilize technologies to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.\nCollaborate with client teams to identify and solution regional and local requirements within the context of the standard global business model for enterprise analytics.\nParticipates in meetings, discussions, strategy sessions where changes, improvements, and enhancements are proposed, evaluated, and approved. \n\n\nThe exceptional team member will have or demonstrate progressive experiences in\u2026 \n\nBachelor\u2019s degree or higher in Computer Science or related field\nExperience working with BI and data warehousing tools\nExperience working with enterprise data, where security is paramount and data governance is critical\nAbility to work independently and in a team environment, and effectively engage all levels of the organization\nUnderstanding of distributed systems driving large-scale data processing and analytics with a successful history of manipulating, processing, and extracting value from large, disconnected datasets.\nAbility to communicate effectively (listening, presenting, and questioning)\nStrong organizational, written, and communication skills\n\n\nAbout ZAG\n\nZAG Technical Services empowers our clients\u2019 success through intelligent business solutions that increase employee productivity, network security and team collaboration. We aim to ensure that our client\u2019s IT investments drive ROI, provide a competitive advantage and help businesses grow. We do this by living our core set of values: Integrity Foremost, Client-Centered, Accountable Always, Teamwork Throughout, and Exceptional is the Goal.\n\nWhat ZAG Has To Offer\n\nCompetitive compensation package \nBenefits including medical, dental, vision, 401K and life insurance \n\n\nZAG Technical Services is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all of our employees.\n\nZAG Technical Services is an E-Verify participant and will provide the Social Security Administration (SSA) and, if necessary, the Department of Homeland Security (DHS), with information from each new employee\u2019s Form I-9 to confirm work authorization."}, {"title": "Junior Data Engineer", "company_name": "HireMeFast LLC - Career Accelerator - Land A Job", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3786249797/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D&trk=flagship3_search_srp_jobs&lipi=urn%3Ali%3Apage%3Ad_flagship3_search_srp_jobs%3BQExogrHBQoSF9zZm%2B9kufw%3D%3D&lici=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D", "id": "3786249797", "description": "About the job\nThis is a remote position.\n\n Job Title: Junior Data Engineer\n\n Employment Type: Full-Time\n\n Salary: $64,000 - $76,000 per annum\n\n Experience Required:  Minimum 1 year of project experience\n\nAbout us: HireMeFast is a leading staffing and recruitment agency specializing in connecting businesses with top-tier talent across various industries. Our mission is to bridge the gap between exceptional candidates and organizations needing their skills, expertise, and unique qualities. Our team of experienced and dedicated recruitment specialists utilizes innovative sourcing strategies, and a vast network to identify and attract top talent. We conduct comprehensive procedures to ensure that only the most qualified candidates are presented to our clients.\n\nPosition Summary\n\nJoin the fast-paced, innovative, and collaborative environment focused on providing an AIOps platform that enhances the intelligence of the CVS Health infrastructure. Work closely with subject matter experts and colleagues to build and scale out machine learning and AI solutions that will detect, predict, and recommend solutions to correct issues before system impact and enhance efficiency, reliability, and performance of CVS Health\u2019s IT operations.\n\nKey Responsibilities include:\n\nData pipeline development: Designed, implemented, and managed data pipelines for extracting, transforming, and loading data from various sources into data lakes for processing, analytics, and correlation. \nData modeling: Create and maintain data models ensuring data quality, scalability, and efficiency \nDevelop and automate processes to clean, transform, and prepare data for analytics, ensuring data accuracy and consistency \nData Integration: Integrate data from disparate sources, both structured and unstructured to provide a unified view of key infrastructure platform and application data \nUtilize big data technologies such as Kafka to process and analyze large volumes of data efficiently \nImplement data security measures to protect sensitive information and ensure compliance with data and privacy regulation \nCreate/maintain documentation for data processes, data flows, and system configurations \nPerformance Optimization- Monitor and optimize data pipelines and systems for performance, scalability and cost-effectiveness \n\nCharacteristics of this role:\n\nTeam Player: Willing to teach, share knowledge, and work with others to make the team successful. \nCommunication: Exceptional verbal, written, organizational, presentation, and communication skills. \nCreativity: Ability to take written and verbal requirements and come up with other innovative ideas. \nAttention to detail: Systematically and accurately research future solutions and current problems. \nStrong work ethic: The innate drive to do work extremely well. \nPassion: A drive to deliver better products and services than expected to customers. \n\nRequired Qualifications\n\n2+ years of programming experience in languages such as Python, Java, SQL \n2+ years of experience with ETL tools and database management (relational, non-relational) \n2+ years of experience in data modeling techniques and tools to design efficient scalable data structures \nSkills in data quality assessment, data cleansing, and data validation \n\nQualifications\n\nKnowledge of big data technologies and cloud platforms \nExperience with technologies like PySpark, Databricks, and Azure Synapse. \n\nEducation\n\nBachelor\u2019s degree in Computer Science, Information Technology, or related field, or equivalent working experience\n\nWhy HireMeFast LLC?\n\nAt HireMeFast, we understand that finding the right individuals to join your team is crucial for success and growth of your organization. We are committed to streamlining the hiring process for our clients, ensuring they have access to a diverse pool of highly qualified candidates who are a perfect fit for their specific needs."}, {"title": "Data Engineer  - Need candidates from Texas ( REMOTE )", "company_name": "Pulivarthi Group (PG)", "location": "United States (Remote)", "link": "/jobs/view/3770685212/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=9gdMwuif%2BpB%2BCQYvQi3imQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3770685212", "description": "About the job\nFollow us on Linkedin: https://www.linkedin.com/company/pulivarthigroup/\n\nPulivarthi Group LLC is a Global Staffing & IT Technology Solutions company, with our prime focus of providing world class solutions to our customers with the right talent. We combine the expertise of our team and the culture of your company to help you with the solution that is affordable and innovative using high quality standards and technologies.\n\nWe\u2019ve served some of the largest healthcare, financial services, and government entities in the U.S.\n\nData Engineer - FT only - Azure SQL with data lakes experience\n\nData Engineer -\n\nthe resource requirement for this Data Engineer role needed more clarification.\n\nOur customer is requesting a resource who knows Azure CI/CD (Continuous Integration / Continuous Deployment) to build and integrate their DevOps pipeline.\n\nOnce the DevOps pipelines are mapped, they need the Data Engineer to connect to all the Data Sources, primarily their Data Lake.\n\nThe Customer (based in Houston) is interested in a resource in a time zone not too far removed from the Central Time Zone to allow real time collaboration."}, {"title": "Data Engineer", "company_name": "Otter Products", "location": "Fort Collins, CO (Remote)", "link": "/jobs/view/3766728302/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=77U3U7CAi1PxgDmpI3%2BiNQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3766728302", "description": "About the job\nOverview \n\nOtter Products is currently recruiting for a Data Engineer to join our Data team! You can be based in our Fort Collins, CO office with flexibility to work a portion of your time remotely, or 100% Remote in the U.S.\n\nThe Data Engineer applies professional experience, concepts, and company objectives toward building and operationalizing the minimally inclusive data necessary for the enterprise data initiatives following industry standard practices and tools. The bulk of the Data Engineer's work would be in designing, managing and optimizing data pipelines, then moving these data pipelines effectively into production for key data and analytics consumers like business/data analysts, data scientists or other stakeholder that needs curated data for data and analytics use cases across the enterprise. The Data Engineer also ensures compliance with data governance and data security requirements while creating, improving and operationalizing these integrated and reusable data pipelines to enable faster data access, integrated data reuse and vastly improved time-to-solution for data and analytics initiatives. Additionally, the Data Engineer will be expected to collaborate with other data team members and data consumers working on the models and algorithms developed by them toward optimization for data quality, security, and governance to put them into production leading to potentially large productivity gains.\n\n About Otter Products \n\nOtter Products, we grow to give. From our founder's garage in 1998 to the global technology leader we are today, Otter Products continues to drive growth through innovation.\n\nThrough our industry-leading brands - OtterBox, Liviri and OtterCares - we provide our partners the number-one selling and most trusted products in our categories. Our philanthropic spirit is the foundation on which we foster our partner relationships, allowing us to grow and to give - together.\n\nBy way of our charitable arm, the OtterCares Foundation, we support our communities and invest in our future through education that inspires kids to change the world.\n\nAnd even as our global community of Otters continues to grow, our founder's core values are still at the heart of everything we do. We measure our success by our ability to give back to our communities and strengthen opportunities for all.\n\nFor more information visit otterproducts.com\n\n Responsibilities \n\nComplete analysis, design and development of BI solutions using Azure SQL Server\nFamiliarity with Data Warehouse concepts\nExperience coding the extraction, transformation, and loading (ETL) processes\nDatabase development primarily in SSIS, Data Factory and SQL\nPartner with the business to determine end user database/reporting needs and requirements\nGenerate ad hoc reports using Power BI or SSRS\nCollaborate with other developers to create and implement best approach solution(s)\nTroubleshoot Azure tools, systems, and software\nReview queries for performance issues, making changes as needed\nCollaborate with team to performance-tune Azure application as necessary\nAssist with the analysis and extraction of relevant information from large amounts of historical business data to feed data science initiatives\nParticipate and support the design and documentation of processes for large scale data analyses, model development, model validation and model implementation\nSupport and maintain a positive safety culture by following all safety policies and procedures and actively contributing to a safe working environment\nOther duties as required\n\n Qualifications \n\nBachelor's degree required. Degree in Computer Science or Mathematics preferred\nMinimum of four years of experience in an IT or analytical role required. Experience in database development, report writing and/or statistics preferred.\n\n EEO \n\nOtter Products, LLC is an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, age, marital status, pregnancy, sex, sexual orientation, gender, gender identity or expression, national origin, disability, veteran status, or any other characteristic or status protected by law.\n\n For US Based Roles Only - Compensation Range Minimum \n\nUSD $90,000.00/Yr.\n\n For US Based Roles Only - Compensation Range Maximum \n\nUSD $120,000.00/Yr.\n\n Additional Total Rewards \n\nProfit Sharing Program Eligible, Benefits Eligible - Full Time- check out otterproducts.com/careers/why for more info"}, {"title": "Software Engineer, Distributed Systems", "company_name": "Hightouch", "location": "United States (Remote)", "link": "/jobs/view/3628572539/?eBP=CwEAAAGMd4QO8APODyE_-mMOPhYyQGg5ZjVbPwq_3V7DK0B2bLAf10pcedM9S2T71X5i4TiJuEwOzWBbSiTKbLuk-VD-_Y-dN5uyf8YpYkHYqEYFxySMS9zr2hRjNrxX9gXbxssh35TFK9YTNV2WeH2Ncb675JgFbJTgNWSZySFC6rrdJGmdSDTr2q5d3VoxORz3GjsDbzjeyZQCvQ07RqYw7oujWSgslycs2LMCY_-N0NEj6e0PvbaYKiZ_5IW34bGVSYLXpHwqa8oOcNttYZ90UA3E_7sDSFzABdIVlfrp5zzERrIYy5sh2MIInxHdDoKgQw8VmOn5uM4S0tXUl-i7LnN0_W0NfPBmLiVDNuveIOm4MfDNPxfe5QKPk6FUAVpvgbcoJn9yRyE&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=rph%2FT6Q6Vkp6YoggJf9cJQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3628572539", "description": "About the job\nAbout Hightouch\n\nHightouch\u2019s mission is to empower everyone to take action on their data. Through our Reverse ETL platform, business and data users can seamlessly sync data from where it resides, such as warehouses and databases, to where it is needed, including operational systems and SaaS tools. Traditionally, acting on data has required engineering time and bandwidth, and left most business users stuck with charts and reports that are unable to take automated action on their data. With Hightouch, every business user, without writing any code, can activate data to streamline critical processes, improve marketing performance, and scale operations.\n\nOur team operates with a focus on making a meaningful impact for our customers. We believe in approaching challenges with a first principles thinking mindset, moving quickly and embracing our value of efficient execution, and treating each other with compassion and kindness. We look for team members that are strong communicators, have a growth mindset, and are motivated and persistent in achieving our goals.\n\nHundreds of companies use Hightouch, including Spotify, Ramp, Retool, NBA, Plaid, and Betterment. We\u2019re based in San Francisco, are remote-friendly, and backed by leading investors such as Amplify Partners, ICONIQ Growth, Bain Capital Ventures, Y-Combinator, and Afore Capital.\n\nAbout The Role\n\nWe are looking for a distributed systems engineer to work on the systems that power the syncing engine our customers and other engineering teams rely on. This presents an exciting challenge where you can apply your expertise in distributed systems, performance optimization, and troubleshooting to push the boundaries of what is possible and meaningful improve our ability to move massive amounts of customer data. This role also provides a unique opportunity to work on a multi-cloud and multi-region infrastructure that supports a global customer base.\n\nWe believe in enabling our engineers to do their best work for our customers by giving them extremely high levels of ownership and autonomy. This comes in different forms: you will own and deliver projects from start to finish, you will work directly with customers to solve their hardest scaling problems, and you will have a lot of influence over what we work on as a team and company.\n\nSome of the problems we\u2019ll be working on include:\n\nSync Speed: Customers want to sync a lot of data to important destinations like Facebook and Snapchat, which requires us to analyze every part of our syncing process and find where we can optimize to sync data more quickly\nStreaming Syncing: We currently sync data from sources in batch only, but envision a world where we also do syncs in real-time and support streaming sources like webhooks and queues\nScalability and Reliability: As part of our rapid growth, we\u2019re always evaluating current future bottlenecks in scaling and reliability, and architecting for the next order of magnitude of growth\nPersonalization API: Our Personalization API product provides a low latency caching layer on top of data warehouses that enable customers to use their data warehouse for real-time personalization use cases with \nMulti-Region and Multi-Cloud: Supporting our multi-region and multi-cloud backend, including extending it to launch Hightouch on in new regions to support data residency requirements of our global customer base\n\nWe are looking for talented, intellectually curious, and motivated individuals who are interested in tackling the problems above. This is a senior role, but we focus on impact and potential for growth more than years of experience. The salary range for this position is $170,000 - $240,000 USD per year, which is location independent in accordance with our remote-first policy.\n\nAbout You\n\nYou are an engineer with a passion for solving hard technical problems that generate real value for customers. You\u2019re motivated by high ownership and are comfortable in a fast-paced, startup environment.\n\nYou have experience and comfort with distributed systems and high-scale systems or the ability and desire to build this experience quickly. This experience can take different forms:\n\nYou\u2019ve built syncing engines that process and move terabytes of data per day in a scalable way\nYou\u2019ve created event collection or data streaming services that power company critical systems\nYou\u2019ve owned systems that have undergone orders of magnitude of growth and helped them scale accordingly\nYou have relevant skills and are a fast learner who is excited to take on the challenges we face\n\nInterview Process\n\nOur goal with the interview process is to balance speed with giving both parties opportunities to assess whether there is a strong mutual fit. We will ask you questions, but we want you to ask us questions! Our technical interviews focus on how you design systems because we believe this is the best way for us to see how you work and for you to see how we collaborate. We don\u2019t ask you to write code to solve technical brainteasers that don\u2019t appear in your day to day job.\n\nRecruiter screen [30m]: Introductory call with our recruiting team to get to know each other and see if the role could be a good mutual fit.\nSystems design screen [45m]: Designing a data processing feature end-to-end.\nHiring manager interview [30m]: Chat with hiring manager about past experiences and future operating preferences to assess fit on company values and operating principles.\nSystems design interview [90m]: Work with the interviewer to architect a system at a conceptual level. The problem will be at a pretty high level - and have both product and customer requirements as well as technical."}, {"title": "Entry-Level Data Scientist Engineer", "company_name": "Phoenix Recruitment LLC", "location": "Texas City, TX (Remote)", "link": "/jobs/view/3788727625/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=8iwtytHsCu4wMwJ4fj3pxw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3788727625", "description": "About the job\nThis is a remote position.\n\nTitle - Entry-Level Data Scientist Engineer, 1 year of project experience\n\nEmployment Type: Full-time\n\nBase Salary: $60K-$70K\n\nPhoenix Recruitment offers a variety of recruiting services to assist both employers and employees. They specialize in marketing open positions, recruiting, and helping employers to find qualified candidates across various industries. Phoenix Recruitment has expertise in streamlining the hiring process. They can help ensure that the process is efficient, well organized, and compliant with relevant regulations.\n\nSkills and Abilities:\n\nStrong knowledge of R or Python for data analysis and modeling. \nProficiency in statistical programs such as R, SAS, MATLAB, or Python. \nFamiliarity with spreadsheets (VBA) and database applications (Access, Oracle, SQL, or equivalent technology). \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \nAbility to learn new methods quickly and work under deadlines. \nExcellent teamwork and communication skills. \nStrong analytical and problem-solving abilities. \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \n\nPreferred:\n\nKnowledge of actuarial concepts and life, health, and/or annuity products. \nExperience with statistical modeling techniques such as GLM, Decision Trees, Time Series, Regression, etc. \nFamiliarity with Microsoft DeployR. \nExposure to insurance risk analysis. \nBasic experience in computational finance, econometrics, statistics, and math. \nKnowledge of SQL and VBA. \nFamiliarity with R or Python for predictive modeling \n\nWhy Phoenix Recruitment LLC?\n\nPhoenix Recruitment often has an extensive network of employers and candidates. This network allows them to tap into a pool of qualified candidates and connect them with suitable job opportunities. They can also leverage their connections to help employers find the right talent efficiently. Outsourcing the recruitment process to a specialized agency can save your time and resources, avoid delays, reduce administrative burdens, and increase the chances of finding the right fit for your organization."}, {"title": "Data Engineer - Remote Work", "company_name": "BairesDev", "location": "United States (Remote)", "link": "/jobs/view/3783162297/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=OtoCrAAHZFomVup04fy35g%3D%3D&trk=flagship3_search_srp_jobs", "id": "3783162297", "description": "About the job\nWho We are\n\nBairesDev is proud to be the fastest-growing company in America. With people in five continents and world-class clients, we are only as strong as the multicultural teams at the heart of our business. To consistently deliver the highest quality solutions to our clients, we only hire the Top 1% of the best talents and nurture their professional growth on exciting projects.\n\nWe are looking for a Data Engineer to join our Development team and participate in different projects made up of multicultural teams distributed throughout the world.\n\nWhat You Will Do\n\n Interview other Data Scientists and Data engineers to vet their technical abilities.\n Input data from those interviews into the product and help develop a predictive, fair, and enjoyable solution.\n Work together with developers, tech leads, and solution architects to build applications.\n Improve existing structures by adding new functionalities or proposing technological updates so that as a result, the impact of your contribution is significant in the core of each business.\n Improve the user experience on scalable and high availability platforms, contributing to the key differential of each business.\n\nHere\u2019s What We Are Looking For\n\n 5+ years of experience working as a developer.\n 3+ years of experience working as a Data Scientist / Data Engineer.\n Proficient with analysis, troubleshooting, and problem-solving.\n Hands-on experience with managing data loads and data quality.\n Advanced English level.\n\nHow we do make your work (and your life) easier:\n\n 100% remote work.\n Hardware setup for you to work from home.\n Flexible hours - make your schedule.\n Paid parental leave, vacation & holidays.\n Diverse and multicultural work environment.\n An innovative environment with the structure and resources of a leading multinational.\n Excellent compensation \u2014 well above the market average.\n Here you can grow at the speed of your learning curve.\n\nOur people work remotely but with a consistent and robust culture that promotes diversity and teamwork. To continue being the leading software development company in Latin America, we want to ensure that every BairesDev member gets the best growth and professional development opportunities in a diverse, welcoming, and innovative environment.\n\nEvery BairesDev team member brings something unique to our company.\n\nWe want to hear your story. Apply now!"}, {"title": "Data Engineer (Remote)", "company_name": "Cascade Debt", "location": "United States (Remote)", "link": "/jobs/view/3755336571/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=pmUtn1j2zyPr25CNPh2RNw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3755336571", "description": "About the job\nDo you want to work in a small team where you can make a real difference in a company? Would you embrace the challenge of building a high impact platform that is used globally? Then we want to speak with you!\n\nAbout Cascade\n\nCascade is a fintech startup backed by Canadian and US investors that empowers companies to grow by democratizing access to institutional debt. We are building tools that modernize how companies raise and manage debt, lowering the barriers to entry to the $7 trillion specialty finance and alternative credit market for companies around the world. Founded just last year, we are gearing up for our next phase of technical development and are seeking a talented Data Engineer to lead the way.\n\nAbout The Role\n\nAs a Data Engineer you\u2019ll be an early member of a growing team building a pioneering platform in the debt infrastructure space. You will design, build, and launch efficient, scalable, and reliable data pipelines to move and transform data .\n\nWhat You\u2019ll Do\n\n Building databases, data lakes and data ingestion pipelines to integrate customer databases and datasets to our systems \n Ingesting financial datasets from external customers, then u pdating and maintaining accurate and complete data mappings to ensure that our product s are displaying high quality data \n M onitoring and alerting across our data pipelines in order to make sure that our data ingests are reliable and correct \n Translat ing business goals and stakeholder requirements into new data flows and deliver analytical insights ensuring that the data flows create real business value continuously \n\nSkills And Experience\n\n 2 + years experience in a data engineering or data analyst role \n E xperience with relational SQL and NoSQL databases, including PostgreSQL , MySQL, MariaDB, MongoDB, Bigtable, etc. \n E xperience working with ETL technologies , such as Databricks, Fivetran , or dbt \n Proficiency in writing SQL queries and knowledge of analytical data warehouses such as RedShift, BigQuery , and Snowflake \n Some experience with CI/CD automation such as GitHub Actions \n Developing APIs and integrating with 3rd party APIs \n Bonus: experience in fintech / SaaS / credit analysis \n\nPeople Who Thrive At Cascade Are\n\n Self-starters, who take the initiative to tackle challenges in a remote work environment \n Good communicators, who can operate without ego to discuss, learn, grow, and help others do the same \n Passionate about creating great digital experiences for users \n Problem solvers who are not satisfied with the status quo \n\nBenefits\n\nRemote: we are a remote-first company and are very flexible on hours as long as things get done \nHome-office: there\u2019s a $1000 USD home office allowance to set yourself up \nEquity: we expect you to have an owner-mentality, and have the equity plan to match \nBenefits: health, dental, vision, and more \nPerks: we offer free lunches weekly and off-site trips \nJob satisfaction: we offer autonomy, ample opportunities for mastery, and an opportunity to make a difference for companies around the world"}, {"title": "Data Engineer", "company_name": "ISC (Integrated Specialty Coverages, LLC)", "location": "Carlsbad, CA (Remote)", "link": "/jobs/view/3778293528/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ew7%2FfEuXSujs5IGabdqUyw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3778293528", "description": "About the job\nAbout Integrated Specialty Coverages\n\nIntegrated Specialty Coverages LLC (ISC) is a growth stage technology and data-driven insurance company leading innovation in the market.\n\nBacked by one of the leading private equity firms, KKR, and led by a forward-thinking management team, ISC is combining the worlds of insurance and technology to create an Insurtech powerhouse. As a leading online distributor of insurance products for a range of industries and \"Main Street USA\", we are looking for the right people to help us in our mission of achieving exponential growth. We strive to be the number one place to go for brokers and agents to source insurance. To accomplish this, we're building a digitally focused team that deeply understands the intersection between user experience, data, and AI/ML to optimize the way we engage with our customers and partners.\n\nJob Summary\n\nThe Data Engineer role will be the technical liaison between multiple groups including a data science team, the engineering team, product management, and business stakeholders. You do not need any insurance knowledge prior, however, you must quickly dive deep into the insurance world and ask questions to become a subject matter expert. You will be responsible for building a data platform to facilitate the data science team. You must be a self-starter that can build out features such as a data pipeline from scratch. There will be support from both engineering and data science for any buildout.\n\nJob Responsibilities\n\nCreate and maintain optimal data pipeline architecture. \nAssemble large, complex data sets that meet functional / non-functional business requirements. \nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. \nBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using AWS technologies, SQL, and Python. \nWork with stakeholders including the Executive, Product, Data Science, and Engineering teams to assist with data-related technical issues and support their data infrastructure needs. \nWork with data science and analytics teams to strive for greater functionality in our data systems. \n\nMinimum Qualifications:\n\nAdvanced working SQL knowledge and experience working with relational databases, strong query authoring (SQL) as well as working familiarity with a variety of databases (Snowflake, Redshift, MySQL, MSSQL, etc.)\nExperience building and optimizing data pipelines, architecture and data sets. \nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. \nBuild processes supporting data transformation, data structures, metadata, dependency, and workload management. \nA successful history of transforming, processing and extracting value from large disconnected, datasets from a variety of data sources (Flat files, Excel, databases, etc.)\nStrong analytic skills related to working with unstructured datasets. \nStrong project management and organizational skills. \nExperience supporting and working with cross-functional teams in a dynamic environment. \nAbility to mentor/guide/collaborate with other team members. \n\nWe are looking for a candidate with 3-5 years of experience in a Data Engineer role. They should also have experience using the following software/tools:\n\nExperience with relational and MPP databases, including Snowflake and MySQL. \nExperience developing software in an agile environment from the requirements stage to production. \nExperience with version control: git\nExperience with container technologies: Docker\nExperience with data pipeline and workflow management tools: Airflow, Jenkins, AWS Glue, Azkaban, Luigi, etc. \nExperience with AWS cloud services: EC2, ECS, Batch, S3, EMR, RDS, Redshift\nExperience with other cloud services: Snowflake, Airflow\nExperience with object-oriented/object function scripting languages: Python, Java, C++, etc. \nExperience with data modeling and data warehouse design\nExperience with data visualization tools (PowerBI, QuickSight)\n\nThe starting annual pay scale for this position is listed below. Actual starting pay will be based on factors such as skills, qualifications, training, and experience. In addition, the company offers comprehensive benefits including medical, dental and vision insurance, 401(k) plan with match, paid time off, and other benefits.\n\nISC's salary ranges are determined by role and level. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations and could be higher or lower based on a multitude of factors, including job-related skills, experience, and relevant education or training.\n\nNational Pay Range\n\n$100,000\u2014$110,000 USD\n\nBenefits of Working at ISC\n\nCompetitive vacation and flexible working arrangements\nComprehensive and inclusive health benefits\nA variety of professional development and mentorship opportunities\nChoice of technology whether at home or in the office\n\nISC's Ownership Behaviors\n\nDo the Right Thing *Be Relentless. Pursue Excellence. *Personal Responsibility & Own Outcomes\nTry Fast. Learn Fast. Fail Fast. Think Big. *Build Extraordinary Owners & Join Forces *Grit & Determination\n\nApplicants with disabilities may contact ISC HR department via e-mail, and other means to request and arrange accommodation. If you need assistance to accommodate a disability, you may request accommodation at any time. Please contact ISC at HR@ISCMGA.com.\n\nISC believes in creating long-term relationships by being responsive and relevant and by consistently delivering value to our community of customers. Specifically, with our employees, we focus on attracting, developing, and retaining the best talent for our business, challenging our people, demonstrating a \"can-do\" attitude, and fostering a collaborative and mutually supportive environment. ISC is an equal opportunity employer and a participant in the US federal E-Verify program (US).\n\nDiversity creates a healthier atmosphere: All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, gender, gender identity, sexual orientation, marital status, medical condition, genetic information, mental or physical disability, military or veteran status, or any other characteristic protected by local, state, or Federal law."}, {"title": "Data Engineer -Remote", "company_name": "RIT Solutions, Inc.", "location": "Denver, CO (Remote)", "link": "/jobs/view/3768007496/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=1hQH8EDaII1eJ7nQ9BACQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3768007496", "description": "About the job\nDuration: 6+ months CTH\n\nMust have a valid LinkedIn profile\n\nRequired Skills\n\n 3-6 years of experience (no more than 6 years)\n Start up environment experience\n Python, SQL, and DBT"}, {"title": "Big Data Engineer - W2", "company_name": "Megan Soft Inc", "location": "United States (Remote)", "link": "/jobs/view/3674845652/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fCAxh4N%2F4YZJ5aEIASaeeA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3674845652", "description": "About the job\nTitle: Big Data Engineer - W2\n\nLocation: Dearborn, MI (Remote)\n\nDuration: 12+ Months\n\nDirect Client: Ford Motors\n\nMust have skills: Should be able to handle GCP requirements\n\n\"Translates business requirements to build repeatable, sustainable, efficient, coded processes that can be productionized by Software Engineers and readily modified by other Data Engineer Creates POC processes in Dev/QA and works with Software Engineers to productionize and align best coding and process practices that maximize efficiency, speed, stability, system resources and capabilities Awareness of and compliance with data privacy, security, legal and contractual guidelines Incorporates data quality and privacy checks/alerts to minimize bad data being consumed by end users, models and dashboards, and to protect customer data; revises checks for new data issues Validates data products and pipelines are functioning as expected following system or application upgrades, source changes, etc. Works closely with Architects if required to align systems, tools and applications being utilized with business use case and performance requirements Provides data product support and maintenance Establishes, tracks and monitors KPIs related to specific data products and deliverable Participate in iteration cadence activities (Iteration Planning, task estimation, daily scrum, demos, etc.). Will be a part of on-call team to resolve critical production support tickets during business hours and off hours.\n\nSkills Required\n\nExperience with Big Data Technologies (Hadoop , ETL Tool, Spark. Hive etc..) Someone who understands Cloud and has experience working with Cloud Platforms (preferably GCP) Should be able to handle GCP requirements - ingesting into GCS/ Big Query, working with GCP components Hands-on in Data Flow, Cloud Storage (GCS), Big Query and GitHub. Proficient in Python and SQL.\n\nSkills Preferred\n\nManages ETL: uses programming and tools for data ingestion, configures pipelines, applies transformations and decoding, integrates and fuses data, moves and securely deliver Experience NoSQL databases\n\nAdditional Information\n\nIf the candidate is remote only, please indicate \"\"100% Remote\"\" under candidate's name on resume. Other similar schedule notations can be \"\"Local Candidate\"\" or \"\"Hybrid Candidate.\"\" ***HRA Test Required***\""}, {"title": "Remote Opportunity: Quantexa Data Engineer", "company_name": "SPAR Information Systems LLC", "location": "United States (Remote)", "link": "/jobs/view/3680073191/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ioykVCZSOQ7BK5%2FwRQHOWg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3680073191", "description": "About the job\nHello All, Hope you are doing great!! Please go through the job descriptions and let me know your interest. Role: Quantexa Data engineer  Location: Remote Duration: Long Term Contract Job Description: 1. End to End usage of Quantexa including guiding our client team on how to use from both front end and data ingestion Reason they are asking for Certification 2. Resolve Entity issues with data 3. Knowledge of Scala and Hive which requires coding experience using parquet 4. Advanced SQL knowledge requiring writing SQL queries 5. Experience having worked with GCP data lake 6. Quantexa certification required\n\nThanks & Regards,\n\nSatnam Singh\n\nDirect: 201 623 3660\n\nEmail : Satnam.singh@sparinfosys.com"}, {"title": "Remote Work - Need Staff Data Engineer - USC only", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3746283980/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=CvIuLBNO%2BPMGi4IXVRkP0A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3746283980", "description": "About the job\n100% Remote, client is in NY\n\nLook for strong job history, Python, Machine Learning, SQL, 10/10 comms, etc. \n\nMUST be an Engineer \n\nSummary\n\nWe're looking for an experienced Data Engineer to join our client's growing team of top notched IT professionals. You will work closely across multiple groups including Editorial and Audio, Marketing, Advertising, etc. to achieve their goals and objectives. The Data Engineering team builds tools throughout the stack while sharing knowledge across these departments.\n\nAs the company grows, they're looking for Data Engineer who will help solidify and expand our pipelines and maintain their data warehouse. Their business model is based on performing complex and very detailed analyses of how our products perform with their subscribers. You'll be responsible for working alongside analysts, product, marketing, and other teams to define new data collection components and measurement schemes that support each new product and feature.\n\nHow Can Add Value To Their Mission\n\nCreate and maintain data pipelines to provide insights and drive business decisions\nEstablish data warehousing strategy (ex. Kimball, Data Vault, etc.)\nMaintain data infrastructure on our AWS accounts\nCollaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.\nWrite unit/integration tests, contributes to engineering wiki, and documents work\nImplement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it\n\nWhat You'll Need To Succeed\n\n7+ years of industry experience measuring product performance and user behavior\nExperience working with a variety of data management technologies, including RedShift, Kinesis/Kafka, Glue, Spark, Postgres, Airflow, dbt, and others.\nExperience implementing BI reporting tools such as Looker\nExperience interfacing with engineers, product managers and analysts to understand data needs\nKnowledge of a variety of measurement beacons, SDKs, APIs including Google Analytics, Amplitude, Braze, Stripe, email service providers\nA commitment to building secure, resilient, fault-tolerant architectures, with clear documentation and procedures in place for support\nA focus on accuracy and detail as you build, ensuring data pipelines produce clear, predictable results\nUnderstanding of the typical metrics a subscription and advertising-supported business needs to measure success\nExperience with Client techniques as applied to behavioral segmentation or anomaly identification is a definite plus\nFamiliarity using developer tools that increase productivity and facilitate the development of resilient code (eg. Docker, CircleCI, Serverless) is a plus"}, {"title": "ENTRY LEVEL DATA ENGINEER (REMOTE)", "company_name": "SynergisticIT", "location": "Jacksonville, FL (Remote)", "link": "/jobs/view/3767587909/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=om7L3CnowiC4PLIcpZD70A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3767587909", "description": "About the job\nSYNERGISTICIT wants every candidate to know that the Job Market is Challenging and to stand out, you need to have exceptional skills and technologies and that's where we come in to make sure you get the attention which you need\n\nSynergisticIT understands the complex nature of the job market and how difficult it can be to secure a position, especially for fresh graduates. Therefore, we assist and help tech-savvies to convert their passions into professions. We go above and beyond to keep you working in your niche.\n\nAs we focus on long-term success, we provide complete career development solutions. From job search to upskilling portfolio and interview preparation, we can guide you at every step of your career.\n\nSynergisticIT spares no efforts to connect you with a large network of tech giants, including Google, Apple, PayPal, Dell, Cisco, Client, etc. Presently, we are actively looking for ENTRY LEVEL DATA ENGINEER a driven mindset. Get the right opportunity and gain experience in building web-centric solutions on Java.\n\nWho Should Apply : Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or anyone looking to make their career in IT Industry\n\nWe also assist in filing for STEM extension and H1b and Green card filing.\n\nCandidates who are serious about their future in the IT Industry and have set big goals for themselves.\n\nCandidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. We also offer Skill Enhancement Programs if the candidates are missing skills or experience which our clients need with great outcomes\n\nCandidates can benefit from skill enhancement if they fall into the below categories. If they are qualified with enough skills then no need for skill enhancement\n\nCandidates Who Lack Experience\n\nHave had a break in careers\n\nLack Technical Competency\n\ncandidates who want to get employed and make a career in the Tech Industry \n\nPlease Also Check The Below Links\n\nhttps://www.synergisticit.com/candidate-outcomes/\n\nhttps://www.synergisticit.com/java-track/\n\nhttps://www.synergisticit.com/data-science-track/\n\nhttps://www.synergisticit.com/which-is-the-best-option-for-tech-job-seekers-staffing-companies-consulting-companies-bootcamps-or-synergisticit/\n\nhttps://www.synergisticit.com/contact-us/\n\nIf the skills are not a match candidates can opt for Skill enhancement. Or their resume can be sent out to clients to see if responses are achievable\n\nREQUIRED SKILLS For Java/Software Programmers\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Core Java , javascript , C++ or software programming\nSpring boot, Microservices and REST API's experience\nExcellent written and verbal communication skills\n\nFor data Science/Machine learning\n\nRequired Skills\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Statistics, Python, data visualization tools\nExcellent written and verbal communication skills\n\nPreferred skills: NLP, Text mining, Tableau, Time series analysis\n\nTechnical skills are required by clients for selection even if its Junior or entry level position each additional Technical skill helps a candidate's resume to be picked by clients over other job seekers.\n\nClients hire candidates with the right technical skills which they need and reject candidates who lack the required technical skills.\n\nNo third party candidates or c2c candidates\n\nPlease apply to the posting\n\nNo phone calls please. Shortlisted candidates would be reached out"}, {"title": "Data Engineer - Remote | WFH", "company_name": "Get It Recruit - Information Technology", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3782241984/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=BgBf1kSAAX%2BSuw6inJsIVw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3782241984", "description": "About the job\nJoin an innovative team at a leading company in the engineering and manufacturing industry, where we are committed to leveraging data for continuous improvement and operational excellence. This hybrid position allows you to enjoy the flexibility of remote work on Mondays and Fridays.\n\nWhy Join Us\n\nAt our company, we prioritize your well-being and professional growth. We offer a comprehensive benefits package, including excellent health benefits, a 6% 401(k) match, generous paid vacation and sick time, 12 paid holidays, and more.\n\nRole Overview\n\nAs a Data Engineer, you will play a pivotal role in transforming raw data into actionable insights, driving improvements in engineering and manufacturing processes, enhancing product quality, and optimizing operational efficiency. Collaborate with dynamic cross-functional teams to extract, analyze, and visualize data, influencing strategic decisions through your valuable insights.\n\nKey Responsibilities\n\nData Collection and Integration:\n\nGather, clean, and integrate data from diverse sources, including production databases, sensor data, and external datasets.\n\nData Analysis\n\nConduct exploratory data analysis to identify patterns, trends, and anomalies.\n\nPerform statistical analysis and hypothesis testing to derive meaningful insights.\n\nData Visualization\n\nCreate informative and visually appealing dashboards and reports using tools like Tableau, Power BI, or Python libraries.\n\nPredictive Modeling\n\nDevelop predictive models to forecast manufacturing performance, quality, and operational outcomes.\n\nRoot Cause Analysis\n\nInvestigate production issues and quality concerns by analyzing data to identify root causes and recommend corrective actions.\n\nPerformance Monitoring\n\nEstablish and maintain key performance indicators (KPIs) to track manufacturing and engineering metrics.\n\nGenerate regular reports to communicate performance trends to stakeholders.\n\nCross-functional Collaboration\n\nCollaborate with engineering, manufacturing, and quality assurance teams to understand their data needs and provide analytical support.\n\nContinuous Improvement\n\nSuggest process improvements and optimizations based on data analysis.\n\nData Security And Compliance\n\nEnsure data security and compliance with relevant regulations, such as GDPR or industry-specific standards.\n\nInfrastructure And Development\n\nBuild and maintain the infrastructure for collecting, storing, and processing data.\n\nDesign, develop, and manage data pipelines, ETL processes, and data warehouses.\n\nRequired Experience\n\nProven experience as a Sr. Data Analyst and/or Data Engineer in an engineering or manufacturing environment.\n\nProficiency in data analysis tools and programming languages, such as Python, R, SQL, and Excel.\n\nStrong data visualization skills with tools like Tableau or Power BI.\n\nExcellent knowledge of statistical analysis and machine learning techniques.\n\nAbility to communicate complex findings and insights effectively to both technical and non-technical stakeholders.\n\nStrong problem-solving skills and attention to detail.\n\nFamiliarity with manufacturing processes and quality control is a plus.\n\nExperience with enterprise ERPs (SAP S4HANA, Oracle EBS), CRMs (Salesforce, Hubspot), and IoT.\n\nProficient in working with cloud computing platforms including AWS, GCP, and Azure.\n\nExpertise in configuring and optimizing cloud-based data solutions for scalability, reliability, and performance.\n\nBachelor's Degree in Data Science, Statistics, Engineering, or Computer Science Required.\n\nApply Now and be part of a team shaping the future of data-driven innovation in engineering and manufacturing!\n\nEmployment Type: Full-Time"}, {"title": "Entry Level Software Engineer - Data Backend Engineer (Remote - Canada)", "company_name": "Yelp", "location": "United States (Remote)", "link": "/jobs/view/3752881757/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fankeoxifvhpTzOYvowHYQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3752881757", "description": "About the job\nYelp engineering culture is driven by our values: we\u2019re a cooperative team that values individual authenticity and encourages creative solutions to problems. All new engineers deploy working code their first week, and we strive to broaden individual impact with support from managers, mentors, and teams. At the end of the day, we\u2019re all about helping our users, growing as engineers, and having fun in a collaborative environment.\n Interested in exploring data and finding innovative ways to collect it, curate it, and make it easily accessible for others to use? How about doing that at an enormous scale? Yelp\u2019s datasets contain billions of interactions between users and local businesses around the globe. If working with our vast amount of data sounds exciting, come and join us!\n\nThe Marketplace Data Observability team is responsible for managing, governing, and monitoring all of Yelp\u2019s Ads data to protect our revenue stream and also support offline analysis that aids in making long term company decisions. We build tools that help us to better understand our data quality, and we strive to uncover data issues before they impact our consumers.\n\nAs a Data Backend Engineer on this team, you will be responsible for building elegant and scalable data products that serve critical, up-to-date, structured information to support different types of analytics within Yelp. As a core contributor to our growing data modeling and data warehousing engineering efforts, you will help design and own mission-critical data flow pipelines and datastores to enable decisions including effective A/B testing and company investments.\n This opportunity is fully remote and does not require you to be located in any particular area in Canada. We welcome applicants from throughout Canada. We\u2019d love to have you apply, even if you don\u2019t feel you meet every single requirement in this posting. At Yelp, we\u2019re looking for great people, not just those who simply check off all the boxes.\n  Build systems that can effectively store and crunch terabytes of data.\nDesign and develop data models for efficient data storage, retrieval, and reporting.\nCreate and maintain conceptual, logical, and physical data models using industry-standard modeling tools.\nCollaborate with cross-functional teams, including engineers, data analysts, business analysts, and data scientists, to understand data requirements and translate them into effective data models.\nParticipate in data integration efforts, including ETL processes and data migration.\nStay up-to-date with industry best practices, emerging technologies, and trends related to data warehousing.\nSupport on-call rotations as needed to operate the team.\n  Understanding of high performing and scalable data systems.\nExperience in building and orchestrating ETL pipelines.\nExperience with Data Lake or Data Warehouse landscape.\nA hunger for tracking down root causes and fixing them in systematic ways.\nAbility to communicate effectively to technical and non-technical cohorts alike. \nExposure to some of the following technologies: Python, AWS Redshift, AWS Athena / Apache Presto, Big Data technologies (e.g S3, Hadoop, Hive, Spark, Flink, Kafka etc), NoSQL systems like Cassandra, DBT."}, {"title": "Data Engineer (Associate Consultant), June 2024", "company_name": "UDig", "location": "Nashville, TN (Remote)", "link": "/jobs/view/3716039960/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=XUjo40kU55HnDCqJ%2F3FdPg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3716039960", "description": "About the job\nCan UDig It? \n\nUDig designs, builds, and implements technology solutions that deliver on business objectives. People join our team because they are excited to have an impact on the future of our organization, passionate about delivering meaningful results, and energized by solving complex problems. Oh, and they like working with other driven, smart, and authentic people!\n\nUDig is proud to be recognized as a \"Best Places to Work\" in Virginia and Tennessee. We're committed to fostering a transparent environment where leadership is accessible, professional development opportunities are abundant, and you are empowered to achieve your career goals. You'll collaborate, develop deep relationships with teammates, and accelerate your success while having fun!\n\nIf you're graduating in May 2024 and excited by the idea of impacting organizations by building data solutions to achieve business goals, you should consider joining our growing technology team.\n\nHow Things Go Down\n\nAs a new, entry-level teammate, you'll take part in our \"Breaking Ground\" training program designed to help you quickly get up-to-speed on UDig's delivery approach within a project team. The program covers:\n\nUDig delivery process and methodologies such as gathering requirements and delivering projects using agile\nTechnical guidance and instruction on the best practices from Data leaders at UDig\nDelivering executive and technical presentations\nCoaching to prepare you to for success on your first client project at UDig\n\nA typical day might entail:\n\nProject standups and check-ins\nDesign and development of data pipelines, BI reports, and more\nPlanning and participating in project demos\nContributing to internal UDig projects\nCareer and job growth through UDig-provided training\nCreating long-lasting friendships with teammates through fun activities as well as your projects (including social events, winning a prize for guessing the weight of pumpkin, or sharing experience on cool tech side projects)\n\nUDig believes it is important for Associate Consultants going through our Breaking Ground program to reside in one of our major markets (Richmond, VA or Nashville, TN metro area) for at least the first year of employment.\n\nWant to learn more? Hear directly from our teammates by checking out our 'A Day in the Life' blog series: https://www.udig.com/digging-in/breaking-ground-2023/\n\nSound Interesting? Here's the Technical Piece\n\nBelow are a few boxes you should be able to check if\u202fyou're interested: Don't check every box? Go ahead and apply!\n\n1+ year of experience with relational database management systems such as Microsoft SQL Server, MySQL, or Oracle\nUnderstanding of the full lifecycle of design, development, and implementation of data solutions including architecture design, development, testing\nSome experience in ETL development or reporting is a plus\nAbility to present technical ideas and high-level concepts and solutions to internal and external team members with varying degrees of technical knowledge\nBachelor's Degree in in computer science, information technology, or any science or business discipline\nAbility to take your work seriously but also seriously enjoy a good time with your UDig colleagues\n\nWhat's in It For You? \n\nAt UDig, you will build your career alongside talented and experienced developers while gaining experience and having fun! We provide continuous challenges and professional development opportunities. Additionally, we offer: \n\nCompetitive salary, merit reviews, and career advancement paths\nFlexible, hybrid environment\nIndividual $1500 Training Budget\nRegular team building and social activities (virtual and/or in-person)\nTransparent culture with strong communication and access to leadership\nGreat benefits like\nGenerous Paid Time Off, company holidays, and parental leave\nMultiple Single and Family Health Insurance plans to choose from\nDental & Vision coverage\nShort-Term and Long-Term disability\nOptional accident and critical illness coverage\nMatching 401(k)\nand more!"}, {"title": "Expression of Interest: Data Engineer", "company_name": "Fingerprint for Success (F4S)", "location": "New York, NY (Remote)", "link": "/jobs/view/3787769880/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=l%2FEDpQqUNmCttGdIyTk7Cg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787769880", "description": "About the job\nWe are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.\n\nThe F4S Talent Pool is a pilot project designed to:\n\nHelp job seekers get discovered by our partners based on their anticipated hiring needs.\nProvide optional support and resources for job seekers in their career endeavors.\nHelp individuals understand, and bring out the best in themselves and each other.\n\nThe F4S Talent Pool process:\n\nOnce you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.\n\nAbout Fingerprint For Success (F4S)\n\nBacked by 20+ years of research, F4S\u2019s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high performing teams.\n\nKeep in mind, joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your F4S work style assessment to match the hiring needs of our partners.\n\nYour feedback is a gift! Write to us via:\n\nPowered by JazzHR\n\noWuPJ9lwq9"}, {"title": "Data Engineer Seattle, WA (remote)", "company_name": "Conch Technologies, Inc", "location": "United States (Remote)", "link": "/jobs/view/3765052211/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=SBETI78UMutCHuRp5FnP%2FA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3765052211", "description": "About the job\nHi,\n\nGreetings from Conch Technologies Inc\n\nData Engineering Lead \u2013 With Databricks and Unity Catalog\n\nLocation \u2013 Seattle, WA (remote)\n\nDuration \u2013 12 months\n\nNeed: Design and implement data architecture and strategy, leveraging technologies such as Databricks, Unity Catalog, Privacera, and Collibra.\n\nAs a Data Engineering Lead, you will play a pivotal role in defining and executing technical deliverables while assessing complexities and levels of effort. You'll engage with both vendor and internal teams to resolve technical challenges and ensure seamless collaboration. Your leadership will extend to planning and prioritization with the SCRUM master, facilitating effective coordination between onsite and offshore teams, and driving feature prioritization through collaboration with product managers.\n\nKey Responsibilities\n\n Lead the definition and breakdown of technical deliverables, assessing complexities and levels of effort.\n Collaborate with vendor and internal teams to address technical issues.\n Drive planning and prioritization in conjunction with the SCRUM master.\n Coordinate with onsite and offshore teams for timely project delivery.\n Collaborate with product managers to prioritize features, communicate with stakeholders, and clarify requirements.\n Collect and manage user-reported issues, ensuring prompt resolution and communication.\n Define comprehensive test plans, oversee their execution, and document results.\n\n--\n\nWith Regards,\n\nNagesh G\n\nMobile: 408-381-5645\n\nDesk: 901-313-3066\n\nEmail: nagesh@conchtech.com \n\nWeb: www.conchtech.com"}, {"title": "Remote Work - Need Azure Fabric Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3759325599/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=C3iVjmcR9wjUip1nNjUiCg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3759325599", "description": "About the job\nThe client is placing heavy emphasis on Microsoft and Azure Fabric experience\n\nInformation Management Engineer \u2013 Take the identified information management sources and assist migration to the Client Data Lake, Azure Fabric. The engineer will transform information management (de-dup, clean and enrich).\n\nTake the identified data sources and bring them into our Data Lake, Azure Fabric. Once the data is in, the engineer will need to be able to transform data (de-dup, clean and enrich the data). We are focused on using a medallion data model for data architecture. The tools we are working with are: Delta Parquet files, python, spark, notebooks, T-SQL, Fabric lakehouse/warehouse, Dataflows Gen2, Data Pipelines, Direct Lake, shortcuts, ADF."}, {"title": "Data Engineer", "company_name": "Catalist", "location": "Washington, DC (Remote)", "link": "/jobs/view/3744951279/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ui1Fk6ICmnPNqAlZDZAcwA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3744951279", "description": "About the job\nFor over 17 years, Catalist has been a leader in civic data and data science innovation. Our mission is to provide progressive organizations with the data, software, and services needed to better identify, understand, and communicate with the people they need to engage and mobilize. Our clients include the largest, most influential organizations in the U.S. active in civic engagement, advocacy, and political campaigns.\n\nCatalist is home to a dedicated, creative team of technologists, data scientists, and campaign experts committed to using our talents and technology to nurture a vibrant and growing progressive community.\n\nAs a Data Engineer at Catalist, you will work closely with the Analytics and Technology departments to design, build, support, and maintain various data pipelines and processes with an end goal of providing data and intelligence to the progressive community. This role primarily involves translating data architecture designs into functional processes, code, and systems.\n\nThe ideal candidate will be a highly motivated individual with excellent technical skills, a strong desire to learn new skills, and an interest in progressive politics. Catalist values creativity and problem-solving. Our work is on the cutting edge of data-driven politics, and your support will help Democratic candidates and progressive organizations conduct successful advocacy and electoral campaigns.\n\nThis position reports to the Deputy Chief Data Officer. The Data Engineer is a part of a growing Data team that supports all underlying work at Catalist.\n\nThis position is included in our CWA bargaining unit.\n\nPrinciple Duties & Responsibilities\n\nDevelop scalable, production processes, code, and systems for data ingest, transformation, modeling, and reporting across a variety of platforms\nTranslate mock-ups and designs into functional processes, code, and systems\nCreate architecture designs when needed\nProvide quality assurance and testing on processes, code, systems, and products\nBecome an internal subject matter expert on various datasets and support other Catalist departments/teams on usage of those datasets\nExecute ad-hoc data and database maintenance tasks as requiredProject manage cross-departmental efforts, with direct responsibility for stakeholder engagement, management, and execution of technical elements\n\n\nRequirements\n\nBS or BA in a technical field, or relevant experience\n1-2 years of experience working with SQL databases\nExperience with data cleaning or ETL processes\nExperience with distributed computing systems and/or distributed datastores (particularly the Hadoop ecosystem)\nExperience managing projects\nFamiliarity with Catalist data, progressive politics, voter files, and/or commercial data\nBackground check required\n\n\nPreferred Skills & Abilities\n\nWillingness to be a problem solver and produce results in a fast paced environment\nAbility to be creative and personable, and articulate ideas clearly\nExperience working with SQL databases\nProficiency with Python or another object-oriented programming language (R, Java, Scala, etc\u2026)\nExperience working in cloud environments (AWS, GCP, etc.)\nExperience working in command line environments such as Bash\nExperience with a version control tool such as git or github\n\n\nBenefits\n\nMedical, Dental, Vision, Prescription Drug\n\nCatalist offers Medical, Dental, Vision, and Prescription Drug coverage for eligible staff and their eligible dependents. Catalist\u2019s Medical plan is a comprehensive PPO program including Prescription Drug coverage with 85% of the premium paid by Catalist. Dental and Vision coverage is provided at no cost to employees.\n\nGroup Term Life Insurance and Long-Term & Short-Term Disability Coverage\n\nGroup Term Life Insurance and Long-Term and Short-Term Disability coverage is available for eligible staff. These benefits are provided at no cost to Catalist employees.\n\n401(k) Safe Harbor Plan\n\nA 401(k) Safe Harbor Plan is available to eligible staff with a 3% contribution from Catalist from the date of hire. Employees may contribute pre-tax or post-tax from their salary up to the legal limits set forth by the IRS.\n\nMedical and Dependent Care Flexible Spending Accounts (FSAs)\n\nCatalist offers an FSA Program that gives eligible staff the ability to pay out-of-pocket medical/dental/vision/child care expenses from pre-tax earnings.\n\nTransit Benefits\n\nCatalist also makes available a Transit benefit FSA program to eligible employees using pre-tax contributions with a company match.\n\nProfessional Development and Remote Work Expenses\n\nEligible employees may be reimbursed up to $750 each year for professional development / education and remote work expenses.\n\nStudent Loan PayDown or SaveUp\n\nCatalist offers a Student Loan PayDown and College SaveUp benefit for eligible staff.\n\nVacation, Personal Leave, Sick Leave Benefits\n\nCatalist offers generous vacation benefits to all eligible staff. Eligible employees also receive:\n\n 14 Paid Holidays\n Personal Days\n Sick Leave\n Parental Leave\n\n\nHybrid Office/Remote Work\n\nCertain positions at Catalist are eligible for Office/Remote Hybrid or full Remote status."}, {"title": "Remote Entry Level Data Analyst/Engineer", "company_name": "SynergisticIT", "location": "White Plains, NY (Remote)", "link": "/jobs/view/3774171423/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=q54sk1%2FRww0z%2BC%2BE8e0jQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774171423", "description": "About the job\nAt SynergisticIT, we're all about making connections. Whatever IT goals you have, our software programmers can help achieve those. Our Software development teams can take up turnkey projects and execute them in an effective and efficient manner. If you are looking to source talent our recruiters will find the ideal IT talent for your company. What's the secret to our success? Well, it all starts with taking quality time to listen to each client's specific needs. After we have a thorough grasp of your IT goals, we can better customize our Developments as per your specific needs. We can also tailor make recruiting programs to exceed your expectations. Since our founding in 2010, SynergisticIT's strategies have earned the company an enviable position in the software development, IT staffing and IT skill enhancement fields. SynergisticIT continues to work with hundreds of satisfied American clients with our software programmers working on our projects and after gaining hands on experience on cutting edge technologies moving to contribute their skills to great clients like Apple, Google, Client, Ebay, Paypal, Kroger, the Walt Disney Company and hundreds more. If you are tired of working with inefficient programmers who take a lot of time to ramp up we want you to try us. Our software programmers can hit the ground running and get you the maximum return on your investment. You have already tried the rest its time you tried the best. SynergisticIT - Home of the Best Data Scientists and Software Programmers in the Bay Area.\n\n Why Us ? \n\nSynergisticIT has a proven track record of successfully skill enhancement and staffing IT employees for some of the world's most iconic brands. Our team takes the time to fully understand every client's needs so we could best meet your IT staffing requirements. The knowledgeable staff at SynergisticIT is always more than happy to work with clients to ensure they reach their software development goals. Besides staffing, SynergisticIT is also committed to helping young IT professionals advance their career with a robust upskill program . Everyone who goes through SynergisticIT's program learns all the skills necessary to succeed in many IT fields ranging from Java to Machine Learning. Additionally, everyone trained at SynergisticIT has been through extensive mock and technical interview screenings to bolster their career prospects. Last, but certainly not least, SynergisticIT takes great care to respect the privacy considerations for every client. All companies who work with SynergisticIT can rest assured their confidential data is protected using the most up-to-date encryption technologies. SynergisticIT also complies with all the latest NDA agreements.\n\n REQUIRED SKILLS For Java /Software Programmers \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Time series analysis \n\n We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2022 and at Gartner Data Analytics Summit (Florida)-2023 \n\nOracle CloudWorld Event (OCW) Las Vegas 2022 | SynergisticIT - YouTube\n\n https://www.youtube.com/watch?v=OAFOhcGy9Z8 \n\n https://www.youtube.com/watch?v=EmO7NrWHkLM \n\n https://www.youtube.com/watch?v=NVBU9RYZ6UI \n\n https://www.youtube.com/watch?v=Yy74yvjatVg \n\nSynergisticIT at Gartner Data and Analytics Summit 2023 - YouTube\n\nFor preparing for interviews please visit  https://www.synergisticit.com/interview-questions/ \n\n We are looking for the right matching candidates for our clients \n\n Please apply via the job posting \n\n REQUIRED SKILLS For Java /Full Stack/Software Programmer \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning Positions \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Tensorflow \n\n If you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team. \n\n No phone calls please.  Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates"}, {"title": "Data Engineer", "company_name": "Anika Systems", "location": "Leesburg, VA (Remote)", "link": "/jobs/view/3787766893/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=mApL9JnN%2B%2Fr2%2BYUPlxRYOA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787766893", "description": "About the job\nAnika Systems is a fast growing, woman-owned small business that specializes in providing innovative IT solutions for federal government agencies. Our expertise lies in accelerating delivery in Data and Analytics, Intelligent Automation, Application Development, and IT Modernization. We are currently expanding our Federal team and are seeking a passionate and talented Data Engineer. This opportunity is 100% remote.\n\nMust be a U.S. Citizen with the ability to obtain and maintain a government suitability clearance\n\nResponsibilities\n\nDesigns, develops, builds, analyzes, evaluates, and installs database management systems to include database modeling and design, relational database architecture, metadata, and repository creation and configuration management.\nDefines and oversees database organizations, standards, controls, procedures, and documentation. Provides technical consulting in the definition, design, and creation of a database environment.\nAdvises applications development staff and users on data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nDesigns and implements databases with respect to access methods, access time, batch processes, device allocation, validation checks, organization, protection and security, documentation, and statistical methods.\nUses data mapping, data mining, and data transformational analysis tools to design and develop databases.\nDetermines data storage and optimum storage requirements.\nPrepares system requirements, source analysis, and process analyses and designs throughout the database implementation.\n\nRequired Skills And Experience\n\nBA/BS and 1 year of relevant experience\nExperience with DataBricks, SQL, Python\nApplications development experience with data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nExcellent communication skills (both oral and written) with process-oriented organizational skills to ensure project success\n\nDesired Skills And Experience\n\nDemonstrated successful development experience with the full life cycle of government database management systems including database modeling and design, relational database architecture, metadata and repository creation, and configuration management.\n\nPowered by JazzHR\n\nTlKWIorucJ"}, {"title": "Remote Work - Need Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3774712978/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=r3Bp%2F0qcOD2OpC5jpbUOcA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774712978", "description": "About the job\nTitle : Data Engineer\n\nLocation : Remote\n\nDuration : 6+ months to start (long term contract (work order are extended 2x year))\n\nPlease Fill out the skill matrix: Technology Some Awareness (1) Novice (2) Intermediate (3) Advanced (4) Expert (5) SQL Tableau Desktop /Creator Tableau API Integration AWS - S3 Glue AWS - Athena Redshift AWS - Lambda AWS - SNS/SQS msExcel\n\nProfile: Data Analyst will work closely with Architects and leads in the Interflow team, key business stakeholders including program staff to support the organizations effort in the areas of data Analysis and information management. Role should have strong experience in writing SQL queries to extract data for reports. Candidate should have experience working with large set of data. As these reports are very critical for the company and we are looking for an expert who is capable of handling, consuming and extracting large sets of data for analysis and reports.\n\nMust Haves\n\nAWS - Athena\nAWS - S3\nGlue\nhands-on development of reporting applications\nMS Excel\nRedShift\nSQL\nTableau Desktop /Creator\n\nNice\n\nAWS - Lambda\nAWS - SNS/SQS\nTableau API Integration\n\nSkills\n\nBA/BS required (major in an analytical field desired) \nA minimum 10+ years work-related experience. \nExperience in hands-on development of reporting applications for the Web in a professional environment \nThe Ethos of continuous improvement and interest in learning new things.\nStrong analytical thinking and structured problem-solving ability\nAbility to handle multiple projects and assignments simultaneously and effectively in a cross-functional team environment. \nVersed on the agile methodology and best practices. \nExcellent interpersonal and collaboration skill with the ability to work with a diverse set of colleagues, across functions, from different organizations, disciplines, etc. \nSelf-starter, ability to set priorities, work independently and attain goals"}, {"title": "Data Engineer", "company_name": "Team Remotely Inc", "location": "Houston, TX (Remote)", "link": "/jobs/view/3786249829/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=78lY5Jo%2FfyRPC6TQYCdvww%3D%3D&trk=flagship3_search_srp_jobs", "id": "3786249829", "description": "About the job\nThis is a remote position.\n\n Data Engineer (US/Canada Residents Only, 1 year experience, remote)\n\nTeam Remotely Inc. is a staffing and recruitment agency that offers a comprehensive solution for talent acquisition, including sourcing, vetting, pay rolling, and managing talent. Whether you need contract staffing, direct hire, direct sourcing, talent pools, or diversity initiatives, our model can support your hiring strategy.\n\n Hiring Type: Full-Time\n\n Base Salary: $55K-$70K Per Annum.\n\nResponsibilities\n\nDesigning user interface changes for web-based DB applications. \nReviewing application requirements and interface designs. \nDeveloping and implementing highly responsive user interface components using react concepts. \nWriting application interface codes using JavaScript following react.js workflows. \nTroubleshooting interface software and debugging application codes. \nDeveloping and implementing front-end architecture to support user interface concepts. \nMonitoring and improving front-end performance. \n\nQualifications And Experience\n\nBachelor\u2019s degree in computer science, information technology, or a similar field. \nPrevious experience working as a react.js developer. \nIn-depth knowledge of JavaScript, CSS, HTML, jQuery, and front-end languages. \nKnowledge of REACT tools including React.js, Webpack, Enzyme, Redux, and Flux. \nExperience with user interface design. \nKnowledge of performance testing frameworks, including Mocha and Jest. \nExperience with browser-based debugging and performance testing software. \nTroubleshooting skills. \nProject management skills. \nProblem-solving skills. \nVerbal communication skills. \n\n Why work with Team Remotely?\n\nTeam Remotely Inc. is a staffing platform offering a seamless experience for employers and candidates. Employers can post job openings and specify their requirements, while candidates can create profiles and upload resumes.\n\nThe team of Team Remotely continuously learns and adapts based on previous successful placements, constantly improving its matching capabilities. This ensures that the recommendations provided by Team Remotely are tailored and accurate, increasing the likelihood of a successful match between employers and candidates. By providing intelligent and data-driven solutions, they strive to enhance the efficiency and effectiveness of the hiring process, ultimately helping companies find the best talent and individuals find their dream jobs."}, {"title": "Data Engineer", "company_name": "ZAG Technical Services", "location": "United States (Remote)", "link": "/jobs/view/3771403464/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=Hf0VefrKXBFIL5MtHmY0Pw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3771403464", "description": "About the job\nWe are looking for a tech savvy Data Engineer to contribute to the building, operating, and scaling of next generation data platforms and tools that will power data-driven analytics capabilities throughout the ZAG products and services portfolio, spanning areas such as business intelligence, reporting, and data analytics.\n\nThe ideal team member is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. Must be comfortable supporting the data needs of multiple clients and teams, systems, and products within the modernization of ecosystems. The right-fit person will be excited by the prospect of optimizing or even re-designing data processes to support next generation of advanced analytics capabilities, which in-turn translates to implementing strategic enhancements to maintain competitive advantage in the industry.\n\nThe successful team member will leverage their proficiency to\u2026\n\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\nBuild the analytics processes required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Data Management and cloud based \u2018big data\u2019 technologies.\nBuild analytics tools that utilize technologies to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.\nCollaborate with client teams to identify and solution regional and local requirements within the context of the standard global business model for enterprise analytics.\nParticipates in meetings, discussions, strategy sessions where changes, improvements, and enhancements are proposed, evaluated, and approved. \n\n\nThe exceptional team member will have or demonstrate progressive experiences in\u2026 \n\nBachelor\u2019s degree or higher in Computer Science or related field\nExperience working with BI and data warehousing tools\nExperience working with enterprise data, where security is paramount and data governance is critical\nAbility to work independently and in a team environment, and effectively engage all levels of the organization\nUnderstanding of distributed systems driving large-scale data processing and analytics with a successful history of manipulating, processing, and extracting value from large, disconnected datasets.\nAbility to communicate effectively (listening, presenting, and questioning)\nStrong organizational, written, and communication skills\n\n\nAbout ZAG\n\nZAG Technical Services empowers our clients\u2019 success through intelligent business solutions that increase employee productivity, network security and team collaboration. We aim to ensure that our client\u2019s IT investments drive ROI, provide a competitive advantage and help businesses grow. We do this by living our core set of values: Integrity Foremost, Client-Centered, Accountable Always, Teamwork Throughout, and Exceptional is the Goal.\n\nWhat ZAG Has To Offer\n\nCompetitive compensation package \nBenefits including medical, dental, vision, 401K and life insurance \n\n\nZAG Technical Services is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all of our employees.\n\nZAG Technical Services is an E-Verify participant and will provide the Social Security Administration (SSA) and, if necessary, the Department of Homeland Security (DHS), with information from each new employee\u2019s Form I-9 to confirm work authorization."}, {"title": "Junior Data Engineer", "company_name": "HireMeFast LLC - Career Accelerator - Land A Job", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3786249797/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D&trk=flagship3_search_srp_jobs&lipi=urn%3Ali%3Apage%3Ad_flagship3_search_srp_jobs%3BQExogrHBQoSF9zZm%2B9kufw%3D%3D&lici=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D", "id": "3786249797", "description": "About the job\nThis is a remote position.\n\n Job Title: Junior Data Engineer\n\n Employment Type: Full-Time\n\n Salary: $64,000 - $76,000 per annum\n\n Experience Required:  Minimum 1 year of project experience\n\nAbout us: HireMeFast is a leading staffing and recruitment agency specializing in connecting businesses with top-tier talent across various industries. Our mission is to bridge the gap between exceptional candidates and organizations needing their skills, expertise, and unique qualities. Our team of experienced and dedicated recruitment specialists utilizes innovative sourcing strategies, and a vast network to identify and attract top talent. We conduct comprehensive procedures to ensure that only the most qualified candidates are presented to our clients.\n\nPosition Summary\n\nJoin the fast-paced, innovative, and collaborative environment focused on providing an AIOps platform that enhances the intelligence of the CVS Health infrastructure. Work closely with subject matter experts and colleagues to build and scale out machine learning and AI solutions that will detect, predict, and recommend solutions to correct issues before system impact and enhance efficiency, reliability, and performance of CVS Health\u2019s IT operations.\n\nKey Responsibilities include:\n\nData pipeline development: Designed, implemented, and managed data pipelines for extracting, transforming, and loading data from various sources into data lakes for processing, analytics, and correlation. \nData modeling: Create and maintain data models ensuring data quality, scalability, and efficiency \nDevelop and automate processes to clean, transform, and prepare data for analytics, ensuring data accuracy and consistency \nData Integration: Integrate data from disparate sources, both structured and unstructured to provide a unified view of key infrastructure platform and application data \nUtilize big data technologies such as Kafka to process and analyze large volumes of data efficiently \nImplement data security measures to protect sensitive information and ensure compliance with data and privacy regulation \nCreate/maintain documentation for data processes, data flows, and system configurations \nPerformance Optimization- Monitor and optimize data pipelines and systems for performance, scalability and cost-effectiveness \n\nCharacteristics of this role:\n\nTeam Player: Willing to teach, share knowledge, and work with others to make the team successful. \nCommunication: Exceptional verbal, written, organizational, presentation, and communication skills. \nCreativity: Ability to take written and verbal requirements and come up with other innovative ideas. \nAttention to detail: Systematically and accurately research future solutions and current problems. \nStrong work ethic: The innate drive to do work extremely well. \nPassion: A drive to deliver better products and services than expected to customers. \n\nRequired Qualifications\n\n2+ years of programming experience in languages such as Python, Java, SQL \n2+ years of experience with ETL tools and database management (relational, non-relational) \n2+ years of experience in data modeling techniques and tools to design efficient scalable data structures \nSkills in data quality assessment, data cleansing, and data validation \n\nQualifications\n\nKnowledge of big data technologies and cloud platforms \nExperience with technologies like PySpark, Databricks, and Azure Synapse. \n\nEducation\n\nBachelor\u2019s degree in Computer Science, Information Technology, or related field, or equivalent working experience\n\nWhy HireMeFast LLC?\n\nAt HireMeFast, we understand that finding the right individuals to join your team is crucial for success and growth of your organization. We are committed to streamlining the hiring process for our clients, ensuring they have access to a diverse pool of highly qualified candidates who are a perfect fit for their specific needs."}, {"title": "Data Engineer  - Need candidates from Texas ( REMOTE )", "company_name": "Pulivarthi Group (PG)", "location": "United States (Remote)", "link": "/jobs/view/3770685212/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=9gdMwuif%2BpB%2BCQYvQi3imQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3770685212", "description": "About the job\nFollow us on Linkedin: https://www.linkedin.com/company/pulivarthigroup/\n\nPulivarthi Group LLC is a Global Staffing & IT Technology Solutions company, with our prime focus of providing world class solutions to our customers with the right talent. We combine the expertise of our team and the culture of your company to help you with the solution that is affordable and innovative using high quality standards and technologies.\n\nWe\u2019ve served some of the largest healthcare, financial services, and government entities in the U.S.\n\nData Engineer - FT only - Azure SQL with data lakes experience\n\nData Engineer -\n\nthe resource requirement for this Data Engineer role needed more clarification.\n\nOur customer is requesting a resource who knows Azure CI/CD (Continuous Integration / Continuous Deployment) to build and integrate their DevOps pipeline.\n\nOnce the DevOps pipelines are mapped, they need the Data Engineer to connect to all the Data Sources, primarily their Data Lake.\n\nThe Customer (based in Houston) is interested in a resource in a time zone not too far removed from the Central Time Zone to allow real time collaboration."}, {"title": "Data Engineer", "company_name": "Otter Products", "location": "Fort Collins, CO (Remote)", "link": "/jobs/view/3766728302/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=77U3U7CAi1PxgDmpI3%2BiNQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3766728302", "description": "About the job\nOverview \n\nOtter Products is currently recruiting for a Data Engineer to join our Data team! You can be based in our Fort Collins, CO office with flexibility to work a portion of your time remotely, or 100% Remote in the U.S.\n\nThe Data Engineer applies professional experience, concepts, and company objectives toward building and operationalizing the minimally inclusive data necessary for the enterprise data initiatives following industry standard practices and tools. The bulk of the Data Engineer's work would be in designing, managing and optimizing data pipelines, then moving these data pipelines effectively into production for key data and analytics consumers like business/data analysts, data scientists or other stakeholder that needs curated data for data and analytics use cases across the enterprise. The Data Engineer also ensures compliance with data governance and data security requirements while creating, improving and operationalizing these integrated and reusable data pipelines to enable faster data access, integrated data reuse and vastly improved time-to-solution for data and analytics initiatives. Additionally, the Data Engineer will be expected to collaborate with other data team members and data consumers working on the models and algorithms developed by them toward optimization for data quality, security, and governance to put them into production leading to potentially large productivity gains.\n\n About Otter Products \n\nOtter Products, we grow to give. From our founder's garage in 1998 to the global technology leader we are today, Otter Products continues to drive growth through innovation.\n\nThrough our industry-leading brands - OtterBox, Liviri and OtterCares - we provide our partners the number-one selling and most trusted products in our categories. Our philanthropic spirit is the foundation on which we foster our partner relationships, allowing us to grow and to give - together.\n\nBy way of our charitable arm, the OtterCares Foundation, we support our communities and invest in our future through education that inspires kids to change the world.\n\nAnd even as our global community of Otters continues to grow, our founder's core values are still at the heart of everything we do. We measure our success by our ability to give back to our communities and strengthen opportunities for all.\n\nFor more information visit otterproducts.com\n\n Responsibilities \n\nComplete analysis, design and development of BI solutions using Azure SQL Server\nFamiliarity with Data Warehouse concepts\nExperience coding the extraction, transformation, and loading (ETL) processes\nDatabase development primarily in SSIS, Data Factory and SQL\nPartner with the business to determine end user database/reporting needs and requirements\nGenerate ad hoc reports using Power BI or SSRS\nCollaborate with other developers to create and implement best approach solution(s)\nTroubleshoot Azure tools, systems, and software\nReview queries for performance issues, making changes as needed\nCollaborate with team to performance-tune Azure application as necessary\nAssist with the analysis and extraction of relevant information from large amounts of historical business data to feed data science initiatives\nParticipate and support the design and documentation of processes for large scale data analyses, model development, model validation and model implementation\nSupport and maintain a positive safety culture by following all safety policies and procedures and actively contributing to a safe working environment\nOther duties as required\n\n Qualifications \n\nBachelor's degree required. Degree in Computer Science or Mathematics preferred\nMinimum of four years of experience in an IT or analytical role required. Experience in database development, report writing and/or statistics preferred.\n\n EEO \n\nOtter Products, LLC is an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, age, marital status, pregnancy, sex, sexual orientation, gender, gender identity or expression, national origin, disability, veteran status, or any other characteristic or status protected by law.\n\n For US Based Roles Only - Compensation Range Minimum \n\nUSD $90,000.00/Yr.\n\n For US Based Roles Only - Compensation Range Maximum \n\nUSD $120,000.00/Yr.\n\n Additional Total Rewards \n\nProfit Sharing Program Eligible, Benefits Eligible - Full Time- check out otterproducts.com/careers/why for more info"}, {"title": "Software Engineer, Distributed Systems", "company_name": "Hightouch", "location": "United States (Remote)", "link": "/jobs/view/3628572539/?eBP=CwEAAAGMd4QO8APODyE_-mMOPhYyQGg5ZjVbPwq_3V7DK0B2bLAf10pcedM9S2T71X5i4TiJuEwOzWBbSiTKbLuk-VD-_Y-dN5uyf8YpYkHYqEYFxySMS9zr2hRjNrxX9gXbxssh35TFK9YTNV2WeH2Ncb675JgFbJTgNWSZySFC6rrdJGmdSDTr2q5d3VoxORz3GjsDbzjeyZQCvQ07RqYw7oujWSgslycs2LMCY_-N0NEj6e0PvbaYKiZ_5IW34bGVSYLXpHwqa8oOcNttYZ90UA3E_7sDSFzABdIVlfrp5zzERrIYy5sh2MIInxHdDoKgQw8VmOn5uM4S0tXUl-i7LnN0_W0NfPBmLiVDNuveIOm4MfDNPxfe5QKPk6FUAVpvgbcoJn9yRyE&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=rph%2FT6Q6Vkp6YoggJf9cJQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3628572539", "description": "About the job\nAbout Hightouch\n\nHightouch\u2019s mission is to empower everyone to take action on their data. Through our Reverse ETL platform, business and data users can seamlessly sync data from where it resides, such as warehouses and databases, to where it is needed, including operational systems and SaaS tools. Traditionally, acting on data has required engineering time and bandwidth, and left most business users stuck with charts and reports that are unable to take automated action on their data. With Hightouch, every business user, without writing any code, can activate data to streamline critical processes, improve marketing performance, and scale operations.\n\nOur team operates with a focus on making a meaningful impact for our customers. We believe in approaching challenges with a first principles thinking mindset, moving quickly and embracing our value of efficient execution, and treating each other with compassion and kindness. We look for team members that are strong communicators, have a growth mindset, and are motivated and persistent in achieving our goals.\n\nHundreds of companies use Hightouch, including Spotify, Ramp, Retool, NBA, Plaid, and Betterment. We\u2019re based in San Francisco, are remote-friendly, and backed by leading investors such as Amplify Partners, ICONIQ Growth, Bain Capital Ventures, Y-Combinator, and Afore Capital.\n\nAbout The Role\n\nWe are looking for a distributed systems engineer to work on the systems that power the syncing engine our customers and other engineering teams rely on. This presents an exciting challenge where you can apply your expertise in distributed systems, performance optimization, and troubleshooting to push the boundaries of what is possible and meaningful improve our ability to move massive amounts of customer data. This role also provides a unique opportunity to work on a multi-cloud and multi-region infrastructure that supports a global customer base.\n\nWe believe in enabling our engineers to do their best work for our customers by giving them extremely high levels of ownership and autonomy. This comes in different forms: you will own and deliver projects from start to finish, you will work directly with customers to solve their hardest scaling problems, and you will have a lot of influence over what we work on as a team and company.\n\nSome of the problems we\u2019ll be working on include:\n\nSync Speed: Customers want to sync a lot of data to important destinations like Facebook and Snapchat, which requires us to analyze every part of our syncing process and find where we can optimize to sync data more quickly\nStreaming Syncing: We currently sync data from sources in batch only, but envision a world where we also do syncs in real-time and support streaming sources like webhooks and queues\nScalability and Reliability: As part of our rapid growth, we\u2019re always evaluating current future bottlenecks in scaling and reliability, and architecting for the next order of magnitude of growth\nPersonalization API: Our Personalization API product provides a low latency caching layer on top of data warehouses that enable customers to use their data warehouse for real-time personalization use cases with \nMulti-Region and Multi-Cloud: Supporting our multi-region and multi-cloud backend, including extending it to launch Hightouch on in new regions to support data residency requirements of our global customer base\n\nWe are looking for talented, intellectually curious, and motivated individuals who are interested in tackling the problems above. This is a senior role, but we focus on impact and potential for growth more than years of experience. The salary range for this position is $170,000 - $240,000 USD per year, which is location independent in accordance with our remote-first policy.\n\nAbout You\n\nYou are an engineer with a passion for solving hard technical problems that generate real value for customers. You\u2019re motivated by high ownership and are comfortable in a fast-paced, startup environment.\n\nYou have experience and comfort with distributed systems and high-scale systems or the ability and desire to build this experience quickly. This experience can take different forms:\n\nYou\u2019ve built syncing engines that process and move terabytes of data per day in a scalable way\nYou\u2019ve created event collection or data streaming services that power company critical systems\nYou\u2019ve owned systems that have undergone orders of magnitude of growth and helped them scale accordingly\nYou have relevant skills and are a fast learner who is excited to take on the challenges we face\n\nInterview Process\n\nOur goal with the interview process is to balance speed with giving both parties opportunities to assess whether there is a strong mutual fit. We will ask you questions, but we want you to ask us questions! Our technical interviews focus on how you design systems because we believe this is the best way for us to see how you work and for you to see how we collaborate. We don\u2019t ask you to write code to solve technical brainteasers that don\u2019t appear in your day to day job.\n\nRecruiter screen [30m]: Introductory call with our recruiting team to get to know each other and see if the role could be a good mutual fit.\nSystems design screen [45m]: Designing a data processing feature end-to-end.\nHiring manager interview [30m]: Chat with hiring manager about past experiences and future operating preferences to assess fit on company values and operating principles.\nSystems design interview [90m]: Work with the interviewer to architect a system at a conceptual level. The problem will be at a pretty high level - and have both product and customer requirements as well as technical."}, {"title": "Entry-Level Data Scientist Engineer", "company_name": "Phoenix Recruitment LLC", "location": "Texas City, TX (Remote)", "link": "/jobs/view/3788727625/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=8iwtytHsCu4wMwJ4fj3pxw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3788727625", "description": "About the job\nThis is a remote position.\n\nTitle - Entry-Level Data Scientist Engineer, 1 year of project experience\n\nEmployment Type: Full-time\n\nBase Salary: $60K-$70K\n\nPhoenix Recruitment offers a variety of recruiting services to assist both employers and employees. They specialize in marketing open positions, recruiting, and helping employers to find qualified candidates across various industries. Phoenix Recruitment has expertise in streamlining the hiring process. They can help ensure that the process is efficient, well organized, and compliant with relevant regulations.\n\nSkills and Abilities:\n\nStrong knowledge of R or Python for data analysis and modeling. \nProficiency in statistical programs such as R, SAS, MATLAB, or Python. \nFamiliarity with spreadsheets (VBA) and database applications (Access, Oracle, SQL, or equivalent technology). \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \nAbility to learn new methods quickly and work under deadlines. \nExcellent teamwork and communication skills. \nStrong analytical and problem-solving abilities. \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \n\nPreferred:\n\nKnowledge of actuarial concepts and life, health, and/or annuity products. \nExperience with statistical modeling techniques such as GLM, Decision Trees, Time Series, Regression, etc. \nFamiliarity with Microsoft DeployR. \nExposure to insurance risk analysis. \nBasic experience in computational finance, econometrics, statistics, and math. \nKnowledge of SQL and VBA. \nFamiliarity with R or Python for predictive modeling \n\nWhy Phoenix Recruitment LLC?\n\nPhoenix Recruitment often has an extensive network of employers and candidates. This network allows them to tap into a pool of qualified candidates and connect them with suitable job opportunities. They can also leverage their connections to help employers find the right talent efficiently. Outsourcing the recruitment process to a specialized agency can save your time and resources, avoid delays, reduce administrative burdens, and increase the chances of finding the right fit for your organization."}, {"title": "Data Engineer - Remote Work", "company_name": "BairesDev", "location": "United States (Remote)", "link": "/jobs/view/3783162297/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=OtoCrAAHZFomVup04fy35g%3D%3D&trk=flagship3_search_srp_jobs", "id": "3783162297", "description": "About the job\nWho We are\n\nBairesDev is proud to be the fastest-growing company in America. With people in five continents and world-class clients, we are only as strong as the multicultural teams at the heart of our business. To consistently deliver the highest quality solutions to our clients, we only hire the Top 1% of the best talents and nurture their professional growth on exciting projects.\n\nWe are looking for a Data Engineer to join our Development team and participate in different projects made up of multicultural teams distributed throughout the world.\n\nWhat You Will Do\n\n Interview other Data Scientists and Data engineers to vet their technical abilities.\n Input data from those interviews into the product and help develop a predictive, fair, and enjoyable solution.\n Work together with developers, tech leads, and solution architects to build applications.\n Improve existing structures by adding new functionalities or proposing technological updates so that as a result, the impact of your contribution is significant in the core of each business.\n Improve the user experience on scalable and high availability platforms, contributing to the key differential of each business.\n\nHere\u2019s What We Are Looking For\n\n 5+ years of experience working as a developer.\n 3+ years of experience working as a Data Scientist / Data Engineer.\n Proficient with analysis, troubleshooting, and problem-solving.\n Hands-on experience with managing data loads and data quality.\n Advanced English level.\n\nHow we do make your work (and your life) easier:\n\n 100% remote work.\n Hardware setup for you to work from home.\n Flexible hours - make your schedule.\n Paid parental leave, vacation & holidays.\n Diverse and multicultural work environment.\n An innovative environment with the structure and resources of a leading multinational.\n Excellent compensation \u2014 well above the market average.\n Here you can grow at the speed of your learning curve.\n\nOur people work remotely but with a consistent and robust culture that promotes diversity and teamwork. To continue being the leading software development company in Latin America, we want to ensure that every BairesDev member gets the best growth and professional development opportunities in a diverse, welcoming, and innovative environment.\n\nEvery BairesDev team member brings something unique to our company.\n\nWe want to hear your story. Apply now!"}, {"title": "Data Engineer (Remote)", "company_name": "Cascade Debt", "location": "United States (Remote)", "link": "/jobs/view/3755336571/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=pmUtn1j2zyPr25CNPh2RNw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3755336571", "description": "About the job\nDo you want to work in a small team where you can make a real difference in a company? Would you embrace the challenge of building a high impact platform that is used globally? Then we want to speak with you!\n\nAbout Cascade\n\nCascade is a fintech startup backed by Canadian and US investors that empowers companies to grow by democratizing access to institutional debt. We are building tools that modernize how companies raise and manage debt, lowering the barriers to entry to the $7 trillion specialty finance and alternative credit market for companies around the world. Founded just last year, we are gearing up for our next phase of technical development and are seeking a talented Data Engineer to lead the way.\n\nAbout The Role\n\nAs a Data Engineer you\u2019ll be an early member of a growing team building a pioneering platform in the debt infrastructure space. You will design, build, and launch efficient, scalable, and reliable data pipelines to move and transform data .\n\nWhat You\u2019ll Do\n\n Building databases, data lakes and data ingestion pipelines to integrate customer databases and datasets to our systems \n Ingesting financial datasets from external customers, then u pdating and maintaining accurate and complete data mappings to ensure that our product s are displaying high quality data \n M onitoring and alerting across our data pipelines in order to make sure that our data ingests are reliable and correct \n Translat ing business goals and stakeholder requirements into new data flows and deliver analytical insights ensuring that the data flows create real business value continuously \n\nSkills And Experience\n\n 2 + years experience in a data engineering or data analyst role \n E xperience with relational SQL and NoSQL databases, including PostgreSQL , MySQL, MariaDB, MongoDB, Bigtable, etc. \n E xperience working with ETL technologies , such as Databricks, Fivetran , or dbt \n Proficiency in writing SQL queries and knowledge of analytical data warehouses such as RedShift, BigQuery , and Snowflake \n Some experience with CI/CD automation such as GitHub Actions \n Developing APIs and integrating with 3rd party APIs \n Bonus: experience in fintech / SaaS / credit analysis \n\nPeople Who Thrive At Cascade Are\n\n Self-starters, who take the initiative to tackle challenges in a remote work environment \n Good communicators, who can operate without ego to discuss, learn, grow, and help others do the same \n Passionate about creating great digital experiences for users \n Problem solvers who are not satisfied with the status quo \n\nBenefits\n\nRemote: we are a remote-first company and are very flexible on hours as long as things get done \nHome-office: there\u2019s a $1000 USD home office allowance to set yourself up \nEquity: we expect you to have an owner-mentality, and have the equity plan to match \nBenefits: health, dental, vision, and more \nPerks: we offer free lunches weekly and off-site trips \nJob satisfaction: we offer autonomy, ample opportunities for mastery, and an opportunity to make a difference for companies around the world"}, {"title": "Data Engineer", "company_name": "ISC (Integrated Specialty Coverages, LLC)", "location": "Carlsbad, CA (Remote)", "link": "/jobs/view/3778293528/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ew7%2FfEuXSujs5IGabdqUyw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3778293528", "description": "About the job\nAbout Integrated Specialty Coverages\n\nIntegrated Specialty Coverages LLC (ISC) is a growth stage technology and data-driven insurance company leading innovation in the market.\n\nBacked by one of the leading private equity firms, KKR, and led by a forward-thinking management team, ISC is combining the worlds of insurance and technology to create an Insurtech powerhouse. As a leading online distributor of insurance products for a range of industries and \"Main Street USA\", we are looking for the right people to help us in our mission of achieving exponential growth. We strive to be the number one place to go for brokers and agents to source insurance. To accomplish this, we're building a digitally focused team that deeply understands the intersection between user experience, data, and AI/ML to optimize the way we engage with our customers and partners.\n\nJob Summary\n\nThe Data Engineer role will be the technical liaison between multiple groups including a data science team, the engineering team, product management, and business stakeholders. You do not need any insurance knowledge prior, however, you must quickly dive deep into the insurance world and ask questions to become a subject matter expert. You will be responsible for building a data platform to facilitate the data science team. You must be a self-starter that can build out features such as a data pipeline from scratch. There will be support from both engineering and data science for any buildout.\n\nJob Responsibilities\n\nCreate and maintain optimal data pipeline architecture. \nAssemble large, complex data sets that meet functional / non-functional business requirements. \nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. \nBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using AWS technologies, SQL, and Python. \nWork with stakeholders including the Executive, Product, Data Science, and Engineering teams to assist with data-related technical issues and support their data infrastructure needs. \nWork with data science and analytics teams to strive for greater functionality in our data systems. \n\nMinimum Qualifications:\n\nAdvanced working SQL knowledge and experience working with relational databases, strong query authoring (SQL) as well as working familiarity with a variety of databases (Snowflake, Redshift, MySQL, MSSQL, etc.)\nExperience building and optimizing data pipelines, architecture and data sets. \nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. \nBuild processes supporting data transformation, data structures, metadata, dependency, and workload management. \nA successful history of transforming, processing and extracting value from large disconnected, datasets from a variety of data sources (Flat files, Excel, databases, etc.)\nStrong analytic skills related to working with unstructured datasets. \nStrong project management and organizational skills. \nExperience supporting and working with cross-functional teams in a dynamic environment. \nAbility to mentor/guide/collaborate with other team members. \n\nWe are looking for a candidate with 3-5 years of experience in a Data Engineer role. They should also have experience using the following software/tools:\n\nExperience with relational and MPP databases, including Snowflake and MySQL. \nExperience developing software in an agile environment from the requirements stage to production. \nExperience with version control: git\nExperience with container technologies: Docker\nExperience with data pipeline and workflow management tools: Airflow, Jenkins, AWS Glue, Azkaban, Luigi, etc. \nExperience with AWS cloud services: EC2, ECS, Batch, S3, EMR, RDS, Redshift\nExperience with other cloud services: Snowflake, Airflow\nExperience with object-oriented/object function scripting languages: Python, Java, C++, etc. \nExperience with data modeling and data warehouse design\nExperience with data visualization tools (PowerBI, QuickSight)\n\nThe starting annual pay scale for this position is listed below. Actual starting pay will be based on factors such as skills, qualifications, training, and experience. In addition, the company offers comprehensive benefits including medical, dental and vision insurance, 401(k) plan with match, paid time off, and other benefits.\n\nISC's salary ranges are determined by role and level. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations and could be higher or lower based on a multitude of factors, including job-related skills, experience, and relevant education or training.\n\nNational Pay Range\n\n$100,000\u2014$110,000 USD\n\nBenefits of Working at ISC\n\nCompetitive vacation and flexible working arrangements\nComprehensive and inclusive health benefits\nA variety of professional development and mentorship opportunities\nChoice of technology whether at home or in the office\n\nISC's Ownership Behaviors\n\nDo the Right Thing *Be Relentless. Pursue Excellence. *Personal Responsibility & Own Outcomes\nTry Fast. Learn Fast. Fail Fast. Think Big. *Build Extraordinary Owners & Join Forces *Grit & Determination\n\nApplicants with disabilities may contact ISC HR department via e-mail, and other means to request and arrange accommodation. If you need assistance to accommodate a disability, you may request accommodation at any time. Please contact ISC at HR@ISCMGA.com.\n\nISC believes in creating long-term relationships by being responsive and relevant and by consistently delivering value to our community of customers. Specifically, with our employees, we focus on attracting, developing, and retaining the best talent for our business, challenging our people, demonstrating a \"can-do\" attitude, and fostering a collaborative and mutually supportive environment. ISC is an equal opportunity employer and a participant in the US federal E-Verify program (US).\n\nDiversity creates a healthier atmosphere: All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, gender, gender identity, sexual orientation, marital status, medical condition, genetic information, mental or physical disability, military or veteran status, or any other characteristic protected by local, state, or Federal law."}, {"title": "Data Engineer -Remote", "company_name": "RIT Solutions, Inc.", "location": "Denver, CO (Remote)", "link": "/jobs/view/3768007496/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=1hQH8EDaII1eJ7nQ9BACQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3768007496", "description": "About the job\nDuration: 6+ months CTH\n\nMust have a valid LinkedIn profile\n\nRequired Skills\n\n 3-6 years of experience (no more than 6 years)\n Start up environment experience\n Python, SQL, and DBT"}, {"title": "Big Data Engineer - W2", "company_name": "Megan Soft Inc", "location": "United States (Remote)", "link": "/jobs/view/3674845652/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fCAxh4N%2F4YZJ5aEIASaeeA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3674845652", "description": "About the job\nTitle: Big Data Engineer - W2\n\nLocation: Dearborn, MI (Remote)\n\nDuration: 12+ Months\n\nDirect Client: Ford Motors\n\nMust have skills: Should be able to handle GCP requirements\n\n\"Translates business requirements to build repeatable, sustainable, efficient, coded processes that can be productionized by Software Engineers and readily modified by other Data Engineer Creates POC processes in Dev/QA and works with Software Engineers to productionize and align best coding and process practices that maximize efficiency, speed, stability, system resources and capabilities Awareness of and compliance with data privacy, security, legal and contractual guidelines Incorporates data quality and privacy checks/alerts to minimize bad data being consumed by end users, models and dashboards, and to protect customer data; revises checks for new data issues Validates data products and pipelines are functioning as expected following system or application upgrades, source changes, etc. Works closely with Architects if required to align systems, tools and applications being utilized with business use case and performance requirements Provides data product support and maintenance Establishes, tracks and monitors KPIs related to specific data products and deliverable Participate in iteration cadence activities (Iteration Planning, task estimation, daily scrum, demos, etc.). Will be a part of on-call team to resolve critical production support tickets during business hours and off hours.\n\nSkills Required\n\nExperience with Big Data Technologies (Hadoop , ETL Tool, Spark. Hive etc..) Someone who understands Cloud and has experience working with Cloud Platforms (preferably GCP) Should be able to handle GCP requirements - ingesting into GCS/ Big Query, working with GCP components Hands-on in Data Flow, Cloud Storage (GCS), Big Query and GitHub. Proficient in Python and SQL.\n\nSkills Preferred\n\nManages ETL: uses programming and tools for data ingestion, configures pipelines, applies transformations and decoding, integrates and fuses data, moves and securely deliver Experience NoSQL databases\n\nAdditional Information\n\nIf the candidate is remote only, please indicate \"\"100% Remote\"\" under candidate's name on resume. Other similar schedule notations can be \"\"Local Candidate\"\" or \"\"Hybrid Candidate.\"\" ***HRA Test Required***\""}, {"title": "Remote Opportunity: Quantexa Data Engineer", "company_name": "SPAR Information Systems LLC", "location": "United States (Remote)", "link": "/jobs/view/3680073191/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ioykVCZSOQ7BK5%2FwRQHOWg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3680073191", "description": "About the job\nHello All, Hope you are doing great!! Please go through the job descriptions and let me know your interest. Role: Quantexa Data engineer  Location: Remote Duration: Long Term Contract Job Description: 1. End to End usage of Quantexa including guiding our client team on how to use from both front end and data ingestion Reason they are asking for Certification 2. Resolve Entity issues with data 3. Knowledge of Scala and Hive which requires coding experience using parquet 4. Advanced SQL knowledge requiring writing SQL queries 5. Experience having worked with GCP data lake 6. Quantexa certification required\n\nThanks & Regards,\n\nSatnam Singh\n\nDirect: 201 623 3660\n\nEmail : Satnam.singh@sparinfosys.com"}, {"title": "Remote Work - Need Staff Data Engineer - USC only", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3746283980/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=CvIuLBNO%2BPMGi4IXVRkP0A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3746283980", "description": "About the job\n100% Remote, client is in NY\n\nLook for strong job history, Python, Machine Learning, SQL, 10/10 comms, etc. \n\nMUST be an Engineer \n\nSummary\n\nWe're looking for an experienced Data Engineer to join our client's growing team of top notched IT professionals. You will work closely across multiple groups including Editorial and Audio, Marketing, Advertising, etc. to achieve their goals and objectives. The Data Engineering team builds tools throughout the stack while sharing knowledge across these departments.\n\nAs the company grows, they're looking for Data Engineer who will help solidify and expand our pipelines and maintain their data warehouse. Their business model is based on performing complex and very detailed analyses of how our products perform with their subscribers. You'll be responsible for working alongside analysts, product, marketing, and other teams to define new data collection components and measurement schemes that support each new product and feature.\n\nHow Can Add Value To Their Mission\n\nCreate and maintain data pipelines to provide insights and drive business decisions\nEstablish data warehousing strategy (ex. Kimball, Data Vault, etc.)\nMaintain data infrastructure on our AWS accounts\nCollaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.\nWrite unit/integration tests, contributes to engineering wiki, and documents work\nImplement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it\n\nWhat You'll Need To Succeed\n\n7+ years of industry experience measuring product performance and user behavior\nExperience working with a variety of data management technologies, including RedShift, Kinesis/Kafka, Glue, Spark, Postgres, Airflow, dbt, and others.\nExperience implementing BI reporting tools such as Looker\nExperience interfacing with engineers, product managers and analysts to understand data needs\nKnowledge of a variety of measurement beacons, SDKs, APIs including Google Analytics, Amplitude, Braze, Stripe, email service providers\nA commitment to building secure, resilient, fault-tolerant architectures, with clear documentation and procedures in place for support\nA focus on accuracy and detail as you build, ensuring data pipelines produce clear, predictable results\nUnderstanding of the typical metrics a subscription and advertising-supported business needs to measure success\nExperience with Client techniques as applied to behavioral segmentation or anomaly identification is a definite plus\nFamiliarity using developer tools that increase productivity and facilitate the development of resilient code (eg. Docker, CircleCI, Serverless) is a plus"}, {"title": "ENTRY LEVEL DATA ENGINEER (REMOTE)", "company_name": "SynergisticIT", "location": "Jacksonville, FL (Remote)", "link": "/jobs/view/3767587909/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=om7L3CnowiC4PLIcpZD70A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3767587909", "description": "About the job\nSYNERGISTICIT wants every candidate to know that the Job Market is Challenging and to stand out, you need to have exceptional skills and technologies and that's where we come in to make sure you get the attention which you need\n\nSynergisticIT understands the complex nature of the job market and how difficult it can be to secure a position, especially for fresh graduates. Therefore, we assist and help tech-savvies to convert their passions into professions. We go above and beyond to keep you working in your niche.\n\nAs we focus on long-term success, we provide complete career development solutions. From job search to upskilling portfolio and interview preparation, we can guide you at every step of your career.\n\nSynergisticIT spares no efforts to connect you with a large network of tech giants, including Google, Apple, PayPal, Dell, Cisco, Client, etc. Presently, we are actively looking for ENTRY LEVEL DATA ENGINEER a driven mindset. Get the right opportunity and gain experience in building web-centric solutions on Java.\n\nWho Should Apply : Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or anyone looking to make their career in IT Industry\n\nWe also assist in filing for STEM extension and H1b and Green card filing.\n\nCandidates who are serious about their future in the IT Industry and have set big goals for themselves.\n\nCandidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. We also offer Skill Enhancement Programs if the candidates are missing skills or experience which our clients need with great outcomes\n\nCandidates can benefit from skill enhancement if they fall into the below categories. If they are qualified with enough skills then no need for skill enhancement\n\nCandidates Who Lack Experience\n\nHave had a break in careers\n\nLack Technical Competency\n\ncandidates who want to get employed and make a career in the Tech Industry \n\nPlease Also Check The Below Links\n\nhttps://www.synergisticit.com/candidate-outcomes/\n\nhttps://www.synergisticit.com/java-track/\n\nhttps://www.synergisticit.com/data-science-track/\n\nhttps://www.synergisticit.com/which-is-the-best-option-for-tech-job-seekers-staffing-companies-consulting-companies-bootcamps-or-synergisticit/\n\nhttps://www.synergisticit.com/contact-us/\n\nIf the skills are not a match candidates can opt for Skill enhancement. Or their resume can be sent out to clients to see if responses are achievable\n\nREQUIRED SKILLS For Java/Software Programmers\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Core Java , javascript , C++ or software programming\nSpring boot, Microservices and REST API's experience\nExcellent written and verbal communication skills\n\nFor data Science/Machine learning\n\nRequired Skills\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Statistics, Python, data visualization tools\nExcellent written and verbal communication skills\n\nPreferred skills: NLP, Text mining, Tableau, Time series analysis\n\nTechnical skills are required by clients for selection even if its Junior or entry level position each additional Technical skill helps a candidate's resume to be picked by clients over other job seekers.\n\nClients hire candidates with the right technical skills which they need and reject candidates who lack the required technical skills.\n\nNo third party candidates or c2c candidates\n\nPlease apply to the posting\n\nNo phone calls please. Shortlisted candidates would be reached out"}, {"title": "Data Engineer - Remote | WFH", "company_name": "Get It Recruit - Information Technology", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3782241984/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=BgBf1kSAAX%2BSuw6inJsIVw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3782241984", "description": "About the job\nJoin an innovative team at a leading company in the engineering and manufacturing industry, where we are committed to leveraging data for continuous improvement and operational excellence. This hybrid position allows you to enjoy the flexibility of remote work on Mondays and Fridays.\n\nWhy Join Us\n\nAt our company, we prioritize your well-being and professional growth. We offer a comprehensive benefits package, including excellent health benefits, a 6% 401(k) match, generous paid vacation and sick time, 12 paid holidays, and more.\n\nRole Overview\n\nAs a Data Engineer, you will play a pivotal role in transforming raw data into actionable insights, driving improvements in engineering and manufacturing processes, enhancing product quality, and optimizing operational efficiency. Collaborate with dynamic cross-functional teams to extract, analyze, and visualize data, influencing strategic decisions through your valuable insights.\n\nKey Responsibilities\n\nData Collection and Integration:\n\nGather, clean, and integrate data from diverse sources, including production databases, sensor data, and external datasets.\n\nData Analysis\n\nConduct exploratory data analysis to identify patterns, trends, and anomalies.\n\nPerform statistical analysis and hypothesis testing to derive meaningful insights.\n\nData Visualization\n\nCreate informative and visually appealing dashboards and reports using tools like Tableau, Power BI, or Python libraries.\n\nPredictive Modeling\n\nDevelop predictive models to forecast manufacturing performance, quality, and operational outcomes.\n\nRoot Cause Analysis\n\nInvestigate production issues and quality concerns by analyzing data to identify root causes and recommend corrective actions.\n\nPerformance Monitoring\n\nEstablish and maintain key performance indicators (KPIs) to track manufacturing and engineering metrics.\n\nGenerate regular reports to communicate performance trends to stakeholders.\n\nCross-functional Collaboration\n\nCollaborate with engineering, manufacturing, and quality assurance teams to understand their data needs and provide analytical support.\n\nContinuous Improvement\n\nSuggest process improvements and optimizations based on data analysis.\n\nData Security And Compliance\n\nEnsure data security and compliance with relevant regulations, such as GDPR or industry-specific standards.\n\nInfrastructure And Development\n\nBuild and maintain the infrastructure for collecting, storing, and processing data.\n\nDesign, develop, and manage data pipelines, ETL processes, and data warehouses.\n\nRequired Experience\n\nProven experience as a Sr. Data Analyst and/or Data Engineer in an engineering or manufacturing environment.\n\nProficiency in data analysis tools and programming languages, such as Python, R, SQL, and Excel.\n\nStrong data visualization skills with tools like Tableau or Power BI.\n\nExcellent knowledge of statistical analysis and machine learning techniques.\n\nAbility to communicate complex findings and insights effectively to both technical and non-technical stakeholders.\n\nStrong problem-solving skills and attention to detail.\n\nFamiliarity with manufacturing processes and quality control is a plus.\n\nExperience with enterprise ERPs (SAP S4HANA, Oracle EBS), CRMs (Salesforce, Hubspot), and IoT.\n\nProficient in working with cloud computing platforms including AWS, GCP, and Azure.\n\nExpertise in configuring and optimizing cloud-based data solutions for scalability, reliability, and performance.\n\nBachelor's Degree in Data Science, Statistics, Engineering, or Computer Science Required.\n\nApply Now and be part of a team shaping the future of data-driven innovation in engineering and manufacturing!\n\nEmployment Type: Full-Time"}, {"title": "Entry Level Software Engineer - Data Backend Engineer (Remote - Canada)", "company_name": "Yelp", "location": "United States (Remote)", "link": "/jobs/view/3752881757/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fankeoxifvhpTzOYvowHYQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3752881757", "description": "About the job\nYelp engineering culture is driven by our values: we\u2019re a cooperative team that values individual authenticity and encourages creative solutions to problems. All new engineers deploy working code their first week, and we strive to broaden individual impact with support from managers, mentors, and teams. At the end of the day, we\u2019re all about helping our users, growing as engineers, and having fun in a collaborative environment.\n Interested in exploring data and finding innovative ways to collect it, curate it, and make it easily accessible for others to use? How about doing that at an enormous scale? Yelp\u2019s datasets contain billions of interactions between users and local businesses around the globe. If working with our vast amount of data sounds exciting, come and join us!\n\nThe Marketplace Data Observability team is responsible for managing, governing, and monitoring all of Yelp\u2019s Ads data to protect our revenue stream and also support offline analysis that aids in making long term company decisions. We build tools that help us to better understand our data quality, and we strive to uncover data issues before they impact our consumers.\n\nAs a Data Backend Engineer on this team, you will be responsible for building elegant and scalable data products that serve critical, up-to-date, structured information to support different types of analytics within Yelp. As a core contributor to our growing data modeling and data warehousing engineering efforts, you will help design and own mission-critical data flow pipelines and datastores to enable decisions including effective A/B testing and company investments.\n This opportunity is fully remote and does not require you to be located in any particular area in Canada. We welcome applicants from throughout Canada. We\u2019d love to have you apply, even if you don\u2019t feel you meet every single requirement in this posting. At Yelp, we\u2019re looking for great people, not just those who simply check off all the boxes.\n  Build systems that can effectively store and crunch terabytes of data.\nDesign and develop data models for efficient data storage, retrieval, and reporting.\nCreate and maintain conceptual, logical, and physical data models using industry-standard modeling tools.\nCollaborate with cross-functional teams, including engineers, data analysts, business analysts, and data scientists, to understand data requirements and translate them into effective data models.\nParticipate in data integration efforts, including ETL processes and data migration.\nStay up-to-date with industry best practices, emerging technologies, and trends related to data warehousing.\nSupport on-call rotations as needed to operate the team.\n  Understanding of high performing and scalable data systems.\nExperience in building and orchestrating ETL pipelines.\nExperience with Data Lake or Data Warehouse landscape.\nA hunger for tracking down root causes and fixing them in systematic ways.\nAbility to communicate effectively to technical and non-technical cohorts alike. \nExposure to some of the following technologies: Python, AWS Redshift, AWS Athena / Apache Presto, Big Data technologies (e.g S3, Hadoop, Hive, Spark, Flink, Kafka etc), NoSQL systems like Cassandra, DBT."}, {"title": "Data Engineer (Associate Consultant), June 2024", "company_name": "UDig", "location": "Nashville, TN (Remote)", "link": "/jobs/view/3716039960/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=XUjo40kU55HnDCqJ%2F3FdPg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3716039960", "description": "About the job\nCan UDig It? \n\nUDig designs, builds, and implements technology solutions that deliver on business objectives. People join our team because they are excited to have an impact on the future of our organization, passionate about delivering meaningful results, and energized by solving complex problems. Oh, and they like working with other driven, smart, and authentic people!\n\nUDig is proud to be recognized as a \"Best Places to Work\" in Virginia and Tennessee. We're committed to fostering a transparent environment where leadership is accessible, professional development opportunities are abundant, and you are empowered to achieve your career goals. You'll collaborate, develop deep relationships with teammates, and accelerate your success while having fun!\n\nIf you're graduating in May 2024 and excited by the idea of impacting organizations by building data solutions to achieve business goals, you should consider joining our growing technology team.\n\nHow Things Go Down\n\nAs a new, entry-level teammate, you'll take part in our \"Breaking Ground\" training program designed to help you quickly get up-to-speed on UDig's delivery approach within a project team. The program covers:\n\nUDig delivery process and methodologies such as gathering requirements and delivering projects using agile\nTechnical guidance and instruction on the best practices from Data leaders at UDig\nDelivering executive and technical presentations\nCoaching to prepare you to for success on your first client project at UDig\n\nA typical day might entail:\n\nProject standups and check-ins\nDesign and development of data pipelines, BI reports, and more\nPlanning and participating in project demos\nContributing to internal UDig projects\nCareer and job growth through UDig-provided training\nCreating long-lasting friendships with teammates through fun activities as well as your projects (including social events, winning a prize for guessing the weight of pumpkin, or sharing experience on cool tech side projects)\n\nUDig believes it is important for Associate Consultants going through our Breaking Ground program to reside in one of our major markets (Richmond, VA or Nashville, TN metro area) for at least the first year of employment.\n\nWant to learn more? Hear directly from our teammates by checking out our 'A Day in the Life' blog series: https://www.udig.com/digging-in/breaking-ground-2023/\n\nSound Interesting? Here's the Technical Piece\n\nBelow are a few boxes you should be able to check if\u202fyou're interested: Don't check every box? Go ahead and apply!\n\n1+ year of experience with relational database management systems such as Microsoft SQL Server, MySQL, or Oracle\nUnderstanding of the full lifecycle of design, development, and implementation of data solutions including architecture design, development, testing\nSome experience in ETL development or reporting is a plus\nAbility to present technical ideas and high-level concepts and solutions to internal and external team members with varying degrees of technical knowledge\nBachelor's Degree in in computer science, information technology, or any science or business discipline\nAbility to take your work seriously but also seriously enjoy a good time with your UDig colleagues\n\nWhat's in It For You? \n\nAt UDig, you will build your career alongside talented and experienced developers while gaining experience and having fun! We provide continuous challenges and professional development opportunities. Additionally, we offer: \n\nCompetitive salary, merit reviews, and career advancement paths\nFlexible, hybrid environment\nIndividual $1500 Training Budget\nRegular team building and social activities (virtual and/or in-person)\nTransparent culture with strong communication and access to leadership\nGreat benefits like\nGenerous Paid Time Off, company holidays, and parental leave\nMultiple Single and Family Health Insurance plans to choose from\nDental & Vision coverage\nShort-Term and Long-Term disability\nOptional accident and critical illness coverage\nMatching 401(k)\nand more!"}, {"title": "Expression of Interest: Data Engineer", "company_name": "Fingerprint for Success (F4S)", "location": "New York, NY (Remote)", "link": "/jobs/view/3787769880/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=l%2FEDpQqUNmCttGdIyTk7Cg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787769880", "description": "About the job\nWe are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.\n\nThe F4S Talent Pool is a pilot project designed to:\n\nHelp job seekers get discovered by our partners based on their anticipated hiring needs.\nProvide optional support and resources for job seekers in their career endeavors.\nHelp individuals understand, and bring out the best in themselves and each other.\n\nThe F4S Talent Pool process:\n\nOnce you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.\n\nAbout Fingerprint For Success (F4S)\n\nBacked by 20+ years of research, F4S\u2019s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high performing teams.\n\nKeep in mind, joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your F4S work style assessment to match the hiring needs of our partners.\n\nYour feedback is a gift! Write to us via:\n\nPowered by JazzHR\n\noWuPJ9lwq9"}, {"title": "Data Engineer Seattle, WA (remote)", "company_name": "Conch Technologies, Inc", "location": "United States (Remote)", "link": "/jobs/view/3765052211/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=SBETI78UMutCHuRp5FnP%2FA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3765052211", "description": "About the job\nHi,\n\nGreetings from Conch Technologies Inc\n\nData Engineering Lead \u2013 With Databricks and Unity Catalog\n\nLocation \u2013 Seattle, WA (remote)\n\nDuration \u2013 12 months\n\nNeed: Design and implement data architecture and strategy, leveraging technologies such as Databricks, Unity Catalog, Privacera, and Collibra.\n\nAs a Data Engineering Lead, you will play a pivotal role in defining and executing technical deliverables while assessing complexities and levels of effort. You'll engage with both vendor and internal teams to resolve technical challenges and ensure seamless collaboration. Your leadership will extend to planning and prioritization with the SCRUM master, facilitating effective coordination between onsite and offshore teams, and driving feature prioritization through collaboration with product managers.\n\nKey Responsibilities\n\n Lead the definition and breakdown of technical deliverables, assessing complexities and levels of effort.\n Collaborate with vendor and internal teams to address technical issues.\n Drive planning and prioritization in conjunction with the SCRUM master.\n Coordinate with onsite and offshore teams for timely project delivery.\n Collaborate with product managers to prioritize features, communicate with stakeholders, and clarify requirements.\n Collect and manage user-reported issues, ensuring prompt resolution and communication.\n Define comprehensive test plans, oversee their execution, and document results.\n\n--\n\nWith Regards,\n\nNagesh G\n\nMobile: 408-381-5645\n\nDesk: 901-313-3066\n\nEmail: nagesh@conchtech.com \n\nWeb: www.conchtech.com"}, {"title": "Remote Work - Need Azure Fabric Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3759325599/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=C3iVjmcR9wjUip1nNjUiCg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3759325599", "description": "About the job\nThe client is placing heavy emphasis on Microsoft and Azure Fabric experience\n\nInformation Management Engineer \u2013 Take the identified information management sources and assist migration to the Client Data Lake, Azure Fabric. The engineer will transform information management (de-dup, clean and enrich).\n\nTake the identified data sources and bring them into our Data Lake, Azure Fabric. Once the data is in, the engineer will need to be able to transform data (de-dup, clean and enrich the data). We are focused on using a medallion data model for data architecture. The tools we are working with are: Delta Parquet files, python, spark, notebooks, T-SQL, Fabric lakehouse/warehouse, Dataflows Gen2, Data Pipelines, Direct Lake, shortcuts, ADF."}, {"title": "Data Engineer", "company_name": "Catalist", "location": "Washington, DC (Remote)", "link": "/jobs/view/3744951279/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ui1Fk6ICmnPNqAlZDZAcwA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3744951279", "description": "About the job\nFor over 17 years, Catalist has been a leader in civic data and data science innovation. Our mission is to provide progressive organizations with the data, software, and services needed to better identify, understand, and communicate with the people they need to engage and mobilize. Our clients include the largest, most influential organizations in the U.S. active in civic engagement, advocacy, and political campaigns.\n\nCatalist is home to a dedicated, creative team of technologists, data scientists, and campaign experts committed to using our talents and technology to nurture a vibrant and growing progressive community.\n\nAs a Data Engineer at Catalist, you will work closely with the Analytics and Technology departments to design, build, support, and maintain various data pipelines and processes with an end goal of providing data and intelligence to the progressive community. This role primarily involves translating data architecture designs into functional processes, code, and systems.\n\nThe ideal candidate will be a highly motivated individual with excellent technical skills, a strong desire to learn new skills, and an interest in progressive politics. Catalist values creativity and problem-solving. Our work is on the cutting edge of data-driven politics, and your support will help Democratic candidates and progressive organizations conduct successful advocacy and electoral campaigns.\n\nThis position reports to the Deputy Chief Data Officer. The Data Engineer is a part of a growing Data team that supports all underlying work at Catalist.\n\nThis position is included in our CWA bargaining unit.\n\nPrinciple Duties & Responsibilities\n\nDevelop scalable, production processes, code, and systems for data ingest, transformation, modeling, and reporting across a variety of platforms\nTranslate mock-ups and designs into functional processes, code, and systems\nCreate architecture designs when needed\nProvide quality assurance and testing on processes, code, systems, and products\nBecome an internal subject matter expert on various datasets and support other Catalist departments/teams on usage of those datasets\nExecute ad-hoc data and database maintenance tasks as requiredProject manage cross-departmental efforts, with direct responsibility for stakeholder engagement, management, and execution of technical elements\n\n\nRequirements\n\nBS or BA in a technical field, or relevant experience\n1-2 years of experience working with SQL databases\nExperience with data cleaning or ETL processes\nExperience with distributed computing systems and/or distributed datastores (particularly the Hadoop ecosystem)\nExperience managing projects\nFamiliarity with Catalist data, progressive politics, voter files, and/or commercial data\nBackground check required\n\n\nPreferred Skills & Abilities\n\nWillingness to be a problem solver and produce results in a fast paced environment\nAbility to be creative and personable, and articulate ideas clearly\nExperience working with SQL databases\nProficiency with Python or another object-oriented programming language (R, Java, Scala, etc\u2026)\nExperience working in cloud environments (AWS, GCP, etc.)\nExperience working in command line environments such as Bash\nExperience with a version control tool such as git or github\n\n\nBenefits\n\nMedical, Dental, Vision, Prescription Drug\n\nCatalist offers Medical, Dental, Vision, and Prescription Drug coverage for eligible staff and their eligible dependents. Catalist\u2019s Medical plan is a comprehensive PPO program including Prescription Drug coverage with 85% of the premium paid by Catalist. Dental and Vision coverage is provided at no cost to employees.\n\nGroup Term Life Insurance and Long-Term & Short-Term Disability Coverage\n\nGroup Term Life Insurance and Long-Term and Short-Term Disability coverage is available for eligible staff. These benefits are provided at no cost to Catalist employees.\n\n401(k) Safe Harbor Plan\n\nA 401(k) Safe Harbor Plan is available to eligible staff with a 3% contribution from Catalist from the date of hire. Employees may contribute pre-tax or post-tax from their salary up to the legal limits set forth by the IRS.\n\nMedical and Dependent Care Flexible Spending Accounts (FSAs)\n\nCatalist offers an FSA Program that gives eligible staff the ability to pay out-of-pocket medical/dental/vision/child care expenses from pre-tax earnings.\n\nTransit Benefits\n\nCatalist also makes available a Transit benefit FSA program to eligible employees using pre-tax contributions with a company match.\n\nProfessional Development and Remote Work Expenses\n\nEligible employees may be reimbursed up to $750 each year for professional development / education and remote work expenses.\n\nStudent Loan PayDown or SaveUp\n\nCatalist offers a Student Loan PayDown and College SaveUp benefit for eligible staff.\n\nVacation, Personal Leave, Sick Leave Benefits\n\nCatalist offers generous vacation benefits to all eligible staff. Eligible employees also receive:\n\n 14 Paid Holidays\n Personal Days\n Sick Leave\n Parental Leave\n\n\nHybrid Office/Remote Work\n\nCertain positions at Catalist are eligible for Office/Remote Hybrid or full Remote status."}, {"title": "Remote Entry Level Data Analyst/Engineer", "company_name": "SynergisticIT", "location": "White Plains, NY (Remote)", "link": "/jobs/view/3774171423/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=q54sk1%2FRww0z%2BC%2BE8e0jQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774171423", "description": "About the job\nAt SynergisticIT, we're all about making connections. Whatever IT goals you have, our software programmers can help achieve those. Our Software development teams can take up turnkey projects and execute them in an effective and efficient manner. If you are looking to source talent our recruiters will find the ideal IT talent for your company. What's the secret to our success? Well, it all starts with taking quality time to listen to each client's specific needs. After we have a thorough grasp of your IT goals, we can better customize our Developments as per your specific needs. We can also tailor make recruiting programs to exceed your expectations. Since our founding in 2010, SynergisticIT's strategies have earned the company an enviable position in the software development, IT staffing and IT skill enhancement fields. SynergisticIT continues to work with hundreds of satisfied American clients with our software programmers working on our projects and after gaining hands on experience on cutting edge technologies moving to contribute their skills to great clients like Apple, Google, Client, Ebay, Paypal, Kroger, the Walt Disney Company and hundreds more. If you are tired of working with inefficient programmers who take a lot of time to ramp up we want you to try us. Our software programmers can hit the ground running and get you the maximum return on your investment. You have already tried the rest its time you tried the best. SynergisticIT - Home of the Best Data Scientists and Software Programmers in the Bay Area.\n\n Why Us ? \n\nSynergisticIT has a proven track record of successfully skill enhancement and staffing IT employees for some of the world's most iconic brands. Our team takes the time to fully understand every client's needs so we could best meet your IT staffing requirements. The knowledgeable staff at SynergisticIT is always more than happy to work with clients to ensure they reach their software development goals. Besides staffing, SynergisticIT is also committed to helping young IT professionals advance their career with a robust upskill program . Everyone who goes through SynergisticIT's program learns all the skills necessary to succeed in many IT fields ranging from Java to Machine Learning. Additionally, everyone trained at SynergisticIT has been through extensive mock and technical interview screenings to bolster their career prospects. Last, but certainly not least, SynergisticIT takes great care to respect the privacy considerations for every client. All companies who work with SynergisticIT can rest assured their confidential data is protected using the most up-to-date encryption technologies. SynergisticIT also complies with all the latest NDA agreements.\n\n REQUIRED SKILLS For Java /Software Programmers \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Time series analysis \n\n We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2022 and at Gartner Data Analytics Summit (Florida)-2023 \n\nOracle CloudWorld Event (OCW) Las Vegas 2022 | SynergisticIT - YouTube\n\n https://www.youtube.com/watch?v=OAFOhcGy9Z8 \n\n https://www.youtube.com/watch?v=EmO7NrWHkLM \n\n https://www.youtube.com/watch?v=NVBU9RYZ6UI \n\n https://www.youtube.com/watch?v=Yy74yvjatVg \n\nSynergisticIT at Gartner Data and Analytics Summit 2023 - YouTube\n\nFor preparing for interviews please visit  https://www.synergisticit.com/interview-questions/ \n\n We are looking for the right matching candidates for our clients \n\n Please apply via the job posting \n\n REQUIRED SKILLS For Java /Full Stack/Software Programmer \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning Positions \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Tensorflow \n\n If you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team. \n\n No phone calls please.  Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates"}, {"title": "Data Engineer", "company_name": "Anika Systems", "location": "Leesburg, VA (Remote)", "link": "/jobs/view/3787766893/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=mApL9JnN%2B%2Fr2%2BYUPlxRYOA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787766893", "description": "About the job\nAnika Systems is a fast growing, woman-owned small business that specializes in providing innovative IT solutions for federal government agencies. Our expertise lies in accelerating delivery in Data and Analytics, Intelligent Automation, Application Development, and IT Modernization. We are currently expanding our Federal team and are seeking a passionate and talented Data Engineer. This opportunity is 100% remote.\n\nMust be a U.S. Citizen with the ability to obtain and maintain a government suitability clearance\n\nResponsibilities\n\nDesigns, develops, builds, analyzes, evaluates, and installs database management systems to include database modeling and design, relational database architecture, metadata, and repository creation and configuration management.\nDefines and oversees database organizations, standards, controls, procedures, and documentation. Provides technical consulting in the definition, design, and creation of a database environment.\nAdvises applications development staff and users on data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nDesigns and implements databases with respect to access methods, access time, batch processes, device allocation, validation checks, organization, protection and security, documentation, and statistical methods.\nUses data mapping, data mining, and data transformational analysis tools to design and develop databases.\nDetermines data storage and optimum storage requirements.\nPrepares system requirements, source analysis, and process analyses and designs throughout the database implementation.\n\nRequired Skills And Experience\n\nBA/BS and 1 year of relevant experience\nExperience with DataBricks, SQL, Python\nApplications development experience with data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nExcellent communication skills (both oral and written) with process-oriented organizational skills to ensure project success\n\nDesired Skills And Experience\n\nDemonstrated successful development experience with the full life cycle of government database management systems including database modeling and design, relational database architecture, metadata and repository creation, and configuration management.\n\nPowered by JazzHR\n\nTlKWIorucJ"}, {"title": "Remote Work - Need Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3774712978/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=r3Bp%2F0qcOD2OpC5jpbUOcA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774712978", "description": "About the job\nTitle : Data Engineer\n\nLocation : Remote\n\nDuration : 6+ months to start (long term contract (work order are extended 2x year))\n\nPlease Fill out the skill matrix: Technology Some Awareness (1) Novice (2) Intermediate (3) Advanced (4) Expert (5) SQL Tableau Desktop /Creator Tableau API Integration AWS - S3 Glue AWS - Athena Redshift AWS - Lambda AWS - SNS/SQS msExcel\n\nProfile: Data Analyst will work closely with Architects and leads in the Interflow team, key business stakeholders including program staff to support the organizations effort in the areas of data Analysis and information management. Role should have strong experience in writing SQL queries to extract data for reports. Candidate should have experience working with large set of data. As these reports are very critical for the company and we are looking for an expert who is capable of handling, consuming and extracting large sets of data for analysis and reports.\n\nMust Haves\n\nAWS - Athena\nAWS - S3\nGlue\nhands-on development of reporting applications\nMS Excel\nRedShift\nSQL\nTableau Desktop /Creator\n\nNice\n\nAWS - Lambda\nAWS - SNS/SQS\nTableau API Integration\n\nSkills\n\nBA/BS required (major in an analytical field desired) \nA minimum 10+ years work-related experience. \nExperience in hands-on development of reporting applications for the Web in a professional environment \nThe Ethos of continuous improvement and interest in learning new things.\nStrong analytical thinking and structured problem-solving ability\nAbility to handle multiple projects and assignments simultaneously and effectively in a cross-functional team environment. \nVersed on the agile methodology and best practices. \nExcellent interpersonal and collaboration skill with the ability to work with a diverse set of colleagues, across functions, from different organizations, disciplines, etc. \nSelf-starter, ability to set priorities, work independently and attain goals"}, {"title": "Data Engineer", "company_name": "Team Remotely Inc", "location": "Houston, TX (Remote)", "link": "/jobs/view/3786249829/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=78lY5Jo%2FfyRPC6TQYCdvww%3D%3D&trk=flagship3_search_srp_jobs", "id": "3786249829", "description": "About the job\nThis is a remote position.\n\n Data Engineer (US/Canada Residents Only, 1 year experience, remote)\n\nTeam Remotely Inc. is a staffing and recruitment agency that offers a comprehensive solution for talent acquisition, including sourcing, vetting, pay rolling, and managing talent. Whether you need contract staffing, direct hire, direct sourcing, talent pools, or diversity initiatives, our model can support your hiring strategy.\n\n Hiring Type: Full-Time\n\n Base Salary: $55K-$70K Per Annum.\n\nResponsibilities\n\nDesigning user interface changes for web-based DB applications. \nReviewing application requirements and interface designs. \nDeveloping and implementing highly responsive user interface components using react concepts. \nWriting application interface codes using JavaScript following react.js workflows. \nTroubleshooting interface software and debugging application codes. \nDeveloping and implementing front-end architecture to support user interface concepts. \nMonitoring and improving front-end performance. \n\nQualifications And Experience\n\nBachelor\u2019s degree in computer science, information technology, or a similar field. \nPrevious experience working as a react.js developer. \nIn-depth knowledge of JavaScript, CSS, HTML, jQuery, and front-end languages. \nKnowledge of REACT tools including React.js, Webpack, Enzyme, Redux, and Flux. \nExperience with user interface design. \nKnowledge of performance testing frameworks, including Mocha and Jest. \nExperience with browser-based debugging and performance testing software. \nTroubleshooting skills. \nProject management skills. \nProblem-solving skills. \nVerbal communication skills. \n\n Why work with Team Remotely?\n\nTeam Remotely Inc. is a staffing platform offering a seamless experience for employers and candidates. Employers can post job openings and specify their requirements, while candidates can create profiles and upload resumes.\n\nThe team of Team Remotely continuously learns and adapts based on previous successful placements, constantly improving its matching capabilities. This ensures that the recommendations provided by Team Remotely are tailored and accurate, increasing the likelihood of a successful match between employers and candidates. By providing intelligent and data-driven solutions, they strive to enhance the efficiency and effectiveness of the hiring process, ultimately helping companies find the best talent and individuals find their dream jobs."}, {"title": "Data Engineer", "company_name": "ZAG Technical Services", "location": "United States (Remote)", "link": "/jobs/view/3771403464/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=Hf0VefrKXBFIL5MtHmY0Pw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3771403464", "description": "About the job\nWe are looking for a tech savvy Data Engineer to contribute to the building, operating, and scaling of next generation data platforms and tools that will power data-driven analytics capabilities throughout the ZAG products and services portfolio, spanning areas such as business intelligence, reporting, and data analytics.\n\nThe ideal team member is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. Must be comfortable supporting the data needs of multiple clients and teams, systems, and products within the modernization of ecosystems. The right-fit person will be excited by the prospect of optimizing or even re-designing data processes to support next generation of advanced analytics capabilities, which in-turn translates to implementing strategic enhancements to maintain competitive advantage in the industry.\n\nThe successful team member will leverage their proficiency to\u2026\n\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\nBuild the analytics processes required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Data Management and cloud based \u2018big data\u2019 technologies.\nBuild analytics tools that utilize technologies to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.\nCollaborate with client teams to identify and solution regional and local requirements within the context of the standard global business model for enterprise analytics.\nParticipates in meetings, discussions, strategy sessions where changes, improvements, and enhancements are proposed, evaluated, and approved. \n\n\nThe exceptional team member will have or demonstrate progressive experiences in\u2026 \n\nBachelor\u2019s degree or higher in Computer Science or related field\nExperience working with BI and data warehousing tools\nExperience working with enterprise data, where security is paramount and data governance is critical\nAbility to work independently and in a team environment, and effectively engage all levels of the organization\nUnderstanding of distributed systems driving large-scale data processing and analytics with a successful history of manipulating, processing, and extracting value from large, disconnected datasets.\nAbility to communicate effectively (listening, presenting, and questioning)\nStrong organizational, written, and communication skills\n\n\nAbout ZAG\n\nZAG Technical Services empowers our clients\u2019 success through intelligent business solutions that increase employee productivity, network security and team collaboration. We aim to ensure that our client\u2019s IT investments drive ROI, provide a competitive advantage and help businesses grow. We do this by living our core set of values: Integrity Foremost, Client-Centered, Accountable Always, Teamwork Throughout, and Exceptional is the Goal.\n\nWhat ZAG Has To Offer\n\nCompetitive compensation package \nBenefits including medical, dental, vision, 401K and life insurance \n\n\nZAG Technical Services is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all of our employees.\n\nZAG Technical Services is an E-Verify participant and will provide the Social Security Administration (SSA) and, if necessary, the Department of Homeland Security (DHS), with information from each new employee\u2019s Form I-9 to confirm work authorization."}, {"title": "Junior Data Engineer", "company_name": "HireMeFast LLC - Career Accelerator - Land A Job", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3786249797/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D&trk=flagship3_search_srp_jobs&lipi=urn%3Ali%3Apage%3Ad_flagship3_search_srp_jobs%3BQExogrHBQoSF9zZm%2B9kufw%3D%3D&lici=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D", "id": "3786249797", "description": "About the job\nThis is a remote position.\n\n Job Title: Junior Data Engineer\n\n Employment Type: Full-Time\n\n Salary: $64,000 - $76,000 per annum\n\n Experience Required:  Minimum 1 year of project experience\n\nAbout us: HireMeFast is a leading staffing and recruitment agency specializing in connecting businesses with top-tier talent across various industries. Our mission is to bridge the gap between exceptional candidates and organizations needing their skills, expertise, and unique qualities. Our team of experienced and dedicated recruitment specialists utilizes innovative sourcing strategies, and a vast network to identify and attract top talent. We conduct comprehensive procedures to ensure that only the most qualified candidates are presented to our clients.\n\nPosition Summary\n\nJoin the fast-paced, innovative, and collaborative environment focused on providing an AIOps platform that enhances the intelligence of the CVS Health infrastructure. Work closely with subject matter experts and colleagues to build and scale out machine learning and AI solutions that will detect, predict, and recommend solutions to correct issues before system impact and enhance efficiency, reliability, and performance of CVS Health\u2019s IT operations.\n\nKey Responsibilities include:\n\nData pipeline development: Designed, implemented, and managed data pipelines for extracting, transforming, and loading data from various sources into data lakes for processing, analytics, and correlation. \nData modeling: Create and maintain data models ensuring data quality, scalability, and efficiency \nDevelop and automate processes to clean, transform, and prepare data for analytics, ensuring data accuracy and consistency \nData Integration: Integrate data from disparate sources, both structured and unstructured to provide a unified view of key infrastructure platform and application data \nUtilize big data technologies such as Kafka to process and analyze large volumes of data efficiently \nImplement data security measures to protect sensitive information and ensure compliance with data and privacy regulation \nCreate/maintain documentation for data processes, data flows, and system configurations \nPerformance Optimization- Monitor and optimize data pipelines and systems for performance, scalability and cost-effectiveness \n\nCharacteristics of this role:\n\nTeam Player: Willing to teach, share knowledge, and work with others to make the team successful. \nCommunication: Exceptional verbal, written, organizational, presentation, and communication skills. \nCreativity: Ability to take written and verbal requirements and come up with other innovative ideas. \nAttention to detail: Systematically and accurately research future solutions and current problems. \nStrong work ethic: The innate drive to do work extremely well. \nPassion: A drive to deliver better products and services than expected to customers. \n\nRequired Qualifications\n\n2+ years of programming experience in languages such as Python, Java, SQL \n2+ years of experience with ETL tools and database management (relational, non-relational) \n2+ years of experience in data modeling techniques and tools to design efficient scalable data structures \nSkills in data quality assessment, data cleansing, and data validation \n\nQualifications\n\nKnowledge of big data technologies and cloud platforms \nExperience with technologies like PySpark, Databricks, and Azure Synapse. \n\nEducation\n\nBachelor\u2019s degree in Computer Science, Information Technology, or related field, or equivalent working experience\n\nWhy HireMeFast LLC?\n\nAt HireMeFast, we understand that finding the right individuals to join your team is crucial for success and growth of your organization. We are committed to streamlining the hiring process for our clients, ensuring they have access to a diverse pool of highly qualified candidates who are a perfect fit for their specific needs."}, {"title": "Data Engineer  - Need candidates from Texas ( REMOTE )", "company_name": "Pulivarthi Group (PG)", "location": "United States (Remote)", "link": "/jobs/view/3770685212/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=9gdMwuif%2BpB%2BCQYvQi3imQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3770685212", "description": "About the job\nFollow us on Linkedin: https://www.linkedin.com/company/pulivarthigroup/\n\nPulivarthi Group LLC is a Global Staffing & IT Technology Solutions company, with our prime focus of providing world class solutions to our customers with the right talent. We combine the expertise of our team and the culture of your company to help you with the solution that is affordable and innovative using high quality standards and technologies.\n\nWe\u2019ve served some of the largest healthcare, financial services, and government entities in the U.S.\n\nData Engineer - FT only - Azure SQL with data lakes experience\n\nData Engineer -\n\nthe resource requirement for this Data Engineer role needed more clarification.\n\nOur customer is requesting a resource who knows Azure CI/CD (Continuous Integration / Continuous Deployment) to build and integrate their DevOps pipeline.\n\nOnce the DevOps pipelines are mapped, they need the Data Engineer to connect to all the Data Sources, primarily their Data Lake.\n\nThe Customer (based in Houston) is interested in a resource in a time zone not too far removed from the Central Time Zone to allow real time collaboration."}, {"title": "Data Engineer", "company_name": "Otter Products", "location": "Fort Collins, CO (Remote)", "link": "/jobs/view/3766728302/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=77U3U7CAi1PxgDmpI3%2BiNQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3766728302", "description": "About the job\nOverview \n\nOtter Products is currently recruiting for a Data Engineer to join our Data team! You can be based in our Fort Collins, CO office with flexibility to work a portion of your time remotely, or 100% Remote in the U.S.\n\nThe Data Engineer applies professional experience, concepts, and company objectives toward building and operationalizing the minimally inclusive data necessary for the enterprise data initiatives following industry standard practices and tools. The bulk of the Data Engineer's work would be in designing, managing and optimizing data pipelines, then moving these data pipelines effectively into production for key data and analytics consumers like business/data analysts, data scientists or other stakeholder that needs curated data for data and analytics use cases across the enterprise. The Data Engineer also ensures compliance with data governance and data security requirements while creating, improving and operationalizing these integrated and reusable data pipelines to enable faster data access, integrated data reuse and vastly improved time-to-solution for data and analytics initiatives. Additionally, the Data Engineer will be expected to collaborate with other data team members and data consumers working on the models and algorithms developed by them toward optimization for data quality, security, and governance to put them into production leading to potentially large productivity gains.\n\n About Otter Products \n\nOtter Products, we grow to give. From our founder's garage in 1998 to the global technology leader we are today, Otter Products continues to drive growth through innovation.\n\nThrough our industry-leading brands - OtterBox, Liviri and OtterCares - we provide our partners the number-one selling and most trusted products in our categories. Our philanthropic spirit is the foundation on which we foster our partner relationships, allowing us to grow and to give - together.\n\nBy way of our charitable arm, the OtterCares Foundation, we support our communities and invest in our future through education that inspires kids to change the world.\n\nAnd even as our global community of Otters continues to grow, our founder's core values are still at the heart of everything we do. We measure our success by our ability to give back to our communities and strengthen opportunities for all.\n\nFor more information visit otterproducts.com\n\n Responsibilities \n\nComplete analysis, design and development of BI solutions using Azure SQL Server\nFamiliarity with Data Warehouse concepts\nExperience coding the extraction, transformation, and loading (ETL) processes\nDatabase development primarily in SSIS, Data Factory and SQL\nPartner with the business to determine end user database/reporting needs and requirements\nGenerate ad hoc reports using Power BI or SSRS\nCollaborate with other developers to create and implement best approach solution(s)\nTroubleshoot Azure tools, systems, and software\nReview queries for performance issues, making changes as needed\nCollaborate with team to performance-tune Azure application as necessary\nAssist with the analysis and extraction of relevant information from large amounts of historical business data to feed data science initiatives\nParticipate and support the design and documentation of processes for large scale data analyses, model development, model validation and model implementation\nSupport and maintain a positive safety culture by following all safety policies and procedures and actively contributing to a safe working environment\nOther duties as required\n\n Qualifications \n\nBachelor's degree required. Degree in Computer Science or Mathematics preferred\nMinimum of four years of experience in an IT or analytical role required. Experience in database development, report writing and/or statistics preferred.\n\n EEO \n\nOtter Products, LLC is an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, age, marital status, pregnancy, sex, sexual orientation, gender, gender identity or expression, national origin, disability, veteran status, or any other characteristic or status protected by law.\n\n For US Based Roles Only - Compensation Range Minimum \n\nUSD $90,000.00/Yr.\n\n For US Based Roles Only - Compensation Range Maximum \n\nUSD $120,000.00/Yr.\n\n Additional Total Rewards \n\nProfit Sharing Program Eligible, Benefits Eligible - Full Time- check out otterproducts.com/careers/why for more info"}, {"title": "Software Engineer, Distributed Systems", "company_name": "Hightouch", "location": "United States (Remote)", "link": "/jobs/view/3628572539/?eBP=CwEAAAGMd4QO8APODyE_-mMOPhYyQGg5ZjVbPwq_3V7DK0B2bLAf10pcedM9S2T71X5i4TiJuEwOzWBbSiTKbLuk-VD-_Y-dN5uyf8YpYkHYqEYFxySMS9zr2hRjNrxX9gXbxssh35TFK9YTNV2WeH2Ncb675JgFbJTgNWSZySFC6rrdJGmdSDTr2q5d3VoxORz3GjsDbzjeyZQCvQ07RqYw7oujWSgslycs2LMCY_-N0NEj6e0PvbaYKiZ_5IW34bGVSYLXpHwqa8oOcNttYZ90UA3E_7sDSFzABdIVlfrp5zzERrIYy5sh2MIInxHdDoKgQw8VmOn5uM4S0tXUl-i7LnN0_W0NfPBmLiVDNuveIOm4MfDNPxfe5QKPk6FUAVpvgbcoJn9yRyE&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=rph%2FT6Q6Vkp6YoggJf9cJQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3628572539", "description": "About the job\nAbout Hightouch\n\nHightouch\u2019s mission is to empower everyone to take action on their data. Through our Reverse ETL platform, business and data users can seamlessly sync data from where it resides, such as warehouses and databases, to where it is needed, including operational systems and SaaS tools. Traditionally, acting on data has required engineering time and bandwidth, and left most business users stuck with charts and reports that are unable to take automated action on their data. With Hightouch, every business user, without writing any code, can activate data to streamline critical processes, improve marketing performance, and scale operations.\n\nOur team operates with a focus on making a meaningful impact for our customers. We believe in approaching challenges with a first principles thinking mindset, moving quickly and embracing our value of efficient execution, and treating each other with compassion and kindness. We look for team members that are strong communicators, have a growth mindset, and are motivated and persistent in achieving our goals.\n\nHundreds of companies use Hightouch, including Spotify, Ramp, Retool, NBA, Plaid, and Betterment. We\u2019re based in San Francisco, are remote-friendly, and backed by leading investors such as Amplify Partners, ICONIQ Growth, Bain Capital Ventures, Y-Combinator, and Afore Capital.\n\nAbout The Role\n\nWe are looking for a distributed systems engineer to work on the systems that power the syncing engine our customers and other engineering teams rely on. This presents an exciting challenge where you can apply your expertise in distributed systems, performance optimization, and troubleshooting to push the boundaries of what is possible and meaningful improve our ability to move massive amounts of customer data. This role also provides a unique opportunity to work on a multi-cloud and multi-region infrastructure that supports a global customer base.\n\nWe believe in enabling our engineers to do their best work for our customers by giving them extremely high levels of ownership and autonomy. This comes in different forms: you will own and deliver projects from start to finish, you will work directly with customers to solve their hardest scaling problems, and you will have a lot of influence over what we work on as a team and company.\n\nSome of the problems we\u2019ll be working on include:\n\nSync Speed: Customers want to sync a lot of data to important destinations like Facebook and Snapchat, which requires us to analyze every part of our syncing process and find where we can optimize to sync data more quickly\nStreaming Syncing: We currently sync data from sources in batch only, but envision a world where we also do syncs in real-time and support streaming sources like webhooks and queues\nScalability and Reliability: As part of our rapid growth, we\u2019re always evaluating current future bottlenecks in scaling and reliability, and architecting for the next order of magnitude of growth\nPersonalization API: Our Personalization API product provides a low latency caching layer on top of data warehouses that enable customers to use their data warehouse for real-time personalization use cases with \nMulti-Region and Multi-Cloud: Supporting our multi-region and multi-cloud backend, including extending it to launch Hightouch on in new regions to support data residency requirements of our global customer base\n\nWe are looking for talented, intellectually curious, and motivated individuals who are interested in tackling the problems above. This is a senior role, but we focus on impact and potential for growth more than years of experience. The salary range for this position is $170,000 - $240,000 USD per year, which is location independent in accordance with our remote-first policy.\n\nAbout You\n\nYou are an engineer with a passion for solving hard technical problems that generate real value for customers. You\u2019re motivated by high ownership and are comfortable in a fast-paced, startup environment.\n\nYou have experience and comfort with distributed systems and high-scale systems or the ability and desire to build this experience quickly. This experience can take different forms:\n\nYou\u2019ve built syncing engines that process and move terabytes of data per day in a scalable way\nYou\u2019ve created event collection or data streaming services that power company critical systems\nYou\u2019ve owned systems that have undergone orders of magnitude of growth and helped them scale accordingly\nYou have relevant skills and are a fast learner who is excited to take on the challenges we face\n\nInterview Process\n\nOur goal with the interview process is to balance speed with giving both parties opportunities to assess whether there is a strong mutual fit. We will ask you questions, but we want you to ask us questions! Our technical interviews focus on how you design systems because we believe this is the best way for us to see how you work and for you to see how we collaborate. We don\u2019t ask you to write code to solve technical brainteasers that don\u2019t appear in your day to day job.\n\nRecruiter screen [30m]: Introductory call with our recruiting team to get to know each other and see if the role could be a good mutual fit.\nSystems design screen [45m]: Designing a data processing feature end-to-end.\nHiring manager interview [30m]: Chat with hiring manager about past experiences and future operating preferences to assess fit on company values and operating principles.\nSystems design interview [90m]: Work with the interviewer to architect a system at a conceptual level. The problem will be at a pretty high level - and have both product and customer requirements as well as technical."}, {"title": "Entry-Level Data Scientist Engineer", "company_name": "Phoenix Recruitment LLC", "location": "Texas City, TX (Remote)", "link": "/jobs/view/3788727625/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=8iwtytHsCu4wMwJ4fj3pxw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3788727625", "description": "About the job\nThis is a remote position.\n\nTitle - Entry-Level Data Scientist Engineer, 1 year of project experience\n\nEmployment Type: Full-time\n\nBase Salary: $60K-$70K\n\nPhoenix Recruitment offers a variety of recruiting services to assist both employers and employees. They specialize in marketing open positions, recruiting, and helping employers to find qualified candidates across various industries. Phoenix Recruitment has expertise in streamlining the hiring process. They can help ensure that the process is efficient, well organized, and compliant with relevant regulations.\n\nSkills and Abilities:\n\nStrong knowledge of R or Python for data analysis and modeling. \nProficiency in statistical programs such as R, SAS, MATLAB, or Python. \nFamiliarity with spreadsheets (VBA) and database applications (Access, Oracle, SQL, or equivalent technology). \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \nAbility to learn new methods quickly and work under deadlines. \nExcellent teamwork and communication skills. \nStrong analytical and problem-solving abilities. \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \n\nPreferred:\n\nKnowledge of actuarial concepts and life, health, and/or annuity products. \nExperience with statistical modeling techniques such as GLM, Decision Trees, Time Series, Regression, etc. \nFamiliarity with Microsoft DeployR. \nExposure to insurance risk analysis. \nBasic experience in computational finance, econometrics, statistics, and math. \nKnowledge of SQL and VBA. \nFamiliarity with R or Python for predictive modeling \n\nWhy Phoenix Recruitment LLC?\n\nPhoenix Recruitment often has an extensive network of employers and candidates. This network allows them to tap into a pool of qualified candidates and connect them with suitable job opportunities. They can also leverage their connections to help employers find the right talent efficiently. Outsourcing the recruitment process to a specialized agency can save your time and resources, avoid delays, reduce administrative burdens, and increase the chances of finding the right fit for your organization."}, {"title": "Data Engineer - Remote Work", "company_name": "BairesDev", "location": "United States (Remote)", "link": "/jobs/view/3783162297/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=OtoCrAAHZFomVup04fy35g%3D%3D&trk=flagship3_search_srp_jobs", "id": "3783162297", "description": "About the job\nWho We are\n\nBairesDev is proud to be the fastest-growing company in America. With people in five continents and world-class clients, we are only as strong as the multicultural teams at the heart of our business. To consistently deliver the highest quality solutions to our clients, we only hire the Top 1% of the best talents and nurture their professional growth on exciting projects.\n\nWe are looking for a Data Engineer to join our Development team and participate in different projects made up of multicultural teams distributed throughout the world.\n\nWhat You Will Do\n\n Interview other Data Scientists and Data engineers to vet their technical abilities.\n Input data from those interviews into the product and help develop a predictive, fair, and enjoyable solution.\n Work together with developers, tech leads, and solution architects to build applications.\n Improve existing structures by adding new functionalities or proposing technological updates so that as a result, the impact of your contribution is significant in the core of each business.\n Improve the user experience on scalable and high availability platforms, contributing to the key differential of each business.\n\nHere\u2019s What We Are Looking For\n\n 5+ years of experience working as a developer.\n 3+ years of experience working as a Data Scientist / Data Engineer.\n Proficient with analysis, troubleshooting, and problem-solving.\n Hands-on experience with managing data loads and data quality.\n Advanced English level.\n\nHow we do make your work (and your life) easier:\n\n 100% remote work.\n Hardware setup for you to work from home.\n Flexible hours - make your schedule.\n Paid parental leave, vacation & holidays.\n Diverse and multicultural work environment.\n An innovative environment with the structure and resources of a leading multinational.\n Excellent compensation \u2014 well above the market average.\n Here you can grow at the speed of your learning curve.\n\nOur people work remotely but with a consistent and robust culture that promotes diversity and teamwork. To continue being the leading software development company in Latin America, we want to ensure that every BairesDev member gets the best growth and professional development opportunities in a diverse, welcoming, and innovative environment.\n\nEvery BairesDev team member brings something unique to our company.\n\nWe want to hear your story. Apply now!"}, {"title": "Data Engineer (Remote)", "company_name": "Cascade Debt", "location": "United States (Remote)", "link": "/jobs/view/3755336571/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=pmUtn1j2zyPr25CNPh2RNw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3755336571", "description": "About the job\nDo you want to work in a small team where you can make a real difference in a company? Would you embrace the challenge of building a high impact platform that is used globally? Then we want to speak with you!\n\nAbout Cascade\n\nCascade is a fintech startup backed by Canadian and US investors that empowers companies to grow by democratizing access to institutional debt. We are building tools that modernize how companies raise and manage debt, lowering the barriers to entry to the $7 trillion specialty finance and alternative credit market for companies around the world. Founded just last year, we are gearing up for our next phase of technical development and are seeking a talented Data Engineer to lead the way.\n\nAbout The Role\n\nAs a Data Engineer you\u2019ll be an early member of a growing team building a pioneering platform in the debt infrastructure space. You will design, build, and launch efficient, scalable, and reliable data pipelines to move and transform data .\n\nWhat You\u2019ll Do\n\n Building databases, data lakes and data ingestion pipelines to integrate customer databases and datasets to our systems \n Ingesting financial datasets from external customers, then u pdating and maintaining accurate and complete data mappings to ensure that our product s are displaying high quality data \n M onitoring and alerting across our data pipelines in order to make sure that our data ingests are reliable and correct \n Translat ing business goals and stakeholder requirements into new data flows and deliver analytical insights ensuring that the data flows create real business value continuously \n\nSkills And Experience\n\n 2 + years experience in a data engineering or data analyst role \n E xperience with relational SQL and NoSQL databases, including PostgreSQL , MySQL, MariaDB, MongoDB, Bigtable, etc. \n E xperience working with ETL technologies , such as Databricks, Fivetran , or dbt \n Proficiency in writing SQL queries and knowledge of analytical data warehouses such as RedShift, BigQuery , and Snowflake \n Some experience with CI/CD automation such as GitHub Actions \n Developing APIs and integrating with 3rd party APIs \n Bonus: experience in fintech / SaaS / credit analysis \n\nPeople Who Thrive At Cascade Are\n\n Self-starters, who take the initiative to tackle challenges in a remote work environment \n Good communicators, who can operate without ego to discuss, learn, grow, and help others do the same \n Passionate about creating great digital experiences for users \n Problem solvers who are not satisfied with the status quo \n\nBenefits\n\nRemote: we are a remote-first company and are very flexible on hours as long as things get done \nHome-office: there\u2019s a $1000 USD home office allowance to set yourself up \nEquity: we expect you to have an owner-mentality, and have the equity plan to match \nBenefits: health, dental, vision, and more \nPerks: we offer free lunches weekly and off-site trips \nJob satisfaction: we offer autonomy, ample opportunities for mastery, and an opportunity to make a difference for companies around the world"}, {"title": "Data Engineer", "company_name": "ISC (Integrated Specialty Coverages, LLC)", "location": "Carlsbad, CA (Remote)", "link": "/jobs/view/3778293528/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ew7%2FfEuXSujs5IGabdqUyw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3778293528", "description": "About the job\nAbout Integrated Specialty Coverages\n\nIntegrated Specialty Coverages LLC (ISC) is a growth stage technology and data-driven insurance company leading innovation in the market.\n\nBacked by one of the leading private equity firms, KKR, and led by a forward-thinking management team, ISC is combining the worlds of insurance and technology to create an Insurtech powerhouse. As a leading online distributor of insurance products for a range of industries and \"Main Street USA\", we are looking for the right people to help us in our mission of achieving exponential growth. We strive to be the number one place to go for brokers and agents to source insurance. To accomplish this, we're building a digitally focused team that deeply understands the intersection between user experience, data, and AI/ML to optimize the way we engage with our customers and partners.\n\nJob Summary\n\nThe Data Engineer role will be the technical liaison between multiple groups including a data science team, the engineering team, product management, and business stakeholders. You do not need any insurance knowledge prior, however, you must quickly dive deep into the insurance world and ask questions to become a subject matter expert. You will be responsible for building a data platform to facilitate the data science team. You must be a self-starter that can build out features such as a data pipeline from scratch. There will be support from both engineering and data science for any buildout.\n\nJob Responsibilities\n\nCreate and maintain optimal data pipeline architecture. \nAssemble large, complex data sets that meet functional / non-functional business requirements. \nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. \nBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using AWS technologies, SQL, and Python. \nWork with stakeholders including the Executive, Product, Data Science, and Engineering teams to assist with data-related technical issues and support their data infrastructure needs. \nWork with data science and analytics teams to strive for greater functionality in our data systems. \n\nMinimum Qualifications:\n\nAdvanced working SQL knowledge and experience working with relational databases, strong query authoring (SQL) as well as working familiarity with a variety of databases (Snowflake, Redshift, MySQL, MSSQL, etc.)\nExperience building and optimizing data pipelines, architecture and data sets. \nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. \nBuild processes supporting data transformation, data structures, metadata, dependency, and workload management. \nA successful history of transforming, processing and extracting value from large disconnected, datasets from a variety of data sources (Flat files, Excel, databases, etc.)\nStrong analytic skills related to working with unstructured datasets. \nStrong project management and organizational skills. \nExperience supporting and working with cross-functional teams in a dynamic environment. \nAbility to mentor/guide/collaborate with other team members. \n\nWe are looking for a candidate with 3-5 years of experience in a Data Engineer role. They should also have experience using the following software/tools:\n\nExperience with relational and MPP databases, including Snowflake and MySQL. \nExperience developing software in an agile environment from the requirements stage to production. \nExperience with version control: git\nExperience with container technologies: Docker\nExperience with data pipeline and workflow management tools: Airflow, Jenkins, AWS Glue, Azkaban, Luigi, etc. \nExperience with AWS cloud services: EC2, ECS, Batch, S3, EMR, RDS, Redshift\nExperience with other cloud services: Snowflake, Airflow\nExperience with object-oriented/object function scripting languages: Python, Java, C++, etc. \nExperience with data modeling and data warehouse design\nExperience with data visualization tools (PowerBI, QuickSight)\n\nThe starting annual pay scale for this position is listed below. Actual starting pay will be based on factors such as skills, qualifications, training, and experience. In addition, the company offers comprehensive benefits including medical, dental and vision insurance, 401(k) plan with match, paid time off, and other benefits.\n\nISC's salary ranges are determined by role and level. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations and could be higher or lower based on a multitude of factors, including job-related skills, experience, and relevant education or training.\n\nNational Pay Range\n\n$100,000\u2014$110,000 USD\n\nBenefits of Working at ISC\n\nCompetitive vacation and flexible working arrangements\nComprehensive and inclusive health benefits\nA variety of professional development and mentorship opportunities\nChoice of technology whether at home or in the office\n\nISC's Ownership Behaviors\n\nDo the Right Thing *Be Relentless. Pursue Excellence. *Personal Responsibility & Own Outcomes\nTry Fast. Learn Fast. Fail Fast. Think Big. *Build Extraordinary Owners & Join Forces *Grit & Determination\n\nApplicants with disabilities may contact ISC HR department via e-mail, and other means to request and arrange accommodation. If you need assistance to accommodate a disability, you may request accommodation at any time. Please contact ISC at HR@ISCMGA.com.\n\nISC believes in creating long-term relationships by being responsive and relevant and by consistently delivering value to our community of customers. Specifically, with our employees, we focus on attracting, developing, and retaining the best talent for our business, challenging our people, demonstrating a \"can-do\" attitude, and fostering a collaborative and mutually supportive environment. ISC is an equal opportunity employer and a participant in the US federal E-Verify program (US).\n\nDiversity creates a healthier atmosphere: All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, gender, gender identity, sexual orientation, marital status, medical condition, genetic information, mental or physical disability, military or veteran status, or any other characteristic protected by local, state, or Federal law."}, {"title": "Data Engineer -Remote", "company_name": "RIT Solutions, Inc.", "location": "Denver, CO (Remote)", "link": "/jobs/view/3768007496/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=1hQH8EDaII1eJ7nQ9BACQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3768007496", "description": "About the job\nDuration: 6+ months CTH\n\nMust have a valid LinkedIn profile\n\nRequired Skills\n\n 3-6 years of experience (no more than 6 years)\n Start up environment experience\n Python, SQL, and DBT"}, {"title": "Big Data Engineer - W2", "company_name": "Megan Soft Inc", "location": "United States (Remote)", "link": "/jobs/view/3674845652/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fCAxh4N%2F4YZJ5aEIASaeeA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3674845652", "description": "About the job\nTitle: Big Data Engineer - W2\n\nLocation: Dearborn, MI (Remote)\n\nDuration: 12+ Months\n\nDirect Client: Ford Motors\n\nMust have skills: Should be able to handle GCP requirements\n\n\"Translates business requirements to build repeatable, sustainable, efficient, coded processes that can be productionized by Software Engineers and readily modified by other Data Engineer Creates POC processes in Dev/QA and works with Software Engineers to productionize and align best coding and process practices that maximize efficiency, speed, stability, system resources and capabilities Awareness of and compliance with data privacy, security, legal and contractual guidelines Incorporates data quality and privacy checks/alerts to minimize bad data being consumed by end users, models and dashboards, and to protect customer data; revises checks for new data issues Validates data products and pipelines are functioning as expected following system or application upgrades, source changes, etc. Works closely with Architects if required to align systems, tools and applications being utilized with business use case and performance requirements Provides data product support and maintenance Establishes, tracks and monitors KPIs related to specific data products and deliverable Participate in iteration cadence activities (Iteration Planning, task estimation, daily scrum, demos, etc.). Will be a part of on-call team to resolve critical production support tickets during business hours and off hours.\n\nSkills Required\n\nExperience with Big Data Technologies (Hadoop , ETL Tool, Spark. Hive etc..) Someone who understands Cloud and has experience working with Cloud Platforms (preferably GCP) Should be able to handle GCP requirements - ingesting into GCS/ Big Query, working with GCP components Hands-on in Data Flow, Cloud Storage (GCS), Big Query and GitHub. Proficient in Python and SQL.\n\nSkills Preferred\n\nManages ETL: uses programming and tools for data ingestion, configures pipelines, applies transformations and decoding, integrates and fuses data, moves and securely deliver Experience NoSQL databases\n\nAdditional Information\n\nIf the candidate is remote only, please indicate \"\"100% Remote\"\" under candidate's name on resume. Other similar schedule notations can be \"\"Local Candidate\"\" or \"\"Hybrid Candidate.\"\" ***HRA Test Required***\""}, {"title": "Remote Opportunity: Quantexa Data Engineer", "company_name": "SPAR Information Systems LLC", "location": "United States (Remote)", "link": "/jobs/view/3680073191/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ioykVCZSOQ7BK5%2FwRQHOWg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3680073191", "description": "About the job\nHello All, Hope you are doing great!! Please go through the job descriptions and let me know your interest. Role: Quantexa Data engineer  Location: Remote Duration: Long Term Contract Job Description: 1. End to End usage of Quantexa including guiding our client team on how to use from both front end and data ingestion Reason they are asking for Certification 2. Resolve Entity issues with data 3. Knowledge of Scala and Hive which requires coding experience using parquet 4. Advanced SQL knowledge requiring writing SQL queries 5. Experience having worked with GCP data lake 6. Quantexa certification required\n\nThanks & Regards,\n\nSatnam Singh\n\nDirect: 201 623 3660\n\nEmail : Satnam.singh@sparinfosys.com"}, {"title": "Remote Work - Need Staff Data Engineer - USC only", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3746283980/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=CvIuLBNO%2BPMGi4IXVRkP0A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3746283980", "description": "About the job\n100% Remote, client is in NY\n\nLook for strong job history, Python, Machine Learning, SQL, 10/10 comms, etc. \n\nMUST be an Engineer \n\nSummary\n\nWe're looking for an experienced Data Engineer to join our client's growing team of top notched IT professionals. You will work closely across multiple groups including Editorial and Audio, Marketing, Advertising, etc. to achieve their goals and objectives. The Data Engineering team builds tools throughout the stack while sharing knowledge across these departments.\n\nAs the company grows, they're looking for Data Engineer who will help solidify and expand our pipelines and maintain their data warehouse. Their business model is based on performing complex and very detailed analyses of how our products perform with their subscribers. You'll be responsible for working alongside analysts, product, marketing, and other teams to define new data collection components and measurement schemes that support each new product and feature.\n\nHow Can Add Value To Their Mission\n\nCreate and maintain data pipelines to provide insights and drive business decisions\nEstablish data warehousing strategy (ex. Kimball, Data Vault, etc.)\nMaintain data infrastructure on our AWS accounts\nCollaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.\nWrite unit/integration tests, contributes to engineering wiki, and documents work\nImplement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it\n\nWhat You'll Need To Succeed\n\n7+ years of industry experience measuring product performance and user behavior\nExperience working with a variety of data management technologies, including RedShift, Kinesis/Kafka, Glue, Spark, Postgres, Airflow, dbt, and others.\nExperience implementing BI reporting tools such as Looker\nExperience interfacing with engineers, product managers and analysts to understand data needs\nKnowledge of a variety of measurement beacons, SDKs, APIs including Google Analytics, Amplitude, Braze, Stripe, email service providers\nA commitment to building secure, resilient, fault-tolerant architectures, with clear documentation and procedures in place for support\nA focus on accuracy and detail as you build, ensuring data pipelines produce clear, predictable results\nUnderstanding of the typical metrics a subscription and advertising-supported business needs to measure success\nExperience with Client techniques as applied to behavioral segmentation or anomaly identification is a definite plus\nFamiliarity using developer tools that increase productivity and facilitate the development of resilient code (eg. Docker, CircleCI, Serverless) is a plus"}, {"title": "ENTRY LEVEL DATA ENGINEER (REMOTE)", "company_name": "SynergisticIT", "location": "Jacksonville, FL (Remote)", "link": "/jobs/view/3767587909/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=om7L3CnowiC4PLIcpZD70A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3767587909", "description": "About the job\nSYNERGISTICIT wants every candidate to know that the Job Market is Challenging and to stand out, you need to have exceptional skills and technologies and that's where we come in to make sure you get the attention which you need\n\nSynergisticIT understands the complex nature of the job market and how difficult it can be to secure a position, especially for fresh graduates. Therefore, we assist and help tech-savvies to convert their passions into professions. We go above and beyond to keep you working in your niche.\n\nAs we focus on long-term success, we provide complete career development solutions. From job search to upskilling portfolio and interview preparation, we can guide you at every step of your career.\n\nSynergisticIT spares no efforts to connect you with a large network of tech giants, including Google, Apple, PayPal, Dell, Cisco, Client, etc. Presently, we are actively looking for ENTRY LEVEL DATA ENGINEER a driven mindset. Get the right opportunity and gain experience in building web-centric solutions on Java.\n\nWho Should Apply : Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or anyone looking to make their career in IT Industry\n\nWe also assist in filing for STEM extension and H1b and Green card filing.\n\nCandidates who are serious about their future in the IT Industry and have set big goals for themselves.\n\nCandidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. We also offer Skill Enhancement Programs if the candidates are missing skills or experience which our clients need with great outcomes\n\nCandidates can benefit from skill enhancement if they fall into the below categories. If they are qualified with enough skills then no need for skill enhancement\n\nCandidates Who Lack Experience\n\nHave had a break in careers\n\nLack Technical Competency\n\ncandidates who want to get employed and make a career in the Tech Industry \n\nPlease Also Check The Below Links\n\nhttps://www.synergisticit.com/candidate-outcomes/\n\nhttps://www.synergisticit.com/java-track/\n\nhttps://www.synergisticit.com/data-science-track/\n\nhttps://www.synergisticit.com/which-is-the-best-option-for-tech-job-seekers-staffing-companies-consulting-companies-bootcamps-or-synergisticit/\n\nhttps://www.synergisticit.com/contact-us/\n\nIf the skills are not a match candidates can opt for Skill enhancement. Or their resume can be sent out to clients to see if responses are achievable\n\nREQUIRED SKILLS For Java/Software Programmers\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Core Java , javascript , C++ or software programming\nSpring boot, Microservices and REST API's experience\nExcellent written and verbal communication skills\n\nFor data Science/Machine learning\n\nRequired Skills\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Statistics, Python, data visualization tools\nExcellent written and verbal communication skills\n\nPreferred skills: NLP, Text mining, Tableau, Time series analysis\n\nTechnical skills are required by clients for selection even if its Junior or entry level position each additional Technical skill helps a candidate's resume to be picked by clients over other job seekers.\n\nClients hire candidates with the right technical skills which they need and reject candidates who lack the required technical skills.\n\nNo third party candidates or c2c candidates\n\nPlease apply to the posting\n\nNo phone calls please. Shortlisted candidates would be reached out"}, {"title": "Data Engineer - Remote | WFH", "company_name": "Get It Recruit - Information Technology", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3782241984/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=BgBf1kSAAX%2BSuw6inJsIVw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3782241984", "description": "About the job\nJoin an innovative team at a leading company in the engineering and manufacturing industry, where we are committed to leveraging data for continuous improvement and operational excellence. This hybrid position allows you to enjoy the flexibility of remote work on Mondays and Fridays.\n\nWhy Join Us\n\nAt our company, we prioritize your well-being and professional growth. We offer a comprehensive benefits package, including excellent health benefits, a 6% 401(k) match, generous paid vacation and sick time, 12 paid holidays, and more.\n\nRole Overview\n\nAs a Data Engineer, you will play a pivotal role in transforming raw data into actionable insights, driving improvements in engineering and manufacturing processes, enhancing product quality, and optimizing operational efficiency. Collaborate with dynamic cross-functional teams to extract, analyze, and visualize data, influencing strategic decisions through your valuable insights.\n\nKey Responsibilities\n\nData Collection and Integration:\n\nGather, clean, and integrate data from diverse sources, including production databases, sensor data, and external datasets.\n\nData Analysis\n\nConduct exploratory data analysis to identify patterns, trends, and anomalies.\n\nPerform statistical analysis and hypothesis testing to derive meaningful insights.\n\nData Visualization\n\nCreate informative and visually appealing dashboards and reports using tools like Tableau, Power BI, or Python libraries.\n\nPredictive Modeling\n\nDevelop predictive models to forecast manufacturing performance, quality, and operational outcomes.\n\nRoot Cause Analysis\n\nInvestigate production issues and quality concerns by analyzing data to identify root causes and recommend corrective actions.\n\nPerformance Monitoring\n\nEstablish and maintain key performance indicators (KPIs) to track manufacturing and engineering metrics.\n\nGenerate regular reports to communicate performance trends to stakeholders.\n\nCross-functional Collaboration\n\nCollaborate with engineering, manufacturing, and quality assurance teams to understand their data needs and provide analytical support.\n\nContinuous Improvement\n\nSuggest process improvements and optimizations based on data analysis.\n\nData Security And Compliance\n\nEnsure data security and compliance with relevant regulations, such as GDPR or industry-specific standards.\n\nInfrastructure And Development\n\nBuild and maintain the infrastructure for collecting, storing, and processing data.\n\nDesign, develop, and manage data pipelines, ETL processes, and data warehouses.\n\nRequired Experience\n\nProven experience as a Sr. Data Analyst and/or Data Engineer in an engineering or manufacturing environment.\n\nProficiency in data analysis tools and programming languages, such as Python, R, SQL, and Excel.\n\nStrong data visualization skills with tools like Tableau or Power BI.\n\nExcellent knowledge of statistical analysis and machine learning techniques.\n\nAbility to communicate complex findings and insights effectively to both technical and non-technical stakeholders.\n\nStrong problem-solving skills and attention to detail.\n\nFamiliarity with manufacturing processes and quality control is a plus.\n\nExperience with enterprise ERPs (SAP S4HANA, Oracle EBS), CRMs (Salesforce, Hubspot), and IoT.\n\nProficient in working with cloud computing platforms including AWS, GCP, and Azure.\n\nExpertise in configuring and optimizing cloud-based data solutions for scalability, reliability, and performance.\n\nBachelor's Degree in Data Science, Statistics, Engineering, or Computer Science Required.\n\nApply Now and be part of a team shaping the future of data-driven innovation in engineering and manufacturing!\n\nEmployment Type: Full-Time"}, {"title": "Entry Level Software Engineer - Data Backend Engineer (Remote - Canada)", "company_name": "Yelp", "location": "United States (Remote)", "link": "/jobs/view/3752881757/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fankeoxifvhpTzOYvowHYQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3752881757", "description": "About the job\nYelp engineering culture is driven by our values: we\u2019re a cooperative team that values individual authenticity and encourages creative solutions to problems. All new engineers deploy working code their first week, and we strive to broaden individual impact with support from managers, mentors, and teams. At the end of the day, we\u2019re all about helping our users, growing as engineers, and having fun in a collaborative environment.\n Interested in exploring data and finding innovative ways to collect it, curate it, and make it easily accessible for others to use? How about doing that at an enormous scale? Yelp\u2019s datasets contain billions of interactions between users and local businesses around the globe. If working with our vast amount of data sounds exciting, come and join us!\n\nThe Marketplace Data Observability team is responsible for managing, governing, and monitoring all of Yelp\u2019s Ads data to protect our revenue stream and also support offline analysis that aids in making long term company decisions. We build tools that help us to better understand our data quality, and we strive to uncover data issues before they impact our consumers.\n\nAs a Data Backend Engineer on this team, you will be responsible for building elegant and scalable data products that serve critical, up-to-date, structured information to support different types of analytics within Yelp. As a core contributor to our growing data modeling and data warehousing engineering efforts, you will help design and own mission-critical data flow pipelines and datastores to enable decisions including effective A/B testing and company investments.\n This opportunity is fully remote and does not require you to be located in any particular area in Canada. We welcome applicants from throughout Canada. We\u2019d love to have you apply, even if you don\u2019t feel you meet every single requirement in this posting. At Yelp, we\u2019re looking for great people, not just those who simply check off all the boxes.\n  Build systems that can effectively store and crunch terabytes of data.\nDesign and develop data models for efficient data storage, retrieval, and reporting.\nCreate and maintain conceptual, logical, and physical data models using industry-standard modeling tools.\nCollaborate with cross-functional teams, including engineers, data analysts, business analysts, and data scientists, to understand data requirements and translate them into effective data models.\nParticipate in data integration efforts, including ETL processes and data migration.\nStay up-to-date with industry best practices, emerging technologies, and trends related to data warehousing.\nSupport on-call rotations as needed to operate the team.\n  Understanding of high performing and scalable data systems.\nExperience in building and orchestrating ETL pipelines.\nExperience with Data Lake or Data Warehouse landscape.\nA hunger for tracking down root causes and fixing them in systematic ways.\nAbility to communicate effectively to technical and non-technical cohorts alike. \nExposure to some of the following technologies: Python, AWS Redshift, AWS Athena / Apache Presto, Big Data technologies (e.g S3, Hadoop, Hive, Spark, Flink, Kafka etc), NoSQL systems like Cassandra, DBT."}, {"title": "Data Engineer (Associate Consultant), June 2024", "company_name": "UDig", "location": "Nashville, TN (Remote)", "link": "/jobs/view/3716039960/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=XUjo40kU55HnDCqJ%2F3FdPg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3716039960", "description": "About the job\nCan UDig It? \n\nUDig designs, builds, and implements technology solutions that deliver on business objectives. People join our team because they are excited to have an impact on the future of our organization, passionate about delivering meaningful results, and energized by solving complex problems. Oh, and they like working with other driven, smart, and authentic people!\n\nUDig is proud to be recognized as a \"Best Places to Work\" in Virginia and Tennessee. We're committed to fostering a transparent environment where leadership is accessible, professional development opportunities are abundant, and you are empowered to achieve your career goals. You'll collaborate, develop deep relationships with teammates, and accelerate your success while having fun!\n\nIf you're graduating in May 2024 and excited by the idea of impacting organizations by building data solutions to achieve business goals, you should consider joining our growing technology team.\n\nHow Things Go Down\n\nAs a new, entry-level teammate, you'll take part in our \"Breaking Ground\" training program designed to help you quickly get up-to-speed on UDig's delivery approach within a project team. The program covers:\n\nUDig delivery process and methodologies such as gathering requirements and delivering projects using agile\nTechnical guidance and instruction on the best practices from Data leaders at UDig\nDelivering executive and technical presentations\nCoaching to prepare you to for success on your first client project at UDig\n\nA typical day might entail:\n\nProject standups and check-ins\nDesign and development of data pipelines, BI reports, and more\nPlanning and participating in project demos\nContributing to internal UDig projects\nCareer and job growth through UDig-provided training\nCreating long-lasting friendships with teammates through fun activities as well as your projects (including social events, winning a prize for guessing the weight of pumpkin, or sharing experience on cool tech side projects)\n\nUDig believes it is important for Associate Consultants going through our Breaking Ground program to reside in one of our major markets (Richmond, VA or Nashville, TN metro area) for at least the first year of employment.\n\nWant to learn more? Hear directly from our teammates by checking out our 'A Day in the Life' blog series: https://www.udig.com/digging-in/breaking-ground-2023/\n\nSound Interesting? Here's the Technical Piece\n\nBelow are a few boxes you should be able to check if\u202fyou're interested: Don't check every box? Go ahead and apply!\n\n1+ year of experience with relational database management systems such as Microsoft SQL Server, MySQL, or Oracle\nUnderstanding of the full lifecycle of design, development, and implementation of data solutions including architecture design, development, testing\nSome experience in ETL development or reporting is a plus\nAbility to present technical ideas and high-level concepts and solutions to internal and external team members with varying degrees of technical knowledge\nBachelor's Degree in in computer science, information technology, or any science or business discipline\nAbility to take your work seriously but also seriously enjoy a good time with your UDig colleagues\n\nWhat's in It For You? \n\nAt UDig, you will build your career alongside talented and experienced developers while gaining experience and having fun! We provide continuous challenges and professional development opportunities. Additionally, we offer: \n\nCompetitive salary, merit reviews, and career advancement paths\nFlexible, hybrid environment\nIndividual $1500 Training Budget\nRegular team building and social activities (virtual and/or in-person)\nTransparent culture with strong communication and access to leadership\nGreat benefits like\nGenerous Paid Time Off, company holidays, and parental leave\nMultiple Single and Family Health Insurance plans to choose from\nDental & Vision coverage\nShort-Term and Long-Term disability\nOptional accident and critical illness coverage\nMatching 401(k)\nand more!"}, {"title": "Expression of Interest: Data Engineer", "company_name": "Fingerprint for Success (F4S)", "location": "New York, NY (Remote)", "link": "/jobs/view/3787769880/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=l%2FEDpQqUNmCttGdIyTk7Cg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787769880", "description": "About the job\nWe are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.\n\nThe F4S Talent Pool is a pilot project designed to:\n\nHelp job seekers get discovered by our partners based on their anticipated hiring needs.\nProvide optional support and resources for job seekers in their career endeavors.\nHelp individuals understand, and bring out the best in themselves and each other.\n\nThe F4S Talent Pool process:\n\nOnce you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.\n\nAbout Fingerprint For Success (F4S)\n\nBacked by 20+ years of research, F4S\u2019s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high performing teams.\n\nKeep in mind, joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your F4S work style assessment to match the hiring needs of our partners.\n\nYour feedback is a gift! Write to us via:\n\nPowered by JazzHR\n\noWuPJ9lwq9"}, {"title": "Data Engineer Seattle, WA (remote)", "company_name": "Conch Technologies, Inc", "location": "United States (Remote)", "link": "/jobs/view/3765052211/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=SBETI78UMutCHuRp5FnP%2FA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3765052211", "description": "About the job\nHi,\n\nGreetings from Conch Technologies Inc\n\nData Engineering Lead \u2013 With Databricks and Unity Catalog\n\nLocation \u2013 Seattle, WA (remote)\n\nDuration \u2013 12 months\n\nNeed: Design and implement data architecture and strategy, leveraging technologies such as Databricks, Unity Catalog, Privacera, and Collibra.\n\nAs a Data Engineering Lead, you will play a pivotal role in defining and executing technical deliverables while assessing complexities and levels of effort. You'll engage with both vendor and internal teams to resolve technical challenges and ensure seamless collaboration. Your leadership will extend to planning and prioritization with the SCRUM master, facilitating effective coordination between onsite and offshore teams, and driving feature prioritization through collaboration with product managers.\n\nKey Responsibilities\n\n Lead the definition and breakdown of technical deliverables, assessing complexities and levels of effort.\n Collaborate with vendor and internal teams to address technical issues.\n Drive planning and prioritization in conjunction with the SCRUM master.\n Coordinate with onsite and offshore teams for timely project delivery.\n Collaborate with product managers to prioritize features, communicate with stakeholders, and clarify requirements.\n Collect and manage user-reported issues, ensuring prompt resolution and communication.\n Define comprehensive test plans, oversee their execution, and document results.\n\n--\n\nWith Regards,\n\nNagesh G\n\nMobile: 408-381-5645\n\nDesk: 901-313-3066\n\nEmail: nagesh@conchtech.com \n\nWeb: www.conchtech.com"}, {"title": "Remote Work - Need Azure Fabric Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3759325599/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=C3iVjmcR9wjUip1nNjUiCg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3759325599", "description": "About the job\nThe client is placing heavy emphasis on Microsoft and Azure Fabric experience\n\nInformation Management Engineer \u2013 Take the identified information management sources and assist migration to the Client Data Lake, Azure Fabric. The engineer will transform information management (de-dup, clean and enrich).\n\nTake the identified data sources and bring them into our Data Lake, Azure Fabric. Once the data is in, the engineer will need to be able to transform data (de-dup, clean and enrich the data). We are focused on using a medallion data model for data architecture. The tools we are working with are: Delta Parquet files, python, spark, notebooks, T-SQL, Fabric lakehouse/warehouse, Dataflows Gen2, Data Pipelines, Direct Lake, shortcuts, ADF."}, {"title": "Data Engineer", "company_name": "Catalist", "location": "Washington, DC (Remote)", "link": "/jobs/view/3744951279/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ui1Fk6ICmnPNqAlZDZAcwA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3744951279", "description": "About the job\nFor over 17 years, Catalist has been a leader in civic data and data science innovation. Our mission is to provide progressive organizations with the data, software, and services needed to better identify, understand, and communicate with the people they need to engage and mobilize. Our clients include the largest, most influential organizations in the U.S. active in civic engagement, advocacy, and political campaigns.\n\nCatalist is home to a dedicated, creative team of technologists, data scientists, and campaign experts committed to using our talents and technology to nurture a vibrant and growing progressive community.\n\nAs a Data Engineer at Catalist, you will work closely with the Analytics and Technology departments to design, build, support, and maintain various data pipelines and processes with an end goal of providing data and intelligence to the progressive community. This role primarily involves translating data architecture designs into functional processes, code, and systems.\n\nThe ideal candidate will be a highly motivated individual with excellent technical skills, a strong desire to learn new skills, and an interest in progressive politics. Catalist values creativity and problem-solving. Our work is on the cutting edge of data-driven politics, and your support will help Democratic candidates and progressive organizations conduct successful advocacy and electoral campaigns.\n\nThis position reports to the Deputy Chief Data Officer. The Data Engineer is a part of a growing Data team that supports all underlying work at Catalist.\n\nThis position is included in our CWA bargaining unit.\n\nPrinciple Duties & Responsibilities\n\nDevelop scalable, production processes, code, and systems for data ingest, transformation, modeling, and reporting across a variety of platforms\nTranslate mock-ups and designs into functional processes, code, and systems\nCreate architecture designs when needed\nProvide quality assurance and testing on processes, code, systems, and products\nBecome an internal subject matter expert on various datasets and support other Catalist departments/teams on usage of those datasets\nExecute ad-hoc data and database maintenance tasks as requiredProject manage cross-departmental efforts, with direct responsibility for stakeholder engagement, management, and execution of technical elements\n\n\nRequirements\n\nBS or BA in a technical field, or relevant experience\n1-2 years of experience working with SQL databases\nExperience with data cleaning or ETL processes\nExperience with distributed computing systems and/or distributed datastores (particularly the Hadoop ecosystem)\nExperience managing projects\nFamiliarity with Catalist data, progressive politics, voter files, and/or commercial data\nBackground check required\n\n\nPreferred Skills & Abilities\n\nWillingness to be a problem solver and produce results in a fast paced environment\nAbility to be creative and personable, and articulate ideas clearly\nExperience working with SQL databases\nProficiency with Python or another object-oriented programming language (R, Java, Scala, etc\u2026)\nExperience working in cloud environments (AWS, GCP, etc.)\nExperience working in command line environments such as Bash\nExperience with a version control tool such as git or github\n\n\nBenefits\n\nMedical, Dental, Vision, Prescription Drug\n\nCatalist offers Medical, Dental, Vision, and Prescription Drug coverage for eligible staff and their eligible dependents. Catalist\u2019s Medical plan is a comprehensive PPO program including Prescription Drug coverage with 85% of the premium paid by Catalist. Dental and Vision coverage is provided at no cost to employees.\n\nGroup Term Life Insurance and Long-Term & Short-Term Disability Coverage\n\nGroup Term Life Insurance and Long-Term and Short-Term Disability coverage is available for eligible staff. These benefits are provided at no cost to Catalist employees.\n\n401(k) Safe Harbor Plan\n\nA 401(k) Safe Harbor Plan is available to eligible staff with a 3% contribution from Catalist from the date of hire. Employees may contribute pre-tax or post-tax from their salary up to the legal limits set forth by the IRS.\n\nMedical and Dependent Care Flexible Spending Accounts (FSAs)\n\nCatalist offers an FSA Program that gives eligible staff the ability to pay out-of-pocket medical/dental/vision/child care expenses from pre-tax earnings.\n\nTransit Benefits\n\nCatalist also makes available a Transit benefit FSA program to eligible employees using pre-tax contributions with a company match.\n\nProfessional Development and Remote Work Expenses\n\nEligible employees may be reimbursed up to $750 each year for professional development / education and remote work expenses.\n\nStudent Loan PayDown or SaveUp\n\nCatalist offers a Student Loan PayDown and College SaveUp benefit for eligible staff.\n\nVacation, Personal Leave, Sick Leave Benefits\n\nCatalist offers generous vacation benefits to all eligible staff. Eligible employees also receive:\n\n 14 Paid Holidays\n Personal Days\n Sick Leave\n Parental Leave\n\n\nHybrid Office/Remote Work\n\nCertain positions at Catalist are eligible for Office/Remote Hybrid or full Remote status."}, {"title": "Remote Entry Level Data Analyst/Engineer", "company_name": "SynergisticIT", "location": "White Plains, NY (Remote)", "link": "/jobs/view/3774171423/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=q54sk1%2FRww0z%2BC%2BE8e0jQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774171423", "description": "About the job\nAt SynergisticIT, we're all about making connections. Whatever IT goals you have, our software programmers can help achieve those. Our Software development teams can take up turnkey projects and execute them in an effective and efficient manner. If you are looking to source talent our recruiters will find the ideal IT talent for your company. What's the secret to our success? Well, it all starts with taking quality time to listen to each client's specific needs. After we have a thorough grasp of your IT goals, we can better customize our Developments as per your specific needs. We can also tailor make recruiting programs to exceed your expectations. Since our founding in 2010, SynergisticIT's strategies have earned the company an enviable position in the software development, IT staffing and IT skill enhancement fields. SynergisticIT continues to work with hundreds of satisfied American clients with our software programmers working on our projects and after gaining hands on experience on cutting edge technologies moving to contribute their skills to great clients like Apple, Google, Client, Ebay, Paypal, Kroger, the Walt Disney Company and hundreds more. If you are tired of working with inefficient programmers who take a lot of time to ramp up we want you to try us. Our software programmers can hit the ground running and get you the maximum return on your investment. You have already tried the rest its time you tried the best. SynergisticIT - Home of the Best Data Scientists and Software Programmers in the Bay Area.\n\n Why Us ? \n\nSynergisticIT has a proven track record of successfully skill enhancement and staffing IT employees for some of the world's most iconic brands. Our team takes the time to fully understand every client's needs so we could best meet your IT staffing requirements. The knowledgeable staff at SynergisticIT is always more than happy to work with clients to ensure they reach their software development goals. Besides staffing, SynergisticIT is also committed to helping young IT professionals advance their career with a robust upskill program . Everyone who goes through SynergisticIT's program learns all the skills necessary to succeed in many IT fields ranging from Java to Machine Learning. Additionally, everyone trained at SynergisticIT has been through extensive mock and technical interview screenings to bolster their career prospects. Last, but certainly not least, SynergisticIT takes great care to respect the privacy considerations for every client. All companies who work with SynergisticIT can rest assured their confidential data is protected using the most up-to-date encryption technologies. SynergisticIT also complies with all the latest NDA agreements.\n\n REQUIRED SKILLS For Java /Software Programmers \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Time series analysis \n\n We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2022 and at Gartner Data Analytics Summit (Florida)-2023 \n\nOracle CloudWorld Event (OCW) Las Vegas 2022 | SynergisticIT - YouTube\n\n https://www.youtube.com/watch?v=OAFOhcGy9Z8 \n\n https://www.youtube.com/watch?v=EmO7NrWHkLM \n\n https://www.youtube.com/watch?v=NVBU9RYZ6UI \n\n https://www.youtube.com/watch?v=Yy74yvjatVg \n\nSynergisticIT at Gartner Data and Analytics Summit 2023 - YouTube\n\nFor preparing for interviews please visit  https://www.synergisticit.com/interview-questions/ \n\n We are looking for the right matching candidates for our clients \n\n Please apply via the job posting \n\n REQUIRED SKILLS For Java /Full Stack/Software Programmer \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning Positions \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Tensorflow \n\n If you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team. \n\n No phone calls please.  Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates"}, {"title": "Data Engineer", "company_name": "Anika Systems", "location": "Leesburg, VA (Remote)", "link": "/jobs/view/3787766893/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=mApL9JnN%2B%2Fr2%2BYUPlxRYOA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787766893", "description": "About the job\nAnika Systems is a fast growing, woman-owned small business that specializes in providing innovative IT solutions for federal government agencies. Our expertise lies in accelerating delivery in Data and Analytics, Intelligent Automation, Application Development, and IT Modernization. We are currently expanding our Federal team and are seeking a passionate and talented Data Engineer. This opportunity is 100% remote.\n\nMust be a U.S. Citizen with the ability to obtain and maintain a government suitability clearance\n\nResponsibilities\n\nDesigns, develops, builds, analyzes, evaluates, and installs database management systems to include database modeling and design, relational database architecture, metadata, and repository creation and configuration management.\nDefines and oversees database organizations, standards, controls, procedures, and documentation. Provides technical consulting in the definition, design, and creation of a database environment.\nAdvises applications development staff and users on data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nDesigns and implements databases with respect to access methods, access time, batch processes, device allocation, validation checks, organization, protection and security, documentation, and statistical methods.\nUses data mapping, data mining, and data transformational analysis tools to design and develop databases.\nDetermines data storage and optimum storage requirements.\nPrepares system requirements, source analysis, and process analyses and designs throughout the database implementation.\n\nRequired Skills And Experience\n\nBA/BS and 1 year of relevant experience\nExperience with DataBricks, SQL, Python\nApplications development experience with data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nExcellent communication skills (both oral and written) with process-oriented organizational skills to ensure project success\n\nDesired Skills And Experience\n\nDemonstrated successful development experience with the full life cycle of government database management systems including database modeling and design, relational database architecture, metadata and repository creation, and configuration management.\n\nPowered by JazzHR\n\nTlKWIorucJ"}, {"title": "Remote Work - Need Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3774712978/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=r3Bp%2F0qcOD2OpC5jpbUOcA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774712978", "description": "About the job\nTitle : Data Engineer\n\nLocation : Remote\n\nDuration : 6+ months to start (long term contract (work order are extended 2x year))\n\nPlease Fill out the skill matrix: Technology Some Awareness (1) Novice (2) Intermediate (3) Advanced (4) Expert (5) SQL Tableau Desktop /Creator Tableau API Integration AWS - S3 Glue AWS - Athena Redshift AWS - Lambda AWS - SNS/SQS msExcel\n\nProfile: Data Analyst will work closely with Architects and leads in the Interflow team, key business stakeholders including program staff to support the organizations effort in the areas of data Analysis and information management. Role should have strong experience in writing SQL queries to extract data for reports. Candidate should have experience working with large set of data. As these reports are very critical for the company and we are looking for an expert who is capable of handling, consuming and extracting large sets of data for analysis and reports.\n\nMust Haves\n\nAWS - Athena\nAWS - S3\nGlue\nhands-on development of reporting applications\nMS Excel\nRedShift\nSQL\nTableau Desktop /Creator\n\nNice\n\nAWS - Lambda\nAWS - SNS/SQS\nTableau API Integration\n\nSkills\n\nBA/BS required (major in an analytical field desired) \nA minimum 10+ years work-related experience. \nExperience in hands-on development of reporting applications for the Web in a professional environment \nThe Ethos of continuous improvement and interest in learning new things.\nStrong analytical thinking and structured problem-solving ability\nAbility to handle multiple projects and assignments simultaneously and effectively in a cross-functional team environment. \nVersed on the agile methodology and best practices. \nExcellent interpersonal and collaboration skill with the ability to work with a diverse set of colleagues, across functions, from different organizations, disciplines, etc. \nSelf-starter, ability to set priorities, work independently and attain goals"}, {"title": "Data Engineer", "company_name": "Team Remotely Inc", "location": "Houston, TX (Remote)", "link": "/jobs/view/3786249829/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=78lY5Jo%2FfyRPC6TQYCdvww%3D%3D&trk=flagship3_search_srp_jobs", "id": "3786249829", "description": "About the job\nThis is a remote position.\n\n Data Engineer (US/Canada Residents Only, 1 year experience, remote)\n\nTeam Remotely Inc. is a staffing and recruitment agency that offers a comprehensive solution for talent acquisition, including sourcing, vetting, pay rolling, and managing talent. Whether you need contract staffing, direct hire, direct sourcing, talent pools, or diversity initiatives, our model can support your hiring strategy.\n\n Hiring Type: Full-Time\n\n Base Salary: $55K-$70K Per Annum.\n\nResponsibilities\n\nDesigning user interface changes for web-based DB applications. \nReviewing application requirements and interface designs. \nDeveloping and implementing highly responsive user interface components using react concepts. \nWriting application interface codes using JavaScript following react.js workflows. \nTroubleshooting interface software and debugging application codes. \nDeveloping and implementing front-end architecture to support user interface concepts. \nMonitoring and improving front-end performance. \n\nQualifications And Experience\n\nBachelor\u2019s degree in computer science, information technology, or a similar field. \nPrevious experience working as a react.js developer. \nIn-depth knowledge of JavaScript, CSS, HTML, jQuery, and front-end languages. \nKnowledge of REACT tools including React.js, Webpack, Enzyme, Redux, and Flux. \nExperience with user interface design. \nKnowledge of performance testing frameworks, including Mocha and Jest. \nExperience with browser-based debugging and performance testing software. \nTroubleshooting skills. \nProject management skills. \nProblem-solving skills. \nVerbal communication skills. \n\n Why work with Team Remotely?\n\nTeam Remotely Inc. is a staffing platform offering a seamless experience for employers and candidates. Employers can post job openings and specify their requirements, while candidates can create profiles and upload resumes.\n\nThe team of Team Remotely continuously learns and adapts based on previous successful placements, constantly improving its matching capabilities. This ensures that the recommendations provided by Team Remotely are tailored and accurate, increasing the likelihood of a successful match between employers and candidates. By providing intelligent and data-driven solutions, they strive to enhance the efficiency and effectiveness of the hiring process, ultimately helping companies find the best talent and individuals find their dream jobs."}, {"title": "Data Engineer", "company_name": "ZAG Technical Services", "location": "United States (Remote)", "link": "/jobs/view/3771403464/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=Hf0VefrKXBFIL5MtHmY0Pw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3771403464", "description": "About the job\nWe are looking for a tech savvy Data Engineer to contribute to the building, operating, and scaling of next generation data platforms and tools that will power data-driven analytics capabilities throughout the ZAG products and services portfolio, spanning areas such as business intelligence, reporting, and data analytics.\n\nThe ideal team member is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. Must be comfortable supporting the data needs of multiple clients and teams, systems, and products within the modernization of ecosystems. The right-fit person will be excited by the prospect of optimizing or even re-designing data processes to support next generation of advanced analytics capabilities, which in-turn translates to implementing strategic enhancements to maintain competitive advantage in the industry.\n\nThe successful team member will leverage their proficiency to\u2026\n\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\nBuild the analytics processes required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Data Management and cloud based \u2018big data\u2019 technologies.\nBuild analytics tools that utilize technologies to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.\nCollaborate with client teams to identify and solution regional and local requirements within the context of the standard global business model for enterprise analytics.\nParticipates in meetings, discussions, strategy sessions where changes, improvements, and enhancements are proposed, evaluated, and approved. \n\n\nThe exceptional team member will have or demonstrate progressive experiences in\u2026 \n\nBachelor\u2019s degree or higher in Computer Science or related field\nExperience working with BI and data warehousing tools\nExperience working with enterprise data, where security is paramount and data governance is critical\nAbility to work independently and in a team environment, and effectively engage all levels of the organization\nUnderstanding of distributed systems driving large-scale data processing and analytics with a successful history of manipulating, processing, and extracting value from large, disconnected datasets.\nAbility to communicate effectively (listening, presenting, and questioning)\nStrong organizational, written, and communication skills\n\n\nAbout ZAG\n\nZAG Technical Services empowers our clients\u2019 success through intelligent business solutions that increase employee productivity, network security and team collaboration. We aim to ensure that our client\u2019s IT investments drive ROI, provide a competitive advantage and help businesses grow. We do this by living our core set of values: Integrity Foremost, Client-Centered, Accountable Always, Teamwork Throughout, and Exceptional is the Goal.\n\nWhat ZAG Has To Offer\n\nCompetitive compensation package \nBenefits including medical, dental, vision, 401K and life insurance \n\n\nZAG Technical Services is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all of our employees.\n\nZAG Technical Services is an E-Verify participant and will provide the Social Security Administration (SSA) and, if necessary, the Department of Homeland Security (DHS), with information from each new employee\u2019s Form I-9 to confirm work authorization."}, {"title": "Junior Data Engineer", "company_name": "HireMeFast LLC - Career Accelerator - Land A Job", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3786249797/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D&trk=flagship3_search_srp_jobs&lipi=urn%3Ali%3Apage%3Ad_flagship3_search_srp_jobs%3BQExogrHBQoSF9zZm%2B9kufw%3D%3D&lici=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D", "id": "3786249797", "description": "About the job\nThis is a remote position.\n\n Job Title: Junior Data Engineer\n\n Employment Type: Full-Time\n\n Salary: $64,000 - $76,000 per annum\n\n Experience Required:  Minimum 1 year of project experience\n\nAbout us: HireMeFast is a leading staffing and recruitment agency specializing in connecting businesses with top-tier talent across various industries. Our mission is to bridge the gap between exceptional candidates and organizations needing their skills, expertise, and unique qualities. Our team of experienced and dedicated recruitment specialists utilizes innovative sourcing strategies, and a vast network to identify and attract top talent. We conduct comprehensive procedures to ensure that only the most qualified candidates are presented to our clients.\n\nPosition Summary\n\nJoin the fast-paced, innovative, and collaborative environment focused on providing an AIOps platform that enhances the intelligence of the CVS Health infrastructure. Work closely with subject matter experts and colleagues to build and scale out machine learning and AI solutions that will detect, predict, and recommend solutions to correct issues before system impact and enhance efficiency, reliability, and performance of CVS Health\u2019s IT operations.\n\nKey Responsibilities include:\n\nData pipeline development: Designed, implemented, and managed data pipelines for extracting, transforming, and loading data from various sources into data lakes for processing, analytics, and correlation. \nData modeling: Create and maintain data models ensuring data quality, scalability, and efficiency \nDevelop and automate processes to clean, transform, and prepare data for analytics, ensuring data accuracy and consistency \nData Integration: Integrate data from disparate sources, both structured and unstructured to provide a unified view of key infrastructure platform and application data \nUtilize big data technologies such as Kafka to process and analyze large volumes of data efficiently \nImplement data security measures to protect sensitive information and ensure compliance with data and privacy regulation \nCreate/maintain documentation for data processes, data flows, and system configurations \nPerformance Optimization- Monitor and optimize data pipelines and systems for performance, scalability and cost-effectiveness \n\nCharacteristics of this role:\n\nTeam Player: Willing to teach, share knowledge, and work with others to make the team successful. \nCommunication: Exceptional verbal, written, organizational, presentation, and communication skills. \nCreativity: Ability to take written and verbal requirements and come up with other innovative ideas. \nAttention to detail: Systematically and accurately research future solutions and current problems. \nStrong work ethic: The innate drive to do work extremely well. \nPassion: A drive to deliver better products and services than expected to customers. \n\nRequired Qualifications\n\n2+ years of programming experience in languages such as Python, Java, SQL \n2+ years of experience with ETL tools and database management (relational, non-relational) \n2+ years of experience in data modeling techniques and tools to design efficient scalable data structures \nSkills in data quality assessment, data cleansing, and data validation \n\nQualifications\n\nKnowledge of big data technologies and cloud platforms \nExperience with technologies like PySpark, Databricks, and Azure Synapse. \n\nEducation\n\nBachelor\u2019s degree in Computer Science, Information Technology, or related field, or equivalent working experience\n\nWhy HireMeFast LLC?\n\nAt HireMeFast, we understand that finding the right individuals to join your team is crucial for success and growth of your organization. We are committed to streamlining the hiring process for our clients, ensuring they have access to a diverse pool of highly qualified candidates who are a perfect fit for their specific needs."}, {"title": "Data Engineer  - Need candidates from Texas ( REMOTE )", "company_name": "Pulivarthi Group (PG)", "location": "United States (Remote)", "link": "/jobs/view/3770685212/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=9gdMwuif%2BpB%2BCQYvQi3imQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3770685212", "description": "About the job\nFollow us on Linkedin: https://www.linkedin.com/company/pulivarthigroup/\n\nPulivarthi Group LLC is a Global Staffing & IT Technology Solutions company, with our prime focus of providing world class solutions to our customers with the right talent. We combine the expertise of our team and the culture of your company to help you with the solution that is affordable and innovative using high quality standards and technologies.\n\nWe\u2019ve served some of the largest healthcare, financial services, and government entities in the U.S.\n\nData Engineer - FT only - Azure SQL with data lakes experience\n\nData Engineer -\n\nthe resource requirement for this Data Engineer role needed more clarification.\n\nOur customer is requesting a resource who knows Azure CI/CD (Continuous Integration / Continuous Deployment) to build and integrate their DevOps pipeline.\n\nOnce the DevOps pipelines are mapped, they need the Data Engineer to connect to all the Data Sources, primarily their Data Lake.\n\nThe Customer (based in Houston) is interested in a resource in a time zone not too far removed from the Central Time Zone to allow real time collaboration."}, {"title": "Data Engineer", "company_name": "Otter Products", "location": "Fort Collins, CO (Remote)", "link": "/jobs/view/3766728302/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=77U3U7CAi1PxgDmpI3%2BiNQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3766728302", "description": "About the job\nOverview \n\nOtter Products is currently recruiting for a Data Engineer to join our Data team! You can be based in our Fort Collins, CO office with flexibility to work a portion of your time remotely, or 100% Remote in the U.S.\n\nThe Data Engineer applies professional experience, concepts, and company objectives toward building and operationalizing the minimally inclusive data necessary for the enterprise data initiatives following industry standard practices and tools. The bulk of the Data Engineer's work would be in designing, managing and optimizing data pipelines, then moving these data pipelines effectively into production for key data and analytics consumers like business/data analysts, data scientists or other stakeholder that needs curated data for data and analytics use cases across the enterprise. The Data Engineer also ensures compliance with data governance and data security requirements while creating, improving and operationalizing these integrated and reusable data pipelines to enable faster data access, integrated data reuse and vastly improved time-to-solution for data and analytics initiatives. Additionally, the Data Engineer will be expected to collaborate with other data team members and data consumers working on the models and algorithms developed by them toward optimization for data quality, security, and governance to put them into production leading to potentially large productivity gains.\n\n About Otter Products \n\nOtter Products, we grow to give. From our founder's garage in 1998 to the global technology leader we are today, Otter Products continues to drive growth through innovation.\n\nThrough our industry-leading brands - OtterBox, Liviri and OtterCares - we provide our partners the number-one selling and most trusted products in our categories. Our philanthropic spirit is the foundation on which we foster our partner relationships, allowing us to grow and to give - together.\n\nBy way of our charitable arm, the OtterCares Foundation, we support our communities and invest in our future through education that inspires kids to change the world.\n\nAnd even as our global community of Otters continues to grow, our founder's core values are still at the heart of everything we do. We measure our success by our ability to give back to our communities and strengthen opportunities for all.\n\nFor more information visit otterproducts.com\n\n Responsibilities \n\nComplete analysis, design and development of BI solutions using Azure SQL Server\nFamiliarity with Data Warehouse concepts\nExperience coding the extraction, transformation, and loading (ETL) processes\nDatabase development primarily in SSIS, Data Factory and SQL\nPartner with the business to determine end user database/reporting needs and requirements\nGenerate ad hoc reports using Power BI or SSRS\nCollaborate with other developers to create and implement best approach solution(s)\nTroubleshoot Azure tools, systems, and software\nReview queries for performance issues, making changes as needed\nCollaborate with team to performance-tune Azure application as necessary\nAssist with the analysis and extraction of relevant information from large amounts of historical business data to feed data science initiatives\nParticipate and support the design and documentation of processes for large scale data analyses, model development, model validation and model implementation\nSupport and maintain a positive safety culture by following all safety policies and procedures and actively contributing to a safe working environment\nOther duties as required\n\n Qualifications \n\nBachelor's degree required. Degree in Computer Science or Mathematics preferred\nMinimum of four years of experience in an IT or analytical role required. Experience in database development, report writing and/or statistics preferred.\n\n EEO \n\nOtter Products, LLC is an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, age, marital status, pregnancy, sex, sexual orientation, gender, gender identity or expression, national origin, disability, veteran status, or any other characteristic or status protected by law.\n\n For US Based Roles Only - Compensation Range Minimum \n\nUSD $90,000.00/Yr.\n\n For US Based Roles Only - Compensation Range Maximum \n\nUSD $120,000.00/Yr.\n\n Additional Total Rewards \n\nProfit Sharing Program Eligible, Benefits Eligible - Full Time- check out otterproducts.com/careers/why for more info"}, {"title": "Software Engineer, Distributed Systems", "company_name": "Hightouch", "location": "United States (Remote)", "link": "/jobs/view/3628572539/?eBP=CwEAAAGMd4QO8APODyE_-mMOPhYyQGg5ZjVbPwq_3V7DK0B2bLAf10pcedM9S2T71X5i4TiJuEwOzWBbSiTKbLuk-VD-_Y-dN5uyf8YpYkHYqEYFxySMS9zr2hRjNrxX9gXbxssh35TFK9YTNV2WeH2Ncb675JgFbJTgNWSZySFC6rrdJGmdSDTr2q5d3VoxORz3GjsDbzjeyZQCvQ07RqYw7oujWSgslycs2LMCY_-N0NEj6e0PvbaYKiZ_5IW34bGVSYLXpHwqa8oOcNttYZ90UA3E_7sDSFzABdIVlfrp5zzERrIYy5sh2MIInxHdDoKgQw8VmOn5uM4S0tXUl-i7LnN0_W0NfPBmLiVDNuveIOm4MfDNPxfe5QKPk6FUAVpvgbcoJn9yRyE&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=rph%2FT6Q6Vkp6YoggJf9cJQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3628572539", "description": "About the job\nAbout Hightouch\n\nHightouch\u2019s mission is to empower everyone to take action on their data. Through our Reverse ETL platform, business and data users can seamlessly sync data from where it resides, such as warehouses and databases, to where it is needed, including operational systems and SaaS tools. Traditionally, acting on data has required engineering time and bandwidth, and left most business users stuck with charts and reports that are unable to take automated action on their data. With Hightouch, every business user, without writing any code, can activate data to streamline critical processes, improve marketing performance, and scale operations.\n\nOur team operates with a focus on making a meaningful impact for our customers. We believe in approaching challenges with a first principles thinking mindset, moving quickly and embracing our value of efficient execution, and treating each other with compassion and kindness. We look for team members that are strong communicators, have a growth mindset, and are motivated and persistent in achieving our goals.\n\nHundreds of companies use Hightouch, including Spotify, Ramp, Retool, NBA, Plaid, and Betterment. We\u2019re based in San Francisco, are remote-friendly, and backed by leading investors such as Amplify Partners, ICONIQ Growth, Bain Capital Ventures, Y-Combinator, and Afore Capital.\n\nAbout The Role\n\nWe are looking for a distributed systems engineer to work on the systems that power the syncing engine our customers and other engineering teams rely on. This presents an exciting challenge where you can apply your expertise in distributed systems, performance optimization, and troubleshooting to push the boundaries of what is possible and meaningful improve our ability to move massive amounts of customer data. This role also provides a unique opportunity to work on a multi-cloud and multi-region infrastructure that supports a global customer base.\n\nWe believe in enabling our engineers to do their best work for our customers by giving them extremely high levels of ownership and autonomy. This comes in different forms: you will own and deliver projects from start to finish, you will work directly with customers to solve their hardest scaling problems, and you will have a lot of influence over what we work on as a team and company.\n\nSome of the problems we\u2019ll be working on include:\n\nSync Speed: Customers want to sync a lot of data to important destinations like Facebook and Snapchat, which requires us to analyze every part of our syncing process and find where we can optimize to sync data more quickly\nStreaming Syncing: We currently sync data from sources in batch only, but envision a world where we also do syncs in real-time and support streaming sources like webhooks and queues\nScalability and Reliability: As part of our rapid growth, we\u2019re always evaluating current future bottlenecks in scaling and reliability, and architecting for the next order of magnitude of growth\nPersonalization API: Our Personalization API product provides a low latency caching layer on top of data warehouses that enable customers to use their data warehouse for real-time personalization use cases with \nMulti-Region and Multi-Cloud: Supporting our multi-region and multi-cloud backend, including extending it to launch Hightouch on in new regions to support data residency requirements of our global customer base\n\nWe are looking for talented, intellectually curious, and motivated individuals who are interested in tackling the problems above. This is a senior role, but we focus on impact and potential for growth more than years of experience. The salary range for this position is $170,000 - $240,000 USD per year, which is location independent in accordance with our remote-first policy.\n\nAbout You\n\nYou are an engineer with a passion for solving hard technical problems that generate real value for customers. You\u2019re motivated by high ownership and are comfortable in a fast-paced, startup environment.\n\nYou have experience and comfort with distributed systems and high-scale systems or the ability and desire to build this experience quickly. This experience can take different forms:\n\nYou\u2019ve built syncing engines that process and move terabytes of data per day in a scalable way\nYou\u2019ve created event collection or data streaming services that power company critical systems\nYou\u2019ve owned systems that have undergone orders of magnitude of growth and helped them scale accordingly\nYou have relevant skills and are a fast learner who is excited to take on the challenges we face\n\nInterview Process\n\nOur goal with the interview process is to balance speed with giving both parties opportunities to assess whether there is a strong mutual fit. We will ask you questions, but we want you to ask us questions! Our technical interviews focus on how you design systems because we believe this is the best way for us to see how you work and for you to see how we collaborate. We don\u2019t ask you to write code to solve technical brainteasers that don\u2019t appear in your day to day job.\n\nRecruiter screen [30m]: Introductory call with our recruiting team to get to know each other and see if the role could be a good mutual fit.\nSystems design screen [45m]: Designing a data processing feature end-to-end.\nHiring manager interview [30m]: Chat with hiring manager about past experiences and future operating preferences to assess fit on company values and operating principles.\nSystems design interview [90m]: Work with the interviewer to architect a system at a conceptual level. The problem will be at a pretty high level - and have both product and customer requirements as well as technical."}, {"title": "Entry-Level Data Scientist Engineer", "company_name": "Phoenix Recruitment LLC", "location": "Texas City, TX (Remote)", "link": "/jobs/view/3788727625/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=8iwtytHsCu4wMwJ4fj3pxw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3788727625", "description": "About the job\nThis is a remote position.\n\nTitle - Entry-Level Data Scientist Engineer, 1 year of project experience\n\nEmployment Type: Full-time\n\nBase Salary: $60K-$70K\n\nPhoenix Recruitment offers a variety of recruiting services to assist both employers and employees. They specialize in marketing open positions, recruiting, and helping employers to find qualified candidates across various industries. Phoenix Recruitment has expertise in streamlining the hiring process. They can help ensure that the process is efficient, well organized, and compliant with relevant regulations.\n\nSkills and Abilities:\n\nStrong knowledge of R or Python for data analysis and modeling. \nProficiency in statistical programs such as R, SAS, MATLAB, or Python. \nFamiliarity with spreadsheets (VBA) and database applications (Access, Oracle, SQL, or equivalent technology). \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \nAbility to learn new methods quickly and work under deadlines. \nExcellent teamwork and communication skills. \nStrong analytical and problem-solving abilities. \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \n\nPreferred:\n\nKnowledge of actuarial concepts and life, health, and/or annuity products. \nExperience with statistical modeling techniques such as GLM, Decision Trees, Time Series, Regression, etc. \nFamiliarity with Microsoft DeployR. \nExposure to insurance risk analysis. \nBasic experience in computational finance, econometrics, statistics, and math. \nKnowledge of SQL and VBA. \nFamiliarity with R or Python for predictive modeling \n\nWhy Phoenix Recruitment LLC?\n\nPhoenix Recruitment often has an extensive network of employers and candidates. This network allows them to tap into a pool of qualified candidates and connect them with suitable job opportunities. They can also leverage their connections to help employers find the right talent efficiently. Outsourcing the recruitment process to a specialized agency can save your time and resources, avoid delays, reduce administrative burdens, and increase the chances of finding the right fit for your organization."}, {"title": "Data Engineer - Remote Work", "company_name": "BairesDev", "location": "United States (Remote)", "link": "/jobs/view/3783162297/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=OtoCrAAHZFomVup04fy35g%3D%3D&trk=flagship3_search_srp_jobs", "id": "3783162297", "description": "About the job\nWho We are\n\nBairesDev is proud to be the fastest-growing company in America. With people in five continents and world-class clients, we are only as strong as the multicultural teams at the heart of our business. To consistently deliver the highest quality solutions to our clients, we only hire the Top 1% of the best talents and nurture their professional growth on exciting projects.\n\nWe are looking for a Data Engineer to join our Development team and participate in different projects made up of multicultural teams distributed throughout the world.\n\nWhat You Will Do\n\n Interview other Data Scientists and Data engineers to vet their technical abilities.\n Input data from those interviews into the product and help develop a predictive, fair, and enjoyable solution.\n Work together with developers, tech leads, and solution architects to build applications.\n Improve existing structures by adding new functionalities or proposing technological updates so that as a result, the impact of your contribution is significant in the core of each business.\n Improve the user experience on scalable and high availability platforms, contributing to the key differential of each business.\n\nHere\u2019s What We Are Looking For\n\n 5+ years of experience working as a developer.\n 3+ years of experience working as a Data Scientist / Data Engineer.\n Proficient with analysis, troubleshooting, and problem-solving.\n Hands-on experience with managing data loads and data quality.\n Advanced English level.\n\nHow we do make your work (and your life) easier:\n\n 100% remote work.\n Hardware setup for you to work from home.\n Flexible hours - make your schedule.\n Paid parental leave, vacation & holidays.\n Diverse and multicultural work environment.\n An innovative environment with the structure and resources of a leading multinational.\n Excellent compensation \u2014 well above the market average.\n Here you can grow at the speed of your learning curve.\n\nOur people work remotely but with a consistent and robust culture that promotes diversity and teamwork. To continue being the leading software development company in Latin America, we want to ensure that every BairesDev member gets the best growth and professional development opportunities in a diverse, welcoming, and innovative environment.\n\nEvery BairesDev team member brings something unique to our company.\n\nWe want to hear your story. Apply now!"}, {"title": "Data Engineer (Remote)", "company_name": "Cascade Debt", "location": "United States (Remote)", "link": "/jobs/view/3755336571/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=pmUtn1j2zyPr25CNPh2RNw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3755336571", "description": "About the job\nDo you want to work in a small team where you can make a real difference in a company? Would you embrace the challenge of building a high impact platform that is used globally? Then we want to speak with you!\n\nAbout Cascade\n\nCascade is a fintech startup backed by Canadian and US investors that empowers companies to grow by democratizing access to institutional debt. We are building tools that modernize how companies raise and manage debt, lowering the barriers to entry to the $7 trillion specialty finance and alternative credit market for companies around the world. Founded just last year, we are gearing up for our next phase of technical development and are seeking a talented Data Engineer to lead the way.\n\nAbout The Role\n\nAs a Data Engineer you\u2019ll be an early member of a growing team building a pioneering platform in the debt infrastructure space. You will design, build, and launch efficient, scalable, and reliable data pipelines to move and transform data .\n\nWhat You\u2019ll Do\n\n Building databases, data lakes and data ingestion pipelines to integrate customer databases and datasets to our systems \n Ingesting financial datasets from external customers, then u pdating and maintaining accurate and complete data mappings to ensure that our product s are displaying high quality data \n M onitoring and alerting across our data pipelines in order to make sure that our data ingests are reliable and correct \n Translat ing business goals and stakeholder requirements into new data flows and deliver analytical insights ensuring that the data flows create real business value continuously \n\nSkills And Experience\n\n 2 + years experience in a data engineering or data analyst role \n E xperience with relational SQL and NoSQL databases, including PostgreSQL , MySQL, MariaDB, MongoDB, Bigtable, etc. \n E xperience working with ETL technologies , such as Databricks, Fivetran , or dbt \n Proficiency in writing SQL queries and knowledge of analytical data warehouses such as RedShift, BigQuery , and Snowflake \n Some experience with CI/CD automation such as GitHub Actions \n Developing APIs and integrating with 3rd party APIs \n Bonus: experience in fintech / SaaS / credit analysis \n\nPeople Who Thrive At Cascade Are\n\n Self-starters, who take the initiative to tackle challenges in a remote work environment \n Good communicators, who can operate without ego to discuss, learn, grow, and help others do the same \n Passionate about creating great digital experiences for users \n Problem solvers who are not satisfied with the status quo \n\nBenefits\n\nRemote: we are a remote-first company and are very flexible on hours as long as things get done \nHome-office: there\u2019s a $1000 USD home office allowance to set yourself up \nEquity: we expect you to have an owner-mentality, and have the equity plan to match \nBenefits: health, dental, vision, and more \nPerks: we offer free lunches weekly and off-site trips \nJob satisfaction: we offer autonomy, ample opportunities for mastery, and an opportunity to make a difference for companies around the world"}, {"title": "Data Engineer", "company_name": "ISC (Integrated Specialty Coverages, LLC)", "location": "Carlsbad, CA (Remote)", "link": "/jobs/view/3778293528/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ew7%2FfEuXSujs5IGabdqUyw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3778293528", "description": "About the job\nAbout Integrated Specialty Coverages\n\nIntegrated Specialty Coverages LLC (ISC) is a growth stage technology and data-driven insurance company leading innovation in the market.\n\nBacked by one of the leading private equity firms, KKR, and led by a forward-thinking management team, ISC is combining the worlds of insurance and technology to create an Insurtech powerhouse. As a leading online distributor of insurance products for a range of industries and \"Main Street USA\", we are looking for the right people to help us in our mission of achieving exponential growth. We strive to be the number one place to go for brokers and agents to source insurance. To accomplish this, we're building a digitally focused team that deeply understands the intersection between user experience, data, and AI/ML to optimize the way we engage with our customers and partners.\n\nJob Summary\n\nThe Data Engineer role will be the technical liaison between multiple groups including a data science team, the engineering team, product management, and business stakeholders. You do not need any insurance knowledge prior, however, you must quickly dive deep into the insurance world and ask questions to become a subject matter expert. You will be responsible for building a data platform to facilitate the data science team. You must be a self-starter that can build out features such as a data pipeline from scratch. There will be support from both engineering and data science for any buildout.\n\nJob Responsibilities\n\nCreate and maintain optimal data pipeline architecture. \nAssemble large, complex data sets that meet functional / non-functional business requirements. \nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. \nBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using AWS technologies, SQL, and Python. \nWork with stakeholders including the Executive, Product, Data Science, and Engineering teams to assist with data-related technical issues and support their data infrastructure needs. \nWork with data science and analytics teams to strive for greater functionality in our data systems. \n\nMinimum Qualifications:\n\nAdvanced working SQL knowledge and experience working with relational databases, strong query authoring (SQL) as well as working familiarity with a variety of databases (Snowflake, Redshift, MySQL, MSSQL, etc.)\nExperience building and optimizing data pipelines, architecture and data sets. \nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. \nBuild processes supporting data transformation, data structures, metadata, dependency, and workload management. \nA successful history of transforming, processing and extracting value from large disconnected, datasets from a variety of data sources (Flat files, Excel, databases, etc.)\nStrong analytic skills related to working with unstructured datasets. \nStrong project management and organizational skills. \nExperience supporting and working with cross-functional teams in a dynamic environment. \nAbility to mentor/guide/collaborate with other team members. \n\nWe are looking for a candidate with 3-5 years of experience in a Data Engineer role. They should also have experience using the following software/tools:\n\nExperience with relational and MPP databases, including Snowflake and MySQL. \nExperience developing software in an agile environment from the requirements stage to production. \nExperience with version control: git\nExperience with container technologies: Docker\nExperience with data pipeline and workflow management tools: Airflow, Jenkins, AWS Glue, Azkaban, Luigi, etc. \nExperience with AWS cloud services: EC2, ECS, Batch, S3, EMR, RDS, Redshift\nExperience with other cloud services: Snowflake, Airflow\nExperience with object-oriented/object function scripting languages: Python, Java, C++, etc. \nExperience with data modeling and data warehouse design\nExperience with data visualization tools (PowerBI, QuickSight)\n\nThe starting annual pay scale for this position is listed below. Actual starting pay will be based on factors such as skills, qualifications, training, and experience. In addition, the company offers comprehensive benefits including medical, dental and vision insurance, 401(k) plan with match, paid time off, and other benefits.\n\nISC's salary ranges are determined by role and level. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations and could be higher or lower based on a multitude of factors, including job-related skills, experience, and relevant education or training.\n\nNational Pay Range\n\n$100,000\u2014$110,000 USD\n\nBenefits of Working at ISC\n\nCompetitive vacation and flexible working arrangements\nComprehensive and inclusive health benefits\nA variety of professional development and mentorship opportunities\nChoice of technology whether at home or in the office\n\nISC's Ownership Behaviors\n\nDo the Right Thing *Be Relentless. Pursue Excellence. *Personal Responsibility & Own Outcomes\nTry Fast. Learn Fast. Fail Fast. Think Big. *Build Extraordinary Owners & Join Forces *Grit & Determination\n\nApplicants with disabilities may contact ISC HR department via e-mail, and other means to request and arrange accommodation. If you need assistance to accommodate a disability, you may request accommodation at any time. Please contact ISC at HR@ISCMGA.com.\n\nISC believes in creating long-term relationships by being responsive and relevant and by consistently delivering value to our community of customers. Specifically, with our employees, we focus on attracting, developing, and retaining the best talent for our business, challenging our people, demonstrating a \"can-do\" attitude, and fostering a collaborative and mutually supportive environment. ISC is an equal opportunity employer and a participant in the US federal E-Verify program (US).\n\nDiversity creates a healthier atmosphere: All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, gender, gender identity, sexual orientation, marital status, medical condition, genetic information, mental or physical disability, military or veteran status, or any other characteristic protected by local, state, or Federal law."}, {"title": "Data Engineer -Remote", "company_name": "RIT Solutions, Inc.", "location": "Denver, CO (Remote)", "link": "/jobs/view/3768007496/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=1hQH8EDaII1eJ7nQ9BACQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3768007496", "description": "About the job\nDuration: 6+ months CTH\n\nMust have a valid LinkedIn profile\n\nRequired Skills\n\n 3-6 years of experience (no more than 6 years)\n Start up environment experience\n Python, SQL, and DBT"}, {"title": "Big Data Engineer - W2", "company_name": "Megan Soft Inc", "location": "United States (Remote)", "link": "/jobs/view/3674845652/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fCAxh4N%2F4YZJ5aEIASaeeA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3674845652", "description": "About the job\nTitle: Big Data Engineer - W2\n\nLocation: Dearborn, MI (Remote)\n\nDuration: 12+ Months\n\nDirect Client: Ford Motors\n\nMust have skills: Should be able to handle GCP requirements\n\n\"Translates business requirements to build repeatable, sustainable, efficient, coded processes that can be productionized by Software Engineers and readily modified by other Data Engineer Creates POC processes in Dev/QA and works with Software Engineers to productionize and align best coding and process practices that maximize efficiency, speed, stability, system resources and capabilities Awareness of and compliance with data privacy, security, legal and contractual guidelines Incorporates data quality and privacy checks/alerts to minimize bad data being consumed by end users, models and dashboards, and to protect customer data; revises checks for new data issues Validates data products and pipelines are functioning as expected following system or application upgrades, source changes, etc. Works closely with Architects if required to align systems, tools and applications being utilized with business use case and performance requirements Provides data product support and maintenance Establishes, tracks and monitors KPIs related to specific data products and deliverable Participate in iteration cadence activities (Iteration Planning, task estimation, daily scrum, demos, etc.). Will be a part of on-call team to resolve critical production support tickets during business hours and off hours.\n\nSkills Required\n\nExperience with Big Data Technologies (Hadoop , ETL Tool, Spark. Hive etc..) Someone who understands Cloud and has experience working with Cloud Platforms (preferably GCP) Should be able to handle GCP requirements - ingesting into GCS/ Big Query, working with GCP components Hands-on in Data Flow, Cloud Storage (GCS), Big Query and GitHub. Proficient in Python and SQL.\n\nSkills Preferred\n\nManages ETL: uses programming and tools for data ingestion, configures pipelines, applies transformations and decoding, integrates and fuses data, moves and securely deliver Experience NoSQL databases\n\nAdditional Information\n\nIf the candidate is remote only, please indicate \"\"100% Remote\"\" under candidate's name on resume. Other similar schedule notations can be \"\"Local Candidate\"\" or \"\"Hybrid Candidate.\"\" ***HRA Test Required***\""}, {"title": "Remote Opportunity: Quantexa Data Engineer", "company_name": "SPAR Information Systems LLC", "location": "United States (Remote)", "link": "/jobs/view/3680073191/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ioykVCZSOQ7BK5%2FwRQHOWg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3680073191", "description": "About the job\nHello All, Hope you are doing great!! Please go through the job descriptions and let me know your interest. Role: Quantexa Data engineer  Location: Remote Duration: Long Term Contract Job Description: 1. End to End usage of Quantexa including guiding our client team on how to use from both front end and data ingestion Reason they are asking for Certification 2. Resolve Entity issues with data 3. Knowledge of Scala and Hive which requires coding experience using parquet 4. Advanced SQL knowledge requiring writing SQL queries 5. Experience having worked with GCP data lake 6. Quantexa certification required\n\nThanks & Regards,\n\nSatnam Singh\n\nDirect: 201 623 3660\n\nEmail : Satnam.singh@sparinfosys.com"}, {"title": "Remote Work - Need Staff Data Engineer - USC only", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3746283980/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=CvIuLBNO%2BPMGi4IXVRkP0A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3746283980", "description": "About the job\n100% Remote, client is in NY\n\nLook for strong job history, Python, Machine Learning, SQL, 10/10 comms, etc. \n\nMUST be an Engineer \n\nSummary\n\nWe're looking for an experienced Data Engineer to join our client's growing team of top notched IT professionals. You will work closely across multiple groups including Editorial and Audio, Marketing, Advertising, etc. to achieve their goals and objectives. The Data Engineering team builds tools throughout the stack while sharing knowledge across these departments.\n\nAs the company grows, they're looking for Data Engineer who will help solidify and expand our pipelines and maintain their data warehouse. Their business model is based on performing complex and very detailed analyses of how our products perform with their subscribers. You'll be responsible for working alongside analysts, product, marketing, and other teams to define new data collection components and measurement schemes that support each new product and feature.\n\nHow Can Add Value To Their Mission\n\nCreate and maintain data pipelines to provide insights and drive business decisions\nEstablish data warehousing strategy (ex. Kimball, Data Vault, etc.)\nMaintain data infrastructure on our AWS accounts\nCollaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.\nWrite unit/integration tests, contributes to engineering wiki, and documents work\nImplement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it\n\nWhat You'll Need To Succeed\n\n7+ years of industry experience measuring product performance and user behavior\nExperience working with a variety of data management technologies, including RedShift, Kinesis/Kafka, Glue, Spark, Postgres, Airflow, dbt, and others.\nExperience implementing BI reporting tools such as Looker\nExperience interfacing with engineers, product managers and analysts to understand data needs\nKnowledge of a variety of measurement beacons, SDKs, APIs including Google Analytics, Amplitude, Braze, Stripe, email service providers\nA commitment to building secure, resilient, fault-tolerant architectures, with clear documentation and procedures in place for support\nA focus on accuracy and detail as you build, ensuring data pipelines produce clear, predictable results\nUnderstanding of the typical metrics a subscription and advertising-supported business needs to measure success\nExperience with Client techniques as applied to behavioral segmentation or anomaly identification is a definite plus\nFamiliarity using developer tools that increase productivity and facilitate the development of resilient code (eg. Docker, CircleCI, Serverless) is a plus"}, {"title": "ENTRY LEVEL DATA ENGINEER (REMOTE)", "company_name": "SynergisticIT", "location": "Jacksonville, FL (Remote)", "link": "/jobs/view/3767587909/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=om7L3CnowiC4PLIcpZD70A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3767587909", "description": "About the job\nSYNERGISTICIT wants every candidate to know that the Job Market is Challenging and to stand out, you need to have exceptional skills and technologies and that's where we come in to make sure you get the attention which you need\n\nSynergisticIT understands the complex nature of the job market and how difficult it can be to secure a position, especially for fresh graduates. Therefore, we assist and help tech-savvies to convert their passions into professions. We go above and beyond to keep you working in your niche.\n\nAs we focus on long-term success, we provide complete career development solutions. From job search to upskilling portfolio and interview preparation, we can guide you at every step of your career.\n\nSynergisticIT spares no efforts to connect you with a large network of tech giants, including Google, Apple, PayPal, Dell, Cisco, Client, etc. Presently, we are actively looking for ENTRY LEVEL DATA ENGINEER a driven mindset. Get the right opportunity and gain experience in building web-centric solutions on Java.\n\nWho Should Apply : Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or anyone looking to make their career in IT Industry\n\nWe also assist in filing for STEM extension and H1b and Green card filing.\n\nCandidates who are serious about their future in the IT Industry and have set big goals for themselves.\n\nCandidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. We also offer Skill Enhancement Programs if the candidates are missing skills or experience which our clients need with great outcomes\n\nCandidates can benefit from skill enhancement if they fall into the below categories. If they are qualified with enough skills then no need for skill enhancement\n\nCandidates Who Lack Experience\n\nHave had a break in careers\n\nLack Technical Competency\n\ncandidates who want to get employed and make a career in the Tech Industry \n\nPlease Also Check The Below Links\n\nhttps://www.synergisticit.com/candidate-outcomes/\n\nhttps://www.synergisticit.com/java-track/\n\nhttps://www.synergisticit.com/data-science-track/\n\nhttps://www.synergisticit.com/which-is-the-best-option-for-tech-job-seekers-staffing-companies-consulting-companies-bootcamps-or-synergisticit/\n\nhttps://www.synergisticit.com/contact-us/\n\nIf the skills are not a match candidates can opt for Skill enhancement. Or their resume can be sent out to clients to see if responses are achievable\n\nREQUIRED SKILLS For Java/Software Programmers\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Core Java , javascript , C++ or software programming\nSpring boot, Microservices and REST API's experience\nExcellent written and verbal communication skills\n\nFor data Science/Machine learning\n\nRequired Skills\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Statistics, Python, data visualization tools\nExcellent written and verbal communication skills\n\nPreferred skills: NLP, Text mining, Tableau, Time series analysis\n\nTechnical skills are required by clients for selection even if its Junior or entry level position each additional Technical skill helps a candidate's resume to be picked by clients over other job seekers.\n\nClients hire candidates with the right technical skills which they need and reject candidates who lack the required technical skills.\n\nNo third party candidates or c2c candidates\n\nPlease apply to the posting\n\nNo phone calls please. Shortlisted candidates would be reached out"}, {"title": "Data Engineer - Remote | WFH", "company_name": "Get It Recruit - Information Technology", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3782241984/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=BgBf1kSAAX%2BSuw6inJsIVw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3782241984", "description": "About the job\nJoin an innovative team at a leading company in the engineering and manufacturing industry, where we are committed to leveraging data for continuous improvement and operational excellence. This hybrid position allows you to enjoy the flexibility of remote work on Mondays and Fridays.\n\nWhy Join Us\n\nAt our company, we prioritize your well-being and professional growth. We offer a comprehensive benefits package, including excellent health benefits, a 6% 401(k) match, generous paid vacation and sick time, 12 paid holidays, and more.\n\nRole Overview\n\nAs a Data Engineer, you will play a pivotal role in transforming raw data into actionable insights, driving improvements in engineering and manufacturing processes, enhancing product quality, and optimizing operational efficiency. Collaborate with dynamic cross-functional teams to extract, analyze, and visualize data, influencing strategic decisions through your valuable insights.\n\nKey Responsibilities\n\nData Collection and Integration:\n\nGather, clean, and integrate data from diverse sources, including production databases, sensor data, and external datasets.\n\nData Analysis\n\nConduct exploratory data analysis to identify patterns, trends, and anomalies.\n\nPerform statistical analysis and hypothesis testing to derive meaningful insights.\n\nData Visualization\n\nCreate informative and visually appealing dashboards and reports using tools like Tableau, Power BI, or Python libraries.\n\nPredictive Modeling\n\nDevelop predictive models to forecast manufacturing performance, quality, and operational outcomes.\n\nRoot Cause Analysis\n\nInvestigate production issues and quality concerns by analyzing data to identify root causes and recommend corrective actions.\n\nPerformance Monitoring\n\nEstablish and maintain key performance indicators (KPIs) to track manufacturing and engineering metrics.\n\nGenerate regular reports to communicate performance trends to stakeholders.\n\nCross-functional Collaboration\n\nCollaborate with engineering, manufacturing, and quality assurance teams to understand their data needs and provide analytical support.\n\nContinuous Improvement\n\nSuggest process improvements and optimizations based on data analysis.\n\nData Security And Compliance\n\nEnsure data security and compliance with relevant regulations, such as GDPR or industry-specific standards.\n\nInfrastructure And Development\n\nBuild and maintain the infrastructure for collecting, storing, and processing data.\n\nDesign, develop, and manage data pipelines, ETL processes, and data warehouses.\n\nRequired Experience\n\nProven experience as a Sr. Data Analyst and/or Data Engineer in an engineering or manufacturing environment.\n\nProficiency in data analysis tools and programming languages, such as Python, R, SQL, and Excel.\n\nStrong data visualization skills with tools like Tableau or Power BI.\n\nExcellent knowledge of statistical analysis and machine learning techniques.\n\nAbility to communicate complex findings and insights effectively to both technical and non-technical stakeholders.\n\nStrong problem-solving skills and attention to detail.\n\nFamiliarity with manufacturing processes and quality control is a plus.\n\nExperience with enterprise ERPs (SAP S4HANA, Oracle EBS), CRMs (Salesforce, Hubspot), and IoT.\n\nProficient in working with cloud computing platforms including AWS, GCP, and Azure.\n\nExpertise in configuring and optimizing cloud-based data solutions for scalability, reliability, and performance.\n\nBachelor's Degree in Data Science, Statistics, Engineering, or Computer Science Required.\n\nApply Now and be part of a team shaping the future of data-driven innovation in engineering and manufacturing!\n\nEmployment Type: Full-Time"}, {"title": "Entry Level Software Engineer - Data Backend Engineer (Remote - Canada)", "company_name": "Yelp", "location": "United States (Remote)", "link": "/jobs/view/3752881757/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fankeoxifvhpTzOYvowHYQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3752881757", "description": "About the job\nYelp engineering culture is driven by our values: we\u2019re a cooperative team that values individual authenticity and encourages creative solutions to problems. All new engineers deploy working code their first week, and we strive to broaden individual impact with support from managers, mentors, and teams. At the end of the day, we\u2019re all about helping our users, growing as engineers, and having fun in a collaborative environment.\n Interested in exploring data and finding innovative ways to collect it, curate it, and make it easily accessible for others to use? How about doing that at an enormous scale? Yelp\u2019s datasets contain billions of interactions between users and local businesses around the globe. If working with our vast amount of data sounds exciting, come and join us!\n\nThe Marketplace Data Observability team is responsible for managing, governing, and monitoring all of Yelp\u2019s Ads data to protect our revenue stream and also support offline analysis that aids in making long term company decisions. We build tools that help us to better understand our data quality, and we strive to uncover data issues before they impact our consumers.\n\nAs a Data Backend Engineer on this team, you will be responsible for building elegant and scalable data products that serve critical, up-to-date, structured information to support different types of analytics within Yelp. As a core contributor to our growing data modeling and data warehousing engineering efforts, you will help design and own mission-critical data flow pipelines and datastores to enable decisions including effective A/B testing and company investments.\n This opportunity is fully remote and does not require you to be located in any particular area in Canada. We welcome applicants from throughout Canada. We\u2019d love to have you apply, even if you don\u2019t feel you meet every single requirement in this posting. At Yelp, we\u2019re looking for great people, not just those who simply check off all the boxes.\n  Build systems that can effectively store and crunch terabytes of data.\nDesign and develop data models for efficient data storage, retrieval, and reporting.\nCreate and maintain conceptual, logical, and physical data models using industry-standard modeling tools.\nCollaborate with cross-functional teams, including engineers, data analysts, business analysts, and data scientists, to understand data requirements and translate them into effective data models.\nParticipate in data integration efforts, including ETL processes and data migration.\nStay up-to-date with industry best practices, emerging technologies, and trends related to data warehousing.\nSupport on-call rotations as needed to operate the team.\n  Understanding of high performing and scalable data systems.\nExperience in building and orchestrating ETL pipelines.\nExperience with Data Lake or Data Warehouse landscape.\nA hunger for tracking down root causes and fixing them in systematic ways.\nAbility to communicate effectively to technical and non-technical cohorts alike. \nExposure to some of the following technologies: Python, AWS Redshift, AWS Athena / Apache Presto, Big Data technologies (e.g S3, Hadoop, Hive, Spark, Flink, Kafka etc), NoSQL systems like Cassandra, DBT."}, {"title": "Data Engineer (Associate Consultant), June 2024", "company_name": "UDig", "location": "Nashville, TN (Remote)", "link": "/jobs/view/3716039960/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=XUjo40kU55HnDCqJ%2F3FdPg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3716039960", "description": "About the job\nCan UDig It? \n\nUDig designs, builds, and implements technology solutions that deliver on business objectives. People join our team because they are excited to have an impact on the future of our organization, passionate about delivering meaningful results, and energized by solving complex problems. Oh, and they like working with other driven, smart, and authentic people!\n\nUDig is proud to be recognized as a \"Best Places to Work\" in Virginia and Tennessee. We're committed to fostering a transparent environment where leadership is accessible, professional development opportunities are abundant, and you are empowered to achieve your career goals. You'll collaborate, develop deep relationships with teammates, and accelerate your success while having fun!\n\nIf you're graduating in May 2024 and excited by the idea of impacting organizations by building data solutions to achieve business goals, you should consider joining our growing technology team.\n\nHow Things Go Down\n\nAs a new, entry-level teammate, you'll take part in our \"Breaking Ground\" training program designed to help you quickly get up-to-speed on UDig's delivery approach within a project team. The program covers:\n\nUDig delivery process and methodologies such as gathering requirements and delivering projects using agile\nTechnical guidance and instruction on the best practices from Data leaders at UDig\nDelivering executive and technical presentations\nCoaching to prepare you to for success on your first client project at UDig\n\nA typical day might entail:\n\nProject standups and check-ins\nDesign and development of data pipelines, BI reports, and more\nPlanning and participating in project demos\nContributing to internal UDig projects\nCareer and job growth through UDig-provided training\nCreating long-lasting friendships with teammates through fun activities as well as your projects (including social events, winning a prize for guessing the weight of pumpkin, or sharing experience on cool tech side projects)\n\nUDig believes it is important for Associate Consultants going through our Breaking Ground program to reside in one of our major markets (Richmond, VA or Nashville, TN metro area) for at least the first year of employment.\n\nWant to learn more? Hear directly from our teammates by checking out our 'A Day in the Life' blog series: https://www.udig.com/digging-in/breaking-ground-2023/\n\nSound Interesting? Here's the Technical Piece\n\nBelow are a few boxes you should be able to check if\u202fyou're interested: Don't check every box? Go ahead and apply!\n\n1+ year of experience with relational database management systems such as Microsoft SQL Server, MySQL, or Oracle\nUnderstanding of the full lifecycle of design, development, and implementation of data solutions including architecture design, development, testing\nSome experience in ETL development or reporting is a plus\nAbility to present technical ideas and high-level concepts and solutions to internal and external team members with varying degrees of technical knowledge\nBachelor's Degree in in computer science, information technology, or any science or business discipline\nAbility to take your work seriously but also seriously enjoy a good time with your UDig colleagues\n\nWhat's in It For You? \n\nAt UDig, you will build your career alongside talented and experienced developers while gaining experience and having fun! We provide continuous challenges and professional development opportunities. Additionally, we offer: \n\nCompetitive salary, merit reviews, and career advancement paths\nFlexible, hybrid environment\nIndividual $1500 Training Budget\nRegular team building and social activities (virtual and/or in-person)\nTransparent culture with strong communication and access to leadership\nGreat benefits like\nGenerous Paid Time Off, company holidays, and parental leave\nMultiple Single and Family Health Insurance plans to choose from\nDental & Vision coverage\nShort-Term and Long-Term disability\nOptional accident and critical illness coverage\nMatching 401(k)\nand more!"}, {"title": "Expression of Interest: Data Engineer", "company_name": "Fingerprint for Success (F4S)", "location": "New York, NY (Remote)", "link": "/jobs/view/3787769880/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=l%2FEDpQqUNmCttGdIyTk7Cg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787769880", "description": "About the job\nWe are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.\n\nThe F4S Talent Pool is a pilot project designed to:\n\nHelp job seekers get discovered by our partners based on their anticipated hiring needs.\nProvide optional support and resources for job seekers in their career endeavors.\nHelp individuals understand, and bring out the best in themselves and each other.\n\nThe F4S Talent Pool process:\n\nOnce you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.\n\nAbout Fingerprint For Success (F4S)\n\nBacked by 20+ years of research, F4S\u2019s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high performing teams.\n\nKeep in mind, joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your F4S work style assessment to match the hiring needs of our partners.\n\nYour feedback is a gift! Write to us via:\n\nPowered by JazzHR\n\noWuPJ9lwq9"}, {"title": "Data Engineer Seattle, WA (remote)", "company_name": "Conch Technologies, Inc", "location": "United States (Remote)", "link": "/jobs/view/3765052211/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=SBETI78UMutCHuRp5FnP%2FA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3765052211", "description": "About the job\nHi,\n\nGreetings from Conch Technologies Inc\n\nData Engineering Lead \u2013 With Databricks and Unity Catalog\n\nLocation \u2013 Seattle, WA (remote)\n\nDuration \u2013 12 months\n\nNeed: Design and implement data architecture and strategy, leveraging technologies such as Databricks, Unity Catalog, Privacera, and Collibra.\n\nAs a Data Engineering Lead, you will play a pivotal role in defining and executing technical deliverables while assessing complexities and levels of effort. You'll engage with both vendor and internal teams to resolve technical challenges and ensure seamless collaboration. Your leadership will extend to planning and prioritization with the SCRUM master, facilitating effective coordination between onsite and offshore teams, and driving feature prioritization through collaboration with product managers.\n\nKey Responsibilities\n\n Lead the definition and breakdown of technical deliverables, assessing complexities and levels of effort.\n Collaborate with vendor and internal teams to address technical issues.\n Drive planning and prioritization in conjunction with the SCRUM master.\n Coordinate with onsite and offshore teams for timely project delivery.\n Collaborate with product managers to prioritize features, communicate with stakeholders, and clarify requirements.\n Collect and manage user-reported issues, ensuring prompt resolution and communication.\n Define comprehensive test plans, oversee their execution, and document results.\n\n--\n\nWith Regards,\n\nNagesh G\n\nMobile: 408-381-5645\n\nDesk: 901-313-3066\n\nEmail: nagesh@conchtech.com \n\nWeb: www.conchtech.com"}, {"title": "Remote Work - Need Azure Fabric Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3759325599/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=C3iVjmcR9wjUip1nNjUiCg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3759325599", "description": "About the job\nThe client is placing heavy emphasis on Microsoft and Azure Fabric experience\n\nInformation Management Engineer \u2013 Take the identified information management sources and assist migration to the Client Data Lake, Azure Fabric. The engineer will transform information management (de-dup, clean and enrich).\n\nTake the identified data sources and bring them into our Data Lake, Azure Fabric. Once the data is in, the engineer will need to be able to transform data (de-dup, clean and enrich the data). We are focused on using a medallion data model for data architecture. The tools we are working with are: Delta Parquet files, python, spark, notebooks, T-SQL, Fabric lakehouse/warehouse, Dataflows Gen2, Data Pipelines, Direct Lake, shortcuts, ADF."}, {"title": "Data Engineer", "company_name": "Catalist", "location": "Washington, DC (Remote)", "link": "/jobs/view/3744951279/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ui1Fk6ICmnPNqAlZDZAcwA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3744951279", "description": "About the job\nFor over 17 years, Catalist has been a leader in civic data and data science innovation. Our mission is to provide progressive organizations with the data, software, and services needed to better identify, understand, and communicate with the people they need to engage and mobilize. Our clients include the largest, most influential organizations in the U.S. active in civic engagement, advocacy, and political campaigns.\n\nCatalist is home to a dedicated, creative team of technologists, data scientists, and campaign experts committed to using our talents and technology to nurture a vibrant and growing progressive community.\n\nAs a Data Engineer at Catalist, you will work closely with the Analytics and Technology departments to design, build, support, and maintain various data pipelines and processes with an end goal of providing data and intelligence to the progressive community. This role primarily involves translating data architecture designs into functional processes, code, and systems.\n\nThe ideal candidate will be a highly motivated individual with excellent technical skills, a strong desire to learn new skills, and an interest in progressive politics. Catalist values creativity and problem-solving. Our work is on the cutting edge of data-driven politics, and your support will help Democratic candidates and progressive organizations conduct successful advocacy and electoral campaigns.\n\nThis position reports to the Deputy Chief Data Officer. The Data Engineer is a part of a growing Data team that supports all underlying work at Catalist.\n\nThis position is included in our CWA bargaining unit.\n\nPrinciple Duties & Responsibilities\n\nDevelop scalable, production processes, code, and systems for data ingest, transformation, modeling, and reporting across a variety of platforms\nTranslate mock-ups and designs into functional processes, code, and systems\nCreate architecture designs when needed\nProvide quality assurance and testing on processes, code, systems, and products\nBecome an internal subject matter expert on various datasets and support other Catalist departments/teams on usage of those datasets\nExecute ad-hoc data and database maintenance tasks as requiredProject manage cross-departmental efforts, with direct responsibility for stakeholder engagement, management, and execution of technical elements\n\n\nRequirements\n\nBS or BA in a technical field, or relevant experience\n1-2 years of experience working with SQL databases\nExperience with data cleaning or ETL processes\nExperience with distributed computing systems and/or distributed datastores (particularly the Hadoop ecosystem)\nExperience managing projects\nFamiliarity with Catalist data, progressive politics, voter files, and/or commercial data\nBackground check required\n\n\nPreferred Skills & Abilities\n\nWillingness to be a problem solver and produce results in a fast paced environment\nAbility to be creative and personable, and articulate ideas clearly\nExperience working with SQL databases\nProficiency with Python or another object-oriented programming language (R, Java, Scala, etc\u2026)\nExperience working in cloud environments (AWS, GCP, etc.)\nExperience working in command line environments such as Bash\nExperience with a version control tool such as git or github\n\n\nBenefits\n\nMedical, Dental, Vision, Prescription Drug\n\nCatalist offers Medical, Dental, Vision, and Prescription Drug coverage for eligible staff and their eligible dependents. Catalist\u2019s Medical plan is a comprehensive PPO program including Prescription Drug coverage with 85% of the premium paid by Catalist. Dental and Vision coverage is provided at no cost to employees.\n\nGroup Term Life Insurance and Long-Term & Short-Term Disability Coverage\n\nGroup Term Life Insurance and Long-Term and Short-Term Disability coverage is available for eligible staff. These benefits are provided at no cost to Catalist employees.\n\n401(k) Safe Harbor Plan\n\nA 401(k) Safe Harbor Plan is available to eligible staff with a 3% contribution from Catalist from the date of hire. Employees may contribute pre-tax or post-tax from their salary up to the legal limits set forth by the IRS.\n\nMedical and Dependent Care Flexible Spending Accounts (FSAs)\n\nCatalist offers an FSA Program that gives eligible staff the ability to pay out-of-pocket medical/dental/vision/child care expenses from pre-tax earnings.\n\nTransit Benefits\n\nCatalist also makes available a Transit benefit FSA program to eligible employees using pre-tax contributions with a company match.\n\nProfessional Development and Remote Work Expenses\n\nEligible employees may be reimbursed up to $750 each year for professional development / education and remote work expenses.\n\nStudent Loan PayDown or SaveUp\n\nCatalist offers a Student Loan PayDown and College SaveUp benefit for eligible staff.\n\nVacation, Personal Leave, Sick Leave Benefits\n\nCatalist offers generous vacation benefits to all eligible staff. Eligible employees also receive:\n\n 14 Paid Holidays\n Personal Days\n Sick Leave\n Parental Leave\n\n\nHybrid Office/Remote Work\n\nCertain positions at Catalist are eligible for Office/Remote Hybrid or full Remote status."}, {"title": "Remote Entry Level Data Analyst/Engineer", "company_name": "SynergisticIT", "location": "White Plains, NY (Remote)", "link": "/jobs/view/3774171423/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=q54sk1%2FRww0z%2BC%2BE8e0jQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774171423", "description": "About the job\nAt SynergisticIT, we're all about making connections. Whatever IT goals you have, our software programmers can help achieve those. Our Software development teams can take up turnkey projects and execute them in an effective and efficient manner. If you are looking to source talent our recruiters will find the ideal IT talent for your company. What's the secret to our success? Well, it all starts with taking quality time to listen to each client's specific needs. After we have a thorough grasp of your IT goals, we can better customize our Developments as per your specific needs. We can also tailor make recruiting programs to exceed your expectations. Since our founding in 2010, SynergisticIT's strategies have earned the company an enviable position in the software development, IT staffing and IT skill enhancement fields. SynergisticIT continues to work with hundreds of satisfied American clients with our software programmers working on our projects and after gaining hands on experience on cutting edge technologies moving to contribute their skills to great clients like Apple, Google, Client, Ebay, Paypal, Kroger, the Walt Disney Company and hundreds more. If you are tired of working with inefficient programmers who take a lot of time to ramp up we want you to try us. Our software programmers can hit the ground running and get you the maximum return on your investment. You have already tried the rest its time you tried the best. SynergisticIT - Home of the Best Data Scientists and Software Programmers in the Bay Area.\n\n Why Us ? \n\nSynergisticIT has a proven track record of successfully skill enhancement and staffing IT employees for some of the world's most iconic brands. Our team takes the time to fully understand every client's needs so we could best meet your IT staffing requirements. The knowledgeable staff at SynergisticIT is always more than happy to work with clients to ensure they reach their software development goals. Besides staffing, SynergisticIT is also committed to helping young IT professionals advance their career with a robust upskill program . Everyone who goes through SynergisticIT's program learns all the skills necessary to succeed in many IT fields ranging from Java to Machine Learning. Additionally, everyone trained at SynergisticIT has been through extensive mock and technical interview screenings to bolster their career prospects. Last, but certainly not least, SynergisticIT takes great care to respect the privacy considerations for every client. All companies who work with SynergisticIT can rest assured their confidential data is protected using the most up-to-date encryption technologies. SynergisticIT also complies with all the latest NDA agreements.\n\n REQUIRED SKILLS For Java /Software Programmers \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Time series analysis \n\n We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2022 and at Gartner Data Analytics Summit (Florida)-2023 \n\nOracle CloudWorld Event (OCW) Las Vegas 2022 | SynergisticIT - YouTube\n\n https://www.youtube.com/watch?v=OAFOhcGy9Z8 \n\n https://www.youtube.com/watch?v=EmO7NrWHkLM \n\n https://www.youtube.com/watch?v=NVBU9RYZ6UI \n\n https://www.youtube.com/watch?v=Yy74yvjatVg \n\nSynergisticIT at Gartner Data and Analytics Summit 2023 - YouTube\n\nFor preparing for interviews please visit  https://www.synergisticit.com/interview-questions/ \n\n We are looking for the right matching candidates for our clients \n\n Please apply via the job posting \n\n REQUIRED SKILLS For Java /Full Stack/Software Programmer \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning Positions \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Tensorflow \n\n If you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team. \n\n No phone calls please.  Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates"}, {"title": "Data Engineer", "company_name": "Anika Systems", "location": "Leesburg, VA (Remote)", "link": "/jobs/view/3787766893/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=mApL9JnN%2B%2Fr2%2BYUPlxRYOA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787766893", "description": "About the job\nAnika Systems is a fast growing, woman-owned small business that specializes in providing innovative IT solutions for federal government agencies. Our expertise lies in accelerating delivery in Data and Analytics, Intelligent Automation, Application Development, and IT Modernization. We are currently expanding our Federal team and are seeking a passionate and talented Data Engineer. This opportunity is 100% remote.\n\nMust be a U.S. Citizen with the ability to obtain and maintain a government suitability clearance\n\nResponsibilities\n\nDesigns, develops, builds, analyzes, evaluates, and installs database management systems to include database modeling and design, relational database architecture, metadata, and repository creation and configuration management.\nDefines and oversees database organizations, standards, controls, procedures, and documentation. Provides technical consulting in the definition, design, and creation of a database environment.\nAdvises applications development staff and users on data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nDesigns and implements databases with respect to access methods, access time, batch processes, device allocation, validation checks, organization, protection and security, documentation, and statistical methods.\nUses data mapping, data mining, and data transformational analysis tools to design and develop databases.\nDetermines data storage and optimum storage requirements.\nPrepares system requirements, source analysis, and process analyses and designs throughout the database implementation.\n\nRequired Skills And Experience\n\nBA/BS and 1 year of relevant experience\nExperience with DataBricks, SQL, Python\nApplications development experience with data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nExcellent communication skills (both oral and written) with process-oriented organizational skills to ensure project success\n\nDesired Skills And Experience\n\nDemonstrated successful development experience with the full life cycle of government database management systems including database modeling and design, relational database architecture, metadata and repository creation, and configuration management.\n\nPowered by JazzHR\n\nTlKWIorucJ"}, {"title": "Remote Work - Need Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3774712978/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=r3Bp%2F0qcOD2OpC5jpbUOcA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774712978", "description": "About the job\nTitle : Data Engineer\n\nLocation : Remote\n\nDuration : 6+ months to start (long term contract (work order are extended 2x year))\n\nPlease Fill out the skill matrix: Technology Some Awareness (1) Novice (2) Intermediate (3) Advanced (4) Expert (5) SQL Tableau Desktop /Creator Tableau API Integration AWS - S3 Glue AWS - Athena Redshift AWS - Lambda AWS - SNS/SQS msExcel\n\nProfile: Data Analyst will work closely with Architects and leads in the Interflow team, key business stakeholders including program staff to support the organizations effort in the areas of data Analysis and information management. Role should have strong experience in writing SQL queries to extract data for reports. Candidate should have experience working with large set of data. As these reports are very critical for the company and we are looking for an expert who is capable of handling, consuming and extracting large sets of data for analysis and reports.\n\nMust Haves\n\nAWS - Athena\nAWS - S3\nGlue\nhands-on development of reporting applications\nMS Excel\nRedShift\nSQL\nTableau Desktop /Creator\n\nNice\n\nAWS - Lambda\nAWS - SNS/SQS\nTableau API Integration\n\nSkills\n\nBA/BS required (major in an analytical field desired) \nA minimum 10+ years work-related experience. \nExperience in hands-on development of reporting applications for the Web in a professional environment \nThe Ethos of continuous improvement and interest in learning new things.\nStrong analytical thinking and structured problem-solving ability\nAbility to handle multiple projects and assignments simultaneously and effectively in a cross-functional team environment. \nVersed on the agile methodology and best practices. \nExcellent interpersonal and collaboration skill with the ability to work with a diverse set of colleagues, across functions, from different organizations, disciplines, etc. \nSelf-starter, ability to set priorities, work independently and attain goals"}, {"title": "Data Engineer", "company_name": "Team Remotely Inc", "location": "Houston, TX (Remote)", "link": "/jobs/view/3786249829/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=78lY5Jo%2FfyRPC6TQYCdvww%3D%3D&trk=flagship3_search_srp_jobs", "id": "3786249829", "description": "About the job\nThis is a remote position.\n\n Data Engineer (US/Canada Residents Only, 1 year experience, remote)\n\nTeam Remotely Inc. is a staffing and recruitment agency that offers a comprehensive solution for talent acquisition, including sourcing, vetting, pay rolling, and managing talent. Whether you need contract staffing, direct hire, direct sourcing, talent pools, or diversity initiatives, our model can support your hiring strategy.\n\n Hiring Type: Full-Time\n\n Base Salary: $55K-$70K Per Annum.\n\nResponsibilities\n\nDesigning user interface changes for web-based DB applications. \nReviewing application requirements and interface designs. \nDeveloping and implementing highly responsive user interface components using react concepts. \nWriting application interface codes using JavaScript following react.js workflows. \nTroubleshooting interface software and debugging application codes. \nDeveloping and implementing front-end architecture to support user interface concepts. \nMonitoring and improving front-end performance. \n\nQualifications And Experience\n\nBachelor\u2019s degree in computer science, information technology, or a similar field. \nPrevious experience working as a react.js developer. \nIn-depth knowledge of JavaScript, CSS, HTML, jQuery, and front-end languages. \nKnowledge of REACT tools including React.js, Webpack, Enzyme, Redux, and Flux. \nExperience with user interface design. \nKnowledge of performance testing frameworks, including Mocha and Jest. \nExperience with browser-based debugging and performance testing software. \nTroubleshooting skills. \nProject management skills. \nProblem-solving skills. \nVerbal communication skills. \n\n Why work with Team Remotely?\n\nTeam Remotely Inc. is a staffing platform offering a seamless experience for employers and candidates. Employers can post job openings and specify their requirements, while candidates can create profiles and upload resumes.\n\nThe team of Team Remotely continuously learns and adapts based on previous successful placements, constantly improving its matching capabilities. This ensures that the recommendations provided by Team Remotely are tailored and accurate, increasing the likelihood of a successful match between employers and candidates. By providing intelligent and data-driven solutions, they strive to enhance the efficiency and effectiveness of the hiring process, ultimately helping companies find the best talent and individuals find their dream jobs."}, {"title": "Data Engineer", "company_name": "ZAG Technical Services", "location": "United States (Remote)", "link": "/jobs/view/3771403464/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=Hf0VefrKXBFIL5MtHmY0Pw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3771403464", "description": "About the job\nWe are looking for a tech savvy Data Engineer to contribute to the building, operating, and scaling of next generation data platforms and tools that will power data-driven analytics capabilities throughout the ZAG products and services portfolio, spanning areas such as business intelligence, reporting, and data analytics.\n\nThe ideal team member is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. Must be comfortable supporting the data needs of multiple clients and teams, systems, and products within the modernization of ecosystems. The right-fit person will be excited by the prospect of optimizing or even re-designing data processes to support next generation of advanced analytics capabilities, which in-turn translates to implementing strategic enhancements to maintain competitive advantage in the industry.\n\nThe successful team member will leverage their proficiency to\u2026\n\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\nBuild the analytics processes required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Data Management and cloud based \u2018big data\u2019 technologies.\nBuild analytics tools that utilize technologies to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.\nCollaborate with client teams to identify and solution regional and local requirements within the context of the standard global business model for enterprise analytics.\nParticipates in meetings, discussions, strategy sessions where changes, improvements, and enhancements are proposed, evaluated, and approved. \n\n\nThe exceptional team member will have or demonstrate progressive experiences in\u2026 \n\nBachelor\u2019s degree or higher in Computer Science or related field\nExperience working with BI and data warehousing tools\nExperience working with enterprise data, where security is paramount and data governance is critical\nAbility to work independently and in a team environment, and effectively engage all levels of the organization\nUnderstanding of distributed systems driving large-scale data processing and analytics with a successful history of manipulating, processing, and extracting value from large, disconnected datasets.\nAbility to communicate effectively (listening, presenting, and questioning)\nStrong organizational, written, and communication skills\n\n\nAbout ZAG\n\nZAG Technical Services empowers our clients\u2019 success through intelligent business solutions that increase employee productivity, network security and team collaboration. We aim to ensure that our client\u2019s IT investments drive ROI, provide a competitive advantage and help businesses grow. We do this by living our core set of values: Integrity Foremost, Client-Centered, Accountable Always, Teamwork Throughout, and Exceptional is the Goal.\n\nWhat ZAG Has To Offer\n\nCompetitive compensation package \nBenefits including medical, dental, vision, 401K and life insurance \n\n\nZAG Technical Services is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all of our employees.\n\nZAG Technical Services is an E-Verify participant and will provide the Social Security Administration (SSA) and, if necessary, the Department of Homeland Security (DHS), with information from each new employee\u2019s Form I-9 to confirm work authorization."}, {"title": "Junior Data Engineer", "company_name": "HireMeFast LLC - Career Accelerator - Land A Job", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3786249797/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D&trk=flagship3_search_srp_jobs&lipi=urn%3Ali%3Apage%3Ad_flagship3_search_srp_jobs%3BQExogrHBQoSF9zZm%2B9kufw%3D%3D&lici=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D", "id": "3786249797", "description": "About the job\nThis is a remote position.\n\n Job Title: Junior Data Engineer\n\n Employment Type: Full-Time\n\n Salary: $64,000 - $76,000 per annum\n\n Experience Required:  Minimum 1 year of project experience\n\nAbout us: HireMeFast is a leading staffing and recruitment agency specializing in connecting businesses with top-tier talent across various industries. Our mission is to bridge the gap between exceptional candidates and organizations needing their skills, expertise, and unique qualities. Our team of experienced and dedicated recruitment specialists utilizes innovative sourcing strategies, and a vast network to identify and attract top talent. We conduct comprehensive procedures to ensure that only the most qualified candidates are presented to our clients.\n\nPosition Summary\n\nJoin the fast-paced, innovative, and collaborative environment focused on providing an AIOps platform that enhances the intelligence of the CVS Health infrastructure. Work closely with subject matter experts and colleagues to build and scale out machine learning and AI solutions that will detect, predict, and recommend solutions to correct issues before system impact and enhance efficiency, reliability, and performance of CVS Health\u2019s IT operations.\n\nKey Responsibilities include:\n\nData pipeline development: Designed, implemented, and managed data pipelines for extracting, transforming, and loading data from various sources into data lakes for processing, analytics, and correlation. \nData modeling: Create and maintain data models ensuring data quality, scalability, and efficiency \nDevelop and automate processes to clean, transform, and prepare data for analytics, ensuring data accuracy and consistency \nData Integration: Integrate data from disparate sources, both structured and unstructured to provide a unified view of key infrastructure platform and application data \nUtilize big data technologies such as Kafka to process and analyze large volumes of data efficiently \nImplement data security measures to protect sensitive information and ensure compliance with data and privacy regulation \nCreate/maintain documentation for data processes, data flows, and system configurations \nPerformance Optimization- Monitor and optimize data pipelines and systems for performance, scalability and cost-effectiveness \n\nCharacteristics of this role:\n\nTeam Player: Willing to teach, share knowledge, and work with others to make the team successful. \nCommunication: Exceptional verbal, written, organizational, presentation, and communication skills. \nCreativity: Ability to take written and verbal requirements and come up with other innovative ideas. \nAttention to detail: Systematically and accurately research future solutions and current problems. \nStrong work ethic: The innate drive to do work extremely well. \nPassion: A drive to deliver better products and services than expected to customers. \n\nRequired Qualifications\n\n2+ years of programming experience in languages such as Python, Java, SQL \n2+ years of experience with ETL tools and database management (relational, non-relational) \n2+ years of experience in data modeling techniques and tools to design efficient scalable data structures \nSkills in data quality assessment, data cleansing, and data validation \n\nQualifications\n\nKnowledge of big data technologies and cloud platforms \nExperience with technologies like PySpark, Databricks, and Azure Synapse. \n\nEducation\n\nBachelor\u2019s degree in Computer Science, Information Technology, or related field, or equivalent working experience\n\nWhy HireMeFast LLC?\n\nAt HireMeFast, we understand that finding the right individuals to join your team is crucial for success and growth of your organization. We are committed to streamlining the hiring process for our clients, ensuring they have access to a diverse pool of highly qualified candidates who are a perfect fit for their specific needs."}, {"title": "Data Engineer  - Need candidates from Texas ( REMOTE )", "company_name": "Pulivarthi Group (PG)", "location": "United States (Remote)", "link": "/jobs/view/3770685212/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=9gdMwuif%2BpB%2BCQYvQi3imQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3770685212", "description": "About the job\nFollow us on Linkedin: https://www.linkedin.com/company/pulivarthigroup/\n\nPulivarthi Group LLC is a Global Staffing & IT Technology Solutions company, with our prime focus of providing world class solutions to our customers with the right talent. We combine the expertise of our team and the culture of your company to help you with the solution that is affordable and innovative using high quality standards and technologies.\n\nWe\u2019ve served some of the largest healthcare, financial services, and government entities in the U.S.\n\nData Engineer - FT only - Azure SQL with data lakes experience\n\nData Engineer -\n\nthe resource requirement for this Data Engineer role needed more clarification.\n\nOur customer is requesting a resource who knows Azure CI/CD (Continuous Integration / Continuous Deployment) to build and integrate their DevOps pipeline.\n\nOnce the DevOps pipelines are mapped, they need the Data Engineer to connect to all the Data Sources, primarily their Data Lake.\n\nThe Customer (based in Houston) is interested in a resource in a time zone not too far removed from the Central Time Zone to allow real time collaboration."}, {"title": "Data Engineer", "company_name": "Otter Products", "location": "Fort Collins, CO (Remote)", "link": "/jobs/view/3766728302/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=77U3U7CAi1PxgDmpI3%2BiNQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3766728302", "description": "About the job\nOverview \n\nOtter Products is currently recruiting for a Data Engineer to join our Data team! You can be based in our Fort Collins, CO office with flexibility to work a portion of your time remotely, or 100% Remote in the U.S.\n\nThe Data Engineer applies professional experience, concepts, and company objectives toward building and operationalizing the minimally inclusive data necessary for the enterprise data initiatives following industry standard practices and tools. The bulk of the Data Engineer's work would be in designing, managing and optimizing data pipelines, then moving these data pipelines effectively into production for key data and analytics consumers like business/data analysts, data scientists or other stakeholder that needs curated data for data and analytics use cases across the enterprise. The Data Engineer also ensures compliance with data governance and data security requirements while creating, improving and operationalizing these integrated and reusable data pipelines to enable faster data access, integrated data reuse and vastly improved time-to-solution for data and analytics initiatives. Additionally, the Data Engineer will be expected to collaborate with other data team members and data consumers working on the models and algorithms developed by them toward optimization for data quality, security, and governance to put them into production leading to potentially large productivity gains.\n\n About Otter Products \n\nOtter Products, we grow to give. From our founder's garage in 1998 to the global technology leader we are today, Otter Products continues to drive growth through innovation.\n\nThrough our industry-leading brands - OtterBox, Liviri and OtterCares - we provide our partners the number-one selling and most trusted products in our categories. Our philanthropic spirit is the foundation on which we foster our partner relationships, allowing us to grow and to give - together.\n\nBy way of our charitable arm, the OtterCares Foundation, we support our communities and invest in our future through education that inspires kids to change the world.\n\nAnd even as our global community of Otters continues to grow, our founder's core values are still at the heart of everything we do. We measure our success by our ability to give back to our communities and strengthen opportunities for all.\n\nFor more information visit otterproducts.com\n\n Responsibilities \n\nComplete analysis, design and development of BI solutions using Azure SQL Server\nFamiliarity with Data Warehouse concepts\nExperience coding the extraction, transformation, and loading (ETL) processes\nDatabase development primarily in SSIS, Data Factory and SQL\nPartner with the business to determine end user database/reporting needs and requirements\nGenerate ad hoc reports using Power BI or SSRS\nCollaborate with other developers to create and implement best approach solution(s)\nTroubleshoot Azure tools, systems, and software\nReview queries for performance issues, making changes as needed\nCollaborate with team to performance-tune Azure application as necessary\nAssist with the analysis and extraction of relevant information from large amounts of historical business data to feed data science initiatives\nParticipate and support the design and documentation of processes for large scale data analyses, model development, model validation and model implementation\nSupport and maintain a positive safety culture by following all safety policies and procedures and actively contributing to a safe working environment\nOther duties as required\n\n Qualifications \n\nBachelor's degree required. Degree in Computer Science or Mathematics preferred\nMinimum of four years of experience in an IT or analytical role required. Experience in database development, report writing and/or statistics preferred.\n\n EEO \n\nOtter Products, LLC is an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, age, marital status, pregnancy, sex, sexual orientation, gender, gender identity or expression, national origin, disability, veteran status, or any other characteristic or status protected by law.\n\n For US Based Roles Only - Compensation Range Minimum \n\nUSD $90,000.00/Yr.\n\n For US Based Roles Only - Compensation Range Maximum \n\nUSD $120,000.00/Yr.\n\n Additional Total Rewards \n\nProfit Sharing Program Eligible, Benefits Eligible - Full Time- check out otterproducts.com/careers/why for more info"}, {"title": "Software Engineer, Distributed Systems", "company_name": "Hightouch", "location": "United States (Remote)", "link": "/jobs/view/3628572539/?eBP=CwEAAAGMd4QO8APODyE_-mMOPhYyQGg5ZjVbPwq_3V7DK0B2bLAf10pcedM9S2T71X5i4TiJuEwOzWBbSiTKbLuk-VD-_Y-dN5uyf8YpYkHYqEYFxySMS9zr2hRjNrxX9gXbxssh35TFK9YTNV2WeH2Ncb675JgFbJTgNWSZySFC6rrdJGmdSDTr2q5d3VoxORz3GjsDbzjeyZQCvQ07RqYw7oujWSgslycs2LMCY_-N0NEj6e0PvbaYKiZ_5IW34bGVSYLXpHwqa8oOcNttYZ90UA3E_7sDSFzABdIVlfrp5zzERrIYy5sh2MIInxHdDoKgQw8VmOn5uM4S0tXUl-i7LnN0_W0NfPBmLiVDNuveIOm4MfDNPxfe5QKPk6FUAVpvgbcoJn9yRyE&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=rph%2FT6Q6Vkp6YoggJf9cJQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3628572539", "description": "About the job\nAbout Hightouch\n\nHightouch\u2019s mission is to empower everyone to take action on their data. Through our Reverse ETL platform, business and data users can seamlessly sync data from where it resides, such as warehouses and databases, to where it is needed, including operational systems and SaaS tools. Traditionally, acting on data has required engineering time and bandwidth, and left most business users stuck with charts and reports that are unable to take automated action on their data. With Hightouch, every business user, without writing any code, can activate data to streamline critical processes, improve marketing performance, and scale operations.\n\nOur team operates with a focus on making a meaningful impact for our customers. We believe in approaching challenges with a first principles thinking mindset, moving quickly and embracing our value of efficient execution, and treating each other with compassion and kindness. We look for team members that are strong communicators, have a growth mindset, and are motivated and persistent in achieving our goals.\n\nHundreds of companies use Hightouch, including Spotify, Ramp, Retool, NBA, Plaid, and Betterment. We\u2019re based in San Francisco, are remote-friendly, and backed by leading investors such as Amplify Partners, ICONIQ Growth, Bain Capital Ventures, Y-Combinator, and Afore Capital.\n\nAbout The Role\n\nWe are looking for a distributed systems engineer to work on the systems that power the syncing engine our customers and other engineering teams rely on. This presents an exciting challenge where you can apply your expertise in distributed systems, performance optimization, and troubleshooting to push the boundaries of what is possible and meaningful improve our ability to move massive amounts of customer data. This role also provides a unique opportunity to work on a multi-cloud and multi-region infrastructure that supports a global customer base.\n\nWe believe in enabling our engineers to do their best work for our customers by giving them extremely high levels of ownership and autonomy. This comes in different forms: you will own and deliver projects from start to finish, you will work directly with customers to solve their hardest scaling problems, and you will have a lot of influence over what we work on as a team and company.\n\nSome of the problems we\u2019ll be working on include:\n\nSync Speed: Customers want to sync a lot of data to important destinations like Facebook and Snapchat, which requires us to analyze every part of our syncing process and find where we can optimize to sync data more quickly\nStreaming Syncing: We currently sync data from sources in batch only, but envision a world where we also do syncs in real-time and support streaming sources like webhooks and queues\nScalability and Reliability: As part of our rapid growth, we\u2019re always evaluating current future bottlenecks in scaling and reliability, and architecting for the next order of magnitude of growth\nPersonalization API: Our Personalization API product provides a low latency caching layer on top of data warehouses that enable customers to use their data warehouse for real-time personalization use cases with \nMulti-Region and Multi-Cloud: Supporting our multi-region and multi-cloud backend, including extending it to launch Hightouch on in new regions to support data residency requirements of our global customer base\n\nWe are looking for talented, intellectually curious, and motivated individuals who are interested in tackling the problems above. This is a senior role, but we focus on impact and potential for growth more than years of experience. The salary range for this position is $170,000 - $240,000 USD per year, which is location independent in accordance with our remote-first policy.\n\nAbout You\n\nYou are an engineer with a passion for solving hard technical problems that generate real value for customers. You\u2019re motivated by high ownership and are comfortable in a fast-paced, startup environment.\n\nYou have experience and comfort with distributed systems and high-scale systems or the ability and desire to build this experience quickly. This experience can take different forms:\n\nYou\u2019ve built syncing engines that process and move terabytes of data per day in a scalable way\nYou\u2019ve created event collection or data streaming services that power company critical systems\nYou\u2019ve owned systems that have undergone orders of magnitude of growth and helped them scale accordingly\nYou have relevant skills and are a fast learner who is excited to take on the challenges we face\n\nInterview Process\n\nOur goal with the interview process is to balance speed with giving both parties opportunities to assess whether there is a strong mutual fit. We will ask you questions, but we want you to ask us questions! Our technical interviews focus on how you design systems because we believe this is the best way for us to see how you work and for you to see how we collaborate. We don\u2019t ask you to write code to solve technical brainteasers that don\u2019t appear in your day to day job.\n\nRecruiter screen [30m]: Introductory call with our recruiting team to get to know each other and see if the role could be a good mutual fit.\nSystems design screen [45m]: Designing a data processing feature end-to-end.\nHiring manager interview [30m]: Chat with hiring manager about past experiences and future operating preferences to assess fit on company values and operating principles.\nSystems design interview [90m]: Work with the interviewer to architect a system at a conceptual level. The problem will be at a pretty high level - and have both product and customer requirements as well as technical."}, {"title": "Entry-Level Data Scientist Engineer", "company_name": "Phoenix Recruitment LLC", "location": "Texas City, TX (Remote)", "link": "/jobs/view/3788727625/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=8iwtytHsCu4wMwJ4fj3pxw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3788727625", "description": "About the job\nThis is a remote position.\n\nTitle - Entry-Level Data Scientist Engineer, 1 year of project experience\n\nEmployment Type: Full-time\n\nBase Salary: $60K-$70K\n\nPhoenix Recruitment offers a variety of recruiting services to assist both employers and employees. They specialize in marketing open positions, recruiting, and helping employers to find qualified candidates across various industries. Phoenix Recruitment has expertise in streamlining the hiring process. They can help ensure that the process is efficient, well organized, and compliant with relevant regulations.\n\nSkills and Abilities:\n\nStrong knowledge of R or Python for data analysis and modeling. \nProficiency in statistical programs such as R, SAS, MATLAB, or Python. \nFamiliarity with spreadsheets (VBA) and database applications (Access, Oracle, SQL, or equivalent technology). \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \nAbility to learn new methods quickly and work under deadlines. \nExcellent teamwork and communication skills. \nStrong analytical and problem-solving abilities. \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \n\nPreferred:\n\nKnowledge of actuarial concepts and life, health, and/or annuity products. \nExperience with statistical modeling techniques such as GLM, Decision Trees, Time Series, Regression, etc. \nFamiliarity with Microsoft DeployR. \nExposure to insurance risk analysis. \nBasic experience in computational finance, econometrics, statistics, and math. \nKnowledge of SQL and VBA. \nFamiliarity with R or Python for predictive modeling \n\nWhy Phoenix Recruitment LLC?\n\nPhoenix Recruitment often has an extensive network of employers and candidates. This network allows them to tap into a pool of qualified candidates and connect them with suitable job opportunities. They can also leverage their connections to help employers find the right talent efficiently. Outsourcing the recruitment process to a specialized agency can save your time and resources, avoid delays, reduce administrative burdens, and increase the chances of finding the right fit for your organization."}, {"title": "Data Engineer - Remote Work", "company_name": "BairesDev", "location": "United States (Remote)", "link": "/jobs/view/3783162297/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=OtoCrAAHZFomVup04fy35g%3D%3D&trk=flagship3_search_srp_jobs", "id": "3783162297", "description": "About the job\nWho We are\n\nBairesDev is proud to be the fastest-growing company in America. With people in five continents and world-class clients, we are only as strong as the multicultural teams at the heart of our business. To consistently deliver the highest quality solutions to our clients, we only hire the Top 1% of the best talents and nurture their professional growth on exciting projects.\n\nWe are looking for a Data Engineer to join our Development team and participate in different projects made up of multicultural teams distributed throughout the world.\n\nWhat You Will Do\n\n Interview other Data Scientists and Data engineers to vet their technical abilities.\n Input data from those interviews into the product and help develop a predictive, fair, and enjoyable solution.\n Work together with developers, tech leads, and solution architects to build applications.\n Improve existing structures by adding new functionalities or proposing technological updates so that as a result, the impact of your contribution is significant in the core of each business.\n Improve the user experience on scalable and high availability platforms, contributing to the key differential of each business.\n\nHere\u2019s What We Are Looking For\n\n 5+ years of experience working as a developer.\n 3+ years of experience working as a Data Scientist / Data Engineer.\n Proficient with analysis, troubleshooting, and problem-solving.\n Hands-on experience with managing data loads and data quality.\n Advanced English level.\n\nHow we do make your work (and your life) easier:\n\n 100% remote work.\n Hardware setup for you to work from home.\n Flexible hours - make your schedule.\n Paid parental leave, vacation & holidays.\n Diverse and multicultural work environment.\n An innovative environment with the structure and resources of a leading multinational.\n Excellent compensation \u2014 well above the market average.\n Here you can grow at the speed of your learning curve.\n\nOur people work remotely but with a consistent and robust culture that promotes diversity and teamwork. To continue being the leading software development company in Latin America, we want to ensure that every BairesDev member gets the best growth and professional development opportunities in a diverse, welcoming, and innovative environment.\n\nEvery BairesDev team member brings something unique to our company.\n\nWe want to hear your story. Apply now!"}, {"title": "Data Engineer (Remote)", "company_name": "Cascade Debt", "location": "United States (Remote)", "link": "/jobs/view/3755336571/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=pmUtn1j2zyPr25CNPh2RNw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3755336571", "description": "About the job\nDo you want to work in a small team where you can make a real difference in a company? Would you embrace the challenge of building a high impact platform that is used globally? Then we want to speak with you!\n\nAbout Cascade\n\nCascade is a fintech startup backed by Canadian and US investors that empowers companies to grow by democratizing access to institutional debt. We are building tools that modernize how companies raise and manage debt, lowering the barriers to entry to the $7 trillion specialty finance and alternative credit market for companies around the world. Founded just last year, we are gearing up for our next phase of technical development and are seeking a talented Data Engineer to lead the way.\n\nAbout The Role\n\nAs a Data Engineer you\u2019ll be an early member of a growing team building a pioneering platform in the debt infrastructure space. You will design, build, and launch efficient, scalable, and reliable data pipelines to move and transform data .\n\nWhat You\u2019ll Do\n\n Building databases, data lakes and data ingestion pipelines to integrate customer databases and datasets to our systems \n Ingesting financial datasets from external customers, then u pdating and maintaining accurate and complete data mappings to ensure that our product s are displaying high quality data \n M onitoring and alerting across our data pipelines in order to make sure that our data ingests are reliable and correct \n Translat ing business goals and stakeholder requirements into new data flows and deliver analytical insights ensuring that the data flows create real business value continuously \n\nSkills And Experience\n\n 2 + years experience in a data engineering or data analyst role \n E xperience with relational SQL and NoSQL databases, including PostgreSQL , MySQL, MariaDB, MongoDB, Bigtable, etc. \n E xperience working with ETL technologies , such as Databricks, Fivetran , or dbt \n Proficiency in writing SQL queries and knowledge of analytical data warehouses such as RedShift, BigQuery , and Snowflake \n Some experience with CI/CD automation such as GitHub Actions \n Developing APIs and integrating with 3rd party APIs \n Bonus: experience in fintech / SaaS / credit analysis \n\nPeople Who Thrive At Cascade Are\n\n Self-starters, who take the initiative to tackle challenges in a remote work environment \n Good communicators, who can operate without ego to discuss, learn, grow, and help others do the same \n Passionate about creating great digital experiences for users \n Problem solvers who are not satisfied with the status quo \n\nBenefits\n\nRemote: we are a remote-first company and are very flexible on hours as long as things get done \nHome-office: there\u2019s a $1000 USD home office allowance to set yourself up \nEquity: we expect you to have an owner-mentality, and have the equity plan to match \nBenefits: health, dental, vision, and more \nPerks: we offer free lunches weekly and off-site trips \nJob satisfaction: we offer autonomy, ample opportunities for mastery, and an opportunity to make a difference for companies around the world"}, {"title": "Data Engineer", "company_name": "ISC (Integrated Specialty Coverages, LLC)", "location": "Carlsbad, CA (Remote)", "link": "/jobs/view/3778293528/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ew7%2FfEuXSujs5IGabdqUyw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3778293528", "description": "About the job\nAbout Integrated Specialty Coverages\n\nIntegrated Specialty Coverages LLC (ISC) is a growth stage technology and data-driven insurance company leading innovation in the market.\n\nBacked by one of the leading private equity firms, KKR, and led by a forward-thinking management team, ISC is combining the worlds of insurance and technology to create an Insurtech powerhouse. As a leading online distributor of insurance products for a range of industries and \"Main Street USA\", we are looking for the right people to help us in our mission of achieving exponential growth. We strive to be the number one place to go for brokers and agents to source insurance. To accomplish this, we're building a digitally focused team that deeply understands the intersection between user experience, data, and AI/ML to optimize the way we engage with our customers and partners.\n\nJob Summary\n\nThe Data Engineer role will be the technical liaison between multiple groups including a data science team, the engineering team, product management, and business stakeholders. You do not need any insurance knowledge prior, however, you must quickly dive deep into the insurance world and ask questions to become a subject matter expert. You will be responsible for building a data platform to facilitate the data science team. You must be a self-starter that can build out features such as a data pipeline from scratch. There will be support from both engineering and data science for any buildout.\n\nJob Responsibilities\n\nCreate and maintain optimal data pipeline architecture. \nAssemble large, complex data sets that meet functional / non-functional business requirements. \nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. \nBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using AWS technologies, SQL, and Python. \nWork with stakeholders including the Executive, Product, Data Science, and Engineering teams to assist with data-related technical issues and support their data infrastructure needs. \nWork with data science and analytics teams to strive for greater functionality in our data systems. \n\nMinimum Qualifications:\n\nAdvanced working SQL knowledge and experience working with relational databases, strong query authoring (SQL) as well as working familiarity with a variety of databases (Snowflake, Redshift, MySQL, MSSQL, etc.)\nExperience building and optimizing data pipelines, architecture and data sets. \nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. \nBuild processes supporting data transformation, data structures, metadata, dependency, and workload management. \nA successful history of transforming, processing and extracting value from large disconnected, datasets from a variety of data sources (Flat files, Excel, databases, etc.)\nStrong analytic skills related to working with unstructured datasets. \nStrong project management and organizational skills. \nExperience supporting and working with cross-functional teams in a dynamic environment. \nAbility to mentor/guide/collaborate with other team members. \n\nWe are looking for a candidate with 3-5 years of experience in a Data Engineer role. They should also have experience using the following software/tools:\n\nExperience with relational and MPP databases, including Snowflake and MySQL. \nExperience developing software in an agile environment from the requirements stage to production. \nExperience with version control: git\nExperience with container technologies: Docker\nExperience with data pipeline and workflow management tools: Airflow, Jenkins, AWS Glue, Azkaban, Luigi, etc. \nExperience with AWS cloud services: EC2, ECS, Batch, S3, EMR, RDS, Redshift\nExperience with other cloud services: Snowflake, Airflow\nExperience with object-oriented/object function scripting languages: Python, Java, C++, etc. \nExperience with data modeling and data warehouse design\nExperience with data visualization tools (PowerBI, QuickSight)\n\nThe starting annual pay scale for this position is listed below. Actual starting pay will be based on factors such as skills, qualifications, training, and experience. In addition, the company offers comprehensive benefits including medical, dental and vision insurance, 401(k) plan with match, paid time off, and other benefits.\n\nISC's salary ranges are determined by role and level. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations and could be higher or lower based on a multitude of factors, including job-related skills, experience, and relevant education or training.\n\nNational Pay Range\n\n$100,000\u2014$110,000 USD\n\nBenefits of Working at ISC\n\nCompetitive vacation and flexible working arrangements\nComprehensive and inclusive health benefits\nA variety of professional development and mentorship opportunities\nChoice of technology whether at home or in the office\n\nISC's Ownership Behaviors\n\nDo the Right Thing *Be Relentless. Pursue Excellence. *Personal Responsibility & Own Outcomes\nTry Fast. Learn Fast. Fail Fast. Think Big. *Build Extraordinary Owners & Join Forces *Grit & Determination\n\nApplicants with disabilities may contact ISC HR department via e-mail, and other means to request and arrange accommodation. If you need assistance to accommodate a disability, you may request accommodation at any time. Please contact ISC at HR@ISCMGA.com.\n\nISC believes in creating long-term relationships by being responsive and relevant and by consistently delivering value to our community of customers. Specifically, with our employees, we focus on attracting, developing, and retaining the best talent for our business, challenging our people, demonstrating a \"can-do\" attitude, and fostering a collaborative and mutually supportive environment. ISC is an equal opportunity employer and a participant in the US federal E-Verify program (US).\n\nDiversity creates a healthier atmosphere: All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, gender, gender identity, sexual orientation, marital status, medical condition, genetic information, mental or physical disability, military or veteran status, or any other characteristic protected by local, state, or Federal law."}, {"title": "Data Engineer -Remote", "company_name": "RIT Solutions, Inc.", "location": "Denver, CO (Remote)", "link": "/jobs/view/3768007496/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=1hQH8EDaII1eJ7nQ9BACQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3768007496", "description": "About the job\nDuration: 6+ months CTH\n\nMust have a valid LinkedIn profile\n\nRequired Skills\n\n 3-6 years of experience (no more than 6 years)\n Start up environment experience\n Python, SQL, and DBT"}, {"title": "Big Data Engineer - W2", "company_name": "Megan Soft Inc", "location": "United States (Remote)", "link": "/jobs/view/3674845652/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fCAxh4N%2F4YZJ5aEIASaeeA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3674845652", "description": "About the job\nTitle: Big Data Engineer - W2\n\nLocation: Dearborn, MI (Remote)\n\nDuration: 12+ Months\n\nDirect Client: Ford Motors\n\nMust have skills: Should be able to handle GCP requirements\n\n\"Translates business requirements to build repeatable, sustainable, efficient, coded processes that can be productionized by Software Engineers and readily modified by other Data Engineer Creates POC processes in Dev/QA and works with Software Engineers to productionize and align best coding and process practices that maximize efficiency, speed, stability, system resources and capabilities Awareness of and compliance with data privacy, security, legal and contractual guidelines Incorporates data quality and privacy checks/alerts to minimize bad data being consumed by end users, models and dashboards, and to protect customer data; revises checks for new data issues Validates data products and pipelines are functioning as expected following system or application upgrades, source changes, etc. Works closely with Architects if required to align systems, tools and applications being utilized with business use case and performance requirements Provides data product support and maintenance Establishes, tracks and monitors KPIs related to specific data products and deliverable Participate in iteration cadence activities (Iteration Planning, task estimation, daily scrum, demos, etc.). Will be a part of on-call team to resolve critical production support tickets during business hours and off hours.\n\nSkills Required\n\nExperience with Big Data Technologies (Hadoop , ETL Tool, Spark. Hive etc..) Someone who understands Cloud and has experience working with Cloud Platforms (preferably GCP) Should be able to handle GCP requirements - ingesting into GCS/ Big Query, working with GCP components Hands-on in Data Flow, Cloud Storage (GCS), Big Query and GitHub. Proficient in Python and SQL.\n\nSkills Preferred\n\nManages ETL: uses programming and tools for data ingestion, configures pipelines, applies transformations and decoding, integrates and fuses data, moves and securely deliver Experience NoSQL databases\n\nAdditional Information\n\nIf the candidate is remote only, please indicate \"\"100% Remote\"\" under candidate's name on resume. Other similar schedule notations can be \"\"Local Candidate\"\" or \"\"Hybrid Candidate.\"\" ***HRA Test Required***\""}, {"title": "Remote Opportunity: Quantexa Data Engineer", "company_name": "SPAR Information Systems LLC", "location": "United States (Remote)", "link": "/jobs/view/3680073191/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ioykVCZSOQ7BK5%2FwRQHOWg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3680073191", "description": "About the job\nHello All, Hope you are doing great!! Please go through the job descriptions and let me know your interest. Role: Quantexa Data engineer  Location: Remote Duration: Long Term Contract Job Description: 1. End to End usage of Quantexa including guiding our client team on how to use from both front end and data ingestion Reason they are asking for Certification 2. Resolve Entity issues with data 3. Knowledge of Scala and Hive which requires coding experience using parquet 4. Advanced SQL knowledge requiring writing SQL queries 5. Experience having worked with GCP data lake 6. Quantexa certification required\n\nThanks & Regards,\n\nSatnam Singh\n\nDirect: 201 623 3660\n\nEmail : Satnam.singh@sparinfosys.com"}, {"title": "Remote Work - Need Staff Data Engineer - USC only", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3746283980/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=CvIuLBNO%2BPMGi4IXVRkP0A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3746283980", "description": "About the job\n100% Remote, client is in NY\n\nLook for strong job history, Python, Machine Learning, SQL, 10/10 comms, etc. \n\nMUST be an Engineer \n\nSummary\n\nWe're looking for an experienced Data Engineer to join our client's growing team of top notched IT professionals. You will work closely across multiple groups including Editorial and Audio, Marketing, Advertising, etc. to achieve their goals and objectives. The Data Engineering team builds tools throughout the stack while sharing knowledge across these departments.\n\nAs the company grows, they're looking for Data Engineer who will help solidify and expand our pipelines and maintain their data warehouse. Their business model is based on performing complex and very detailed analyses of how our products perform with their subscribers. You'll be responsible for working alongside analysts, product, marketing, and other teams to define new data collection components and measurement schemes that support each new product and feature.\n\nHow Can Add Value To Their Mission\n\nCreate and maintain data pipelines to provide insights and drive business decisions\nEstablish data warehousing strategy (ex. Kimball, Data Vault, etc.)\nMaintain data infrastructure on our AWS accounts\nCollaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.\nWrite unit/integration tests, contributes to engineering wiki, and documents work\nImplement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it\n\nWhat You'll Need To Succeed\n\n7+ years of industry experience measuring product performance and user behavior\nExperience working with a variety of data management technologies, including RedShift, Kinesis/Kafka, Glue, Spark, Postgres, Airflow, dbt, and others.\nExperience implementing BI reporting tools such as Looker\nExperience interfacing with engineers, product managers and analysts to understand data needs\nKnowledge of a variety of measurement beacons, SDKs, APIs including Google Analytics, Amplitude, Braze, Stripe, email service providers\nA commitment to building secure, resilient, fault-tolerant architectures, with clear documentation and procedures in place for support\nA focus on accuracy and detail as you build, ensuring data pipelines produce clear, predictable results\nUnderstanding of the typical metrics a subscription and advertising-supported business needs to measure success\nExperience with Client techniques as applied to behavioral segmentation or anomaly identification is a definite plus\nFamiliarity using developer tools that increase productivity and facilitate the development of resilient code (eg. Docker, CircleCI, Serverless) is a plus"}, {"title": "ENTRY LEVEL DATA ENGINEER (REMOTE)", "company_name": "SynergisticIT", "location": "Jacksonville, FL (Remote)", "link": "/jobs/view/3767587909/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=om7L3CnowiC4PLIcpZD70A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3767587909", "description": "About the job\nSYNERGISTICIT wants every candidate to know that the Job Market is Challenging and to stand out, you need to have exceptional skills and technologies and that's where we come in to make sure you get the attention which you need\n\nSynergisticIT understands the complex nature of the job market and how difficult it can be to secure a position, especially for fresh graduates. Therefore, we assist and help tech-savvies to convert their passions into professions. We go above and beyond to keep you working in your niche.\n\nAs we focus on long-term success, we provide complete career development solutions. From job search to upskilling portfolio and interview preparation, we can guide you at every step of your career.\n\nSynergisticIT spares no efforts to connect you with a large network of tech giants, including Google, Apple, PayPal, Dell, Cisco, Client, etc. Presently, we are actively looking for ENTRY LEVEL DATA ENGINEER a driven mindset. Get the right opportunity and gain experience in building web-centric solutions on Java.\n\nWho Should Apply : Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or anyone looking to make their career in IT Industry\n\nWe also assist in filing for STEM extension and H1b and Green card filing.\n\nCandidates who are serious about their future in the IT Industry and have set big goals for themselves.\n\nCandidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. We also offer Skill Enhancement Programs if the candidates are missing skills or experience which our clients need with great outcomes\n\nCandidates can benefit from skill enhancement if they fall into the below categories. If they are qualified with enough skills then no need for skill enhancement\n\nCandidates Who Lack Experience\n\nHave had a break in careers\n\nLack Technical Competency\n\ncandidates who want to get employed and make a career in the Tech Industry \n\nPlease Also Check The Below Links\n\nhttps://www.synergisticit.com/candidate-outcomes/\n\nhttps://www.synergisticit.com/java-track/\n\nhttps://www.synergisticit.com/data-science-track/\n\nhttps://www.synergisticit.com/which-is-the-best-option-for-tech-job-seekers-staffing-companies-consulting-companies-bootcamps-or-synergisticit/\n\nhttps://www.synergisticit.com/contact-us/\n\nIf the skills are not a match candidates can opt for Skill enhancement. Or their resume can be sent out to clients to see if responses are achievable\n\nREQUIRED SKILLS For Java/Software Programmers\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Core Java , javascript , C++ or software programming\nSpring boot, Microservices and REST API's experience\nExcellent written and verbal communication skills\n\nFor data Science/Machine learning\n\nRequired Skills\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Statistics, Python, data visualization tools\nExcellent written and verbal communication skills\n\nPreferred skills: NLP, Text mining, Tableau, Time series analysis\n\nTechnical skills are required by clients for selection even if its Junior or entry level position each additional Technical skill helps a candidate's resume to be picked by clients over other job seekers.\n\nClients hire candidates with the right technical skills which they need and reject candidates who lack the required technical skills.\n\nNo third party candidates or c2c candidates\n\nPlease apply to the posting\n\nNo phone calls please. Shortlisted candidates would be reached out"}, {"title": "Data Engineer - Remote | WFH", "company_name": "Get It Recruit - Information Technology", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3782241984/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=BgBf1kSAAX%2BSuw6inJsIVw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3782241984", "description": "About the job\nJoin an innovative team at a leading company in the engineering and manufacturing industry, where we are committed to leveraging data for continuous improvement and operational excellence. This hybrid position allows you to enjoy the flexibility of remote work on Mondays and Fridays.\n\nWhy Join Us\n\nAt our company, we prioritize your well-being and professional growth. We offer a comprehensive benefits package, including excellent health benefits, a 6% 401(k) match, generous paid vacation and sick time, 12 paid holidays, and more.\n\nRole Overview\n\nAs a Data Engineer, you will play a pivotal role in transforming raw data into actionable insights, driving improvements in engineering and manufacturing processes, enhancing product quality, and optimizing operational efficiency. Collaborate with dynamic cross-functional teams to extract, analyze, and visualize data, influencing strategic decisions through your valuable insights.\n\nKey Responsibilities\n\nData Collection and Integration:\n\nGather, clean, and integrate data from diverse sources, including production databases, sensor data, and external datasets.\n\nData Analysis\n\nConduct exploratory data analysis to identify patterns, trends, and anomalies.\n\nPerform statistical analysis and hypothesis testing to derive meaningful insights.\n\nData Visualization\n\nCreate informative and visually appealing dashboards and reports using tools like Tableau, Power BI, or Python libraries.\n\nPredictive Modeling\n\nDevelop predictive models to forecast manufacturing performance, quality, and operational outcomes.\n\nRoot Cause Analysis\n\nInvestigate production issues and quality concerns by analyzing data to identify root causes and recommend corrective actions.\n\nPerformance Monitoring\n\nEstablish and maintain key performance indicators (KPIs) to track manufacturing and engineering metrics.\n\nGenerate regular reports to communicate performance trends to stakeholders.\n\nCross-functional Collaboration\n\nCollaborate with engineering, manufacturing, and quality assurance teams to understand their data needs and provide analytical support.\n\nContinuous Improvement\n\nSuggest process improvements and optimizations based on data analysis.\n\nData Security And Compliance\n\nEnsure data security and compliance with relevant regulations, such as GDPR or industry-specific standards.\n\nInfrastructure And Development\n\nBuild and maintain the infrastructure for collecting, storing, and processing data.\n\nDesign, develop, and manage data pipelines, ETL processes, and data warehouses.\n\nRequired Experience\n\nProven experience as a Sr. Data Analyst and/or Data Engineer in an engineering or manufacturing environment.\n\nProficiency in data analysis tools and programming languages, such as Python, R, SQL, and Excel.\n\nStrong data visualization skills with tools like Tableau or Power BI.\n\nExcellent knowledge of statistical analysis and machine learning techniques.\n\nAbility to communicate complex findings and insights effectively to both technical and non-technical stakeholders.\n\nStrong problem-solving skills and attention to detail.\n\nFamiliarity with manufacturing processes and quality control is a plus.\n\nExperience with enterprise ERPs (SAP S4HANA, Oracle EBS), CRMs (Salesforce, Hubspot), and IoT.\n\nProficient in working with cloud computing platforms including AWS, GCP, and Azure.\n\nExpertise in configuring and optimizing cloud-based data solutions for scalability, reliability, and performance.\n\nBachelor's Degree in Data Science, Statistics, Engineering, or Computer Science Required.\n\nApply Now and be part of a team shaping the future of data-driven innovation in engineering and manufacturing!\n\nEmployment Type: Full-Time"}, {"title": "Entry Level Software Engineer - Data Backend Engineer (Remote - Canada)", "company_name": "Yelp", "location": "United States (Remote)", "link": "/jobs/view/3752881757/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fankeoxifvhpTzOYvowHYQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3752881757", "description": "About the job\nYelp engineering culture is driven by our values: we\u2019re a cooperative team that values individual authenticity and encourages creative solutions to problems. All new engineers deploy working code their first week, and we strive to broaden individual impact with support from managers, mentors, and teams. At the end of the day, we\u2019re all about helping our users, growing as engineers, and having fun in a collaborative environment.\n Interested in exploring data and finding innovative ways to collect it, curate it, and make it easily accessible for others to use? How about doing that at an enormous scale? Yelp\u2019s datasets contain billions of interactions between users and local businesses around the globe. If working with our vast amount of data sounds exciting, come and join us!\n\nThe Marketplace Data Observability team is responsible for managing, governing, and monitoring all of Yelp\u2019s Ads data to protect our revenue stream and also support offline analysis that aids in making long term company decisions. We build tools that help us to better understand our data quality, and we strive to uncover data issues before they impact our consumers.\n\nAs a Data Backend Engineer on this team, you will be responsible for building elegant and scalable data products that serve critical, up-to-date, structured information to support different types of analytics within Yelp. As a core contributor to our growing data modeling and data warehousing engineering efforts, you will help design and own mission-critical data flow pipelines and datastores to enable decisions including effective A/B testing and company investments.\n This opportunity is fully remote and does not require you to be located in any particular area in Canada. We welcome applicants from throughout Canada. We\u2019d love to have you apply, even if you don\u2019t feel you meet every single requirement in this posting. At Yelp, we\u2019re looking for great people, not just those who simply check off all the boxes.\n  Build systems that can effectively store and crunch terabytes of data.\nDesign and develop data models for efficient data storage, retrieval, and reporting.\nCreate and maintain conceptual, logical, and physical data models using industry-standard modeling tools.\nCollaborate with cross-functional teams, including engineers, data analysts, business analysts, and data scientists, to understand data requirements and translate them into effective data models.\nParticipate in data integration efforts, including ETL processes and data migration.\nStay up-to-date with industry best practices, emerging technologies, and trends related to data warehousing.\nSupport on-call rotations as needed to operate the team.\n  Understanding of high performing and scalable data systems.\nExperience in building and orchestrating ETL pipelines.\nExperience with Data Lake or Data Warehouse landscape.\nA hunger for tracking down root causes and fixing them in systematic ways.\nAbility to communicate effectively to technical and non-technical cohorts alike. \nExposure to some of the following technologies: Python, AWS Redshift, AWS Athena / Apache Presto, Big Data technologies (e.g S3, Hadoop, Hive, Spark, Flink, Kafka etc), NoSQL systems like Cassandra, DBT."}, {"title": "Data Engineer (Associate Consultant), June 2024", "company_name": "UDig", "location": "Nashville, TN (Remote)", "link": "/jobs/view/3716039960/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=XUjo40kU55HnDCqJ%2F3FdPg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3716039960", "description": "About the job\nCan UDig It? \n\nUDig designs, builds, and implements technology solutions that deliver on business objectives. People join our team because they are excited to have an impact on the future of our organization, passionate about delivering meaningful results, and energized by solving complex problems. Oh, and they like working with other driven, smart, and authentic people!\n\nUDig is proud to be recognized as a \"Best Places to Work\" in Virginia and Tennessee. We're committed to fostering a transparent environment where leadership is accessible, professional development opportunities are abundant, and you are empowered to achieve your career goals. You'll collaborate, develop deep relationships with teammates, and accelerate your success while having fun!\n\nIf you're graduating in May 2024 and excited by the idea of impacting organizations by building data solutions to achieve business goals, you should consider joining our growing technology team.\n\nHow Things Go Down\n\nAs a new, entry-level teammate, you'll take part in our \"Breaking Ground\" training program designed to help you quickly get up-to-speed on UDig's delivery approach within a project team. The program covers:\n\nUDig delivery process and methodologies such as gathering requirements and delivering projects using agile\nTechnical guidance and instruction on the best practices from Data leaders at UDig\nDelivering executive and technical presentations\nCoaching to prepare you to for success on your first client project at UDig\n\nA typical day might entail:\n\nProject standups and check-ins\nDesign and development of data pipelines, BI reports, and more\nPlanning and participating in project demos\nContributing to internal UDig projects\nCareer and job growth through UDig-provided training\nCreating long-lasting friendships with teammates through fun activities as well as your projects (including social events, winning a prize for guessing the weight of pumpkin, or sharing experience on cool tech side projects)\n\nUDig believes it is important for Associate Consultants going through our Breaking Ground program to reside in one of our major markets (Richmond, VA or Nashville, TN metro area) for at least the first year of employment.\n\nWant to learn more? Hear directly from our teammates by checking out our 'A Day in the Life' blog series: https://www.udig.com/digging-in/breaking-ground-2023/\n\nSound Interesting? Here's the Technical Piece\n\nBelow are a few boxes you should be able to check if\u202fyou're interested: Don't check every box? Go ahead and apply!\n\n1+ year of experience with relational database management systems such as Microsoft SQL Server, MySQL, or Oracle\nUnderstanding of the full lifecycle of design, development, and implementation of data solutions including architecture design, development, testing\nSome experience in ETL development or reporting is a plus\nAbility to present technical ideas and high-level concepts and solutions to internal and external team members with varying degrees of technical knowledge\nBachelor's Degree in in computer science, information technology, or any science or business discipline\nAbility to take your work seriously but also seriously enjoy a good time with your UDig colleagues\n\nWhat's in It For You? \n\nAt UDig, you will build your career alongside talented and experienced developers while gaining experience and having fun! We provide continuous challenges and professional development opportunities. Additionally, we offer: \n\nCompetitive salary, merit reviews, and career advancement paths\nFlexible, hybrid environment\nIndividual $1500 Training Budget\nRegular team building and social activities (virtual and/or in-person)\nTransparent culture with strong communication and access to leadership\nGreat benefits like\nGenerous Paid Time Off, company holidays, and parental leave\nMultiple Single and Family Health Insurance plans to choose from\nDental & Vision coverage\nShort-Term and Long-Term disability\nOptional accident and critical illness coverage\nMatching 401(k)\nand more!"}, {"title": "Expression of Interest: Data Engineer", "company_name": "Fingerprint for Success (F4S)", "location": "New York, NY (Remote)", "link": "/jobs/view/3787769880/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=l%2FEDpQqUNmCttGdIyTk7Cg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787769880", "description": "About the job\nWe are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.\n\nThe F4S Talent Pool is a pilot project designed to:\n\nHelp job seekers get discovered by our partners based on their anticipated hiring needs.\nProvide optional support and resources for job seekers in their career endeavors.\nHelp individuals understand, and bring out the best in themselves and each other.\n\nThe F4S Talent Pool process:\n\nOnce you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.\n\nAbout Fingerprint For Success (F4S)\n\nBacked by 20+ years of research, F4S\u2019s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high performing teams.\n\nKeep in mind, joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your F4S work style assessment to match the hiring needs of our partners.\n\nYour feedback is a gift! Write to us via:\n\nPowered by JazzHR\n\noWuPJ9lwq9"}, {"title": "Data Engineer Seattle, WA (remote)", "company_name": "Conch Technologies, Inc", "location": "United States (Remote)", "link": "/jobs/view/3765052211/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=SBETI78UMutCHuRp5FnP%2FA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3765052211", "description": "About the job\nHi,\n\nGreetings from Conch Technologies Inc\n\nData Engineering Lead \u2013 With Databricks and Unity Catalog\n\nLocation \u2013 Seattle, WA (remote)\n\nDuration \u2013 12 months\n\nNeed: Design and implement data architecture and strategy, leveraging technologies such as Databricks, Unity Catalog, Privacera, and Collibra.\n\nAs a Data Engineering Lead, you will play a pivotal role in defining and executing technical deliverables while assessing complexities and levels of effort. You'll engage with both vendor and internal teams to resolve technical challenges and ensure seamless collaboration. Your leadership will extend to planning and prioritization with the SCRUM master, facilitating effective coordination between onsite and offshore teams, and driving feature prioritization through collaboration with product managers.\n\nKey Responsibilities\n\n Lead the definition and breakdown of technical deliverables, assessing complexities and levels of effort.\n Collaborate with vendor and internal teams to address technical issues.\n Drive planning and prioritization in conjunction with the SCRUM master.\n Coordinate with onsite and offshore teams for timely project delivery.\n Collaborate with product managers to prioritize features, communicate with stakeholders, and clarify requirements.\n Collect and manage user-reported issues, ensuring prompt resolution and communication.\n Define comprehensive test plans, oversee their execution, and document results.\n\n--\n\nWith Regards,\n\nNagesh G\n\nMobile: 408-381-5645\n\nDesk: 901-313-3066\n\nEmail: nagesh@conchtech.com \n\nWeb: www.conchtech.com"}, {"title": "Remote Work - Need Azure Fabric Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3759325599/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=C3iVjmcR9wjUip1nNjUiCg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3759325599", "description": "About the job\nThe client is placing heavy emphasis on Microsoft and Azure Fabric experience\n\nInformation Management Engineer \u2013 Take the identified information management sources and assist migration to the Client Data Lake, Azure Fabric. The engineer will transform information management (de-dup, clean and enrich).\n\nTake the identified data sources and bring them into our Data Lake, Azure Fabric. Once the data is in, the engineer will need to be able to transform data (de-dup, clean and enrich the data). We are focused on using a medallion data model for data architecture. The tools we are working with are: Delta Parquet files, python, spark, notebooks, T-SQL, Fabric lakehouse/warehouse, Dataflows Gen2, Data Pipelines, Direct Lake, shortcuts, ADF."}, {"title": "Data Engineer", "company_name": "Catalist", "location": "Washington, DC (Remote)", "link": "/jobs/view/3744951279/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ui1Fk6ICmnPNqAlZDZAcwA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3744951279", "description": "About the job\nFor over 17 years, Catalist has been a leader in civic data and data science innovation. Our mission is to provide progressive organizations with the data, software, and services needed to better identify, understand, and communicate with the people they need to engage and mobilize. Our clients include the largest, most influential organizations in the U.S. active in civic engagement, advocacy, and political campaigns.\n\nCatalist is home to a dedicated, creative team of technologists, data scientists, and campaign experts committed to using our talents and technology to nurture a vibrant and growing progressive community.\n\nAs a Data Engineer at Catalist, you will work closely with the Analytics and Technology departments to design, build, support, and maintain various data pipelines and processes with an end goal of providing data and intelligence to the progressive community. This role primarily involves translating data architecture designs into functional processes, code, and systems.\n\nThe ideal candidate will be a highly motivated individual with excellent technical skills, a strong desire to learn new skills, and an interest in progressive politics. Catalist values creativity and problem-solving. Our work is on the cutting edge of data-driven politics, and your support will help Democratic candidates and progressive organizations conduct successful advocacy and electoral campaigns.\n\nThis position reports to the Deputy Chief Data Officer. The Data Engineer is a part of a growing Data team that supports all underlying work at Catalist.\n\nThis position is included in our CWA bargaining unit.\n\nPrinciple Duties & Responsibilities\n\nDevelop scalable, production processes, code, and systems for data ingest, transformation, modeling, and reporting across a variety of platforms\nTranslate mock-ups and designs into functional processes, code, and systems\nCreate architecture designs when needed\nProvide quality assurance and testing on processes, code, systems, and products\nBecome an internal subject matter expert on various datasets and support other Catalist departments/teams on usage of those datasets\nExecute ad-hoc data and database maintenance tasks as requiredProject manage cross-departmental efforts, with direct responsibility for stakeholder engagement, management, and execution of technical elements\n\n\nRequirements\n\nBS or BA in a technical field, or relevant experience\n1-2 years of experience working with SQL databases\nExperience with data cleaning or ETL processes\nExperience with distributed computing systems and/or distributed datastores (particularly the Hadoop ecosystem)\nExperience managing projects\nFamiliarity with Catalist data, progressive politics, voter files, and/or commercial data\nBackground check required\n\n\nPreferred Skills & Abilities\n\nWillingness to be a problem solver and produce results in a fast paced environment\nAbility to be creative and personable, and articulate ideas clearly\nExperience working with SQL databases\nProficiency with Python or another object-oriented programming language (R, Java, Scala, etc\u2026)\nExperience working in cloud environments (AWS, GCP, etc.)\nExperience working in command line environments such as Bash\nExperience with a version control tool such as git or github\n\n\nBenefits\n\nMedical, Dental, Vision, Prescription Drug\n\nCatalist offers Medical, Dental, Vision, and Prescription Drug coverage for eligible staff and their eligible dependents. Catalist\u2019s Medical plan is a comprehensive PPO program including Prescription Drug coverage with 85% of the premium paid by Catalist. Dental and Vision coverage is provided at no cost to employees.\n\nGroup Term Life Insurance and Long-Term & Short-Term Disability Coverage\n\nGroup Term Life Insurance and Long-Term and Short-Term Disability coverage is available for eligible staff. These benefits are provided at no cost to Catalist employees.\n\n401(k) Safe Harbor Plan\n\nA 401(k) Safe Harbor Plan is available to eligible staff with a 3% contribution from Catalist from the date of hire. Employees may contribute pre-tax or post-tax from their salary up to the legal limits set forth by the IRS.\n\nMedical and Dependent Care Flexible Spending Accounts (FSAs)\n\nCatalist offers an FSA Program that gives eligible staff the ability to pay out-of-pocket medical/dental/vision/child care expenses from pre-tax earnings.\n\nTransit Benefits\n\nCatalist also makes available a Transit benefit FSA program to eligible employees using pre-tax contributions with a company match.\n\nProfessional Development and Remote Work Expenses\n\nEligible employees may be reimbursed up to $750 each year for professional development / education and remote work expenses.\n\nStudent Loan PayDown or SaveUp\n\nCatalist offers a Student Loan PayDown and College SaveUp benefit for eligible staff.\n\nVacation, Personal Leave, Sick Leave Benefits\n\nCatalist offers generous vacation benefits to all eligible staff. Eligible employees also receive:\n\n 14 Paid Holidays\n Personal Days\n Sick Leave\n Parental Leave\n\n\nHybrid Office/Remote Work\n\nCertain positions at Catalist are eligible for Office/Remote Hybrid or full Remote status."}, {"title": "Remote Entry Level Data Analyst/Engineer", "company_name": "SynergisticIT", "location": "White Plains, NY (Remote)", "link": "/jobs/view/3774171423/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=q54sk1%2FRww0z%2BC%2BE8e0jQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774171423", "description": "About the job\nAt SynergisticIT, we're all about making connections. Whatever IT goals you have, our software programmers can help achieve those. Our Software development teams can take up turnkey projects and execute them in an effective and efficient manner. If you are looking to source talent our recruiters will find the ideal IT talent for your company. What's the secret to our success? Well, it all starts with taking quality time to listen to each client's specific needs. After we have a thorough grasp of your IT goals, we can better customize our Developments as per your specific needs. We can also tailor make recruiting programs to exceed your expectations. Since our founding in 2010, SynergisticIT's strategies have earned the company an enviable position in the software development, IT staffing and IT skill enhancement fields. SynergisticIT continues to work with hundreds of satisfied American clients with our software programmers working on our projects and after gaining hands on experience on cutting edge technologies moving to contribute their skills to great clients like Apple, Google, Client, Ebay, Paypal, Kroger, the Walt Disney Company and hundreds more. If you are tired of working with inefficient programmers who take a lot of time to ramp up we want you to try us. Our software programmers can hit the ground running and get you the maximum return on your investment. You have already tried the rest its time you tried the best. SynergisticIT - Home of the Best Data Scientists and Software Programmers in the Bay Area.\n\n Why Us ? \n\nSynergisticIT has a proven track record of successfully skill enhancement and staffing IT employees for some of the world's most iconic brands. Our team takes the time to fully understand every client's needs so we could best meet your IT staffing requirements. The knowledgeable staff at SynergisticIT is always more than happy to work with clients to ensure they reach their software development goals. Besides staffing, SynergisticIT is also committed to helping young IT professionals advance their career with a robust upskill program . Everyone who goes through SynergisticIT's program learns all the skills necessary to succeed in many IT fields ranging from Java to Machine Learning. Additionally, everyone trained at SynergisticIT has been through extensive mock and technical interview screenings to bolster their career prospects. Last, but certainly not least, SynergisticIT takes great care to respect the privacy considerations for every client. All companies who work with SynergisticIT can rest assured their confidential data is protected using the most up-to-date encryption technologies. SynergisticIT also complies with all the latest NDA agreements.\n\n REQUIRED SKILLS For Java /Software Programmers \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Time series analysis \n\n We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2022 and at Gartner Data Analytics Summit (Florida)-2023 \n\nOracle CloudWorld Event (OCW) Las Vegas 2022 | SynergisticIT - YouTube\n\n https://www.youtube.com/watch?v=OAFOhcGy9Z8 \n\n https://www.youtube.com/watch?v=EmO7NrWHkLM \n\n https://www.youtube.com/watch?v=NVBU9RYZ6UI \n\n https://www.youtube.com/watch?v=Yy74yvjatVg \n\nSynergisticIT at Gartner Data and Analytics Summit 2023 - YouTube\n\nFor preparing for interviews please visit  https://www.synergisticit.com/interview-questions/ \n\n We are looking for the right matching candidates for our clients \n\n Please apply via the job posting \n\n REQUIRED SKILLS For Java /Full Stack/Software Programmer \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning Positions \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Tensorflow \n\n If you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team. \n\n No phone calls please.  Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates"}, {"title": "Data Engineer", "company_name": "Anika Systems", "location": "Leesburg, VA (Remote)", "link": "/jobs/view/3787766893/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=mApL9JnN%2B%2Fr2%2BYUPlxRYOA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787766893", "description": "About the job\nAnika Systems is a fast growing, woman-owned small business that specializes in providing innovative IT solutions for federal government agencies. Our expertise lies in accelerating delivery in Data and Analytics, Intelligent Automation, Application Development, and IT Modernization. We are currently expanding our Federal team and are seeking a passionate and talented Data Engineer. This opportunity is 100% remote.\n\nMust be a U.S. Citizen with the ability to obtain and maintain a government suitability clearance\n\nResponsibilities\n\nDesigns, develops, builds, analyzes, evaluates, and installs database management systems to include database modeling and design, relational database architecture, metadata, and repository creation and configuration management.\nDefines and oversees database organizations, standards, controls, procedures, and documentation. Provides technical consulting in the definition, design, and creation of a database environment.\nAdvises applications development staff and users on data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nDesigns and implements databases with respect to access methods, access time, batch processes, device allocation, validation checks, organization, protection and security, documentation, and statistical methods.\nUses data mapping, data mining, and data transformational analysis tools to design and develop databases.\nDetermines data storage and optimum storage requirements.\nPrepares system requirements, source analysis, and process analyses and designs throughout the database implementation.\n\nRequired Skills And Experience\n\nBA/BS and 1 year of relevant experience\nExperience with DataBricks, SQL, Python\nApplications development experience with data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nExcellent communication skills (both oral and written) with process-oriented organizational skills to ensure project success\n\nDesired Skills And Experience\n\nDemonstrated successful development experience with the full life cycle of government database management systems including database modeling and design, relational database architecture, metadata and repository creation, and configuration management.\n\nPowered by JazzHR\n\nTlKWIorucJ"}, {"title": "Remote Work - Need Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3774712978/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=r3Bp%2F0qcOD2OpC5jpbUOcA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774712978", "description": "About the job\nTitle : Data Engineer\n\nLocation : Remote\n\nDuration : 6+ months to start (long term contract (work order are extended 2x year))\n\nPlease Fill out the skill matrix: Technology Some Awareness (1) Novice (2) Intermediate (3) Advanced (4) Expert (5) SQL Tableau Desktop /Creator Tableau API Integration AWS - S3 Glue AWS - Athena Redshift AWS - Lambda AWS - SNS/SQS msExcel\n\nProfile: Data Analyst will work closely with Architects and leads in the Interflow team, key business stakeholders including program staff to support the organizations effort in the areas of data Analysis and information management. Role should have strong experience in writing SQL queries to extract data for reports. Candidate should have experience working with large set of data. As these reports are very critical for the company and we are looking for an expert who is capable of handling, consuming and extracting large sets of data for analysis and reports.\n\nMust Haves\n\nAWS - Athena\nAWS - S3\nGlue\nhands-on development of reporting applications\nMS Excel\nRedShift\nSQL\nTableau Desktop /Creator\n\nNice\n\nAWS - Lambda\nAWS - SNS/SQS\nTableau API Integration\n\nSkills\n\nBA/BS required (major in an analytical field desired) \nA minimum 10+ years work-related experience. \nExperience in hands-on development of reporting applications for the Web in a professional environment \nThe Ethos of continuous improvement and interest in learning new things.\nStrong analytical thinking and structured problem-solving ability\nAbility to handle multiple projects and assignments simultaneously and effectively in a cross-functional team environment. \nVersed on the agile methodology and best practices. \nExcellent interpersonal and collaboration skill with the ability to work with a diverse set of colleagues, across functions, from different organizations, disciplines, etc. \nSelf-starter, ability to set priorities, work independently and attain goals"}, {"title": "Data Engineer", "company_name": "Team Remotely Inc", "location": "Houston, TX (Remote)", "link": "/jobs/view/3786249829/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=78lY5Jo%2FfyRPC6TQYCdvww%3D%3D&trk=flagship3_search_srp_jobs", "id": "3786249829", "description": "About the job\nThis is a remote position.\n\n Data Engineer (US/Canada Residents Only, 1 year experience, remote)\n\nTeam Remotely Inc. is a staffing and recruitment agency that offers a comprehensive solution for talent acquisition, including sourcing, vetting, pay rolling, and managing talent. Whether you need contract staffing, direct hire, direct sourcing, talent pools, or diversity initiatives, our model can support your hiring strategy.\n\n Hiring Type: Full-Time\n\n Base Salary: $55K-$70K Per Annum.\n\nResponsibilities\n\nDesigning user interface changes for web-based DB applications. \nReviewing application requirements and interface designs. \nDeveloping and implementing highly responsive user interface components using react concepts. \nWriting application interface codes using JavaScript following react.js workflows. \nTroubleshooting interface software and debugging application codes. \nDeveloping and implementing front-end architecture to support user interface concepts. \nMonitoring and improving front-end performance. \n\nQualifications And Experience\n\nBachelor\u2019s degree in computer science, information technology, or a similar field. \nPrevious experience working as a react.js developer. \nIn-depth knowledge of JavaScript, CSS, HTML, jQuery, and front-end languages. \nKnowledge of REACT tools including React.js, Webpack, Enzyme, Redux, and Flux. \nExperience with user interface design. \nKnowledge of performance testing frameworks, including Mocha and Jest. \nExperience with browser-based debugging and performance testing software. \nTroubleshooting skills. \nProject management skills. \nProblem-solving skills. \nVerbal communication skills. \n\n Why work with Team Remotely?\n\nTeam Remotely Inc. is a staffing platform offering a seamless experience for employers and candidates. Employers can post job openings and specify their requirements, while candidates can create profiles and upload resumes.\n\nThe team of Team Remotely continuously learns and adapts based on previous successful placements, constantly improving its matching capabilities. This ensures that the recommendations provided by Team Remotely are tailored and accurate, increasing the likelihood of a successful match between employers and candidates. By providing intelligent and data-driven solutions, they strive to enhance the efficiency and effectiveness of the hiring process, ultimately helping companies find the best talent and individuals find their dream jobs."}, {"title": "Data Engineer", "company_name": "ZAG Technical Services", "location": "United States (Remote)", "link": "/jobs/view/3771403464/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=Hf0VefrKXBFIL5MtHmY0Pw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3771403464", "description": "About the job\nWe are looking for a tech savvy Data Engineer to contribute to the building, operating, and scaling of next generation data platforms and tools that will power data-driven analytics capabilities throughout the ZAG products and services portfolio, spanning areas such as business intelligence, reporting, and data analytics.\n\nThe ideal team member is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. Must be comfortable supporting the data needs of multiple clients and teams, systems, and products within the modernization of ecosystems. The right-fit person will be excited by the prospect of optimizing or even re-designing data processes to support next generation of advanced analytics capabilities, which in-turn translates to implementing strategic enhancements to maintain competitive advantage in the industry.\n\nThe successful team member will leverage their proficiency to\u2026\n\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\nBuild the analytics processes required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Data Management and cloud based \u2018big data\u2019 technologies.\nBuild analytics tools that utilize technologies to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.\nCollaborate with client teams to identify and solution regional and local requirements within the context of the standard global business model for enterprise analytics.\nParticipates in meetings, discussions, strategy sessions where changes, improvements, and enhancements are proposed, evaluated, and approved. \n\n\nThe exceptional team member will have or demonstrate progressive experiences in\u2026 \n\nBachelor\u2019s degree or higher in Computer Science or related field\nExperience working with BI and data warehousing tools\nExperience working with enterprise data, where security is paramount and data governance is critical\nAbility to work independently and in a team environment, and effectively engage all levels of the organization\nUnderstanding of distributed systems driving large-scale data processing and analytics with a successful history of manipulating, processing, and extracting value from large, disconnected datasets.\nAbility to communicate effectively (listening, presenting, and questioning)\nStrong organizational, written, and communication skills\n\n\nAbout ZAG\n\nZAG Technical Services empowers our clients\u2019 success through intelligent business solutions that increase employee productivity, network security and team collaboration. We aim to ensure that our client\u2019s IT investments drive ROI, provide a competitive advantage and help businesses grow. We do this by living our core set of values: Integrity Foremost, Client-Centered, Accountable Always, Teamwork Throughout, and Exceptional is the Goal.\n\nWhat ZAG Has To Offer\n\nCompetitive compensation package \nBenefits including medical, dental, vision, 401K and life insurance \n\n\nZAG Technical Services is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all of our employees.\n\nZAG Technical Services is an E-Verify participant and will provide the Social Security Administration (SSA) and, if necessary, the Department of Homeland Security (DHS), with information from each new employee\u2019s Form I-9 to confirm work authorization."}, {"title": "Junior Data Engineer", "company_name": "HireMeFast LLC - Career Accelerator - Land A Job", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3786249797/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D&trk=flagship3_search_srp_jobs&lipi=urn%3Ali%3Apage%3Ad_flagship3_search_srp_jobs%3BQExogrHBQoSF9zZm%2B9kufw%3D%3D&lici=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D", "id": "3786249797", "description": "About the job\nThis is a remote position.\n\n Job Title: Junior Data Engineer\n\n Employment Type: Full-Time\n\n Salary: $64,000 - $76,000 per annum\n\n Experience Required:  Minimum 1 year of project experience\n\nAbout us: HireMeFast is a leading staffing and recruitment agency specializing in connecting businesses with top-tier talent across various industries. Our mission is to bridge the gap between exceptional candidates and organizations needing their skills, expertise, and unique qualities. Our team of experienced and dedicated recruitment specialists utilizes innovative sourcing strategies, and a vast network to identify and attract top talent. We conduct comprehensive procedures to ensure that only the most qualified candidates are presented to our clients.\n\nPosition Summary\n\nJoin the fast-paced, innovative, and collaborative environment focused on providing an AIOps platform that enhances the intelligence of the CVS Health infrastructure. Work closely with subject matter experts and colleagues to build and scale out machine learning and AI solutions that will detect, predict, and recommend solutions to correct issues before system impact and enhance efficiency, reliability, and performance of CVS Health\u2019s IT operations.\n\nKey Responsibilities include:\n\nData pipeline development: Designed, implemented, and managed data pipelines for extracting, transforming, and loading data from various sources into data lakes for processing, analytics, and correlation. \nData modeling: Create and maintain data models ensuring data quality, scalability, and efficiency \nDevelop and automate processes to clean, transform, and prepare data for analytics, ensuring data accuracy and consistency \nData Integration: Integrate data from disparate sources, both structured and unstructured to provide a unified view of key infrastructure platform and application data \nUtilize big data technologies such as Kafka to process and analyze large volumes of data efficiently \nImplement data security measures to protect sensitive information and ensure compliance with data and privacy regulation \nCreate/maintain documentation for data processes, data flows, and system configurations \nPerformance Optimization- Monitor and optimize data pipelines and systems for performance, scalability and cost-effectiveness \n\nCharacteristics of this role:\n\nTeam Player: Willing to teach, share knowledge, and work with others to make the team successful. \nCommunication: Exceptional verbal, written, organizational, presentation, and communication skills. \nCreativity: Ability to take written and verbal requirements and come up with other innovative ideas. \nAttention to detail: Systematically and accurately research future solutions and current problems. \nStrong work ethic: The innate drive to do work extremely well. \nPassion: A drive to deliver better products and services than expected to customers. \n\nRequired Qualifications\n\n2+ years of programming experience in languages such as Python, Java, SQL \n2+ years of experience with ETL tools and database management (relational, non-relational) \n2+ years of experience in data modeling techniques and tools to design efficient scalable data structures \nSkills in data quality assessment, data cleansing, and data validation \n\nQualifications\n\nKnowledge of big data technologies and cloud platforms \nExperience with technologies like PySpark, Databricks, and Azure Synapse. \n\nEducation\n\nBachelor\u2019s degree in Computer Science, Information Technology, or related field, or equivalent working experience\n\nWhy HireMeFast LLC?\n\nAt HireMeFast, we understand that finding the right individuals to join your team is crucial for success and growth of your organization. We are committed to streamlining the hiring process for our clients, ensuring they have access to a diverse pool of highly qualified candidates who are a perfect fit for their specific needs."}, {"title": "Data Engineer  - Need candidates from Texas ( REMOTE )", "company_name": "Pulivarthi Group (PG)", "location": "United States (Remote)", "link": "/jobs/view/3770685212/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=9gdMwuif%2BpB%2BCQYvQi3imQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3770685212", "description": "About the job\nFollow us on Linkedin: https://www.linkedin.com/company/pulivarthigroup/\n\nPulivarthi Group LLC is a Global Staffing & IT Technology Solutions company, with our prime focus of providing world class solutions to our customers with the right talent. We combine the expertise of our team and the culture of your company to help you with the solution that is affordable and innovative using high quality standards and technologies.\n\nWe\u2019ve served some of the largest healthcare, financial services, and government entities in the U.S.\n\nData Engineer - FT only - Azure SQL with data lakes experience\n\nData Engineer -\n\nthe resource requirement for this Data Engineer role needed more clarification.\n\nOur customer is requesting a resource who knows Azure CI/CD (Continuous Integration / Continuous Deployment) to build and integrate their DevOps pipeline.\n\nOnce the DevOps pipelines are mapped, they need the Data Engineer to connect to all the Data Sources, primarily their Data Lake.\n\nThe Customer (based in Houston) is interested in a resource in a time zone not too far removed from the Central Time Zone to allow real time collaboration."}, {"title": "Data Engineer", "company_name": "Otter Products", "location": "Fort Collins, CO (Remote)", "link": "/jobs/view/3766728302/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=77U3U7CAi1PxgDmpI3%2BiNQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3766728302", "description": "About the job\nOverview \n\nOtter Products is currently recruiting for a Data Engineer to join our Data team! You can be based in our Fort Collins, CO office with flexibility to work a portion of your time remotely, or 100% Remote in the U.S.\n\nThe Data Engineer applies professional experience, concepts, and company objectives toward building and operationalizing the minimally inclusive data necessary for the enterprise data initiatives following industry standard practices and tools. The bulk of the Data Engineer's work would be in designing, managing and optimizing data pipelines, then moving these data pipelines effectively into production for key data and analytics consumers like business/data analysts, data scientists or other stakeholder that needs curated data for data and analytics use cases across the enterprise. The Data Engineer also ensures compliance with data governance and data security requirements while creating, improving and operationalizing these integrated and reusable data pipelines to enable faster data access, integrated data reuse and vastly improved time-to-solution for data and analytics initiatives. Additionally, the Data Engineer will be expected to collaborate with other data team members and data consumers working on the models and algorithms developed by them toward optimization for data quality, security, and governance to put them into production leading to potentially large productivity gains.\n\n About Otter Products \n\nOtter Products, we grow to give. From our founder's garage in 1998 to the global technology leader we are today, Otter Products continues to drive growth through innovation.\n\nThrough our industry-leading brands - OtterBox, Liviri and OtterCares - we provide our partners the number-one selling and most trusted products in our categories. Our philanthropic spirit is the foundation on which we foster our partner relationships, allowing us to grow and to give - together.\n\nBy way of our charitable arm, the OtterCares Foundation, we support our communities and invest in our future through education that inspires kids to change the world.\n\nAnd even as our global community of Otters continues to grow, our founder's core values are still at the heart of everything we do. We measure our success by our ability to give back to our communities and strengthen opportunities for all.\n\nFor more information visit otterproducts.com\n\n Responsibilities \n\nComplete analysis, design and development of BI solutions using Azure SQL Server\nFamiliarity with Data Warehouse concepts\nExperience coding the extraction, transformation, and loading (ETL) processes\nDatabase development primarily in SSIS, Data Factory and SQL\nPartner with the business to determine end user database/reporting needs and requirements\nGenerate ad hoc reports using Power BI or SSRS\nCollaborate with other developers to create and implement best approach solution(s)\nTroubleshoot Azure tools, systems, and software\nReview queries for performance issues, making changes as needed\nCollaborate with team to performance-tune Azure application as necessary\nAssist with the analysis and extraction of relevant information from large amounts of historical business data to feed data science initiatives\nParticipate and support the design and documentation of processes for large scale data analyses, model development, model validation and model implementation\nSupport and maintain a positive safety culture by following all safety policies and procedures and actively contributing to a safe working environment\nOther duties as required\n\n Qualifications \n\nBachelor's degree required. Degree in Computer Science or Mathematics preferred\nMinimum of four years of experience in an IT or analytical role required. Experience in database development, report writing and/or statistics preferred.\n\n EEO \n\nOtter Products, LLC is an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, age, marital status, pregnancy, sex, sexual orientation, gender, gender identity or expression, national origin, disability, veteran status, or any other characteristic or status protected by law.\n\n For US Based Roles Only - Compensation Range Minimum \n\nUSD $90,000.00/Yr.\n\n For US Based Roles Only - Compensation Range Maximum \n\nUSD $120,000.00/Yr.\n\n Additional Total Rewards \n\nProfit Sharing Program Eligible, Benefits Eligible - Full Time- check out otterproducts.com/careers/why for more info"}, {"title": "Software Engineer, Distributed Systems", "company_name": "Hightouch", "location": "United States (Remote)", "link": "/jobs/view/3628572539/?eBP=CwEAAAGMd4QO8APODyE_-mMOPhYyQGg5ZjVbPwq_3V7DK0B2bLAf10pcedM9S2T71X5i4TiJuEwOzWBbSiTKbLuk-VD-_Y-dN5uyf8YpYkHYqEYFxySMS9zr2hRjNrxX9gXbxssh35TFK9YTNV2WeH2Ncb675JgFbJTgNWSZySFC6rrdJGmdSDTr2q5d3VoxORz3GjsDbzjeyZQCvQ07RqYw7oujWSgslycs2LMCY_-N0NEj6e0PvbaYKiZ_5IW34bGVSYLXpHwqa8oOcNttYZ90UA3E_7sDSFzABdIVlfrp5zzERrIYy5sh2MIInxHdDoKgQw8VmOn5uM4S0tXUl-i7LnN0_W0NfPBmLiVDNuveIOm4MfDNPxfe5QKPk6FUAVpvgbcoJn9yRyE&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=rph%2FT6Q6Vkp6YoggJf9cJQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3628572539", "description": "About the job\nAbout Hightouch\n\nHightouch\u2019s mission is to empower everyone to take action on their data. Through our Reverse ETL platform, business and data users can seamlessly sync data from where it resides, such as warehouses and databases, to where it is needed, including operational systems and SaaS tools. Traditionally, acting on data has required engineering time and bandwidth, and left most business users stuck with charts and reports that are unable to take automated action on their data. With Hightouch, every business user, without writing any code, can activate data to streamline critical processes, improve marketing performance, and scale operations.\n\nOur team operates with a focus on making a meaningful impact for our customers. We believe in approaching challenges with a first principles thinking mindset, moving quickly and embracing our value of efficient execution, and treating each other with compassion and kindness. We look for team members that are strong communicators, have a growth mindset, and are motivated and persistent in achieving our goals.\n\nHundreds of companies use Hightouch, including Spotify, Ramp, Retool, NBA, Plaid, and Betterment. We\u2019re based in San Francisco, are remote-friendly, and backed by leading investors such as Amplify Partners, ICONIQ Growth, Bain Capital Ventures, Y-Combinator, and Afore Capital.\n\nAbout The Role\n\nWe are looking for a distributed systems engineer to work on the systems that power the syncing engine our customers and other engineering teams rely on. This presents an exciting challenge where you can apply your expertise in distributed systems, performance optimization, and troubleshooting to push the boundaries of what is possible and meaningful improve our ability to move massive amounts of customer data. This role also provides a unique opportunity to work on a multi-cloud and multi-region infrastructure that supports a global customer base.\n\nWe believe in enabling our engineers to do their best work for our customers by giving them extremely high levels of ownership and autonomy. This comes in different forms: you will own and deliver projects from start to finish, you will work directly with customers to solve their hardest scaling problems, and you will have a lot of influence over what we work on as a team and company.\n\nSome of the problems we\u2019ll be working on include:\n\nSync Speed: Customers want to sync a lot of data to important destinations like Facebook and Snapchat, which requires us to analyze every part of our syncing process and find where we can optimize to sync data more quickly\nStreaming Syncing: We currently sync data from sources in batch only, but envision a world where we also do syncs in real-time and support streaming sources like webhooks and queues\nScalability and Reliability: As part of our rapid growth, we\u2019re always evaluating current future bottlenecks in scaling and reliability, and architecting for the next order of magnitude of growth\nPersonalization API: Our Personalization API product provides a low latency caching layer on top of data warehouses that enable customers to use their data warehouse for real-time personalization use cases with \nMulti-Region and Multi-Cloud: Supporting our multi-region and multi-cloud backend, including extending it to launch Hightouch on in new regions to support data residency requirements of our global customer base\n\nWe are looking for talented, intellectually curious, and motivated individuals who are interested in tackling the problems above. This is a senior role, but we focus on impact and potential for growth more than years of experience. The salary range for this position is $170,000 - $240,000 USD per year, which is location independent in accordance with our remote-first policy.\n\nAbout You\n\nYou are an engineer with a passion for solving hard technical problems that generate real value for customers. You\u2019re motivated by high ownership and are comfortable in a fast-paced, startup environment.\n\nYou have experience and comfort with distributed systems and high-scale systems or the ability and desire to build this experience quickly. This experience can take different forms:\n\nYou\u2019ve built syncing engines that process and move terabytes of data per day in a scalable way\nYou\u2019ve created event collection or data streaming services that power company critical systems\nYou\u2019ve owned systems that have undergone orders of magnitude of growth and helped them scale accordingly\nYou have relevant skills and are a fast learner who is excited to take on the challenges we face\n\nInterview Process\n\nOur goal with the interview process is to balance speed with giving both parties opportunities to assess whether there is a strong mutual fit. We will ask you questions, but we want you to ask us questions! Our technical interviews focus on how you design systems because we believe this is the best way for us to see how you work and for you to see how we collaborate. We don\u2019t ask you to write code to solve technical brainteasers that don\u2019t appear in your day to day job.\n\nRecruiter screen [30m]: Introductory call with our recruiting team to get to know each other and see if the role could be a good mutual fit.\nSystems design screen [45m]: Designing a data processing feature end-to-end.\nHiring manager interview [30m]: Chat with hiring manager about past experiences and future operating preferences to assess fit on company values and operating principles.\nSystems design interview [90m]: Work with the interviewer to architect a system at a conceptual level. The problem will be at a pretty high level - and have both product and customer requirements as well as technical."}, {"title": "Entry-Level Data Scientist Engineer", "company_name": "Phoenix Recruitment LLC", "location": "Texas City, TX (Remote)", "link": "/jobs/view/3788727625/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=8iwtytHsCu4wMwJ4fj3pxw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3788727625", "description": "About the job\nThis is a remote position.\n\nTitle - Entry-Level Data Scientist Engineer, 1 year of project experience\n\nEmployment Type: Full-time\n\nBase Salary: $60K-$70K\n\nPhoenix Recruitment offers a variety of recruiting services to assist both employers and employees. They specialize in marketing open positions, recruiting, and helping employers to find qualified candidates across various industries. Phoenix Recruitment has expertise in streamlining the hiring process. They can help ensure that the process is efficient, well organized, and compliant with relevant regulations.\n\nSkills and Abilities:\n\nStrong knowledge of R or Python for data analysis and modeling. \nProficiency in statistical programs such as R, SAS, MATLAB, or Python. \nFamiliarity with spreadsheets (VBA) and database applications (Access, Oracle, SQL, or equivalent technology). \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \nAbility to learn new methods quickly and work under deadlines. \nExcellent teamwork and communication skills. \nStrong analytical and problem-solving abilities. \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \n\nPreferred:\n\nKnowledge of actuarial concepts and life, health, and/or annuity products. \nExperience with statistical modeling techniques such as GLM, Decision Trees, Time Series, Regression, etc. \nFamiliarity with Microsoft DeployR. \nExposure to insurance risk analysis. \nBasic experience in computational finance, econometrics, statistics, and math. \nKnowledge of SQL and VBA. \nFamiliarity with R or Python for predictive modeling \n\nWhy Phoenix Recruitment LLC?\n\nPhoenix Recruitment often has an extensive network of employers and candidates. This network allows them to tap into a pool of qualified candidates and connect them with suitable job opportunities. They can also leverage their connections to help employers find the right talent efficiently. Outsourcing the recruitment process to a specialized agency can save your time and resources, avoid delays, reduce administrative burdens, and increase the chances of finding the right fit for your organization."}, {"title": "Data Engineer - Remote Work", "company_name": "BairesDev", "location": "United States (Remote)", "link": "/jobs/view/3783162297/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=OtoCrAAHZFomVup04fy35g%3D%3D&trk=flagship3_search_srp_jobs", "id": "3783162297", "description": "About the job\nWho We are\n\nBairesDev is proud to be the fastest-growing company in America. With people in five continents and world-class clients, we are only as strong as the multicultural teams at the heart of our business. To consistently deliver the highest quality solutions to our clients, we only hire the Top 1% of the best talents and nurture their professional growth on exciting projects.\n\nWe are looking for a Data Engineer to join our Development team and participate in different projects made up of multicultural teams distributed throughout the world.\n\nWhat You Will Do\n\n Interview other Data Scientists and Data engineers to vet their technical abilities.\n Input data from those interviews into the product and help develop a predictive, fair, and enjoyable solution.\n Work together with developers, tech leads, and solution architects to build applications.\n Improve existing structures by adding new functionalities or proposing technological updates so that as a result, the impact of your contribution is significant in the core of each business.\n Improve the user experience on scalable and high availability platforms, contributing to the key differential of each business.\n\nHere\u2019s What We Are Looking For\n\n 5+ years of experience working as a developer.\n 3+ years of experience working as a Data Scientist / Data Engineer.\n Proficient with analysis, troubleshooting, and problem-solving.\n Hands-on experience with managing data loads and data quality.\n Advanced English level.\n\nHow we do make your work (and your life) easier:\n\n 100% remote work.\n Hardware setup for you to work from home.\n Flexible hours - make your schedule.\n Paid parental leave, vacation & holidays.\n Diverse and multicultural work environment.\n An innovative environment with the structure and resources of a leading multinational.\n Excellent compensation \u2014 well above the market average.\n Here you can grow at the speed of your learning curve.\n\nOur people work remotely but with a consistent and robust culture that promotes diversity and teamwork. To continue being the leading software development company in Latin America, we want to ensure that every BairesDev member gets the best growth and professional development opportunities in a diverse, welcoming, and innovative environment.\n\nEvery BairesDev team member brings something unique to our company.\n\nWe want to hear your story. Apply now!"}, {"title": "Data Engineer (Remote)", "company_name": "Cascade Debt", "location": "United States (Remote)", "link": "/jobs/view/3755336571/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=pmUtn1j2zyPr25CNPh2RNw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3755336571", "description": "About the job\nDo you want to work in a small team where you can make a real difference in a company? Would you embrace the challenge of building a high impact platform that is used globally? Then we want to speak with you!\n\nAbout Cascade\n\nCascade is a fintech startup backed by Canadian and US investors that empowers companies to grow by democratizing access to institutional debt. We are building tools that modernize how companies raise and manage debt, lowering the barriers to entry to the $7 trillion specialty finance and alternative credit market for companies around the world. Founded just last year, we are gearing up for our next phase of technical development and are seeking a talented Data Engineer to lead the way.\n\nAbout The Role\n\nAs a Data Engineer you\u2019ll be an early member of a growing team building a pioneering platform in the debt infrastructure space. You will design, build, and launch efficient, scalable, and reliable data pipelines to move and transform data .\n\nWhat You\u2019ll Do\n\n Building databases, data lakes and data ingestion pipelines to integrate customer databases and datasets to our systems \n Ingesting financial datasets from external customers, then u pdating and maintaining accurate and complete data mappings to ensure that our product s are displaying high quality data \n M onitoring and alerting across our data pipelines in order to make sure that our data ingests are reliable and correct \n Translat ing business goals and stakeholder requirements into new data flows and deliver analytical insights ensuring that the data flows create real business value continuously \n\nSkills And Experience\n\n 2 + years experience in a data engineering or data analyst role \n E xperience with relational SQL and NoSQL databases, including PostgreSQL , MySQL, MariaDB, MongoDB, Bigtable, etc. \n E xperience working with ETL technologies , such as Databricks, Fivetran , or dbt \n Proficiency in writing SQL queries and knowledge of analytical data warehouses such as RedShift, BigQuery , and Snowflake \n Some experience with CI/CD automation such as GitHub Actions \n Developing APIs and integrating with 3rd party APIs \n Bonus: experience in fintech / SaaS / credit analysis \n\nPeople Who Thrive At Cascade Are\n\n Self-starters, who take the initiative to tackle challenges in a remote work environment \n Good communicators, who can operate without ego to discuss, learn, grow, and help others do the same \n Passionate about creating great digital experiences for users \n Problem solvers who are not satisfied with the status quo \n\nBenefits\n\nRemote: we are a remote-first company and are very flexible on hours as long as things get done \nHome-office: there\u2019s a $1000 USD home office allowance to set yourself up \nEquity: we expect you to have an owner-mentality, and have the equity plan to match \nBenefits: health, dental, vision, and more \nPerks: we offer free lunches weekly and off-site trips \nJob satisfaction: we offer autonomy, ample opportunities for mastery, and an opportunity to make a difference for companies around the world"}, {"title": "Data Engineer", "company_name": "ISC (Integrated Specialty Coverages, LLC)", "location": "Carlsbad, CA (Remote)", "link": "/jobs/view/3778293528/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ew7%2FfEuXSujs5IGabdqUyw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3778293528", "description": "About the job\nAbout Integrated Specialty Coverages\n\nIntegrated Specialty Coverages LLC (ISC) is a growth stage technology and data-driven insurance company leading innovation in the market.\n\nBacked by one of the leading private equity firms, KKR, and led by a forward-thinking management team, ISC is combining the worlds of insurance and technology to create an Insurtech powerhouse. As a leading online distributor of insurance products for a range of industries and \"Main Street USA\", we are looking for the right people to help us in our mission of achieving exponential growth. We strive to be the number one place to go for brokers and agents to source insurance. To accomplish this, we're building a digitally focused team that deeply understands the intersection between user experience, data, and AI/ML to optimize the way we engage with our customers and partners.\n\nJob Summary\n\nThe Data Engineer role will be the technical liaison between multiple groups including a data science team, the engineering team, product management, and business stakeholders. You do not need any insurance knowledge prior, however, you must quickly dive deep into the insurance world and ask questions to become a subject matter expert. You will be responsible for building a data platform to facilitate the data science team. You must be a self-starter that can build out features such as a data pipeline from scratch. There will be support from both engineering and data science for any buildout.\n\nJob Responsibilities\n\nCreate and maintain optimal data pipeline architecture. \nAssemble large, complex data sets that meet functional / non-functional business requirements. \nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. \nBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using AWS technologies, SQL, and Python. \nWork with stakeholders including the Executive, Product, Data Science, and Engineering teams to assist with data-related technical issues and support their data infrastructure needs. \nWork with data science and analytics teams to strive for greater functionality in our data systems. \n\nMinimum Qualifications:\n\nAdvanced working SQL knowledge and experience working with relational databases, strong query authoring (SQL) as well as working familiarity with a variety of databases (Snowflake, Redshift, MySQL, MSSQL, etc.)\nExperience building and optimizing data pipelines, architecture and data sets. \nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. \nBuild processes supporting data transformation, data structures, metadata, dependency, and workload management. \nA successful history of transforming, processing and extracting value from large disconnected, datasets from a variety of data sources (Flat files, Excel, databases, etc.)\nStrong analytic skills related to working with unstructured datasets. \nStrong project management and organizational skills. \nExperience supporting and working with cross-functional teams in a dynamic environment. \nAbility to mentor/guide/collaborate with other team members. \n\nWe are looking for a candidate with 3-5 years of experience in a Data Engineer role. They should also have experience using the following software/tools:\n\nExperience with relational and MPP databases, including Snowflake and MySQL. \nExperience developing software in an agile environment from the requirements stage to production. \nExperience with version control: git\nExperience with container technologies: Docker\nExperience with data pipeline and workflow management tools: Airflow, Jenkins, AWS Glue, Azkaban, Luigi, etc. \nExperience with AWS cloud services: EC2, ECS, Batch, S3, EMR, RDS, Redshift\nExperience with other cloud services: Snowflake, Airflow\nExperience with object-oriented/object function scripting languages: Python, Java, C++, etc. \nExperience with data modeling and data warehouse design\nExperience with data visualization tools (PowerBI, QuickSight)\n\nThe starting annual pay scale for this position is listed below. Actual starting pay will be based on factors such as skills, qualifications, training, and experience. In addition, the company offers comprehensive benefits including medical, dental and vision insurance, 401(k) plan with match, paid time off, and other benefits.\n\nISC's salary ranges are determined by role and level. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations and could be higher or lower based on a multitude of factors, including job-related skills, experience, and relevant education or training.\n\nNational Pay Range\n\n$100,000\u2014$110,000 USD\n\nBenefits of Working at ISC\n\nCompetitive vacation and flexible working arrangements\nComprehensive and inclusive health benefits\nA variety of professional development and mentorship opportunities\nChoice of technology whether at home or in the office\n\nISC's Ownership Behaviors\n\nDo the Right Thing *Be Relentless. Pursue Excellence. *Personal Responsibility & Own Outcomes\nTry Fast. Learn Fast. Fail Fast. Think Big. *Build Extraordinary Owners & Join Forces *Grit & Determination\n\nApplicants with disabilities may contact ISC HR department via e-mail, and other means to request and arrange accommodation. If you need assistance to accommodate a disability, you may request accommodation at any time. Please contact ISC at HR@ISCMGA.com.\n\nISC believes in creating long-term relationships by being responsive and relevant and by consistently delivering value to our community of customers. Specifically, with our employees, we focus on attracting, developing, and retaining the best talent for our business, challenging our people, demonstrating a \"can-do\" attitude, and fostering a collaborative and mutually supportive environment. ISC is an equal opportunity employer and a participant in the US federal E-Verify program (US).\n\nDiversity creates a healthier atmosphere: All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, gender, gender identity, sexual orientation, marital status, medical condition, genetic information, mental or physical disability, military or veteran status, or any other characteristic protected by local, state, or Federal law."}, {"title": "Data Engineer -Remote", "company_name": "RIT Solutions, Inc.", "location": "Denver, CO (Remote)", "link": "/jobs/view/3768007496/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=1hQH8EDaII1eJ7nQ9BACQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3768007496", "description": "About the job\nDuration: 6+ months CTH\n\nMust have a valid LinkedIn profile\n\nRequired Skills\n\n 3-6 years of experience (no more than 6 years)\n Start up environment experience\n Python, SQL, and DBT"}, {"title": "Big Data Engineer - W2", "company_name": "Megan Soft Inc", "location": "United States (Remote)", "link": "/jobs/view/3674845652/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fCAxh4N%2F4YZJ5aEIASaeeA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3674845652", "description": "About the job\nTitle: Big Data Engineer - W2\n\nLocation: Dearborn, MI (Remote)\n\nDuration: 12+ Months\n\nDirect Client: Ford Motors\n\nMust have skills: Should be able to handle GCP requirements\n\n\"Translates business requirements to build repeatable, sustainable, efficient, coded processes that can be productionized by Software Engineers and readily modified by other Data Engineer Creates POC processes in Dev/QA and works with Software Engineers to productionize and align best coding and process practices that maximize efficiency, speed, stability, system resources and capabilities Awareness of and compliance with data privacy, security, legal and contractual guidelines Incorporates data quality and privacy checks/alerts to minimize bad data being consumed by end users, models and dashboards, and to protect customer data; revises checks for new data issues Validates data products and pipelines are functioning as expected following system or application upgrades, source changes, etc. Works closely with Architects if required to align systems, tools and applications being utilized with business use case and performance requirements Provides data product support and maintenance Establishes, tracks and monitors KPIs related to specific data products and deliverable Participate in iteration cadence activities (Iteration Planning, task estimation, daily scrum, demos, etc.). Will be a part of on-call team to resolve critical production support tickets during business hours and off hours.\n\nSkills Required\n\nExperience with Big Data Technologies (Hadoop , ETL Tool, Spark. Hive etc..) Someone who understands Cloud and has experience working with Cloud Platforms (preferably GCP) Should be able to handle GCP requirements - ingesting into GCS/ Big Query, working with GCP components Hands-on in Data Flow, Cloud Storage (GCS), Big Query and GitHub. Proficient in Python and SQL.\n\nSkills Preferred\n\nManages ETL: uses programming and tools for data ingestion, configures pipelines, applies transformations and decoding, integrates and fuses data, moves and securely deliver Experience NoSQL databases\n\nAdditional Information\n\nIf the candidate is remote only, please indicate \"\"100% Remote\"\" under candidate's name on resume. Other similar schedule notations can be \"\"Local Candidate\"\" or \"\"Hybrid Candidate.\"\" ***HRA Test Required***\""}, {"title": "Remote Opportunity: Quantexa Data Engineer", "company_name": "SPAR Information Systems LLC", "location": "United States (Remote)", "link": "/jobs/view/3680073191/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ioykVCZSOQ7BK5%2FwRQHOWg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3680073191", "description": "About the job\nHello All, Hope you are doing great!! Please go through the job descriptions and let me know your interest. Role: Quantexa Data engineer  Location: Remote Duration: Long Term Contract Job Description: 1. End to End usage of Quantexa including guiding our client team on how to use from both front end and data ingestion Reason they are asking for Certification 2. Resolve Entity issues with data 3. Knowledge of Scala and Hive which requires coding experience using parquet 4. Advanced SQL knowledge requiring writing SQL queries 5. Experience having worked with GCP data lake 6. Quantexa certification required\n\nThanks & Regards,\n\nSatnam Singh\n\nDirect: 201 623 3660\n\nEmail : Satnam.singh@sparinfosys.com"}, {"title": "Remote Work - Need Staff Data Engineer - USC only", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3746283980/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=CvIuLBNO%2BPMGi4IXVRkP0A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3746283980", "description": "About the job\n100% Remote, client is in NY\n\nLook for strong job history, Python, Machine Learning, SQL, 10/10 comms, etc. \n\nMUST be an Engineer \n\nSummary\n\nWe're looking for an experienced Data Engineer to join our client's growing team of top notched IT professionals. You will work closely across multiple groups including Editorial and Audio, Marketing, Advertising, etc. to achieve their goals and objectives. The Data Engineering team builds tools throughout the stack while sharing knowledge across these departments.\n\nAs the company grows, they're looking for Data Engineer who will help solidify and expand our pipelines and maintain their data warehouse. Their business model is based on performing complex and very detailed analyses of how our products perform with their subscribers. You'll be responsible for working alongside analysts, product, marketing, and other teams to define new data collection components and measurement schemes that support each new product and feature.\n\nHow Can Add Value To Their Mission\n\nCreate and maintain data pipelines to provide insights and drive business decisions\nEstablish data warehousing strategy (ex. Kimball, Data Vault, etc.)\nMaintain data infrastructure on our AWS accounts\nCollaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.\nWrite unit/integration tests, contributes to engineering wiki, and documents work\nImplement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it\n\nWhat You'll Need To Succeed\n\n7+ years of industry experience measuring product performance and user behavior\nExperience working with a variety of data management technologies, including RedShift, Kinesis/Kafka, Glue, Spark, Postgres, Airflow, dbt, and others.\nExperience implementing BI reporting tools such as Looker\nExperience interfacing with engineers, product managers and analysts to understand data needs\nKnowledge of a variety of measurement beacons, SDKs, APIs including Google Analytics, Amplitude, Braze, Stripe, email service providers\nA commitment to building secure, resilient, fault-tolerant architectures, with clear documentation and procedures in place for support\nA focus on accuracy and detail as you build, ensuring data pipelines produce clear, predictable results\nUnderstanding of the typical metrics a subscription and advertising-supported business needs to measure success\nExperience with Client techniques as applied to behavioral segmentation or anomaly identification is a definite plus\nFamiliarity using developer tools that increase productivity and facilitate the development of resilient code (eg. Docker, CircleCI, Serverless) is a plus"}, {"title": "ENTRY LEVEL DATA ENGINEER (REMOTE)", "company_name": "SynergisticIT", "location": "Jacksonville, FL (Remote)", "link": "/jobs/view/3767587909/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=om7L3CnowiC4PLIcpZD70A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3767587909", "description": "About the job\nSYNERGISTICIT wants every candidate to know that the Job Market is Challenging and to stand out, you need to have exceptional skills and technologies and that's where we come in to make sure you get the attention which you need\n\nSynergisticIT understands the complex nature of the job market and how difficult it can be to secure a position, especially for fresh graduates. Therefore, we assist and help tech-savvies to convert their passions into professions. We go above and beyond to keep you working in your niche.\n\nAs we focus on long-term success, we provide complete career development solutions. From job search to upskilling portfolio and interview preparation, we can guide you at every step of your career.\n\nSynergisticIT spares no efforts to connect you with a large network of tech giants, including Google, Apple, PayPal, Dell, Cisco, Client, etc. Presently, we are actively looking for ENTRY LEVEL DATA ENGINEER a driven mindset. Get the right opportunity and gain experience in building web-centric solutions on Java.\n\nWho Should Apply : Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or anyone looking to make their career in IT Industry\n\nWe also assist in filing for STEM extension and H1b and Green card filing.\n\nCandidates who are serious about their future in the IT Industry and have set big goals for themselves.\n\nCandidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. We also offer Skill Enhancement Programs if the candidates are missing skills or experience which our clients need with great outcomes\n\nCandidates can benefit from skill enhancement if they fall into the below categories. If they are qualified with enough skills then no need for skill enhancement\n\nCandidates Who Lack Experience\n\nHave had a break in careers\n\nLack Technical Competency\n\ncandidates who want to get employed and make a career in the Tech Industry \n\nPlease Also Check The Below Links\n\nhttps://www.synergisticit.com/candidate-outcomes/\n\nhttps://www.synergisticit.com/java-track/\n\nhttps://www.synergisticit.com/data-science-track/\n\nhttps://www.synergisticit.com/which-is-the-best-option-for-tech-job-seekers-staffing-companies-consulting-companies-bootcamps-or-synergisticit/\n\nhttps://www.synergisticit.com/contact-us/\n\nIf the skills are not a match candidates can opt for Skill enhancement. Or their resume can be sent out to clients to see if responses are achievable\n\nREQUIRED SKILLS For Java/Software Programmers\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Core Java , javascript , C++ or software programming\nSpring boot, Microservices and REST API's experience\nExcellent written and verbal communication skills\n\nFor data Science/Machine learning\n\nRequired Skills\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Statistics, Python, data visualization tools\nExcellent written and verbal communication skills\n\nPreferred skills: NLP, Text mining, Tableau, Time series analysis\n\nTechnical skills are required by clients for selection even if its Junior or entry level position each additional Technical skill helps a candidate's resume to be picked by clients over other job seekers.\n\nClients hire candidates with the right technical skills which they need and reject candidates who lack the required technical skills.\n\nNo third party candidates or c2c candidates\n\nPlease apply to the posting\n\nNo phone calls please. Shortlisted candidates would be reached out"}, {"title": "Data Engineer - Remote | WFH", "company_name": "Get It Recruit - Information Technology", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3782241984/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=BgBf1kSAAX%2BSuw6inJsIVw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3782241984", "description": "About the job\nJoin an innovative team at a leading company in the engineering and manufacturing industry, where we are committed to leveraging data for continuous improvement and operational excellence. This hybrid position allows you to enjoy the flexibility of remote work on Mondays and Fridays.\n\nWhy Join Us\n\nAt our company, we prioritize your well-being and professional growth. We offer a comprehensive benefits package, including excellent health benefits, a 6% 401(k) match, generous paid vacation and sick time, 12 paid holidays, and more.\n\nRole Overview\n\nAs a Data Engineer, you will play a pivotal role in transforming raw data into actionable insights, driving improvements in engineering and manufacturing processes, enhancing product quality, and optimizing operational efficiency. Collaborate with dynamic cross-functional teams to extract, analyze, and visualize data, influencing strategic decisions through your valuable insights.\n\nKey Responsibilities\n\nData Collection and Integration:\n\nGather, clean, and integrate data from diverse sources, including production databases, sensor data, and external datasets.\n\nData Analysis\n\nConduct exploratory data analysis to identify patterns, trends, and anomalies.\n\nPerform statistical analysis and hypothesis testing to derive meaningful insights.\n\nData Visualization\n\nCreate informative and visually appealing dashboards and reports using tools like Tableau, Power BI, or Python libraries.\n\nPredictive Modeling\n\nDevelop predictive models to forecast manufacturing performance, quality, and operational outcomes.\n\nRoot Cause Analysis\n\nInvestigate production issues and quality concerns by analyzing data to identify root causes and recommend corrective actions.\n\nPerformance Monitoring\n\nEstablish and maintain key performance indicators (KPIs) to track manufacturing and engineering metrics.\n\nGenerate regular reports to communicate performance trends to stakeholders.\n\nCross-functional Collaboration\n\nCollaborate with engineering, manufacturing, and quality assurance teams to understand their data needs and provide analytical support.\n\nContinuous Improvement\n\nSuggest process improvements and optimizations based on data analysis.\n\nData Security And Compliance\n\nEnsure data security and compliance with relevant regulations, such as GDPR or industry-specific standards.\n\nInfrastructure And Development\n\nBuild and maintain the infrastructure for collecting, storing, and processing data.\n\nDesign, develop, and manage data pipelines, ETL processes, and data warehouses.\n\nRequired Experience\n\nProven experience as a Sr. Data Analyst and/or Data Engineer in an engineering or manufacturing environment.\n\nProficiency in data analysis tools and programming languages, such as Python, R, SQL, and Excel.\n\nStrong data visualization skills with tools like Tableau or Power BI.\n\nExcellent knowledge of statistical analysis and machine learning techniques.\n\nAbility to communicate complex findings and insights effectively to both technical and non-technical stakeholders.\n\nStrong problem-solving skills and attention to detail.\n\nFamiliarity with manufacturing processes and quality control is a plus.\n\nExperience with enterprise ERPs (SAP S4HANA, Oracle EBS), CRMs (Salesforce, Hubspot), and IoT.\n\nProficient in working with cloud computing platforms including AWS, GCP, and Azure.\n\nExpertise in configuring and optimizing cloud-based data solutions for scalability, reliability, and performance.\n\nBachelor's Degree in Data Science, Statistics, Engineering, or Computer Science Required.\n\nApply Now and be part of a team shaping the future of data-driven innovation in engineering and manufacturing!\n\nEmployment Type: Full-Time"}, {"title": "Entry Level Software Engineer - Data Backend Engineer (Remote - Canada)", "company_name": "Yelp", "location": "United States (Remote)", "link": "/jobs/view/3752881757/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fankeoxifvhpTzOYvowHYQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3752881757", "description": "About the job\nYelp engineering culture is driven by our values: we\u2019re a cooperative team that values individual authenticity and encourages creative solutions to problems. All new engineers deploy working code their first week, and we strive to broaden individual impact with support from managers, mentors, and teams. At the end of the day, we\u2019re all about helping our users, growing as engineers, and having fun in a collaborative environment.\n Interested in exploring data and finding innovative ways to collect it, curate it, and make it easily accessible for others to use? How about doing that at an enormous scale? Yelp\u2019s datasets contain billions of interactions between users and local businesses around the globe. If working with our vast amount of data sounds exciting, come and join us!\n\nThe Marketplace Data Observability team is responsible for managing, governing, and monitoring all of Yelp\u2019s Ads data to protect our revenue stream and also support offline analysis that aids in making long term company decisions. We build tools that help us to better understand our data quality, and we strive to uncover data issues before they impact our consumers.\n\nAs a Data Backend Engineer on this team, you will be responsible for building elegant and scalable data products that serve critical, up-to-date, structured information to support different types of analytics within Yelp. As a core contributor to our growing data modeling and data warehousing engineering efforts, you will help design and own mission-critical data flow pipelines and datastores to enable decisions including effective A/B testing and company investments.\n This opportunity is fully remote and does not require you to be located in any particular area in Canada. We welcome applicants from throughout Canada. We\u2019d love to have you apply, even if you don\u2019t feel you meet every single requirement in this posting. At Yelp, we\u2019re looking for great people, not just those who simply check off all the boxes.\n  Build systems that can effectively store and crunch terabytes of data.\nDesign and develop data models for efficient data storage, retrieval, and reporting.\nCreate and maintain conceptual, logical, and physical data models using industry-standard modeling tools.\nCollaborate with cross-functional teams, including engineers, data analysts, business analysts, and data scientists, to understand data requirements and translate them into effective data models.\nParticipate in data integration efforts, including ETL processes and data migration.\nStay up-to-date with industry best practices, emerging technologies, and trends related to data warehousing.\nSupport on-call rotations as needed to operate the team.\n  Understanding of high performing and scalable data systems.\nExperience in building and orchestrating ETL pipelines.\nExperience with Data Lake or Data Warehouse landscape.\nA hunger for tracking down root causes and fixing them in systematic ways.\nAbility to communicate effectively to technical and non-technical cohorts alike. \nExposure to some of the following technologies: Python, AWS Redshift, AWS Athena / Apache Presto, Big Data technologies (e.g S3, Hadoop, Hive, Spark, Flink, Kafka etc), NoSQL systems like Cassandra, DBT."}, {"title": "Data Engineer (Associate Consultant), June 2024", "company_name": "UDig", "location": "Nashville, TN (Remote)", "link": "/jobs/view/3716039960/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=XUjo40kU55HnDCqJ%2F3FdPg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3716039960", "description": "About the job\nCan UDig It? \n\nUDig designs, builds, and implements technology solutions that deliver on business objectives. People join our team because they are excited to have an impact on the future of our organization, passionate about delivering meaningful results, and energized by solving complex problems. Oh, and they like working with other driven, smart, and authentic people!\n\nUDig is proud to be recognized as a \"Best Places to Work\" in Virginia and Tennessee. We're committed to fostering a transparent environment where leadership is accessible, professional development opportunities are abundant, and you are empowered to achieve your career goals. You'll collaborate, develop deep relationships with teammates, and accelerate your success while having fun!\n\nIf you're graduating in May 2024 and excited by the idea of impacting organizations by building data solutions to achieve business goals, you should consider joining our growing technology team.\n\nHow Things Go Down\n\nAs a new, entry-level teammate, you'll take part in our \"Breaking Ground\" training program designed to help you quickly get up-to-speed on UDig's delivery approach within a project team. The program covers:\n\nUDig delivery process and methodologies such as gathering requirements and delivering projects using agile\nTechnical guidance and instruction on the best practices from Data leaders at UDig\nDelivering executive and technical presentations\nCoaching to prepare you to for success on your first client project at UDig\n\nA typical day might entail:\n\nProject standups and check-ins\nDesign and development of data pipelines, BI reports, and more\nPlanning and participating in project demos\nContributing to internal UDig projects\nCareer and job growth through UDig-provided training\nCreating long-lasting friendships with teammates through fun activities as well as your projects (including social events, winning a prize for guessing the weight of pumpkin, or sharing experience on cool tech side projects)\n\nUDig believes it is important for Associate Consultants going through our Breaking Ground program to reside in one of our major markets (Richmond, VA or Nashville, TN metro area) for at least the first year of employment.\n\nWant to learn more? Hear directly from our teammates by checking out our 'A Day in the Life' blog series: https://www.udig.com/digging-in/breaking-ground-2023/\n\nSound Interesting? Here's the Technical Piece\n\nBelow are a few boxes you should be able to check if\u202fyou're interested: Don't check every box? Go ahead and apply!\n\n1+ year of experience with relational database management systems such as Microsoft SQL Server, MySQL, or Oracle\nUnderstanding of the full lifecycle of design, development, and implementation of data solutions including architecture design, development, testing\nSome experience in ETL development or reporting is a plus\nAbility to present technical ideas and high-level concepts and solutions to internal and external team members with varying degrees of technical knowledge\nBachelor's Degree in in computer science, information technology, or any science or business discipline\nAbility to take your work seriously but also seriously enjoy a good time with your UDig colleagues\n\nWhat's in It For You? \n\nAt UDig, you will build your career alongside talented and experienced developers while gaining experience and having fun! We provide continuous challenges and professional development opportunities. Additionally, we offer: \n\nCompetitive salary, merit reviews, and career advancement paths\nFlexible, hybrid environment\nIndividual $1500 Training Budget\nRegular team building and social activities (virtual and/or in-person)\nTransparent culture with strong communication and access to leadership\nGreat benefits like\nGenerous Paid Time Off, company holidays, and parental leave\nMultiple Single and Family Health Insurance plans to choose from\nDental & Vision coverage\nShort-Term and Long-Term disability\nOptional accident and critical illness coverage\nMatching 401(k)\nand more!"}, {"title": "Expression of Interest: Data Engineer", "company_name": "Fingerprint for Success (F4S)", "location": "New York, NY (Remote)", "link": "/jobs/view/3787769880/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=l%2FEDpQqUNmCttGdIyTk7Cg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787769880", "description": "About the job\nWe are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.\n\nThe F4S Talent Pool is a pilot project designed to:\n\nHelp job seekers get discovered by our partners based on their anticipated hiring needs.\nProvide optional support and resources for job seekers in their career endeavors.\nHelp individuals understand, and bring out the best in themselves and each other.\n\nThe F4S Talent Pool process:\n\nOnce you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.\n\nAbout Fingerprint For Success (F4S)\n\nBacked by 20+ years of research, F4S\u2019s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high performing teams.\n\nKeep in mind, joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your F4S work style assessment to match the hiring needs of our partners.\n\nYour feedback is a gift! Write to us via:\n\nPowered by JazzHR\n\noWuPJ9lwq9"}, {"title": "Data Engineer Seattle, WA (remote)", "company_name": "Conch Technologies, Inc", "location": "United States (Remote)", "link": "/jobs/view/3765052211/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=SBETI78UMutCHuRp5FnP%2FA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3765052211", "description": "About the job\nHi,\n\nGreetings from Conch Technologies Inc\n\nData Engineering Lead \u2013 With Databricks and Unity Catalog\n\nLocation \u2013 Seattle, WA (remote)\n\nDuration \u2013 12 months\n\nNeed: Design and implement data architecture and strategy, leveraging technologies such as Databricks, Unity Catalog, Privacera, and Collibra.\n\nAs a Data Engineering Lead, you will play a pivotal role in defining and executing technical deliverables while assessing complexities and levels of effort. You'll engage with both vendor and internal teams to resolve technical challenges and ensure seamless collaboration. Your leadership will extend to planning and prioritization with the SCRUM master, facilitating effective coordination between onsite and offshore teams, and driving feature prioritization through collaboration with product managers.\n\nKey Responsibilities\n\n Lead the definition and breakdown of technical deliverables, assessing complexities and levels of effort.\n Collaborate with vendor and internal teams to address technical issues.\n Drive planning and prioritization in conjunction with the SCRUM master.\n Coordinate with onsite and offshore teams for timely project delivery.\n Collaborate with product managers to prioritize features, communicate with stakeholders, and clarify requirements.\n Collect and manage user-reported issues, ensuring prompt resolution and communication.\n Define comprehensive test plans, oversee their execution, and document results.\n\n--\n\nWith Regards,\n\nNagesh G\n\nMobile: 408-381-5645\n\nDesk: 901-313-3066\n\nEmail: nagesh@conchtech.com \n\nWeb: www.conchtech.com"}, {"title": "Remote Work - Need Azure Fabric Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3759325599/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=C3iVjmcR9wjUip1nNjUiCg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3759325599", "description": "About the job\nThe client is placing heavy emphasis on Microsoft and Azure Fabric experience\n\nInformation Management Engineer \u2013 Take the identified information management sources and assist migration to the Client Data Lake, Azure Fabric. The engineer will transform information management (de-dup, clean and enrich).\n\nTake the identified data sources and bring them into our Data Lake, Azure Fabric. Once the data is in, the engineer will need to be able to transform data (de-dup, clean and enrich the data). We are focused on using a medallion data model for data architecture. The tools we are working with are: Delta Parquet files, python, spark, notebooks, T-SQL, Fabric lakehouse/warehouse, Dataflows Gen2, Data Pipelines, Direct Lake, shortcuts, ADF."}, {"title": "Data Engineer", "company_name": "Catalist", "location": "Washington, DC (Remote)", "link": "/jobs/view/3744951279/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ui1Fk6ICmnPNqAlZDZAcwA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3744951279", "description": "About the job\nFor over 17 years, Catalist has been a leader in civic data and data science innovation. Our mission is to provide progressive organizations with the data, software, and services needed to better identify, understand, and communicate with the people they need to engage and mobilize. Our clients include the largest, most influential organizations in the U.S. active in civic engagement, advocacy, and political campaigns.\n\nCatalist is home to a dedicated, creative team of technologists, data scientists, and campaign experts committed to using our talents and technology to nurture a vibrant and growing progressive community.\n\nAs a Data Engineer at Catalist, you will work closely with the Analytics and Technology departments to design, build, support, and maintain various data pipelines and processes with an end goal of providing data and intelligence to the progressive community. This role primarily involves translating data architecture designs into functional processes, code, and systems.\n\nThe ideal candidate will be a highly motivated individual with excellent technical skills, a strong desire to learn new skills, and an interest in progressive politics. Catalist values creativity and problem-solving. Our work is on the cutting edge of data-driven politics, and your support will help Democratic candidates and progressive organizations conduct successful advocacy and electoral campaigns.\n\nThis position reports to the Deputy Chief Data Officer. The Data Engineer is a part of a growing Data team that supports all underlying work at Catalist.\n\nThis position is included in our CWA bargaining unit.\n\nPrinciple Duties & Responsibilities\n\nDevelop scalable, production processes, code, and systems for data ingest, transformation, modeling, and reporting across a variety of platforms\nTranslate mock-ups and designs into functional processes, code, and systems\nCreate architecture designs when needed\nProvide quality assurance and testing on processes, code, systems, and products\nBecome an internal subject matter expert on various datasets and support other Catalist departments/teams on usage of those datasets\nExecute ad-hoc data and database maintenance tasks as requiredProject manage cross-departmental efforts, with direct responsibility for stakeholder engagement, management, and execution of technical elements\n\n\nRequirements\n\nBS or BA in a technical field, or relevant experience\n1-2 years of experience working with SQL databases\nExperience with data cleaning or ETL processes\nExperience with distributed computing systems and/or distributed datastores (particularly the Hadoop ecosystem)\nExperience managing projects\nFamiliarity with Catalist data, progressive politics, voter files, and/or commercial data\nBackground check required\n\n\nPreferred Skills & Abilities\n\nWillingness to be a problem solver and produce results in a fast paced environment\nAbility to be creative and personable, and articulate ideas clearly\nExperience working with SQL databases\nProficiency with Python or another object-oriented programming language (R, Java, Scala, etc\u2026)\nExperience working in cloud environments (AWS, GCP, etc.)\nExperience working in command line environments such as Bash\nExperience with a version control tool such as git or github\n\n\nBenefits\n\nMedical, Dental, Vision, Prescription Drug\n\nCatalist offers Medical, Dental, Vision, and Prescription Drug coverage for eligible staff and their eligible dependents. Catalist\u2019s Medical plan is a comprehensive PPO program including Prescription Drug coverage with 85% of the premium paid by Catalist. Dental and Vision coverage is provided at no cost to employees.\n\nGroup Term Life Insurance and Long-Term & Short-Term Disability Coverage\n\nGroup Term Life Insurance and Long-Term and Short-Term Disability coverage is available for eligible staff. These benefits are provided at no cost to Catalist employees.\n\n401(k) Safe Harbor Plan\n\nA 401(k) Safe Harbor Plan is available to eligible staff with a 3% contribution from Catalist from the date of hire. Employees may contribute pre-tax or post-tax from their salary up to the legal limits set forth by the IRS.\n\nMedical and Dependent Care Flexible Spending Accounts (FSAs)\n\nCatalist offers an FSA Program that gives eligible staff the ability to pay out-of-pocket medical/dental/vision/child care expenses from pre-tax earnings.\n\nTransit Benefits\n\nCatalist also makes available a Transit benefit FSA program to eligible employees using pre-tax contributions with a company match.\n\nProfessional Development and Remote Work Expenses\n\nEligible employees may be reimbursed up to $750 each year for professional development / education and remote work expenses.\n\nStudent Loan PayDown or SaveUp\n\nCatalist offers a Student Loan PayDown and College SaveUp benefit for eligible staff.\n\nVacation, Personal Leave, Sick Leave Benefits\n\nCatalist offers generous vacation benefits to all eligible staff. Eligible employees also receive:\n\n 14 Paid Holidays\n Personal Days\n Sick Leave\n Parental Leave\n\n\nHybrid Office/Remote Work\n\nCertain positions at Catalist are eligible for Office/Remote Hybrid or full Remote status."}, {"title": "Remote Entry Level Data Analyst/Engineer", "company_name": "SynergisticIT", "location": "White Plains, NY (Remote)", "link": "/jobs/view/3774171423/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=q54sk1%2FRww0z%2BC%2BE8e0jQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774171423", "description": "About the job\nAt SynergisticIT, we're all about making connections. Whatever IT goals you have, our software programmers can help achieve those. Our Software development teams can take up turnkey projects and execute them in an effective and efficient manner. If you are looking to source talent our recruiters will find the ideal IT talent for your company. What's the secret to our success? Well, it all starts with taking quality time to listen to each client's specific needs. After we have a thorough grasp of your IT goals, we can better customize our Developments as per your specific needs. We can also tailor make recruiting programs to exceed your expectations. Since our founding in 2010, SynergisticIT's strategies have earned the company an enviable position in the software development, IT staffing and IT skill enhancement fields. SynergisticIT continues to work with hundreds of satisfied American clients with our software programmers working on our projects and after gaining hands on experience on cutting edge technologies moving to contribute their skills to great clients like Apple, Google, Client, Ebay, Paypal, Kroger, the Walt Disney Company and hundreds more. If you are tired of working with inefficient programmers who take a lot of time to ramp up we want you to try us. Our software programmers can hit the ground running and get you the maximum return on your investment. You have already tried the rest its time you tried the best. SynergisticIT - Home of the Best Data Scientists and Software Programmers in the Bay Area.\n\n Why Us ? \n\nSynergisticIT has a proven track record of successfully skill enhancement and staffing IT employees for some of the world's most iconic brands. Our team takes the time to fully understand every client's needs so we could best meet your IT staffing requirements. The knowledgeable staff at SynergisticIT is always more than happy to work with clients to ensure they reach their software development goals. Besides staffing, SynergisticIT is also committed to helping young IT professionals advance their career with a robust upskill program . Everyone who goes through SynergisticIT's program learns all the skills necessary to succeed in many IT fields ranging from Java to Machine Learning. Additionally, everyone trained at SynergisticIT has been through extensive mock and technical interview screenings to bolster their career prospects. Last, but certainly not least, SynergisticIT takes great care to respect the privacy considerations for every client. All companies who work with SynergisticIT can rest assured their confidential data is protected using the most up-to-date encryption technologies. SynergisticIT also complies with all the latest NDA agreements.\n\n REQUIRED SKILLS For Java /Software Programmers \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Time series analysis \n\n We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2022 and at Gartner Data Analytics Summit (Florida)-2023 \n\nOracle CloudWorld Event (OCW) Las Vegas 2022 | SynergisticIT - YouTube\n\n https://www.youtube.com/watch?v=OAFOhcGy9Z8 \n\n https://www.youtube.com/watch?v=EmO7NrWHkLM \n\n https://www.youtube.com/watch?v=NVBU9RYZ6UI \n\n https://www.youtube.com/watch?v=Yy74yvjatVg \n\nSynergisticIT at Gartner Data and Analytics Summit 2023 - YouTube\n\nFor preparing for interviews please visit  https://www.synergisticit.com/interview-questions/ \n\n We are looking for the right matching candidates for our clients \n\n Please apply via the job posting \n\n REQUIRED SKILLS For Java /Full Stack/Software Programmer \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning Positions \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Tensorflow \n\n If you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team. \n\n No phone calls please.  Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates"}, {"title": "Data Engineer", "company_name": "Anika Systems", "location": "Leesburg, VA (Remote)", "link": "/jobs/view/3787766893/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=mApL9JnN%2B%2Fr2%2BYUPlxRYOA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787766893", "description": "About the job\nAnika Systems is a fast growing, woman-owned small business that specializes in providing innovative IT solutions for federal government agencies. Our expertise lies in accelerating delivery in Data and Analytics, Intelligent Automation, Application Development, and IT Modernization. We are currently expanding our Federal team and are seeking a passionate and talented Data Engineer. This opportunity is 100% remote.\n\nMust be a U.S. Citizen with the ability to obtain and maintain a government suitability clearance\n\nResponsibilities\n\nDesigns, develops, builds, analyzes, evaluates, and installs database management systems to include database modeling and design, relational database architecture, metadata, and repository creation and configuration management.\nDefines and oversees database organizations, standards, controls, procedures, and documentation. Provides technical consulting in the definition, design, and creation of a database environment.\nAdvises applications development staff and users on data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nDesigns and implements databases with respect to access methods, access time, batch processes, device allocation, validation checks, organization, protection and security, documentation, and statistical methods.\nUses data mapping, data mining, and data transformational analysis tools to design and develop databases.\nDetermines data storage and optimum storage requirements.\nPrepares system requirements, source analysis, and process analyses and designs throughout the database implementation.\n\nRequired Skills And Experience\n\nBA/BS and 1 year of relevant experience\nExperience with DataBricks, SQL, Python\nApplications development experience with data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nExcellent communication skills (both oral and written) with process-oriented organizational skills to ensure project success\n\nDesired Skills And Experience\n\nDemonstrated successful development experience with the full life cycle of government database management systems including database modeling and design, relational database architecture, metadata and repository creation, and configuration management.\n\nPowered by JazzHR\n\nTlKWIorucJ"}, {"title": "Remote Work - Need Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3774712978/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=r3Bp%2F0qcOD2OpC5jpbUOcA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774712978", "description": "About the job\nTitle : Data Engineer\n\nLocation : Remote\n\nDuration : 6+ months to start (long term contract (work order are extended 2x year))\n\nPlease Fill out the skill matrix: Technology Some Awareness (1) Novice (2) Intermediate (3) Advanced (4) Expert (5) SQL Tableau Desktop /Creator Tableau API Integration AWS - S3 Glue AWS - Athena Redshift AWS - Lambda AWS - SNS/SQS msExcel\n\nProfile: Data Analyst will work closely with Architects and leads in the Interflow team, key business stakeholders including program staff to support the organizations effort in the areas of data Analysis and information management. Role should have strong experience in writing SQL queries to extract data for reports. Candidate should have experience working with large set of data. As these reports are very critical for the company and we are looking for an expert who is capable of handling, consuming and extracting large sets of data for analysis and reports.\n\nMust Haves\n\nAWS - Athena\nAWS - S3\nGlue\nhands-on development of reporting applications\nMS Excel\nRedShift\nSQL\nTableau Desktop /Creator\n\nNice\n\nAWS - Lambda\nAWS - SNS/SQS\nTableau API Integration\n\nSkills\n\nBA/BS required (major in an analytical field desired) \nA minimum 10+ years work-related experience. \nExperience in hands-on development of reporting applications for the Web in a professional environment \nThe Ethos of continuous improvement and interest in learning new things.\nStrong analytical thinking and structured problem-solving ability\nAbility to handle multiple projects and assignments simultaneously and effectively in a cross-functional team environment. \nVersed on the agile methodology and best practices. \nExcellent interpersonal and collaboration skill with the ability to work with a diverse set of colleagues, across functions, from different organizations, disciplines, etc. \nSelf-starter, ability to set priorities, work independently and attain goals"}, {"title": "Data Engineer", "company_name": "Team Remotely Inc", "location": "Houston, TX (Remote)", "link": "/jobs/view/3786249829/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=78lY5Jo%2FfyRPC6TQYCdvww%3D%3D&trk=flagship3_search_srp_jobs", "id": "3786249829", "description": "About the job\nThis is a remote position.\n\n Data Engineer (US/Canada Residents Only, 1 year experience, remote)\n\nTeam Remotely Inc. is a staffing and recruitment agency that offers a comprehensive solution for talent acquisition, including sourcing, vetting, pay rolling, and managing talent. Whether you need contract staffing, direct hire, direct sourcing, talent pools, or diversity initiatives, our model can support your hiring strategy.\n\n Hiring Type: Full-Time\n\n Base Salary: $55K-$70K Per Annum.\n\nResponsibilities\n\nDesigning user interface changes for web-based DB applications. \nReviewing application requirements and interface designs. \nDeveloping and implementing highly responsive user interface components using react concepts. \nWriting application interface codes using JavaScript following react.js workflows. \nTroubleshooting interface software and debugging application codes. \nDeveloping and implementing front-end architecture to support user interface concepts. \nMonitoring and improving front-end performance. \n\nQualifications And Experience\n\nBachelor\u2019s degree in computer science, information technology, or a similar field. \nPrevious experience working as a react.js developer. \nIn-depth knowledge of JavaScript, CSS, HTML, jQuery, and front-end languages. \nKnowledge of REACT tools including React.js, Webpack, Enzyme, Redux, and Flux. \nExperience with user interface design. \nKnowledge of performance testing frameworks, including Mocha and Jest. \nExperience with browser-based debugging and performance testing software. \nTroubleshooting skills. \nProject management skills. \nProblem-solving skills. \nVerbal communication skills. \n\n Why work with Team Remotely?\n\nTeam Remotely Inc. is a staffing platform offering a seamless experience for employers and candidates. Employers can post job openings and specify their requirements, while candidates can create profiles and upload resumes.\n\nThe team of Team Remotely continuously learns and adapts based on previous successful placements, constantly improving its matching capabilities. This ensures that the recommendations provided by Team Remotely are tailored and accurate, increasing the likelihood of a successful match between employers and candidates. By providing intelligent and data-driven solutions, they strive to enhance the efficiency and effectiveness of the hiring process, ultimately helping companies find the best talent and individuals find their dream jobs."}, {"title": "Data Engineer", "company_name": "ZAG Technical Services", "location": "United States (Remote)", "link": "/jobs/view/3771403464/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=Hf0VefrKXBFIL5MtHmY0Pw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3771403464", "description": "About the job\nWe are looking for a tech savvy Data Engineer to contribute to the building, operating, and scaling of next generation data platforms and tools that will power data-driven analytics capabilities throughout the ZAG products and services portfolio, spanning areas such as business intelligence, reporting, and data analytics.\n\nThe ideal team member is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. Must be comfortable supporting the data needs of multiple clients and teams, systems, and products within the modernization of ecosystems. The right-fit person will be excited by the prospect of optimizing or even re-designing data processes to support next generation of advanced analytics capabilities, which in-turn translates to implementing strategic enhancements to maintain competitive advantage in the industry.\n\nThe successful team member will leverage their proficiency to\u2026\n\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\nBuild the analytics processes required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Data Management and cloud based \u2018big data\u2019 technologies.\nBuild analytics tools that utilize technologies to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.\nCollaborate with client teams to identify and solution regional and local requirements within the context of the standard global business model for enterprise analytics.\nParticipates in meetings, discussions, strategy sessions where changes, improvements, and enhancements are proposed, evaluated, and approved. \n\n\nThe exceptional team member will have or demonstrate progressive experiences in\u2026 \n\nBachelor\u2019s degree or higher in Computer Science or related field\nExperience working with BI and data warehousing tools\nExperience working with enterprise data, where security is paramount and data governance is critical\nAbility to work independently and in a team environment, and effectively engage all levels of the organization\nUnderstanding of distributed systems driving large-scale data processing and analytics with a successful history of manipulating, processing, and extracting value from large, disconnected datasets.\nAbility to communicate effectively (listening, presenting, and questioning)\nStrong organizational, written, and communication skills\n\n\nAbout ZAG\n\nZAG Technical Services empowers our clients\u2019 success through intelligent business solutions that increase employee productivity, network security and team collaboration. We aim to ensure that our client\u2019s IT investments drive ROI, provide a competitive advantage and help businesses grow. We do this by living our core set of values: Integrity Foremost, Client-Centered, Accountable Always, Teamwork Throughout, and Exceptional is the Goal.\n\nWhat ZAG Has To Offer\n\nCompetitive compensation package \nBenefits including medical, dental, vision, 401K and life insurance \n\n\nZAG Technical Services is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all of our employees.\n\nZAG Technical Services is an E-Verify participant and will provide the Social Security Administration (SSA) and, if necessary, the Department of Homeland Security (DHS), with information from each new employee\u2019s Form I-9 to confirm work authorization."}, {"title": "Junior Data Engineer", "company_name": "HireMeFast LLC - Career Accelerator - Land A Job", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3786249797/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D&trk=flagship3_search_srp_jobs&lipi=urn%3Ali%3Apage%3Ad_flagship3_search_srp_jobs%3BQExogrHBQoSF9zZm%2B9kufw%3D%3D&lici=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D", "id": "3786249797", "description": "About the job\nThis is a remote position.\n\n Job Title: Junior Data Engineer\n\n Employment Type: Full-Time\n\n Salary: $64,000 - $76,000 per annum\n\n Experience Required:  Minimum 1 year of project experience\n\nAbout us: HireMeFast is a leading staffing and recruitment agency specializing in connecting businesses with top-tier talent across various industries. Our mission is to bridge the gap between exceptional candidates and organizations needing their skills, expertise, and unique qualities. Our team of experienced and dedicated recruitment specialists utilizes innovative sourcing strategies, and a vast network to identify and attract top talent. We conduct comprehensive procedures to ensure that only the most qualified candidates are presented to our clients.\n\nPosition Summary\n\nJoin the fast-paced, innovative, and collaborative environment focused on providing an AIOps platform that enhances the intelligence of the CVS Health infrastructure. Work closely with subject matter experts and colleagues to build and scale out machine learning and AI solutions that will detect, predict, and recommend solutions to correct issues before system impact and enhance efficiency, reliability, and performance of CVS Health\u2019s IT operations.\n\nKey Responsibilities include:\n\nData pipeline development: Designed, implemented, and managed data pipelines for extracting, transforming, and loading data from various sources into data lakes for processing, analytics, and correlation. \nData modeling: Create and maintain data models ensuring data quality, scalability, and efficiency \nDevelop and automate processes to clean, transform, and prepare data for analytics, ensuring data accuracy and consistency \nData Integration: Integrate data from disparate sources, both structured and unstructured to provide a unified view of key infrastructure platform and application data \nUtilize big data technologies such as Kafka to process and analyze large volumes of data efficiently \nImplement data security measures to protect sensitive information and ensure compliance with data and privacy regulation \nCreate/maintain documentation for data processes, data flows, and system configurations \nPerformance Optimization- Monitor and optimize data pipelines and systems for performance, scalability and cost-effectiveness \n\nCharacteristics of this role:\n\nTeam Player: Willing to teach, share knowledge, and work with others to make the team successful. \nCommunication: Exceptional verbal, written, organizational, presentation, and communication skills. \nCreativity: Ability to take written and verbal requirements and come up with other innovative ideas. \nAttention to detail: Systematically and accurately research future solutions and current problems. \nStrong work ethic: The innate drive to do work extremely well. \nPassion: A drive to deliver better products and services than expected to customers. \n\nRequired Qualifications\n\n2+ years of programming experience in languages such as Python, Java, SQL \n2+ years of experience with ETL tools and database management (relational, non-relational) \n2+ years of experience in data modeling techniques and tools to design efficient scalable data structures \nSkills in data quality assessment, data cleansing, and data validation \n\nQualifications\n\nKnowledge of big data technologies and cloud platforms \nExperience with technologies like PySpark, Databricks, and Azure Synapse. \n\nEducation\n\nBachelor\u2019s degree in Computer Science, Information Technology, or related field, or equivalent working experience\n\nWhy HireMeFast LLC?\n\nAt HireMeFast, we understand that finding the right individuals to join your team is crucial for success and growth of your organization. We are committed to streamlining the hiring process for our clients, ensuring they have access to a diverse pool of highly qualified candidates who are a perfect fit for their specific needs."}, {"title": "Data Engineer  - Need candidates from Texas ( REMOTE )", "company_name": "Pulivarthi Group (PG)", "location": "United States (Remote)", "link": "/jobs/view/3770685212/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=9gdMwuif%2BpB%2BCQYvQi3imQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3770685212", "description": "About the job\nFollow us on Linkedin: https://www.linkedin.com/company/pulivarthigroup/\n\nPulivarthi Group LLC is a Global Staffing & IT Technology Solutions company, with our prime focus of providing world class solutions to our customers with the right talent. We combine the expertise of our team and the culture of your company to help you with the solution that is affordable and innovative using high quality standards and technologies.\n\nWe\u2019ve served some of the largest healthcare, financial services, and government entities in the U.S.\n\nData Engineer - FT only - Azure SQL with data lakes experience\n\nData Engineer -\n\nthe resource requirement for this Data Engineer role needed more clarification.\n\nOur customer is requesting a resource who knows Azure CI/CD (Continuous Integration / Continuous Deployment) to build and integrate their DevOps pipeline.\n\nOnce the DevOps pipelines are mapped, they need the Data Engineer to connect to all the Data Sources, primarily their Data Lake.\n\nThe Customer (based in Houston) is interested in a resource in a time zone not too far removed from the Central Time Zone to allow real time collaboration."}, {"title": "Data Engineer", "company_name": "Otter Products", "location": "Fort Collins, CO (Remote)", "link": "/jobs/view/3766728302/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=77U3U7CAi1PxgDmpI3%2BiNQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3766728302", "description": "About the job\nOverview \n\nOtter Products is currently recruiting for a Data Engineer to join our Data team! You can be based in our Fort Collins, CO office with flexibility to work a portion of your time remotely, or 100% Remote in the U.S.\n\nThe Data Engineer applies professional experience, concepts, and company objectives toward building and operationalizing the minimally inclusive data necessary for the enterprise data initiatives following industry standard practices and tools. The bulk of the Data Engineer's work would be in designing, managing and optimizing data pipelines, then moving these data pipelines effectively into production for key data and analytics consumers like business/data analysts, data scientists or other stakeholder that needs curated data for data and analytics use cases across the enterprise. The Data Engineer also ensures compliance with data governance and data security requirements while creating, improving and operationalizing these integrated and reusable data pipelines to enable faster data access, integrated data reuse and vastly improved time-to-solution for data and analytics initiatives. Additionally, the Data Engineer will be expected to collaborate with other data team members and data consumers working on the models and algorithms developed by them toward optimization for data quality, security, and governance to put them into production leading to potentially large productivity gains.\n\n About Otter Products \n\nOtter Products, we grow to give. From our founder's garage in 1998 to the global technology leader we are today, Otter Products continues to drive growth through innovation.\n\nThrough our industry-leading brands - OtterBox, Liviri and OtterCares - we provide our partners the number-one selling and most trusted products in our categories. Our philanthropic spirit is the foundation on which we foster our partner relationships, allowing us to grow and to give - together.\n\nBy way of our charitable arm, the OtterCares Foundation, we support our communities and invest in our future through education that inspires kids to change the world.\n\nAnd even as our global community of Otters continues to grow, our founder's core values are still at the heart of everything we do. We measure our success by our ability to give back to our communities and strengthen opportunities for all.\n\nFor more information visit otterproducts.com\n\n Responsibilities \n\nComplete analysis, design and development of BI solutions using Azure SQL Server\nFamiliarity with Data Warehouse concepts\nExperience coding the extraction, transformation, and loading (ETL) processes\nDatabase development primarily in SSIS, Data Factory and SQL\nPartner with the business to determine end user database/reporting needs and requirements\nGenerate ad hoc reports using Power BI or SSRS\nCollaborate with other developers to create and implement best approach solution(s)\nTroubleshoot Azure tools, systems, and software\nReview queries for performance issues, making changes as needed\nCollaborate with team to performance-tune Azure application as necessary\nAssist with the analysis and extraction of relevant information from large amounts of historical business data to feed data science initiatives\nParticipate and support the design and documentation of processes for large scale data analyses, model development, model validation and model implementation\nSupport and maintain a positive safety culture by following all safety policies and procedures and actively contributing to a safe working environment\nOther duties as required\n\n Qualifications \n\nBachelor's degree required. Degree in Computer Science or Mathematics preferred\nMinimum of four years of experience in an IT or analytical role required. Experience in database development, report writing and/or statistics preferred.\n\n EEO \n\nOtter Products, LLC is an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, age, marital status, pregnancy, sex, sexual orientation, gender, gender identity or expression, national origin, disability, veteran status, or any other characteristic or status protected by law.\n\n For US Based Roles Only - Compensation Range Minimum \n\nUSD $90,000.00/Yr.\n\n For US Based Roles Only - Compensation Range Maximum \n\nUSD $120,000.00/Yr.\n\n Additional Total Rewards \n\nProfit Sharing Program Eligible, Benefits Eligible - Full Time- check out otterproducts.com/careers/why for more info"}, {"title": "Software Engineer, Distributed Systems", "company_name": "Hightouch", "location": "United States (Remote)", "link": "/jobs/view/3628572539/?eBP=CwEAAAGMd4QO8APODyE_-mMOPhYyQGg5ZjVbPwq_3V7DK0B2bLAf10pcedM9S2T71X5i4TiJuEwOzWBbSiTKbLuk-VD-_Y-dN5uyf8YpYkHYqEYFxySMS9zr2hRjNrxX9gXbxssh35TFK9YTNV2WeH2Ncb675JgFbJTgNWSZySFC6rrdJGmdSDTr2q5d3VoxORz3GjsDbzjeyZQCvQ07RqYw7oujWSgslycs2LMCY_-N0NEj6e0PvbaYKiZ_5IW34bGVSYLXpHwqa8oOcNttYZ90UA3E_7sDSFzABdIVlfrp5zzERrIYy5sh2MIInxHdDoKgQw8VmOn5uM4S0tXUl-i7LnN0_W0NfPBmLiVDNuveIOm4MfDNPxfe5QKPk6FUAVpvgbcoJn9yRyE&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=rph%2FT6Q6Vkp6YoggJf9cJQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3628572539", "description": "About the job\nAbout Hightouch\n\nHightouch\u2019s mission is to empower everyone to take action on their data. Through our Reverse ETL platform, business and data users can seamlessly sync data from where it resides, such as warehouses and databases, to where it is needed, including operational systems and SaaS tools. Traditionally, acting on data has required engineering time and bandwidth, and left most business users stuck with charts and reports that are unable to take automated action on their data. With Hightouch, every business user, without writing any code, can activate data to streamline critical processes, improve marketing performance, and scale operations.\n\nOur team operates with a focus on making a meaningful impact for our customers. We believe in approaching challenges with a first principles thinking mindset, moving quickly and embracing our value of efficient execution, and treating each other with compassion and kindness. We look for team members that are strong communicators, have a growth mindset, and are motivated and persistent in achieving our goals.\n\nHundreds of companies use Hightouch, including Spotify, Ramp, Retool, NBA, Plaid, and Betterment. We\u2019re based in San Francisco, are remote-friendly, and backed by leading investors such as Amplify Partners, ICONIQ Growth, Bain Capital Ventures, Y-Combinator, and Afore Capital.\n\nAbout The Role\n\nWe are looking for a distributed systems engineer to work on the systems that power the syncing engine our customers and other engineering teams rely on. This presents an exciting challenge where you can apply your expertise in distributed systems, performance optimization, and troubleshooting to push the boundaries of what is possible and meaningful improve our ability to move massive amounts of customer data. This role also provides a unique opportunity to work on a multi-cloud and multi-region infrastructure that supports a global customer base.\n\nWe believe in enabling our engineers to do their best work for our customers by giving them extremely high levels of ownership and autonomy. This comes in different forms: you will own and deliver projects from start to finish, you will work directly with customers to solve their hardest scaling problems, and you will have a lot of influence over what we work on as a team and company.\n\nSome of the problems we\u2019ll be working on include:\n\nSync Speed: Customers want to sync a lot of data to important destinations like Facebook and Snapchat, which requires us to analyze every part of our syncing process and find where we can optimize to sync data more quickly\nStreaming Syncing: We currently sync data from sources in batch only, but envision a world where we also do syncs in real-time and support streaming sources like webhooks and queues\nScalability and Reliability: As part of our rapid growth, we\u2019re always evaluating current future bottlenecks in scaling and reliability, and architecting for the next order of magnitude of growth\nPersonalization API: Our Personalization API product provides a low latency caching layer on top of data warehouses that enable customers to use their data warehouse for real-time personalization use cases with \nMulti-Region and Multi-Cloud: Supporting our multi-region and multi-cloud backend, including extending it to launch Hightouch on in new regions to support data residency requirements of our global customer base\n\nWe are looking for talented, intellectually curious, and motivated individuals who are interested in tackling the problems above. This is a senior role, but we focus on impact and potential for growth more than years of experience. The salary range for this position is $170,000 - $240,000 USD per year, which is location independent in accordance with our remote-first policy.\n\nAbout You\n\nYou are an engineer with a passion for solving hard technical problems that generate real value for customers. You\u2019re motivated by high ownership and are comfortable in a fast-paced, startup environment.\n\nYou have experience and comfort with distributed systems and high-scale systems or the ability and desire to build this experience quickly. This experience can take different forms:\n\nYou\u2019ve built syncing engines that process and move terabytes of data per day in a scalable way\nYou\u2019ve created event collection or data streaming services that power company critical systems\nYou\u2019ve owned systems that have undergone orders of magnitude of growth and helped them scale accordingly\nYou have relevant skills and are a fast learner who is excited to take on the challenges we face\n\nInterview Process\n\nOur goal with the interview process is to balance speed with giving both parties opportunities to assess whether there is a strong mutual fit. We will ask you questions, but we want you to ask us questions! Our technical interviews focus on how you design systems because we believe this is the best way for us to see how you work and for you to see how we collaborate. We don\u2019t ask you to write code to solve technical brainteasers that don\u2019t appear in your day to day job.\n\nRecruiter screen [30m]: Introductory call with our recruiting team to get to know each other and see if the role could be a good mutual fit.\nSystems design screen [45m]: Designing a data processing feature end-to-end.\nHiring manager interview [30m]: Chat with hiring manager about past experiences and future operating preferences to assess fit on company values and operating principles.\nSystems design interview [90m]: Work with the interviewer to architect a system at a conceptual level. The problem will be at a pretty high level - and have both product and customer requirements as well as technical."}, {"title": "Entry-Level Data Scientist Engineer", "company_name": "Phoenix Recruitment LLC", "location": "Texas City, TX (Remote)", "link": "/jobs/view/3788727625/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=8iwtytHsCu4wMwJ4fj3pxw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3788727625", "description": "About the job\nThis is a remote position.\n\nTitle - Entry-Level Data Scientist Engineer, 1 year of project experience\n\nEmployment Type: Full-time\n\nBase Salary: $60K-$70K\n\nPhoenix Recruitment offers a variety of recruiting services to assist both employers and employees. They specialize in marketing open positions, recruiting, and helping employers to find qualified candidates across various industries. Phoenix Recruitment has expertise in streamlining the hiring process. They can help ensure that the process is efficient, well organized, and compliant with relevant regulations.\n\nSkills and Abilities:\n\nStrong knowledge of R or Python for data analysis and modeling. \nProficiency in statistical programs such as R, SAS, MATLAB, or Python. \nFamiliarity with spreadsheets (VBA) and database applications (Access, Oracle, SQL, or equivalent technology). \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \nAbility to learn new methods quickly and work under deadlines. \nExcellent teamwork and communication skills. \nStrong analytical and problem-solving abilities. \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \n\nPreferred:\n\nKnowledge of actuarial concepts and life, health, and/or annuity products. \nExperience with statistical modeling techniques such as GLM, Decision Trees, Time Series, Regression, etc. \nFamiliarity with Microsoft DeployR. \nExposure to insurance risk analysis. \nBasic experience in computational finance, econometrics, statistics, and math. \nKnowledge of SQL and VBA. \nFamiliarity with R or Python for predictive modeling \n\nWhy Phoenix Recruitment LLC?\n\nPhoenix Recruitment often has an extensive network of employers and candidates. This network allows them to tap into a pool of qualified candidates and connect them with suitable job opportunities. They can also leverage their connections to help employers find the right talent efficiently. Outsourcing the recruitment process to a specialized agency can save your time and resources, avoid delays, reduce administrative burdens, and increase the chances of finding the right fit for your organization."}, {"title": "Data Engineer - Remote Work", "company_name": "BairesDev", "location": "United States (Remote)", "link": "/jobs/view/3783162297/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=OtoCrAAHZFomVup04fy35g%3D%3D&trk=flagship3_search_srp_jobs", "id": "3783162297", "description": "About the job\nWho We are\n\nBairesDev is proud to be the fastest-growing company in America. With people in five continents and world-class clients, we are only as strong as the multicultural teams at the heart of our business. To consistently deliver the highest quality solutions to our clients, we only hire the Top 1% of the best talents and nurture their professional growth on exciting projects.\n\nWe are looking for a Data Engineer to join our Development team and participate in different projects made up of multicultural teams distributed throughout the world.\n\nWhat You Will Do\n\n Interview other Data Scientists and Data engineers to vet their technical abilities.\n Input data from those interviews into the product and help develop a predictive, fair, and enjoyable solution.\n Work together with developers, tech leads, and solution architects to build applications.\n Improve existing structures by adding new functionalities or proposing technological updates so that as a result, the impact of your contribution is significant in the core of each business.\n Improve the user experience on scalable and high availability platforms, contributing to the key differential of each business.\n\nHere\u2019s What We Are Looking For\n\n 5+ years of experience working as a developer.\n 3+ years of experience working as a Data Scientist / Data Engineer.\n Proficient with analysis, troubleshooting, and problem-solving.\n Hands-on experience with managing data loads and data quality.\n Advanced English level.\n\nHow we do make your work (and your life) easier:\n\n 100% remote work.\n Hardware setup for you to work from home.\n Flexible hours - make your schedule.\n Paid parental leave, vacation & holidays.\n Diverse and multicultural work environment.\n An innovative environment with the structure and resources of a leading multinational.\n Excellent compensation \u2014 well above the market average.\n Here you can grow at the speed of your learning curve.\n\nOur people work remotely but with a consistent and robust culture that promotes diversity and teamwork. To continue being the leading software development company in Latin America, we want to ensure that every BairesDev member gets the best growth and professional development opportunities in a diverse, welcoming, and innovative environment.\n\nEvery BairesDev team member brings something unique to our company.\n\nWe want to hear your story. Apply now!"}, {"title": "Data Engineer (Remote)", "company_name": "Cascade Debt", "location": "United States (Remote)", "link": "/jobs/view/3755336571/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=pmUtn1j2zyPr25CNPh2RNw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3755336571", "description": "About the job\nDo you want to work in a small team where you can make a real difference in a company? Would you embrace the challenge of building a high impact platform that is used globally? Then we want to speak with you!\n\nAbout Cascade\n\nCascade is a fintech startup backed by Canadian and US investors that empowers companies to grow by democratizing access to institutional debt. We are building tools that modernize how companies raise and manage debt, lowering the barriers to entry to the $7 trillion specialty finance and alternative credit market for companies around the world. Founded just last year, we are gearing up for our next phase of technical development and are seeking a talented Data Engineer to lead the way.\n\nAbout The Role\n\nAs a Data Engineer you\u2019ll be an early member of a growing team building a pioneering platform in the debt infrastructure space. You will design, build, and launch efficient, scalable, and reliable data pipelines to move and transform data .\n\nWhat You\u2019ll Do\n\n Building databases, data lakes and data ingestion pipelines to integrate customer databases and datasets to our systems \n Ingesting financial datasets from external customers, then u pdating and maintaining accurate and complete data mappings to ensure that our product s are displaying high quality data \n M onitoring and alerting across our data pipelines in order to make sure that our data ingests are reliable and correct \n Translat ing business goals and stakeholder requirements into new data flows and deliver analytical insights ensuring that the data flows create real business value continuously \n\nSkills And Experience\n\n 2 + years experience in a data engineering or data analyst role \n E xperience with relational SQL and NoSQL databases, including PostgreSQL , MySQL, MariaDB, MongoDB, Bigtable, etc. \n E xperience working with ETL technologies , such as Databricks, Fivetran , or dbt \n Proficiency in writing SQL queries and knowledge of analytical data warehouses such as RedShift, BigQuery , and Snowflake \n Some experience with CI/CD automation such as GitHub Actions \n Developing APIs and integrating with 3rd party APIs \n Bonus: experience in fintech / SaaS / credit analysis \n\nPeople Who Thrive At Cascade Are\n\n Self-starters, who take the initiative to tackle challenges in a remote work environment \n Good communicators, who can operate without ego to discuss, learn, grow, and help others do the same \n Passionate about creating great digital experiences for users \n Problem solvers who are not satisfied with the status quo \n\nBenefits\n\nRemote: we are a remote-first company and are very flexible on hours as long as things get done \nHome-office: there\u2019s a $1000 USD home office allowance to set yourself up \nEquity: we expect you to have an owner-mentality, and have the equity plan to match \nBenefits: health, dental, vision, and more \nPerks: we offer free lunches weekly and off-site trips \nJob satisfaction: we offer autonomy, ample opportunities for mastery, and an opportunity to make a difference for companies around the world"}, {"title": "Data Engineer", "company_name": "ISC (Integrated Specialty Coverages, LLC)", "location": "Carlsbad, CA (Remote)", "link": "/jobs/view/3778293528/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ew7%2FfEuXSujs5IGabdqUyw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3778293528", "description": "About the job\nAbout Integrated Specialty Coverages\n\nIntegrated Specialty Coverages LLC (ISC) is a growth stage technology and data-driven insurance company leading innovation in the market.\n\nBacked by one of the leading private equity firms, KKR, and led by a forward-thinking management team, ISC is combining the worlds of insurance and technology to create an Insurtech powerhouse. As a leading online distributor of insurance products for a range of industries and \"Main Street USA\", we are looking for the right people to help us in our mission of achieving exponential growth. We strive to be the number one place to go for brokers and agents to source insurance. To accomplish this, we're building a digitally focused team that deeply understands the intersection between user experience, data, and AI/ML to optimize the way we engage with our customers and partners.\n\nJob Summary\n\nThe Data Engineer role will be the technical liaison between multiple groups including a data science team, the engineering team, product management, and business stakeholders. You do not need any insurance knowledge prior, however, you must quickly dive deep into the insurance world and ask questions to become a subject matter expert. You will be responsible for building a data platform to facilitate the data science team. You must be a self-starter that can build out features such as a data pipeline from scratch. There will be support from both engineering and data science for any buildout.\n\nJob Responsibilities\n\nCreate and maintain optimal data pipeline architecture. \nAssemble large, complex data sets that meet functional / non-functional business requirements. \nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. \nBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using AWS technologies, SQL, and Python. \nWork with stakeholders including the Executive, Product, Data Science, and Engineering teams to assist with data-related technical issues and support their data infrastructure needs. \nWork with data science and analytics teams to strive for greater functionality in our data systems. \n\nMinimum Qualifications:\n\nAdvanced working SQL knowledge and experience working with relational databases, strong query authoring (SQL) as well as working familiarity with a variety of databases (Snowflake, Redshift, MySQL, MSSQL, etc.)\nExperience building and optimizing data pipelines, architecture and data sets. \nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. \nBuild processes supporting data transformation, data structures, metadata, dependency, and workload management. \nA successful history of transforming, processing and extracting value from large disconnected, datasets from a variety of data sources (Flat files, Excel, databases, etc.)\nStrong analytic skills related to working with unstructured datasets. \nStrong project management and organizational skills. \nExperience supporting and working with cross-functional teams in a dynamic environment. \nAbility to mentor/guide/collaborate with other team members. \n\nWe are looking for a candidate with 3-5 years of experience in a Data Engineer role. They should also have experience using the following software/tools:\n\nExperience with relational and MPP databases, including Snowflake and MySQL. \nExperience developing software in an agile environment from the requirements stage to production. \nExperience with version control: git\nExperience with container technologies: Docker\nExperience with data pipeline and workflow management tools: Airflow, Jenkins, AWS Glue, Azkaban, Luigi, etc. \nExperience with AWS cloud services: EC2, ECS, Batch, S3, EMR, RDS, Redshift\nExperience with other cloud services: Snowflake, Airflow\nExperience with object-oriented/object function scripting languages: Python, Java, C++, etc. \nExperience with data modeling and data warehouse design\nExperience with data visualization tools (PowerBI, QuickSight)\n\nThe starting annual pay scale for this position is listed below. Actual starting pay will be based on factors such as skills, qualifications, training, and experience. In addition, the company offers comprehensive benefits including medical, dental and vision insurance, 401(k) plan with match, paid time off, and other benefits.\n\nISC's salary ranges are determined by role and level. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations and could be higher or lower based on a multitude of factors, including job-related skills, experience, and relevant education or training.\n\nNational Pay Range\n\n$100,000\u2014$110,000 USD\n\nBenefits of Working at ISC\n\nCompetitive vacation and flexible working arrangements\nComprehensive and inclusive health benefits\nA variety of professional development and mentorship opportunities\nChoice of technology whether at home or in the office\n\nISC's Ownership Behaviors\n\nDo the Right Thing *Be Relentless. Pursue Excellence. *Personal Responsibility & Own Outcomes\nTry Fast. Learn Fast. Fail Fast. Think Big. *Build Extraordinary Owners & Join Forces *Grit & Determination\n\nApplicants with disabilities may contact ISC HR department via e-mail, and other means to request and arrange accommodation. If you need assistance to accommodate a disability, you may request accommodation at any time. Please contact ISC at HR@ISCMGA.com.\n\nISC believes in creating long-term relationships by being responsive and relevant and by consistently delivering value to our community of customers. Specifically, with our employees, we focus on attracting, developing, and retaining the best talent for our business, challenging our people, demonstrating a \"can-do\" attitude, and fostering a collaborative and mutually supportive environment. ISC is an equal opportunity employer and a participant in the US federal E-Verify program (US).\n\nDiversity creates a healthier atmosphere: All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, gender, gender identity, sexual orientation, marital status, medical condition, genetic information, mental or physical disability, military or veteran status, or any other characteristic protected by local, state, or Federal law."}, {"title": "Data Engineer -Remote", "company_name": "RIT Solutions, Inc.", "location": "Denver, CO (Remote)", "link": "/jobs/view/3768007496/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=1hQH8EDaII1eJ7nQ9BACQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3768007496", "description": "About the job\nDuration: 6+ months CTH\n\nMust have a valid LinkedIn profile\n\nRequired Skills\n\n 3-6 years of experience (no more than 6 years)\n Start up environment experience\n Python, SQL, and DBT"}, {"title": "Big Data Engineer - W2", "company_name": "Megan Soft Inc", "location": "United States (Remote)", "link": "/jobs/view/3674845652/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fCAxh4N%2F4YZJ5aEIASaeeA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3674845652", "description": "About the job\nTitle: Big Data Engineer - W2\n\nLocation: Dearborn, MI (Remote)\n\nDuration: 12+ Months\n\nDirect Client: Ford Motors\n\nMust have skills: Should be able to handle GCP requirements\n\n\"Translates business requirements to build repeatable, sustainable, efficient, coded processes that can be productionized by Software Engineers and readily modified by other Data Engineer Creates POC processes in Dev/QA and works with Software Engineers to productionize and align best coding and process practices that maximize efficiency, speed, stability, system resources and capabilities Awareness of and compliance with data privacy, security, legal and contractual guidelines Incorporates data quality and privacy checks/alerts to minimize bad data being consumed by end users, models and dashboards, and to protect customer data; revises checks for new data issues Validates data products and pipelines are functioning as expected following system or application upgrades, source changes, etc. Works closely with Architects if required to align systems, tools and applications being utilized with business use case and performance requirements Provides data product support and maintenance Establishes, tracks and monitors KPIs related to specific data products and deliverable Participate in iteration cadence activities (Iteration Planning, task estimation, daily scrum, demos, etc.). Will be a part of on-call team to resolve critical production support tickets during business hours and off hours.\n\nSkills Required\n\nExperience with Big Data Technologies (Hadoop , ETL Tool, Spark. Hive etc..) Someone who understands Cloud and has experience working with Cloud Platforms (preferably GCP) Should be able to handle GCP requirements - ingesting into GCS/ Big Query, working with GCP components Hands-on in Data Flow, Cloud Storage (GCS), Big Query and GitHub. Proficient in Python and SQL.\n\nSkills Preferred\n\nManages ETL: uses programming and tools for data ingestion, configures pipelines, applies transformations and decoding, integrates and fuses data, moves and securely deliver Experience NoSQL databases\n\nAdditional Information\n\nIf the candidate is remote only, please indicate \"\"100% Remote\"\" under candidate's name on resume. Other similar schedule notations can be \"\"Local Candidate\"\" or \"\"Hybrid Candidate.\"\" ***HRA Test Required***\""}, {"title": "Remote Opportunity: Quantexa Data Engineer", "company_name": "SPAR Information Systems LLC", "location": "United States (Remote)", "link": "/jobs/view/3680073191/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ioykVCZSOQ7BK5%2FwRQHOWg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3680073191", "description": "About the job\nHello All, Hope you are doing great!! Please go through the job descriptions and let me know your interest. Role: Quantexa Data engineer  Location: Remote Duration: Long Term Contract Job Description: 1. End to End usage of Quantexa including guiding our client team on how to use from both front end and data ingestion Reason they are asking for Certification 2. Resolve Entity issues with data 3. Knowledge of Scala and Hive which requires coding experience using parquet 4. Advanced SQL knowledge requiring writing SQL queries 5. Experience having worked with GCP data lake 6. Quantexa certification required\n\nThanks & Regards,\n\nSatnam Singh\n\nDirect: 201 623 3660\n\nEmail : Satnam.singh@sparinfosys.com"}, {"title": "Remote Work - Need Staff Data Engineer - USC only", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3746283980/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=CvIuLBNO%2BPMGi4IXVRkP0A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3746283980", "description": "About the job\n100% Remote, client is in NY\n\nLook for strong job history, Python, Machine Learning, SQL, 10/10 comms, etc. \n\nMUST be an Engineer \n\nSummary\n\nWe're looking for an experienced Data Engineer to join our client's growing team of top notched IT professionals. You will work closely across multiple groups including Editorial and Audio, Marketing, Advertising, etc. to achieve their goals and objectives. The Data Engineering team builds tools throughout the stack while sharing knowledge across these departments.\n\nAs the company grows, they're looking for Data Engineer who will help solidify and expand our pipelines and maintain their data warehouse. Their business model is based on performing complex and very detailed analyses of how our products perform with their subscribers. You'll be responsible for working alongside analysts, product, marketing, and other teams to define new data collection components and measurement schemes that support each new product and feature.\n\nHow Can Add Value To Their Mission\n\nCreate and maintain data pipelines to provide insights and drive business decisions\nEstablish data warehousing strategy (ex. Kimball, Data Vault, etc.)\nMaintain data infrastructure on our AWS accounts\nCollaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.\nWrite unit/integration tests, contributes to engineering wiki, and documents work\nImplement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it\n\nWhat You'll Need To Succeed\n\n7+ years of industry experience measuring product performance and user behavior\nExperience working with a variety of data management technologies, including RedShift, Kinesis/Kafka, Glue, Spark, Postgres, Airflow, dbt, and others.\nExperience implementing BI reporting tools such as Looker\nExperience interfacing with engineers, product managers and analysts to understand data needs\nKnowledge of a variety of measurement beacons, SDKs, APIs including Google Analytics, Amplitude, Braze, Stripe, email service providers\nA commitment to building secure, resilient, fault-tolerant architectures, with clear documentation and procedures in place for support\nA focus on accuracy and detail as you build, ensuring data pipelines produce clear, predictable results\nUnderstanding of the typical metrics a subscription and advertising-supported business needs to measure success\nExperience with Client techniques as applied to behavioral segmentation or anomaly identification is a definite plus\nFamiliarity using developer tools that increase productivity and facilitate the development of resilient code (eg. Docker, CircleCI, Serverless) is a plus"}, {"title": "ENTRY LEVEL DATA ENGINEER (REMOTE)", "company_name": "SynergisticIT", "location": "Jacksonville, FL (Remote)", "link": "/jobs/view/3767587909/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=om7L3CnowiC4PLIcpZD70A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3767587909", "description": "About the job\nSYNERGISTICIT wants every candidate to know that the Job Market is Challenging and to stand out, you need to have exceptional skills and technologies and that's where we come in to make sure you get the attention which you need\n\nSynergisticIT understands the complex nature of the job market and how difficult it can be to secure a position, especially for fresh graduates. Therefore, we assist and help tech-savvies to convert their passions into professions. We go above and beyond to keep you working in your niche.\n\nAs we focus on long-term success, we provide complete career development solutions. From job search to upskilling portfolio and interview preparation, we can guide you at every step of your career.\n\nSynergisticIT spares no efforts to connect you with a large network of tech giants, including Google, Apple, PayPal, Dell, Cisco, Client, etc. Presently, we are actively looking for ENTRY LEVEL DATA ENGINEER a driven mindset. Get the right opportunity and gain experience in building web-centric solutions on Java.\n\nWho Should Apply : Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or anyone looking to make their career in IT Industry\n\nWe also assist in filing for STEM extension and H1b and Green card filing.\n\nCandidates who are serious about their future in the IT Industry and have set big goals for themselves.\n\nCandidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. We also offer Skill Enhancement Programs if the candidates are missing skills or experience which our clients need with great outcomes\n\nCandidates can benefit from skill enhancement if they fall into the below categories. If they are qualified with enough skills then no need for skill enhancement\n\nCandidates Who Lack Experience\n\nHave had a break in careers\n\nLack Technical Competency\n\ncandidates who want to get employed and make a career in the Tech Industry \n\nPlease Also Check The Below Links\n\nhttps://www.synergisticit.com/candidate-outcomes/\n\nhttps://www.synergisticit.com/java-track/\n\nhttps://www.synergisticit.com/data-science-track/\n\nhttps://www.synergisticit.com/which-is-the-best-option-for-tech-job-seekers-staffing-companies-consulting-companies-bootcamps-or-synergisticit/\n\nhttps://www.synergisticit.com/contact-us/\n\nIf the skills are not a match candidates can opt for Skill enhancement. Or their resume can be sent out to clients to see if responses are achievable\n\nREQUIRED SKILLS For Java/Software Programmers\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Core Java , javascript , C++ or software programming\nSpring boot, Microservices and REST API's experience\nExcellent written and verbal communication skills\n\nFor data Science/Machine learning\n\nRequired Skills\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Statistics, Python, data visualization tools\nExcellent written and verbal communication skills\n\nPreferred skills: NLP, Text mining, Tableau, Time series analysis\n\nTechnical skills are required by clients for selection even if its Junior or entry level position each additional Technical skill helps a candidate's resume to be picked by clients over other job seekers.\n\nClients hire candidates with the right technical skills which they need and reject candidates who lack the required technical skills.\n\nNo third party candidates or c2c candidates\n\nPlease apply to the posting\n\nNo phone calls please. Shortlisted candidates would be reached out"}, {"title": "Data Engineer - Remote | WFH", "company_name": "Get It Recruit - Information Technology", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3782241984/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=BgBf1kSAAX%2BSuw6inJsIVw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3782241984", "description": "About the job\nJoin an innovative team at a leading company in the engineering and manufacturing industry, where we are committed to leveraging data for continuous improvement and operational excellence. This hybrid position allows you to enjoy the flexibility of remote work on Mondays and Fridays.\n\nWhy Join Us\n\nAt our company, we prioritize your well-being and professional growth. We offer a comprehensive benefits package, including excellent health benefits, a 6% 401(k) match, generous paid vacation and sick time, 12 paid holidays, and more.\n\nRole Overview\n\nAs a Data Engineer, you will play a pivotal role in transforming raw data into actionable insights, driving improvements in engineering and manufacturing processes, enhancing product quality, and optimizing operational efficiency. Collaborate with dynamic cross-functional teams to extract, analyze, and visualize data, influencing strategic decisions through your valuable insights.\n\nKey Responsibilities\n\nData Collection and Integration:\n\nGather, clean, and integrate data from diverse sources, including production databases, sensor data, and external datasets.\n\nData Analysis\n\nConduct exploratory data analysis to identify patterns, trends, and anomalies.\n\nPerform statistical analysis and hypothesis testing to derive meaningful insights.\n\nData Visualization\n\nCreate informative and visually appealing dashboards and reports using tools like Tableau, Power BI, or Python libraries.\n\nPredictive Modeling\n\nDevelop predictive models to forecast manufacturing performance, quality, and operational outcomes.\n\nRoot Cause Analysis\n\nInvestigate production issues and quality concerns by analyzing data to identify root causes and recommend corrective actions.\n\nPerformance Monitoring\n\nEstablish and maintain key performance indicators (KPIs) to track manufacturing and engineering metrics.\n\nGenerate regular reports to communicate performance trends to stakeholders.\n\nCross-functional Collaboration\n\nCollaborate with engineering, manufacturing, and quality assurance teams to understand their data needs and provide analytical support.\n\nContinuous Improvement\n\nSuggest process improvements and optimizations based on data analysis.\n\nData Security And Compliance\n\nEnsure data security and compliance with relevant regulations, such as GDPR or industry-specific standards.\n\nInfrastructure And Development\n\nBuild and maintain the infrastructure for collecting, storing, and processing data.\n\nDesign, develop, and manage data pipelines, ETL processes, and data warehouses.\n\nRequired Experience\n\nProven experience as a Sr. Data Analyst and/or Data Engineer in an engineering or manufacturing environment.\n\nProficiency in data analysis tools and programming languages, such as Python, R, SQL, and Excel.\n\nStrong data visualization skills with tools like Tableau or Power BI.\n\nExcellent knowledge of statistical analysis and machine learning techniques.\n\nAbility to communicate complex findings and insights effectively to both technical and non-technical stakeholders.\n\nStrong problem-solving skills and attention to detail.\n\nFamiliarity with manufacturing processes and quality control is a plus.\n\nExperience with enterprise ERPs (SAP S4HANA, Oracle EBS), CRMs (Salesforce, Hubspot), and IoT.\n\nProficient in working with cloud computing platforms including AWS, GCP, and Azure.\n\nExpertise in configuring and optimizing cloud-based data solutions for scalability, reliability, and performance.\n\nBachelor's Degree in Data Science, Statistics, Engineering, or Computer Science Required.\n\nApply Now and be part of a team shaping the future of data-driven innovation in engineering and manufacturing!\n\nEmployment Type: Full-Time"}, {"title": "Entry Level Software Engineer - Data Backend Engineer (Remote - Canada)", "company_name": "Yelp", "location": "United States (Remote)", "link": "/jobs/view/3752881757/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fankeoxifvhpTzOYvowHYQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3752881757", "description": "About the job\nYelp engineering culture is driven by our values: we\u2019re a cooperative team that values individual authenticity and encourages creative solutions to problems. All new engineers deploy working code their first week, and we strive to broaden individual impact with support from managers, mentors, and teams. At the end of the day, we\u2019re all about helping our users, growing as engineers, and having fun in a collaborative environment.\n Interested in exploring data and finding innovative ways to collect it, curate it, and make it easily accessible for others to use? How about doing that at an enormous scale? Yelp\u2019s datasets contain billions of interactions between users and local businesses around the globe. If working with our vast amount of data sounds exciting, come and join us!\n\nThe Marketplace Data Observability team is responsible for managing, governing, and monitoring all of Yelp\u2019s Ads data to protect our revenue stream and also support offline analysis that aids in making long term company decisions. We build tools that help us to better understand our data quality, and we strive to uncover data issues before they impact our consumers.\n\nAs a Data Backend Engineer on this team, you will be responsible for building elegant and scalable data products that serve critical, up-to-date, structured information to support different types of analytics within Yelp. As a core contributor to our growing data modeling and data warehousing engineering efforts, you will help design and own mission-critical data flow pipelines and datastores to enable decisions including effective A/B testing and company investments.\n This opportunity is fully remote and does not require you to be located in any particular area in Canada. We welcome applicants from throughout Canada. We\u2019d love to have you apply, even if you don\u2019t feel you meet every single requirement in this posting. At Yelp, we\u2019re looking for great people, not just those who simply check off all the boxes.\n  Build systems that can effectively store and crunch terabytes of data.\nDesign and develop data models for efficient data storage, retrieval, and reporting.\nCreate and maintain conceptual, logical, and physical data models using industry-standard modeling tools.\nCollaborate with cross-functional teams, including engineers, data analysts, business analysts, and data scientists, to understand data requirements and translate them into effective data models.\nParticipate in data integration efforts, including ETL processes and data migration.\nStay up-to-date with industry best practices, emerging technologies, and trends related to data warehousing.\nSupport on-call rotations as needed to operate the team.\n  Understanding of high performing and scalable data systems.\nExperience in building and orchestrating ETL pipelines.\nExperience with Data Lake or Data Warehouse landscape.\nA hunger for tracking down root causes and fixing them in systematic ways.\nAbility to communicate effectively to technical and non-technical cohorts alike. \nExposure to some of the following technologies: Python, AWS Redshift, AWS Athena / Apache Presto, Big Data technologies (e.g S3, Hadoop, Hive, Spark, Flink, Kafka etc), NoSQL systems like Cassandra, DBT."}, {"title": "Data Engineer (Associate Consultant), June 2024", "company_name": "UDig", "location": "Nashville, TN (Remote)", "link": "/jobs/view/3716039960/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=XUjo40kU55HnDCqJ%2F3FdPg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3716039960", "description": "About the job\nCan UDig It? \n\nUDig designs, builds, and implements technology solutions that deliver on business objectives. People join our team because they are excited to have an impact on the future of our organization, passionate about delivering meaningful results, and energized by solving complex problems. Oh, and they like working with other driven, smart, and authentic people!\n\nUDig is proud to be recognized as a \"Best Places to Work\" in Virginia and Tennessee. We're committed to fostering a transparent environment where leadership is accessible, professional development opportunities are abundant, and you are empowered to achieve your career goals. You'll collaborate, develop deep relationships with teammates, and accelerate your success while having fun!\n\nIf you're graduating in May 2024 and excited by the idea of impacting organizations by building data solutions to achieve business goals, you should consider joining our growing technology team.\n\nHow Things Go Down\n\nAs a new, entry-level teammate, you'll take part in our \"Breaking Ground\" training program designed to help you quickly get up-to-speed on UDig's delivery approach within a project team. The program covers:\n\nUDig delivery process and methodologies such as gathering requirements and delivering projects using agile\nTechnical guidance and instruction on the best practices from Data leaders at UDig\nDelivering executive and technical presentations\nCoaching to prepare you to for success on your first client project at UDig\n\nA typical day might entail:\n\nProject standups and check-ins\nDesign and development of data pipelines, BI reports, and more\nPlanning and participating in project demos\nContributing to internal UDig projects\nCareer and job growth through UDig-provided training\nCreating long-lasting friendships with teammates through fun activities as well as your projects (including social events, winning a prize for guessing the weight of pumpkin, or sharing experience on cool tech side projects)\n\nUDig believes it is important for Associate Consultants going through our Breaking Ground program to reside in one of our major markets (Richmond, VA or Nashville, TN metro area) for at least the first year of employment.\n\nWant to learn more? Hear directly from our teammates by checking out our 'A Day in the Life' blog series: https://www.udig.com/digging-in/breaking-ground-2023/\n\nSound Interesting? Here's the Technical Piece\n\nBelow are a few boxes you should be able to check if\u202fyou're interested: Don't check every box? Go ahead and apply!\n\n1+ year of experience with relational database management systems such as Microsoft SQL Server, MySQL, or Oracle\nUnderstanding of the full lifecycle of design, development, and implementation of data solutions including architecture design, development, testing\nSome experience in ETL development or reporting is a plus\nAbility to present technical ideas and high-level concepts and solutions to internal and external team members with varying degrees of technical knowledge\nBachelor's Degree in in computer science, information technology, or any science or business discipline\nAbility to take your work seriously but also seriously enjoy a good time with your UDig colleagues\n\nWhat's in It For You? \n\nAt UDig, you will build your career alongside talented and experienced developers while gaining experience and having fun! We provide continuous challenges and professional development opportunities. Additionally, we offer: \n\nCompetitive salary, merit reviews, and career advancement paths\nFlexible, hybrid environment\nIndividual $1500 Training Budget\nRegular team building and social activities (virtual and/or in-person)\nTransparent culture with strong communication and access to leadership\nGreat benefits like\nGenerous Paid Time Off, company holidays, and parental leave\nMultiple Single and Family Health Insurance plans to choose from\nDental & Vision coverage\nShort-Term and Long-Term disability\nOptional accident and critical illness coverage\nMatching 401(k)\nand more!"}, {"title": "Expression of Interest: Data Engineer", "company_name": "Fingerprint for Success (F4S)", "location": "New York, NY (Remote)", "link": "/jobs/view/3787769880/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=l%2FEDpQqUNmCttGdIyTk7Cg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787769880", "description": "About the job\nWe are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.\n\nThe F4S Talent Pool is a pilot project designed to:\n\nHelp job seekers get discovered by our partners based on their anticipated hiring needs.\nProvide optional support and resources for job seekers in their career endeavors.\nHelp individuals understand, and bring out the best in themselves and each other.\n\nThe F4S Talent Pool process:\n\nOnce you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.\n\nAbout Fingerprint For Success (F4S)\n\nBacked by 20+ years of research, F4S\u2019s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high performing teams.\n\nKeep in mind, joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your F4S work style assessment to match the hiring needs of our partners.\n\nYour feedback is a gift! Write to us via:\n\nPowered by JazzHR\n\noWuPJ9lwq9"}, {"title": "Data Engineer Seattle, WA (remote)", "company_name": "Conch Technologies, Inc", "location": "United States (Remote)", "link": "/jobs/view/3765052211/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=SBETI78UMutCHuRp5FnP%2FA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3765052211", "description": "About the job\nHi,\n\nGreetings from Conch Technologies Inc\n\nData Engineering Lead \u2013 With Databricks and Unity Catalog\n\nLocation \u2013 Seattle, WA (remote)\n\nDuration \u2013 12 months\n\nNeed: Design and implement data architecture and strategy, leveraging technologies such as Databricks, Unity Catalog, Privacera, and Collibra.\n\nAs a Data Engineering Lead, you will play a pivotal role in defining and executing technical deliverables while assessing complexities and levels of effort. You'll engage with both vendor and internal teams to resolve technical challenges and ensure seamless collaboration. Your leadership will extend to planning and prioritization with the SCRUM master, facilitating effective coordination between onsite and offshore teams, and driving feature prioritization through collaboration with product managers.\n\nKey Responsibilities\n\n Lead the definition and breakdown of technical deliverables, assessing complexities and levels of effort.\n Collaborate with vendor and internal teams to address technical issues.\n Drive planning and prioritization in conjunction with the SCRUM master.\n Coordinate with onsite and offshore teams for timely project delivery.\n Collaborate with product managers to prioritize features, communicate with stakeholders, and clarify requirements.\n Collect and manage user-reported issues, ensuring prompt resolution and communication.\n Define comprehensive test plans, oversee their execution, and document results.\n\n--\n\nWith Regards,\n\nNagesh G\n\nMobile: 408-381-5645\n\nDesk: 901-313-3066\n\nEmail: nagesh@conchtech.com \n\nWeb: www.conchtech.com"}, {"title": "Remote Work - Need Azure Fabric Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3759325599/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=C3iVjmcR9wjUip1nNjUiCg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3759325599", "description": "About the job\nThe client is placing heavy emphasis on Microsoft and Azure Fabric experience\n\nInformation Management Engineer \u2013 Take the identified information management sources and assist migration to the Client Data Lake, Azure Fabric. The engineer will transform information management (de-dup, clean and enrich).\n\nTake the identified data sources and bring them into our Data Lake, Azure Fabric. Once the data is in, the engineer will need to be able to transform data (de-dup, clean and enrich the data). We are focused on using a medallion data model for data architecture. The tools we are working with are: Delta Parquet files, python, spark, notebooks, T-SQL, Fabric lakehouse/warehouse, Dataflows Gen2, Data Pipelines, Direct Lake, shortcuts, ADF."}, {"title": "Data Engineer", "company_name": "Catalist", "location": "Washington, DC (Remote)", "link": "/jobs/view/3744951279/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ui1Fk6ICmnPNqAlZDZAcwA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3744951279", "description": "About the job\nFor over 17 years, Catalist has been a leader in civic data and data science innovation. Our mission is to provide progressive organizations with the data, software, and services needed to better identify, understand, and communicate with the people they need to engage and mobilize. Our clients include the largest, most influential organizations in the U.S. active in civic engagement, advocacy, and political campaigns.\n\nCatalist is home to a dedicated, creative team of technologists, data scientists, and campaign experts committed to using our talents and technology to nurture a vibrant and growing progressive community.\n\nAs a Data Engineer at Catalist, you will work closely with the Analytics and Technology departments to design, build, support, and maintain various data pipelines and processes with an end goal of providing data and intelligence to the progressive community. This role primarily involves translating data architecture designs into functional processes, code, and systems.\n\nThe ideal candidate will be a highly motivated individual with excellent technical skills, a strong desire to learn new skills, and an interest in progressive politics. Catalist values creativity and problem-solving. Our work is on the cutting edge of data-driven politics, and your support will help Democratic candidates and progressive organizations conduct successful advocacy and electoral campaigns.\n\nThis position reports to the Deputy Chief Data Officer. The Data Engineer is a part of a growing Data team that supports all underlying work at Catalist.\n\nThis position is included in our CWA bargaining unit.\n\nPrinciple Duties & Responsibilities\n\nDevelop scalable, production processes, code, and systems for data ingest, transformation, modeling, and reporting across a variety of platforms\nTranslate mock-ups and designs into functional processes, code, and systems\nCreate architecture designs when needed\nProvide quality assurance and testing on processes, code, systems, and products\nBecome an internal subject matter expert on various datasets and support other Catalist departments/teams on usage of those datasets\nExecute ad-hoc data and database maintenance tasks as requiredProject manage cross-departmental efforts, with direct responsibility for stakeholder engagement, management, and execution of technical elements\n\n\nRequirements\n\nBS or BA in a technical field, or relevant experience\n1-2 years of experience working with SQL databases\nExperience with data cleaning or ETL processes\nExperience with distributed computing systems and/or distributed datastores (particularly the Hadoop ecosystem)\nExperience managing projects\nFamiliarity with Catalist data, progressive politics, voter files, and/or commercial data\nBackground check required\n\n\nPreferred Skills & Abilities\n\nWillingness to be a problem solver and produce results in a fast paced environment\nAbility to be creative and personable, and articulate ideas clearly\nExperience working with SQL databases\nProficiency with Python or another object-oriented programming language (R, Java, Scala, etc\u2026)\nExperience working in cloud environments (AWS, GCP, etc.)\nExperience working in command line environments such as Bash\nExperience with a version control tool such as git or github\n\n\nBenefits\n\nMedical, Dental, Vision, Prescription Drug\n\nCatalist offers Medical, Dental, Vision, and Prescription Drug coverage for eligible staff and their eligible dependents. Catalist\u2019s Medical plan is a comprehensive PPO program including Prescription Drug coverage with 85% of the premium paid by Catalist. Dental and Vision coverage is provided at no cost to employees.\n\nGroup Term Life Insurance and Long-Term & Short-Term Disability Coverage\n\nGroup Term Life Insurance and Long-Term and Short-Term Disability coverage is available for eligible staff. These benefits are provided at no cost to Catalist employees.\n\n401(k) Safe Harbor Plan\n\nA 401(k) Safe Harbor Plan is available to eligible staff with a 3% contribution from Catalist from the date of hire. Employees may contribute pre-tax or post-tax from their salary up to the legal limits set forth by the IRS.\n\nMedical and Dependent Care Flexible Spending Accounts (FSAs)\n\nCatalist offers an FSA Program that gives eligible staff the ability to pay out-of-pocket medical/dental/vision/child care expenses from pre-tax earnings.\n\nTransit Benefits\n\nCatalist also makes available a Transit benefit FSA program to eligible employees using pre-tax contributions with a company match.\n\nProfessional Development and Remote Work Expenses\n\nEligible employees may be reimbursed up to $750 each year for professional development / education and remote work expenses.\n\nStudent Loan PayDown or SaveUp\n\nCatalist offers a Student Loan PayDown and College SaveUp benefit for eligible staff.\n\nVacation, Personal Leave, Sick Leave Benefits\n\nCatalist offers generous vacation benefits to all eligible staff. Eligible employees also receive:\n\n 14 Paid Holidays\n Personal Days\n Sick Leave\n Parental Leave\n\n\nHybrid Office/Remote Work\n\nCertain positions at Catalist are eligible for Office/Remote Hybrid or full Remote status."}, {"title": "Remote Entry Level Data Analyst/Engineer", "company_name": "SynergisticIT", "location": "White Plains, NY (Remote)", "link": "/jobs/view/3774171423/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=q54sk1%2FRww0z%2BC%2BE8e0jQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774171423", "description": "About the job\nAt SynergisticIT, we're all about making connections. Whatever IT goals you have, our software programmers can help achieve those. Our Software development teams can take up turnkey projects and execute them in an effective and efficient manner. If you are looking to source talent our recruiters will find the ideal IT talent for your company. What's the secret to our success? Well, it all starts with taking quality time to listen to each client's specific needs. After we have a thorough grasp of your IT goals, we can better customize our Developments as per your specific needs. We can also tailor make recruiting programs to exceed your expectations. Since our founding in 2010, SynergisticIT's strategies have earned the company an enviable position in the software development, IT staffing and IT skill enhancement fields. SynergisticIT continues to work with hundreds of satisfied American clients with our software programmers working on our projects and after gaining hands on experience on cutting edge technologies moving to contribute their skills to great clients like Apple, Google, Client, Ebay, Paypal, Kroger, the Walt Disney Company and hundreds more. If you are tired of working with inefficient programmers who take a lot of time to ramp up we want you to try us. Our software programmers can hit the ground running and get you the maximum return on your investment. You have already tried the rest its time you tried the best. SynergisticIT - Home of the Best Data Scientists and Software Programmers in the Bay Area.\n\n Why Us ? \n\nSynergisticIT has a proven track record of successfully skill enhancement and staffing IT employees for some of the world's most iconic brands. Our team takes the time to fully understand every client's needs so we could best meet your IT staffing requirements. The knowledgeable staff at SynergisticIT is always more than happy to work with clients to ensure they reach their software development goals. Besides staffing, SynergisticIT is also committed to helping young IT professionals advance their career with a robust upskill program . Everyone who goes through SynergisticIT's program learns all the skills necessary to succeed in many IT fields ranging from Java to Machine Learning. Additionally, everyone trained at SynergisticIT has been through extensive mock and technical interview screenings to bolster their career prospects. Last, but certainly not least, SynergisticIT takes great care to respect the privacy considerations for every client. All companies who work with SynergisticIT can rest assured their confidential data is protected using the most up-to-date encryption technologies. SynergisticIT also complies with all the latest NDA agreements.\n\n REQUIRED SKILLS For Java /Software Programmers \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Time series analysis \n\n We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2022 and at Gartner Data Analytics Summit (Florida)-2023 \n\nOracle CloudWorld Event (OCW) Las Vegas 2022 | SynergisticIT - YouTube\n\n https://www.youtube.com/watch?v=OAFOhcGy9Z8 \n\n https://www.youtube.com/watch?v=EmO7NrWHkLM \n\n https://www.youtube.com/watch?v=NVBU9RYZ6UI \n\n https://www.youtube.com/watch?v=Yy74yvjatVg \n\nSynergisticIT at Gartner Data and Analytics Summit 2023 - YouTube\n\nFor preparing for interviews please visit  https://www.synergisticit.com/interview-questions/ \n\n We are looking for the right matching candidates for our clients \n\n Please apply via the job posting \n\n REQUIRED SKILLS For Java /Full Stack/Software Programmer \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning Positions \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Tensorflow \n\n If you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team. \n\n No phone calls please.  Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates"}, {"title": "Data Engineer", "company_name": "Anika Systems", "location": "Leesburg, VA (Remote)", "link": "/jobs/view/3787766893/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=mApL9JnN%2B%2Fr2%2BYUPlxRYOA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787766893", "description": "About the job\nAnika Systems is a fast growing, woman-owned small business that specializes in providing innovative IT solutions for federal government agencies. Our expertise lies in accelerating delivery in Data and Analytics, Intelligent Automation, Application Development, and IT Modernization. We are currently expanding our Federal team and are seeking a passionate and talented Data Engineer. This opportunity is 100% remote.\n\nMust be a U.S. Citizen with the ability to obtain and maintain a government suitability clearance\n\nResponsibilities\n\nDesigns, develops, builds, analyzes, evaluates, and installs database management systems to include database modeling and design, relational database architecture, metadata, and repository creation and configuration management.\nDefines and oversees database organizations, standards, controls, procedures, and documentation. Provides technical consulting in the definition, design, and creation of a database environment.\nAdvises applications development staff and users on data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nDesigns and implements databases with respect to access methods, access time, batch processes, device allocation, validation checks, organization, protection and security, documentation, and statistical methods.\nUses data mapping, data mining, and data transformational analysis tools to design and develop databases.\nDetermines data storage and optimum storage requirements.\nPrepares system requirements, source analysis, and process analyses and designs throughout the database implementation.\n\nRequired Skills And Experience\n\nBA/BS and 1 year of relevant experience\nExperience with DataBricks, SQL, Python\nApplications development experience with data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nExcellent communication skills (both oral and written) with process-oriented organizational skills to ensure project success\n\nDesired Skills And Experience\n\nDemonstrated successful development experience with the full life cycle of government database management systems including database modeling and design, relational database architecture, metadata and repository creation, and configuration management.\n\nPowered by JazzHR\n\nTlKWIorucJ"}, {"title": "Remote Work - Need Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3774712978/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=r3Bp%2F0qcOD2OpC5jpbUOcA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774712978", "description": "About the job\nTitle : Data Engineer\n\nLocation : Remote\n\nDuration : 6+ months to start (long term contract (work order are extended 2x year))\n\nPlease Fill out the skill matrix: Technology Some Awareness (1) Novice (2) Intermediate (3) Advanced (4) Expert (5) SQL Tableau Desktop /Creator Tableau API Integration AWS - S3 Glue AWS - Athena Redshift AWS - Lambda AWS - SNS/SQS msExcel\n\nProfile: Data Analyst will work closely with Architects and leads in the Interflow team, key business stakeholders including program staff to support the organizations effort in the areas of data Analysis and information management. Role should have strong experience in writing SQL queries to extract data for reports. Candidate should have experience working with large set of data. As these reports are very critical for the company and we are looking for an expert who is capable of handling, consuming and extracting large sets of data for analysis and reports.\n\nMust Haves\n\nAWS - Athena\nAWS - S3\nGlue\nhands-on development of reporting applications\nMS Excel\nRedShift\nSQL\nTableau Desktop /Creator\n\nNice\n\nAWS - Lambda\nAWS - SNS/SQS\nTableau API Integration\n\nSkills\n\nBA/BS required (major in an analytical field desired) \nA minimum 10+ years work-related experience. \nExperience in hands-on development of reporting applications for the Web in a professional environment \nThe Ethos of continuous improvement and interest in learning new things.\nStrong analytical thinking and structured problem-solving ability\nAbility to handle multiple projects and assignments simultaneously and effectively in a cross-functional team environment. \nVersed on the agile methodology and best practices. \nExcellent interpersonal and collaboration skill with the ability to work with a diverse set of colleagues, across functions, from different organizations, disciplines, etc. \nSelf-starter, ability to set priorities, work independently and attain goals"}, {"title": "Data Engineer", "company_name": "Team Remotely Inc", "location": "Houston, TX (Remote)", "link": "/jobs/view/3786249829/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=78lY5Jo%2FfyRPC6TQYCdvww%3D%3D&trk=flagship3_search_srp_jobs", "id": "3786249829", "description": "About the job\nThis is a remote position.\n\n Data Engineer (US/Canada Residents Only, 1 year experience, remote)\n\nTeam Remotely Inc. is a staffing and recruitment agency that offers a comprehensive solution for talent acquisition, including sourcing, vetting, pay rolling, and managing talent. Whether you need contract staffing, direct hire, direct sourcing, talent pools, or diversity initiatives, our model can support your hiring strategy.\n\n Hiring Type: Full-Time\n\n Base Salary: $55K-$70K Per Annum.\n\nResponsibilities\n\nDesigning user interface changes for web-based DB applications. \nReviewing application requirements and interface designs. \nDeveloping and implementing highly responsive user interface components using react concepts. \nWriting application interface codes using JavaScript following react.js workflows. \nTroubleshooting interface software and debugging application codes. \nDeveloping and implementing front-end architecture to support user interface concepts. \nMonitoring and improving front-end performance. \n\nQualifications And Experience\n\nBachelor\u2019s degree in computer science, information technology, or a similar field. \nPrevious experience working as a react.js developer. \nIn-depth knowledge of JavaScript, CSS, HTML, jQuery, and front-end languages. \nKnowledge of REACT tools including React.js, Webpack, Enzyme, Redux, and Flux. \nExperience with user interface design. \nKnowledge of performance testing frameworks, including Mocha and Jest. \nExperience with browser-based debugging and performance testing software. \nTroubleshooting skills. \nProject management skills. \nProblem-solving skills. \nVerbal communication skills. \n\n Why work with Team Remotely?\n\nTeam Remotely Inc. is a staffing platform offering a seamless experience for employers and candidates. Employers can post job openings and specify their requirements, while candidates can create profiles and upload resumes.\n\nThe team of Team Remotely continuously learns and adapts based on previous successful placements, constantly improving its matching capabilities. This ensures that the recommendations provided by Team Remotely are tailored and accurate, increasing the likelihood of a successful match between employers and candidates. By providing intelligent and data-driven solutions, they strive to enhance the efficiency and effectiveness of the hiring process, ultimately helping companies find the best talent and individuals find their dream jobs."}, {"title": "Data Engineer", "company_name": "ZAG Technical Services", "location": "United States (Remote)", "link": "/jobs/view/3771403464/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=Hf0VefrKXBFIL5MtHmY0Pw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3771403464", "description": "About the job\nWe are looking for a tech savvy Data Engineer to contribute to the building, operating, and scaling of next generation data platforms and tools that will power data-driven analytics capabilities throughout the ZAG products and services portfolio, spanning areas such as business intelligence, reporting, and data analytics.\n\nThe ideal team member is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. Must be comfortable supporting the data needs of multiple clients and teams, systems, and products within the modernization of ecosystems. The right-fit person will be excited by the prospect of optimizing or even re-designing data processes to support next generation of advanced analytics capabilities, which in-turn translates to implementing strategic enhancements to maintain competitive advantage in the industry.\n\nThe successful team member will leverage their proficiency to\u2026\n\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\nBuild the analytics processes required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Data Management and cloud based \u2018big data\u2019 technologies.\nBuild analytics tools that utilize technologies to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.\nCollaborate with client teams to identify and solution regional and local requirements within the context of the standard global business model for enterprise analytics.\nParticipates in meetings, discussions, strategy sessions where changes, improvements, and enhancements are proposed, evaluated, and approved. \n\n\nThe exceptional team member will have or demonstrate progressive experiences in\u2026 \n\nBachelor\u2019s degree or higher in Computer Science or related field\nExperience working with BI and data warehousing tools\nExperience working with enterprise data, where security is paramount and data governance is critical\nAbility to work independently and in a team environment, and effectively engage all levels of the organization\nUnderstanding of distributed systems driving large-scale data processing and analytics with a successful history of manipulating, processing, and extracting value from large, disconnected datasets.\nAbility to communicate effectively (listening, presenting, and questioning)\nStrong organizational, written, and communication skills\n\n\nAbout ZAG\n\nZAG Technical Services empowers our clients\u2019 success through intelligent business solutions that increase employee productivity, network security and team collaboration. We aim to ensure that our client\u2019s IT investments drive ROI, provide a competitive advantage and help businesses grow. We do this by living our core set of values: Integrity Foremost, Client-Centered, Accountable Always, Teamwork Throughout, and Exceptional is the Goal.\n\nWhat ZAG Has To Offer\n\nCompetitive compensation package \nBenefits including medical, dental, vision, 401K and life insurance \n\n\nZAG Technical Services is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all of our employees.\n\nZAG Technical Services is an E-Verify participant and will provide the Social Security Administration (SSA) and, if necessary, the Department of Homeland Security (DHS), with information from each new employee\u2019s Form I-9 to confirm work authorization."}, {"title": "Junior Data Engineer", "company_name": "HireMeFast LLC - Career Accelerator - Land A Job", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3786249797/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D&trk=flagship3_search_srp_jobs&lipi=urn%3Ali%3Apage%3Ad_flagship3_search_srp_jobs%3BQExogrHBQoSF9zZm%2B9kufw%3D%3D&lici=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D", "id": "3786249797", "description": "About the job\nThis is a remote position.\n\n Job Title: Junior Data Engineer\n\n Employment Type: Full-Time\n\n Salary: $64,000 - $76,000 per annum\n\n Experience Required:  Minimum 1 year of project experience\n\nAbout us: HireMeFast is a leading staffing and recruitment agency specializing in connecting businesses with top-tier talent across various industries. Our mission is to bridge the gap between exceptional candidates and organizations needing their skills, expertise, and unique qualities. Our team of experienced and dedicated recruitment specialists utilizes innovative sourcing strategies, and a vast network to identify and attract top talent. We conduct comprehensive procedures to ensure that only the most qualified candidates are presented to our clients.\n\nPosition Summary\n\nJoin the fast-paced, innovative, and collaborative environment focused on providing an AIOps platform that enhances the intelligence of the CVS Health infrastructure. Work closely with subject matter experts and colleagues to build and scale out machine learning and AI solutions that will detect, predict, and recommend solutions to correct issues before system impact and enhance efficiency, reliability, and performance of CVS Health\u2019s IT operations.\n\nKey Responsibilities include:\n\nData pipeline development: Designed, implemented, and managed data pipelines for extracting, transforming, and loading data from various sources into data lakes for processing, analytics, and correlation. \nData modeling: Create and maintain data models ensuring data quality, scalability, and efficiency \nDevelop and automate processes to clean, transform, and prepare data for analytics, ensuring data accuracy and consistency \nData Integration: Integrate data from disparate sources, both structured and unstructured to provide a unified view of key infrastructure platform and application data \nUtilize big data technologies such as Kafka to process and analyze large volumes of data efficiently \nImplement data security measures to protect sensitive information and ensure compliance with data and privacy regulation \nCreate/maintain documentation for data processes, data flows, and system configurations \nPerformance Optimization- Monitor and optimize data pipelines and systems for performance, scalability and cost-effectiveness \n\nCharacteristics of this role:\n\nTeam Player: Willing to teach, share knowledge, and work with others to make the team successful. \nCommunication: Exceptional verbal, written, organizational, presentation, and communication skills. \nCreativity: Ability to take written and verbal requirements and come up with other innovative ideas. \nAttention to detail: Systematically and accurately research future solutions and current problems. \nStrong work ethic: The innate drive to do work extremely well. \nPassion: A drive to deliver better products and services than expected to customers. \n\nRequired Qualifications\n\n2+ years of programming experience in languages such as Python, Java, SQL \n2+ years of experience with ETL tools and database management (relational, non-relational) \n2+ years of experience in data modeling techniques and tools to design efficient scalable data structures \nSkills in data quality assessment, data cleansing, and data validation \n\nQualifications\n\nKnowledge of big data technologies and cloud platforms \nExperience with technologies like PySpark, Databricks, and Azure Synapse. \n\nEducation\n\nBachelor\u2019s degree in Computer Science, Information Technology, or related field, or equivalent working experience\n\nWhy HireMeFast LLC?\n\nAt HireMeFast, we understand that finding the right individuals to join your team is crucial for success and growth of your organization. We are committed to streamlining the hiring process for our clients, ensuring they have access to a diverse pool of highly qualified candidates who are a perfect fit for their specific needs."}, {"title": "Data Engineer  - Need candidates from Texas ( REMOTE )", "company_name": "Pulivarthi Group (PG)", "location": "United States (Remote)", "link": "/jobs/view/3770685212/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=9gdMwuif%2BpB%2BCQYvQi3imQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3770685212", "description": "About the job\nFollow us on Linkedin: https://www.linkedin.com/company/pulivarthigroup/\n\nPulivarthi Group LLC is a Global Staffing & IT Technology Solutions company, with our prime focus of providing world class solutions to our customers with the right talent. We combine the expertise of our team and the culture of your company to help you with the solution that is affordable and innovative using high quality standards and technologies.\n\nWe\u2019ve served some of the largest healthcare, financial services, and government entities in the U.S.\n\nData Engineer - FT only - Azure SQL with data lakes experience\n\nData Engineer -\n\nthe resource requirement for this Data Engineer role needed more clarification.\n\nOur customer is requesting a resource who knows Azure CI/CD (Continuous Integration / Continuous Deployment) to build and integrate their DevOps pipeline.\n\nOnce the DevOps pipelines are mapped, they need the Data Engineer to connect to all the Data Sources, primarily their Data Lake.\n\nThe Customer (based in Houston) is interested in a resource in a time zone not too far removed from the Central Time Zone to allow real time collaboration."}, {"title": "Data Engineer", "company_name": "Otter Products", "location": "Fort Collins, CO (Remote)", "link": "/jobs/view/3766728302/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=77U3U7CAi1PxgDmpI3%2BiNQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3766728302", "description": "About the job\nOverview \n\nOtter Products is currently recruiting for a Data Engineer to join our Data team! You can be based in our Fort Collins, CO office with flexibility to work a portion of your time remotely, or 100% Remote in the U.S.\n\nThe Data Engineer applies professional experience, concepts, and company objectives toward building and operationalizing the minimally inclusive data necessary for the enterprise data initiatives following industry standard practices and tools. The bulk of the Data Engineer's work would be in designing, managing and optimizing data pipelines, then moving these data pipelines effectively into production for key data and analytics consumers like business/data analysts, data scientists or other stakeholder that needs curated data for data and analytics use cases across the enterprise. The Data Engineer also ensures compliance with data governance and data security requirements while creating, improving and operationalizing these integrated and reusable data pipelines to enable faster data access, integrated data reuse and vastly improved time-to-solution for data and analytics initiatives. Additionally, the Data Engineer will be expected to collaborate with other data team members and data consumers working on the models and algorithms developed by them toward optimization for data quality, security, and governance to put them into production leading to potentially large productivity gains.\n\n About Otter Products \n\nOtter Products, we grow to give. From our founder's garage in 1998 to the global technology leader we are today, Otter Products continues to drive growth through innovation.\n\nThrough our industry-leading brands - OtterBox, Liviri and OtterCares - we provide our partners the number-one selling and most trusted products in our categories. Our philanthropic spirit is the foundation on which we foster our partner relationships, allowing us to grow and to give - together.\n\nBy way of our charitable arm, the OtterCares Foundation, we support our communities and invest in our future through education that inspires kids to change the world.\n\nAnd even as our global community of Otters continues to grow, our founder's core values are still at the heart of everything we do. We measure our success by our ability to give back to our communities and strengthen opportunities for all.\n\nFor more information visit otterproducts.com\n\n Responsibilities \n\nComplete analysis, design and development of BI solutions using Azure SQL Server\nFamiliarity with Data Warehouse concepts\nExperience coding the extraction, transformation, and loading (ETL) processes\nDatabase development primarily in SSIS, Data Factory and SQL\nPartner with the business to determine end user database/reporting needs and requirements\nGenerate ad hoc reports using Power BI or SSRS\nCollaborate with other developers to create and implement best approach solution(s)\nTroubleshoot Azure tools, systems, and software\nReview queries for performance issues, making changes as needed\nCollaborate with team to performance-tune Azure application as necessary\nAssist with the analysis and extraction of relevant information from large amounts of historical business data to feed data science initiatives\nParticipate and support the design and documentation of processes for large scale data analyses, model development, model validation and model implementation\nSupport and maintain a positive safety culture by following all safety policies and procedures and actively contributing to a safe working environment\nOther duties as required\n\n Qualifications \n\nBachelor's degree required. Degree in Computer Science or Mathematics preferred\nMinimum of four years of experience in an IT or analytical role required. Experience in database development, report writing and/or statistics preferred.\n\n EEO \n\nOtter Products, LLC is an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, age, marital status, pregnancy, sex, sexual orientation, gender, gender identity or expression, national origin, disability, veteran status, or any other characteristic or status protected by law.\n\n For US Based Roles Only - Compensation Range Minimum \n\nUSD $90,000.00/Yr.\n\n For US Based Roles Only - Compensation Range Maximum \n\nUSD $120,000.00/Yr.\n\n Additional Total Rewards \n\nProfit Sharing Program Eligible, Benefits Eligible - Full Time- check out otterproducts.com/careers/why for more info"}, {"title": "Software Engineer, Distributed Systems", "company_name": "Hightouch", "location": "United States (Remote)", "link": "/jobs/view/3628572539/?eBP=CwEAAAGMd4QO8APODyE_-mMOPhYyQGg5ZjVbPwq_3V7DK0B2bLAf10pcedM9S2T71X5i4TiJuEwOzWBbSiTKbLuk-VD-_Y-dN5uyf8YpYkHYqEYFxySMS9zr2hRjNrxX9gXbxssh35TFK9YTNV2WeH2Ncb675JgFbJTgNWSZySFC6rrdJGmdSDTr2q5d3VoxORz3GjsDbzjeyZQCvQ07RqYw7oujWSgslycs2LMCY_-N0NEj6e0PvbaYKiZ_5IW34bGVSYLXpHwqa8oOcNttYZ90UA3E_7sDSFzABdIVlfrp5zzERrIYy5sh2MIInxHdDoKgQw8VmOn5uM4S0tXUl-i7LnN0_W0NfPBmLiVDNuveIOm4MfDNPxfe5QKPk6FUAVpvgbcoJn9yRyE&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=rph%2FT6Q6Vkp6YoggJf9cJQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3628572539", "description": "About the job\nAbout Hightouch\n\nHightouch\u2019s mission is to empower everyone to take action on their data. Through our Reverse ETL platform, business and data users can seamlessly sync data from where it resides, such as warehouses and databases, to where it is needed, including operational systems and SaaS tools. Traditionally, acting on data has required engineering time and bandwidth, and left most business users stuck with charts and reports that are unable to take automated action on their data. With Hightouch, every business user, without writing any code, can activate data to streamline critical processes, improve marketing performance, and scale operations.\n\nOur team operates with a focus on making a meaningful impact for our customers. We believe in approaching challenges with a first principles thinking mindset, moving quickly and embracing our value of efficient execution, and treating each other with compassion and kindness. We look for team members that are strong communicators, have a growth mindset, and are motivated and persistent in achieving our goals.\n\nHundreds of companies use Hightouch, including Spotify, Ramp, Retool, NBA, Plaid, and Betterment. We\u2019re based in San Francisco, are remote-friendly, and backed by leading investors such as Amplify Partners, ICONIQ Growth, Bain Capital Ventures, Y-Combinator, and Afore Capital.\n\nAbout The Role\n\nWe are looking for a distributed systems engineer to work on the systems that power the syncing engine our customers and other engineering teams rely on. This presents an exciting challenge where you can apply your expertise in distributed systems, performance optimization, and troubleshooting to push the boundaries of what is possible and meaningful improve our ability to move massive amounts of customer data. This role also provides a unique opportunity to work on a multi-cloud and multi-region infrastructure that supports a global customer base.\n\nWe believe in enabling our engineers to do their best work for our customers by giving them extremely high levels of ownership and autonomy. This comes in different forms: you will own and deliver projects from start to finish, you will work directly with customers to solve their hardest scaling problems, and you will have a lot of influence over what we work on as a team and company.\n\nSome of the problems we\u2019ll be working on include:\n\nSync Speed: Customers want to sync a lot of data to important destinations like Facebook and Snapchat, which requires us to analyze every part of our syncing process and find where we can optimize to sync data more quickly\nStreaming Syncing: We currently sync data from sources in batch only, but envision a world where we also do syncs in real-time and support streaming sources like webhooks and queues\nScalability and Reliability: As part of our rapid growth, we\u2019re always evaluating current future bottlenecks in scaling and reliability, and architecting for the next order of magnitude of growth\nPersonalization API: Our Personalization API product provides a low latency caching layer on top of data warehouses that enable customers to use their data warehouse for real-time personalization use cases with \nMulti-Region and Multi-Cloud: Supporting our multi-region and multi-cloud backend, including extending it to launch Hightouch on in new regions to support data residency requirements of our global customer base\n\nWe are looking for talented, intellectually curious, and motivated individuals who are interested in tackling the problems above. This is a senior role, but we focus on impact and potential for growth more than years of experience. The salary range for this position is $170,000 - $240,000 USD per year, which is location independent in accordance with our remote-first policy.\n\nAbout You\n\nYou are an engineer with a passion for solving hard technical problems that generate real value for customers. You\u2019re motivated by high ownership and are comfortable in a fast-paced, startup environment.\n\nYou have experience and comfort with distributed systems and high-scale systems or the ability and desire to build this experience quickly. This experience can take different forms:\n\nYou\u2019ve built syncing engines that process and move terabytes of data per day in a scalable way\nYou\u2019ve created event collection or data streaming services that power company critical systems\nYou\u2019ve owned systems that have undergone orders of magnitude of growth and helped them scale accordingly\nYou have relevant skills and are a fast learner who is excited to take on the challenges we face\n\nInterview Process\n\nOur goal with the interview process is to balance speed with giving both parties opportunities to assess whether there is a strong mutual fit. We will ask you questions, but we want you to ask us questions! Our technical interviews focus on how you design systems because we believe this is the best way for us to see how you work and for you to see how we collaborate. We don\u2019t ask you to write code to solve technical brainteasers that don\u2019t appear in your day to day job.\n\nRecruiter screen [30m]: Introductory call with our recruiting team to get to know each other and see if the role could be a good mutual fit.\nSystems design screen [45m]: Designing a data processing feature end-to-end.\nHiring manager interview [30m]: Chat with hiring manager about past experiences and future operating preferences to assess fit on company values and operating principles.\nSystems design interview [90m]: Work with the interviewer to architect a system at a conceptual level. The problem will be at a pretty high level - and have both product and customer requirements as well as technical."}, {"title": "Entry-Level Data Scientist Engineer", "company_name": "Phoenix Recruitment LLC", "location": "Texas City, TX (Remote)", "link": "/jobs/view/3788727625/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=8iwtytHsCu4wMwJ4fj3pxw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3788727625", "description": "About the job\nThis is a remote position.\n\nTitle - Entry-Level Data Scientist Engineer, 1 year of project experience\n\nEmployment Type: Full-time\n\nBase Salary: $60K-$70K\n\nPhoenix Recruitment offers a variety of recruiting services to assist both employers and employees. They specialize in marketing open positions, recruiting, and helping employers to find qualified candidates across various industries. Phoenix Recruitment has expertise in streamlining the hiring process. They can help ensure that the process is efficient, well organized, and compliant with relevant regulations.\n\nSkills and Abilities:\n\nStrong knowledge of R or Python for data analysis and modeling. \nProficiency in statistical programs such as R, SAS, MATLAB, or Python. \nFamiliarity with spreadsheets (VBA) and database applications (Access, Oracle, SQL, or equivalent technology). \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \nAbility to learn new methods quickly and work under deadlines. \nExcellent teamwork and communication skills. \nStrong analytical and problem-solving abilities. \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \n\nPreferred:\n\nKnowledge of actuarial concepts and life, health, and/or annuity products. \nExperience with statistical modeling techniques such as GLM, Decision Trees, Time Series, Regression, etc. \nFamiliarity with Microsoft DeployR. \nExposure to insurance risk analysis. \nBasic experience in computational finance, econometrics, statistics, and math. \nKnowledge of SQL and VBA. \nFamiliarity with R or Python for predictive modeling \n\nWhy Phoenix Recruitment LLC?\n\nPhoenix Recruitment often has an extensive network of employers and candidates. This network allows them to tap into a pool of qualified candidates and connect them with suitable job opportunities. They can also leverage their connections to help employers find the right talent efficiently. Outsourcing the recruitment process to a specialized agency can save your time and resources, avoid delays, reduce administrative burdens, and increase the chances of finding the right fit for your organization."}, {"title": "Data Engineer - Remote Work", "company_name": "BairesDev", "location": "United States (Remote)", "link": "/jobs/view/3783162297/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=OtoCrAAHZFomVup04fy35g%3D%3D&trk=flagship3_search_srp_jobs", "id": "3783162297", "description": "About the job\nWho We are\n\nBairesDev is proud to be the fastest-growing company in America. With people in five continents and world-class clients, we are only as strong as the multicultural teams at the heart of our business. To consistently deliver the highest quality solutions to our clients, we only hire the Top 1% of the best talents and nurture their professional growth on exciting projects.\n\nWe are looking for a Data Engineer to join our Development team and participate in different projects made up of multicultural teams distributed throughout the world.\n\nWhat You Will Do\n\n Interview other Data Scientists and Data engineers to vet their technical abilities.\n Input data from those interviews into the product and help develop a predictive, fair, and enjoyable solution.\n Work together with developers, tech leads, and solution architects to build applications.\n Improve existing structures by adding new functionalities or proposing technological updates so that as a result, the impact of your contribution is significant in the core of each business.\n Improve the user experience on scalable and high availability platforms, contributing to the key differential of each business.\n\nHere\u2019s What We Are Looking For\n\n 5+ years of experience working as a developer.\n 3+ years of experience working as a Data Scientist / Data Engineer.\n Proficient with analysis, troubleshooting, and problem-solving.\n Hands-on experience with managing data loads and data quality.\n Advanced English level.\n\nHow we do make your work (and your life) easier:\n\n 100% remote work.\n Hardware setup for you to work from home.\n Flexible hours - make your schedule.\n Paid parental leave, vacation & holidays.\n Diverse and multicultural work environment.\n An innovative environment with the structure and resources of a leading multinational.\n Excellent compensation \u2014 well above the market average.\n Here you can grow at the speed of your learning curve.\n\nOur people work remotely but with a consistent and robust culture that promotes diversity and teamwork. To continue being the leading software development company in Latin America, we want to ensure that every BairesDev member gets the best growth and professional development opportunities in a diverse, welcoming, and innovative environment.\n\nEvery BairesDev team member brings something unique to our company.\n\nWe want to hear your story. Apply now!"}, {"title": "Data Engineer (Remote)", "company_name": "Cascade Debt", "location": "United States (Remote)", "link": "/jobs/view/3755336571/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=pmUtn1j2zyPr25CNPh2RNw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3755336571", "description": "About the job\nDo you want to work in a small team where you can make a real difference in a company? Would you embrace the challenge of building a high impact platform that is used globally? Then we want to speak with you!\n\nAbout Cascade\n\nCascade is a fintech startup backed by Canadian and US investors that empowers companies to grow by democratizing access to institutional debt. We are building tools that modernize how companies raise and manage debt, lowering the barriers to entry to the $7 trillion specialty finance and alternative credit market for companies around the world. Founded just last year, we are gearing up for our next phase of technical development and are seeking a talented Data Engineer to lead the way.\n\nAbout The Role\n\nAs a Data Engineer you\u2019ll be an early member of a growing team building a pioneering platform in the debt infrastructure space. You will design, build, and launch efficient, scalable, and reliable data pipelines to move and transform data .\n\nWhat You\u2019ll Do\n\n Building databases, data lakes and data ingestion pipelines to integrate customer databases and datasets to our systems \n Ingesting financial datasets from external customers, then u pdating and maintaining accurate and complete data mappings to ensure that our product s are displaying high quality data \n M onitoring and alerting across our data pipelines in order to make sure that our data ingests are reliable and correct \n Translat ing business goals and stakeholder requirements into new data flows and deliver analytical insights ensuring that the data flows create real business value continuously \n\nSkills And Experience\n\n 2 + years experience in a data engineering or data analyst role \n E xperience with relational SQL and NoSQL databases, including PostgreSQL , MySQL, MariaDB, MongoDB, Bigtable, etc. \n E xperience working with ETL technologies , such as Databricks, Fivetran , or dbt \n Proficiency in writing SQL queries and knowledge of analytical data warehouses such as RedShift, BigQuery , and Snowflake \n Some experience with CI/CD automation such as GitHub Actions \n Developing APIs and integrating with 3rd party APIs \n Bonus: experience in fintech / SaaS / credit analysis \n\nPeople Who Thrive At Cascade Are\n\n Self-starters, who take the initiative to tackle challenges in a remote work environment \n Good communicators, who can operate without ego to discuss, learn, grow, and help others do the same \n Passionate about creating great digital experiences for users \n Problem solvers who are not satisfied with the status quo \n\nBenefits\n\nRemote: we are a remote-first company and are very flexible on hours as long as things get done \nHome-office: there\u2019s a $1000 USD home office allowance to set yourself up \nEquity: we expect you to have an owner-mentality, and have the equity plan to match \nBenefits: health, dental, vision, and more \nPerks: we offer free lunches weekly and off-site trips \nJob satisfaction: we offer autonomy, ample opportunities for mastery, and an opportunity to make a difference for companies around the world"}, {"title": "Data Engineer", "company_name": "ISC (Integrated Specialty Coverages, LLC)", "location": "Carlsbad, CA (Remote)", "link": "/jobs/view/3778293528/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ew7%2FfEuXSujs5IGabdqUyw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3778293528", "description": "About the job\nAbout Integrated Specialty Coverages\n\nIntegrated Specialty Coverages LLC (ISC) is a growth stage technology and data-driven insurance company leading innovation in the market.\n\nBacked by one of the leading private equity firms, KKR, and led by a forward-thinking management team, ISC is combining the worlds of insurance and technology to create an Insurtech powerhouse. As a leading online distributor of insurance products for a range of industries and \"Main Street USA\", we are looking for the right people to help us in our mission of achieving exponential growth. We strive to be the number one place to go for brokers and agents to source insurance. To accomplish this, we're building a digitally focused team that deeply understands the intersection between user experience, data, and AI/ML to optimize the way we engage with our customers and partners.\n\nJob Summary\n\nThe Data Engineer role will be the technical liaison between multiple groups including a data science team, the engineering team, product management, and business stakeholders. You do not need any insurance knowledge prior, however, you must quickly dive deep into the insurance world and ask questions to become a subject matter expert. You will be responsible for building a data platform to facilitate the data science team. You must be a self-starter that can build out features such as a data pipeline from scratch. There will be support from both engineering and data science for any buildout.\n\nJob Responsibilities\n\nCreate and maintain optimal data pipeline architecture. \nAssemble large, complex data sets that meet functional / non-functional business requirements. \nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. \nBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using AWS technologies, SQL, and Python. \nWork with stakeholders including the Executive, Product, Data Science, and Engineering teams to assist with data-related technical issues and support their data infrastructure needs. \nWork with data science and analytics teams to strive for greater functionality in our data systems. \n\nMinimum Qualifications:\n\nAdvanced working SQL knowledge and experience working with relational databases, strong query authoring (SQL) as well as working familiarity with a variety of databases (Snowflake, Redshift, MySQL, MSSQL, etc.)\nExperience building and optimizing data pipelines, architecture and data sets. \nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. \nBuild processes supporting data transformation, data structures, metadata, dependency, and workload management. \nA successful history of transforming, processing and extracting value from large disconnected, datasets from a variety of data sources (Flat files, Excel, databases, etc.)\nStrong analytic skills related to working with unstructured datasets. \nStrong project management and organizational skills. \nExperience supporting and working with cross-functional teams in a dynamic environment. \nAbility to mentor/guide/collaborate with other team members. \n\nWe are looking for a candidate with 3-5 years of experience in a Data Engineer role. They should also have experience using the following software/tools:\n\nExperience with relational and MPP databases, including Snowflake and MySQL. \nExperience developing software in an agile environment from the requirements stage to production. \nExperience with version control: git\nExperience with container technologies: Docker\nExperience with data pipeline and workflow management tools: Airflow, Jenkins, AWS Glue, Azkaban, Luigi, etc. \nExperience with AWS cloud services: EC2, ECS, Batch, S3, EMR, RDS, Redshift\nExperience with other cloud services: Snowflake, Airflow\nExperience with object-oriented/object function scripting languages: Python, Java, C++, etc. \nExperience with data modeling and data warehouse design\nExperience with data visualization tools (PowerBI, QuickSight)\n\nThe starting annual pay scale for this position is listed below. Actual starting pay will be based on factors such as skills, qualifications, training, and experience. In addition, the company offers comprehensive benefits including medical, dental and vision insurance, 401(k) plan with match, paid time off, and other benefits.\n\nISC's salary ranges are determined by role and level. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations and could be higher or lower based on a multitude of factors, including job-related skills, experience, and relevant education or training.\n\nNational Pay Range\n\n$100,000\u2014$110,000 USD\n\nBenefits of Working at ISC\n\nCompetitive vacation and flexible working arrangements\nComprehensive and inclusive health benefits\nA variety of professional development and mentorship opportunities\nChoice of technology whether at home or in the office\n\nISC's Ownership Behaviors\n\nDo the Right Thing *Be Relentless. Pursue Excellence. *Personal Responsibility & Own Outcomes\nTry Fast. Learn Fast. Fail Fast. Think Big. *Build Extraordinary Owners & Join Forces *Grit & Determination\n\nApplicants with disabilities may contact ISC HR department via e-mail, and other means to request and arrange accommodation. If you need assistance to accommodate a disability, you may request accommodation at any time. Please contact ISC at HR@ISCMGA.com.\n\nISC believes in creating long-term relationships by being responsive and relevant and by consistently delivering value to our community of customers. Specifically, with our employees, we focus on attracting, developing, and retaining the best talent for our business, challenging our people, demonstrating a \"can-do\" attitude, and fostering a collaborative and mutually supportive environment. ISC is an equal opportunity employer and a participant in the US federal E-Verify program (US).\n\nDiversity creates a healthier atmosphere: All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, gender, gender identity, sexual orientation, marital status, medical condition, genetic information, mental or physical disability, military or veteran status, or any other characteristic protected by local, state, or Federal law."}, {"title": "Data Engineer -Remote", "company_name": "RIT Solutions, Inc.", "location": "Denver, CO (Remote)", "link": "/jobs/view/3768007496/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=1hQH8EDaII1eJ7nQ9BACQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3768007496", "description": "About the job\nDuration: 6+ months CTH\n\nMust have a valid LinkedIn profile\n\nRequired Skills\n\n 3-6 years of experience (no more than 6 years)\n Start up environment experience\n Python, SQL, and DBT"}, {"title": "Big Data Engineer - W2", "company_name": "Megan Soft Inc", "location": "United States (Remote)", "link": "/jobs/view/3674845652/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fCAxh4N%2F4YZJ5aEIASaeeA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3674845652", "description": "About the job\nTitle: Big Data Engineer - W2\n\nLocation: Dearborn, MI (Remote)\n\nDuration: 12+ Months\n\nDirect Client: Ford Motors\n\nMust have skills: Should be able to handle GCP requirements\n\n\"Translates business requirements to build repeatable, sustainable, efficient, coded processes that can be productionized by Software Engineers and readily modified by other Data Engineer Creates POC processes in Dev/QA and works with Software Engineers to productionize and align best coding and process practices that maximize efficiency, speed, stability, system resources and capabilities Awareness of and compliance with data privacy, security, legal and contractual guidelines Incorporates data quality and privacy checks/alerts to minimize bad data being consumed by end users, models and dashboards, and to protect customer data; revises checks for new data issues Validates data products and pipelines are functioning as expected following system or application upgrades, source changes, etc. Works closely with Architects if required to align systems, tools and applications being utilized with business use case and performance requirements Provides data product support and maintenance Establishes, tracks and monitors KPIs related to specific data products and deliverable Participate in iteration cadence activities (Iteration Planning, task estimation, daily scrum, demos, etc.). Will be a part of on-call team to resolve critical production support tickets during business hours and off hours.\n\nSkills Required\n\nExperience with Big Data Technologies (Hadoop , ETL Tool, Spark. Hive etc..) Someone who understands Cloud and has experience working with Cloud Platforms (preferably GCP) Should be able to handle GCP requirements - ingesting into GCS/ Big Query, working with GCP components Hands-on in Data Flow, Cloud Storage (GCS), Big Query and GitHub. Proficient in Python and SQL.\n\nSkills Preferred\n\nManages ETL: uses programming and tools for data ingestion, configures pipelines, applies transformations and decoding, integrates and fuses data, moves and securely deliver Experience NoSQL databases\n\nAdditional Information\n\nIf the candidate is remote only, please indicate \"\"100% Remote\"\" under candidate's name on resume. Other similar schedule notations can be \"\"Local Candidate\"\" or \"\"Hybrid Candidate.\"\" ***HRA Test Required***\""}, {"title": "Remote Opportunity: Quantexa Data Engineer", "company_name": "SPAR Information Systems LLC", "location": "United States (Remote)", "link": "/jobs/view/3680073191/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ioykVCZSOQ7BK5%2FwRQHOWg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3680073191", "description": "About the job\nHello All, Hope you are doing great!! Please go through the job descriptions and let me know your interest. Role: Quantexa Data engineer  Location: Remote Duration: Long Term Contract Job Description: 1. End to End usage of Quantexa including guiding our client team on how to use from both front end and data ingestion Reason they are asking for Certification 2. Resolve Entity issues with data 3. Knowledge of Scala and Hive which requires coding experience using parquet 4. Advanced SQL knowledge requiring writing SQL queries 5. Experience having worked with GCP data lake 6. Quantexa certification required\n\nThanks & Regards,\n\nSatnam Singh\n\nDirect: 201 623 3660\n\nEmail : Satnam.singh@sparinfosys.com"}, {"title": "Remote Work - Need Staff Data Engineer - USC only", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3746283980/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=CvIuLBNO%2BPMGi4IXVRkP0A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3746283980", "description": "About the job\n100% Remote, client is in NY\n\nLook for strong job history, Python, Machine Learning, SQL, 10/10 comms, etc. \n\nMUST be an Engineer \n\nSummary\n\nWe're looking for an experienced Data Engineer to join our client's growing team of top notched IT professionals. You will work closely across multiple groups including Editorial and Audio, Marketing, Advertising, etc. to achieve their goals and objectives. The Data Engineering team builds tools throughout the stack while sharing knowledge across these departments.\n\nAs the company grows, they're looking for Data Engineer who will help solidify and expand our pipelines and maintain their data warehouse. Their business model is based on performing complex and very detailed analyses of how our products perform with their subscribers. You'll be responsible for working alongside analysts, product, marketing, and other teams to define new data collection components and measurement schemes that support each new product and feature.\n\nHow Can Add Value To Their Mission\n\nCreate and maintain data pipelines to provide insights and drive business decisions\nEstablish data warehousing strategy (ex. Kimball, Data Vault, etc.)\nMaintain data infrastructure on our AWS accounts\nCollaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.\nWrite unit/integration tests, contributes to engineering wiki, and documents work\nImplement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it\n\nWhat You'll Need To Succeed\n\n7+ years of industry experience measuring product performance and user behavior\nExperience working with a variety of data management technologies, including RedShift, Kinesis/Kafka, Glue, Spark, Postgres, Airflow, dbt, and others.\nExperience implementing BI reporting tools such as Looker\nExperience interfacing with engineers, product managers and analysts to understand data needs\nKnowledge of a variety of measurement beacons, SDKs, APIs including Google Analytics, Amplitude, Braze, Stripe, email service providers\nA commitment to building secure, resilient, fault-tolerant architectures, with clear documentation and procedures in place for support\nA focus on accuracy and detail as you build, ensuring data pipelines produce clear, predictable results\nUnderstanding of the typical metrics a subscription and advertising-supported business needs to measure success\nExperience with Client techniques as applied to behavioral segmentation or anomaly identification is a definite plus\nFamiliarity using developer tools that increase productivity and facilitate the development of resilient code (eg. Docker, CircleCI, Serverless) is a plus"}, {"title": "ENTRY LEVEL DATA ENGINEER (REMOTE)", "company_name": "SynergisticIT", "location": "Jacksonville, FL (Remote)", "link": "/jobs/view/3767587909/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=om7L3CnowiC4PLIcpZD70A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3767587909", "description": "About the job\nSYNERGISTICIT wants every candidate to know that the Job Market is Challenging and to stand out, you need to have exceptional skills and technologies and that's where we come in to make sure you get the attention which you need\n\nSynergisticIT understands the complex nature of the job market and how difficult it can be to secure a position, especially for fresh graduates. Therefore, we assist and help tech-savvies to convert their passions into professions. We go above and beyond to keep you working in your niche.\n\nAs we focus on long-term success, we provide complete career development solutions. From job search to upskilling portfolio and interview preparation, we can guide you at every step of your career.\n\nSynergisticIT spares no efforts to connect you with a large network of tech giants, including Google, Apple, PayPal, Dell, Cisco, Client, etc. Presently, we are actively looking for ENTRY LEVEL DATA ENGINEER a driven mindset. Get the right opportunity and gain experience in building web-centric solutions on Java.\n\nWho Should Apply : Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or anyone looking to make their career in IT Industry\n\nWe also assist in filing for STEM extension and H1b and Green card filing.\n\nCandidates who are serious about their future in the IT Industry and have set big goals for themselves.\n\nCandidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. We also offer Skill Enhancement Programs if the candidates are missing skills or experience which our clients need with great outcomes\n\nCandidates can benefit from skill enhancement if they fall into the below categories. If they are qualified with enough skills then no need for skill enhancement\n\nCandidates Who Lack Experience\n\nHave had a break in careers\n\nLack Technical Competency\n\ncandidates who want to get employed and make a career in the Tech Industry \n\nPlease Also Check The Below Links\n\nhttps://www.synergisticit.com/candidate-outcomes/\n\nhttps://www.synergisticit.com/java-track/\n\nhttps://www.synergisticit.com/data-science-track/\n\nhttps://www.synergisticit.com/which-is-the-best-option-for-tech-job-seekers-staffing-companies-consulting-companies-bootcamps-or-synergisticit/\n\nhttps://www.synergisticit.com/contact-us/\n\nIf the skills are not a match candidates can opt for Skill enhancement. Or their resume can be sent out to clients to see if responses are achievable\n\nREQUIRED SKILLS For Java/Software Programmers\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Core Java , javascript , C++ or software programming\nSpring boot, Microservices and REST API's experience\nExcellent written and verbal communication skills\n\nFor data Science/Machine learning\n\nRequired Skills\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Statistics, Python, data visualization tools\nExcellent written and verbal communication skills\n\nPreferred skills: NLP, Text mining, Tableau, Time series analysis\n\nTechnical skills are required by clients for selection even if its Junior or entry level position each additional Technical skill helps a candidate's resume to be picked by clients over other job seekers.\n\nClients hire candidates with the right technical skills which they need and reject candidates who lack the required technical skills.\n\nNo third party candidates or c2c candidates\n\nPlease apply to the posting\n\nNo phone calls please. Shortlisted candidates would be reached out"}, {"title": "Data Engineer - Remote | WFH", "company_name": "Get It Recruit - Information Technology", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3782241984/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=BgBf1kSAAX%2BSuw6inJsIVw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3782241984", "description": "About the job\nJoin an innovative team at a leading company in the engineering and manufacturing industry, where we are committed to leveraging data for continuous improvement and operational excellence. This hybrid position allows you to enjoy the flexibility of remote work on Mondays and Fridays.\n\nWhy Join Us\n\nAt our company, we prioritize your well-being and professional growth. We offer a comprehensive benefits package, including excellent health benefits, a 6% 401(k) match, generous paid vacation and sick time, 12 paid holidays, and more.\n\nRole Overview\n\nAs a Data Engineer, you will play a pivotal role in transforming raw data into actionable insights, driving improvements in engineering and manufacturing processes, enhancing product quality, and optimizing operational efficiency. Collaborate with dynamic cross-functional teams to extract, analyze, and visualize data, influencing strategic decisions through your valuable insights.\n\nKey Responsibilities\n\nData Collection and Integration:\n\nGather, clean, and integrate data from diverse sources, including production databases, sensor data, and external datasets.\n\nData Analysis\n\nConduct exploratory data analysis to identify patterns, trends, and anomalies.\n\nPerform statistical analysis and hypothesis testing to derive meaningful insights.\n\nData Visualization\n\nCreate informative and visually appealing dashboards and reports using tools like Tableau, Power BI, or Python libraries.\n\nPredictive Modeling\n\nDevelop predictive models to forecast manufacturing performance, quality, and operational outcomes.\n\nRoot Cause Analysis\n\nInvestigate production issues and quality concerns by analyzing data to identify root causes and recommend corrective actions.\n\nPerformance Monitoring\n\nEstablish and maintain key performance indicators (KPIs) to track manufacturing and engineering metrics.\n\nGenerate regular reports to communicate performance trends to stakeholders.\n\nCross-functional Collaboration\n\nCollaborate with engineering, manufacturing, and quality assurance teams to understand their data needs and provide analytical support.\n\nContinuous Improvement\n\nSuggest process improvements and optimizations based on data analysis.\n\nData Security And Compliance\n\nEnsure data security and compliance with relevant regulations, such as GDPR or industry-specific standards.\n\nInfrastructure And Development\n\nBuild and maintain the infrastructure for collecting, storing, and processing data.\n\nDesign, develop, and manage data pipelines, ETL processes, and data warehouses.\n\nRequired Experience\n\nProven experience as a Sr. Data Analyst and/or Data Engineer in an engineering or manufacturing environment.\n\nProficiency in data analysis tools and programming languages, such as Python, R, SQL, and Excel.\n\nStrong data visualization skills with tools like Tableau or Power BI.\n\nExcellent knowledge of statistical analysis and machine learning techniques.\n\nAbility to communicate complex findings and insights effectively to both technical and non-technical stakeholders.\n\nStrong problem-solving skills and attention to detail.\n\nFamiliarity with manufacturing processes and quality control is a plus.\n\nExperience with enterprise ERPs (SAP S4HANA, Oracle EBS), CRMs (Salesforce, Hubspot), and IoT.\n\nProficient in working with cloud computing platforms including AWS, GCP, and Azure.\n\nExpertise in configuring and optimizing cloud-based data solutions for scalability, reliability, and performance.\n\nBachelor's Degree in Data Science, Statistics, Engineering, or Computer Science Required.\n\nApply Now and be part of a team shaping the future of data-driven innovation in engineering and manufacturing!\n\nEmployment Type: Full-Time"}, {"title": "Entry Level Software Engineer - Data Backend Engineer (Remote - Canada)", "company_name": "Yelp", "location": "United States (Remote)", "link": "/jobs/view/3752881757/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fankeoxifvhpTzOYvowHYQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3752881757", "description": "About the job\nYelp engineering culture is driven by our values: we\u2019re a cooperative team that values individual authenticity and encourages creative solutions to problems. All new engineers deploy working code their first week, and we strive to broaden individual impact with support from managers, mentors, and teams. At the end of the day, we\u2019re all about helping our users, growing as engineers, and having fun in a collaborative environment.\n Interested in exploring data and finding innovative ways to collect it, curate it, and make it easily accessible for others to use? How about doing that at an enormous scale? Yelp\u2019s datasets contain billions of interactions between users and local businesses around the globe. If working with our vast amount of data sounds exciting, come and join us!\n\nThe Marketplace Data Observability team is responsible for managing, governing, and monitoring all of Yelp\u2019s Ads data to protect our revenue stream and also support offline analysis that aids in making long term company decisions. We build tools that help us to better understand our data quality, and we strive to uncover data issues before they impact our consumers.\n\nAs a Data Backend Engineer on this team, you will be responsible for building elegant and scalable data products that serve critical, up-to-date, structured information to support different types of analytics within Yelp. As a core contributor to our growing data modeling and data warehousing engineering efforts, you will help design and own mission-critical data flow pipelines and datastores to enable decisions including effective A/B testing and company investments.\n This opportunity is fully remote and does not require you to be located in any particular area in Canada. We welcome applicants from throughout Canada. We\u2019d love to have you apply, even if you don\u2019t feel you meet every single requirement in this posting. At Yelp, we\u2019re looking for great people, not just those who simply check off all the boxes.\n  Build systems that can effectively store and crunch terabytes of data.\nDesign and develop data models for efficient data storage, retrieval, and reporting.\nCreate and maintain conceptual, logical, and physical data models using industry-standard modeling tools.\nCollaborate with cross-functional teams, including engineers, data analysts, business analysts, and data scientists, to understand data requirements and translate them into effective data models.\nParticipate in data integration efforts, including ETL processes and data migration.\nStay up-to-date with industry best practices, emerging technologies, and trends related to data warehousing.\nSupport on-call rotations as needed to operate the team.\n  Understanding of high performing and scalable data systems.\nExperience in building and orchestrating ETL pipelines.\nExperience with Data Lake or Data Warehouse landscape.\nA hunger for tracking down root causes and fixing them in systematic ways.\nAbility to communicate effectively to technical and non-technical cohorts alike. \nExposure to some of the following technologies: Python, AWS Redshift, AWS Athena / Apache Presto, Big Data technologies (e.g S3, Hadoop, Hive, Spark, Flink, Kafka etc), NoSQL systems like Cassandra, DBT."}, {"title": "Data Engineer (Associate Consultant), June 2024", "company_name": "UDig", "location": "Nashville, TN (Remote)", "link": "/jobs/view/3716039960/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=XUjo40kU55HnDCqJ%2F3FdPg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3716039960", "description": "About the job\nCan UDig It? \n\nUDig designs, builds, and implements technology solutions that deliver on business objectives. People join our team because they are excited to have an impact on the future of our organization, passionate about delivering meaningful results, and energized by solving complex problems. Oh, and they like working with other driven, smart, and authentic people!\n\nUDig is proud to be recognized as a \"Best Places to Work\" in Virginia and Tennessee. We're committed to fostering a transparent environment where leadership is accessible, professional development opportunities are abundant, and you are empowered to achieve your career goals. You'll collaborate, develop deep relationships with teammates, and accelerate your success while having fun!\n\nIf you're graduating in May 2024 and excited by the idea of impacting organizations by building data solutions to achieve business goals, you should consider joining our growing technology team.\n\nHow Things Go Down\n\nAs a new, entry-level teammate, you'll take part in our \"Breaking Ground\" training program designed to help you quickly get up-to-speed on UDig's delivery approach within a project team. The program covers:\n\nUDig delivery process and methodologies such as gathering requirements and delivering projects using agile\nTechnical guidance and instruction on the best practices from Data leaders at UDig\nDelivering executive and technical presentations\nCoaching to prepare you to for success on your first client project at UDig\n\nA typical day might entail:\n\nProject standups and check-ins\nDesign and development of data pipelines, BI reports, and more\nPlanning and participating in project demos\nContributing to internal UDig projects\nCareer and job growth through UDig-provided training\nCreating long-lasting friendships with teammates through fun activities as well as your projects (including social events, winning a prize for guessing the weight of pumpkin, or sharing experience on cool tech side projects)\n\nUDig believes it is important for Associate Consultants going through our Breaking Ground program to reside in one of our major markets (Richmond, VA or Nashville, TN metro area) for at least the first year of employment.\n\nWant to learn more? Hear directly from our teammates by checking out our 'A Day in the Life' blog series: https://www.udig.com/digging-in/breaking-ground-2023/\n\nSound Interesting? Here's the Technical Piece\n\nBelow are a few boxes you should be able to check if\u202fyou're interested: Don't check every box? Go ahead and apply!\n\n1+ year of experience with relational database management systems such as Microsoft SQL Server, MySQL, or Oracle\nUnderstanding of the full lifecycle of design, development, and implementation of data solutions including architecture design, development, testing\nSome experience in ETL development or reporting is a plus\nAbility to present technical ideas and high-level concepts and solutions to internal and external team members with varying degrees of technical knowledge\nBachelor's Degree in in computer science, information technology, or any science or business discipline\nAbility to take your work seriously but also seriously enjoy a good time with your UDig colleagues\n\nWhat's in It For You? \n\nAt UDig, you will build your career alongside talented and experienced developers while gaining experience and having fun! We provide continuous challenges and professional development opportunities. Additionally, we offer: \n\nCompetitive salary, merit reviews, and career advancement paths\nFlexible, hybrid environment\nIndividual $1500 Training Budget\nRegular team building and social activities (virtual and/or in-person)\nTransparent culture with strong communication and access to leadership\nGreat benefits like\nGenerous Paid Time Off, company holidays, and parental leave\nMultiple Single and Family Health Insurance plans to choose from\nDental & Vision coverage\nShort-Term and Long-Term disability\nOptional accident and critical illness coverage\nMatching 401(k)\nand more!"}, {"title": "Expression of Interest: Data Engineer", "company_name": "Fingerprint for Success (F4S)", "location": "New York, NY (Remote)", "link": "/jobs/view/3787769880/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=l%2FEDpQqUNmCttGdIyTk7Cg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787769880", "description": "About the job\nWe are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.\n\nThe F4S Talent Pool is a pilot project designed to:\n\nHelp job seekers get discovered by our partners based on their anticipated hiring needs.\nProvide optional support and resources for job seekers in their career endeavors.\nHelp individuals understand, and bring out the best in themselves and each other.\n\nThe F4S Talent Pool process:\n\nOnce you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.\n\nAbout Fingerprint For Success (F4S)\n\nBacked by 20+ years of research, F4S\u2019s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high performing teams.\n\nKeep in mind, joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your F4S work style assessment to match the hiring needs of our partners.\n\nYour feedback is a gift! Write to us via:\n\nPowered by JazzHR\n\noWuPJ9lwq9"}, {"title": "Data Engineer Seattle, WA (remote)", "company_name": "Conch Technologies, Inc", "location": "United States (Remote)", "link": "/jobs/view/3765052211/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=SBETI78UMutCHuRp5FnP%2FA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3765052211", "description": "About the job\nHi,\n\nGreetings from Conch Technologies Inc\n\nData Engineering Lead \u2013 With Databricks and Unity Catalog\n\nLocation \u2013 Seattle, WA (remote)\n\nDuration \u2013 12 months\n\nNeed: Design and implement data architecture and strategy, leveraging technologies such as Databricks, Unity Catalog, Privacera, and Collibra.\n\nAs a Data Engineering Lead, you will play a pivotal role in defining and executing technical deliverables while assessing complexities and levels of effort. You'll engage with both vendor and internal teams to resolve technical challenges and ensure seamless collaboration. Your leadership will extend to planning and prioritization with the SCRUM master, facilitating effective coordination between onsite and offshore teams, and driving feature prioritization through collaboration with product managers.\n\nKey Responsibilities\n\n Lead the definition and breakdown of technical deliverables, assessing complexities and levels of effort.\n Collaborate with vendor and internal teams to address technical issues.\n Drive planning and prioritization in conjunction with the SCRUM master.\n Coordinate with onsite and offshore teams for timely project delivery.\n Collaborate with product managers to prioritize features, communicate with stakeholders, and clarify requirements.\n Collect and manage user-reported issues, ensuring prompt resolution and communication.\n Define comprehensive test plans, oversee their execution, and document results.\n\n--\n\nWith Regards,\n\nNagesh G\n\nMobile: 408-381-5645\n\nDesk: 901-313-3066\n\nEmail: nagesh@conchtech.com \n\nWeb: www.conchtech.com"}, {"title": "Remote Work - Need Azure Fabric Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3759325599/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=C3iVjmcR9wjUip1nNjUiCg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3759325599", "description": "About the job\nThe client is placing heavy emphasis on Microsoft and Azure Fabric experience\n\nInformation Management Engineer \u2013 Take the identified information management sources and assist migration to the Client Data Lake, Azure Fabric. The engineer will transform information management (de-dup, clean and enrich).\n\nTake the identified data sources and bring them into our Data Lake, Azure Fabric. Once the data is in, the engineer will need to be able to transform data (de-dup, clean and enrich the data). We are focused on using a medallion data model for data architecture. The tools we are working with are: Delta Parquet files, python, spark, notebooks, T-SQL, Fabric lakehouse/warehouse, Dataflows Gen2, Data Pipelines, Direct Lake, shortcuts, ADF."}, {"title": "Data Engineer", "company_name": "Catalist", "location": "Washington, DC (Remote)", "link": "/jobs/view/3744951279/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ui1Fk6ICmnPNqAlZDZAcwA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3744951279", "description": "About the job\nFor over 17 years, Catalist has been a leader in civic data and data science innovation. Our mission is to provide progressive organizations with the data, software, and services needed to better identify, understand, and communicate with the people they need to engage and mobilize. Our clients include the largest, most influential organizations in the U.S. active in civic engagement, advocacy, and political campaigns.\n\nCatalist is home to a dedicated, creative team of technologists, data scientists, and campaign experts committed to using our talents and technology to nurture a vibrant and growing progressive community.\n\nAs a Data Engineer at Catalist, you will work closely with the Analytics and Technology departments to design, build, support, and maintain various data pipelines and processes with an end goal of providing data and intelligence to the progressive community. This role primarily involves translating data architecture designs into functional processes, code, and systems.\n\nThe ideal candidate will be a highly motivated individual with excellent technical skills, a strong desire to learn new skills, and an interest in progressive politics. Catalist values creativity and problem-solving. Our work is on the cutting edge of data-driven politics, and your support will help Democratic candidates and progressive organizations conduct successful advocacy and electoral campaigns.\n\nThis position reports to the Deputy Chief Data Officer. The Data Engineer is a part of a growing Data team that supports all underlying work at Catalist.\n\nThis position is included in our CWA bargaining unit.\n\nPrinciple Duties & Responsibilities\n\nDevelop scalable, production processes, code, and systems for data ingest, transformation, modeling, and reporting across a variety of platforms\nTranslate mock-ups and designs into functional processes, code, and systems\nCreate architecture designs when needed\nProvide quality assurance and testing on processes, code, systems, and products\nBecome an internal subject matter expert on various datasets and support other Catalist departments/teams on usage of those datasets\nExecute ad-hoc data and database maintenance tasks as requiredProject manage cross-departmental efforts, with direct responsibility for stakeholder engagement, management, and execution of technical elements\n\n\nRequirements\n\nBS or BA in a technical field, or relevant experience\n1-2 years of experience working with SQL databases\nExperience with data cleaning or ETL processes\nExperience with distributed computing systems and/or distributed datastores (particularly the Hadoop ecosystem)\nExperience managing projects\nFamiliarity with Catalist data, progressive politics, voter files, and/or commercial data\nBackground check required\n\n\nPreferred Skills & Abilities\n\nWillingness to be a problem solver and produce results in a fast paced environment\nAbility to be creative and personable, and articulate ideas clearly\nExperience working with SQL databases\nProficiency with Python or another object-oriented programming language (R, Java, Scala, etc\u2026)\nExperience working in cloud environments (AWS, GCP, etc.)\nExperience working in command line environments such as Bash\nExperience with a version control tool such as git or github\n\n\nBenefits\n\nMedical, Dental, Vision, Prescription Drug\n\nCatalist offers Medical, Dental, Vision, and Prescription Drug coverage for eligible staff and their eligible dependents. Catalist\u2019s Medical plan is a comprehensive PPO program including Prescription Drug coverage with 85% of the premium paid by Catalist. Dental and Vision coverage is provided at no cost to employees.\n\nGroup Term Life Insurance and Long-Term & Short-Term Disability Coverage\n\nGroup Term Life Insurance and Long-Term and Short-Term Disability coverage is available for eligible staff. These benefits are provided at no cost to Catalist employees.\n\n401(k) Safe Harbor Plan\n\nA 401(k) Safe Harbor Plan is available to eligible staff with a 3% contribution from Catalist from the date of hire. Employees may contribute pre-tax or post-tax from their salary up to the legal limits set forth by the IRS.\n\nMedical and Dependent Care Flexible Spending Accounts (FSAs)\n\nCatalist offers an FSA Program that gives eligible staff the ability to pay out-of-pocket medical/dental/vision/child care expenses from pre-tax earnings.\n\nTransit Benefits\n\nCatalist also makes available a Transit benefit FSA program to eligible employees using pre-tax contributions with a company match.\n\nProfessional Development and Remote Work Expenses\n\nEligible employees may be reimbursed up to $750 each year for professional development / education and remote work expenses.\n\nStudent Loan PayDown or SaveUp\n\nCatalist offers a Student Loan PayDown and College SaveUp benefit for eligible staff.\n\nVacation, Personal Leave, Sick Leave Benefits\n\nCatalist offers generous vacation benefits to all eligible staff. Eligible employees also receive:\n\n 14 Paid Holidays\n Personal Days\n Sick Leave\n Parental Leave\n\n\nHybrid Office/Remote Work\n\nCertain positions at Catalist are eligible for Office/Remote Hybrid or full Remote status."}, {"title": "Remote Entry Level Data Analyst/Engineer", "company_name": "SynergisticIT", "location": "White Plains, NY (Remote)", "link": "/jobs/view/3774171423/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=q54sk1%2FRww0z%2BC%2BE8e0jQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774171423", "description": "About the job\nAt SynergisticIT, we're all about making connections. Whatever IT goals you have, our software programmers can help achieve those. Our Software development teams can take up turnkey projects and execute them in an effective and efficient manner. If you are looking to source talent our recruiters will find the ideal IT talent for your company. What's the secret to our success? Well, it all starts with taking quality time to listen to each client's specific needs. After we have a thorough grasp of your IT goals, we can better customize our Developments as per your specific needs. We can also tailor make recruiting programs to exceed your expectations. Since our founding in 2010, SynergisticIT's strategies have earned the company an enviable position in the software development, IT staffing and IT skill enhancement fields. SynergisticIT continues to work with hundreds of satisfied American clients with our software programmers working on our projects and after gaining hands on experience on cutting edge technologies moving to contribute their skills to great clients like Apple, Google, Client, Ebay, Paypal, Kroger, the Walt Disney Company and hundreds more. If you are tired of working with inefficient programmers who take a lot of time to ramp up we want you to try us. Our software programmers can hit the ground running and get you the maximum return on your investment. You have already tried the rest its time you tried the best. SynergisticIT - Home of the Best Data Scientists and Software Programmers in the Bay Area.\n\n Why Us ? \n\nSynergisticIT has a proven track record of successfully skill enhancement and staffing IT employees for some of the world's most iconic brands. Our team takes the time to fully understand every client's needs so we could best meet your IT staffing requirements. The knowledgeable staff at SynergisticIT is always more than happy to work with clients to ensure they reach their software development goals. Besides staffing, SynergisticIT is also committed to helping young IT professionals advance their career with a robust upskill program . Everyone who goes through SynergisticIT's program learns all the skills necessary to succeed in many IT fields ranging from Java to Machine Learning. Additionally, everyone trained at SynergisticIT has been through extensive mock and technical interview screenings to bolster their career prospects. Last, but certainly not least, SynergisticIT takes great care to respect the privacy considerations for every client. All companies who work with SynergisticIT can rest assured their confidential data is protected using the most up-to-date encryption technologies. SynergisticIT also complies with all the latest NDA agreements.\n\n REQUIRED SKILLS For Java /Software Programmers \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Time series analysis \n\n We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2022 and at Gartner Data Analytics Summit (Florida)-2023 \n\nOracle CloudWorld Event (OCW) Las Vegas 2022 | SynergisticIT - YouTube\n\n https://www.youtube.com/watch?v=OAFOhcGy9Z8 \n\n https://www.youtube.com/watch?v=EmO7NrWHkLM \n\n https://www.youtube.com/watch?v=NVBU9RYZ6UI \n\n https://www.youtube.com/watch?v=Yy74yvjatVg \n\nSynergisticIT at Gartner Data and Analytics Summit 2023 - YouTube\n\nFor preparing for interviews please visit  https://www.synergisticit.com/interview-questions/ \n\n We are looking for the right matching candidates for our clients \n\n Please apply via the job posting \n\n REQUIRED SKILLS For Java /Full Stack/Software Programmer \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning Positions \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Tensorflow \n\n If you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team. \n\n No phone calls please.  Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates"}, {"title": "Data Engineer", "company_name": "Anika Systems", "location": "Leesburg, VA (Remote)", "link": "/jobs/view/3787766893/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=mApL9JnN%2B%2Fr2%2BYUPlxRYOA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787766893", "description": "About the job\nAnika Systems is a fast growing, woman-owned small business that specializes in providing innovative IT solutions for federal government agencies. Our expertise lies in accelerating delivery in Data and Analytics, Intelligent Automation, Application Development, and IT Modernization. We are currently expanding our Federal team and are seeking a passionate and talented Data Engineer. This opportunity is 100% remote.\n\nMust be a U.S. Citizen with the ability to obtain and maintain a government suitability clearance\n\nResponsibilities\n\nDesigns, develops, builds, analyzes, evaluates, and installs database management systems to include database modeling and design, relational database architecture, metadata, and repository creation and configuration management.\nDefines and oversees database organizations, standards, controls, procedures, and documentation. Provides technical consulting in the definition, design, and creation of a database environment.\nAdvises applications development staff and users on data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nDesigns and implements databases with respect to access methods, access time, batch processes, device allocation, validation checks, organization, protection and security, documentation, and statistical methods.\nUses data mapping, data mining, and data transformational analysis tools to design and develop databases.\nDetermines data storage and optimum storage requirements.\nPrepares system requirements, source analysis, and process analyses and designs throughout the database implementation.\n\nRequired Skills And Experience\n\nBA/BS and 1 year of relevant experience\nExperience with DataBricks, SQL, Python\nApplications development experience with data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nExcellent communication skills (both oral and written) with process-oriented organizational skills to ensure project success\n\nDesired Skills And Experience\n\nDemonstrated successful development experience with the full life cycle of government database management systems including database modeling and design, relational database architecture, metadata and repository creation, and configuration management.\n\nPowered by JazzHR\n\nTlKWIorucJ"}, {"title": "Remote Work - Need Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3774712978/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=r3Bp%2F0qcOD2OpC5jpbUOcA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774712978", "description": "About the job\nTitle : Data Engineer\n\nLocation : Remote\n\nDuration : 6+ months to start (long term contract (work order are extended 2x year))\n\nPlease Fill out the skill matrix: Technology Some Awareness (1) Novice (2) Intermediate (3) Advanced (4) Expert (5) SQL Tableau Desktop /Creator Tableau API Integration AWS - S3 Glue AWS - Athena Redshift AWS - Lambda AWS - SNS/SQS msExcel\n\nProfile: Data Analyst will work closely with Architects and leads in the Interflow team, key business stakeholders including program staff to support the organizations effort in the areas of data Analysis and information management. Role should have strong experience in writing SQL queries to extract data for reports. Candidate should have experience working with large set of data. As these reports are very critical for the company and we are looking for an expert who is capable of handling, consuming and extracting large sets of data for analysis and reports.\n\nMust Haves\n\nAWS - Athena\nAWS - S3\nGlue\nhands-on development of reporting applications\nMS Excel\nRedShift\nSQL\nTableau Desktop /Creator\n\nNice\n\nAWS - Lambda\nAWS - SNS/SQS\nTableau API Integration\n\nSkills\n\nBA/BS required (major in an analytical field desired) \nA minimum 10+ years work-related experience. \nExperience in hands-on development of reporting applications for the Web in a professional environment \nThe Ethos of continuous improvement and interest in learning new things.\nStrong analytical thinking and structured problem-solving ability\nAbility to handle multiple projects and assignments simultaneously and effectively in a cross-functional team environment. \nVersed on the agile methodology and best practices. \nExcellent interpersonal and collaboration skill with the ability to work with a diverse set of colleagues, across functions, from different organizations, disciplines, etc. \nSelf-starter, ability to set priorities, work independently and attain goals"}, {"title": "Data Engineer", "company_name": "Team Remotely Inc", "location": "Houston, TX (Remote)", "link": "/jobs/view/3786249829/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=78lY5Jo%2FfyRPC6TQYCdvww%3D%3D&trk=flagship3_search_srp_jobs", "id": "3786249829", "description": "About the job\nThis is a remote position.\n\n Data Engineer (US/Canada Residents Only, 1 year experience, remote)\n\nTeam Remotely Inc. is a staffing and recruitment agency that offers a comprehensive solution for talent acquisition, including sourcing, vetting, pay rolling, and managing talent. Whether you need contract staffing, direct hire, direct sourcing, talent pools, or diversity initiatives, our model can support your hiring strategy.\n\n Hiring Type: Full-Time\n\n Base Salary: $55K-$70K Per Annum.\n\nResponsibilities\n\nDesigning user interface changes for web-based DB applications. \nReviewing application requirements and interface designs. \nDeveloping and implementing highly responsive user interface components using react concepts. \nWriting application interface codes using JavaScript following react.js workflows. \nTroubleshooting interface software and debugging application codes. \nDeveloping and implementing front-end architecture to support user interface concepts. \nMonitoring and improving front-end performance. \n\nQualifications And Experience\n\nBachelor\u2019s degree in computer science, information technology, or a similar field. \nPrevious experience working as a react.js developer. \nIn-depth knowledge of JavaScript, CSS, HTML, jQuery, and front-end languages. \nKnowledge of REACT tools including React.js, Webpack, Enzyme, Redux, and Flux. \nExperience with user interface design. \nKnowledge of performance testing frameworks, including Mocha and Jest. \nExperience with browser-based debugging and performance testing software. \nTroubleshooting skills. \nProject management skills. \nProblem-solving skills. \nVerbal communication skills. \n\n Why work with Team Remotely?\n\nTeam Remotely Inc. is a staffing platform offering a seamless experience for employers and candidates. Employers can post job openings and specify their requirements, while candidates can create profiles and upload resumes.\n\nThe team of Team Remotely continuously learns and adapts based on previous successful placements, constantly improving its matching capabilities. This ensures that the recommendations provided by Team Remotely are tailored and accurate, increasing the likelihood of a successful match between employers and candidates. By providing intelligent and data-driven solutions, they strive to enhance the efficiency and effectiveness of the hiring process, ultimately helping companies find the best talent and individuals find their dream jobs."}, {"title": "Data Engineer", "company_name": "ZAG Technical Services", "location": "United States (Remote)", "link": "/jobs/view/3771403464/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=Hf0VefrKXBFIL5MtHmY0Pw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3771403464", "description": "About the job\nWe are looking for a tech savvy Data Engineer to contribute to the building, operating, and scaling of next generation data platforms and tools that will power data-driven analytics capabilities throughout the ZAG products and services portfolio, spanning areas such as business intelligence, reporting, and data analytics.\n\nThe ideal team member is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. Must be comfortable supporting the data needs of multiple clients and teams, systems, and products within the modernization of ecosystems. The right-fit person will be excited by the prospect of optimizing or even re-designing data processes to support next generation of advanced analytics capabilities, which in-turn translates to implementing strategic enhancements to maintain competitive advantage in the industry.\n\nThe successful team member will leverage their proficiency to\u2026\n\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\nBuild the analytics processes required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Data Management and cloud based \u2018big data\u2019 technologies.\nBuild analytics tools that utilize technologies to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.\nCollaborate with client teams to identify and solution regional and local requirements within the context of the standard global business model for enterprise analytics.\nParticipates in meetings, discussions, strategy sessions where changes, improvements, and enhancements are proposed, evaluated, and approved. \n\n\nThe exceptional team member will have or demonstrate progressive experiences in\u2026 \n\nBachelor\u2019s degree or higher in Computer Science or related field\nExperience working with BI and data warehousing tools\nExperience working with enterprise data, where security is paramount and data governance is critical\nAbility to work independently and in a team environment, and effectively engage all levels of the organization\nUnderstanding of distributed systems driving large-scale data processing and analytics with a successful history of manipulating, processing, and extracting value from large, disconnected datasets.\nAbility to communicate effectively (listening, presenting, and questioning)\nStrong organizational, written, and communication skills\n\n\nAbout ZAG\n\nZAG Technical Services empowers our clients\u2019 success through intelligent business solutions that increase employee productivity, network security and team collaboration. We aim to ensure that our client\u2019s IT investments drive ROI, provide a competitive advantage and help businesses grow. We do this by living our core set of values: Integrity Foremost, Client-Centered, Accountable Always, Teamwork Throughout, and Exceptional is the Goal.\n\nWhat ZAG Has To Offer\n\nCompetitive compensation package \nBenefits including medical, dental, vision, 401K and life insurance \n\n\nZAG Technical Services is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all of our employees.\n\nZAG Technical Services is an E-Verify participant and will provide the Social Security Administration (SSA) and, if necessary, the Department of Homeland Security (DHS), with information from each new employee\u2019s Form I-9 to confirm work authorization."}, {"title": "Junior Data Engineer", "company_name": "HireMeFast LLC - Career Accelerator - Land A Job", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3786249797/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D&trk=flagship3_search_srp_jobs&lipi=urn%3Ali%3Apage%3Ad_flagship3_search_srp_jobs%3BQExogrHBQoSF9zZm%2B9kufw%3D%3D&lici=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D", "id": "3786249797", "description": "About the job\nThis is a remote position.\n\n Job Title: Junior Data Engineer\n\n Employment Type: Full-Time\n\n Salary: $64,000 - $76,000 per annum\n\n Experience Required:  Minimum 1 year of project experience\n\nAbout us: HireMeFast is a leading staffing and recruitment agency specializing in connecting businesses with top-tier talent across various industries. Our mission is to bridge the gap between exceptional candidates and organizations needing their skills, expertise, and unique qualities. Our team of experienced and dedicated recruitment specialists utilizes innovative sourcing strategies, and a vast network to identify and attract top talent. We conduct comprehensive procedures to ensure that only the most qualified candidates are presented to our clients.\n\nPosition Summary\n\nJoin the fast-paced, innovative, and collaborative environment focused on providing an AIOps platform that enhances the intelligence of the CVS Health infrastructure. Work closely with subject matter experts and colleagues to build and scale out machine learning and AI solutions that will detect, predict, and recommend solutions to correct issues before system impact and enhance efficiency, reliability, and performance of CVS Health\u2019s IT operations.\n\nKey Responsibilities include:\n\nData pipeline development: Designed, implemented, and managed data pipelines for extracting, transforming, and loading data from various sources into data lakes for processing, analytics, and correlation. \nData modeling: Create and maintain data models ensuring data quality, scalability, and efficiency \nDevelop and automate processes to clean, transform, and prepare data for analytics, ensuring data accuracy and consistency \nData Integration: Integrate data from disparate sources, both structured and unstructured to provide a unified view of key infrastructure platform and application data \nUtilize big data technologies such as Kafka to process and analyze large volumes of data efficiently \nImplement data security measures to protect sensitive information and ensure compliance with data and privacy regulation \nCreate/maintain documentation for data processes, data flows, and system configurations \nPerformance Optimization- Monitor and optimize data pipelines and systems for performance, scalability and cost-effectiveness \n\nCharacteristics of this role:\n\nTeam Player: Willing to teach, share knowledge, and work with others to make the team successful. \nCommunication: Exceptional verbal, written, organizational, presentation, and communication skills. \nCreativity: Ability to take written and verbal requirements and come up with other innovative ideas. \nAttention to detail: Systematically and accurately research future solutions and current problems. \nStrong work ethic: The innate drive to do work extremely well. \nPassion: A drive to deliver better products and services than expected to customers. \n\nRequired Qualifications\n\n2+ years of programming experience in languages such as Python, Java, SQL \n2+ years of experience with ETL tools and database management (relational, non-relational) \n2+ years of experience in data modeling techniques and tools to design efficient scalable data structures \nSkills in data quality assessment, data cleansing, and data validation \n\nQualifications\n\nKnowledge of big data technologies and cloud platforms \nExperience with technologies like PySpark, Databricks, and Azure Synapse. \n\nEducation\n\nBachelor\u2019s degree in Computer Science, Information Technology, or related field, or equivalent working experience\n\nWhy HireMeFast LLC?\n\nAt HireMeFast, we understand that finding the right individuals to join your team is crucial for success and growth of your organization. We are committed to streamlining the hiring process for our clients, ensuring they have access to a diverse pool of highly qualified candidates who are a perfect fit for their specific needs."}, {"title": "Data Engineer  - Need candidates from Texas ( REMOTE )", "company_name": "Pulivarthi Group (PG)", "location": "United States (Remote)", "link": "/jobs/view/3770685212/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=9gdMwuif%2BpB%2BCQYvQi3imQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3770685212", "description": "About the job\nFollow us on Linkedin: https://www.linkedin.com/company/pulivarthigroup/\n\nPulivarthi Group LLC is a Global Staffing & IT Technology Solutions company, with our prime focus of providing world class solutions to our customers with the right talent. We combine the expertise of our team and the culture of your company to help you with the solution that is affordable and innovative using high quality standards and technologies.\n\nWe\u2019ve served some of the largest healthcare, financial services, and government entities in the U.S.\n\nData Engineer - FT only - Azure SQL with data lakes experience\n\nData Engineer -\n\nthe resource requirement for this Data Engineer role needed more clarification.\n\nOur customer is requesting a resource who knows Azure CI/CD (Continuous Integration / Continuous Deployment) to build and integrate their DevOps pipeline.\n\nOnce the DevOps pipelines are mapped, they need the Data Engineer to connect to all the Data Sources, primarily their Data Lake.\n\nThe Customer (based in Houston) is interested in a resource in a time zone not too far removed from the Central Time Zone to allow real time collaboration."}, {"title": "Data Engineer", "company_name": "Otter Products", "location": "Fort Collins, CO (Remote)", "link": "/jobs/view/3766728302/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=77U3U7CAi1PxgDmpI3%2BiNQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3766728302", "description": "About the job\nOverview \n\nOtter Products is currently recruiting for a Data Engineer to join our Data team! You can be based in our Fort Collins, CO office with flexibility to work a portion of your time remotely, or 100% Remote in the U.S.\n\nThe Data Engineer applies professional experience, concepts, and company objectives toward building and operationalizing the minimally inclusive data necessary for the enterprise data initiatives following industry standard practices and tools. The bulk of the Data Engineer's work would be in designing, managing and optimizing data pipelines, then moving these data pipelines effectively into production for key data and analytics consumers like business/data analysts, data scientists or other stakeholder that needs curated data for data and analytics use cases across the enterprise. The Data Engineer also ensures compliance with data governance and data security requirements while creating, improving and operationalizing these integrated and reusable data pipelines to enable faster data access, integrated data reuse and vastly improved time-to-solution for data and analytics initiatives. Additionally, the Data Engineer will be expected to collaborate with other data team members and data consumers working on the models and algorithms developed by them toward optimization for data quality, security, and governance to put them into production leading to potentially large productivity gains.\n\n About Otter Products \n\nOtter Products, we grow to give. From our founder's garage in 1998 to the global technology leader we are today, Otter Products continues to drive growth through innovation.\n\nThrough our industry-leading brands - OtterBox, Liviri and OtterCares - we provide our partners the number-one selling and most trusted products in our categories. Our philanthropic spirit is the foundation on which we foster our partner relationships, allowing us to grow and to give - together.\n\nBy way of our charitable arm, the OtterCares Foundation, we support our communities and invest in our future through education that inspires kids to change the world.\n\nAnd even as our global community of Otters continues to grow, our founder's core values are still at the heart of everything we do. We measure our success by our ability to give back to our communities and strengthen opportunities for all.\n\nFor more information visit otterproducts.com\n\n Responsibilities \n\nComplete analysis, design and development of BI solutions using Azure SQL Server\nFamiliarity with Data Warehouse concepts\nExperience coding the extraction, transformation, and loading (ETL) processes\nDatabase development primarily in SSIS, Data Factory and SQL\nPartner with the business to determine end user database/reporting needs and requirements\nGenerate ad hoc reports using Power BI or SSRS\nCollaborate with other developers to create and implement best approach solution(s)\nTroubleshoot Azure tools, systems, and software\nReview queries for performance issues, making changes as needed\nCollaborate with team to performance-tune Azure application as necessary\nAssist with the analysis and extraction of relevant information from large amounts of historical business data to feed data science initiatives\nParticipate and support the design and documentation of processes for large scale data analyses, model development, model validation and model implementation\nSupport and maintain a positive safety culture by following all safety policies and procedures and actively contributing to a safe working environment\nOther duties as required\n\n Qualifications \n\nBachelor's degree required. Degree in Computer Science or Mathematics preferred\nMinimum of four years of experience in an IT or analytical role required. Experience in database development, report writing and/or statistics preferred.\n\n EEO \n\nOtter Products, LLC is an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, age, marital status, pregnancy, sex, sexual orientation, gender, gender identity or expression, national origin, disability, veteran status, or any other characteristic or status protected by law.\n\n For US Based Roles Only - Compensation Range Minimum \n\nUSD $90,000.00/Yr.\n\n For US Based Roles Only - Compensation Range Maximum \n\nUSD $120,000.00/Yr.\n\n Additional Total Rewards \n\nProfit Sharing Program Eligible, Benefits Eligible - Full Time- check out otterproducts.com/careers/why for more info"}, {"title": "Software Engineer, Distributed Systems", "company_name": "Hightouch", "location": "United States (Remote)", "link": "/jobs/view/3628572539/?eBP=CwEAAAGMd4QO8APODyE_-mMOPhYyQGg5ZjVbPwq_3V7DK0B2bLAf10pcedM9S2T71X5i4TiJuEwOzWBbSiTKbLuk-VD-_Y-dN5uyf8YpYkHYqEYFxySMS9zr2hRjNrxX9gXbxssh35TFK9YTNV2WeH2Ncb675JgFbJTgNWSZySFC6rrdJGmdSDTr2q5d3VoxORz3GjsDbzjeyZQCvQ07RqYw7oujWSgslycs2LMCY_-N0NEj6e0PvbaYKiZ_5IW34bGVSYLXpHwqa8oOcNttYZ90UA3E_7sDSFzABdIVlfrp5zzERrIYy5sh2MIInxHdDoKgQw8VmOn5uM4S0tXUl-i7LnN0_W0NfPBmLiVDNuveIOm4MfDNPxfe5QKPk6FUAVpvgbcoJn9yRyE&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=rph%2FT6Q6Vkp6YoggJf9cJQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3628572539", "description": "About the job\nAbout Hightouch\n\nHightouch\u2019s mission is to empower everyone to take action on their data. Through our Reverse ETL platform, business and data users can seamlessly sync data from where it resides, such as warehouses and databases, to where it is needed, including operational systems and SaaS tools. Traditionally, acting on data has required engineering time and bandwidth, and left most business users stuck with charts and reports that are unable to take automated action on their data. With Hightouch, every business user, without writing any code, can activate data to streamline critical processes, improve marketing performance, and scale operations.\n\nOur team operates with a focus on making a meaningful impact for our customers. We believe in approaching challenges with a first principles thinking mindset, moving quickly and embracing our value of efficient execution, and treating each other with compassion and kindness. We look for team members that are strong communicators, have a growth mindset, and are motivated and persistent in achieving our goals.\n\nHundreds of companies use Hightouch, including Spotify, Ramp, Retool, NBA, Plaid, and Betterment. We\u2019re based in San Francisco, are remote-friendly, and backed by leading investors such as Amplify Partners, ICONIQ Growth, Bain Capital Ventures, Y-Combinator, and Afore Capital.\n\nAbout The Role\n\nWe are looking for a distributed systems engineer to work on the systems that power the syncing engine our customers and other engineering teams rely on. This presents an exciting challenge where you can apply your expertise in distributed systems, performance optimization, and troubleshooting to push the boundaries of what is possible and meaningful improve our ability to move massive amounts of customer data. This role also provides a unique opportunity to work on a multi-cloud and multi-region infrastructure that supports a global customer base.\n\nWe believe in enabling our engineers to do their best work for our customers by giving them extremely high levels of ownership and autonomy. This comes in different forms: you will own and deliver projects from start to finish, you will work directly with customers to solve their hardest scaling problems, and you will have a lot of influence over what we work on as a team and company.\n\nSome of the problems we\u2019ll be working on include:\n\nSync Speed: Customers want to sync a lot of data to important destinations like Facebook and Snapchat, which requires us to analyze every part of our syncing process and find where we can optimize to sync data more quickly\nStreaming Syncing: We currently sync data from sources in batch only, but envision a world where we also do syncs in real-time and support streaming sources like webhooks and queues\nScalability and Reliability: As part of our rapid growth, we\u2019re always evaluating current future bottlenecks in scaling and reliability, and architecting for the next order of magnitude of growth\nPersonalization API: Our Personalization API product provides a low latency caching layer on top of data warehouses that enable customers to use their data warehouse for real-time personalization use cases with \nMulti-Region and Multi-Cloud: Supporting our multi-region and multi-cloud backend, including extending it to launch Hightouch on in new regions to support data residency requirements of our global customer base\n\nWe are looking for talented, intellectually curious, and motivated individuals who are interested in tackling the problems above. This is a senior role, but we focus on impact and potential for growth more than years of experience. The salary range for this position is $170,000 - $240,000 USD per year, which is location independent in accordance with our remote-first policy.\n\nAbout You\n\nYou are an engineer with a passion for solving hard technical problems that generate real value for customers. You\u2019re motivated by high ownership and are comfortable in a fast-paced, startup environment.\n\nYou have experience and comfort with distributed systems and high-scale systems or the ability and desire to build this experience quickly. This experience can take different forms:\n\nYou\u2019ve built syncing engines that process and move terabytes of data per day in a scalable way\nYou\u2019ve created event collection or data streaming services that power company critical systems\nYou\u2019ve owned systems that have undergone orders of magnitude of growth and helped them scale accordingly\nYou have relevant skills and are a fast learner who is excited to take on the challenges we face\n\nInterview Process\n\nOur goal with the interview process is to balance speed with giving both parties opportunities to assess whether there is a strong mutual fit. We will ask you questions, but we want you to ask us questions! Our technical interviews focus on how you design systems because we believe this is the best way for us to see how you work and for you to see how we collaborate. We don\u2019t ask you to write code to solve technical brainteasers that don\u2019t appear in your day to day job.\n\nRecruiter screen [30m]: Introductory call with our recruiting team to get to know each other and see if the role could be a good mutual fit.\nSystems design screen [45m]: Designing a data processing feature end-to-end.\nHiring manager interview [30m]: Chat with hiring manager about past experiences and future operating preferences to assess fit on company values and operating principles.\nSystems design interview [90m]: Work with the interviewer to architect a system at a conceptual level. The problem will be at a pretty high level - and have both product and customer requirements as well as technical."}, {"title": "Entry-Level Data Scientist Engineer", "company_name": "Phoenix Recruitment LLC", "location": "Texas City, TX (Remote)", "link": "/jobs/view/3788727625/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=8iwtytHsCu4wMwJ4fj3pxw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3788727625", "description": "About the job\nThis is a remote position.\n\nTitle - Entry-Level Data Scientist Engineer, 1 year of project experience\n\nEmployment Type: Full-time\n\nBase Salary: $60K-$70K\n\nPhoenix Recruitment offers a variety of recruiting services to assist both employers and employees. They specialize in marketing open positions, recruiting, and helping employers to find qualified candidates across various industries. Phoenix Recruitment has expertise in streamlining the hiring process. They can help ensure that the process is efficient, well organized, and compliant with relevant regulations.\n\nSkills and Abilities:\n\nStrong knowledge of R or Python for data analysis and modeling. \nProficiency in statistical programs such as R, SAS, MATLAB, or Python. \nFamiliarity with spreadsheets (VBA) and database applications (Access, Oracle, SQL, or equivalent technology). \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \nAbility to learn new methods quickly and work under deadlines. \nExcellent teamwork and communication skills. \nStrong analytical and problem-solving abilities. \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \n\nPreferred:\n\nKnowledge of actuarial concepts and life, health, and/or annuity products. \nExperience with statistical modeling techniques such as GLM, Decision Trees, Time Series, Regression, etc. \nFamiliarity with Microsoft DeployR. \nExposure to insurance risk analysis. \nBasic experience in computational finance, econometrics, statistics, and math. \nKnowledge of SQL and VBA. \nFamiliarity with R or Python for predictive modeling \n\nWhy Phoenix Recruitment LLC?\n\nPhoenix Recruitment often has an extensive network of employers and candidates. This network allows them to tap into a pool of qualified candidates and connect them with suitable job opportunities. They can also leverage their connections to help employers find the right talent efficiently. Outsourcing the recruitment process to a specialized agency can save your time and resources, avoid delays, reduce administrative burdens, and increase the chances of finding the right fit for your organization."}, {"title": "Data Engineer - Remote Work", "company_name": "BairesDev", "location": "United States (Remote)", "link": "/jobs/view/3783162297/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=OtoCrAAHZFomVup04fy35g%3D%3D&trk=flagship3_search_srp_jobs", "id": "3783162297", "description": "About the job\nWho We are\n\nBairesDev is proud to be the fastest-growing company in America. With people in five continents and world-class clients, we are only as strong as the multicultural teams at the heart of our business. To consistently deliver the highest quality solutions to our clients, we only hire the Top 1% of the best talents and nurture their professional growth on exciting projects.\n\nWe are looking for a Data Engineer to join our Development team and participate in different projects made up of multicultural teams distributed throughout the world.\n\nWhat You Will Do\n\n Interview other Data Scientists and Data engineers to vet their technical abilities.\n Input data from those interviews into the product and help develop a predictive, fair, and enjoyable solution.\n Work together with developers, tech leads, and solution architects to build applications.\n Improve existing structures by adding new functionalities or proposing technological updates so that as a result, the impact of your contribution is significant in the core of each business.\n Improve the user experience on scalable and high availability platforms, contributing to the key differential of each business.\n\nHere\u2019s What We Are Looking For\n\n 5+ years of experience working as a developer.\n 3+ years of experience working as a Data Scientist / Data Engineer.\n Proficient with analysis, troubleshooting, and problem-solving.\n Hands-on experience with managing data loads and data quality.\n Advanced English level.\n\nHow we do make your work (and your life) easier:\n\n 100% remote work.\n Hardware setup for you to work from home.\n Flexible hours - make your schedule.\n Paid parental leave, vacation & holidays.\n Diverse and multicultural work environment.\n An innovative environment with the structure and resources of a leading multinational.\n Excellent compensation \u2014 well above the market average.\n Here you can grow at the speed of your learning curve.\n\nOur people work remotely but with a consistent and robust culture that promotes diversity and teamwork. To continue being the leading software development company in Latin America, we want to ensure that every BairesDev member gets the best growth and professional development opportunities in a diverse, welcoming, and innovative environment.\n\nEvery BairesDev team member brings something unique to our company.\n\nWe want to hear your story. Apply now!"}, {"title": "Data Engineer (Remote)", "company_name": "Cascade Debt", "location": "United States (Remote)", "link": "/jobs/view/3755336571/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=pmUtn1j2zyPr25CNPh2RNw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3755336571", "description": "About the job\nDo you want to work in a small team where you can make a real difference in a company? Would you embrace the challenge of building a high impact platform that is used globally? Then we want to speak with you!\n\nAbout Cascade\n\nCascade is a fintech startup backed by Canadian and US investors that empowers companies to grow by democratizing access to institutional debt. We are building tools that modernize how companies raise and manage debt, lowering the barriers to entry to the $7 trillion specialty finance and alternative credit market for companies around the world. Founded just last year, we are gearing up for our next phase of technical development and are seeking a talented Data Engineer to lead the way.\n\nAbout The Role\n\nAs a Data Engineer you\u2019ll be an early member of a growing team building a pioneering platform in the debt infrastructure space. You will design, build, and launch efficient, scalable, and reliable data pipelines to move and transform data .\n\nWhat You\u2019ll Do\n\n Building databases, data lakes and data ingestion pipelines to integrate customer databases and datasets to our systems \n Ingesting financial datasets from external customers, then u pdating and maintaining accurate and complete data mappings to ensure that our product s are displaying high quality data \n M onitoring and alerting across our data pipelines in order to make sure that our data ingests are reliable and correct \n Translat ing business goals and stakeholder requirements into new data flows and deliver analytical insights ensuring that the data flows create real business value continuously \n\nSkills And Experience\n\n 2 + years experience in a data engineering or data analyst role \n E xperience with relational SQL and NoSQL databases, including PostgreSQL , MySQL, MariaDB, MongoDB, Bigtable, etc. \n E xperience working with ETL technologies , such as Databricks, Fivetran , or dbt \n Proficiency in writing SQL queries and knowledge of analytical data warehouses such as RedShift, BigQuery , and Snowflake \n Some experience with CI/CD automation such as GitHub Actions \n Developing APIs and integrating with 3rd party APIs \n Bonus: experience in fintech / SaaS / credit analysis \n\nPeople Who Thrive At Cascade Are\n\n Self-starters, who take the initiative to tackle challenges in a remote work environment \n Good communicators, who can operate without ego to discuss, learn, grow, and help others do the same \n Passionate about creating great digital experiences for users \n Problem solvers who are not satisfied with the status quo \n\nBenefits\n\nRemote: we are a remote-first company and are very flexible on hours as long as things get done \nHome-office: there\u2019s a $1000 USD home office allowance to set yourself up \nEquity: we expect you to have an owner-mentality, and have the equity plan to match \nBenefits: health, dental, vision, and more \nPerks: we offer free lunches weekly and off-site trips \nJob satisfaction: we offer autonomy, ample opportunities for mastery, and an opportunity to make a difference for companies around the world"}, {"title": "Data Engineer", "company_name": "ISC (Integrated Specialty Coverages, LLC)", "location": "Carlsbad, CA (Remote)", "link": "/jobs/view/3778293528/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ew7%2FfEuXSujs5IGabdqUyw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3778293528", "description": "About the job\nAbout Integrated Specialty Coverages\n\nIntegrated Specialty Coverages LLC (ISC) is a growth stage technology and data-driven insurance company leading innovation in the market.\n\nBacked by one of the leading private equity firms, KKR, and led by a forward-thinking management team, ISC is combining the worlds of insurance and technology to create an Insurtech powerhouse. As a leading online distributor of insurance products for a range of industries and \"Main Street USA\", we are looking for the right people to help us in our mission of achieving exponential growth. We strive to be the number one place to go for brokers and agents to source insurance. To accomplish this, we're building a digitally focused team that deeply understands the intersection between user experience, data, and AI/ML to optimize the way we engage with our customers and partners.\n\nJob Summary\n\nThe Data Engineer role will be the technical liaison between multiple groups including a data science team, the engineering team, product management, and business stakeholders. You do not need any insurance knowledge prior, however, you must quickly dive deep into the insurance world and ask questions to become a subject matter expert. You will be responsible for building a data platform to facilitate the data science team. You must be a self-starter that can build out features such as a data pipeline from scratch. There will be support from both engineering and data science for any buildout.\n\nJob Responsibilities\n\nCreate and maintain optimal data pipeline architecture. \nAssemble large, complex data sets that meet functional / non-functional business requirements. \nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. \nBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using AWS technologies, SQL, and Python. \nWork with stakeholders including the Executive, Product, Data Science, and Engineering teams to assist with data-related technical issues and support their data infrastructure needs. \nWork with data science and analytics teams to strive for greater functionality in our data systems. \n\nMinimum Qualifications:\n\nAdvanced working SQL knowledge and experience working with relational databases, strong query authoring (SQL) as well as working familiarity with a variety of databases (Snowflake, Redshift, MySQL, MSSQL, etc.)\nExperience building and optimizing data pipelines, architecture and data sets. \nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. \nBuild processes supporting data transformation, data structures, metadata, dependency, and workload management. \nA successful history of transforming, processing and extracting value from large disconnected, datasets from a variety of data sources (Flat files, Excel, databases, etc.)\nStrong analytic skills related to working with unstructured datasets. \nStrong project management and organizational skills. \nExperience supporting and working with cross-functional teams in a dynamic environment. \nAbility to mentor/guide/collaborate with other team members. \n\nWe are looking for a candidate with 3-5 years of experience in a Data Engineer role. They should also have experience using the following software/tools:\n\nExperience with relational and MPP databases, including Snowflake and MySQL. \nExperience developing software in an agile environment from the requirements stage to production. \nExperience with version control: git\nExperience with container technologies: Docker\nExperience with data pipeline and workflow management tools: Airflow, Jenkins, AWS Glue, Azkaban, Luigi, etc. \nExperience with AWS cloud services: EC2, ECS, Batch, S3, EMR, RDS, Redshift\nExperience with other cloud services: Snowflake, Airflow\nExperience with object-oriented/object function scripting languages: Python, Java, C++, etc. \nExperience with data modeling and data warehouse design\nExperience with data visualization tools (PowerBI, QuickSight)\n\nThe starting annual pay scale for this position is listed below. Actual starting pay will be based on factors such as skills, qualifications, training, and experience. In addition, the company offers comprehensive benefits including medical, dental and vision insurance, 401(k) plan with match, paid time off, and other benefits.\n\nISC's salary ranges are determined by role and level. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations and could be higher or lower based on a multitude of factors, including job-related skills, experience, and relevant education or training.\n\nNational Pay Range\n\n$100,000\u2014$110,000 USD\n\nBenefits of Working at ISC\n\nCompetitive vacation and flexible working arrangements\nComprehensive and inclusive health benefits\nA variety of professional development and mentorship opportunities\nChoice of technology whether at home or in the office\n\nISC's Ownership Behaviors\n\nDo the Right Thing *Be Relentless. Pursue Excellence. *Personal Responsibility & Own Outcomes\nTry Fast. Learn Fast. Fail Fast. Think Big. *Build Extraordinary Owners & Join Forces *Grit & Determination\n\nApplicants with disabilities may contact ISC HR department via e-mail, and other means to request and arrange accommodation. If you need assistance to accommodate a disability, you may request accommodation at any time. Please contact ISC at HR@ISCMGA.com.\n\nISC believes in creating long-term relationships by being responsive and relevant and by consistently delivering value to our community of customers. Specifically, with our employees, we focus on attracting, developing, and retaining the best talent for our business, challenging our people, demonstrating a \"can-do\" attitude, and fostering a collaborative and mutually supportive environment. ISC is an equal opportunity employer and a participant in the US federal E-Verify program (US).\n\nDiversity creates a healthier atmosphere: All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, gender, gender identity, sexual orientation, marital status, medical condition, genetic information, mental or physical disability, military or veteran status, or any other characteristic protected by local, state, or Federal law."}, {"title": "Data Engineer -Remote", "company_name": "RIT Solutions, Inc.", "location": "Denver, CO (Remote)", "link": "/jobs/view/3768007496/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=1hQH8EDaII1eJ7nQ9BACQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3768007496", "description": "About the job\nDuration: 6+ months CTH\n\nMust have a valid LinkedIn profile\n\nRequired Skills\n\n 3-6 years of experience (no more than 6 years)\n Start up environment experience\n Python, SQL, and DBT"}, {"title": "Big Data Engineer - W2", "company_name": "Megan Soft Inc", "location": "United States (Remote)", "link": "/jobs/view/3674845652/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fCAxh4N%2F4YZJ5aEIASaeeA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3674845652", "description": "About the job\nTitle: Big Data Engineer - W2\n\nLocation: Dearborn, MI (Remote)\n\nDuration: 12+ Months\n\nDirect Client: Ford Motors\n\nMust have skills: Should be able to handle GCP requirements\n\n\"Translates business requirements to build repeatable, sustainable, efficient, coded processes that can be productionized by Software Engineers and readily modified by other Data Engineer Creates POC processes in Dev/QA and works with Software Engineers to productionize and align best coding and process practices that maximize efficiency, speed, stability, system resources and capabilities Awareness of and compliance with data privacy, security, legal and contractual guidelines Incorporates data quality and privacy checks/alerts to minimize bad data being consumed by end users, models and dashboards, and to protect customer data; revises checks for new data issues Validates data products and pipelines are functioning as expected following system or application upgrades, source changes, etc. Works closely with Architects if required to align systems, tools and applications being utilized with business use case and performance requirements Provides data product support and maintenance Establishes, tracks and monitors KPIs related to specific data products and deliverable Participate in iteration cadence activities (Iteration Planning, task estimation, daily scrum, demos, etc.). Will be a part of on-call team to resolve critical production support tickets during business hours and off hours.\n\nSkills Required\n\nExperience with Big Data Technologies (Hadoop , ETL Tool, Spark. Hive etc..) Someone who understands Cloud and has experience working with Cloud Platforms (preferably GCP) Should be able to handle GCP requirements - ingesting into GCS/ Big Query, working with GCP components Hands-on in Data Flow, Cloud Storage (GCS), Big Query and GitHub. Proficient in Python and SQL.\n\nSkills Preferred\n\nManages ETL: uses programming and tools for data ingestion, configures pipelines, applies transformations and decoding, integrates and fuses data, moves and securely deliver Experience NoSQL databases\n\nAdditional Information\n\nIf the candidate is remote only, please indicate \"\"100% Remote\"\" under candidate's name on resume. Other similar schedule notations can be \"\"Local Candidate\"\" or \"\"Hybrid Candidate.\"\" ***HRA Test Required***\""}, {"title": "Remote Opportunity: Quantexa Data Engineer", "company_name": "SPAR Information Systems LLC", "location": "United States (Remote)", "link": "/jobs/view/3680073191/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ioykVCZSOQ7BK5%2FwRQHOWg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3680073191", "description": "About the job\nHello All, Hope you are doing great!! Please go through the job descriptions and let me know your interest. Role: Quantexa Data engineer  Location: Remote Duration: Long Term Contract Job Description: 1. End to End usage of Quantexa including guiding our client team on how to use from both front end and data ingestion Reason they are asking for Certification 2. Resolve Entity issues with data 3. Knowledge of Scala and Hive which requires coding experience using parquet 4. Advanced SQL knowledge requiring writing SQL queries 5. Experience having worked with GCP data lake 6. Quantexa certification required\n\nThanks & Regards,\n\nSatnam Singh\n\nDirect: 201 623 3660\n\nEmail : Satnam.singh@sparinfosys.com"}, {"title": "Remote Work - Need Staff Data Engineer - USC only", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3746283980/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=CvIuLBNO%2BPMGi4IXVRkP0A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3746283980", "description": "About the job\n100% Remote, client is in NY\n\nLook for strong job history, Python, Machine Learning, SQL, 10/10 comms, etc. \n\nMUST be an Engineer \n\nSummary\n\nWe're looking for an experienced Data Engineer to join our client's growing team of top notched IT professionals. You will work closely across multiple groups including Editorial and Audio, Marketing, Advertising, etc. to achieve their goals and objectives. The Data Engineering team builds tools throughout the stack while sharing knowledge across these departments.\n\nAs the company grows, they're looking for Data Engineer who will help solidify and expand our pipelines and maintain their data warehouse. Their business model is based on performing complex and very detailed analyses of how our products perform with their subscribers. You'll be responsible for working alongside analysts, product, marketing, and other teams to define new data collection components and measurement schemes that support each new product and feature.\n\nHow Can Add Value To Their Mission\n\nCreate and maintain data pipelines to provide insights and drive business decisions\nEstablish data warehousing strategy (ex. Kimball, Data Vault, etc.)\nMaintain data infrastructure on our AWS accounts\nCollaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.\nWrite unit/integration tests, contributes to engineering wiki, and documents work\nImplement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it\n\nWhat You'll Need To Succeed\n\n7+ years of industry experience measuring product performance and user behavior\nExperience working with a variety of data management technologies, including RedShift, Kinesis/Kafka, Glue, Spark, Postgres, Airflow, dbt, and others.\nExperience implementing BI reporting tools such as Looker\nExperience interfacing with engineers, product managers and analysts to understand data needs\nKnowledge of a variety of measurement beacons, SDKs, APIs including Google Analytics, Amplitude, Braze, Stripe, email service providers\nA commitment to building secure, resilient, fault-tolerant architectures, with clear documentation and procedures in place for support\nA focus on accuracy and detail as you build, ensuring data pipelines produce clear, predictable results\nUnderstanding of the typical metrics a subscription and advertising-supported business needs to measure success\nExperience with Client techniques as applied to behavioral segmentation or anomaly identification is a definite plus\nFamiliarity using developer tools that increase productivity and facilitate the development of resilient code (eg. Docker, CircleCI, Serverless) is a plus"}, {"title": "ENTRY LEVEL DATA ENGINEER (REMOTE)", "company_name": "SynergisticIT", "location": "Jacksonville, FL (Remote)", "link": "/jobs/view/3767587909/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=om7L3CnowiC4PLIcpZD70A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3767587909", "description": "About the job\nSYNERGISTICIT wants every candidate to know that the Job Market is Challenging and to stand out, you need to have exceptional skills and technologies and that's where we come in to make sure you get the attention which you need\n\nSynergisticIT understands the complex nature of the job market and how difficult it can be to secure a position, especially for fresh graduates. Therefore, we assist and help tech-savvies to convert their passions into professions. We go above and beyond to keep you working in your niche.\n\nAs we focus on long-term success, we provide complete career development solutions. From job search to upskilling portfolio and interview preparation, we can guide you at every step of your career.\n\nSynergisticIT spares no efforts to connect you with a large network of tech giants, including Google, Apple, PayPal, Dell, Cisco, Client, etc. Presently, we are actively looking for ENTRY LEVEL DATA ENGINEER a driven mindset. Get the right opportunity and gain experience in building web-centric solutions on Java.\n\nWho Should Apply : Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or anyone looking to make their career in IT Industry\n\nWe also assist in filing for STEM extension and H1b and Green card filing.\n\nCandidates who are serious about their future in the IT Industry and have set big goals for themselves.\n\nCandidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. We also offer Skill Enhancement Programs if the candidates are missing skills or experience which our clients need with great outcomes\n\nCandidates can benefit from skill enhancement if they fall into the below categories. If they are qualified with enough skills then no need for skill enhancement\n\nCandidates Who Lack Experience\n\nHave had a break in careers\n\nLack Technical Competency\n\ncandidates who want to get employed and make a career in the Tech Industry \n\nPlease Also Check The Below Links\n\nhttps://www.synergisticit.com/candidate-outcomes/\n\nhttps://www.synergisticit.com/java-track/\n\nhttps://www.synergisticit.com/data-science-track/\n\nhttps://www.synergisticit.com/which-is-the-best-option-for-tech-job-seekers-staffing-companies-consulting-companies-bootcamps-or-synergisticit/\n\nhttps://www.synergisticit.com/contact-us/\n\nIf the skills are not a match candidates can opt for Skill enhancement. Or their resume can be sent out to clients to see if responses are achievable\n\nREQUIRED SKILLS For Java/Software Programmers\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Core Java , javascript , C++ or software programming\nSpring boot, Microservices and REST API's experience\nExcellent written and verbal communication skills\n\nFor data Science/Machine learning\n\nRequired Skills\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Statistics, Python, data visualization tools\nExcellent written and verbal communication skills\n\nPreferred skills: NLP, Text mining, Tableau, Time series analysis\n\nTechnical skills are required by clients for selection even if its Junior or entry level position each additional Technical skill helps a candidate's resume to be picked by clients over other job seekers.\n\nClients hire candidates with the right technical skills which they need and reject candidates who lack the required technical skills.\n\nNo third party candidates or c2c candidates\n\nPlease apply to the posting\n\nNo phone calls please. Shortlisted candidates would be reached out"}, {"title": "Data Engineer - Remote | WFH", "company_name": "Get It Recruit - Information Technology", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3782241984/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=BgBf1kSAAX%2BSuw6inJsIVw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3782241984", "description": "About the job\nJoin an innovative team at a leading company in the engineering and manufacturing industry, where we are committed to leveraging data for continuous improvement and operational excellence. This hybrid position allows you to enjoy the flexibility of remote work on Mondays and Fridays.\n\nWhy Join Us\n\nAt our company, we prioritize your well-being and professional growth. We offer a comprehensive benefits package, including excellent health benefits, a 6% 401(k) match, generous paid vacation and sick time, 12 paid holidays, and more.\n\nRole Overview\n\nAs a Data Engineer, you will play a pivotal role in transforming raw data into actionable insights, driving improvements in engineering and manufacturing processes, enhancing product quality, and optimizing operational efficiency. Collaborate with dynamic cross-functional teams to extract, analyze, and visualize data, influencing strategic decisions through your valuable insights.\n\nKey Responsibilities\n\nData Collection and Integration:\n\nGather, clean, and integrate data from diverse sources, including production databases, sensor data, and external datasets.\n\nData Analysis\n\nConduct exploratory data analysis to identify patterns, trends, and anomalies.\n\nPerform statistical analysis and hypothesis testing to derive meaningful insights.\n\nData Visualization\n\nCreate informative and visually appealing dashboards and reports using tools like Tableau, Power BI, or Python libraries.\n\nPredictive Modeling\n\nDevelop predictive models to forecast manufacturing performance, quality, and operational outcomes.\n\nRoot Cause Analysis\n\nInvestigate production issues and quality concerns by analyzing data to identify root causes and recommend corrective actions.\n\nPerformance Monitoring\n\nEstablish and maintain key performance indicators (KPIs) to track manufacturing and engineering metrics.\n\nGenerate regular reports to communicate performance trends to stakeholders.\n\nCross-functional Collaboration\n\nCollaborate with engineering, manufacturing, and quality assurance teams to understand their data needs and provide analytical support.\n\nContinuous Improvement\n\nSuggest process improvements and optimizations based on data analysis.\n\nData Security And Compliance\n\nEnsure data security and compliance with relevant regulations, such as GDPR or industry-specific standards.\n\nInfrastructure And Development\n\nBuild and maintain the infrastructure for collecting, storing, and processing data.\n\nDesign, develop, and manage data pipelines, ETL processes, and data warehouses.\n\nRequired Experience\n\nProven experience as a Sr. Data Analyst and/or Data Engineer in an engineering or manufacturing environment.\n\nProficiency in data analysis tools and programming languages, such as Python, R, SQL, and Excel.\n\nStrong data visualization skills with tools like Tableau or Power BI.\n\nExcellent knowledge of statistical analysis and machine learning techniques.\n\nAbility to communicate complex findings and insights effectively to both technical and non-technical stakeholders.\n\nStrong problem-solving skills and attention to detail.\n\nFamiliarity with manufacturing processes and quality control is a plus.\n\nExperience with enterprise ERPs (SAP S4HANA, Oracle EBS), CRMs (Salesforce, Hubspot), and IoT.\n\nProficient in working with cloud computing platforms including AWS, GCP, and Azure.\n\nExpertise in configuring and optimizing cloud-based data solutions for scalability, reliability, and performance.\n\nBachelor's Degree in Data Science, Statistics, Engineering, or Computer Science Required.\n\nApply Now and be part of a team shaping the future of data-driven innovation in engineering and manufacturing!\n\nEmployment Type: Full-Time"}, {"title": "Entry Level Software Engineer - Data Backend Engineer (Remote - Canada)", "company_name": "Yelp", "location": "United States (Remote)", "link": "/jobs/view/3752881757/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fankeoxifvhpTzOYvowHYQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3752881757", "description": "About the job\nYelp engineering culture is driven by our values: we\u2019re a cooperative team that values individual authenticity and encourages creative solutions to problems. All new engineers deploy working code their first week, and we strive to broaden individual impact with support from managers, mentors, and teams. At the end of the day, we\u2019re all about helping our users, growing as engineers, and having fun in a collaborative environment.\n Interested in exploring data and finding innovative ways to collect it, curate it, and make it easily accessible for others to use? How about doing that at an enormous scale? Yelp\u2019s datasets contain billions of interactions between users and local businesses around the globe. If working with our vast amount of data sounds exciting, come and join us!\n\nThe Marketplace Data Observability team is responsible for managing, governing, and monitoring all of Yelp\u2019s Ads data to protect our revenue stream and also support offline analysis that aids in making long term company decisions. We build tools that help us to better understand our data quality, and we strive to uncover data issues before they impact our consumers.\n\nAs a Data Backend Engineer on this team, you will be responsible for building elegant and scalable data products that serve critical, up-to-date, structured information to support different types of analytics within Yelp. As a core contributor to our growing data modeling and data warehousing engineering efforts, you will help design and own mission-critical data flow pipelines and datastores to enable decisions including effective A/B testing and company investments.\n This opportunity is fully remote and does not require you to be located in any particular area in Canada. We welcome applicants from throughout Canada. We\u2019d love to have you apply, even if you don\u2019t feel you meet every single requirement in this posting. At Yelp, we\u2019re looking for great people, not just those who simply check off all the boxes.\n  Build systems that can effectively store and crunch terabytes of data.\nDesign and develop data models for efficient data storage, retrieval, and reporting.\nCreate and maintain conceptual, logical, and physical data models using industry-standard modeling tools.\nCollaborate with cross-functional teams, including engineers, data analysts, business analysts, and data scientists, to understand data requirements and translate them into effective data models.\nParticipate in data integration efforts, including ETL processes and data migration.\nStay up-to-date with industry best practices, emerging technologies, and trends related to data warehousing.\nSupport on-call rotations as needed to operate the team.\n  Understanding of high performing and scalable data systems.\nExperience in building and orchestrating ETL pipelines.\nExperience with Data Lake or Data Warehouse landscape.\nA hunger for tracking down root causes and fixing them in systematic ways.\nAbility to communicate effectively to technical and non-technical cohorts alike. \nExposure to some of the following technologies: Python, AWS Redshift, AWS Athena / Apache Presto, Big Data technologies (e.g S3, Hadoop, Hive, Spark, Flink, Kafka etc), NoSQL systems like Cassandra, DBT."}, {"title": "Data Engineer (Associate Consultant), June 2024", "company_name": "UDig", "location": "Nashville, TN (Remote)", "link": "/jobs/view/3716039960/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=XUjo40kU55HnDCqJ%2F3FdPg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3716039960", "description": "About the job\nCan UDig It? \n\nUDig designs, builds, and implements technology solutions that deliver on business objectives. People join our team because they are excited to have an impact on the future of our organization, passionate about delivering meaningful results, and energized by solving complex problems. Oh, and they like working with other driven, smart, and authentic people!\n\nUDig is proud to be recognized as a \"Best Places to Work\" in Virginia and Tennessee. We're committed to fostering a transparent environment where leadership is accessible, professional development opportunities are abundant, and you are empowered to achieve your career goals. You'll collaborate, develop deep relationships with teammates, and accelerate your success while having fun!\n\nIf you're graduating in May 2024 and excited by the idea of impacting organizations by building data solutions to achieve business goals, you should consider joining our growing technology team.\n\nHow Things Go Down\n\nAs a new, entry-level teammate, you'll take part in our \"Breaking Ground\" training program designed to help you quickly get up-to-speed on UDig's delivery approach within a project team. The program covers:\n\nUDig delivery process and methodologies such as gathering requirements and delivering projects using agile\nTechnical guidance and instruction on the best practices from Data leaders at UDig\nDelivering executive and technical presentations\nCoaching to prepare you to for success on your first client project at UDig\n\nA typical day might entail:\n\nProject standups and check-ins\nDesign and development of data pipelines, BI reports, and more\nPlanning and participating in project demos\nContributing to internal UDig projects\nCareer and job growth through UDig-provided training\nCreating long-lasting friendships with teammates through fun activities as well as your projects (including social events, winning a prize for guessing the weight of pumpkin, or sharing experience on cool tech side projects)\n\nUDig believes it is important for Associate Consultants going through our Breaking Ground program to reside in one of our major markets (Richmond, VA or Nashville, TN metro area) for at least the first year of employment.\n\nWant to learn more? Hear directly from our teammates by checking out our 'A Day in the Life' blog series: https://www.udig.com/digging-in/breaking-ground-2023/\n\nSound Interesting? Here's the Technical Piece\n\nBelow are a few boxes you should be able to check if\u202fyou're interested: Don't check every box? Go ahead and apply!\n\n1+ year of experience with relational database management systems such as Microsoft SQL Server, MySQL, or Oracle\nUnderstanding of the full lifecycle of design, development, and implementation of data solutions including architecture design, development, testing\nSome experience in ETL development or reporting is a plus\nAbility to present technical ideas and high-level concepts and solutions to internal and external team members with varying degrees of technical knowledge\nBachelor's Degree in in computer science, information technology, or any science or business discipline\nAbility to take your work seriously but also seriously enjoy a good time with your UDig colleagues\n\nWhat's in It For You? \n\nAt UDig, you will build your career alongside talented and experienced developers while gaining experience and having fun! We provide continuous challenges and professional development opportunities. Additionally, we offer: \n\nCompetitive salary, merit reviews, and career advancement paths\nFlexible, hybrid environment\nIndividual $1500 Training Budget\nRegular team building and social activities (virtual and/or in-person)\nTransparent culture with strong communication and access to leadership\nGreat benefits like\nGenerous Paid Time Off, company holidays, and parental leave\nMultiple Single and Family Health Insurance plans to choose from\nDental & Vision coverage\nShort-Term and Long-Term disability\nOptional accident and critical illness coverage\nMatching 401(k)\nand more!"}, {"title": "Expression of Interest: Data Engineer", "company_name": "Fingerprint for Success (F4S)", "location": "New York, NY (Remote)", "link": "/jobs/view/3787769880/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=l%2FEDpQqUNmCttGdIyTk7Cg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787769880", "description": "About the job\nWe are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.\n\nThe F4S Talent Pool is a pilot project designed to:\n\nHelp job seekers get discovered by our partners based on their anticipated hiring needs.\nProvide optional support and resources for job seekers in their career endeavors.\nHelp individuals understand, and bring out the best in themselves and each other.\n\nThe F4S Talent Pool process:\n\nOnce you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.\n\nAbout Fingerprint For Success (F4S)\n\nBacked by 20+ years of research, F4S\u2019s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high performing teams.\n\nKeep in mind, joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your F4S work style assessment to match the hiring needs of our partners.\n\nYour feedback is a gift! Write to us via:\n\nPowered by JazzHR\n\noWuPJ9lwq9"}, {"title": "Data Engineer Seattle, WA (remote)", "company_name": "Conch Technologies, Inc", "location": "United States (Remote)", "link": "/jobs/view/3765052211/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=SBETI78UMutCHuRp5FnP%2FA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3765052211", "description": "About the job\nHi,\n\nGreetings from Conch Technologies Inc\n\nData Engineering Lead \u2013 With Databricks and Unity Catalog\n\nLocation \u2013 Seattle, WA (remote)\n\nDuration \u2013 12 months\n\nNeed: Design and implement data architecture and strategy, leveraging technologies such as Databricks, Unity Catalog, Privacera, and Collibra.\n\nAs a Data Engineering Lead, you will play a pivotal role in defining and executing technical deliverables while assessing complexities and levels of effort. You'll engage with both vendor and internal teams to resolve technical challenges and ensure seamless collaboration. Your leadership will extend to planning and prioritization with the SCRUM master, facilitating effective coordination between onsite and offshore teams, and driving feature prioritization through collaboration with product managers.\n\nKey Responsibilities\n\n Lead the definition and breakdown of technical deliverables, assessing complexities and levels of effort.\n Collaborate with vendor and internal teams to address technical issues.\n Drive planning and prioritization in conjunction with the SCRUM master.\n Coordinate with onsite and offshore teams for timely project delivery.\n Collaborate with product managers to prioritize features, communicate with stakeholders, and clarify requirements.\n Collect and manage user-reported issues, ensuring prompt resolution and communication.\n Define comprehensive test plans, oversee their execution, and document results.\n\n--\n\nWith Regards,\n\nNagesh G\n\nMobile: 408-381-5645\n\nDesk: 901-313-3066\n\nEmail: nagesh@conchtech.com \n\nWeb: www.conchtech.com"}, {"title": "Remote Work - Need Azure Fabric Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3759325599/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=C3iVjmcR9wjUip1nNjUiCg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3759325599", "description": "About the job\nThe client is placing heavy emphasis on Microsoft and Azure Fabric experience\n\nInformation Management Engineer \u2013 Take the identified information management sources and assist migration to the Client Data Lake, Azure Fabric. The engineer will transform information management (de-dup, clean and enrich).\n\nTake the identified data sources and bring them into our Data Lake, Azure Fabric. Once the data is in, the engineer will need to be able to transform data (de-dup, clean and enrich the data). We are focused on using a medallion data model for data architecture. The tools we are working with are: Delta Parquet files, python, spark, notebooks, T-SQL, Fabric lakehouse/warehouse, Dataflows Gen2, Data Pipelines, Direct Lake, shortcuts, ADF."}, {"title": "Data Engineer", "company_name": "Catalist", "location": "Washington, DC (Remote)", "link": "/jobs/view/3744951279/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ui1Fk6ICmnPNqAlZDZAcwA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3744951279", "description": "About the job\nFor over 17 years, Catalist has been a leader in civic data and data science innovation. Our mission is to provide progressive organizations with the data, software, and services needed to better identify, understand, and communicate with the people they need to engage and mobilize. Our clients include the largest, most influential organizations in the U.S. active in civic engagement, advocacy, and political campaigns.\n\nCatalist is home to a dedicated, creative team of technologists, data scientists, and campaign experts committed to using our talents and technology to nurture a vibrant and growing progressive community.\n\nAs a Data Engineer at Catalist, you will work closely with the Analytics and Technology departments to design, build, support, and maintain various data pipelines and processes with an end goal of providing data and intelligence to the progressive community. This role primarily involves translating data architecture designs into functional processes, code, and systems.\n\nThe ideal candidate will be a highly motivated individual with excellent technical skills, a strong desire to learn new skills, and an interest in progressive politics. Catalist values creativity and problem-solving. Our work is on the cutting edge of data-driven politics, and your support will help Democratic candidates and progressive organizations conduct successful advocacy and electoral campaigns.\n\nThis position reports to the Deputy Chief Data Officer. The Data Engineer is a part of a growing Data team that supports all underlying work at Catalist.\n\nThis position is included in our CWA bargaining unit.\n\nPrinciple Duties & Responsibilities\n\nDevelop scalable, production processes, code, and systems for data ingest, transformation, modeling, and reporting across a variety of platforms\nTranslate mock-ups and designs into functional processes, code, and systems\nCreate architecture designs when needed\nProvide quality assurance and testing on processes, code, systems, and products\nBecome an internal subject matter expert on various datasets and support other Catalist departments/teams on usage of those datasets\nExecute ad-hoc data and database maintenance tasks as requiredProject manage cross-departmental efforts, with direct responsibility for stakeholder engagement, management, and execution of technical elements\n\n\nRequirements\n\nBS or BA in a technical field, or relevant experience\n1-2 years of experience working with SQL databases\nExperience with data cleaning or ETL processes\nExperience with distributed computing systems and/or distributed datastores (particularly the Hadoop ecosystem)\nExperience managing projects\nFamiliarity with Catalist data, progressive politics, voter files, and/or commercial data\nBackground check required\n\n\nPreferred Skills & Abilities\n\nWillingness to be a problem solver and produce results in a fast paced environment\nAbility to be creative and personable, and articulate ideas clearly\nExperience working with SQL databases\nProficiency with Python or another object-oriented programming language (R, Java, Scala, etc\u2026)\nExperience working in cloud environments (AWS, GCP, etc.)\nExperience working in command line environments such as Bash\nExperience with a version control tool such as git or github\n\n\nBenefits\n\nMedical, Dental, Vision, Prescription Drug\n\nCatalist offers Medical, Dental, Vision, and Prescription Drug coverage for eligible staff and their eligible dependents. Catalist\u2019s Medical plan is a comprehensive PPO program including Prescription Drug coverage with 85% of the premium paid by Catalist. Dental and Vision coverage is provided at no cost to employees.\n\nGroup Term Life Insurance and Long-Term & Short-Term Disability Coverage\n\nGroup Term Life Insurance and Long-Term and Short-Term Disability coverage is available for eligible staff. These benefits are provided at no cost to Catalist employees.\n\n401(k) Safe Harbor Plan\n\nA 401(k) Safe Harbor Plan is available to eligible staff with a 3% contribution from Catalist from the date of hire. Employees may contribute pre-tax or post-tax from their salary up to the legal limits set forth by the IRS.\n\nMedical and Dependent Care Flexible Spending Accounts (FSAs)\n\nCatalist offers an FSA Program that gives eligible staff the ability to pay out-of-pocket medical/dental/vision/child care expenses from pre-tax earnings.\n\nTransit Benefits\n\nCatalist also makes available a Transit benefit FSA program to eligible employees using pre-tax contributions with a company match.\n\nProfessional Development and Remote Work Expenses\n\nEligible employees may be reimbursed up to $750 each year for professional development / education and remote work expenses.\n\nStudent Loan PayDown or SaveUp\n\nCatalist offers a Student Loan PayDown and College SaveUp benefit for eligible staff.\n\nVacation, Personal Leave, Sick Leave Benefits\n\nCatalist offers generous vacation benefits to all eligible staff. Eligible employees also receive:\n\n 14 Paid Holidays\n Personal Days\n Sick Leave\n Parental Leave\n\n\nHybrid Office/Remote Work\n\nCertain positions at Catalist are eligible for Office/Remote Hybrid or full Remote status."}, {"title": "Remote Entry Level Data Analyst/Engineer", "company_name": "SynergisticIT", "location": "White Plains, NY (Remote)", "link": "/jobs/view/3774171423/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=q54sk1%2FRww0z%2BC%2BE8e0jQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774171423", "description": "About the job\nAt SynergisticIT, we're all about making connections. Whatever IT goals you have, our software programmers can help achieve those. Our Software development teams can take up turnkey projects and execute them in an effective and efficient manner. If you are looking to source talent our recruiters will find the ideal IT talent for your company. What's the secret to our success? Well, it all starts with taking quality time to listen to each client's specific needs. After we have a thorough grasp of your IT goals, we can better customize our Developments as per your specific needs. We can also tailor make recruiting programs to exceed your expectations. Since our founding in 2010, SynergisticIT's strategies have earned the company an enviable position in the software development, IT staffing and IT skill enhancement fields. SynergisticIT continues to work with hundreds of satisfied American clients with our software programmers working on our projects and after gaining hands on experience on cutting edge technologies moving to contribute their skills to great clients like Apple, Google, Client, Ebay, Paypal, Kroger, the Walt Disney Company and hundreds more. If you are tired of working with inefficient programmers who take a lot of time to ramp up we want you to try us. Our software programmers can hit the ground running and get you the maximum return on your investment. You have already tried the rest its time you tried the best. SynergisticIT - Home of the Best Data Scientists and Software Programmers in the Bay Area.\n\n Why Us ? \n\nSynergisticIT has a proven track record of successfully skill enhancement and staffing IT employees for some of the world's most iconic brands. Our team takes the time to fully understand every client's needs so we could best meet your IT staffing requirements. The knowledgeable staff at SynergisticIT is always more than happy to work with clients to ensure they reach their software development goals. Besides staffing, SynergisticIT is also committed to helping young IT professionals advance their career with a robust upskill program . Everyone who goes through SynergisticIT's program learns all the skills necessary to succeed in many IT fields ranging from Java to Machine Learning. Additionally, everyone trained at SynergisticIT has been through extensive mock and technical interview screenings to bolster their career prospects. Last, but certainly not least, SynergisticIT takes great care to respect the privacy considerations for every client. All companies who work with SynergisticIT can rest assured their confidential data is protected using the most up-to-date encryption technologies. SynergisticIT also complies with all the latest NDA agreements.\n\n REQUIRED SKILLS For Java /Software Programmers \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Time series analysis \n\n We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2022 and at Gartner Data Analytics Summit (Florida)-2023 \n\nOracle CloudWorld Event (OCW) Las Vegas 2022 | SynergisticIT - YouTube\n\n https://www.youtube.com/watch?v=OAFOhcGy9Z8 \n\n https://www.youtube.com/watch?v=EmO7NrWHkLM \n\n https://www.youtube.com/watch?v=NVBU9RYZ6UI \n\n https://www.youtube.com/watch?v=Yy74yvjatVg \n\nSynergisticIT at Gartner Data and Analytics Summit 2023 - YouTube\n\nFor preparing for interviews please visit  https://www.synergisticit.com/interview-questions/ \n\n We are looking for the right matching candidates for our clients \n\n Please apply via the job posting \n\n REQUIRED SKILLS For Java /Full Stack/Software Programmer \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning Positions \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Tensorflow \n\n If you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team. \n\n No phone calls please.  Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates"}, {"title": "Data Engineer", "company_name": "Anika Systems", "location": "Leesburg, VA (Remote)", "link": "/jobs/view/3787766893/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=mApL9JnN%2B%2Fr2%2BYUPlxRYOA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787766893", "description": "About the job\nAnika Systems is a fast growing, woman-owned small business that specializes in providing innovative IT solutions for federal government agencies. Our expertise lies in accelerating delivery in Data and Analytics, Intelligent Automation, Application Development, and IT Modernization. We are currently expanding our Federal team and are seeking a passionate and talented Data Engineer. This opportunity is 100% remote.\n\nMust be a U.S. Citizen with the ability to obtain and maintain a government suitability clearance\n\nResponsibilities\n\nDesigns, develops, builds, analyzes, evaluates, and installs database management systems to include database modeling and design, relational database architecture, metadata, and repository creation and configuration management.\nDefines and oversees database organizations, standards, controls, procedures, and documentation. Provides technical consulting in the definition, design, and creation of a database environment.\nAdvises applications development staff and users on data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nDesigns and implements databases with respect to access methods, access time, batch processes, device allocation, validation checks, organization, protection and security, documentation, and statistical methods.\nUses data mapping, data mining, and data transformational analysis tools to design and develop databases.\nDetermines data storage and optimum storage requirements.\nPrepares system requirements, source analysis, and process analyses and designs throughout the database implementation.\n\nRequired Skills And Experience\n\nBA/BS and 1 year of relevant experience\nExperience with DataBricks, SQL, Python\nApplications development experience with data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nExcellent communication skills (both oral and written) with process-oriented organizational skills to ensure project success\n\nDesired Skills And Experience\n\nDemonstrated successful development experience with the full life cycle of government database management systems including database modeling and design, relational database architecture, metadata and repository creation, and configuration management.\n\nPowered by JazzHR\n\nTlKWIorucJ"}, {"title": "Remote Work - Need Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3774712978/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=r3Bp%2F0qcOD2OpC5jpbUOcA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774712978", "description": "About the job\nTitle : Data Engineer\n\nLocation : Remote\n\nDuration : 6+ months to start (long term contract (work order are extended 2x year))\n\nPlease Fill out the skill matrix: Technology Some Awareness (1) Novice (2) Intermediate (3) Advanced (4) Expert (5) SQL Tableau Desktop /Creator Tableau API Integration AWS - S3 Glue AWS - Athena Redshift AWS - Lambda AWS - SNS/SQS msExcel\n\nProfile: Data Analyst will work closely with Architects and leads in the Interflow team, key business stakeholders including program staff to support the organizations effort in the areas of data Analysis and information management. Role should have strong experience in writing SQL queries to extract data for reports. Candidate should have experience working with large set of data. As these reports are very critical for the company and we are looking for an expert who is capable of handling, consuming and extracting large sets of data for analysis and reports.\n\nMust Haves\n\nAWS - Athena\nAWS - S3\nGlue\nhands-on development of reporting applications\nMS Excel\nRedShift\nSQL\nTableau Desktop /Creator\n\nNice\n\nAWS - Lambda\nAWS - SNS/SQS\nTableau API Integration\n\nSkills\n\nBA/BS required (major in an analytical field desired) \nA minimum 10+ years work-related experience. \nExperience in hands-on development of reporting applications for the Web in a professional environment \nThe Ethos of continuous improvement and interest in learning new things.\nStrong analytical thinking and structured problem-solving ability\nAbility to handle multiple projects and assignments simultaneously and effectively in a cross-functional team environment. \nVersed on the agile methodology and best practices. \nExcellent interpersonal and collaboration skill with the ability to work with a diverse set of colleagues, across functions, from different organizations, disciplines, etc. \nSelf-starter, ability to set priorities, work independently and attain goals"}, {"title": "Data Engineer", "company_name": "Team Remotely Inc", "location": "Houston, TX (Remote)", "link": "/jobs/view/3786249829/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=78lY5Jo%2FfyRPC6TQYCdvww%3D%3D&trk=flagship3_search_srp_jobs", "id": "3786249829", "description": "About the job\nThis is a remote position.\n\n Data Engineer (US/Canada Residents Only, 1 year experience, remote)\n\nTeam Remotely Inc. is a staffing and recruitment agency that offers a comprehensive solution for talent acquisition, including sourcing, vetting, pay rolling, and managing talent. Whether you need contract staffing, direct hire, direct sourcing, talent pools, or diversity initiatives, our model can support your hiring strategy.\n\n Hiring Type: Full-Time\n\n Base Salary: $55K-$70K Per Annum.\n\nResponsibilities\n\nDesigning user interface changes for web-based DB applications. \nReviewing application requirements and interface designs. \nDeveloping and implementing highly responsive user interface components using react concepts. \nWriting application interface codes using JavaScript following react.js workflows. \nTroubleshooting interface software and debugging application codes. \nDeveloping and implementing front-end architecture to support user interface concepts. \nMonitoring and improving front-end performance. \n\nQualifications And Experience\n\nBachelor\u2019s degree in computer science, information technology, or a similar field. \nPrevious experience working as a react.js developer. \nIn-depth knowledge of JavaScript, CSS, HTML, jQuery, and front-end languages. \nKnowledge of REACT tools including React.js, Webpack, Enzyme, Redux, and Flux. \nExperience with user interface design. \nKnowledge of performance testing frameworks, including Mocha and Jest. \nExperience with browser-based debugging and performance testing software. \nTroubleshooting skills. \nProject management skills. \nProblem-solving skills. \nVerbal communication skills. \n\n Why work with Team Remotely?\n\nTeam Remotely Inc. is a staffing platform offering a seamless experience for employers and candidates. Employers can post job openings and specify their requirements, while candidates can create profiles and upload resumes.\n\nThe team of Team Remotely continuously learns and adapts based on previous successful placements, constantly improving its matching capabilities. This ensures that the recommendations provided by Team Remotely are tailored and accurate, increasing the likelihood of a successful match between employers and candidates. By providing intelligent and data-driven solutions, they strive to enhance the efficiency and effectiveness of the hiring process, ultimately helping companies find the best talent and individuals find their dream jobs."}, {"title": "Data Engineer", "company_name": "ZAG Technical Services", "location": "United States (Remote)", "link": "/jobs/view/3771403464/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=Hf0VefrKXBFIL5MtHmY0Pw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3771403464", "description": "About the job\nWe are looking for a tech savvy Data Engineer to contribute to the building, operating, and scaling of next generation data platforms and tools that will power data-driven analytics capabilities throughout the ZAG products and services portfolio, spanning areas such as business intelligence, reporting, and data analytics.\n\nThe ideal team member is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. Must be comfortable supporting the data needs of multiple clients and teams, systems, and products within the modernization of ecosystems. The right-fit person will be excited by the prospect of optimizing or even re-designing data processes to support next generation of advanced analytics capabilities, which in-turn translates to implementing strategic enhancements to maintain competitive advantage in the industry.\n\nThe successful team member will leverage their proficiency to\u2026\n\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\nBuild the analytics processes required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Data Management and cloud based \u2018big data\u2019 technologies.\nBuild analytics tools that utilize technologies to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.\nCollaborate with client teams to identify and solution regional and local requirements within the context of the standard global business model for enterprise analytics.\nParticipates in meetings, discussions, strategy sessions where changes, improvements, and enhancements are proposed, evaluated, and approved. \n\n\nThe exceptional team member will have or demonstrate progressive experiences in\u2026 \n\nBachelor\u2019s degree or higher in Computer Science or related field\nExperience working with BI and data warehousing tools\nExperience working with enterprise data, where security is paramount and data governance is critical\nAbility to work independently and in a team environment, and effectively engage all levels of the organization\nUnderstanding of distributed systems driving large-scale data processing and analytics with a successful history of manipulating, processing, and extracting value from large, disconnected datasets.\nAbility to communicate effectively (listening, presenting, and questioning)\nStrong organizational, written, and communication skills\n\n\nAbout ZAG\n\nZAG Technical Services empowers our clients\u2019 success through intelligent business solutions that increase employee productivity, network security and team collaboration. We aim to ensure that our client\u2019s IT investments drive ROI, provide a competitive advantage and help businesses grow. We do this by living our core set of values: Integrity Foremost, Client-Centered, Accountable Always, Teamwork Throughout, and Exceptional is the Goal.\n\nWhat ZAG Has To Offer\n\nCompetitive compensation package \nBenefits including medical, dental, vision, 401K and life insurance \n\n\nZAG Technical Services is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all of our employees.\n\nZAG Technical Services is an E-Verify participant and will provide the Social Security Administration (SSA) and, if necessary, the Department of Homeland Security (DHS), with information from each new employee\u2019s Form I-9 to confirm work authorization."}, {"title": "Junior Data Engineer", "company_name": "HireMeFast LLC - Career Accelerator - Land A Job", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3786249797/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D&trk=flagship3_search_srp_jobs&lipi=urn%3Ali%3Apage%3Ad_flagship3_search_srp_jobs%3BQExogrHBQoSF9zZm%2B9kufw%3D%3D&lici=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D", "id": "3786249797", "description": "About the job\nThis is a remote position.\n\n Job Title: Junior Data Engineer\n\n Employment Type: Full-Time\n\n Salary: $64,000 - $76,000 per annum\n\n Experience Required:  Minimum 1 year of project experience\n\nAbout us: HireMeFast is a leading staffing and recruitment agency specializing in connecting businesses with top-tier talent across various industries. Our mission is to bridge the gap between exceptional candidates and organizations needing their skills, expertise, and unique qualities. Our team of experienced and dedicated recruitment specialists utilizes innovative sourcing strategies, and a vast network to identify and attract top talent. We conduct comprehensive procedures to ensure that only the most qualified candidates are presented to our clients.\n\nPosition Summary\n\nJoin the fast-paced, innovative, and collaborative environment focused on providing an AIOps platform that enhances the intelligence of the CVS Health infrastructure. Work closely with subject matter experts and colleagues to build and scale out machine learning and AI solutions that will detect, predict, and recommend solutions to correct issues before system impact and enhance efficiency, reliability, and performance of CVS Health\u2019s IT operations.\n\nKey Responsibilities include:\n\nData pipeline development: Designed, implemented, and managed data pipelines for extracting, transforming, and loading data from various sources into data lakes for processing, analytics, and correlation. \nData modeling: Create and maintain data models ensuring data quality, scalability, and efficiency \nDevelop and automate processes to clean, transform, and prepare data for analytics, ensuring data accuracy and consistency \nData Integration: Integrate data from disparate sources, both structured and unstructured to provide a unified view of key infrastructure platform and application data \nUtilize big data technologies such as Kafka to process and analyze large volumes of data efficiently \nImplement data security measures to protect sensitive information and ensure compliance with data and privacy regulation \nCreate/maintain documentation for data processes, data flows, and system configurations \nPerformance Optimization- Monitor and optimize data pipelines and systems for performance, scalability and cost-effectiveness \n\nCharacteristics of this role:\n\nTeam Player: Willing to teach, share knowledge, and work with others to make the team successful. \nCommunication: Exceptional verbal, written, organizational, presentation, and communication skills. \nCreativity: Ability to take written and verbal requirements and come up with other innovative ideas. \nAttention to detail: Systematically and accurately research future solutions and current problems. \nStrong work ethic: The innate drive to do work extremely well. \nPassion: A drive to deliver better products and services than expected to customers. \n\nRequired Qualifications\n\n2+ years of programming experience in languages such as Python, Java, SQL \n2+ years of experience with ETL tools and database management (relational, non-relational) \n2+ years of experience in data modeling techniques and tools to design efficient scalable data structures \nSkills in data quality assessment, data cleansing, and data validation \n\nQualifications\n\nKnowledge of big data technologies and cloud platforms \nExperience with technologies like PySpark, Databricks, and Azure Synapse. \n\nEducation\n\nBachelor\u2019s degree in Computer Science, Information Technology, or related field, or equivalent working experience\n\nWhy HireMeFast LLC?\n\nAt HireMeFast, we understand that finding the right individuals to join your team is crucial for success and growth of your organization. We are committed to streamlining the hiring process for our clients, ensuring they have access to a diverse pool of highly qualified candidates who are a perfect fit for their specific needs."}, {"title": "Data Engineer  - Need candidates from Texas ( REMOTE )", "company_name": "Pulivarthi Group (PG)", "location": "United States (Remote)", "link": "/jobs/view/3770685212/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=9gdMwuif%2BpB%2BCQYvQi3imQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3770685212", "description": "About the job\nFollow us on Linkedin: https://www.linkedin.com/company/pulivarthigroup/\n\nPulivarthi Group LLC is a Global Staffing & IT Technology Solutions company, with our prime focus of providing world class solutions to our customers with the right talent. We combine the expertise of our team and the culture of your company to help you with the solution that is affordable and innovative using high quality standards and technologies.\n\nWe\u2019ve served some of the largest healthcare, financial services, and government entities in the U.S.\n\nData Engineer - FT only - Azure SQL with data lakes experience\n\nData Engineer -\n\nthe resource requirement for this Data Engineer role needed more clarification.\n\nOur customer is requesting a resource who knows Azure CI/CD (Continuous Integration / Continuous Deployment) to build and integrate their DevOps pipeline.\n\nOnce the DevOps pipelines are mapped, they need the Data Engineer to connect to all the Data Sources, primarily their Data Lake.\n\nThe Customer (based in Houston) is interested in a resource in a time zone not too far removed from the Central Time Zone to allow real time collaboration."}, {"title": "Data Engineer", "company_name": "Otter Products", "location": "Fort Collins, CO (Remote)", "link": "/jobs/view/3766728302/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=77U3U7CAi1PxgDmpI3%2BiNQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3766728302", "description": "About the job\nOverview \n\nOtter Products is currently recruiting for a Data Engineer to join our Data team! You can be based in our Fort Collins, CO office with flexibility to work a portion of your time remotely, or 100% Remote in the U.S.\n\nThe Data Engineer applies professional experience, concepts, and company objectives toward building and operationalizing the minimally inclusive data necessary for the enterprise data initiatives following industry standard practices and tools. The bulk of the Data Engineer's work would be in designing, managing and optimizing data pipelines, then moving these data pipelines effectively into production for key data and analytics consumers like business/data analysts, data scientists or other stakeholder that needs curated data for data and analytics use cases across the enterprise. The Data Engineer also ensures compliance with data governance and data security requirements while creating, improving and operationalizing these integrated and reusable data pipelines to enable faster data access, integrated data reuse and vastly improved time-to-solution for data and analytics initiatives. Additionally, the Data Engineer will be expected to collaborate with other data team members and data consumers working on the models and algorithms developed by them toward optimization for data quality, security, and governance to put them into production leading to potentially large productivity gains.\n\n About Otter Products \n\nOtter Products, we grow to give. From our founder's garage in 1998 to the global technology leader we are today, Otter Products continues to drive growth through innovation.\n\nThrough our industry-leading brands - OtterBox, Liviri and OtterCares - we provide our partners the number-one selling and most trusted products in our categories. Our philanthropic spirit is the foundation on which we foster our partner relationships, allowing us to grow and to give - together.\n\nBy way of our charitable arm, the OtterCares Foundation, we support our communities and invest in our future through education that inspires kids to change the world.\n\nAnd even as our global community of Otters continues to grow, our founder's core values are still at the heart of everything we do. We measure our success by our ability to give back to our communities and strengthen opportunities for all.\n\nFor more information visit otterproducts.com\n\n Responsibilities \n\nComplete analysis, design and development of BI solutions using Azure SQL Server\nFamiliarity with Data Warehouse concepts\nExperience coding the extraction, transformation, and loading (ETL) processes\nDatabase development primarily in SSIS, Data Factory and SQL\nPartner with the business to determine end user database/reporting needs and requirements\nGenerate ad hoc reports using Power BI or SSRS\nCollaborate with other developers to create and implement best approach solution(s)\nTroubleshoot Azure tools, systems, and software\nReview queries for performance issues, making changes as needed\nCollaborate with team to performance-tune Azure application as necessary\nAssist with the analysis and extraction of relevant information from large amounts of historical business data to feed data science initiatives\nParticipate and support the design and documentation of processes for large scale data analyses, model development, model validation and model implementation\nSupport and maintain a positive safety culture by following all safety policies and procedures and actively contributing to a safe working environment\nOther duties as required\n\n Qualifications \n\nBachelor's degree required. Degree in Computer Science or Mathematics preferred\nMinimum of four years of experience in an IT or analytical role required. Experience in database development, report writing and/or statistics preferred.\n\n EEO \n\nOtter Products, LLC is an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, age, marital status, pregnancy, sex, sexual orientation, gender, gender identity or expression, national origin, disability, veteran status, or any other characteristic or status protected by law.\n\n For US Based Roles Only - Compensation Range Minimum \n\nUSD $90,000.00/Yr.\n\n For US Based Roles Only - Compensation Range Maximum \n\nUSD $120,000.00/Yr.\n\n Additional Total Rewards \n\nProfit Sharing Program Eligible, Benefits Eligible - Full Time- check out otterproducts.com/careers/why for more info"}, {"title": "Software Engineer, Distributed Systems", "company_name": "Hightouch", "location": "United States (Remote)", "link": "/jobs/view/3628572539/?eBP=CwEAAAGMd4QO8APODyE_-mMOPhYyQGg5ZjVbPwq_3V7DK0B2bLAf10pcedM9S2T71X5i4TiJuEwOzWBbSiTKbLuk-VD-_Y-dN5uyf8YpYkHYqEYFxySMS9zr2hRjNrxX9gXbxssh35TFK9YTNV2WeH2Ncb675JgFbJTgNWSZySFC6rrdJGmdSDTr2q5d3VoxORz3GjsDbzjeyZQCvQ07RqYw7oujWSgslycs2LMCY_-N0NEj6e0PvbaYKiZ_5IW34bGVSYLXpHwqa8oOcNttYZ90UA3E_7sDSFzABdIVlfrp5zzERrIYy5sh2MIInxHdDoKgQw8VmOn5uM4S0tXUl-i7LnN0_W0NfPBmLiVDNuveIOm4MfDNPxfe5QKPk6FUAVpvgbcoJn9yRyE&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=rph%2FT6Q6Vkp6YoggJf9cJQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3628572539", "description": "About the job\nAbout Hightouch\n\nHightouch\u2019s mission is to empower everyone to take action on their data. Through our Reverse ETL platform, business and data users can seamlessly sync data from where it resides, such as warehouses and databases, to where it is needed, including operational systems and SaaS tools. Traditionally, acting on data has required engineering time and bandwidth, and left most business users stuck with charts and reports that are unable to take automated action on their data. With Hightouch, every business user, without writing any code, can activate data to streamline critical processes, improve marketing performance, and scale operations.\n\nOur team operates with a focus on making a meaningful impact for our customers. We believe in approaching challenges with a first principles thinking mindset, moving quickly and embracing our value of efficient execution, and treating each other with compassion and kindness. We look for team members that are strong communicators, have a growth mindset, and are motivated and persistent in achieving our goals.\n\nHundreds of companies use Hightouch, including Spotify, Ramp, Retool, NBA, Plaid, and Betterment. We\u2019re based in San Francisco, are remote-friendly, and backed by leading investors such as Amplify Partners, ICONIQ Growth, Bain Capital Ventures, Y-Combinator, and Afore Capital.\n\nAbout The Role\n\nWe are looking for a distributed systems engineer to work on the systems that power the syncing engine our customers and other engineering teams rely on. This presents an exciting challenge where you can apply your expertise in distributed systems, performance optimization, and troubleshooting to push the boundaries of what is possible and meaningful improve our ability to move massive amounts of customer data. This role also provides a unique opportunity to work on a multi-cloud and multi-region infrastructure that supports a global customer base.\n\nWe believe in enabling our engineers to do their best work for our customers by giving them extremely high levels of ownership and autonomy. This comes in different forms: you will own and deliver projects from start to finish, you will work directly with customers to solve their hardest scaling problems, and you will have a lot of influence over what we work on as a team and company.\n\nSome of the problems we\u2019ll be working on include:\n\nSync Speed: Customers want to sync a lot of data to important destinations like Facebook and Snapchat, which requires us to analyze every part of our syncing process and find where we can optimize to sync data more quickly\nStreaming Syncing: We currently sync data from sources in batch only, but envision a world where we also do syncs in real-time and support streaming sources like webhooks and queues\nScalability and Reliability: As part of our rapid growth, we\u2019re always evaluating current future bottlenecks in scaling and reliability, and architecting for the next order of magnitude of growth\nPersonalization API: Our Personalization API product provides a low latency caching layer on top of data warehouses that enable customers to use their data warehouse for real-time personalization use cases with \nMulti-Region and Multi-Cloud: Supporting our multi-region and multi-cloud backend, including extending it to launch Hightouch on in new regions to support data residency requirements of our global customer base\n\nWe are looking for talented, intellectually curious, and motivated individuals who are interested in tackling the problems above. This is a senior role, but we focus on impact and potential for growth more than years of experience. The salary range for this position is $170,000 - $240,000 USD per year, which is location independent in accordance with our remote-first policy.\n\nAbout You\n\nYou are an engineer with a passion for solving hard technical problems that generate real value for customers. You\u2019re motivated by high ownership and are comfortable in a fast-paced, startup environment.\n\nYou have experience and comfort with distributed systems and high-scale systems or the ability and desire to build this experience quickly. This experience can take different forms:\n\nYou\u2019ve built syncing engines that process and move terabytes of data per day in a scalable way\nYou\u2019ve created event collection or data streaming services that power company critical systems\nYou\u2019ve owned systems that have undergone orders of magnitude of growth and helped them scale accordingly\nYou have relevant skills and are a fast learner who is excited to take on the challenges we face\n\nInterview Process\n\nOur goal with the interview process is to balance speed with giving both parties opportunities to assess whether there is a strong mutual fit. We will ask you questions, but we want you to ask us questions! Our technical interviews focus on how you design systems because we believe this is the best way for us to see how you work and for you to see how we collaborate. We don\u2019t ask you to write code to solve technical brainteasers that don\u2019t appear in your day to day job.\n\nRecruiter screen [30m]: Introductory call with our recruiting team to get to know each other and see if the role could be a good mutual fit.\nSystems design screen [45m]: Designing a data processing feature end-to-end.\nHiring manager interview [30m]: Chat with hiring manager about past experiences and future operating preferences to assess fit on company values and operating principles.\nSystems design interview [90m]: Work with the interviewer to architect a system at a conceptual level. The problem will be at a pretty high level - and have both product and customer requirements as well as technical."}, {"title": "Entry-Level Data Scientist Engineer", "company_name": "Phoenix Recruitment LLC", "location": "Texas City, TX (Remote)", "link": "/jobs/view/3788727625/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=8iwtytHsCu4wMwJ4fj3pxw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3788727625", "description": "About the job\nThis is a remote position.\n\nTitle - Entry-Level Data Scientist Engineer, 1 year of project experience\n\nEmployment Type: Full-time\n\nBase Salary: $60K-$70K\n\nPhoenix Recruitment offers a variety of recruiting services to assist both employers and employees. They specialize in marketing open positions, recruiting, and helping employers to find qualified candidates across various industries. Phoenix Recruitment has expertise in streamlining the hiring process. They can help ensure that the process is efficient, well organized, and compliant with relevant regulations.\n\nSkills and Abilities:\n\nStrong knowledge of R or Python for data analysis and modeling. \nProficiency in statistical programs such as R, SAS, MATLAB, or Python. \nFamiliarity with spreadsheets (VBA) and database applications (Access, Oracle, SQL, or equivalent technology). \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \nAbility to learn new methods quickly and work under deadlines. \nExcellent teamwork and communication skills. \nStrong analytical and problem-solving abilities. \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \n\nPreferred:\n\nKnowledge of actuarial concepts and life, health, and/or annuity products. \nExperience with statistical modeling techniques such as GLM, Decision Trees, Time Series, Regression, etc. \nFamiliarity with Microsoft DeployR. \nExposure to insurance risk analysis. \nBasic experience in computational finance, econometrics, statistics, and math. \nKnowledge of SQL and VBA. \nFamiliarity with R or Python for predictive modeling \n\nWhy Phoenix Recruitment LLC?\n\nPhoenix Recruitment often has an extensive network of employers and candidates. This network allows them to tap into a pool of qualified candidates and connect them with suitable job opportunities. They can also leverage their connections to help employers find the right talent efficiently. Outsourcing the recruitment process to a specialized agency can save your time and resources, avoid delays, reduce administrative burdens, and increase the chances of finding the right fit for your organization."}, {"title": "Data Engineer - Remote Work", "company_name": "BairesDev", "location": "United States (Remote)", "link": "/jobs/view/3783162297/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=OtoCrAAHZFomVup04fy35g%3D%3D&trk=flagship3_search_srp_jobs", "id": "3783162297", "description": "About the job\nWho We are\n\nBairesDev is proud to be the fastest-growing company in America. With people in five continents and world-class clients, we are only as strong as the multicultural teams at the heart of our business. To consistently deliver the highest quality solutions to our clients, we only hire the Top 1% of the best talents and nurture their professional growth on exciting projects.\n\nWe are looking for a Data Engineer to join our Development team and participate in different projects made up of multicultural teams distributed throughout the world.\n\nWhat You Will Do\n\n Interview other Data Scientists and Data engineers to vet their technical abilities.\n Input data from those interviews into the product and help develop a predictive, fair, and enjoyable solution.\n Work together with developers, tech leads, and solution architects to build applications.\n Improve existing structures by adding new functionalities or proposing technological updates so that as a result, the impact of your contribution is significant in the core of each business.\n Improve the user experience on scalable and high availability platforms, contributing to the key differential of each business.\n\nHere\u2019s What We Are Looking For\n\n 5+ years of experience working as a developer.\n 3+ years of experience working as a Data Scientist / Data Engineer.\n Proficient with analysis, troubleshooting, and problem-solving.\n Hands-on experience with managing data loads and data quality.\n Advanced English level.\n\nHow we do make your work (and your life) easier:\n\n 100% remote work.\n Hardware setup for you to work from home.\n Flexible hours - make your schedule.\n Paid parental leave, vacation & holidays.\n Diverse and multicultural work environment.\n An innovative environment with the structure and resources of a leading multinational.\n Excellent compensation \u2014 well above the market average.\n Here you can grow at the speed of your learning curve.\n\nOur people work remotely but with a consistent and robust culture that promotes diversity and teamwork. To continue being the leading software development company in Latin America, we want to ensure that every BairesDev member gets the best growth and professional development opportunities in a diverse, welcoming, and innovative environment.\n\nEvery BairesDev team member brings something unique to our company.\n\nWe want to hear your story. Apply now!"}, {"title": "Data Engineer (Remote)", "company_name": "Cascade Debt", "location": "United States (Remote)", "link": "/jobs/view/3755336571/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=pmUtn1j2zyPr25CNPh2RNw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3755336571", "description": "About the job\nDo you want to work in a small team where you can make a real difference in a company? Would you embrace the challenge of building a high impact platform that is used globally? Then we want to speak with you!\n\nAbout Cascade\n\nCascade is a fintech startup backed by Canadian and US investors that empowers companies to grow by democratizing access to institutional debt. We are building tools that modernize how companies raise and manage debt, lowering the barriers to entry to the $7 trillion specialty finance and alternative credit market for companies around the world. Founded just last year, we are gearing up for our next phase of technical development and are seeking a talented Data Engineer to lead the way.\n\nAbout The Role\n\nAs a Data Engineer you\u2019ll be an early member of a growing team building a pioneering platform in the debt infrastructure space. You will design, build, and launch efficient, scalable, and reliable data pipelines to move and transform data .\n\nWhat You\u2019ll Do\n\n Building databases, data lakes and data ingestion pipelines to integrate customer databases and datasets to our systems \n Ingesting financial datasets from external customers, then u pdating and maintaining accurate and complete data mappings to ensure that our product s are displaying high quality data \n M onitoring and alerting across our data pipelines in order to make sure that our data ingests are reliable and correct \n Translat ing business goals and stakeholder requirements into new data flows and deliver analytical insights ensuring that the data flows create real business value continuously \n\nSkills And Experience\n\n 2 + years experience in a data engineering or data analyst role \n E xperience with relational SQL and NoSQL databases, including PostgreSQL , MySQL, MariaDB, MongoDB, Bigtable, etc. \n E xperience working with ETL technologies , such as Databricks, Fivetran , or dbt \n Proficiency in writing SQL queries and knowledge of analytical data warehouses such as RedShift, BigQuery , and Snowflake \n Some experience with CI/CD automation such as GitHub Actions \n Developing APIs and integrating with 3rd party APIs \n Bonus: experience in fintech / SaaS / credit analysis \n\nPeople Who Thrive At Cascade Are\n\n Self-starters, who take the initiative to tackle challenges in a remote work environment \n Good communicators, who can operate without ego to discuss, learn, grow, and help others do the same \n Passionate about creating great digital experiences for users \n Problem solvers who are not satisfied with the status quo \n\nBenefits\n\nRemote: we are a remote-first company and are very flexible on hours as long as things get done \nHome-office: there\u2019s a $1000 USD home office allowance to set yourself up \nEquity: we expect you to have an owner-mentality, and have the equity plan to match \nBenefits: health, dental, vision, and more \nPerks: we offer free lunches weekly and off-site trips \nJob satisfaction: we offer autonomy, ample opportunities for mastery, and an opportunity to make a difference for companies around the world"}, {"title": "Data Engineer", "company_name": "ISC (Integrated Specialty Coverages, LLC)", "location": "Carlsbad, CA (Remote)", "link": "/jobs/view/3778293528/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ew7%2FfEuXSujs5IGabdqUyw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3778293528", "description": "About the job\nAbout Integrated Specialty Coverages\n\nIntegrated Specialty Coverages LLC (ISC) is a growth stage technology and data-driven insurance company leading innovation in the market.\n\nBacked by one of the leading private equity firms, KKR, and led by a forward-thinking management team, ISC is combining the worlds of insurance and technology to create an Insurtech powerhouse. As a leading online distributor of insurance products for a range of industries and \"Main Street USA\", we are looking for the right people to help us in our mission of achieving exponential growth. We strive to be the number one place to go for brokers and agents to source insurance. To accomplish this, we're building a digitally focused team that deeply understands the intersection between user experience, data, and AI/ML to optimize the way we engage with our customers and partners.\n\nJob Summary\n\nThe Data Engineer role will be the technical liaison between multiple groups including a data science team, the engineering team, product management, and business stakeholders. You do not need any insurance knowledge prior, however, you must quickly dive deep into the insurance world and ask questions to become a subject matter expert. You will be responsible for building a data platform to facilitate the data science team. You must be a self-starter that can build out features such as a data pipeline from scratch. There will be support from both engineering and data science for any buildout.\n\nJob Responsibilities\n\nCreate and maintain optimal data pipeline architecture. \nAssemble large, complex data sets that meet functional / non-functional business requirements. \nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. \nBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using AWS technologies, SQL, and Python. \nWork with stakeholders including the Executive, Product, Data Science, and Engineering teams to assist with data-related technical issues and support their data infrastructure needs. \nWork with data science and analytics teams to strive for greater functionality in our data systems. \n\nMinimum Qualifications:\n\nAdvanced working SQL knowledge and experience working with relational databases, strong query authoring (SQL) as well as working familiarity with a variety of databases (Snowflake, Redshift, MySQL, MSSQL, etc.)\nExperience building and optimizing data pipelines, architecture and data sets. \nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. \nBuild processes supporting data transformation, data structures, metadata, dependency, and workload management. \nA successful history of transforming, processing and extracting value from large disconnected, datasets from a variety of data sources (Flat files, Excel, databases, etc.)\nStrong analytic skills related to working with unstructured datasets. \nStrong project management and organizational skills. \nExperience supporting and working with cross-functional teams in a dynamic environment. \nAbility to mentor/guide/collaborate with other team members. \n\nWe are looking for a candidate with 3-5 years of experience in a Data Engineer role. They should also have experience using the following software/tools:\n\nExperience with relational and MPP databases, including Snowflake and MySQL. \nExperience developing software in an agile environment from the requirements stage to production. \nExperience with version control: git\nExperience with container technologies: Docker\nExperience with data pipeline and workflow management tools: Airflow, Jenkins, AWS Glue, Azkaban, Luigi, etc. \nExperience with AWS cloud services: EC2, ECS, Batch, S3, EMR, RDS, Redshift\nExperience with other cloud services: Snowflake, Airflow\nExperience with object-oriented/object function scripting languages: Python, Java, C++, etc. \nExperience with data modeling and data warehouse design\nExperience with data visualization tools (PowerBI, QuickSight)\n\nThe starting annual pay scale for this position is listed below. Actual starting pay will be based on factors such as skills, qualifications, training, and experience. In addition, the company offers comprehensive benefits including medical, dental and vision insurance, 401(k) plan with match, paid time off, and other benefits.\n\nISC's salary ranges are determined by role and level. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations and could be higher or lower based on a multitude of factors, including job-related skills, experience, and relevant education or training.\n\nNational Pay Range\n\n$100,000\u2014$110,000 USD\n\nBenefits of Working at ISC\n\nCompetitive vacation and flexible working arrangements\nComprehensive and inclusive health benefits\nA variety of professional development and mentorship opportunities\nChoice of technology whether at home or in the office\n\nISC's Ownership Behaviors\n\nDo the Right Thing *Be Relentless. Pursue Excellence. *Personal Responsibility & Own Outcomes\nTry Fast. Learn Fast. Fail Fast. Think Big. *Build Extraordinary Owners & Join Forces *Grit & Determination\n\nApplicants with disabilities may contact ISC HR department via e-mail, and other means to request and arrange accommodation. If you need assistance to accommodate a disability, you may request accommodation at any time. Please contact ISC at HR@ISCMGA.com.\n\nISC believes in creating long-term relationships by being responsive and relevant and by consistently delivering value to our community of customers. Specifically, with our employees, we focus on attracting, developing, and retaining the best talent for our business, challenging our people, demonstrating a \"can-do\" attitude, and fostering a collaborative and mutually supportive environment. ISC is an equal opportunity employer and a participant in the US federal E-Verify program (US).\n\nDiversity creates a healthier atmosphere: All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, gender, gender identity, sexual orientation, marital status, medical condition, genetic information, mental or physical disability, military or veteran status, or any other characteristic protected by local, state, or Federal law."}, {"title": "Data Engineer -Remote", "company_name": "RIT Solutions, Inc.", "location": "Denver, CO (Remote)", "link": "/jobs/view/3768007496/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=1hQH8EDaII1eJ7nQ9BACQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3768007496", "description": "About the job\nDuration: 6+ months CTH\n\nMust have a valid LinkedIn profile\n\nRequired Skills\n\n 3-6 years of experience (no more than 6 years)\n Start up environment experience\n Python, SQL, and DBT"}, {"title": "Big Data Engineer - W2", "company_name": "Megan Soft Inc", "location": "United States (Remote)", "link": "/jobs/view/3674845652/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fCAxh4N%2F4YZJ5aEIASaeeA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3674845652", "description": "About the job\nTitle: Big Data Engineer - W2\n\nLocation: Dearborn, MI (Remote)\n\nDuration: 12+ Months\n\nDirect Client: Ford Motors\n\nMust have skills: Should be able to handle GCP requirements\n\n\"Translates business requirements to build repeatable, sustainable, efficient, coded processes that can be productionized by Software Engineers and readily modified by other Data Engineer Creates POC processes in Dev/QA and works with Software Engineers to productionize and align best coding and process practices that maximize efficiency, speed, stability, system resources and capabilities Awareness of and compliance with data privacy, security, legal and contractual guidelines Incorporates data quality and privacy checks/alerts to minimize bad data being consumed by end users, models and dashboards, and to protect customer data; revises checks for new data issues Validates data products and pipelines are functioning as expected following system or application upgrades, source changes, etc. Works closely with Architects if required to align systems, tools and applications being utilized with business use case and performance requirements Provides data product support and maintenance Establishes, tracks and monitors KPIs related to specific data products and deliverable Participate in iteration cadence activities (Iteration Planning, task estimation, daily scrum, demos, etc.). Will be a part of on-call team to resolve critical production support tickets during business hours and off hours.\n\nSkills Required\n\nExperience with Big Data Technologies (Hadoop , ETL Tool, Spark. Hive etc..) Someone who understands Cloud and has experience working with Cloud Platforms (preferably GCP) Should be able to handle GCP requirements - ingesting into GCS/ Big Query, working with GCP components Hands-on in Data Flow, Cloud Storage (GCS), Big Query and GitHub. Proficient in Python and SQL.\n\nSkills Preferred\n\nManages ETL: uses programming and tools for data ingestion, configures pipelines, applies transformations and decoding, integrates and fuses data, moves and securely deliver Experience NoSQL databases\n\nAdditional Information\n\nIf the candidate is remote only, please indicate \"\"100% Remote\"\" under candidate's name on resume. Other similar schedule notations can be \"\"Local Candidate\"\" or \"\"Hybrid Candidate.\"\" ***HRA Test Required***\""}, {"title": "Remote Opportunity: Quantexa Data Engineer", "company_name": "SPAR Information Systems LLC", "location": "United States (Remote)", "link": "/jobs/view/3680073191/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ioykVCZSOQ7BK5%2FwRQHOWg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3680073191", "description": "About the job\nHello All, Hope you are doing great!! Please go through the job descriptions and let me know your interest. Role: Quantexa Data engineer  Location: Remote Duration: Long Term Contract Job Description: 1. End to End usage of Quantexa including guiding our client team on how to use from both front end and data ingestion Reason they are asking for Certification 2. Resolve Entity issues with data 3. Knowledge of Scala and Hive which requires coding experience using parquet 4. Advanced SQL knowledge requiring writing SQL queries 5. Experience having worked with GCP data lake 6. Quantexa certification required\n\nThanks & Regards,\n\nSatnam Singh\n\nDirect: 201 623 3660\n\nEmail : Satnam.singh@sparinfosys.com"}, {"title": "Remote Work - Need Staff Data Engineer - USC only", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3746283980/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=CvIuLBNO%2BPMGi4IXVRkP0A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3746283980", "description": "About the job\n100% Remote, client is in NY\n\nLook for strong job history, Python, Machine Learning, SQL, 10/10 comms, etc. \n\nMUST be an Engineer \n\nSummary\n\nWe're looking for an experienced Data Engineer to join our client's growing team of top notched IT professionals. You will work closely across multiple groups including Editorial and Audio, Marketing, Advertising, etc. to achieve their goals and objectives. The Data Engineering team builds tools throughout the stack while sharing knowledge across these departments.\n\nAs the company grows, they're looking for Data Engineer who will help solidify and expand our pipelines and maintain their data warehouse. Their business model is based on performing complex and very detailed analyses of how our products perform with their subscribers. You'll be responsible for working alongside analysts, product, marketing, and other teams to define new data collection components and measurement schemes that support each new product and feature.\n\nHow Can Add Value To Their Mission\n\nCreate and maintain data pipelines to provide insights and drive business decisions\nEstablish data warehousing strategy (ex. Kimball, Data Vault, etc.)\nMaintain data infrastructure on our AWS accounts\nCollaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.\nWrite unit/integration tests, contributes to engineering wiki, and documents work\nImplement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it\n\nWhat You'll Need To Succeed\n\n7+ years of industry experience measuring product performance and user behavior\nExperience working with a variety of data management technologies, including RedShift, Kinesis/Kafka, Glue, Spark, Postgres, Airflow, dbt, and others.\nExperience implementing BI reporting tools such as Looker\nExperience interfacing with engineers, product managers and analysts to understand data needs\nKnowledge of a variety of measurement beacons, SDKs, APIs including Google Analytics, Amplitude, Braze, Stripe, email service providers\nA commitment to building secure, resilient, fault-tolerant architectures, with clear documentation and procedures in place for support\nA focus on accuracy and detail as you build, ensuring data pipelines produce clear, predictable results\nUnderstanding of the typical metrics a subscription and advertising-supported business needs to measure success\nExperience with Client techniques as applied to behavioral segmentation or anomaly identification is a definite plus\nFamiliarity using developer tools that increase productivity and facilitate the development of resilient code (eg. Docker, CircleCI, Serverless) is a plus"}, {"title": "ENTRY LEVEL DATA ENGINEER (REMOTE)", "company_name": "SynergisticIT", "location": "Jacksonville, FL (Remote)", "link": "/jobs/view/3767587909/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=om7L3CnowiC4PLIcpZD70A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3767587909", "description": "About the job\nSYNERGISTICIT wants every candidate to know that the Job Market is Challenging and to stand out, you need to have exceptional skills and technologies and that's where we come in to make sure you get the attention which you need\n\nSynergisticIT understands the complex nature of the job market and how difficult it can be to secure a position, especially for fresh graduates. Therefore, we assist and help tech-savvies to convert their passions into professions. We go above and beyond to keep you working in your niche.\n\nAs we focus on long-term success, we provide complete career development solutions. From job search to upskilling portfolio and interview preparation, we can guide you at every step of your career.\n\nSynergisticIT spares no efforts to connect you with a large network of tech giants, including Google, Apple, PayPal, Dell, Cisco, Client, etc. Presently, we are actively looking for ENTRY LEVEL DATA ENGINEER a driven mindset. Get the right opportunity and gain experience in building web-centric solutions on Java.\n\nWho Should Apply : Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or anyone looking to make their career in IT Industry\n\nWe also assist in filing for STEM extension and H1b and Green card filing.\n\nCandidates who are serious about their future in the IT Industry and have set big goals for themselves.\n\nCandidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. We also offer Skill Enhancement Programs if the candidates are missing skills or experience which our clients need with great outcomes\n\nCandidates can benefit from skill enhancement if they fall into the below categories. If they are qualified with enough skills then no need for skill enhancement\n\nCandidates Who Lack Experience\n\nHave had a break in careers\n\nLack Technical Competency\n\ncandidates who want to get employed and make a career in the Tech Industry \n\nPlease Also Check The Below Links\n\nhttps://www.synergisticit.com/candidate-outcomes/\n\nhttps://www.synergisticit.com/java-track/\n\nhttps://www.synergisticit.com/data-science-track/\n\nhttps://www.synergisticit.com/which-is-the-best-option-for-tech-job-seekers-staffing-companies-consulting-companies-bootcamps-or-synergisticit/\n\nhttps://www.synergisticit.com/contact-us/\n\nIf the skills are not a match candidates can opt for Skill enhancement. Or their resume can be sent out to clients to see if responses are achievable\n\nREQUIRED SKILLS For Java/Software Programmers\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Core Java , javascript , C++ or software programming\nSpring boot, Microservices and REST API's experience\nExcellent written and verbal communication skills\n\nFor data Science/Machine learning\n\nRequired Skills\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Statistics, Python, data visualization tools\nExcellent written and verbal communication skills\n\nPreferred skills: NLP, Text mining, Tableau, Time series analysis\n\nTechnical skills are required by clients for selection even if its Junior or entry level position each additional Technical skill helps a candidate's resume to be picked by clients over other job seekers.\n\nClients hire candidates with the right technical skills which they need and reject candidates who lack the required technical skills.\n\nNo third party candidates or c2c candidates\n\nPlease apply to the posting\n\nNo phone calls please. Shortlisted candidates would be reached out"}, {"title": "Data Engineer - Remote | WFH", "company_name": "Get It Recruit - Information Technology", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3782241984/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=BgBf1kSAAX%2BSuw6inJsIVw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3782241984", "description": "About the job\nJoin an innovative team at a leading company in the engineering and manufacturing industry, where we are committed to leveraging data for continuous improvement and operational excellence. This hybrid position allows you to enjoy the flexibility of remote work on Mondays and Fridays.\n\nWhy Join Us\n\nAt our company, we prioritize your well-being and professional growth. We offer a comprehensive benefits package, including excellent health benefits, a 6% 401(k) match, generous paid vacation and sick time, 12 paid holidays, and more.\n\nRole Overview\n\nAs a Data Engineer, you will play a pivotal role in transforming raw data into actionable insights, driving improvements in engineering and manufacturing processes, enhancing product quality, and optimizing operational efficiency. Collaborate with dynamic cross-functional teams to extract, analyze, and visualize data, influencing strategic decisions through your valuable insights.\n\nKey Responsibilities\n\nData Collection and Integration:\n\nGather, clean, and integrate data from diverse sources, including production databases, sensor data, and external datasets.\n\nData Analysis\n\nConduct exploratory data analysis to identify patterns, trends, and anomalies.\n\nPerform statistical analysis and hypothesis testing to derive meaningful insights.\n\nData Visualization\n\nCreate informative and visually appealing dashboards and reports using tools like Tableau, Power BI, or Python libraries.\n\nPredictive Modeling\n\nDevelop predictive models to forecast manufacturing performance, quality, and operational outcomes.\n\nRoot Cause Analysis\n\nInvestigate production issues and quality concerns by analyzing data to identify root causes and recommend corrective actions.\n\nPerformance Monitoring\n\nEstablish and maintain key performance indicators (KPIs) to track manufacturing and engineering metrics.\n\nGenerate regular reports to communicate performance trends to stakeholders.\n\nCross-functional Collaboration\n\nCollaborate with engineering, manufacturing, and quality assurance teams to understand their data needs and provide analytical support.\n\nContinuous Improvement\n\nSuggest process improvements and optimizations based on data analysis.\n\nData Security And Compliance\n\nEnsure data security and compliance with relevant regulations, such as GDPR or industry-specific standards.\n\nInfrastructure And Development\n\nBuild and maintain the infrastructure for collecting, storing, and processing data.\n\nDesign, develop, and manage data pipelines, ETL processes, and data warehouses.\n\nRequired Experience\n\nProven experience as a Sr. Data Analyst and/or Data Engineer in an engineering or manufacturing environment.\n\nProficiency in data analysis tools and programming languages, such as Python, R, SQL, and Excel.\n\nStrong data visualization skills with tools like Tableau or Power BI.\n\nExcellent knowledge of statistical analysis and machine learning techniques.\n\nAbility to communicate complex findings and insights effectively to both technical and non-technical stakeholders.\n\nStrong problem-solving skills and attention to detail.\n\nFamiliarity with manufacturing processes and quality control is a plus.\n\nExperience with enterprise ERPs (SAP S4HANA, Oracle EBS), CRMs (Salesforce, Hubspot), and IoT.\n\nProficient in working with cloud computing platforms including AWS, GCP, and Azure.\n\nExpertise in configuring and optimizing cloud-based data solutions for scalability, reliability, and performance.\n\nBachelor's Degree in Data Science, Statistics, Engineering, or Computer Science Required.\n\nApply Now and be part of a team shaping the future of data-driven innovation in engineering and manufacturing!\n\nEmployment Type: Full-Time"}, {"title": "Entry Level Software Engineer - Data Backend Engineer (Remote - Canada)", "company_name": "Yelp", "location": "United States (Remote)", "link": "/jobs/view/3752881757/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fankeoxifvhpTzOYvowHYQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3752881757", "description": "About the job\nYelp engineering culture is driven by our values: we\u2019re a cooperative team that values individual authenticity and encourages creative solutions to problems. All new engineers deploy working code their first week, and we strive to broaden individual impact with support from managers, mentors, and teams. At the end of the day, we\u2019re all about helping our users, growing as engineers, and having fun in a collaborative environment.\n Interested in exploring data and finding innovative ways to collect it, curate it, and make it easily accessible for others to use? How about doing that at an enormous scale? Yelp\u2019s datasets contain billions of interactions between users and local businesses around the globe. If working with our vast amount of data sounds exciting, come and join us!\n\nThe Marketplace Data Observability team is responsible for managing, governing, and monitoring all of Yelp\u2019s Ads data to protect our revenue stream and also support offline analysis that aids in making long term company decisions. We build tools that help us to better understand our data quality, and we strive to uncover data issues before they impact our consumers.\n\nAs a Data Backend Engineer on this team, you will be responsible for building elegant and scalable data products that serve critical, up-to-date, structured information to support different types of analytics within Yelp. As a core contributor to our growing data modeling and data warehousing engineering efforts, you will help design and own mission-critical data flow pipelines and datastores to enable decisions including effective A/B testing and company investments.\n This opportunity is fully remote and does not require you to be located in any particular area in Canada. We welcome applicants from throughout Canada. We\u2019d love to have you apply, even if you don\u2019t feel you meet every single requirement in this posting. At Yelp, we\u2019re looking for great people, not just those who simply check off all the boxes.\n  Build systems that can effectively store and crunch terabytes of data.\nDesign and develop data models for efficient data storage, retrieval, and reporting.\nCreate and maintain conceptual, logical, and physical data models using industry-standard modeling tools.\nCollaborate with cross-functional teams, including engineers, data analysts, business analysts, and data scientists, to understand data requirements and translate them into effective data models.\nParticipate in data integration efforts, including ETL processes and data migration.\nStay up-to-date with industry best practices, emerging technologies, and trends related to data warehousing.\nSupport on-call rotations as needed to operate the team.\n  Understanding of high performing and scalable data systems.\nExperience in building and orchestrating ETL pipelines.\nExperience with Data Lake or Data Warehouse landscape.\nA hunger for tracking down root causes and fixing them in systematic ways.\nAbility to communicate effectively to technical and non-technical cohorts alike. \nExposure to some of the following technologies: Python, AWS Redshift, AWS Athena / Apache Presto, Big Data technologies (e.g S3, Hadoop, Hive, Spark, Flink, Kafka etc), NoSQL systems like Cassandra, DBT."}, {"title": "Data Engineer (Associate Consultant), June 2024", "company_name": "UDig", "location": "Nashville, TN (Remote)", "link": "/jobs/view/3716039960/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=XUjo40kU55HnDCqJ%2F3FdPg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3716039960", "description": "About the job\nCan UDig It? \n\nUDig designs, builds, and implements technology solutions that deliver on business objectives. People join our team because they are excited to have an impact on the future of our organization, passionate about delivering meaningful results, and energized by solving complex problems. Oh, and they like working with other driven, smart, and authentic people!\n\nUDig is proud to be recognized as a \"Best Places to Work\" in Virginia and Tennessee. We're committed to fostering a transparent environment where leadership is accessible, professional development opportunities are abundant, and you are empowered to achieve your career goals. You'll collaborate, develop deep relationships with teammates, and accelerate your success while having fun!\n\nIf you're graduating in May 2024 and excited by the idea of impacting organizations by building data solutions to achieve business goals, you should consider joining our growing technology team.\n\nHow Things Go Down\n\nAs a new, entry-level teammate, you'll take part in our \"Breaking Ground\" training program designed to help you quickly get up-to-speed on UDig's delivery approach within a project team. The program covers:\n\nUDig delivery process and methodologies such as gathering requirements and delivering projects using agile\nTechnical guidance and instruction on the best practices from Data leaders at UDig\nDelivering executive and technical presentations\nCoaching to prepare you to for success on your first client project at UDig\n\nA typical day might entail:\n\nProject standups and check-ins\nDesign and development of data pipelines, BI reports, and more\nPlanning and participating in project demos\nContributing to internal UDig projects\nCareer and job growth through UDig-provided training\nCreating long-lasting friendships with teammates through fun activities as well as your projects (including social events, winning a prize for guessing the weight of pumpkin, or sharing experience on cool tech side projects)\n\nUDig believes it is important for Associate Consultants going through our Breaking Ground program to reside in one of our major markets (Richmond, VA or Nashville, TN metro area) for at least the first year of employment.\n\nWant to learn more? Hear directly from our teammates by checking out our 'A Day in the Life' blog series: https://www.udig.com/digging-in/breaking-ground-2023/\n\nSound Interesting? Here's the Technical Piece\n\nBelow are a few boxes you should be able to check if\u202fyou're interested: Don't check every box? Go ahead and apply!\n\n1+ year of experience with relational database management systems such as Microsoft SQL Server, MySQL, or Oracle\nUnderstanding of the full lifecycle of design, development, and implementation of data solutions including architecture design, development, testing\nSome experience in ETL development or reporting is a plus\nAbility to present technical ideas and high-level concepts and solutions to internal and external team members with varying degrees of technical knowledge\nBachelor's Degree in in computer science, information technology, or any science or business discipline\nAbility to take your work seriously but also seriously enjoy a good time with your UDig colleagues\n\nWhat's in It For You? \n\nAt UDig, you will build your career alongside talented and experienced developers while gaining experience and having fun! We provide continuous challenges and professional development opportunities. Additionally, we offer: \n\nCompetitive salary, merit reviews, and career advancement paths\nFlexible, hybrid environment\nIndividual $1500 Training Budget\nRegular team building and social activities (virtual and/or in-person)\nTransparent culture with strong communication and access to leadership\nGreat benefits like\nGenerous Paid Time Off, company holidays, and parental leave\nMultiple Single and Family Health Insurance plans to choose from\nDental & Vision coverage\nShort-Term and Long-Term disability\nOptional accident and critical illness coverage\nMatching 401(k)\nand more!"}, {"title": "Expression of Interest: Data Engineer", "company_name": "Fingerprint for Success (F4S)", "location": "New York, NY (Remote)", "link": "/jobs/view/3787769880/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=l%2FEDpQqUNmCttGdIyTk7Cg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787769880", "description": "About the job\nWe are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.\n\nThe F4S Talent Pool is a pilot project designed to:\n\nHelp job seekers get discovered by our partners based on their anticipated hiring needs.\nProvide optional support and resources for job seekers in their career endeavors.\nHelp individuals understand, and bring out the best in themselves and each other.\n\nThe F4S Talent Pool process:\n\nOnce you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.\n\nAbout Fingerprint For Success (F4S)\n\nBacked by 20+ years of research, F4S\u2019s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high performing teams.\n\nKeep in mind, joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your F4S work style assessment to match the hiring needs of our partners.\n\nYour feedback is a gift! Write to us via:\n\nPowered by JazzHR\n\noWuPJ9lwq9"}, {"title": "Data Engineer Seattle, WA (remote)", "company_name": "Conch Technologies, Inc", "location": "United States (Remote)", "link": "/jobs/view/3765052211/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=SBETI78UMutCHuRp5FnP%2FA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3765052211", "description": "About the job\nHi,\n\nGreetings from Conch Technologies Inc\n\nData Engineering Lead \u2013 With Databricks and Unity Catalog\n\nLocation \u2013 Seattle, WA (remote)\n\nDuration \u2013 12 months\n\nNeed: Design and implement data architecture and strategy, leveraging technologies such as Databricks, Unity Catalog, Privacera, and Collibra.\n\nAs a Data Engineering Lead, you will play a pivotal role in defining and executing technical deliverables while assessing complexities and levels of effort. You'll engage with both vendor and internal teams to resolve technical challenges and ensure seamless collaboration. Your leadership will extend to planning and prioritization with the SCRUM master, facilitating effective coordination between onsite and offshore teams, and driving feature prioritization through collaboration with product managers.\n\nKey Responsibilities\n\n Lead the definition and breakdown of technical deliverables, assessing complexities and levels of effort.\n Collaborate with vendor and internal teams to address technical issues.\n Drive planning and prioritization in conjunction with the SCRUM master.\n Coordinate with onsite and offshore teams for timely project delivery.\n Collaborate with product managers to prioritize features, communicate with stakeholders, and clarify requirements.\n Collect and manage user-reported issues, ensuring prompt resolution and communication.\n Define comprehensive test plans, oversee their execution, and document results.\n\n--\n\nWith Regards,\n\nNagesh G\n\nMobile: 408-381-5645\n\nDesk: 901-313-3066\n\nEmail: nagesh@conchtech.com \n\nWeb: www.conchtech.com"}, {"title": "Remote Work - Need Azure Fabric Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3759325599/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=C3iVjmcR9wjUip1nNjUiCg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3759325599", "description": "About the job\nThe client is placing heavy emphasis on Microsoft and Azure Fabric experience\n\nInformation Management Engineer \u2013 Take the identified information management sources and assist migration to the Client Data Lake, Azure Fabric. The engineer will transform information management (de-dup, clean and enrich).\n\nTake the identified data sources and bring them into our Data Lake, Azure Fabric. Once the data is in, the engineer will need to be able to transform data (de-dup, clean and enrich the data). We are focused on using a medallion data model for data architecture. The tools we are working with are: Delta Parquet files, python, spark, notebooks, T-SQL, Fabric lakehouse/warehouse, Dataflows Gen2, Data Pipelines, Direct Lake, shortcuts, ADF."}, {"title": "Data Engineer", "company_name": "Catalist", "location": "Washington, DC (Remote)", "link": "/jobs/view/3744951279/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ui1Fk6ICmnPNqAlZDZAcwA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3744951279", "description": "About the job\nFor over 17 years, Catalist has been a leader in civic data and data science innovation. Our mission is to provide progressive organizations with the data, software, and services needed to better identify, understand, and communicate with the people they need to engage and mobilize. Our clients include the largest, most influential organizations in the U.S. active in civic engagement, advocacy, and political campaigns.\n\nCatalist is home to a dedicated, creative team of technologists, data scientists, and campaign experts committed to using our talents and technology to nurture a vibrant and growing progressive community.\n\nAs a Data Engineer at Catalist, you will work closely with the Analytics and Technology departments to design, build, support, and maintain various data pipelines and processes with an end goal of providing data and intelligence to the progressive community. This role primarily involves translating data architecture designs into functional processes, code, and systems.\n\nThe ideal candidate will be a highly motivated individual with excellent technical skills, a strong desire to learn new skills, and an interest in progressive politics. Catalist values creativity and problem-solving. Our work is on the cutting edge of data-driven politics, and your support will help Democratic candidates and progressive organizations conduct successful advocacy and electoral campaigns.\n\nThis position reports to the Deputy Chief Data Officer. The Data Engineer is a part of a growing Data team that supports all underlying work at Catalist.\n\nThis position is included in our CWA bargaining unit.\n\nPrinciple Duties & Responsibilities\n\nDevelop scalable, production processes, code, and systems for data ingest, transformation, modeling, and reporting across a variety of platforms\nTranslate mock-ups and designs into functional processes, code, and systems\nCreate architecture designs when needed\nProvide quality assurance and testing on processes, code, systems, and products\nBecome an internal subject matter expert on various datasets and support other Catalist departments/teams on usage of those datasets\nExecute ad-hoc data and database maintenance tasks as requiredProject manage cross-departmental efforts, with direct responsibility for stakeholder engagement, management, and execution of technical elements\n\n\nRequirements\n\nBS or BA in a technical field, or relevant experience\n1-2 years of experience working with SQL databases\nExperience with data cleaning or ETL processes\nExperience with distributed computing systems and/or distributed datastores (particularly the Hadoop ecosystem)\nExperience managing projects\nFamiliarity with Catalist data, progressive politics, voter files, and/or commercial data\nBackground check required\n\n\nPreferred Skills & Abilities\n\nWillingness to be a problem solver and produce results in a fast paced environment\nAbility to be creative and personable, and articulate ideas clearly\nExperience working with SQL databases\nProficiency with Python or another object-oriented programming language (R, Java, Scala, etc\u2026)\nExperience working in cloud environments (AWS, GCP, etc.)\nExperience working in command line environments such as Bash\nExperience with a version control tool such as git or github\n\n\nBenefits\n\nMedical, Dental, Vision, Prescription Drug\n\nCatalist offers Medical, Dental, Vision, and Prescription Drug coverage for eligible staff and their eligible dependents. Catalist\u2019s Medical plan is a comprehensive PPO program including Prescription Drug coverage with 85% of the premium paid by Catalist. Dental and Vision coverage is provided at no cost to employees.\n\nGroup Term Life Insurance and Long-Term & Short-Term Disability Coverage\n\nGroup Term Life Insurance and Long-Term and Short-Term Disability coverage is available for eligible staff. These benefits are provided at no cost to Catalist employees.\n\n401(k) Safe Harbor Plan\n\nA 401(k) Safe Harbor Plan is available to eligible staff with a 3% contribution from Catalist from the date of hire. Employees may contribute pre-tax or post-tax from their salary up to the legal limits set forth by the IRS.\n\nMedical and Dependent Care Flexible Spending Accounts (FSAs)\n\nCatalist offers an FSA Program that gives eligible staff the ability to pay out-of-pocket medical/dental/vision/child care expenses from pre-tax earnings.\n\nTransit Benefits\n\nCatalist also makes available a Transit benefit FSA program to eligible employees using pre-tax contributions with a company match.\n\nProfessional Development and Remote Work Expenses\n\nEligible employees may be reimbursed up to $750 each year for professional development / education and remote work expenses.\n\nStudent Loan PayDown or SaveUp\n\nCatalist offers a Student Loan PayDown and College SaveUp benefit for eligible staff.\n\nVacation, Personal Leave, Sick Leave Benefits\n\nCatalist offers generous vacation benefits to all eligible staff. Eligible employees also receive:\n\n 14 Paid Holidays\n Personal Days\n Sick Leave\n Parental Leave\n\n\nHybrid Office/Remote Work\n\nCertain positions at Catalist are eligible for Office/Remote Hybrid or full Remote status."}, {"title": "Remote Entry Level Data Analyst/Engineer", "company_name": "SynergisticIT", "location": "White Plains, NY (Remote)", "link": "/jobs/view/3774171423/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=q54sk1%2FRww0z%2BC%2BE8e0jQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774171423", "description": "About the job\nAt SynergisticIT, we're all about making connections. Whatever IT goals you have, our software programmers can help achieve those. Our Software development teams can take up turnkey projects and execute them in an effective and efficient manner. If you are looking to source talent our recruiters will find the ideal IT talent for your company. What's the secret to our success? Well, it all starts with taking quality time to listen to each client's specific needs. After we have a thorough grasp of your IT goals, we can better customize our Developments as per your specific needs. We can also tailor make recruiting programs to exceed your expectations. Since our founding in 2010, SynergisticIT's strategies have earned the company an enviable position in the software development, IT staffing and IT skill enhancement fields. SynergisticIT continues to work with hundreds of satisfied American clients with our software programmers working on our projects and after gaining hands on experience on cutting edge technologies moving to contribute their skills to great clients like Apple, Google, Client, Ebay, Paypal, Kroger, the Walt Disney Company and hundreds more. If you are tired of working with inefficient programmers who take a lot of time to ramp up we want you to try us. Our software programmers can hit the ground running and get you the maximum return on your investment. You have already tried the rest its time you tried the best. SynergisticIT - Home of the Best Data Scientists and Software Programmers in the Bay Area.\n\n Why Us ? \n\nSynergisticIT has a proven track record of successfully skill enhancement and staffing IT employees for some of the world's most iconic brands. Our team takes the time to fully understand every client's needs so we could best meet your IT staffing requirements. The knowledgeable staff at SynergisticIT is always more than happy to work with clients to ensure they reach their software development goals. Besides staffing, SynergisticIT is also committed to helping young IT professionals advance their career with a robust upskill program . Everyone who goes through SynergisticIT's program learns all the skills necessary to succeed in many IT fields ranging from Java to Machine Learning. Additionally, everyone trained at SynergisticIT has been through extensive mock and technical interview screenings to bolster their career prospects. Last, but certainly not least, SynergisticIT takes great care to respect the privacy considerations for every client. All companies who work with SynergisticIT can rest assured their confidential data is protected using the most up-to-date encryption technologies. SynergisticIT also complies with all the latest NDA agreements.\n\n REQUIRED SKILLS For Java /Software Programmers \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Time series analysis \n\n We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2022 and at Gartner Data Analytics Summit (Florida)-2023 \n\nOracle CloudWorld Event (OCW) Las Vegas 2022 | SynergisticIT - YouTube\n\n https://www.youtube.com/watch?v=OAFOhcGy9Z8 \n\n https://www.youtube.com/watch?v=EmO7NrWHkLM \n\n https://www.youtube.com/watch?v=NVBU9RYZ6UI \n\n https://www.youtube.com/watch?v=Yy74yvjatVg \n\nSynergisticIT at Gartner Data and Analytics Summit 2023 - YouTube\n\nFor preparing for interviews please visit  https://www.synergisticit.com/interview-questions/ \n\n We are looking for the right matching candidates for our clients \n\n Please apply via the job posting \n\n REQUIRED SKILLS For Java /Full Stack/Software Programmer \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning Positions \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Tensorflow \n\n If you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team. \n\n No phone calls please.  Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates"}, {"title": "Data Engineer", "company_name": "Anika Systems", "location": "Leesburg, VA (Remote)", "link": "/jobs/view/3787766893/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=mApL9JnN%2B%2Fr2%2BYUPlxRYOA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787766893", "description": "About the job\nAnika Systems is a fast growing, woman-owned small business that specializes in providing innovative IT solutions for federal government agencies. Our expertise lies in accelerating delivery in Data and Analytics, Intelligent Automation, Application Development, and IT Modernization. We are currently expanding our Federal team and are seeking a passionate and talented Data Engineer. This opportunity is 100% remote.\n\nMust be a U.S. Citizen with the ability to obtain and maintain a government suitability clearance\n\nResponsibilities\n\nDesigns, develops, builds, analyzes, evaluates, and installs database management systems to include database modeling and design, relational database architecture, metadata, and repository creation and configuration management.\nDefines and oversees database organizations, standards, controls, procedures, and documentation. Provides technical consulting in the definition, design, and creation of a database environment.\nAdvises applications development staff and users on data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nDesigns and implements databases with respect to access methods, access time, batch processes, device allocation, validation checks, organization, protection and security, documentation, and statistical methods.\nUses data mapping, data mining, and data transformational analysis tools to design and develop databases.\nDetermines data storage and optimum storage requirements.\nPrepares system requirements, source analysis, and process analyses and designs throughout the database implementation.\n\nRequired Skills And Experience\n\nBA/BS and 1 year of relevant experience\nExperience with DataBricks, SQL, Python\nApplications development experience with data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nExcellent communication skills (both oral and written) with process-oriented organizational skills to ensure project success\n\nDesired Skills And Experience\n\nDemonstrated successful development experience with the full life cycle of government database management systems including database modeling and design, relational database architecture, metadata and repository creation, and configuration management.\n\nPowered by JazzHR\n\nTlKWIorucJ"}, {"title": "Remote Work - Need Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3774712978/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=r3Bp%2F0qcOD2OpC5jpbUOcA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774712978", "description": "About the job\nTitle : Data Engineer\n\nLocation : Remote\n\nDuration : 6+ months to start (long term contract (work order are extended 2x year))\n\nPlease Fill out the skill matrix: Technology Some Awareness (1) Novice (2) Intermediate (3) Advanced (4) Expert (5) SQL Tableau Desktop /Creator Tableau API Integration AWS - S3 Glue AWS - Athena Redshift AWS - Lambda AWS - SNS/SQS msExcel\n\nProfile: Data Analyst will work closely with Architects and leads in the Interflow team, key business stakeholders including program staff to support the organizations effort in the areas of data Analysis and information management. Role should have strong experience in writing SQL queries to extract data for reports. Candidate should have experience working with large set of data. As these reports are very critical for the company and we are looking for an expert who is capable of handling, consuming and extracting large sets of data for analysis and reports.\n\nMust Haves\n\nAWS - Athena\nAWS - S3\nGlue\nhands-on development of reporting applications\nMS Excel\nRedShift\nSQL\nTableau Desktop /Creator\n\nNice\n\nAWS - Lambda\nAWS - SNS/SQS\nTableau API Integration\n\nSkills\n\nBA/BS required (major in an analytical field desired) \nA minimum 10+ years work-related experience. \nExperience in hands-on development of reporting applications for the Web in a professional environment \nThe Ethos of continuous improvement and interest in learning new things.\nStrong analytical thinking and structured problem-solving ability\nAbility to handle multiple projects and assignments simultaneously and effectively in a cross-functional team environment. \nVersed on the agile methodology and best practices. \nExcellent interpersonal and collaboration skill with the ability to work with a diverse set of colleagues, across functions, from different organizations, disciplines, etc. \nSelf-starter, ability to set priorities, work independently and attain goals"}, {"title": "Data Engineer", "company_name": "Team Remotely Inc", "location": "Houston, TX (Remote)", "link": "/jobs/view/3786249829/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=78lY5Jo%2FfyRPC6TQYCdvww%3D%3D&trk=flagship3_search_srp_jobs", "id": "3786249829", "description": "About the job\nThis is a remote position.\n\n Data Engineer (US/Canada Residents Only, 1 year experience, remote)\n\nTeam Remotely Inc. is a staffing and recruitment agency that offers a comprehensive solution for talent acquisition, including sourcing, vetting, pay rolling, and managing talent. Whether you need contract staffing, direct hire, direct sourcing, talent pools, or diversity initiatives, our model can support your hiring strategy.\n\n Hiring Type: Full-Time\n\n Base Salary: $55K-$70K Per Annum.\n\nResponsibilities\n\nDesigning user interface changes for web-based DB applications. \nReviewing application requirements and interface designs. \nDeveloping and implementing highly responsive user interface components using react concepts. \nWriting application interface codes using JavaScript following react.js workflows. \nTroubleshooting interface software and debugging application codes. \nDeveloping and implementing front-end architecture to support user interface concepts. \nMonitoring and improving front-end performance. \n\nQualifications And Experience\n\nBachelor\u2019s degree in computer science, information technology, or a similar field. \nPrevious experience working as a react.js developer. \nIn-depth knowledge of JavaScript, CSS, HTML, jQuery, and front-end languages. \nKnowledge of REACT tools including React.js, Webpack, Enzyme, Redux, and Flux. \nExperience with user interface design. \nKnowledge of performance testing frameworks, including Mocha and Jest. \nExperience with browser-based debugging and performance testing software. \nTroubleshooting skills. \nProject management skills. \nProblem-solving skills. \nVerbal communication skills. \n\n Why work with Team Remotely?\n\nTeam Remotely Inc. is a staffing platform offering a seamless experience for employers and candidates. Employers can post job openings and specify their requirements, while candidates can create profiles and upload resumes.\n\nThe team of Team Remotely continuously learns and adapts based on previous successful placements, constantly improving its matching capabilities. This ensures that the recommendations provided by Team Remotely are tailored and accurate, increasing the likelihood of a successful match between employers and candidates. By providing intelligent and data-driven solutions, they strive to enhance the efficiency and effectiveness of the hiring process, ultimately helping companies find the best talent and individuals find their dream jobs."}, {"title": "Data Engineer", "company_name": "ZAG Technical Services", "location": "United States (Remote)", "link": "/jobs/view/3771403464/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=Hf0VefrKXBFIL5MtHmY0Pw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3771403464", "description": "About the job\nWe are looking for a tech savvy Data Engineer to contribute to the building, operating, and scaling of next generation data platforms and tools that will power data-driven analytics capabilities throughout the ZAG products and services portfolio, spanning areas such as business intelligence, reporting, and data analytics.\n\nThe ideal team member is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. Must be comfortable supporting the data needs of multiple clients and teams, systems, and products within the modernization of ecosystems. The right-fit person will be excited by the prospect of optimizing or even re-designing data processes to support next generation of advanced analytics capabilities, which in-turn translates to implementing strategic enhancements to maintain competitive advantage in the industry.\n\nThe successful team member will leverage their proficiency to\u2026\n\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\nBuild the analytics processes required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Data Management and cloud based \u2018big data\u2019 technologies.\nBuild analytics tools that utilize technologies to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.\nCollaborate with client teams to identify and solution regional and local requirements within the context of the standard global business model for enterprise analytics.\nParticipates in meetings, discussions, strategy sessions where changes, improvements, and enhancements are proposed, evaluated, and approved. \n\n\nThe exceptional team member will have or demonstrate progressive experiences in\u2026 \n\nBachelor\u2019s degree or higher in Computer Science or related field\nExperience working with BI and data warehousing tools\nExperience working with enterprise data, where security is paramount and data governance is critical\nAbility to work independently and in a team environment, and effectively engage all levels of the organization\nUnderstanding of distributed systems driving large-scale data processing and analytics with a successful history of manipulating, processing, and extracting value from large, disconnected datasets.\nAbility to communicate effectively (listening, presenting, and questioning)\nStrong organizational, written, and communication skills\n\n\nAbout ZAG\n\nZAG Technical Services empowers our clients\u2019 success through intelligent business solutions that increase employee productivity, network security and team collaboration. We aim to ensure that our client\u2019s IT investments drive ROI, provide a competitive advantage and help businesses grow. We do this by living our core set of values: Integrity Foremost, Client-Centered, Accountable Always, Teamwork Throughout, and Exceptional is the Goal.\n\nWhat ZAG Has To Offer\n\nCompetitive compensation package \nBenefits including medical, dental, vision, 401K and life insurance \n\n\nZAG Technical Services is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all of our employees.\n\nZAG Technical Services is an E-Verify participant and will provide the Social Security Administration (SSA) and, if necessary, the Department of Homeland Security (DHS), with information from each new employee\u2019s Form I-9 to confirm work authorization."}, {"title": "Junior Data Engineer", "company_name": "HireMeFast LLC - Career Accelerator - Land A Job", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3786249797/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D&trk=flagship3_search_srp_jobs&lipi=urn%3Ali%3Apage%3Ad_flagship3_search_srp_jobs%3BQExogrHBQoSF9zZm%2B9kufw%3D%3D&lici=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D", "id": "3786249797", "description": "About the job\nThis is a remote position.\n\n Job Title: Junior Data Engineer\n\n Employment Type: Full-Time\n\n Salary: $64,000 - $76,000 per annum\n\n Experience Required:  Minimum 1 year of project experience\n\nAbout us: HireMeFast is a leading staffing and recruitment agency specializing in connecting businesses with top-tier talent across various industries. Our mission is to bridge the gap between exceptional candidates and organizations needing their skills, expertise, and unique qualities. Our team of experienced and dedicated recruitment specialists utilizes innovative sourcing strategies, and a vast network to identify and attract top talent. We conduct comprehensive procedures to ensure that only the most qualified candidates are presented to our clients.\n\nPosition Summary\n\nJoin the fast-paced, innovative, and collaborative environment focused on providing an AIOps platform that enhances the intelligence of the CVS Health infrastructure. Work closely with subject matter experts and colleagues to build and scale out machine learning and AI solutions that will detect, predict, and recommend solutions to correct issues before system impact and enhance efficiency, reliability, and performance of CVS Health\u2019s IT operations.\n\nKey Responsibilities include:\n\nData pipeline development: Designed, implemented, and managed data pipelines for extracting, transforming, and loading data from various sources into data lakes for processing, analytics, and correlation. \nData modeling: Create and maintain data models ensuring data quality, scalability, and efficiency \nDevelop and automate processes to clean, transform, and prepare data for analytics, ensuring data accuracy and consistency \nData Integration: Integrate data from disparate sources, both structured and unstructured to provide a unified view of key infrastructure platform and application data \nUtilize big data technologies such as Kafka to process and analyze large volumes of data efficiently \nImplement data security measures to protect sensitive information and ensure compliance with data and privacy regulation \nCreate/maintain documentation for data processes, data flows, and system configurations \nPerformance Optimization- Monitor and optimize data pipelines and systems for performance, scalability and cost-effectiveness \n\nCharacteristics of this role:\n\nTeam Player: Willing to teach, share knowledge, and work with others to make the team successful. \nCommunication: Exceptional verbal, written, organizational, presentation, and communication skills. \nCreativity: Ability to take written and verbal requirements and come up with other innovative ideas. \nAttention to detail: Systematically and accurately research future solutions and current problems. \nStrong work ethic: The innate drive to do work extremely well. \nPassion: A drive to deliver better products and services than expected to customers. \n\nRequired Qualifications\n\n2+ years of programming experience in languages such as Python, Java, SQL \n2+ years of experience with ETL tools and database management (relational, non-relational) \n2+ years of experience in data modeling techniques and tools to design efficient scalable data structures \nSkills in data quality assessment, data cleansing, and data validation \n\nQualifications\n\nKnowledge of big data technologies and cloud platforms \nExperience with technologies like PySpark, Databricks, and Azure Synapse. \n\nEducation\n\nBachelor\u2019s degree in Computer Science, Information Technology, or related field, or equivalent working experience\n\nWhy HireMeFast LLC?\n\nAt HireMeFast, we understand that finding the right individuals to join your team is crucial for success and growth of your organization. We are committed to streamlining the hiring process for our clients, ensuring they have access to a diverse pool of highly qualified candidates who are a perfect fit for their specific needs."}, {"title": "Data Engineer  - Need candidates from Texas ( REMOTE )", "company_name": "Pulivarthi Group (PG)", "location": "United States (Remote)", "link": "/jobs/view/3770685212/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=9gdMwuif%2BpB%2BCQYvQi3imQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3770685212", "description": "About the job\nFollow us on Linkedin: https://www.linkedin.com/company/pulivarthigroup/\n\nPulivarthi Group LLC is a Global Staffing & IT Technology Solutions company, with our prime focus of providing world class solutions to our customers with the right talent. We combine the expertise of our team and the culture of your company to help you with the solution that is affordable and innovative using high quality standards and technologies.\n\nWe\u2019ve served some of the largest healthcare, financial services, and government entities in the U.S.\n\nData Engineer - FT only - Azure SQL with data lakes experience\n\nData Engineer -\n\nthe resource requirement for this Data Engineer role needed more clarification.\n\nOur customer is requesting a resource who knows Azure CI/CD (Continuous Integration / Continuous Deployment) to build and integrate their DevOps pipeline.\n\nOnce the DevOps pipelines are mapped, they need the Data Engineer to connect to all the Data Sources, primarily their Data Lake.\n\nThe Customer (based in Houston) is interested in a resource in a time zone not too far removed from the Central Time Zone to allow real time collaboration."}, {"title": "Data Engineer", "company_name": "Otter Products", "location": "Fort Collins, CO (Remote)", "link": "/jobs/view/3766728302/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=77U3U7CAi1PxgDmpI3%2BiNQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3766728302", "description": "About the job\nOverview \n\nOtter Products is currently recruiting for a Data Engineer to join our Data team! You can be based in our Fort Collins, CO office with flexibility to work a portion of your time remotely, or 100% Remote in the U.S.\n\nThe Data Engineer applies professional experience, concepts, and company objectives toward building and operationalizing the minimally inclusive data necessary for the enterprise data initiatives following industry standard practices and tools. The bulk of the Data Engineer's work would be in designing, managing and optimizing data pipelines, then moving these data pipelines effectively into production for key data and analytics consumers like business/data analysts, data scientists or other stakeholder that needs curated data for data and analytics use cases across the enterprise. The Data Engineer also ensures compliance with data governance and data security requirements while creating, improving and operationalizing these integrated and reusable data pipelines to enable faster data access, integrated data reuse and vastly improved time-to-solution for data and analytics initiatives. Additionally, the Data Engineer will be expected to collaborate with other data team members and data consumers working on the models and algorithms developed by them toward optimization for data quality, security, and governance to put them into production leading to potentially large productivity gains.\n\n About Otter Products \n\nOtter Products, we grow to give. From our founder's garage in 1998 to the global technology leader we are today, Otter Products continues to drive growth through innovation.\n\nThrough our industry-leading brands - OtterBox, Liviri and OtterCares - we provide our partners the number-one selling and most trusted products in our categories. Our philanthropic spirit is the foundation on which we foster our partner relationships, allowing us to grow and to give - together.\n\nBy way of our charitable arm, the OtterCares Foundation, we support our communities and invest in our future through education that inspires kids to change the world.\n\nAnd even as our global community of Otters continues to grow, our founder's core values are still at the heart of everything we do. We measure our success by our ability to give back to our communities and strengthen opportunities for all.\n\nFor more information visit otterproducts.com\n\n Responsibilities \n\nComplete analysis, design and development of BI solutions using Azure SQL Server\nFamiliarity with Data Warehouse concepts\nExperience coding the extraction, transformation, and loading (ETL) processes\nDatabase development primarily in SSIS, Data Factory and SQL\nPartner with the business to determine end user database/reporting needs and requirements\nGenerate ad hoc reports using Power BI or SSRS\nCollaborate with other developers to create and implement best approach solution(s)\nTroubleshoot Azure tools, systems, and software\nReview queries for performance issues, making changes as needed\nCollaborate with team to performance-tune Azure application as necessary\nAssist with the analysis and extraction of relevant information from large amounts of historical business data to feed data science initiatives\nParticipate and support the design and documentation of processes for large scale data analyses, model development, model validation and model implementation\nSupport and maintain a positive safety culture by following all safety policies and procedures and actively contributing to a safe working environment\nOther duties as required\n\n Qualifications \n\nBachelor's degree required. Degree in Computer Science or Mathematics preferred\nMinimum of four years of experience in an IT or analytical role required. Experience in database development, report writing and/or statistics preferred.\n\n EEO \n\nOtter Products, LLC is an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, age, marital status, pregnancy, sex, sexual orientation, gender, gender identity or expression, national origin, disability, veteran status, or any other characteristic or status protected by law.\n\n For US Based Roles Only - Compensation Range Minimum \n\nUSD $90,000.00/Yr.\n\n For US Based Roles Only - Compensation Range Maximum \n\nUSD $120,000.00/Yr.\n\n Additional Total Rewards \n\nProfit Sharing Program Eligible, Benefits Eligible - Full Time- check out otterproducts.com/careers/why for more info"}, {"title": "Software Engineer, Distributed Systems", "company_name": "Hightouch", "location": "United States (Remote)", "link": "/jobs/view/3628572539/?eBP=CwEAAAGMd4QO8APODyE_-mMOPhYyQGg5ZjVbPwq_3V7DK0B2bLAf10pcedM9S2T71X5i4TiJuEwOzWBbSiTKbLuk-VD-_Y-dN5uyf8YpYkHYqEYFxySMS9zr2hRjNrxX9gXbxssh35TFK9YTNV2WeH2Ncb675JgFbJTgNWSZySFC6rrdJGmdSDTr2q5d3VoxORz3GjsDbzjeyZQCvQ07RqYw7oujWSgslycs2LMCY_-N0NEj6e0PvbaYKiZ_5IW34bGVSYLXpHwqa8oOcNttYZ90UA3E_7sDSFzABdIVlfrp5zzERrIYy5sh2MIInxHdDoKgQw8VmOn5uM4S0tXUl-i7LnN0_W0NfPBmLiVDNuveIOm4MfDNPxfe5QKPk6FUAVpvgbcoJn9yRyE&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=rph%2FT6Q6Vkp6YoggJf9cJQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3628572539", "description": "About the job\nAbout Hightouch\n\nHightouch\u2019s mission is to empower everyone to take action on their data. Through our Reverse ETL platform, business and data users can seamlessly sync data from where it resides, such as warehouses and databases, to where it is needed, including operational systems and SaaS tools. Traditionally, acting on data has required engineering time and bandwidth, and left most business users stuck with charts and reports that are unable to take automated action on their data. With Hightouch, every business user, without writing any code, can activate data to streamline critical processes, improve marketing performance, and scale operations.\n\nOur team operates with a focus on making a meaningful impact for our customers. We believe in approaching challenges with a first principles thinking mindset, moving quickly and embracing our value of efficient execution, and treating each other with compassion and kindness. We look for team members that are strong communicators, have a growth mindset, and are motivated and persistent in achieving our goals.\n\nHundreds of companies use Hightouch, including Spotify, Ramp, Retool, NBA, Plaid, and Betterment. We\u2019re based in San Francisco, are remote-friendly, and backed by leading investors such as Amplify Partners, ICONIQ Growth, Bain Capital Ventures, Y-Combinator, and Afore Capital.\n\nAbout The Role\n\nWe are looking for a distributed systems engineer to work on the systems that power the syncing engine our customers and other engineering teams rely on. This presents an exciting challenge where you can apply your expertise in distributed systems, performance optimization, and troubleshooting to push the boundaries of what is possible and meaningful improve our ability to move massive amounts of customer data. This role also provides a unique opportunity to work on a multi-cloud and multi-region infrastructure that supports a global customer base.\n\nWe believe in enabling our engineers to do their best work for our customers by giving them extremely high levels of ownership and autonomy. This comes in different forms: you will own and deliver projects from start to finish, you will work directly with customers to solve their hardest scaling problems, and you will have a lot of influence over what we work on as a team and company.\n\nSome of the problems we\u2019ll be working on include:\n\nSync Speed: Customers want to sync a lot of data to important destinations like Facebook and Snapchat, which requires us to analyze every part of our syncing process and find where we can optimize to sync data more quickly\nStreaming Syncing: We currently sync data from sources in batch only, but envision a world where we also do syncs in real-time and support streaming sources like webhooks and queues\nScalability and Reliability: As part of our rapid growth, we\u2019re always evaluating current future bottlenecks in scaling and reliability, and architecting for the next order of magnitude of growth\nPersonalization API: Our Personalization API product provides a low latency caching layer on top of data warehouses that enable customers to use their data warehouse for real-time personalization use cases with \nMulti-Region and Multi-Cloud: Supporting our multi-region and multi-cloud backend, including extending it to launch Hightouch on in new regions to support data residency requirements of our global customer base\n\nWe are looking for talented, intellectually curious, and motivated individuals who are interested in tackling the problems above. This is a senior role, but we focus on impact and potential for growth more than years of experience. The salary range for this position is $170,000 - $240,000 USD per year, which is location independent in accordance with our remote-first policy.\n\nAbout You\n\nYou are an engineer with a passion for solving hard technical problems that generate real value for customers. You\u2019re motivated by high ownership and are comfortable in a fast-paced, startup environment.\n\nYou have experience and comfort with distributed systems and high-scale systems or the ability and desire to build this experience quickly. This experience can take different forms:\n\nYou\u2019ve built syncing engines that process and move terabytes of data per day in a scalable way\nYou\u2019ve created event collection or data streaming services that power company critical systems\nYou\u2019ve owned systems that have undergone orders of magnitude of growth and helped them scale accordingly\nYou have relevant skills and are a fast learner who is excited to take on the challenges we face\n\nInterview Process\n\nOur goal with the interview process is to balance speed with giving both parties opportunities to assess whether there is a strong mutual fit. We will ask you questions, but we want you to ask us questions! Our technical interviews focus on how you design systems because we believe this is the best way for us to see how you work and for you to see how we collaborate. We don\u2019t ask you to write code to solve technical brainteasers that don\u2019t appear in your day to day job.\n\nRecruiter screen [30m]: Introductory call with our recruiting team to get to know each other and see if the role could be a good mutual fit.\nSystems design screen [45m]: Designing a data processing feature end-to-end.\nHiring manager interview [30m]: Chat with hiring manager about past experiences and future operating preferences to assess fit on company values and operating principles.\nSystems design interview [90m]: Work with the interviewer to architect a system at a conceptual level. The problem will be at a pretty high level - and have both product and customer requirements as well as technical."}, {"title": "Entry-Level Data Scientist Engineer", "company_name": "Phoenix Recruitment LLC", "location": "Texas City, TX (Remote)", "link": "/jobs/view/3788727625/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=8iwtytHsCu4wMwJ4fj3pxw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3788727625", "description": "About the job\nThis is a remote position.\n\nTitle - Entry-Level Data Scientist Engineer, 1 year of project experience\n\nEmployment Type: Full-time\n\nBase Salary: $60K-$70K\n\nPhoenix Recruitment offers a variety of recruiting services to assist both employers and employees. They specialize in marketing open positions, recruiting, and helping employers to find qualified candidates across various industries. Phoenix Recruitment has expertise in streamlining the hiring process. They can help ensure that the process is efficient, well organized, and compliant with relevant regulations.\n\nSkills and Abilities:\n\nStrong knowledge of R or Python for data analysis and modeling. \nProficiency in statistical programs such as R, SAS, MATLAB, or Python. \nFamiliarity with spreadsheets (VBA) and database applications (Access, Oracle, SQL, or equivalent technology). \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \nAbility to learn new methods quickly and work under deadlines. \nExcellent teamwork and communication skills. \nStrong analytical and problem-solving abilities. \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \n\nPreferred:\n\nKnowledge of actuarial concepts and life, health, and/or annuity products. \nExperience with statistical modeling techniques such as GLM, Decision Trees, Time Series, Regression, etc. \nFamiliarity with Microsoft DeployR. \nExposure to insurance risk analysis. \nBasic experience in computational finance, econometrics, statistics, and math. \nKnowledge of SQL and VBA. \nFamiliarity with R or Python for predictive modeling \n\nWhy Phoenix Recruitment LLC?\n\nPhoenix Recruitment often has an extensive network of employers and candidates. This network allows them to tap into a pool of qualified candidates and connect them with suitable job opportunities. They can also leverage their connections to help employers find the right talent efficiently. Outsourcing the recruitment process to a specialized agency can save your time and resources, avoid delays, reduce administrative burdens, and increase the chances of finding the right fit for your organization."}, {"title": "Data Engineer - Remote Work", "company_name": "BairesDev", "location": "United States (Remote)", "link": "/jobs/view/3783162297/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=OtoCrAAHZFomVup04fy35g%3D%3D&trk=flagship3_search_srp_jobs", "id": "3783162297", "description": "About the job\nWho We are\n\nBairesDev is proud to be the fastest-growing company in America. With people in five continents and world-class clients, we are only as strong as the multicultural teams at the heart of our business. To consistently deliver the highest quality solutions to our clients, we only hire the Top 1% of the best talents and nurture their professional growth on exciting projects.\n\nWe are looking for a Data Engineer to join our Development team and participate in different projects made up of multicultural teams distributed throughout the world.\n\nWhat You Will Do\n\n Interview other Data Scientists and Data engineers to vet their technical abilities.\n Input data from those interviews into the product and help develop a predictive, fair, and enjoyable solution.\n Work together with developers, tech leads, and solution architects to build applications.\n Improve existing structures by adding new functionalities or proposing technological updates so that as a result, the impact of your contribution is significant in the core of each business.\n Improve the user experience on scalable and high availability platforms, contributing to the key differential of each business.\n\nHere\u2019s What We Are Looking For\n\n 5+ years of experience working as a developer.\n 3+ years of experience working as a Data Scientist / Data Engineer.\n Proficient with analysis, troubleshooting, and problem-solving.\n Hands-on experience with managing data loads and data quality.\n Advanced English level.\n\nHow we do make your work (and your life) easier:\n\n 100% remote work.\n Hardware setup for you to work from home.\n Flexible hours - make your schedule.\n Paid parental leave, vacation & holidays.\n Diverse and multicultural work environment.\n An innovative environment with the structure and resources of a leading multinational.\n Excellent compensation \u2014 well above the market average.\n Here you can grow at the speed of your learning curve.\n\nOur people work remotely but with a consistent and robust culture that promotes diversity and teamwork. To continue being the leading software development company in Latin America, we want to ensure that every BairesDev member gets the best growth and professional development opportunities in a diverse, welcoming, and innovative environment.\n\nEvery BairesDev team member brings something unique to our company.\n\nWe want to hear your story. Apply now!"}, {"title": "Data Engineer (Remote)", "company_name": "Cascade Debt", "location": "United States (Remote)", "link": "/jobs/view/3755336571/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=pmUtn1j2zyPr25CNPh2RNw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3755336571", "description": "About the job\nDo you want to work in a small team where you can make a real difference in a company? Would you embrace the challenge of building a high impact platform that is used globally? Then we want to speak with you!\n\nAbout Cascade\n\nCascade is a fintech startup backed by Canadian and US investors that empowers companies to grow by democratizing access to institutional debt. We are building tools that modernize how companies raise and manage debt, lowering the barriers to entry to the $7 trillion specialty finance and alternative credit market for companies around the world. Founded just last year, we are gearing up for our next phase of technical development and are seeking a talented Data Engineer to lead the way.\n\nAbout The Role\n\nAs a Data Engineer you\u2019ll be an early member of a growing team building a pioneering platform in the debt infrastructure space. You will design, build, and launch efficient, scalable, and reliable data pipelines to move and transform data .\n\nWhat You\u2019ll Do\n\n Building databases, data lakes and data ingestion pipelines to integrate customer databases and datasets to our systems \n Ingesting financial datasets from external customers, then u pdating and maintaining accurate and complete data mappings to ensure that our product s are displaying high quality data \n M onitoring and alerting across our data pipelines in order to make sure that our data ingests are reliable and correct \n Translat ing business goals and stakeholder requirements into new data flows and deliver analytical insights ensuring that the data flows create real business value continuously \n\nSkills And Experience\n\n 2 + years experience in a data engineering or data analyst role \n E xperience with relational SQL and NoSQL databases, including PostgreSQL , MySQL, MariaDB, MongoDB, Bigtable, etc. \n E xperience working with ETL technologies , such as Databricks, Fivetran , or dbt \n Proficiency in writing SQL queries and knowledge of analytical data warehouses such as RedShift, BigQuery , and Snowflake \n Some experience with CI/CD automation such as GitHub Actions \n Developing APIs and integrating with 3rd party APIs \n Bonus: experience in fintech / SaaS / credit analysis \n\nPeople Who Thrive At Cascade Are\n\n Self-starters, who take the initiative to tackle challenges in a remote work environment \n Good communicators, who can operate without ego to discuss, learn, grow, and help others do the same \n Passionate about creating great digital experiences for users \n Problem solvers who are not satisfied with the status quo \n\nBenefits\n\nRemote: we are a remote-first company and are very flexible on hours as long as things get done \nHome-office: there\u2019s a $1000 USD home office allowance to set yourself up \nEquity: we expect you to have an owner-mentality, and have the equity plan to match \nBenefits: health, dental, vision, and more \nPerks: we offer free lunches weekly and off-site trips \nJob satisfaction: we offer autonomy, ample opportunities for mastery, and an opportunity to make a difference for companies around the world"}, {"title": "Data Engineer", "company_name": "ISC (Integrated Specialty Coverages, LLC)", "location": "Carlsbad, CA (Remote)", "link": "/jobs/view/3778293528/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ew7%2FfEuXSujs5IGabdqUyw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3778293528", "description": "About the job\nAbout Integrated Specialty Coverages\n\nIntegrated Specialty Coverages LLC (ISC) is a growth stage technology and data-driven insurance company leading innovation in the market.\n\nBacked by one of the leading private equity firms, KKR, and led by a forward-thinking management team, ISC is combining the worlds of insurance and technology to create an Insurtech powerhouse. As a leading online distributor of insurance products for a range of industries and \"Main Street USA\", we are looking for the right people to help us in our mission of achieving exponential growth. We strive to be the number one place to go for brokers and agents to source insurance. To accomplish this, we're building a digitally focused team that deeply understands the intersection between user experience, data, and AI/ML to optimize the way we engage with our customers and partners.\n\nJob Summary\n\nThe Data Engineer role will be the technical liaison between multiple groups including a data science team, the engineering team, product management, and business stakeholders. You do not need any insurance knowledge prior, however, you must quickly dive deep into the insurance world and ask questions to become a subject matter expert. You will be responsible for building a data platform to facilitate the data science team. You must be a self-starter that can build out features such as a data pipeline from scratch. There will be support from both engineering and data science for any buildout.\n\nJob Responsibilities\n\nCreate and maintain optimal data pipeline architecture. \nAssemble large, complex data sets that meet functional / non-functional business requirements. \nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. \nBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using AWS technologies, SQL, and Python. \nWork with stakeholders including the Executive, Product, Data Science, and Engineering teams to assist with data-related technical issues and support their data infrastructure needs. \nWork with data science and analytics teams to strive for greater functionality in our data systems. \n\nMinimum Qualifications:\n\nAdvanced working SQL knowledge and experience working with relational databases, strong query authoring (SQL) as well as working familiarity with a variety of databases (Snowflake, Redshift, MySQL, MSSQL, etc.)\nExperience building and optimizing data pipelines, architecture and data sets. \nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. \nBuild processes supporting data transformation, data structures, metadata, dependency, and workload management. \nA successful history of transforming, processing and extracting value from large disconnected, datasets from a variety of data sources (Flat files, Excel, databases, etc.)\nStrong analytic skills related to working with unstructured datasets. \nStrong project management and organizational skills. \nExperience supporting and working with cross-functional teams in a dynamic environment. \nAbility to mentor/guide/collaborate with other team members. \n\nWe are looking for a candidate with 3-5 years of experience in a Data Engineer role. They should also have experience using the following software/tools:\n\nExperience with relational and MPP databases, including Snowflake and MySQL. \nExperience developing software in an agile environment from the requirements stage to production. \nExperience with version control: git\nExperience with container technologies: Docker\nExperience with data pipeline and workflow management tools: Airflow, Jenkins, AWS Glue, Azkaban, Luigi, etc. \nExperience with AWS cloud services: EC2, ECS, Batch, S3, EMR, RDS, Redshift\nExperience with other cloud services: Snowflake, Airflow\nExperience with object-oriented/object function scripting languages: Python, Java, C++, etc. \nExperience with data modeling and data warehouse design\nExperience with data visualization tools (PowerBI, QuickSight)\n\nThe starting annual pay scale for this position is listed below. Actual starting pay will be based on factors such as skills, qualifications, training, and experience. In addition, the company offers comprehensive benefits including medical, dental and vision insurance, 401(k) plan with match, paid time off, and other benefits.\n\nISC's salary ranges are determined by role and level. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations and could be higher or lower based on a multitude of factors, including job-related skills, experience, and relevant education or training.\n\nNational Pay Range\n\n$100,000\u2014$110,000 USD\n\nBenefits of Working at ISC\n\nCompetitive vacation and flexible working arrangements\nComprehensive and inclusive health benefits\nA variety of professional development and mentorship opportunities\nChoice of technology whether at home or in the office\n\nISC's Ownership Behaviors\n\nDo the Right Thing *Be Relentless. Pursue Excellence. *Personal Responsibility & Own Outcomes\nTry Fast. Learn Fast. Fail Fast. Think Big. *Build Extraordinary Owners & Join Forces *Grit & Determination\n\nApplicants with disabilities may contact ISC HR department via e-mail, and other means to request and arrange accommodation. If you need assistance to accommodate a disability, you may request accommodation at any time. Please contact ISC at HR@ISCMGA.com.\n\nISC believes in creating long-term relationships by being responsive and relevant and by consistently delivering value to our community of customers. Specifically, with our employees, we focus on attracting, developing, and retaining the best talent for our business, challenging our people, demonstrating a \"can-do\" attitude, and fostering a collaborative and mutually supportive environment. ISC is an equal opportunity employer and a participant in the US federal E-Verify program (US).\n\nDiversity creates a healthier atmosphere: All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, gender, gender identity, sexual orientation, marital status, medical condition, genetic information, mental or physical disability, military or veteran status, or any other characteristic protected by local, state, or Federal law."}, {"title": "Data Engineer -Remote", "company_name": "RIT Solutions, Inc.", "location": "Denver, CO (Remote)", "link": "/jobs/view/3768007496/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=1hQH8EDaII1eJ7nQ9BACQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3768007496", "description": "About the job\nDuration: 6+ months CTH\n\nMust have a valid LinkedIn profile\n\nRequired Skills\n\n 3-6 years of experience (no more than 6 years)\n Start up environment experience\n Python, SQL, and DBT"}, {"title": "Big Data Engineer - W2", "company_name": "Megan Soft Inc", "location": "United States (Remote)", "link": "/jobs/view/3674845652/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fCAxh4N%2F4YZJ5aEIASaeeA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3674845652", "description": "About the job\nTitle: Big Data Engineer - W2\n\nLocation: Dearborn, MI (Remote)\n\nDuration: 12+ Months\n\nDirect Client: Ford Motors\n\nMust have skills: Should be able to handle GCP requirements\n\n\"Translates business requirements to build repeatable, sustainable, efficient, coded processes that can be productionized by Software Engineers and readily modified by other Data Engineer Creates POC processes in Dev/QA and works with Software Engineers to productionize and align best coding and process practices that maximize efficiency, speed, stability, system resources and capabilities Awareness of and compliance with data privacy, security, legal and contractual guidelines Incorporates data quality and privacy checks/alerts to minimize bad data being consumed by end users, models and dashboards, and to protect customer data; revises checks for new data issues Validates data products and pipelines are functioning as expected following system or application upgrades, source changes, etc. Works closely with Architects if required to align systems, tools and applications being utilized with business use case and performance requirements Provides data product support and maintenance Establishes, tracks and monitors KPIs related to specific data products and deliverable Participate in iteration cadence activities (Iteration Planning, task estimation, daily scrum, demos, etc.). Will be a part of on-call team to resolve critical production support tickets during business hours and off hours.\n\nSkills Required\n\nExperience with Big Data Technologies (Hadoop , ETL Tool, Spark. Hive etc..) Someone who understands Cloud and has experience working with Cloud Platforms (preferably GCP) Should be able to handle GCP requirements - ingesting into GCS/ Big Query, working with GCP components Hands-on in Data Flow, Cloud Storage (GCS), Big Query and GitHub. Proficient in Python and SQL.\n\nSkills Preferred\n\nManages ETL: uses programming and tools for data ingestion, configures pipelines, applies transformations and decoding, integrates and fuses data, moves and securely deliver Experience NoSQL databases\n\nAdditional Information\n\nIf the candidate is remote only, please indicate \"\"100% Remote\"\" under candidate's name on resume. Other similar schedule notations can be \"\"Local Candidate\"\" or \"\"Hybrid Candidate.\"\" ***HRA Test Required***\""}, {"title": "Remote Opportunity: Quantexa Data Engineer", "company_name": "SPAR Information Systems LLC", "location": "United States (Remote)", "link": "/jobs/view/3680073191/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ioykVCZSOQ7BK5%2FwRQHOWg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3680073191", "description": "About the job\nHello All, Hope you are doing great!! Please go through the job descriptions and let me know your interest. Role: Quantexa Data engineer  Location: Remote Duration: Long Term Contract Job Description: 1. End to End usage of Quantexa including guiding our client team on how to use from both front end and data ingestion Reason they are asking for Certification 2. Resolve Entity issues with data 3. Knowledge of Scala and Hive which requires coding experience using parquet 4. Advanced SQL knowledge requiring writing SQL queries 5. Experience having worked with GCP data lake 6. Quantexa certification required\n\nThanks & Regards,\n\nSatnam Singh\n\nDirect: 201 623 3660\n\nEmail : Satnam.singh@sparinfosys.com"}, {"title": "Remote Work - Need Staff Data Engineer - USC only", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3746283980/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=CvIuLBNO%2BPMGi4IXVRkP0A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3746283980", "description": "About the job\n100% Remote, client is in NY\n\nLook for strong job history, Python, Machine Learning, SQL, 10/10 comms, etc. \n\nMUST be an Engineer \n\nSummary\n\nWe're looking for an experienced Data Engineer to join our client's growing team of top notched IT professionals. You will work closely across multiple groups including Editorial and Audio, Marketing, Advertising, etc. to achieve their goals and objectives. The Data Engineering team builds tools throughout the stack while sharing knowledge across these departments.\n\nAs the company grows, they're looking for Data Engineer who will help solidify and expand our pipelines and maintain their data warehouse. Their business model is based on performing complex and very detailed analyses of how our products perform with their subscribers. You'll be responsible for working alongside analysts, product, marketing, and other teams to define new data collection components and measurement schemes that support each new product and feature.\n\nHow Can Add Value To Their Mission\n\nCreate and maintain data pipelines to provide insights and drive business decisions\nEstablish data warehousing strategy (ex. Kimball, Data Vault, etc.)\nMaintain data infrastructure on our AWS accounts\nCollaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.\nWrite unit/integration tests, contributes to engineering wiki, and documents work\nImplement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it\n\nWhat You'll Need To Succeed\n\n7+ years of industry experience measuring product performance and user behavior\nExperience working with a variety of data management technologies, including RedShift, Kinesis/Kafka, Glue, Spark, Postgres, Airflow, dbt, and others.\nExperience implementing BI reporting tools such as Looker\nExperience interfacing with engineers, product managers and analysts to understand data needs\nKnowledge of a variety of measurement beacons, SDKs, APIs including Google Analytics, Amplitude, Braze, Stripe, email service providers\nA commitment to building secure, resilient, fault-tolerant architectures, with clear documentation and procedures in place for support\nA focus on accuracy and detail as you build, ensuring data pipelines produce clear, predictable results\nUnderstanding of the typical metrics a subscription and advertising-supported business needs to measure success\nExperience with Client techniques as applied to behavioral segmentation or anomaly identification is a definite plus\nFamiliarity using developer tools that increase productivity and facilitate the development of resilient code (eg. Docker, CircleCI, Serverless) is a plus"}, {"title": "ENTRY LEVEL DATA ENGINEER (REMOTE)", "company_name": "SynergisticIT", "location": "Jacksonville, FL (Remote)", "link": "/jobs/view/3767587909/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=om7L3CnowiC4PLIcpZD70A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3767587909", "description": "About the job\nSYNERGISTICIT wants every candidate to know that the Job Market is Challenging and to stand out, you need to have exceptional skills and technologies and that's where we come in to make sure you get the attention which you need\n\nSynergisticIT understands the complex nature of the job market and how difficult it can be to secure a position, especially for fresh graduates. Therefore, we assist and help tech-savvies to convert their passions into professions. We go above and beyond to keep you working in your niche.\n\nAs we focus on long-term success, we provide complete career development solutions. From job search to upskilling portfolio and interview preparation, we can guide you at every step of your career.\n\nSynergisticIT spares no efforts to connect you with a large network of tech giants, including Google, Apple, PayPal, Dell, Cisco, Client, etc. Presently, we are actively looking for ENTRY LEVEL DATA ENGINEER a driven mindset. Get the right opportunity and gain experience in building web-centric solutions on Java.\n\nWho Should Apply : Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or anyone looking to make their career in IT Industry\n\nWe also assist in filing for STEM extension and H1b and Green card filing.\n\nCandidates who are serious about their future in the IT Industry and have set big goals for themselves.\n\nCandidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. We also offer Skill Enhancement Programs if the candidates are missing skills or experience which our clients need with great outcomes\n\nCandidates can benefit from skill enhancement if they fall into the below categories. If they are qualified with enough skills then no need for skill enhancement\n\nCandidates Who Lack Experience\n\nHave had a break in careers\n\nLack Technical Competency\n\ncandidates who want to get employed and make a career in the Tech Industry \n\nPlease Also Check The Below Links\n\nhttps://www.synergisticit.com/candidate-outcomes/\n\nhttps://www.synergisticit.com/java-track/\n\nhttps://www.synergisticit.com/data-science-track/\n\nhttps://www.synergisticit.com/which-is-the-best-option-for-tech-job-seekers-staffing-companies-consulting-companies-bootcamps-or-synergisticit/\n\nhttps://www.synergisticit.com/contact-us/\n\nIf the skills are not a match candidates can opt for Skill enhancement. Or their resume can be sent out to clients to see if responses are achievable\n\nREQUIRED SKILLS For Java/Software Programmers\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Core Java , javascript , C++ or software programming\nSpring boot, Microservices and REST API's experience\nExcellent written and verbal communication skills\n\nFor data Science/Machine learning\n\nRequired Skills\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Statistics, Python, data visualization tools\nExcellent written and verbal communication skills\n\nPreferred skills: NLP, Text mining, Tableau, Time series analysis\n\nTechnical skills are required by clients for selection even if its Junior or entry level position each additional Technical skill helps a candidate's resume to be picked by clients over other job seekers.\n\nClients hire candidates with the right technical skills which they need and reject candidates who lack the required technical skills.\n\nNo third party candidates or c2c candidates\n\nPlease apply to the posting\n\nNo phone calls please. Shortlisted candidates would be reached out"}, {"title": "Data Engineer - Remote | WFH", "company_name": "Get It Recruit - Information Technology", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3782241984/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=BgBf1kSAAX%2BSuw6inJsIVw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3782241984", "description": "About the job\nJoin an innovative team at a leading company in the engineering and manufacturing industry, where we are committed to leveraging data for continuous improvement and operational excellence. This hybrid position allows you to enjoy the flexibility of remote work on Mondays and Fridays.\n\nWhy Join Us\n\nAt our company, we prioritize your well-being and professional growth. We offer a comprehensive benefits package, including excellent health benefits, a 6% 401(k) match, generous paid vacation and sick time, 12 paid holidays, and more.\n\nRole Overview\n\nAs a Data Engineer, you will play a pivotal role in transforming raw data into actionable insights, driving improvements in engineering and manufacturing processes, enhancing product quality, and optimizing operational efficiency. Collaborate with dynamic cross-functional teams to extract, analyze, and visualize data, influencing strategic decisions through your valuable insights.\n\nKey Responsibilities\n\nData Collection and Integration:\n\nGather, clean, and integrate data from diverse sources, including production databases, sensor data, and external datasets.\n\nData Analysis\n\nConduct exploratory data analysis to identify patterns, trends, and anomalies.\n\nPerform statistical analysis and hypothesis testing to derive meaningful insights.\n\nData Visualization\n\nCreate informative and visually appealing dashboards and reports using tools like Tableau, Power BI, or Python libraries.\n\nPredictive Modeling\n\nDevelop predictive models to forecast manufacturing performance, quality, and operational outcomes.\n\nRoot Cause Analysis\n\nInvestigate production issues and quality concerns by analyzing data to identify root causes and recommend corrective actions.\n\nPerformance Monitoring\n\nEstablish and maintain key performance indicators (KPIs) to track manufacturing and engineering metrics.\n\nGenerate regular reports to communicate performance trends to stakeholders.\n\nCross-functional Collaboration\n\nCollaborate with engineering, manufacturing, and quality assurance teams to understand their data needs and provide analytical support.\n\nContinuous Improvement\n\nSuggest process improvements and optimizations based on data analysis.\n\nData Security And Compliance\n\nEnsure data security and compliance with relevant regulations, such as GDPR or industry-specific standards.\n\nInfrastructure And Development\n\nBuild and maintain the infrastructure for collecting, storing, and processing data.\n\nDesign, develop, and manage data pipelines, ETL processes, and data warehouses.\n\nRequired Experience\n\nProven experience as a Sr. Data Analyst and/or Data Engineer in an engineering or manufacturing environment.\n\nProficiency in data analysis tools and programming languages, such as Python, R, SQL, and Excel.\n\nStrong data visualization skills with tools like Tableau or Power BI.\n\nExcellent knowledge of statistical analysis and machine learning techniques.\n\nAbility to communicate complex findings and insights effectively to both technical and non-technical stakeholders.\n\nStrong problem-solving skills and attention to detail.\n\nFamiliarity with manufacturing processes and quality control is a plus.\n\nExperience with enterprise ERPs (SAP S4HANA, Oracle EBS), CRMs (Salesforce, Hubspot), and IoT.\n\nProficient in working with cloud computing platforms including AWS, GCP, and Azure.\n\nExpertise in configuring and optimizing cloud-based data solutions for scalability, reliability, and performance.\n\nBachelor's Degree in Data Science, Statistics, Engineering, or Computer Science Required.\n\nApply Now and be part of a team shaping the future of data-driven innovation in engineering and manufacturing!\n\nEmployment Type: Full-Time"}, {"title": "Entry Level Software Engineer - Data Backend Engineer (Remote - Canada)", "company_name": "Yelp", "location": "United States (Remote)", "link": "/jobs/view/3752881757/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fankeoxifvhpTzOYvowHYQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3752881757", "description": "About the job\nYelp engineering culture is driven by our values: we\u2019re a cooperative team that values individual authenticity and encourages creative solutions to problems. All new engineers deploy working code their first week, and we strive to broaden individual impact with support from managers, mentors, and teams. At the end of the day, we\u2019re all about helping our users, growing as engineers, and having fun in a collaborative environment.\n Interested in exploring data and finding innovative ways to collect it, curate it, and make it easily accessible for others to use? How about doing that at an enormous scale? Yelp\u2019s datasets contain billions of interactions between users and local businesses around the globe. If working with our vast amount of data sounds exciting, come and join us!\n\nThe Marketplace Data Observability team is responsible for managing, governing, and monitoring all of Yelp\u2019s Ads data to protect our revenue stream and also support offline analysis that aids in making long term company decisions. We build tools that help us to better understand our data quality, and we strive to uncover data issues before they impact our consumers.\n\nAs a Data Backend Engineer on this team, you will be responsible for building elegant and scalable data products that serve critical, up-to-date, structured information to support different types of analytics within Yelp. As a core contributor to our growing data modeling and data warehousing engineering efforts, you will help design and own mission-critical data flow pipelines and datastores to enable decisions including effective A/B testing and company investments.\n This opportunity is fully remote and does not require you to be located in any particular area in Canada. We welcome applicants from throughout Canada. We\u2019d love to have you apply, even if you don\u2019t feel you meet every single requirement in this posting. At Yelp, we\u2019re looking for great people, not just those who simply check off all the boxes.\n  Build systems that can effectively store and crunch terabytes of data.\nDesign and develop data models for efficient data storage, retrieval, and reporting.\nCreate and maintain conceptual, logical, and physical data models using industry-standard modeling tools.\nCollaborate with cross-functional teams, including engineers, data analysts, business analysts, and data scientists, to understand data requirements and translate them into effective data models.\nParticipate in data integration efforts, including ETL processes and data migration.\nStay up-to-date with industry best practices, emerging technologies, and trends related to data warehousing.\nSupport on-call rotations as needed to operate the team.\n  Understanding of high performing and scalable data systems.\nExperience in building and orchestrating ETL pipelines.\nExperience with Data Lake or Data Warehouse landscape.\nA hunger for tracking down root causes and fixing them in systematic ways.\nAbility to communicate effectively to technical and non-technical cohorts alike. \nExposure to some of the following technologies: Python, AWS Redshift, AWS Athena / Apache Presto, Big Data technologies (e.g S3, Hadoop, Hive, Spark, Flink, Kafka etc), NoSQL systems like Cassandra, DBT."}, {"title": "Data Engineer (Associate Consultant), June 2024", "company_name": "UDig", "location": "Nashville, TN (Remote)", "link": "/jobs/view/3716039960/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=XUjo40kU55HnDCqJ%2F3FdPg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3716039960", "description": "About the job\nCan UDig It? \n\nUDig designs, builds, and implements technology solutions that deliver on business objectives. People join our team because they are excited to have an impact on the future of our organization, passionate about delivering meaningful results, and energized by solving complex problems. Oh, and they like working with other driven, smart, and authentic people!\n\nUDig is proud to be recognized as a \"Best Places to Work\" in Virginia and Tennessee. We're committed to fostering a transparent environment where leadership is accessible, professional development opportunities are abundant, and you are empowered to achieve your career goals. You'll collaborate, develop deep relationships with teammates, and accelerate your success while having fun!\n\nIf you're graduating in May 2024 and excited by the idea of impacting organizations by building data solutions to achieve business goals, you should consider joining our growing technology team.\n\nHow Things Go Down\n\nAs a new, entry-level teammate, you'll take part in our \"Breaking Ground\" training program designed to help you quickly get up-to-speed on UDig's delivery approach within a project team. The program covers:\n\nUDig delivery process and methodologies such as gathering requirements and delivering projects using agile\nTechnical guidance and instruction on the best practices from Data leaders at UDig\nDelivering executive and technical presentations\nCoaching to prepare you to for success on your first client project at UDig\n\nA typical day might entail:\n\nProject standups and check-ins\nDesign and development of data pipelines, BI reports, and more\nPlanning and participating in project demos\nContributing to internal UDig projects\nCareer and job growth through UDig-provided training\nCreating long-lasting friendships with teammates through fun activities as well as your projects (including social events, winning a prize for guessing the weight of pumpkin, or sharing experience on cool tech side projects)\n\nUDig believes it is important for Associate Consultants going through our Breaking Ground program to reside in one of our major markets (Richmond, VA or Nashville, TN metro area) for at least the first year of employment.\n\nWant to learn more? Hear directly from our teammates by checking out our 'A Day in the Life' blog series: https://www.udig.com/digging-in/breaking-ground-2023/\n\nSound Interesting? Here's the Technical Piece\n\nBelow are a few boxes you should be able to check if\u202fyou're interested: Don't check every box? Go ahead and apply!\n\n1+ year of experience with relational database management systems such as Microsoft SQL Server, MySQL, or Oracle\nUnderstanding of the full lifecycle of design, development, and implementation of data solutions including architecture design, development, testing\nSome experience in ETL development or reporting is a plus\nAbility to present technical ideas and high-level concepts and solutions to internal and external team members with varying degrees of technical knowledge\nBachelor's Degree in in computer science, information technology, or any science or business discipline\nAbility to take your work seriously but also seriously enjoy a good time with your UDig colleagues\n\nWhat's in It For You? \n\nAt UDig, you will build your career alongside talented and experienced developers while gaining experience and having fun! We provide continuous challenges and professional development opportunities. Additionally, we offer: \n\nCompetitive salary, merit reviews, and career advancement paths\nFlexible, hybrid environment\nIndividual $1500 Training Budget\nRegular team building and social activities (virtual and/or in-person)\nTransparent culture with strong communication and access to leadership\nGreat benefits like\nGenerous Paid Time Off, company holidays, and parental leave\nMultiple Single and Family Health Insurance plans to choose from\nDental & Vision coverage\nShort-Term and Long-Term disability\nOptional accident and critical illness coverage\nMatching 401(k)\nand more!"}, {"title": "Expression of Interest: Data Engineer", "company_name": "Fingerprint for Success (F4S)", "location": "New York, NY (Remote)", "link": "/jobs/view/3787769880/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=l%2FEDpQqUNmCttGdIyTk7Cg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787769880", "description": "About the job\nWe are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.\n\nThe F4S Talent Pool is a pilot project designed to:\n\nHelp job seekers get discovered by our partners based on their anticipated hiring needs.\nProvide optional support and resources for job seekers in their career endeavors.\nHelp individuals understand, and bring out the best in themselves and each other.\n\nThe F4S Talent Pool process:\n\nOnce you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.\n\nAbout Fingerprint For Success (F4S)\n\nBacked by 20+ years of research, F4S\u2019s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high performing teams.\n\nKeep in mind, joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your F4S work style assessment to match the hiring needs of our partners.\n\nYour feedback is a gift! Write to us via:\n\nPowered by JazzHR\n\noWuPJ9lwq9"}, {"title": "Data Engineer Seattle, WA (remote)", "company_name": "Conch Technologies, Inc", "location": "United States (Remote)", "link": "/jobs/view/3765052211/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=SBETI78UMutCHuRp5FnP%2FA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3765052211", "description": "About the job\nHi,\n\nGreetings from Conch Technologies Inc\n\nData Engineering Lead \u2013 With Databricks and Unity Catalog\n\nLocation \u2013 Seattle, WA (remote)\n\nDuration \u2013 12 months\n\nNeed: Design and implement data architecture and strategy, leveraging technologies such as Databricks, Unity Catalog, Privacera, and Collibra.\n\nAs a Data Engineering Lead, you will play a pivotal role in defining and executing technical deliverables while assessing complexities and levels of effort. You'll engage with both vendor and internal teams to resolve technical challenges and ensure seamless collaboration. Your leadership will extend to planning and prioritization with the SCRUM master, facilitating effective coordination between onsite and offshore teams, and driving feature prioritization through collaboration with product managers.\n\nKey Responsibilities\n\n Lead the definition and breakdown of technical deliverables, assessing complexities and levels of effort.\n Collaborate with vendor and internal teams to address technical issues.\n Drive planning and prioritization in conjunction with the SCRUM master.\n Coordinate with onsite and offshore teams for timely project delivery.\n Collaborate with product managers to prioritize features, communicate with stakeholders, and clarify requirements.\n Collect and manage user-reported issues, ensuring prompt resolution and communication.\n Define comprehensive test plans, oversee their execution, and document results.\n\n--\n\nWith Regards,\n\nNagesh G\n\nMobile: 408-381-5645\n\nDesk: 901-313-3066\n\nEmail: nagesh@conchtech.com \n\nWeb: www.conchtech.com"}, {"title": "Remote Work - Need Azure Fabric Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3759325599/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=C3iVjmcR9wjUip1nNjUiCg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3759325599", "description": "About the job\nThe client is placing heavy emphasis on Microsoft and Azure Fabric experience\n\nInformation Management Engineer \u2013 Take the identified information management sources and assist migration to the Client Data Lake, Azure Fabric. The engineer will transform information management (de-dup, clean and enrich).\n\nTake the identified data sources and bring them into our Data Lake, Azure Fabric. Once the data is in, the engineer will need to be able to transform data (de-dup, clean and enrich the data). We are focused on using a medallion data model for data architecture. The tools we are working with are: Delta Parquet files, python, spark, notebooks, T-SQL, Fabric lakehouse/warehouse, Dataflows Gen2, Data Pipelines, Direct Lake, shortcuts, ADF."}, {"title": "Data Engineer", "company_name": "Catalist", "location": "Washington, DC (Remote)", "link": "/jobs/view/3744951279/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ui1Fk6ICmnPNqAlZDZAcwA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3744951279", "description": "About the job\nFor over 17 years, Catalist has been a leader in civic data and data science innovation. Our mission is to provide progressive organizations with the data, software, and services needed to better identify, understand, and communicate with the people they need to engage and mobilize. Our clients include the largest, most influential organizations in the U.S. active in civic engagement, advocacy, and political campaigns.\n\nCatalist is home to a dedicated, creative team of technologists, data scientists, and campaign experts committed to using our talents and technology to nurture a vibrant and growing progressive community.\n\nAs a Data Engineer at Catalist, you will work closely with the Analytics and Technology departments to design, build, support, and maintain various data pipelines and processes with an end goal of providing data and intelligence to the progressive community. This role primarily involves translating data architecture designs into functional processes, code, and systems.\n\nThe ideal candidate will be a highly motivated individual with excellent technical skills, a strong desire to learn new skills, and an interest in progressive politics. Catalist values creativity and problem-solving. Our work is on the cutting edge of data-driven politics, and your support will help Democratic candidates and progressive organizations conduct successful advocacy and electoral campaigns.\n\nThis position reports to the Deputy Chief Data Officer. The Data Engineer is a part of a growing Data team that supports all underlying work at Catalist.\n\nThis position is included in our CWA bargaining unit.\n\nPrinciple Duties & Responsibilities\n\nDevelop scalable, production processes, code, and systems for data ingest, transformation, modeling, and reporting across a variety of platforms\nTranslate mock-ups and designs into functional processes, code, and systems\nCreate architecture designs when needed\nProvide quality assurance and testing on processes, code, systems, and products\nBecome an internal subject matter expert on various datasets and support other Catalist departments/teams on usage of those datasets\nExecute ad-hoc data and database maintenance tasks as requiredProject manage cross-departmental efforts, with direct responsibility for stakeholder engagement, management, and execution of technical elements\n\n\nRequirements\n\nBS or BA in a technical field, or relevant experience\n1-2 years of experience working with SQL databases\nExperience with data cleaning or ETL processes\nExperience with distributed computing systems and/or distributed datastores (particularly the Hadoop ecosystem)\nExperience managing projects\nFamiliarity with Catalist data, progressive politics, voter files, and/or commercial data\nBackground check required\n\n\nPreferred Skills & Abilities\n\nWillingness to be a problem solver and produce results in a fast paced environment\nAbility to be creative and personable, and articulate ideas clearly\nExperience working with SQL databases\nProficiency with Python or another object-oriented programming language (R, Java, Scala, etc\u2026)\nExperience working in cloud environments (AWS, GCP, etc.)\nExperience working in command line environments such as Bash\nExperience with a version control tool such as git or github\n\n\nBenefits\n\nMedical, Dental, Vision, Prescription Drug\n\nCatalist offers Medical, Dental, Vision, and Prescription Drug coverage for eligible staff and their eligible dependents. Catalist\u2019s Medical plan is a comprehensive PPO program including Prescription Drug coverage with 85% of the premium paid by Catalist. Dental and Vision coverage is provided at no cost to employees.\n\nGroup Term Life Insurance and Long-Term & Short-Term Disability Coverage\n\nGroup Term Life Insurance and Long-Term and Short-Term Disability coverage is available for eligible staff. These benefits are provided at no cost to Catalist employees.\n\n401(k) Safe Harbor Plan\n\nA 401(k) Safe Harbor Plan is available to eligible staff with a 3% contribution from Catalist from the date of hire. Employees may contribute pre-tax or post-tax from their salary up to the legal limits set forth by the IRS.\n\nMedical and Dependent Care Flexible Spending Accounts (FSAs)\n\nCatalist offers an FSA Program that gives eligible staff the ability to pay out-of-pocket medical/dental/vision/child care expenses from pre-tax earnings.\n\nTransit Benefits\n\nCatalist also makes available a Transit benefit FSA program to eligible employees using pre-tax contributions with a company match.\n\nProfessional Development and Remote Work Expenses\n\nEligible employees may be reimbursed up to $750 each year for professional development / education and remote work expenses.\n\nStudent Loan PayDown or SaveUp\n\nCatalist offers a Student Loan PayDown and College SaveUp benefit for eligible staff.\n\nVacation, Personal Leave, Sick Leave Benefits\n\nCatalist offers generous vacation benefits to all eligible staff. Eligible employees also receive:\n\n 14 Paid Holidays\n Personal Days\n Sick Leave\n Parental Leave\n\n\nHybrid Office/Remote Work\n\nCertain positions at Catalist are eligible for Office/Remote Hybrid or full Remote status."}, {"title": "Remote Entry Level Data Analyst/Engineer", "company_name": "SynergisticIT", "location": "White Plains, NY (Remote)", "link": "/jobs/view/3774171423/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=q54sk1%2FRww0z%2BC%2BE8e0jQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774171423", "description": "About the job\nAt SynergisticIT, we're all about making connections. Whatever IT goals you have, our software programmers can help achieve those. Our Software development teams can take up turnkey projects and execute them in an effective and efficient manner. If you are looking to source talent our recruiters will find the ideal IT talent for your company. What's the secret to our success? Well, it all starts with taking quality time to listen to each client's specific needs. After we have a thorough grasp of your IT goals, we can better customize our Developments as per your specific needs. We can also tailor make recruiting programs to exceed your expectations. Since our founding in 2010, SynergisticIT's strategies have earned the company an enviable position in the software development, IT staffing and IT skill enhancement fields. SynergisticIT continues to work with hundreds of satisfied American clients with our software programmers working on our projects and after gaining hands on experience on cutting edge technologies moving to contribute their skills to great clients like Apple, Google, Client, Ebay, Paypal, Kroger, the Walt Disney Company and hundreds more. If you are tired of working with inefficient programmers who take a lot of time to ramp up we want you to try us. Our software programmers can hit the ground running and get you the maximum return on your investment. You have already tried the rest its time you tried the best. SynergisticIT - Home of the Best Data Scientists and Software Programmers in the Bay Area.\n\n Why Us ? \n\nSynergisticIT has a proven track record of successfully skill enhancement and staffing IT employees for some of the world's most iconic brands. Our team takes the time to fully understand every client's needs so we could best meet your IT staffing requirements. The knowledgeable staff at SynergisticIT is always more than happy to work with clients to ensure they reach their software development goals. Besides staffing, SynergisticIT is also committed to helping young IT professionals advance their career with a robust upskill program . Everyone who goes through SynergisticIT's program learns all the skills necessary to succeed in many IT fields ranging from Java to Machine Learning. Additionally, everyone trained at SynergisticIT has been through extensive mock and technical interview screenings to bolster their career prospects. Last, but certainly not least, SynergisticIT takes great care to respect the privacy considerations for every client. All companies who work with SynergisticIT can rest assured their confidential data is protected using the most up-to-date encryption technologies. SynergisticIT also complies with all the latest NDA agreements.\n\n REQUIRED SKILLS For Java /Software Programmers \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Time series analysis \n\n We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2022 and at Gartner Data Analytics Summit (Florida)-2023 \n\nOracle CloudWorld Event (OCW) Las Vegas 2022 | SynergisticIT - YouTube\n\n https://www.youtube.com/watch?v=OAFOhcGy9Z8 \n\n https://www.youtube.com/watch?v=EmO7NrWHkLM \n\n https://www.youtube.com/watch?v=NVBU9RYZ6UI \n\n https://www.youtube.com/watch?v=Yy74yvjatVg \n\nSynergisticIT at Gartner Data and Analytics Summit 2023 - YouTube\n\nFor preparing for interviews please visit  https://www.synergisticit.com/interview-questions/ \n\n We are looking for the right matching candidates for our clients \n\n Please apply via the job posting \n\n REQUIRED SKILLS For Java /Full Stack/Software Programmer \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning Positions \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Tensorflow \n\n If you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team. \n\n No phone calls please.  Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates"}, {"title": "Data Engineer", "company_name": "Anika Systems", "location": "Leesburg, VA (Remote)", "link": "/jobs/view/3787766893/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=mApL9JnN%2B%2Fr2%2BYUPlxRYOA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787766893", "description": "About the job\nAnika Systems is a fast growing, woman-owned small business that specializes in providing innovative IT solutions for federal government agencies. Our expertise lies in accelerating delivery in Data and Analytics, Intelligent Automation, Application Development, and IT Modernization. We are currently expanding our Federal team and are seeking a passionate and talented Data Engineer. This opportunity is 100% remote.\n\nMust be a U.S. Citizen with the ability to obtain and maintain a government suitability clearance\n\nResponsibilities\n\nDesigns, develops, builds, analyzes, evaluates, and installs database management systems to include database modeling and design, relational database architecture, metadata, and repository creation and configuration management.\nDefines and oversees database organizations, standards, controls, procedures, and documentation. Provides technical consulting in the definition, design, and creation of a database environment.\nAdvises applications development staff and users on data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nDesigns and implements databases with respect to access methods, access time, batch processes, device allocation, validation checks, organization, protection and security, documentation, and statistical methods.\nUses data mapping, data mining, and data transformational analysis tools to design and develop databases.\nDetermines data storage and optimum storage requirements.\nPrepares system requirements, source analysis, and process analyses and designs throughout the database implementation.\n\nRequired Skills And Experience\n\nBA/BS and 1 year of relevant experience\nExperience with DataBricks, SQL, Python\nApplications development experience with data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nExcellent communication skills (both oral and written) with process-oriented organizational skills to ensure project success\n\nDesired Skills And Experience\n\nDemonstrated successful development experience with the full life cycle of government database management systems including database modeling and design, relational database architecture, metadata and repository creation, and configuration management.\n\nPowered by JazzHR\n\nTlKWIorucJ"}, {"title": "Remote Work - Need Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3774712978/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=r3Bp%2F0qcOD2OpC5jpbUOcA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774712978", "description": "About the job\nTitle : Data Engineer\n\nLocation : Remote\n\nDuration : 6+ months to start (long term contract (work order are extended 2x year))\n\nPlease Fill out the skill matrix: Technology Some Awareness (1) Novice (2) Intermediate (3) Advanced (4) Expert (5) SQL Tableau Desktop /Creator Tableau API Integration AWS - S3 Glue AWS - Athena Redshift AWS - Lambda AWS - SNS/SQS msExcel\n\nProfile: Data Analyst will work closely with Architects and leads in the Interflow team, key business stakeholders including program staff to support the organizations effort in the areas of data Analysis and information management. Role should have strong experience in writing SQL queries to extract data for reports. Candidate should have experience working with large set of data. As these reports are very critical for the company and we are looking for an expert who is capable of handling, consuming and extracting large sets of data for analysis and reports.\n\nMust Haves\n\nAWS - Athena\nAWS - S3\nGlue\nhands-on development of reporting applications\nMS Excel\nRedShift\nSQL\nTableau Desktop /Creator\n\nNice\n\nAWS - Lambda\nAWS - SNS/SQS\nTableau API Integration\n\nSkills\n\nBA/BS required (major in an analytical field desired) \nA minimum 10+ years work-related experience. \nExperience in hands-on development of reporting applications for the Web in a professional environment \nThe Ethos of continuous improvement and interest in learning new things.\nStrong analytical thinking and structured problem-solving ability\nAbility to handle multiple projects and assignments simultaneously and effectively in a cross-functional team environment. \nVersed on the agile methodology and best practices. \nExcellent interpersonal and collaboration skill with the ability to work with a diverse set of colleagues, across functions, from different organizations, disciplines, etc. \nSelf-starter, ability to set priorities, work independently and attain goals"}, {"title": "Data Engineer", "company_name": "Team Remotely Inc", "location": "Houston, TX (Remote)", "link": "/jobs/view/3786249829/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=78lY5Jo%2FfyRPC6TQYCdvww%3D%3D&trk=flagship3_search_srp_jobs", "id": "3786249829", "description": "About the job\nThis is a remote position.\n\n Data Engineer (US/Canada Residents Only, 1 year experience, remote)\n\nTeam Remotely Inc. is a staffing and recruitment agency that offers a comprehensive solution for talent acquisition, including sourcing, vetting, pay rolling, and managing talent. Whether you need contract staffing, direct hire, direct sourcing, talent pools, or diversity initiatives, our model can support your hiring strategy.\n\n Hiring Type: Full-Time\n\n Base Salary: $55K-$70K Per Annum.\n\nResponsibilities\n\nDesigning user interface changes for web-based DB applications. \nReviewing application requirements and interface designs. \nDeveloping and implementing highly responsive user interface components using react concepts. \nWriting application interface codes using JavaScript following react.js workflows. \nTroubleshooting interface software and debugging application codes. \nDeveloping and implementing front-end architecture to support user interface concepts. \nMonitoring and improving front-end performance. \n\nQualifications And Experience\n\nBachelor\u2019s degree in computer science, information technology, or a similar field. \nPrevious experience working as a react.js developer. \nIn-depth knowledge of JavaScript, CSS, HTML, jQuery, and front-end languages. \nKnowledge of REACT tools including React.js, Webpack, Enzyme, Redux, and Flux. \nExperience with user interface design. \nKnowledge of performance testing frameworks, including Mocha and Jest. \nExperience with browser-based debugging and performance testing software. \nTroubleshooting skills. \nProject management skills. \nProblem-solving skills. \nVerbal communication skills. \n\n Why work with Team Remotely?\n\nTeam Remotely Inc. is a staffing platform offering a seamless experience for employers and candidates. Employers can post job openings and specify their requirements, while candidates can create profiles and upload resumes.\n\nThe team of Team Remotely continuously learns and adapts based on previous successful placements, constantly improving its matching capabilities. This ensures that the recommendations provided by Team Remotely are tailored and accurate, increasing the likelihood of a successful match between employers and candidates. By providing intelligent and data-driven solutions, they strive to enhance the efficiency and effectiveness of the hiring process, ultimately helping companies find the best talent and individuals find their dream jobs."}, {"title": "Data Engineer", "company_name": "ZAG Technical Services", "location": "United States (Remote)", "link": "/jobs/view/3771403464/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=Hf0VefrKXBFIL5MtHmY0Pw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3771403464", "description": "About the job\nWe are looking for a tech savvy Data Engineer to contribute to the building, operating, and scaling of next generation data platforms and tools that will power data-driven analytics capabilities throughout the ZAG products and services portfolio, spanning areas such as business intelligence, reporting, and data analytics.\n\nThe ideal team member is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. Must be comfortable supporting the data needs of multiple clients and teams, systems, and products within the modernization of ecosystems. The right-fit person will be excited by the prospect of optimizing or even re-designing data processes to support next generation of advanced analytics capabilities, which in-turn translates to implementing strategic enhancements to maintain competitive advantage in the industry.\n\nThe successful team member will leverage their proficiency to\u2026\n\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\nBuild the analytics processes required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Data Management and cloud based \u2018big data\u2019 technologies.\nBuild analytics tools that utilize technologies to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.\nCollaborate with client teams to identify and solution regional and local requirements within the context of the standard global business model for enterprise analytics.\nParticipates in meetings, discussions, strategy sessions where changes, improvements, and enhancements are proposed, evaluated, and approved. \n\n\nThe exceptional team member will have or demonstrate progressive experiences in\u2026 \n\nBachelor\u2019s degree or higher in Computer Science or related field\nExperience working with BI and data warehousing tools\nExperience working with enterprise data, where security is paramount and data governance is critical\nAbility to work independently and in a team environment, and effectively engage all levels of the organization\nUnderstanding of distributed systems driving large-scale data processing and analytics with a successful history of manipulating, processing, and extracting value from large, disconnected datasets.\nAbility to communicate effectively (listening, presenting, and questioning)\nStrong organizational, written, and communication skills\n\n\nAbout ZAG\n\nZAG Technical Services empowers our clients\u2019 success through intelligent business solutions that increase employee productivity, network security and team collaboration. We aim to ensure that our client\u2019s IT investments drive ROI, provide a competitive advantage and help businesses grow. We do this by living our core set of values: Integrity Foremost, Client-Centered, Accountable Always, Teamwork Throughout, and Exceptional is the Goal.\n\nWhat ZAG Has To Offer\n\nCompetitive compensation package \nBenefits including medical, dental, vision, 401K and life insurance \n\n\nZAG Technical Services is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all of our employees.\n\nZAG Technical Services is an E-Verify participant and will provide the Social Security Administration (SSA) and, if necessary, the Department of Homeland Security (DHS), with information from each new employee\u2019s Form I-9 to confirm work authorization."}, {"title": "Junior Data Engineer", "company_name": "HireMeFast LLC - Career Accelerator - Land A Job", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3786249797/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D&trk=flagship3_search_srp_jobs&lipi=urn%3Ali%3Apage%3Ad_flagship3_search_srp_jobs%3BQExogrHBQoSF9zZm%2B9kufw%3D%3D&lici=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D", "id": "3786249797", "description": "About the job\nThis is a remote position.\n\n Job Title: Junior Data Engineer\n\n Employment Type: Full-Time\n\n Salary: $64,000 - $76,000 per annum\n\n Experience Required:  Minimum 1 year of project experience\n\nAbout us: HireMeFast is a leading staffing and recruitment agency specializing in connecting businesses with top-tier talent across various industries. Our mission is to bridge the gap between exceptional candidates and organizations needing their skills, expertise, and unique qualities. Our team of experienced and dedicated recruitment specialists utilizes innovative sourcing strategies, and a vast network to identify and attract top talent. We conduct comprehensive procedures to ensure that only the most qualified candidates are presented to our clients.\n\nPosition Summary\n\nJoin the fast-paced, innovative, and collaborative environment focused on providing an AIOps platform that enhances the intelligence of the CVS Health infrastructure. Work closely with subject matter experts and colleagues to build and scale out machine learning and AI solutions that will detect, predict, and recommend solutions to correct issues before system impact and enhance efficiency, reliability, and performance of CVS Health\u2019s IT operations.\n\nKey Responsibilities include:\n\nData pipeline development: Designed, implemented, and managed data pipelines for extracting, transforming, and loading data from various sources into data lakes for processing, analytics, and correlation. \nData modeling: Create and maintain data models ensuring data quality, scalability, and efficiency \nDevelop and automate processes to clean, transform, and prepare data for analytics, ensuring data accuracy and consistency \nData Integration: Integrate data from disparate sources, both structured and unstructured to provide a unified view of key infrastructure platform and application data \nUtilize big data technologies such as Kafka to process and analyze large volumes of data efficiently \nImplement data security measures to protect sensitive information and ensure compliance with data and privacy regulation \nCreate/maintain documentation for data processes, data flows, and system configurations \nPerformance Optimization- Monitor and optimize data pipelines and systems for performance, scalability and cost-effectiveness \n\nCharacteristics of this role:\n\nTeam Player: Willing to teach, share knowledge, and work with others to make the team successful. \nCommunication: Exceptional verbal, written, organizational, presentation, and communication skills. \nCreativity: Ability to take written and verbal requirements and come up with other innovative ideas. \nAttention to detail: Systematically and accurately research future solutions and current problems. \nStrong work ethic: The innate drive to do work extremely well. \nPassion: A drive to deliver better products and services than expected to customers. \n\nRequired Qualifications\n\n2+ years of programming experience in languages such as Python, Java, SQL \n2+ years of experience with ETL tools and database management (relational, non-relational) \n2+ years of experience in data modeling techniques and tools to design efficient scalable data structures \nSkills in data quality assessment, data cleansing, and data validation \n\nQualifications\n\nKnowledge of big data technologies and cloud platforms \nExperience with technologies like PySpark, Databricks, and Azure Synapse. \n\nEducation\n\nBachelor\u2019s degree in Computer Science, Information Technology, or related field, or equivalent working experience\n\nWhy HireMeFast LLC?\n\nAt HireMeFast, we understand that finding the right individuals to join your team is crucial for success and growth of your organization. We are committed to streamlining the hiring process for our clients, ensuring they have access to a diverse pool of highly qualified candidates who are a perfect fit for their specific needs."}, {"title": "Data Engineer  - Need candidates from Texas ( REMOTE )", "company_name": "Pulivarthi Group (PG)", "location": "United States (Remote)", "link": "/jobs/view/3770685212/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=9gdMwuif%2BpB%2BCQYvQi3imQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3770685212", "description": "About the job\nFollow us on Linkedin: https://www.linkedin.com/company/pulivarthigroup/\n\nPulivarthi Group LLC is a Global Staffing & IT Technology Solutions company, with our prime focus of providing world class solutions to our customers with the right talent. We combine the expertise of our team and the culture of your company to help you with the solution that is affordable and innovative using high quality standards and technologies.\n\nWe\u2019ve served some of the largest healthcare, financial services, and government entities in the U.S.\n\nData Engineer - FT only - Azure SQL with data lakes experience\n\nData Engineer -\n\nthe resource requirement for this Data Engineer role needed more clarification.\n\nOur customer is requesting a resource who knows Azure CI/CD (Continuous Integration / Continuous Deployment) to build and integrate their DevOps pipeline.\n\nOnce the DevOps pipelines are mapped, they need the Data Engineer to connect to all the Data Sources, primarily their Data Lake.\n\nThe Customer (based in Houston) is interested in a resource in a time zone not too far removed from the Central Time Zone to allow real time collaboration."}, {"title": "Data Engineer", "company_name": "Otter Products", "location": "Fort Collins, CO (Remote)", "link": "/jobs/view/3766728302/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=77U3U7CAi1PxgDmpI3%2BiNQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3766728302", "description": "About the job\nOverview \n\nOtter Products is currently recruiting for a Data Engineer to join our Data team! You can be based in our Fort Collins, CO office with flexibility to work a portion of your time remotely, or 100% Remote in the U.S.\n\nThe Data Engineer applies professional experience, concepts, and company objectives toward building and operationalizing the minimally inclusive data necessary for the enterprise data initiatives following industry standard practices and tools. The bulk of the Data Engineer's work would be in designing, managing and optimizing data pipelines, then moving these data pipelines effectively into production for key data and analytics consumers like business/data analysts, data scientists or other stakeholder that needs curated data for data and analytics use cases across the enterprise. The Data Engineer also ensures compliance with data governance and data security requirements while creating, improving and operationalizing these integrated and reusable data pipelines to enable faster data access, integrated data reuse and vastly improved time-to-solution for data and analytics initiatives. Additionally, the Data Engineer will be expected to collaborate with other data team members and data consumers working on the models and algorithms developed by them toward optimization for data quality, security, and governance to put them into production leading to potentially large productivity gains.\n\n About Otter Products \n\nOtter Products, we grow to give. From our founder's garage in 1998 to the global technology leader we are today, Otter Products continues to drive growth through innovation.\n\nThrough our industry-leading brands - OtterBox, Liviri and OtterCares - we provide our partners the number-one selling and most trusted products in our categories. Our philanthropic spirit is the foundation on which we foster our partner relationships, allowing us to grow and to give - together.\n\nBy way of our charitable arm, the OtterCares Foundation, we support our communities and invest in our future through education that inspires kids to change the world.\n\nAnd even as our global community of Otters continues to grow, our founder's core values are still at the heart of everything we do. We measure our success by our ability to give back to our communities and strengthen opportunities for all.\n\nFor more information visit otterproducts.com\n\n Responsibilities \n\nComplete analysis, design and development of BI solutions using Azure SQL Server\nFamiliarity with Data Warehouse concepts\nExperience coding the extraction, transformation, and loading (ETL) processes\nDatabase development primarily in SSIS, Data Factory and SQL\nPartner with the business to determine end user database/reporting needs and requirements\nGenerate ad hoc reports using Power BI or SSRS\nCollaborate with other developers to create and implement best approach solution(s)\nTroubleshoot Azure tools, systems, and software\nReview queries for performance issues, making changes as needed\nCollaborate with team to performance-tune Azure application as necessary\nAssist with the analysis and extraction of relevant information from large amounts of historical business data to feed data science initiatives\nParticipate and support the design and documentation of processes for large scale data analyses, model development, model validation and model implementation\nSupport and maintain a positive safety culture by following all safety policies and procedures and actively contributing to a safe working environment\nOther duties as required\n\n Qualifications \n\nBachelor's degree required. Degree in Computer Science or Mathematics preferred\nMinimum of four years of experience in an IT or analytical role required. Experience in database development, report writing and/or statistics preferred.\n\n EEO \n\nOtter Products, LLC is an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, age, marital status, pregnancy, sex, sexual orientation, gender, gender identity or expression, national origin, disability, veteran status, or any other characteristic or status protected by law.\n\n For US Based Roles Only - Compensation Range Minimum \n\nUSD $90,000.00/Yr.\n\n For US Based Roles Only - Compensation Range Maximum \n\nUSD $120,000.00/Yr.\n\n Additional Total Rewards \n\nProfit Sharing Program Eligible, Benefits Eligible - Full Time- check out otterproducts.com/careers/why for more info"}, {"title": "Software Engineer, Distributed Systems", "company_name": "Hightouch", "location": "United States (Remote)", "link": "/jobs/view/3628572539/?eBP=CwEAAAGMd4QO8APODyE_-mMOPhYyQGg5ZjVbPwq_3V7DK0B2bLAf10pcedM9S2T71X5i4TiJuEwOzWBbSiTKbLuk-VD-_Y-dN5uyf8YpYkHYqEYFxySMS9zr2hRjNrxX9gXbxssh35TFK9YTNV2WeH2Ncb675JgFbJTgNWSZySFC6rrdJGmdSDTr2q5d3VoxORz3GjsDbzjeyZQCvQ07RqYw7oujWSgslycs2LMCY_-N0NEj6e0PvbaYKiZ_5IW34bGVSYLXpHwqa8oOcNttYZ90UA3E_7sDSFzABdIVlfrp5zzERrIYy5sh2MIInxHdDoKgQw8VmOn5uM4S0tXUl-i7LnN0_W0NfPBmLiVDNuveIOm4MfDNPxfe5QKPk6FUAVpvgbcoJn9yRyE&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=rph%2FT6Q6Vkp6YoggJf9cJQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3628572539", "description": "About the job\nAbout Hightouch\n\nHightouch\u2019s mission is to empower everyone to take action on their data. Through our Reverse ETL platform, business and data users can seamlessly sync data from where it resides, such as warehouses and databases, to where it is needed, including operational systems and SaaS tools. Traditionally, acting on data has required engineering time and bandwidth, and left most business users stuck with charts and reports that are unable to take automated action on their data. With Hightouch, every business user, without writing any code, can activate data to streamline critical processes, improve marketing performance, and scale operations.\n\nOur team operates with a focus on making a meaningful impact for our customers. We believe in approaching challenges with a first principles thinking mindset, moving quickly and embracing our value of efficient execution, and treating each other with compassion and kindness. We look for team members that are strong communicators, have a growth mindset, and are motivated and persistent in achieving our goals.\n\nHundreds of companies use Hightouch, including Spotify, Ramp, Retool, NBA, Plaid, and Betterment. We\u2019re based in San Francisco, are remote-friendly, and backed by leading investors such as Amplify Partners, ICONIQ Growth, Bain Capital Ventures, Y-Combinator, and Afore Capital.\n\nAbout The Role\n\nWe are looking for a distributed systems engineer to work on the systems that power the syncing engine our customers and other engineering teams rely on. This presents an exciting challenge where you can apply your expertise in distributed systems, performance optimization, and troubleshooting to push the boundaries of what is possible and meaningful improve our ability to move massive amounts of customer data. This role also provides a unique opportunity to work on a multi-cloud and multi-region infrastructure that supports a global customer base.\n\nWe believe in enabling our engineers to do their best work for our customers by giving them extremely high levels of ownership and autonomy. This comes in different forms: you will own and deliver projects from start to finish, you will work directly with customers to solve their hardest scaling problems, and you will have a lot of influence over what we work on as a team and company.\n\nSome of the problems we\u2019ll be working on include:\n\nSync Speed: Customers want to sync a lot of data to important destinations like Facebook and Snapchat, which requires us to analyze every part of our syncing process and find where we can optimize to sync data more quickly\nStreaming Syncing: We currently sync data from sources in batch only, but envision a world where we also do syncs in real-time and support streaming sources like webhooks and queues\nScalability and Reliability: As part of our rapid growth, we\u2019re always evaluating current future bottlenecks in scaling and reliability, and architecting for the next order of magnitude of growth\nPersonalization API: Our Personalization API product provides a low latency caching layer on top of data warehouses that enable customers to use their data warehouse for real-time personalization use cases with \nMulti-Region and Multi-Cloud: Supporting our multi-region and multi-cloud backend, including extending it to launch Hightouch on in new regions to support data residency requirements of our global customer base\n\nWe are looking for talented, intellectually curious, and motivated individuals who are interested in tackling the problems above. This is a senior role, but we focus on impact and potential for growth more than years of experience. The salary range for this position is $170,000 - $240,000 USD per year, which is location independent in accordance with our remote-first policy.\n\nAbout You\n\nYou are an engineer with a passion for solving hard technical problems that generate real value for customers. You\u2019re motivated by high ownership and are comfortable in a fast-paced, startup environment.\n\nYou have experience and comfort with distributed systems and high-scale systems or the ability and desire to build this experience quickly. This experience can take different forms:\n\nYou\u2019ve built syncing engines that process and move terabytes of data per day in a scalable way\nYou\u2019ve created event collection or data streaming services that power company critical systems\nYou\u2019ve owned systems that have undergone orders of magnitude of growth and helped them scale accordingly\nYou have relevant skills and are a fast learner who is excited to take on the challenges we face\n\nInterview Process\n\nOur goal with the interview process is to balance speed with giving both parties opportunities to assess whether there is a strong mutual fit. We will ask you questions, but we want you to ask us questions! Our technical interviews focus on how you design systems because we believe this is the best way for us to see how you work and for you to see how we collaborate. We don\u2019t ask you to write code to solve technical brainteasers that don\u2019t appear in your day to day job.\n\nRecruiter screen [30m]: Introductory call with our recruiting team to get to know each other and see if the role could be a good mutual fit.\nSystems design screen [45m]: Designing a data processing feature end-to-end.\nHiring manager interview [30m]: Chat with hiring manager about past experiences and future operating preferences to assess fit on company values and operating principles.\nSystems design interview [90m]: Work with the interviewer to architect a system at a conceptual level. The problem will be at a pretty high level - and have both product and customer requirements as well as technical."}, {"title": "Entry-Level Data Scientist Engineer", "company_name": "Phoenix Recruitment LLC", "location": "Texas City, TX (Remote)", "link": "/jobs/view/3788727625/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=8iwtytHsCu4wMwJ4fj3pxw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3788727625", "description": "About the job\nThis is a remote position.\n\nTitle - Entry-Level Data Scientist Engineer, 1 year of project experience\n\nEmployment Type: Full-time\n\nBase Salary: $60K-$70K\n\nPhoenix Recruitment offers a variety of recruiting services to assist both employers and employees. They specialize in marketing open positions, recruiting, and helping employers to find qualified candidates across various industries. Phoenix Recruitment has expertise in streamlining the hiring process. They can help ensure that the process is efficient, well organized, and compliant with relevant regulations.\n\nSkills and Abilities:\n\nStrong knowledge of R or Python for data analysis and modeling. \nProficiency in statistical programs such as R, SAS, MATLAB, or Python. \nFamiliarity with spreadsheets (VBA) and database applications (Access, Oracle, SQL, or equivalent technology). \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \nAbility to learn new methods quickly and work under deadlines. \nExcellent teamwork and communication skills. \nStrong analytical and problem-solving abilities. \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \n\nPreferred:\n\nKnowledge of actuarial concepts and life, health, and/or annuity products. \nExperience with statistical modeling techniques such as GLM, Decision Trees, Time Series, Regression, etc. \nFamiliarity with Microsoft DeployR. \nExposure to insurance risk analysis. \nBasic experience in computational finance, econometrics, statistics, and math. \nKnowledge of SQL and VBA. \nFamiliarity with R or Python for predictive modeling \n\nWhy Phoenix Recruitment LLC?\n\nPhoenix Recruitment often has an extensive network of employers and candidates. This network allows them to tap into a pool of qualified candidates and connect them with suitable job opportunities. They can also leverage their connections to help employers find the right talent efficiently. Outsourcing the recruitment process to a specialized agency can save your time and resources, avoid delays, reduce administrative burdens, and increase the chances of finding the right fit for your organization."}, {"title": "Data Engineer - Remote Work", "company_name": "BairesDev", "location": "United States (Remote)", "link": "/jobs/view/3783162297/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=OtoCrAAHZFomVup04fy35g%3D%3D&trk=flagship3_search_srp_jobs", "id": "3783162297", "description": "About the job\nWho We are\n\nBairesDev is proud to be the fastest-growing company in America. With people in five continents and world-class clients, we are only as strong as the multicultural teams at the heart of our business. To consistently deliver the highest quality solutions to our clients, we only hire the Top 1% of the best talents and nurture their professional growth on exciting projects.\n\nWe are looking for a Data Engineer to join our Development team and participate in different projects made up of multicultural teams distributed throughout the world.\n\nWhat You Will Do\n\n Interview other Data Scientists and Data engineers to vet their technical abilities.\n Input data from those interviews into the product and help develop a predictive, fair, and enjoyable solution.\n Work together with developers, tech leads, and solution architects to build applications.\n Improve existing structures by adding new functionalities or proposing technological updates so that as a result, the impact of your contribution is significant in the core of each business.\n Improve the user experience on scalable and high availability platforms, contributing to the key differential of each business.\n\nHere\u2019s What We Are Looking For\n\n 5+ years of experience working as a developer.\n 3+ years of experience working as a Data Scientist / Data Engineer.\n Proficient with analysis, troubleshooting, and problem-solving.\n Hands-on experience with managing data loads and data quality.\n Advanced English level.\n\nHow we do make your work (and your life) easier:\n\n 100% remote work.\n Hardware setup for you to work from home.\n Flexible hours - make your schedule.\n Paid parental leave, vacation & holidays.\n Diverse and multicultural work environment.\n An innovative environment with the structure and resources of a leading multinational.\n Excellent compensation \u2014 well above the market average.\n Here you can grow at the speed of your learning curve.\n\nOur people work remotely but with a consistent and robust culture that promotes diversity and teamwork. To continue being the leading software development company in Latin America, we want to ensure that every BairesDev member gets the best growth and professional development opportunities in a diverse, welcoming, and innovative environment.\n\nEvery BairesDev team member brings something unique to our company.\n\nWe want to hear your story. Apply now!"}, {"title": "Data Engineer (Remote)", "company_name": "Cascade Debt", "location": "United States (Remote)", "link": "/jobs/view/3755336571/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=pmUtn1j2zyPr25CNPh2RNw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3755336571", "description": "About the job\nDo you want to work in a small team where you can make a real difference in a company? Would you embrace the challenge of building a high impact platform that is used globally? Then we want to speak with you!\n\nAbout Cascade\n\nCascade is a fintech startup backed by Canadian and US investors that empowers companies to grow by democratizing access to institutional debt. We are building tools that modernize how companies raise and manage debt, lowering the barriers to entry to the $7 trillion specialty finance and alternative credit market for companies around the world. Founded just last year, we are gearing up for our next phase of technical development and are seeking a talented Data Engineer to lead the way.\n\nAbout The Role\n\nAs a Data Engineer you\u2019ll be an early member of a growing team building a pioneering platform in the debt infrastructure space. You will design, build, and launch efficient, scalable, and reliable data pipelines to move and transform data .\n\nWhat You\u2019ll Do\n\n Building databases, data lakes and data ingestion pipelines to integrate customer databases and datasets to our systems \n Ingesting financial datasets from external customers, then u pdating and maintaining accurate and complete data mappings to ensure that our product s are displaying high quality data \n M onitoring and alerting across our data pipelines in order to make sure that our data ingests are reliable and correct \n Translat ing business goals and stakeholder requirements into new data flows and deliver analytical insights ensuring that the data flows create real business value continuously \n\nSkills And Experience\n\n 2 + years experience in a data engineering or data analyst role \n E xperience with relational SQL and NoSQL databases, including PostgreSQL , MySQL, MariaDB, MongoDB, Bigtable, etc. \n E xperience working with ETL technologies , such as Databricks, Fivetran , or dbt \n Proficiency in writing SQL queries and knowledge of analytical data warehouses such as RedShift, BigQuery , and Snowflake \n Some experience with CI/CD automation such as GitHub Actions \n Developing APIs and integrating with 3rd party APIs \n Bonus: experience in fintech / SaaS / credit analysis \n\nPeople Who Thrive At Cascade Are\n\n Self-starters, who take the initiative to tackle challenges in a remote work environment \n Good communicators, who can operate without ego to discuss, learn, grow, and help others do the same \n Passionate about creating great digital experiences for users \n Problem solvers who are not satisfied with the status quo \n\nBenefits\n\nRemote: we are a remote-first company and are very flexible on hours as long as things get done \nHome-office: there\u2019s a $1000 USD home office allowance to set yourself up \nEquity: we expect you to have an owner-mentality, and have the equity plan to match \nBenefits: health, dental, vision, and more \nPerks: we offer free lunches weekly and off-site trips \nJob satisfaction: we offer autonomy, ample opportunities for mastery, and an opportunity to make a difference for companies around the world"}, {"title": "Data Engineer", "company_name": "ISC (Integrated Specialty Coverages, LLC)", "location": "Carlsbad, CA (Remote)", "link": "/jobs/view/3778293528/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ew7%2FfEuXSujs5IGabdqUyw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3778293528", "description": "About the job\nAbout Integrated Specialty Coverages\n\nIntegrated Specialty Coverages LLC (ISC) is a growth stage technology and data-driven insurance company leading innovation in the market.\n\nBacked by one of the leading private equity firms, KKR, and led by a forward-thinking management team, ISC is combining the worlds of insurance and technology to create an Insurtech powerhouse. As a leading online distributor of insurance products for a range of industries and \"Main Street USA\", we are looking for the right people to help us in our mission of achieving exponential growth. We strive to be the number one place to go for brokers and agents to source insurance. To accomplish this, we're building a digitally focused team that deeply understands the intersection between user experience, data, and AI/ML to optimize the way we engage with our customers and partners.\n\nJob Summary\n\nThe Data Engineer role will be the technical liaison between multiple groups including a data science team, the engineering team, product management, and business stakeholders. You do not need any insurance knowledge prior, however, you must quickly dive deep into the insurance world and ask questions to become a subject matter expert. You will be responsible for building a data platform to facilitate the data science team. You must be a self-starter that can build out features such as a data pipeline from scratch. There will be support from both engineering and data science for any buildout.\n\nJob Responsibilities\n\nCreate and maintain optimal data pipeline architecture. \nAssemble large, complex data sets that meet functional / non-functional business requirements. \nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. \nBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using AWS technologies, SQL, and Python. \nWork with stakeholders including the Executive, Product, Data Science, and Engineering teams to assist with data-related technical issues and support their data infrastructure needs. \nWork with data science and analytics teams to strive for greater functionality in our data systems. \n\nMinimum Qualifications:\n\nAdvanced working SQL knowledge and experience working with relational databases, strong query authoring (SQL) as well as working familiarity with a variety of databases (Snowflake, Redshift, MySQL, MSSQL, etc.)\nExperience building and optimizing data pipelines, architecture and data sets. \nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. \nBuild processes supporting data transformation, data structures, metadata, dependency, and workload management. \nA successful history of transforming, processing and extracting value from large disconnected, datasets from a variety of data sources (Flat files, Excel, databases, etc.)\nStrong analytic skills related to working with unstructured datasets. \nStrong project management and organizational skills. \nExperience supporting and working with cross-functional teams in a dynamic environment. \nAbility to mentor/guide/collaborate with other team members. \n\nWe are looking for a candidate with 3-5 years of experience in a Data Engineer role. They should also have experience using the following software/tools:\n\nExperience with relational and MPP databases, including Snowflake and MySQL. \nExperience developing software in an agile environment from the requirements stage to production. \nExperience with version control: git\nExperience with container technologies: Docker\nExperience with data pipeline and workflow management tools: Airflow, Jenkins, AWS Glue, Azkaban, Luigi, etc. \nExperience with AWS cloud services: EC2, ECS, Batch, S3, EMR, RDS, Redshift\nExperience with other cloud services: Snowflake, Airflow\nExperience with object-oriented/object function scripting languages: Python, Java, C++, etc. \nExperience with data modeling and data warehouse design\nExperience with data visualization tools (PowerBI, QuickSight)\n\nThe starting annual pay scale for this position is listed below. Actual starting pay will be based on factors such as skills, qualifications, training, and experience. In addition, the company offers comprehensive benefits including medical, dental and vision insurance, 401(k) plan with match, paid time off, and other benefits.\n\nISC's salary ranges are determined by role and level. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations and could be higher or lower based on a multitude of factors, including job-related skills, experience, and relevant education or training.\n\nNational Pay Range\n\n$100,000\u2014$110,000 USD\n\nBenefits of Working at ISC\n\nCompetitive vacation and flexible working arrangements\nComprehensive and inclusive health benefits\nA variety of professional development and mentorship opportunities\nChoice of technology whether at home or in the office\n\nISC's Ownership Behaviors\n\nDo the Right Thing *Be Relentless. Pursue Excellence. *Personal Responsibility & Own Outcomes\nTry Fast. Learn Fast. Fail Fast. Think Big. *Build Extraordinary Owners & Join Forces *Grit & Determination\n\nApplicants with disabilities may contact ISC HR department via e-mail, and other means to request and arrange accommodation. If you need assistance to accommodate a disability, you may request accommodation at any time. Please contact ISC at HR@ISCMGA.com.\n\nISC believes in creating long-term relationships by being responsive and relevant and by consistently delivering value to our community of customers. Specifically, with our employees, we focus on attracting, developing, and retaining the best talent for our business, challenging our people, demonstrating a \"can-do\" attitude, and fostering a collaborative and mutually supportive environment. ISC is an equal opportunity employer and a participant in the US federal E-Verify program (US).\n\nDiversity creates a healthier atmosphere: All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, gender, gender identity, sexual orientation, marital status, medical condition, genetic information, mental or physical disability, military or veteran status, or any other characteristic protected by local, state, or Federal law."}, {"title": "Data Engineer -Remote", "company_name": "RIT Solutions, Inc.", "location": "Denver, CO (Remote)", "link": "/jobs/view/3768007496/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=1hQH8EDaII1eJ7nQ9BACQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3768007496", "description": "About the job\nDuration: 6+ months CTH\n\nMust have a valid LinkedIn profile\n\nRequired Skills\n\n 3-6 years of experience (no more than 6 years)\n Start up environment experience\n Python, SQL, and DBT"}, {"title": "Big Data Engineer - W2", "company_name": "Megan Soft Inc", "location": "United States (Remote)", "link": "/jobs/view/3674845652/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fCAxh4N%2F4YZJ5aEIASaeeA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3674845652", "description": "About the job\nTitle: Big Data Engineer - W2\n\nLocation: Dearborn, MI (Remote)\n\nDuration: 12+ Months\n\nDirect Client: Ford Motors\n\nMust have skills: Should be able to handle GCP requirements\n\n\"Translates business requirements to build repeatable, sustainable, efficient, coded processes that can be productionized by Software Engineers and readily modified by other Data Engineer Creates POC processes in Dev/QA and works with Software Engineers to productionize and align best coding and process practices that maximize efficiency, speed, stability, system resources and capabilities Awareness of and compliance with data privacy, security, legal and contractual guidelines Incorporates data quality and privacy checks/alerts to minimize bad data being consumed by end users, models and dashboards, and to protect customer data; revises checks for new data issues Validates data products and pipelines are functioning as expected following system or application upgrades, source changes, etc. Works closely with Architects if required to align systems, tools and applications being utilized with business use case and performance requirements Provides data product support and maintenance Establishes, tracks and monitors KPIs related to specific data products and deliverable Participate in iteration cadence activities (Iteration Planning, task estimation, daily scrum, demos, etc.). Will be a part of on-call team to resolve critical production support tickets during business hours and off hours.\n\nSkills Required\n\nExperience with Big Data Technologies (Hadoop , ETL Tool, Spark. Hive etc..) Someone who understands Cloud and has experience working with Cloud Platforms (preferably GCP) Should be able to handle GCP requirements - ingesting into GCS/ Big Query, working with GCP components Hands-on in Data Flow, Cloud Storage (GCS), Big Query and GitHub. Proficient in Python and SQL.\n\nSkills Preferred\n\nManages ETL: uses programming and tools for data ingestion, configures pipelines, applies transformations and decoding, integrates and fuses data, moves and securely deliver Experience NoSQL databases\n\nAdditional Information\n\nIf the candidate is remote only, please indicate \"\"100% Remote\"\" under candidate's name on resume. Other similar schedule notations can be \"\"Local Candidate\"\" or \"\"Hybrid Candidate.\"\" ***HRA Test Required***\""}, {"title": "Remote Opportunity: Quantexa Data Engineer", "company_name": "SPAR Information Systems LLC", "location": "United States (Remote)", "link": "/jobs/view/3680073191/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ioykVCZSOQ7BK5%2FwRQHOWg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3680073191", "description": "About the job\nHello All, Hope you are doing great!! Please go through the job descriptions and let me know your interest. Role: Quantexa Data engineer  Location: Remote Duration: Long Term Contract Job Description: 1. End to End usage of Quantexa including guiding our client team on how to use from both front end and data ingestion Reason they are asking for Certification 2. Resolve Entity issues with data 3. Knowledge of Scala and Hive which requires coding experience using parquet 4. Advanced SQL knowledge requiring writing SQL queries 5. Experience having worked with GCP data lake 6. Quantexa certification required\n\nThanks & Regards,\n\nSatnam Singh\n\nDirect: 201 623 3660\n\nEmail : Satnam.singh@sparinfosys.com"}, {"title": "Remote Work - Need Staff Data Engineer - USC only", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3746283980/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=CvIuLBNO%2BPMGi4IXVRkP0A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3746283980", "description": "About the job\n100% Remote, client is in NY\n\nLook for strong job history, Python, Machine Learning, SQL, 10/10 comms, etc. \n\nMUST be an Engineer \n\nSummary\n\nWe're looking for an experienced Data Engineer to join our client's growing team of top notched IT professionals. You will work closely across multiple groups including Editorial and Audio, Marketing, Advertising, etc. to achieve their goals and objectives. The Data Engineering team builds tools throughout the stack while sharing knowledge across these departments.\n\nAs the company grows, they're looking for Data Engineer who will help solidify and expand our pipelines and maintain their data warehouse. Their business model is based on performing complex and very detailed analyses of how our products perform with their subscribers. You'll be responsible for working alongside analysts, product, marketing, and other teams to define new data collection components and measurement schemes that support each new product and feature.\n\nHow Can Add Value To Their Mission\n\nCreate and maintain data pipelines to provide insights and drive business decisions\nEstablish data warehousing strategy (ex. Kimball, Data Vault, etc.)\nMaintain data infrastructure on our AWS accounts\nCollaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.\nWrite unit/integration tests, contributes to engineering wiki, and documents work\nImplement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it\n\nWhat You'll Need To Succeed\n\n7+ years of industry experience measuring product performance and user behavior\nExperience working with a variety of data management technologies, including RedShift, Kinesis/Kafka, Glue, Spark, Postgres, Airflow, dbt, and others.\nExperience implementing BI reporting tools such as Looker\nExperience interfacing with engineers, product managers and analysts to understand data needs\nKnowledge of a variety of measurement beacons, SDKs, APIs including Google Analytics, Amplitude, Braze, Stripe, email service providers\nA commitment to building secure, resilient, fault-tolerant architectures, with clear documentation and procedures in place for support\nA focus on accuracy and detail as you build, ensuring data pipelines produce clear, predictable results\nUnderstanding of the typical metrics a subscription and advertising-supported business needs to measure success\nExperience with Client techniques as applied to behavioral segmentation or anomaly identification is a definite plus\nFamiliarity using developer tools that increase productivity and facilitate the development of resilient code (eg. Docker, CircleCI, Serverless) is a plus"}, {"title": "ENTRY LEVEL DATA ENGINEER (REMOTE)", "company_name": "SynergisticIT", "location": "Jacksonville, FL (Remote)", "link": "/jobs/view/3767587909/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=om7L3CnowiC4PLIcpZD70A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3767587909", "description": "About the job\nSYNERGISTICIT wants every candidate to know that the Job Market is Challenging and to stand out, you need to have exceptional skills and technologies and that's where we come in to make sure you get the attention which you need\n\nSynergisticIT understands the complex nature of the job market and how difficult it can be to secure a position, especially for fresh graduates. Therefore, we assist and help tech-savvies to convert their passions into professions. We go above and beyond to keep you working in your niche.\n\nAs we focus on long-term success, we provide complete career development solutions. From job search to upskilling portfolio and interview preparation, we can guide you at every step of your career.\n\nSynergisticIT spares no efforts to connect you with a large network of tech giants, including Google, Apple, PayPal, Dell, Cisco, Client, etc. Presently, we are actively looking for ENTRY LEVEL DATA ENGINEER a driven mindset. Get the right opportunity and gain experience in building web-centric solutions on Java.\n\nWho Should Apply : Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or anyone looking to make their career in IT Industry\n\nWe also assist in filing for STEM extension and H1b and Green card filing.\n\nCandidates who are serious about their future in the IT Industry and have set big goals for themselves.\n\nCandidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. We also offer Skill Enhancement Programs if the candidates are missing skills or experience which our clients need with great outcomes\n\nCandidates can benefit from skill enhancement if they fall into the below categories. If they are qualified with enough skills then no need for skill enhancement\n\nCandidates Who Lack Experience\n\nHave had a break in careers\n\nLack Technical Competency\n\ncandidates who want to get employed and make a career in the Tech Industry \n\nPlease Also Check The Below Links\n\nhttps://www.synergisticit.com/candidate-outcomes/\n\nhttps://www.synergisticit.com/java-track/\n\nhttps://www.synergisticit.com/data-science-track/\n\nhttps://www.synergisticit.com/which-is-the-best-option-for-tech-job-seekers-staffing-companies-consulting-companies-bootcamps-or-synergisticit/\n\nhttps://www.synergisticit.com/contact-us/\n\nIf the skills are not a match candidates can opt for Skill enhancement. Or their resume can be sent out to clients to see if responses are achievable\n\nREQUIRED SKILLS For Java/Software Programmers\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Core Java , javascript , C++ or software programming\nSpring boot, Microservices and REST API's experience\nExcellent written and verbal communication skills\n\nFor data Science/Machine learning\n\nRequired Skills\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Statistics, Python, data visualization tools\nExcellent written and verbal communication skills\n\nPreferred skills: NLP, Text mining, Tableau, Time series analysis\n\nTechnical skills are required by clients for selection even if its Junior or entry level position each additional Technical skill helps a candidate's resume to be picked by clients over other job seekers.\n\nClients hire candidates with the right technical skills which they need and reject candidates who lack the required technical skills.\n\nNo third party candidates or c2c candidates\n\nPlease apply to the posting\n\nNo phone calls please. Shortlisted candidates would be reached out"}, {"title": "Data Engineer - Remote | WFH", "company_name": "Get It Recruit - Information Technology", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3782241984/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=BgBf1kSAAX%2BSuw6inJsIVw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3782241984", "description": "About the job\nJoin an innovative team at a leading company in the engineering and manufacturing industry, where we are committed to leveraging data for continuous improvement and operational excellence. This hybrid position allows you to enjoy the flexibility of remote work on Mondays and Fridays.\n\nWhy Join Us\n\nAt our company, we prioritize your well-being and professional growth. We offer a comprehensive benefits package, including excellent health benefits, a 6% 401(k) match, generous paid vacation and sick time, 12 paid holidays, and more.\n\nRole Overview\n\nAs a Data Engineer, you will play a pivotal role in transforming raw data into actionable insights, driving improvements in engineering and manufacturing processes, enhancing product quality, and optimizing operational efficiency. Collaborate with dynamic cross-functional teams to extract, analyze, and visualize data, influencing strategic decisions through your valuable insights.\n\nKey Responsibilities\n\nData Collection and Integration:\n\nGather, clean, and integrate data from diverse sources, including production databases, sensor data, and external datasets.\n\nData Analysis\n\nConduct exploratory data analysis to identify patterns, trends, and anomalies.\n\nPerform statistical analysis and hypothesis testing to derive meaningful insights.\n\nData Visualization\n\nCreate informative and visually appealing dashboards and reports using tools like Tableau, Power BI, or Python libraries.\n\nPredictive Modeling\n\nDevelop predictive models to forecast manufacturing performance, quality, and operational outcomes.\n\nRoot Cause Analysis\n\nInvestigate production issues and quality concerns by analyzing data to identify root causes and recommend corrective actions.\n\nPerformance Monitoring\n\nEstablish and maintain key performance indicators (KPIs) to track manufacturing and engineering metrics.\n\nGenerate regular reports to communicate performance trends to stakeholders.\n\nCross-functional Collaboration\n\nCollaborate with engineering, manufacturing, and quality assurance teams to understand their data needs and provide analytical support.\n\nContinuous Improvement\n\nSuggest process improvements and optimizations based on data analysis.\n\nData Security And Compliance\n\nEnsure data security and compliance with relevant regulations, such as GDPR or industry-specific standards.\n\nInfrastructure And Development\n\nBuild and maintain the infrastructure for collecting, storing, and processing data.\n\nDesign, develop, and manage data pipelines, ETL processes, and data warehouses.\n\nRequired Experience\n\nProven experience as a Sr. Data Analyst and/or Data Engineer in an engineering or manufacturing environment.\n\nProficiency in data analysis tools and programming languages, such as Python, R, SQL, and Excel.\n\nStrong data visualization skills with tools like Tableau or Power BI.\n\nExcellent knowledge of statistical analysis and machine learning techniques.\n\nAbility to communicate complex findings and insights effectively to both technical and non-technical stakeholders.\n\nStrong problem-solving skills and attention to detail.\n\nFamiliarity with manufacturing processes and quality control is a plus.\n\nExperience with enterprise ERPs (SAP S4HANA, Oracle EBS), CRMs (Salesforce, Hubspot), and IoT.\n\nProficient in working with cloud computing platforms including AWS, GCP, and Azure.\n\nExpertise in configuring and optimizing cloud-based data solutions for scalability, reliability, and performance.\n\nBachelor's Degree in Data Science, Statistics, Engineering, or Computer Science Required.\n\nApply Now and be part of a team shaping the future of data-driven innovation in engineering and manufacturing!\n\nEmployment Type: Full-Time"}, {"title": "Entry Level Software Engineer - Data Backend Engineer (Remote - Canada)", "company_name": "Yelp", "location": "United States (Remote)", "link": "/jobs/view/3752881757/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fankeoxifvhpTzOYvowHYQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3752881757", "description": "About the job\nYelp engineering culture is driven by our values: we\u2019re a cooperative team that values individual authenticity and encourages creative solutions to problems. All new engineers deploy working code their first week, and we strive to broaden individual impact with support from managers, mentors, and teams. At the end of the day, we\u2019re all about helping our users, growing as engineers, and having fun in a collaborative environment.\n Interested in exploring data and finding innovative ways to collect it, curate it, and make it easily accessible for others to use? How about doing that at an enormous scale? Yelp\u2019s datasets contain billions of interactions between users and local businesses around the globe. If working with our vast amount of data sounds exciting, come and join us!\n\nThe Marketplace Data Observability team is responsible for managing, governing, and monitoring all of Yelp\u2019s Ads data to protect our revenue stream and also support offline analysis that aids in making long term company decisions. We build tools that help us to better understand our data quality, and we strive to uncover data issues before they impact our consumers.\n\nAs a Data Backend Engineer on this team, you will be responsible for building elegant and scalable data products that serve critical, up-to-date, structured information to support different types of analytics within Yelp. As a core contributor to our growing data modeling and data warehousing engineering efforts, you will help design and own mission-critical data flow pipelines and datastores to enable decisions including effective A/B testing and company investments.\n This opportunity is fully remote and does not require you to be located in any particular area in Canada. We welcome applicants from throughout Canada. We\u2019d love to have you apply, even if you don\u2019t feel you meet every single requirement in this posting. At Yelp, we\u2019re looking for great people, not just those who simply check off all the boxes.\n  Build systems that can effectively store and crunch terabytes of data.\nDesign and develop data models for efficient data storage, retrieval, and reporting.\nCreate and maintain conceptual, logical, and physical data models using industry-standard modeling tools.\nCollaborate with cross-functional teams, including engineers, data analysts, business analysts, and data scientists, to understand data requirements and translate them into effective data models.\nParticipate in data integration efforts, including ETL processes and data migration.\nStay up-to-date with industry best practices, emerging technologies, and trends related to data warehousing.\nSupport on-call rotations as needed to operate the team.\n  Understanding of high performing and scalable data systems.\nExperience in building and orchestrating ETL pipelines.\nExperience with Data Lake or Data Warehouse landscape.\nA hunger for tracking down root causes and fixing them in systematic ways.\nAbility to communicate effectively to technical and non-technical cohorts alike. \nExposure to some of the following technologies: Python, AWS Redshift, AWS Athena / Apache Presto, Big Data technologies (e.g S3, Hadoop, Hive, Spark, Flink, Kafka etc), NoSQL systems like Cassandra, DBT."}, {"title": "Data Engineer (Associate Consultant), June 2024", "company_name": "UDig", "location": "Nashville, TN (Remote)", "link": "/jobs/view/3716039960/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=XUjo40kU55HnDCqJ%2F3FdPg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3716039960", "description": "About the job\nCan UDig It? \n\nUDig designs, builds, and implements technology solutions that deliver on business objectives. People join our team because they are excited to have an impact on the future of our organization, passionate about delivering meaningful results, and energized by solving complex problems. Oh, and they like working with other driven, smart, and authentic people!\n\nUDig is proud to be recognized as a \"Best Places to Work\" in Virginia and Tennessee. We're committed to fostering a transparent environment where leadership is accessible, professional development opportunities are abundant, and you are empowered to achieve your career goals. You'll collaborate, develop deep relationships with teammates, and accelerate your success while having fun!\n\nIf you're graduating in May 2024 and excited by the idea of impacting organizations by building data solutions to achieve business goals, you should consider joining our growing technology team.\n\nHow Things Go Down\n\nAs a new, entry-level teammate, you'll take part in our \"Breaking Ground\" training program designed to help you quickly get up-to-speed on UDig's delivery approach within a project team. The program covers:\n\nUDig delivery process and methodologies such as gathering requirements and delivering projects using agile\nTechnical guidance and instruction on the best practices from Data leaders at UDig\nDelivering executive and technical presentations\nCoaching to prepare you to for success on your first client project at UDig\n\nA typical day might entail:\n\nProject standups and check-ins\nDesign and development of data pipelines, BI reports, and more\nPlanning and participating in project demos\nContributing to internal UDig projects\nCareer and job growth through UDig-provided training\nCreating long-lasting friendships with teammates through fun activities as well as your projects (including social events, winning a prize for guessing the weight of pumpkin, or sharing experience on cool tech side projects)\n\nUDig believes it is important for Associate Consultants going through our Breaking Ground program to reside in one of our major markets (Richmond, VA or Nashville, TN metro area) for at least the first year of employment.\n\nWant to learn more? Hear directly from our teammates by checking out our 'A Day in the Life' blog series: https://www.udig.com/digging-in/breaking-ground-2023/\n\nSound Interesting? Here's the Technical Piece\n\nBelow are a few boxes you should be able to check if\u202fyou're interested: Don't check every box? Go ahead and apply!\n\n1+ year of experience with relational database management systems such as Microsoft SQL Server, MySQL, or Oracle\nUnderstanding of the full lifecycle of design, development, and implementation of data solutions including architecture design, development, testing\nSome experience in ETL development or reporting is a plus\nAbility to present technical ideas and high-level concepts and solutions to internal and external team members with varying degrees of technical knowledge\nBachelor's Degree in in computer science, information technology, or any science or business discipline\nAbility to take your work seriously but also seriously enjoy a good time with your UDig colleagues\n\nWhat's in It For You? \n\nAt UDig, you will build your career alongside talented and experienced developers while gaining experience and having fun! We provide continuous challenges and professional development opportunities. Additionally, we offer: \n\nCompetitive salary, merit reviews, and career advancement paths\nFlexible, hybrid environment\nIndividual $1500 Training Budget\nRegular team building and social activities (virtual and/or in-person)\nTransparent culture with strong communication and access to leadership\nGreat benefits like\nGenerous Paid Time Off, company holidays, and parental leave\nMultiple Single and Family Health Insurance plans to choose from\nDental & Vision coverage\nShort-Term and Long-Term disability\nOptional accident and critical illness coverage\nMatching 401(k)\nand more!"}, {"title": "Expression of Interest: Data Engineer", "company_name": "Fingerprint for Success (F4S)", "location": "New York, NY (Remote)", "link": "/jobs/view/3787769880/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=l%2FEDpQqUNmCttGdIyTk7Cg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787769880", "description": "About the job\nWe are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.\n\nThe F4S Talent Pool is a pilot project designed to:\n\nHelp job seekers get discovered by our partners based on their anticipated hiring needs.\nProvide optional support and resources for job seekers in their career endeavors.\nHelp individuals understand, and bring out the best in themselves and each other.\n\nThe F4S Talent Pool process:\n\nOnce you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.\n\nAbout Fingerprint For Success (F4S)\n\nBacked by 20+ years of research, F4S\u2019s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high performing teams.\n\nKeep in mind, joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your F4S work style assessment to match the hiring needs of our partners.\n\nYour feedback is a gift! Write to us via:\n\nPowered by JazzHR\n\noWuPJ9lwq9"}, {"title": "Data Engineer Seattle, WA (remote)", "company_name": "Conch Technologies, Inc", "location": "United States (Remote)", "link": "/jobs/view/3765052211/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=SBETI78UMutCHuRp5FnP%2FA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3765052211", "description": "About the job\nHi,\n\nGreetings from Conch Technologies Inc\n\nData Engineering Lead \u2013 With Databricks and Unity Catalog\n\nLocation \u2013 Seattle, WA (remote)\n\nDuration \u2013 12 months\n\nNeed: Design and implement data architecture and strategy, leveraging technologies such as Databricks, Unity Catalog, Privacera, and Collibra.\n\nAs a Data Engineering Lead, you will play a pivotal role in defining and executing technical deliverables while assessing complexities and levels of effort. You'll engage with both vendor and internal teams to resolve technical challenges and ensure seamless collaboration. Your leadership will extend to planning and prioritization with the SCRUM master, facilitating effective coordination between onsite and offshore teams, and driving feature prioritization through collaboration with product managers.\n\nKey Responsibilities\n\n Lead the definition and breakdown of technical deliverables, assessing complexities and levels of effort.\n Collaborate with vendor and internal teams to address technical issues.\n Drive planning and prioritization in conjunction with the SCRUM master.\n Coordinate with onsite and offshore teams for timely project delivery.\n Collaborate with product managers to prioritize features, communicate with stakeholders, and clarify requirements.\n Collect and manage user-reported issues, ensuring prompt resolution and communication.\n Define comprehensive test plans, oversee their execution, and document results.\n\n--\n\nWith Regards,\n\nNagesh G\n\nMobile: 408-381-5645\n\nDesk: 901-313-3066\n\nEmail: nagesh@conchtech.com \n\nWeb: www.conchtech.com"}, {"title": "Remote Work - Need Azure Fabric Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3759325599/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=C3iVjmcR9wjUip1nNjUiCg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3759325599", "description": "About the job\nThe client is placing heavy emphasis on Microsoft and Azure Fabric experience\n\nInformation Management Engineer \u2013 Take the identified information management sources and assist migration to the Client Data Lake, Azure Fabric. The engineer will transform information management (de-dup, clean and enrich).\n\nTake the identified data sources and bring them into our Data Lake, Azure Fabric. Once the data is in, the engineer will need to be able to transform data (de-dup, clean and enrich the data). We are focused on using a medallion data model for data architecture. The tools we are working with are: Delta Parquet files, python, spark, notebooks, T-SQL, Fabric lakehouse/warehouse, Dataflows Gen2, Data Pipelines, Direct Lake, shortcuts, ADF."}, {"title": "Data Engineer", "company_name": "Catalist", "location": "Washington, DC (Remote)", "link": "/jobs/view/3744951279/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ui1Fk6ICmnPNqAlZDZAcwA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3744951279", "description": "About the job\nFor over 17 years, Catalist has been a leader in civic data and data science innovation. Our mission is to provide progressive organizations with the data, software, and services needed to better identify, understand, and communicate with the people they need to engage and mobilize. Our clients include the largest, most influential organizations in the U.S. active in civic engagement, advocacy, and political campaigns.\n\nCatalist is home to a dedicated, creative team of technologists, data scientists, and campaign experts committed to using our talents and technology to nurture a vibrant and growing progressive community.\n\nAs a Data Engineer at Catalist, you will work closely with the Analytics and Technology departments to design, build, support, and maintain various data pipelines and processes with an end goal of providing data and intelligence to the progressive community. This role primarily involves translating data architecture designs into functional processes, code, and systems.\n\nThe ideal candidate will be a highly motivated individual with excellent technical skills, a strong desire to learn new skills, and an interest in progressive politics. Catalist values creativity and problem-solving. Our work is on the cutting edge of data-driven politics, and your support will help Democratic candidates and progressive organizations conduct successful advocacy and electoral campaigns.\n\nThis position reports to the Deputy Chief Data Officer. The Data Engineer is a part of a growing Data team that supports all underlying work at Catalist.\n\nThis position is included in our CWA bargaining unit.\n\nPrinciple Duties & Responsibilities\n\nDevelop scalable, production processes, code, and systems for data ingest, transformation, modeling, and reporting across a variety of platforms\nTranslate mock-ups and designs into functional processes, code, and systems\nCreate architecture designs when needed\nProvide quality assurance and testing on processes, code, systems, and products\nBecome an internal subject matter expert on various datasets and support other Catalist departments/teams on usage of those datasets\nExecute ad-hoc data and database maintenance tasks as requiredProject manage cross-departmental efforts, with direct responsibility for stakeholder engagement, management, and execution of technical elements\n\n\nRequirements\n\nBS or BA in a technical field, or relevant experience\n1-2 years of experience working with SQL databases\nExperience with data cleaning or ETL processes\nExperience with distributed computing systems and/or distributed datastores (particularly the Hadoop ecosystem)\nExperience managing projects\nFamiliarity with Catalist data, progressive politics, voter files, and/or commercial data\nBackground check required\n\n\nPreferred Skills & Abilities\n\nWillingness to be a problem solver and produce results in a fast paced environment\nAbility to be creative and personable, and articulate ideas clearly\nExperience working with SQL databases\nProficiency with Python or another object-oriented programming language (R, Java, Scala, etc\u2026)\nExperience working in cloud environments (AWS, GCP, etc.)\nExperience working in command line environments such as Bash\nExperience with a version control tool such as git or github\n\n\nBenefits\n\nMedical, Dental, Vision, Prescription Drug\n\nCatalist offers Medical, Dental, Vision, and Prescription Drug coverage for eligible staff and their eligible dependents. Catalist\u2019s Medical plan is a comprehensive PPO program including Prescription Drug coverage with 85% of the premium paid by Catalist. Dental and Vision coverage is provided at no cost to employees.\n\nGroup Term Life Insurance and Long-Term & Short-Term Disability Coverage\n\nGroup Term Life Insurance and Long-Term and Short-Term Disability coverage is available for eligible staff. These benefits are provided at no cost to Catalist employees.\n\n401(k) Safe Harbor Plan\n\nA 401(k) Safe Harbor Plan is available to eligible staff with a 3% contribution from Catalist from the date of hire. Employees may contribute pre-tax or post-tax from their salary up to the legal limits set forth by the IRS.\n\nMedical and Dependent Care Flexible Spending Accounts (FSAs)\n\nCatalist offers an FSA Program that gives eligible staff the ability to pay out-of-pocket medical/dental/vision/child care expenses from pre-tax earnings.\n\nTransit Benefits\n\nCatalist also makes available a Transit benefit FSA program to eligible employees using pre-tax contributions with a company match.\n\nProfessional Development and Remote Work Expenses\n\nEligible employees may be reimbursed up to $750 each year for professional development / education and remote work expenses.\n\nStudent Loan PayDown or SaveUp\n\nCatalist offers a Student Loan PayDown and College SaveUp benefit for eligible staff.\n\nVacation, Personal Leave, Sick Leave Benefits\n\nCatalist offers generous vacation benefits to all eligible staff. Eligible employees also receive:\n\n 14 Paid Holidays\n Personal Days\n Sick Leave\n Parental Leave\n\n\nHybrid Office/Remote Work\n\nCertain positions at Catalist are eligible for Office/Remote Hybrid or full Remote status."}, {"title": "Remote Entry Level Data Analyst/Engineer", "company_name": "SynergisticIT", "location": "White Plains, NY (Remote)", "link": "/jobs/view/3774171423/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=q54sk1%2FRww0z%2BC%2BE8e0jQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774171423", "description": "About the job\nAt SynergisticIT, we're all about making connections. Whatever IT goals you have, our software programmers can help achieve those. Our Software development teams can take up turnkey projects and execute them in an effective and efficient manner. If you are looking to source talent our recruiters will find the ideal IT talent for your company. What's the secret to our success? Well, it all starts with taking quality time to listen to each client's specific needs. After we have a thorough grasp of your IT goals, we can better customize our Developments as per your specific needs. We can also tailor make recruiting programs to exceed your expectations. Since our founding in 2010, SynergisticIT's strategies have earned the company an enviable position in the software development, IT staffing and IT skill enhancement fields. SynergisticIT continues to work with hundreds of satisfied American clients with our software programmers working on our projects and after gaining hands on experience on cutting edge technologies moving to contribute their skills to great clients like Apple, Google, Client, Ebay, Paypal, Kroger, the Walt Disney Company and hundreds more. If you are tired of working with inefficient programmers who take a lot of time to ramp up we want you to try us. Our software programmers can hit the ground running and get you the maximum return on your investment. You have already tried the rest its time you tried the best. SynergisticIT - Home of the Best Data Scientists and Software Programmers in the Bay Area.\n\n Why Us ? \n\nSynergisticIT has a proven track record of successfully skill enhancement and staffing IT employees for some of the world's most iconic brands. Our team takes the time to fully understand every client's needs so we could best meet your IT staffing requirements. The knowledgeable staff at SynergisticIT is always more than happy to work with clients to ensure they reach their software development goals. Besides staffing, SynergisticIT is also committed to helping young IT professionals advance their career with a robust upskill program . Everyone who goes through SynergisticIT's program learns all the skills necessary to succeed in many IT fields ranging from Java to Machine Learning. Additionally, everyone trained at SynergisticIT has been through extensive mock and technical interview screenings to bolster their career prospects. Last, but certainly not least, SynergisticIT takes great care to respect the privacy considerations for every client. All companies who work with SynergisticIT can rest assured their confidential data is protected using the most up-to-date encryption technologies. SynergisticIT also complies with all the latest NDA agreements.\n\n REQUIRED SKILLS For Java /Software Programmers \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Time series analysis \n\n We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2022 and at Gartner Data Analytics Summit (Florida)-2023 \n\nOracle CloudWorld Event (OCW) Las Vegas 2022 | SynergisticIT - YouTube\n\n https://www.youtube.com/watch?v=OAFOhcGy9Z8 \n\n https://www.youtube.com/watch?v=EmO7NrWHkLM \n\n https://www.youtube.com/watch?v=NVBU9RYZ6UI \n\n https://www.youtube.com/watch?v=Yy74yvjatVg \n\nSynergisticIT at Gartner Data and Analytics Summit 2023 - YouTube\n\nFor preparing for interviews please visit  https://www.synergisticit.com/interview-questions/ \n\n We are looking for the right matching candidates for our clients \n\n Please apply via the job posting \n\n REQUIRED SKILLS For Java /Full Stack/Software Programmer \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning Positions \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Tensorflow \n\n If you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team. \n\n No phone calls please.  Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates"}, {"title": "Data Engineer", "company_name": "Anika Systems", "location": "Leesburg, VA (Remote)", "link": "/jobs/view/3787766893/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=mApL9JnN%2B%2Fr2%2BYUPlxRYOA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787766893", "description": "About the job\nAnika Systems is a fast growing, woman-owned small business that specializes in providing innovative IT solutions for federal government agencies. Our expertise lies in accelerating delivery in Data and Analytics, Intelligent Automation, Application Development, and IT Modernization. We are currently expanding our Federal team and are seeking a passionate and talented Data Engineer. This opportunity is 100% remote.\n\nMust be a U.S. Citizen with the ability to obtain and maintain a government suitability clearance\n\nResponsibilities\n\nDesigns, develops, builds, analyzes, evaluates, and installs database management systems to include database modeling and design, relational database architecture, metadata, and repository creation and configuration management.\nDefines and oversees database organizations, standards, controls, procedures, and documentation. Provides technical consulting in the definition, design, and creation of a database environment.\nAdvises applications development staff and users on data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nDesigns and implements databases with respect to access methods, access time, batch processes, device allocation, validation checks, organization, protection and security, documentation, and statistical methods.\nUses data mapping, data mining, and data transformational analysis tools to design and develop databases.\nDetermines data storage and optimum storage requirements.\nPrepares system requirements, source analysis, and process analyses and designs throughout the database implementation.\n\nRequired Skills And Experience\n\nBA/BS and 1 year of relevant experience\nExperience with DataBricks, SQL, Python\nApplications development experience with data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nExcellent communication skills (both oral and written) with process-oriented organizational skills to ensure project success\n\nDesired Skills And Experience\n\nDemonstrated successful development experience with the full life cycle of government database management systems including database modeling and design, relational database architecture, metadata and repository creation, and configuration management.\n\nPowered by JazzHR\n\nTlKWIorucJ"}, {"title": "Remote Work - Need Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3774712978/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=r3Bp%2F0qcOD2OpC5jpbUOcA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774712978", "description": "About the job\nTitle : Data Engineer\n\nLocation : Remote\n\nDuration : 6+ months to start (long term contract (work order are extended 2x year))\n\nPlease Fill out the skill matrix: Technology Some Awareness (1) Novice (2) Intermediate (3) Advanced (4) Expert (5) SQL Tableau Desktop /Creator Tableau API Integration AWS - S3 Glue AWS - Athena Redshift AWS - Lambda AWS - SNS/SQS msExcel\n\nProfile: Data Analyst will work closely with Architects and leads in the Interflow team, key business stakeholders including program staff to support the organizations effort in the areas of data Analysis and information management. Role should have strong experience in writing SQL queries to extract data for reports. Candidate should have experience working with large set of data. As these reports are very critical for the company and we are looking for an expert who is capable of handling, consuming and extracting large sets of data for analysis and reports.\n\nMust Haves\n\nAWS - Athena\nAWS - S3\nGlue\nhands-on development of reporting applications\nMS Excel\nRedShift\nSQL\nTableau Desktop /Creator\n\nNice\n\nAWS - Lambda\nAWS - SNS/SQS\nTableau API Integration\n\nSkills\n\nBA/BS required (major in an analytical field desired) \nA minimum 10+ years work-related experience. \nExperience in hands-on development of reporting applications for the Web in a professional environment \nThe Ethos of continuous improvement and interest in learning new things.\nStrong analytical thinking and structured problem-solving ability\nAbility to handle multiple projects and assignments simultaneously and effectively in a cross-functional team environment. \nVersed on the agile methodology and best practices. \nExcellent interpersonal and collaboration skill with the ability to work with a diverse set of colleagues, across functions, from different organizations, disciplines, etc. \nSelf-starter, ability to set priorities, work independently and attain goals"}, {"title": "Data Engineer", "company_name": "Team Remotely Inc", "location": "Houston, TX (Remote)", "link": "/jobs/view/3786249829/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=78lY5Jo%2FfyRPC6TQYCdvww%3D%3D&trk=flagship3_search_srp_jobs", "id": "3786249829", "description": "About the job\nThis is a remote position.\n\n Data Engineer (US/Canada Residents Only, 1 year experience, remote)\n\nTeam Remotely Inc. is a staffing and recruitment agency that offers a comprehensive solution for talent acquisition, including sourcing, vetting, pay rolling, and managing talent. Whether you need contract staffing, direct hire, direct sourcing, talent pools, or diversity initiatives, our model can support your hiring strategy.\n\n Hiring Type: Full-Time\n\n Base Salary: $55K-$70K Per Annum.\n\nResponsibilities\n\nDesigning user interface changes for web-based DB applications. \nReviewing application requirements and interface designs. \nDeveloping and implementing highly responsive user interface components using react concepts. \nWriting application interface codes using JavaScript following react.js workflows. \nTroubleshooting interface software and debugging application codes. \nDeveloping and implementing front-end architecture to support user interface concepts. \nMonitoring and improving front-end performance. \n\nQualifications And Experience\n\nBachelor\u2019s degree in computer science, information technology, or a similar field. \nPrevious experience working as a react.js developer. \nIn-depth knowledge of JavaScript, CSS, HTML, jQuery, and front-end languages. \nKnowledge of REACT tools including React.js, Webpack, Enzyme, Redux, and Flux. \nExperience with user interface design. \nKnowledge of performance testing frameworks, including Mocha and Jest. \nExperience with browser-based debugging and performance testing software. \nTroubleshooting skills. \nProject management skills. \nProblem-solving skills. \nVerbal communication skills. \n\n Why work with Team Remotely?\n\nTeam Remotely Inc. is a staffing platform offering a seamless experience for employers and candidates. Employers can post job openings and specify their requirements, while candidates can create profiles and upload resumes.\n\nThe team of Team Remotely continuously learns and adapts based on previous successful placements, constantly improving its matching capabilities. This ensures that the recommendations provided by Team Remotely are tailored and accurate, increasing the likelihood of a successful match between employers and candidates. By providing intelligent and data-driven solutions, they strive to enhance the efficiency and effectiveness of the hiring process, ultimately helping companies find the best talent and individuals find their dream jobs."}, {"title": "Data Engineer", "company_name": "ZAG Technical Services", "location": "United States (Remote)", "link": "/jobs/view/3771403464/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=Hf0VefrKXBFIL5MtHmY0Pw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3771403464", "description": "About the job\nWe are looking for a tech savvy Data Engineer to contribute to the building, operating, and scaling of next generation data platforms and tools that will power data-driven analytics capabilities throughout the ZAG products and services portfolio, spanning areas such as business intelligence, reporting, and data analytics.\n\nThe ideal team member is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. Must be comfortable supporting the data needs of multiple clients and teams, systems, and products within the modernization of ecosystems. The right-fit person will be excited by the prospect of optimizing or even re-designing data processes to support next generation of advanced analytics capabilities, which in-turn translates to implementing strategic enhancements to maintain competitive advantage in the industry.\n\nThe successful team member will leverage their proficiency to\u2026\n\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\nBuild the analytics processes required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Data Management and cloud based \u2018big data\u2019 technologies.\nBuild analytics tools that utilize technologies to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.\nCollaborate with client teams to identify and solution regional and local requirements within the context of the standard global business model for enterprise analytics.\nParticipates in meetings, discussions, strategy sessions where changes, improvements, and enhancements are proposed, evaluated, and approved. \n\n\nThe exceptional team member will have or demonstrate progressive experiences in\u2026 \n\nBachelor\u2019s degree or higher in Computer Science or related field\nExperience working with BI and data warehousing tools\nExperience working with enterprise data, where security is paramount and data governance is critical\nAbility to work independently and in a team environment, and effectively engage all levels of the organization\nUnderstanding of distributed systems driving large-scale data processing and analytics with a successful history of manipulating, processing, and extracting value from large, disconnected datasets.\nAbility to communicate effectively (listening, presenting, and questioning)\nStrong organizational, written, and communication skills\n\n\nAbout ZAG\n\nZAG Technical Services empowers our clients\u2019 success through intelligent business solutions that increase employee productivity, network security and team collaboration. We aim to ensure that our client\u2019s IT investments drive ROI, provide a competitive advantage and help businesses grow. We do this by living our core set of values: Integrity Foremost, Client-Centered, Accountable Always, Teamwork Throughout, and Exceptional is the Goal.\n\nWhat ZAG Has To Offer\n\nCompetitive compensation package \nBenefits including medical, dental, vision, 401K and life insurance \n\n\nZAG Technical Services is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all of our employees.\n\nZAG Technical Services is an E-Verify participant and will provide the Social Security Administration (SSA) and, if necessary, the Department of Homeland Security (DHS), with information from each new employee\u2019s Form I-9 to confirm work authorization."}, {"title": "Junior Data Engineer", "company_name": "HireMeFast LLC - Career Accelerator - Land A Job", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3786249797/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D&trk=flagship3_search_srp_jobs&lipi=urn%3Ali%3Apage%3Ad_flagship3_search_srp_jobs%3BQExogrHBQoSF9zZm%2B9kufw%3D%3D&lici=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D", "id": "3786249797", "description": "About the job\nThis is a remote position.\n\n Job Title: Junior Data Engineer\n\n Employment Type: Full-Time\n\n Salary: $64,000 - $76,000 per annum\n\n Experience Required:  Minimum 1 year of project experience\n\nAbout us: HireMeFast is a leading staffing and recruitment agency specializing in connecting businesses with top-tier talent across various industries. Our mission is to bridge the gap between exceptional candidates and organizations needing their skills, expertise, and unique qualities. Our team of experienced and dedicated recruitment specialists utilizes innovative sourcing strategies, and a vast network to identify and attract top talent. We conduct comprehensive procedures to ensure that only the most qualified candidates are presented to our clients.\n\nPosition Summary\n\nJoin the fast-paced, innovative, and collaborative environment focused on providing an AIOps platform that enhances the intelligence of the CVS Health infrastructure. Work closely with subject matter experts and colleagues to build and scale out machine learning and AI solutions that will detect, predict, and recommend solutions to correct issues before system impact and enhance efficiency, reliability, and performance of CVS Health\u2019s IT operations.\n\nKey Responsibilities include:\n\nData pipeline development: Designed, implemented, and managed data pipelines for extracting, transforming, and loading data from various sources into data lakes for processing, analytics, and correlation. \nData modeling: Create and maintain data models ensuring data quality, scalability, and efficiency \nDevelop and automate processes to clean, transform, and prepare data for analytics, ensuring data accuracy and consistency \nData Integration: Integrate data from disparate sources, both structured and unstructured to provide a unified view of key infrastructure platform and application data \nUtilize big data technologies such as Kafka to process and analyze large volumes of data efficiently \nImplement data security measures to protect sensitive information and ensure compliance with data and privacy regulation \nCreate/maintain documentation for data processes, data flows, and system configurations \nPerformance Optimization- Monitor and optimize data pipelines and systems for performance, scalability and cost-effectiveness \n\nCharacteristics of this role:\n\nTeam Player: Willing to teach, share knowledge, and work with others to make the team successful. \nCommunication: Exceptional verbal, written, organizational, presentation, and communication skills. \nCreativity: Ability to take written and verbal requirements and come up with other innovative ideas. \nAttention to detail: Systematically and accurately research future solutions and current problems. \nStrong work ethic: The innate drive to do work extremely well. \nPassion: A drive to deliver better products and services than expected to customers. \n\nRequired Qualifications\n\n2+ years of programming experience in languages such as Python, Java, SQL \n2+ years of experience with ETL tools and database management (relational, non-relational) \n2+ years of experience in data modeling techniques and tools to design efficient scalable data structures \nSkills in data quality assessment, data cleansing, and data validation \n\nQualifications\n\nKnowledge of big data technologies and cloud platforms \nExperience with technologies like PySpark, Databricks, and Azure Synapse. \n\nEducation\n\nBachelor\u2019s degree in Computer Science, Information Technology, or related field, or equivalent working experience\n\nWhy HireMeFast LLC?\n\nAt HireMeFast, we understand that finding the right individuals to join your team is crucial for success and growth of your organization. We are committed to streamlining the hiring process for our clients, ensuring they have access to a diverse pool of highly qualified candidates who are a perfect fit for their specific needs."}, {"title": "Data Engineer  - Need candidates from Texas ( REMOTE )", "company_name": "Pulivarthi Group (PG)", "location": "United States (Remote)", "link": "/jobs/view/3770685212/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=9gdMwuif%2BpB%2BCQYvQi3imQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3770685212", "description": "About the job\nFollow us on Linkedin: https://www.linkedin.com/company/pulivarthigroup/\n\nPulivarthi Group LLC is a Global Staffing & IT Technology Solutions company, with our prime focus of providing world class solutions to our customers with the right talent. We combine the expertise of our team and the culture of your company to help you with the solution that is affordable and innovative using high quality standards and technologies.\n\nWe\u2019ve served some of the largest healthcare, financial services, and government entities in the U.S.\n\nData Engineer - FT only - Azure SQL with data lakes experience\n\nData Engineer -\n\nthe resource requirement for this Data Engineer role needed more clarification.\n\nOur customer is requesting a resource who knows Azure CI/CD (Continuous Integration / Continuous Deployment) to build and integrate their DevOps pipeline.\n\nOnce the DevOps pipelines are mapped, they need the Data Engineer to connect to all the Data Sources, primarily their Data Lake.\n\nThe Customer (based in Houston) is interested in a resource in a time zone not too far removed from the Central Time Zone to allow real time collaboration."}, {"title": "Data Engineer", "company_name": "Otter Products", "location": "Fort Collins, CO (Remote)", "link": "/jobs/view/3766728302/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=77U3U7CAi1PxgDmpI3%2BiNQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3766728302", "description": "About the job\nOverview \n\nOtter Products is currently recruiting for a Data Engineer to join our Data team! You can be based in our Fort Collins, CO office with flexibility to work a portion of your time remotely, or 100% Remote in the U.S.\n\nThe Data Engineer applies professional experience, concepts, and company objectives toward building and operationalizing the minimally inclusive data necessary for the enterprise data initiatives following industry standard practices and tools. The bulk of the Data Engineer's work would be in designing, managing and optimizing data pipelines, then moving these data pipelines effectively into production for key data and analytics consumers like business/data analysts, data scientists or other stakeholder that needs curated data for data and analytics use cases across the enterprise. The Data Engineer also ensures compliance with data governance and data security requirements while creating, improving and operationalizing these integrated and reusable data pipelines to enable faster data access, integrated data reuse and vastly improved time-to-solution for data and analytics initiatives. Additionally, the Data Engineer will be expected to collaborate with other data team members and data consumers working on the models and algorithms developed by them toward optimization for data quality, security, and governance to put them into production leading to potentially large productivity gains.\n\n About Otter Products \n\nOtter Products, we grow to give. From our founder's garage in 1998 to the global technology leader we are today, Otter Products continues to drive growth through innovation.\n\nThrough our industry-leading brands - OtterBox, Liviri and OtterCares - we provide our partners the number-one selling and most trusted products in our categories. Our philanthropic spirit is the foundation on which we foster our partner relationships, allowing us to grow and to give - together.\n\nBy way of our charitable arm, the OtterCares Foundation, we support our communities and invest in our future through education that inspires kids to change the world.\n\nAnd even as our global community of Otters continues to grow, our founder's core values are still at the heart of everything we do. We measure our success by our ability to give back to our communities and strengthen opportunities for all.\n\nFor more information visit otterproducts.com\n\n Responsibilities \n\nComplete analysis, design and development of BI solutions using Azure SQL Server\nFamiliarity with Data Warehouse concepts\nExperience coding the extraction, transformation, and loading (ETL) processes\nDatabase development primarily in SSIS, Data Factory and SQL\nPartner with the business to determine end user database/reporting needs and requirements\nGenerate ad hoc reports using Power BI or SSRS\nCollaborate with other developers to create and implement best approach solution(s)\nTroubleshoot Azure tools, systems, and software\nReview queries for performance issues, making changes as needed\nCollaborate with team to performance-tune Azure application as necessary\nAssist with the analysis and extraction of relevant information from large amounts of historical business data to feed data science initiatives\nParticipate and support the design and documentation of processes for large scale data analyses, model development, model validation and model implementation\nSupport and maintain a positive safety culture by following all safety policies and procedures and actively contributing to a safe working environment\nOther duties as required\n\n Qualifications \n\nBachelor's degree required. Degree in Computer Science or Mathematics preferred\nMinimum of four years of experience in an IT or analytical role required. Experience in database development, report writing and/or statistics preferred.\n\n EEO \n\nOtter Products, LLC is an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, age, marital status, pregnancy, sex, sexual orientation, gender, gender identity or expression, national origin, disability, veteran status, or any other characteristic or status protected by law.\n\n For US Based Roles Only - Compensation Range Minimum \n\nUSD $90,000.00/Yr.\n\n For US Based Roles Only - Compensation Range Maximum \n\nUSD $120,000.00/Yr.\n\n Additional Total Rewards \n\nProfit Sharing Program Eligible, Benefits Eligible - Full Time- check out otterproducts.com/careers/why for more info"}, {"title": "Software Engineer, Distributed Systems", "company_name": "Hightouch", "location": "United States (Remote)", "link": "/jobs/view/3628572539/?eBP=CwEAAAGMd4QO8APODyE_-mMOPhYyQGg5ZjVbPwq_3V7DK0B2bLAf10pcedM9S2T71X5i4TiJuEwOzWBbSiTKbLuk-VD-_Y-dN5uyf8YpYkHYqEYFxySMS9zr2hRjNrxX9gXbxssh35TFK9YTNV2WeH2Ncb675JgFbJTgNWSZySFC6rrdJGmdSDTr2q5d3VoxORz3GjsDbzjeyZQCvQ07RqYw7oujWSgslycs2LMCY_-N0NEj6e0PvbaYKiZ_5IW34bGVSYLXpHwqa8oOcNttYZ90UA3E_7sDSFzABdIVlfrp5zzERrIYy5sh2MIInxHdDoKgQw8VmOn5uM4S0tXUl-i7LnN0_W0NfPBmLiVDNuveIOm4MfDNPxfe5QKPk6FUAVpvgbcoJn9yRyE&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=rph%2FT6Q6Vkp6YoggJf9cJQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3628572539", "description": "About the job\nAbout Hightouch\n\nHightouch\u2019s mission is to empower everyone to take action on their data. Through our Reverse ETL platform, business and data users can seamlessly sync data from where it resides, such as warehouses and databases, to where it is needed, including operational systems and SaaS tools. Traditionally, acting on data has required engineering time and bandwidth, and left most business users stuck with charts and reports that are unable to take automated action on their data. With Hightouch, every business user, without writing any code, can activate data to streamline critical processes, improve marketing performance, and scale operations.\n\nOur team operates with a focus on making a meaningful impact for our customers. We believe in approaching challenges with a first principles thinking mindset, moving quickly and embracing our value of efficient execution, and treating each other with compassion and kindness. We look for team members that are strong communicators, have a growth mindset, and are motivated and persistent in achieving our goals.\n\nHundreds of companies use Hightouch, including Spotify, Ramp, Retool, NBA, Plaid, and Betterment. We\u2019re based in San Francisco, are remote-friendly, and backed by leading investors such as Amplify Partners, ICONIQ Growth, Bain Capital Ventures, Y-Combinator, and Afore Capital.\n\nAbout The Role\n\nWe are looking for a distributed systems engineer to work on the systems that power the syncing engine our customers and other engineering teams rely on. This presents an exciting challenge where you can apply your expertise in distributed systems, performance optimization, and troubleshooting to push the boundaries of what is possible and meaningful improve our ability to move massive amounts of customer data. This role also provides a unique opportunity to work on a multi-cloud and multi-region infrastructure that supports a global customer base.\n\nWe believe in enabling our engineers to do their best work for our customers by giving them extremely high levels of ownership and autonomy. This comes in different forms: you will own and deliver projects from start to finish, you will work directly with customers to solve their hardest scaling problems, and you will have a lot of influence over what we work on as a team and company.\n\nSome of the problems we\u2019ll be working on include:\n\nSync Speed: Customers want to sync a lot of data to important destinations like Facebook and Snapchat, which requires us to analyze every part of our syncing process and find where we can optimize to sync data more quickly\nStreaming Syncing: We currently sync data from sources in batch only, but envision a world where we also do syncs in real-time and support streaming sources like webhooks and queues\nScalability and Reliability: As part of our rapid growth, we\u2019re always evaluating current future bottlenecks in scaling and reliability, and architecting for the next order of magnitude of growth\nPersonalization API: Our Personalization API product provides a low latency caching layer on top of data warehouses that enable customers to use their data warehouse for real-time personalization use cases with \nMulti-Region and Multi-Cloud: Supporting our multi-region and multi-cloud backend, including extending it to launch Hightouch on in new regions to support data residency requirements of our global customer base\n\nWe are looking for talented, intellectually curious, and motivated individuals who are interested in tackling the problems above. This is a senior role, but we focus on impact and potential for growth more than years of experience. The salary range for this position is $170,000 - $240,000 USD per year, which is location independent in accordance with our remote-first policy.\n\nAbout You\n\nYou are an engineer with a passion for solving hard technical problems that generate real value for customers. You\u2019re motivated by high ownership and are comfortable in a fast-paced, startup environment.\n\nYou have experience and comfort with distributed systems and high-scale systems or the ability and desire to build this experience quickly. This experience can take different forms:\n\nYou\u2019ve built syncing engines that process and move terabytes of data per day in a scalable way\nYou\u2019ve created event collection or data streaming services that power company critical systems\nYou\u2019ve owned systems that have undergone orders of magnitude of growth and helped them scale accordingly\nYou have relevant skills and are a fast learner who is excited to take on the challenges we face\n\nInterview Process\n\nOur goal with the interview process is to balance speed with giving both parties opportunities to assess whether there is a strong mutual fit. We will ask you questions, but we want you to ask us questions! Our technical interviews focus on how you design systems because we believe this is the best way for us to see how you work and for you to see how we collaborate. We don\u2019t ask you to write code to solve technical brainteasers that don\u2019t appear in your day to day job.\n\nRecruiter screen [30m]: Introductory call with our recruiting team to get to know each other and see if the role could be a good mutual fit.\nSystems design screen [45m]: Designing a data processing feature end-to-end.\nHiring manager interview [30m]: Chat with hiring manager about past experiences and future operating preferences to assess fit on company values and operating principles.\nSystems design interview [90m]: Work with the interviewer to architect a system at a conceptual level. The problem will be at a pretty high level - and have both product and customer requirements as well as technical."}, {"title": "Entry-Level Data Scientist Engineer", "company_name": "Phoenix Recruitment LLC", "location": "Texas City, TX (Remote)", "link": "/jobs/view/3788727625/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=8iwtytHsCu4wMwJ4fj3pxw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3788727625", "description": "About the job\nThis is a remote position.\n\nTitle - Entry-Level Data Scientist Engineer, 1 year of project experience\n\nEmployment Type: Full-time\n\nBase Salary: $60K-$70K\n\nPhoenix Recruitment offers a variety of recruiting services to assist both employers and employees. They specialize in marketing open positions, recruiting, and helping employers to find qualified candidates across various industries. Phoenix Recruitment has expertise in streamlining the hiring process. They can help ensure that the process is efficient, well organized, and compliant with relevant regulations.\n\nSkills and Abilities:\n\nStrong knowledge of R or Python for data analysis and modeling. \nProficiency in statistical programs such as R, SAS, MATLAB, or Python. \nFamiliarity with spreadsheets (VBA) and database applications (Access, Oracle, SQL, or equivalent technology). \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \nAbility to learn new methods quickly and work under deadlines. \nExcellent teamwork and communication skills. \nStrong analytical and problem-solving abilities. \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \n\nPreferred:\n\nKnowledge of actuarial concepts and life, health, and/or annuity products. \nExperience with statistical modeling techniques such as GLM, Decision Trees, Time Series, Regression, etc. \nFamiliarity with Microsoft DeployR. \nExposure to insurance risk analysis. \nBasic experience in computational finance, econometrics, statistics, and math. \nKnowledge of SQL and VBA. \nFamiliarity with R or Python for predictive modeling \n\nWhy Phoenix Recruitment LLC?\n\nPhoenix Recruitment often has an extensive network of employers and candidates. This network allows them to tap into a pool of qualified candidates and connect them with suitable job opportunities. They can also leverage their connections to help employers find the right talent efficiently. Outsourcing the recruitment process to a specialized agency can save your time and resources, avoid delays, reduce administrative burdens, and increase the chances of finding the right fit for your organization."}, {"title": "Data Engineer - Remote Work", "company_name": "BairesDev", "location": "United States (Remote)", "link": "/jobs/view/3783162297/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=OtoCrAAHZFomVup04fy35g%3D%3D&trk=flagship3_search_srp_jobs", "id": "3783162297", "description": "About the job\nWho We are\n\nBairesDev is proud to be the fastest-growing company in America. With people in five continents and world-class clients, we are only as strong as the multicultural teams at the heart of our business. To consistently deliver the highest quality solutions to our clients, we only hire the Top 1% of the best talents and nurture their professional growth on exciting projects.\n\nWe are looking for a Data Engineer to join our Development team and participate in different projects made up of multicultural teams distributed throughout the world.\n\nWhat You Will Do\n\n Interview other Data Scientists and Data engineers to vet their technical abilities.\n Input data from those interviews into the product and help develop a predictive, fair, and enjoyable solution.\n Work together with developers, tech leads, and solution architects to build applications.\n Improve existing structures by adding new functionalities or proposing technological updates so that as a result, the impact of your contribution is significant in the core of each business.\n Improve the user experience on scalable and high availability platforms, contributing to the key differential of each business.\n\nHere\u2019s What We Are Looking For\n\n 5+ years of experience working as a developer.\n 3+ years of experience working as a Data Scientist / Data Engineer.\n Proficient with analysis, troubleshooting, and problem-solving.\n Hands-on experience with managing data loads and data quality.\n Advanced English level.\n\nHow we do make your work (and your life) easier:\n\n 100% remote work.\n Hardware setup for you to work from home.\n Flexible hours - make your schedule.\n Paid parental leave, vacation & holidays.\n Diverse and multicultural work environment.\n An innovative environment with the structure and resources of a leading multinational.\n Excellent compensation \u2014 well above the market average.\n Here you can grow at the speed of your learning curve.\n\nOur people work remotely but with a consistent and robust culture that promotes diversity and teamwork. To continue being the leading software development company in Latin America, we want to ensure that every BairesDev member gets the best growth and professional development opportunities in a diverse, welcoming, and innovative environment.\n\nEvery BairesDev team member brings something unique to our company.\n\nWe want to hear your story. Apply now!"}, {"title": "Data Engineer (Remote)", "company_name": "Cascade Debt", "location": "United States (Remote)", "link": "/jobs/view/3755336571/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=pmUtn1j2zyPr25CNPh2RNw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3755336571", "description": "About the job\nDo you want to work in a small team where you can make a real difference in a company? Would you embrace the challenge of building a high impact platform that is used globally? Then we want to speak with you!\n\nAbout Cascade\n\nCascade is a fintech startup backed by Canadian and US investors that empowers companies to grow by democratizing access to institutional debt. We are building tools that modernize how companies raise and manage debt, lowering the barriers to entry to the $7 trillion specialty finance and alternative credit market for companies around the world. Founded just last year, we are gearing up for our next phase of technical development and are seeking a talented Data Engineer to lead the way.\n\nAbout The Role\n\nAs a Data Engineer you\u2019ll be an early member of a growing team building a pioneering platform in the debt infrastructure space. You will design, build, and launch efficient, scalable, and reliable data pipelines to move and transform data .\n\nWhat You\u2019ll Do\n\n Building databases, data lakes and data ingestion pipelines to integrate customer databases and datasets to our systems \n Ingesting financial datasets from external customers, then u pdating and maintaining accurate and complete data mappings to ensure that our product s are displaying high quality data \n M onitoring and alerting across our data pipelines in order to make sure that our data ingests are reliable and correct \n Translat ing business goals and stakeholder requirements into new data flows and deliver analytical insights ensuring that the data flows create real business value continuously \n\nSkills And Experience\n\n 2 + years experience in a data engineering or data analyst role \n E xperience with relational SQL and NoSQL databases, including PostgreSQL , MySQL, MariaDB, MongoDB, Bigtable, etc. \n E xperience working with ETL technologies , such as Databricks, Fivetran , or dbt \n Proficiency in writing SQL queries and knowledge of analytical data warehouses such as RedShift, BigQuery , and Snowflake \n Some experience with CI/CD automation such as GitHub Actions \n Developing APIs and integrating with 3rd party APIs \n Bonus: experience in fintech / SaaS / credit analysis \n\nPeople Who Thrive At Cascade Are\n\n Self-starters, who take the initiative to tackle challenges in a remote work environment \n Good communicators, who can operate without ego to discuss, learn, grow, and help others do the same \n Passionate about creating great digital experiences for users \n Problem solvers who are not satisfied with the status quo \n\nBenefits\n\nRemote: we are a remote-first company and are very flexible on hours as long as things get done \nHome-office: there\u2019s a $1000 USD home office allowance to set yourself up \nEquity: we expect you to have an owner-mentality, and have the equity plan to match \nBenefits: health, dental, vision, and more \nPerks: we offer free lunches weekly and off-site trips \nJob satisfaction: we offer autonomy, ample opportunities for mastery, and an opportunity to make a difference for companies around the world"}, {"title": "Data Engineer", "company_name": "ISC (Integrated Specialty Coverages, LLC)", "location": "Carlsbad, CA (Remote)", "link": "/jobs/view/3778293528/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ew7%2FfEuXSujs5IGabdqUyw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3778293528", "description": "About the job\nAbout Integrated Specialty Coverages\n\nIntegrated Specialty Coverages LLC (ISC) is a growth stage technology and data-driven insurance company leading innovation in the market.\n\nBacked by one of the leading private equity firms, KKR, and led by a forward-thinking management team, ISC is combining the worlds of insurance and technology to create an Insurtech powerhouse. As a leading online distributor of insurance products for a range of industries and \"Main Street USA\", we are looking for the right people to help us in our mission of achieving exponential growth. We strive to be the number one place to go for brokers and agents to source insurance. To accomplish this, we're building a digitally focused team that deeply understands the intersection between user experience, data, and AI/ML to optimize the way we engage with our customers and partners.\n\nJob Summary\n\nThe Data Engineer role will be the technical liaison between multiple groups including a data science team, the engineering team, product management, and business stakeholders. You do not need any insurance knowledge prior, however, you must quickly dive deep into the insurance world and ask questions to become a subject matter expert. You will be responsible for building a data platform to facilitate the data science team. You must be a self-starter that can build out features such as a data pipeline from scratch. There will be support from both engineering and data science for any buildout.\n\nJob Responsibilities\n\nCreate and maintain optimal data pipeline architecture. \nAssemble large, complex data sets that meet functional / non-functional business requirements. \nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. \nBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using AWS technologies, SQL, and Python. \nWork with stakeholders including the Executive, Product, Data Science, and Engineering teams to assist with data-related technical issues and support their data infrastructure needs. \nWork with data science and analytics teams to strive for greater functionality in our data systems. \n\nMinimum Qualifications:\n\nAdvanced working SQL knowledge and experience working with relational databases, strong query authoring (SQL) as well as working familiarity with a variety of databases (Snowflake, Redshift, MySQL, MSSQL, etc.)\nExperience building and optimizing data pipelines, architecture and data sets. \nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. \nBuild processes supporting data transformation, data structures, metadata, dependency, and workload management. \nA successful history of transforming, processing and extracting value from large disconnected, datasets from a variety of data sources (Flat files, Excel, databases, etc.)\nStrong analytic skills related to working with unstructured datasets. \nStrong project management and organizational skills. \nExperience supporting and working with cross-functional teams in a dynamic environment. \nAbility to mentor/guide/collaborate with other team members. \n\nWe are looking for a candidate with 3-5 years of experience in a Data Engineer role. They should also have experience using the following software/tools:\n\nExperience with relational and MPP databases, including Snowflake and MySQL. \nExperience developing software in an agile environment from the requirements stage to production. \nExperience with version control: git\nExperience with container technologies: Docker\nExperience with data pipeline and workflow management tools: Airflow, Jenkins, AWS Glue, Azkaban, Luigi, etc. \nExperience with AWS cloud services: EC2, ECS, Batch, S3, EMR, RDS, Redshift\nExperience with other cloud services: Snowflake, Airflow\nExperience with object-oriented/object function scripting languages: Python, Java, C++, etc. \nExperience with data modeling and data warehouse design\nExperience with data visualization tools (PowerBI, QuickSight)\n\nThe starting annual pay scale for this position is listed below. Actual starting pay will be based on factors such as skills, qualifications, training, and experience. In addition, the company offers comprehensive benefits including medical, dental and vision insurance, 401(k) plan with match, paid time off, and other benefits.\n\nISC's salary ranges are determined by role and level. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations and could be higher or lower based on a multitude of factors, including job-related skills, experience, and relevant education or training.\n\nNational Pay Range\n\n$100,000\u2014$110,000 USD\n\nBenefits of Working at ISC\n\nCompetitive vacation and flexible working arrangements\nComprehensive and inclusive health benefits\nA variety of professional development and mentorship opportunities\nChoice of technology whether at home or in the office\n\nISC's Ownership Behaviors\n\nDo the Right Thing *Be Relentless. Pursue Excellence. *Personal Responsibility & Own Outcomes\nTry Fast. Learn Fast. Fail Fast. Think Big. *Build Extraordinary Owners & Join Forces *Grit & Determination\n\nApplicants with disabilities may contact ISC HR department via e-mail, and other means to request and arrange accommodation. If you need assistance to accommodate a disability, you may request accommodation at any time. Please contact ISC at HR@ISCMGA.com.\n\nISC believes in creating long-term relationships by being responsive and relevant and by consistently delivering value to our community of customers. Specifically, with our employees, we focus on attracting, developing, and retaining the best talent for our business, challenging our people, demonstrating a \"can-do\" attitude, and fostering a collaborative and mutually supportive environment. ISC is an equal opportunity employer and a participant in the US federal E-Verify program (US).\n\nDiversity creates a healthier atmosphere: All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, gender, gender identity, sexual orientation, marital status, medical condition, genetic information, mental or physical disability, military or veteran status, or any other characteristic protected by local, state, or Federal law."}, {"title": "Data Engineer -Remote", "company_name": "RIT Solutions, Inc.", "location": "Denver, CO (Remote)", "link": "/jobs/view/3768007496/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=1hQH8EDaII1eJ7nQ9BACQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3768007496", "description": "About the job\nDuration: 6+ months CTH\n\nMust have a valid LinkedIn profile\n\nRequired Skills\n\n 3-6 years of experience (no more than 6 years)\n Start up environment experience\n Python, SQL, and DBT"}, {"title": "Big Data Engineer - W2", "company_name": "Megan Soft Inc", "location": "United States (Remote)", "link": "/jobs/view/3674845652/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fCAxh4N%2F4YZJ5aEIASaeeA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3674845652", "description": "About the job\nTitle: Big Data Engineer - W2\n\nLocation: Dearborn, MI (Remote)\n\nDuration: 12+ Months\n\nDirect Client: Ford Motors\n\nMust have skills: Should be able to handle GCP requirements\n\n\"Translates business requirements to build repeatable, sustainable, efficient, coded processes that can be productionized by Software Engineers and readily modified by other Data Engineer Creates POC processes in Dev/QA and works with Software Engineers to productionize and align best coding and process practices that maximize efficiency, speed, stability, system resources and capabilities Awareness of and compliance with data privacy, security, legal and contractual guidelines Incorporates data quality and privacy checks/alerts to minimize bad data being consumed by end users, models and dashboards, and to protect customer data; revises checks for new data issues Validates data products and pipelines are functioning as expected following system or application upgrades, source changes, etc. Works closely with Architects if required to align systems, tools and applications being utilized with business use case and performance requirements Provides data product support and maintenance Establishes, tracks and monitors KPIs related to specific data products and deliverable Participate in iteration cadence activities (Iteration Planning, task estimation, daily scrum, demos, etc.). Will be a part of on-call team to resolve critical production support tickets during business hours and off hours.\n\nSkills Required\n\nExperience with Big Data Technologies (Hadoop , ETL Tool, Spark. Hive etc..) Someone who understands Cloud and has experience working with Cloud Platforms (preferably GCP) Should be able to handle GCP requirements - ingesting into GCS/ Big Query, working with GCP components Hands-on in Data Flow, Cloud Storage (GCS), Big Query and GitHub. Proficient in Python and SQL.\n\nSkills Preferred\n\nManages ETL: uses programming and tools for data ingestion, configures pipelines, applies transformations and decoding, integrates and fuses data, moves and securely deliver Experience NoSQL databases\n\nAdditional Information\n\nIf the candidate is remote only, please indicate \"\"100% Remote\"\" under candidate's name on resume. Other similar schedule notations can be \"\"Local Candidate\"\" or \"\"Hybrid Candidate.\"\" ***HRA Test Required***\""}, {"title": "Remote Opportunity: Quantexa Data Engineer", "company_name": "SPAR Information Systems LLC", "location": "United States (Remote)", "link": "/jobs/view/3680073191/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ioykVCZSOQ7BK5%2FwRQHOWg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3680073191", "description": "About the job\nHello All, Hope you are doing great!! Please go through the job descriptions and let me know your interest. Role: Quantexa Data engineer  Location: Remote Duration: Long Term Contract Job Description: 1. End to End usage of Quantexa including guiding our client team on how to use from both front end and data ingestion Reason they are asking for Certification 2. Resolve Entity issues with data 3. Knowledge of Scala and Hive which requires coding experience using parquet 4. Advanced SQL knowledge requiring writing SQL queries 5. Experience having worked with GCP data lake 6. Quantexa certification required\n\nThanks & Regards,\n\nSatnam Singh\n\nDirect: 201 623 3660\n\nEmail : Satnam.singh@sparinfosys.com"}, {"title": "Remote Work - Need Staff Data Engineer - USC only", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3746283980/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=CvIuLBNO%2BPMGi4IXVRkP0A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3746283980", "description": "About the job\n100% Remote, client is in NY\n\nLook for strong job history, Python, Machine Learning, SQL, 10/10 comms, etc. \n\nMUST be an Engineer \n\nSummary\n\nWe're looking for an experienced Data Engineer to join our client's growing team of top notched IT professionals. You will work closely across multiple groups including Editorial and Audio, Marketing, Advertising, etc. to achieve their goals and objectives. The Data Engineering team builds tools throughout the stack while sharing knowledge across these departments.\n\nAs the company grows, they're looking for Data Engineer who will help solidify and expand our pipelines and maintain their data warehouse. Their business model is based on performing complex and very detailed analyses of how our products perform with their subscribers. You'll be responsible for working alongside analysts, product, marketing, and other teams to define new data collection components and measurement schemes that support each new product and feature.\n\nHow Can Add Value To Their Mission\n\nCreate and maintain data pipelines to provide insights and drive business decisions\nEstablish data warehousing strategy (ex. Kimball, Data Vault, etc.)\nMaintain data infrastructure on our AWS accounts\nCollaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.\nWrite unit/integration tests, contributes to engineering wiki, and documents work\nImplement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it\n\nWhat You'll Need To Succeed\n\n7+ years of industry experience measuring product performance and user behavior\nExperience working with a variety of data management technologies, including RedShift, Kinesis/Kafka, Glue, Spark, Postgres, Airflow, dbt, and others.\nExperience implementing BI reporting tools such as Looker\nExperience interfacing with engineers, product managers and analysts to understand data needs\nKnowledge of a variety of measurement beacons, SDKs, APIs including Google Analytics, Amplitude, Braze, Stripe, email service providers\nA commitment to building secure, resilient, fault-tolerant architectures, with clear documentation and procedures in place for support\nA focus on accuracy and detail as you build, ensuring data pipelines produce clear, predictable results\nUnderstanding of the typical metrics a subscription and advertising-supported business needs to measure success\nExperience with Client techniques as applied to behavioral segmentation or anomaly identification is a definite plus\nFamiliarity using developer tools that increase productivity and facilitate the development of resilient code (eg. Docker, CircleCI, Serverless) is a plus"}, {"title": "ENTRY LEVEL DATA ENGINEER (REMOTE)", "company_name": "SynergisticIT", "location": "Jacksonville, FL (Remote)", "link": "/jobs/view/3767587909/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=om7L3CnowiC4PLIcpZD70A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3767587909", "description": "About the job\nSYNERGISTICIT wants every candidate to know that the Job Market is Challenging and to stand out, you need to have exceptional skills and technologies and that's where we come in to make sure you get the attention which you need\n\nSynergisticIT understands the complex nature of the job market and how difficult it can be to secure a position, especially for fresh graduates. Therefore, we assist and help tech-savvies to convert their passions into professions. We go above and beyond to keep you working in your niche.\n\nAs we focus on long-term success, we provide complete career development solutions. From job search to upskilling portfolio and interview preparation, we can guide you at every step of your career.\n\nSynergisticIT spares no efforts to connect you with a large network of tech giants, including Google, Apple, PayPal, Dell, Cisco, Client, etc. Presently, we are actively looking for ENTRY LEVEL DATA ENGINEER a driven mindset. Get the right opportunity and gain experience in building web-centric solutions on Java.\n\nWho Should Apply : Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or anyone looking to make their career in IT Industry\n\nWe also assist in filing for STEM extension and H1b and Green card filing.\n\nCandidates who are serious about their future in the IT Industry and have set big goals for themselves.\n\nCandidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. We also offer Skill Enhancement Programs if the candidates are missing skills or experience which our clients need with great outcomes\n\nCandidates can benefit from skill enhancement if they fall into the below categories. If they are qualified with enough skills then no need for skill enhancement\n\nCandidates Who Lack Experience\n\nHave had a break in careers\n\nLack Technical Competency\n\ncandidates who want to get employed and make a career in the Tech Industry \n\nPlease Also Check The Below Links\n\nhttps://www.synergisticit.com/candidate-outcomes/\n\nhttps://www.synergisticit.com/java-track/\n\nhttps://www.synergisticit.com/data-science-track/\n\nhttps://www.synergisticit.com/which-is-the-best-option-for-tech-job-seekers-staffing-companies-consulting-companies-bootcamps-or-synergisticit/\n\nhttps://www.synergisticit.com/contact-us/\n\nIf the skills are not a match candidates can opt for Skill enhancement. Or their resume can be sent out to clients to see if responses are achievable\n\nREQUIRED SKILLS For Java/Software Programmers\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Core Java , javascript , C++ or software programming\nSpring boot, Microservices and REST API's experience\nExcellent written and verbal communication skills\n\nFor data Science/Machine learning\n\nRequired Skills\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Statistics, Python, data visualization tools\nExcellent written and verbal communication skills\n\nPreferred skills: NLP, Text mining, Tableau, Time series analysis\n\nTechnical skills are required by clients for selection even if its Junior or entry level position each additional Technical skill helps a candidate's resume to be picked by clients over other job seekers.\n\nClients hire candidates with the right technical skills which they need and reject candidates who lack the required technical skills.\n\nNo third party candidates or c2c candidates\n\nPlease apply to the posting\n\nNo phone calls please. Shortlisted candidates would be reached out"}, {"title": "Data Engineer - Remote | WFH", "company_name": "Get It Recruit - Information Technology", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3782241984/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=BgBf1kSAAX%2BSuw6inJsIVw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3782241984", "description": "About the job\nJoin an innovative team at a leading company in the engineering and manufacturing industry, where we are committed to leveraging data for continuous improvement and operational excellence. This hybrid position allows you to enjoy the flexibility of remote work on Mondays and Fridays.\n\nWhy Join Us\n\nAt our company, we prioritize your well-being and professional growth. We offer a comprehensive benefits package, including excellent health benefits, a 6% 401(k) match, generous paid vacation and sick time, 12 paid holidays, and more.\n\nRole Overview\n\nAs a Data Engineer, you will play a pivotal role in transforming raw data into actionable insights, driving improvements in engineering and manufacturing processes, enhancing product quality, and optimizing operational efficiency. Collaborate with dynamic cross-functional teams to extract, analyze, and visualize data, influencing strategic decisions through your valuable insights.\n\nKey Responsibilities\n\nData Collection and Integration:\n\nGather, clean, and integrate data from diverse sources, including production databases, sensor data, and external datasets.\n\nData Analysis\n\nConduct exploratory data analysis to identify patterns, trends, and anomalies.\n\nPerform statistical analysis and hypothesis testing to derive meaningful insights.\n\nData Visualization\n\nCreate informative and visually appealing dashboards and reports using tools like Tableau, Power BI, or Python libraries.\n\nPredictive Modeling\n\nDevelop predictive models to forecast manufacturing performance, quality, and operational outcomes.\n\nRoot Cause Analysis\n\nInvestigate production issues and quality concerns by analyzing data to identify root causes and recommend corrective actions.\n\nPerformance Monitoring\n\nEstablish and maintain key performance indicators (KPIs) to track manufacturing and engineering metrics.\n\nGenerate regular reports to communicate performance trends to stakeholders.\n\nCross-functional Collaboration\n\nCollaborate with engineering, manufacturing, and quality assurance teams to understand their data needs and provide analytical support.\n\nContinuous Improvement\n\nSuggest process improvements and optimizations based on data analysis.\n\nData Security And Compliance\n\nEnsure data security and compliance with relevant regulations, such as GDPR or industry-specific standards.\n\nInfrastructure And Development\n\nBuild and maintain the infrastructure for collecting, storing, and processing data.\n\nDesign, develop, and manage data pipelines, ETL processes, and data warehouses.\n\nRequired Experience\n\nProven experience as a Sr. Data Analyst and/or Data Engineer in an engineering or manufacturing environment.\n\nProficiency in data analysis tools and programming languages, such as Python, R, SQL, and Excel.\n\nStrong data visualization skills with tools like Tableau or Power BI.\n\nExcellent knowledge of statistical analysis and machine learning techniques.\n\nAbility to communicate complex findings and insights effectively to both technical and non-technical stakeholders.\n\nStrong problem-solving skills and attention to detail.\n\nFamiliarity with manufacturing processes and quality control is a plus.\n\nExperience with enterprise ERPs (SAP S4HANA, Oracle EBS), CRMs (Salesforce, Hubspot), and IoT.\n\nProficient in working with cloud computing platforms including AWS, GCP, and Azure.\n\nExpertise in configuring and optimizing cloud-based data solutions for scalability, reliability, and performance.\n\nBachelor's Degree in Data Science, Statistics, Engineering, or Computer Science Required.\n\nApply Now and be part of a team shaping the future of data-driven innovation in engineering and manufacturing!\n\nEmployment Type: Full-Time"}, {"title": "Entry Level Software Engineer - Data Backend Engineer (Remote - Canada)", "company_name": "Yelp", "location": "United States (Remote)", "link": "/jobs/view/3752881757/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fankeoxifvhpTzOYvowHYQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3752881757", "description": "About the job\nYelp engineering culture is driven by our values: we\u2019re a cooperative team that values individual authenticity and encourages creative solutions to problems. All new engineers deploy working code their first week, and we strive to broaden individual impact with support from managers, mentors, and teams. At the end of the day, we\u2019re all about helping our users, growing as engineers, and having fun in a collaborative environment.\n Interested in exploring data and finding innovative ways to collect it, curate it, and make it easily accessible for others to use? How about doing that at an enormous scale? Yelp\u2019s datasets contain billions of interactions between users and local businesses around the globe. If working with our vast amount of data sounds exciting, come and join us!\n\nThe Marketplace Data Observability team is responsible for managing, governing, and monitoring all of Yelp\u2019s Ads data to protect our revenue stream and also support offline analysis that aids in making long term company decisions. We build tools that help us to better understand our data quality, and we strive to uncover data issues before they impact our consumers.\n\nAs a Data Backend Engineer on this team, you will be responsible for building elegant and scalable data products that serve critical, up-to-date, structured information to support different types of analytics within Yelp. As a core contributor to our growing data modeling and data warehousing engineering efforts, you will help design and own mission-critical data flow pipelines and datastores to enable decisions including effective A/B testing and company investments.\n This opportunity is fully remote and does not require you to be located in any particular area in Canada. We welcome applicants from throughout Canada. We\u2019d love to have you apply, even if you don\u2019t feel you meet every single requirement in this posting. At Yelp, we\u2019re looking for great people, not just those who simply check off all the boxes.\n  Build systems that can effectively store and crunch terabytes of data.\nDesign and develop data models for efficient data storage, retrieval, and reporting.\nCreate and maintain conceptual, logical, and physical data models using industry-standard modeling tools.\nCollaborate with cross-functional teams, including engineers, data analysts, business analysts, and data scientists, to understand data requirements and translate them into effective data models.\nParticipate in data integration efforts, including ETL processes and data migration.\nStay up-to-date with industry best practices, emerging technologies, and trends related to data warehousing.\nSupport on-call rotations as needed to operate the team.\n  Understanding of high performing and scalable data systems.\nExperience in building and orchestrating ETL pipelines.\nExperience with Data Lake or Data Warehouse landscape.\nA hunger for tracking down root causes and fixing them in systematic ways.\nAbility to communicate effectively to technical and non-technical cohorts alike. \nExposure to some of the following technologies: Python, AWS Redshift, AWS Athena / Apache Presto, Big Data technologies (e.g S3, Hadoop, Hive, Spark, Flink, Kafka etc), NoSQL systems like Cassandra, DBT."}, {"title": "Data Engineer (Associate Consultant), June 2024", "company_name": "UDig", "location": "Nashville, TN (Remote)", "link": "/jobs/view/3716039960/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=XUjo40kU55HnDCqJ%2F3FdPg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3716039960", "description": "About the job\nCan UDig It? \n\nUDig designs, builds, and implements technology solutions that deliver on business objectives. People join our team because they are excited to have an impact on the future of our organization, passionate about delivering meaningful results, and energized by solving complex problems. Oh, and they like working with other driven, smart, and authentic people!\n\nUDig is proud to be recognized as a \"Best Places to Work\" in Virginia and Tennessee. We're committed to fostering a transparent environment where leadership is accessible, professional development opportunities are abundant, and you are empowered to achieve your career goals. You'll collaborate, develop deep relationships with teammates, and accelerate your success while having fun!\n\nIf you're graduating in May 2024 and excited by the idea of impacting organizations by building data solutions to achieve business goals, you should consider joining our growing technology team.\n\nHow Things Go Down\n\nAs a new, entry-level teammate, you'll take part in our \"Breaking Ground\" training program designed to help you quickly get up-to-speed on UDig's delivery approach within a project team. The program covers:\n\nUDig delivery process and methodologies such as gathering requirements and delivering projects using agile\nTechnical guidance and instruction on the best practices from Data leaders at UDig\nDelivering executive and technical presentations\nCoaching to prepare you to for success on your first client project at UDig\n\nA typical day might entail:\n\nProject standups and check-ins\nDesign and development of data pipelines, BI reports, and more\nPlanning and participating in project demos\nContributing to internal UDig projects\nCareer and job growth through UDig-provided training\nCreating long-lasting friendships with teammates through fun activities as well as your projects (including social events, winning a prize for guessing the weight of pumpkin, or sharing experience on cool tech side projects)\n\nUDig believes it is important for Associate Consultants going through our Breaking Ground program to reside in one of our major markets (Richmond, VA or Nashville, TN metro area) for at least the first year of employment.\n\nWant to learn more? Hear directly from our teammates by checking out our 'A Day in the Life' blog series: https://www.udig.com/digging-in/breaking-ground-2023/\n\nSound Interesting? Here's the Technical Piece\n\nBelow are a few boxes you should be able to check if\u202fyou're interested: Don't check every box? Go ahead and apply!\n\n1+ year of experience with relational database management systems such as Microsoft SQL Server, MySQL, or Oracle\nUnderstanding of the full lifecycle of design, development, and implementation of data solutions including architecture design, development, testing\nSome experience in ETL development or reporting is a plus\nAbility to present technical ideas and high-level concepts and solutions to internal and external team members with varying degrees of technical knowledge\nBachelor's Degree in in computer science, information technology, or any science or business discipline\nAbility to take your work seriously but also seriously enjoy a good time with your UDig colleagues\n\nWhat's in It For You? \n\nAt UDig, you will build your career alongside talented and experienced developers while gaining experience and having fun! We provide continuous challenges and professional development opportunities. Additionally, we offer: \n\nCompetitive salary, merit reviews, and career advancement paths\nFlexible, hybrid environment\nIndividual $1500 Training Budget\nRegular team building and social activities (virtual and/or in-person)\nTransparent culture with strong communication and access to leadership\nGreat benefits like\nGenerous Paid Time Off, company holidays, and parental leave\nMultiple Single and Family Health Insurance plans to choose from\nDental & Vision coverage\nShort-Term and Long-Term disability\nOptional accident and critical illness coverage\nMatching 401(k)\nand more!"}, {"title": "Expression of Interest: Data Engineer", "company_name": "Fingerprint for Success (F4S)", "location": "New York, NY (Remote)", "link": "/jobs/view/3787769880/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=l%2FEDpQqUNmCttGdIyTk7Cg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787769880", "description": "About the job\nWe are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.\n\nThe F4S Talent Pool is a pilot project designed to:\n\nHelp job seekers get discovered by our partners based on their anticipated hiring needs.\nProvide optional support and resources for job seekers in their career endeavors.\nHelp individuals understand, and bring out the best in themselves and each other.\n\nThe F4S Talent Pool process:\n\nOnce you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.\n\nAbout Fingerprint For Success (F4S)\n\nBacked by 20+ years of research, F4S\u2019s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high performing teams.\n\nKeep in mind, joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your F4S work style assessment to match the hiring needs of our partners.\n\nYour feedback is a gift! Write to us via:\n\nPowered by JazzHR\n\noWuPJ9lwq9"}, {"title": "Data Engineer Seattle, WA (remote)", "company_name": "Conch Technologies, Inc", "location": "United States (Remote)", "link": "/jobs/view/3765052211/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=SBETI78UMutCHuRp5FnP%2FA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3765052211", "description": "About the job\nHi,\n\nGreetings from Conch Technologies Inc\n\nData Engineering Lead \u2013 With Databricks and Unity Catalog\n\nLocation \u2013 Seattle, WA (remote)\n\nDuration \u2013 12 months\n\nNeed: Design and implement data architecture and strategy, leveraging technologies such as Databricks, Unity Catalog, Privacera, and Collibra.\n\nAs a Data Engineering Lead, you will play a pivotal role in defining and executing technical deliverables while assessing complexities and levels of effort. You'll engage with both vendor and internal teams to resolve technical challenges and ensure seamless collaboration. Your leadership will extend to planning and prioritization with the SCRUM master, facilitating effective coordination between onsite and offshore teams, and driving feature prioritization through collaboration with product managers.\n\nKey Responsibilities\n\n Lead the definition and breakdown of technical deliverables, assessing complexities and levels of effort.\n Collaborate with vendor and internal teams to address technical issues.\n Drive planning and prioritization in conjunction with the SCRUM master.\n Coordinate with onsite and offshore teams for timely project delivery.\n Collaborate with product managers to prioritize features, communicate with stakeholders, and clarify requirements.\n Collect and manage user-reported issues, ensuring prompt resolution and communication.\n Define comprehensive test plans, oversee their execution, and document results.\n\n--\n\nWith Regards,\n\nNagesh G\n\nMobile: 408-381-5645\n\nDesk: 901-313-3066\n\nEmail: nagesh@conchtech.com \n\nWeb: www.conchtech.com"}, {"title": "Remote Work - Need Azure Fabric Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3759325599/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=C3iVjmcR9wjUip1nNjUiCg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3759325599", "description": "About the job\nThe client is placing heavy emphasis on Microsoft and Azure Fabric experience\n\nInformation Management Engineer \u2013 Take the identified information management sources and assist migration to the Client Data Lake, Azure Fabric. The engineer will transform information management (de-dup, clean and enrich).\n\nTake the identified data sources and bring them into our Data Lake, Azure Fabric. Once the data is in, the engineer will need to be able to transform data (de-dup, clean and enrich the data). We are focused on using a medallion data model for data architecture. The tools we are working with are: Delta Parquet files, python, spark, notebooks, T-SQL, Fabric lakehouse/warehouse, Dataflows Gen2, Data Pipelines, Direct Lake, shortcuts, ADF."}, {"title": "Data Engineer", "company_name": "Catalist", "location": "Washington, DC (Remote)", "link": "/jobs/view/3744951279/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ui1Fk6ICmnPNqAlZDZAcwA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3744951279", "description": "About the job\nFor over 17 years, Catalist has been a leader in civic data and data science innovation. Our mission is to provide progressive organizations with the data, software, and services needed to better identify, understand, and communicate with the people they need to engage and mobilize. Our clients include the largest, most influential organizations in the U.S. active in civic engagement, advocacy, and political campaigns.\n\nCatalist is home to a dedicated, creative team of technologists, data scientists, and campaign experts committed to using our talents and technology to nurture a vibrant and growing progressive community.\n\nAs a Data Engineer at Catalist, you will work closely with the Analytics and Technology departments to design, build, support, and maintain various data pipelines and processes with an end goal of providing data and intelligence to the progressive community. This role primarily involves translating data architecture designs into functional processes, code, and systems.\n\nThe ideal candidate will be a highly motivated individual with excellent technical skills, a strong desire to learn new skills, and an interest in progressive politics. Catalist values creativity and problem-solving. Our work is on the cutting edge of data-driven politics, and your support will help Democratic candidates and progressive organizations conduct successful advocacy and electoral campaigns.\n\nThis position reports to the Deputy Chief Data Officer. The Data Engineer is a part of a growing Data team that supports all underlying work at Catalist.\n\nThis position is included in our CWA bargaining unit.\n\nPrinciple Duties & Responsibilities\n\nDevelop scalable, production processes, code, and systems for data ingest, transformation, modeling, and reporting across a variety of platforms\nTranslate mock-ups and designs into functional processes, code, and systems\nCreate architecture designs when needed\nProvide quality assurance and testing on processes, code, systems, and products\nBecome an internal subject matter expert on various datasets and support other Catalist departments/teams on usage of those datasets\nExecute ad-hoc data and database maintenance tasks as requiredProject manage cross-departmental efforts, with direct responsibility for stakeholder engagement, management, and execution of technical elements\n\n\nRequirements\n\nBS or BA in a technical field, or relevant experience\n1-2 years of experience working with SQL databases\nExperience with data cleaning or ETL processes\nExperience with distributed computing systems and/or distributed datastores (particularly the Hadoop ecosystem)\nExperience managing projects\nFamiliarity with Catalist data, progressive politics, voter files, and/or commercial data\nBackground check required\n\n\nPreferred Skills & Abilities\n\nWillingness to be a problem solver and produce results in a fast paced environment\nAbility to be creative and personable, and articulate ideas clearly\nExperience working with SQL databases\nProficiency with Python or another object-oriented programming language (R, Java, Scala, etc\u2026)\nExperience working in cloud environments (AWS, GCP, etc.)\nExperience working in command line environments such as Bash\nExperience with a version control tool such as git or github\n\n\nBenefits\n\nMedical, Dental, Vision, Prescription Drug\n\nCatalist offers Medical, Dental, Vision, and Prescription Drug coverage for eligible staff and their eligible dependents. Catalist\u2019s Medical plan is a comprehensive PPO program including Prescription Drug coverage with 85% of the premium paid by Catalist. Dental and Vision coverage is provided at no cost to employees.\n\nGroup Term Life Insurance and Long-Term & Short-Term Disability Coverage\n\nGroup Term Life Insurance and Long-Term and Short-Term Disability coverage is available for eligible staff. These benefits are provided at no cost to Catalist employees.\n\n401(k) Safe Harbor Plan\n\nA 401(k) Safe Harbor Plan is available to eligible staff with a 3% contribution from Catalist from the date of hire. Employees may contribute pre-tax or post-tax from their salary up to the legal limits set forth by the IRS.\n\nMedical and Dependent Care Flexible Spending Accounts (FSAs)\n\nCatalist offers an FSA Program that gives eligible staff the ability to pay out-of-pocket medical/dental/vision/child care expenses from pre-tax earnings.\n\nTransit Benefits\n\nCatalist also makes available a Transit benefit FSA program to eligible employees using pre-tax contributions with a company match.\n\nProfessional Development and Remote Work Expenses\n\nEligible employees may be reimbursed up to $750 each year for professional development / education and remote work expenses.\n\nStudent Loan PayDown or SaveUp\n\nCatalist offers a Student Loan PayDown and College SaveUp benefit for eligible staff.\n\nVacation, Personal Leave, Sick Leave Benefits\n\nCatalist offers generous vacation benefits to all eligible staff. Eligible employees also receive:\n\n 14 Paid Holidays\n Personal Days\n Sick Leave\n Parental Leave\n\n\nHybrid Office/Remote Work\n\nCertain positions at Catalist are eligible for Office/Remote Hybrid or full Remote status."}, {"title": "Remote Entry Level Data Analyst/Engineer", "company_name": "SynergisticIT", "location": "White Plains, NY (Remote)", "link": "/jobs/view/3774171423/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=q54sk1%2FRww0z%2BC%2BE8e0jQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774171423", "description": "About the job\nAt SynergisticIT, we're all about making connections. Whatever IT goals you have, our software programmers can help achieve those. Our Software development teams can take up turnkey projects and execute them in an effective and efficient manner. If you are looking to source talent our recruiters will find the ideal IT talent for your company. What's the secret to our success? Well, it all starts with taking quality time to listen to each client's specific needs. After we have a thorough grasp of your IT goals, we can better customize our Developments as per your specific needs. We can also tailor make recruiting programs to exceed your expectations. Since our founding in 2010, SynergisticIT's strategies have earned the company an enviable position in the software development, IT staffing and IT skill enhancement fields. SynergisticIT continues to work with hundreds of satisfied American clients with our software programmers working on our projects and after gaining hands on experience on cutting edge technologies moving to contribute their skills to great clients like Apple, Google, Client, Ebay, Paypal, Kroger, the Walt Disney Company and hundreds more. If you are tired of working with inefficient programmers who take a lot of time to ramp up we want you to try us. Our software programmers can hit the ground running and get you the maximum return on your investment. You have already tried the rest its time you tried the best. SynergisticIT - Home of the Best Data Scientists and Software Programmers in the Bay Area.\n\n Why Us ? \n\nSynergisticIT has a proven track record of successfully skill enhancement and staffing IT employees for some of the world's most iconic brands. Our team takes the time to fully understand every client's needs so we could best meet your IT staffing requirements. The knowledgeable staff at SynergisticIT is always more than happy to work with clients to ensure they reach their software development goals. Besides staffing, SynergisticIT is also committed to helping young IT professionals advance their career with a robust upskill program . Everyone who goes through SynergisticIT's program learns all the skills necessary to succeed in many IT fields ranging from Java to Machine Learning. Additionally, everyone trained at SynergisticIT has been through extensive mock and technical interview screenings to bolster their career prospects. Last, but certainly not least, SynergisticIT takes great care to respect the privacy considerations for every client. All companies who work with SynergisticIT can rest assured their confidential data is protected using the most up-to-date encryption technologies. SynergisticIT also complies with all the latest NDA agreements.\n\n REQUIRED SKILLS For Java /Software Programmers \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Time series analysis \n\n We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2022 and at Gartner Data Analytics Summit (Florida)-2023 \n\nOracle CloudWorld Event (OCW) Las Vegas 2022 | SynergisticIT - YouTube\n\n https://www.youtube.com/watch?v=OAFOhcGy9Z8 \n\n https://www.youtube.com/watch?v=EmO7NrWHkLM \n\n https://www.youtube.com/watch?v=NVBU9RYZ6UI \n\n https://www.youtube.com/watch?v=Yy74yvjatVg \n\nSynergisticIT at Gartner Data and Analytics Summit 2023 - YouTube\n\nFor preparing for interviews please visit  https://www.synergisticit.com/interview-questions/ \n\n We are looking for the right matching candidates for our clients \n\n Please apply via the job posting \n\n REQUIRED SKILLS For Java /Full Stack/Software Programmer \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning Positions \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Tensorflow \n\n If you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team. \n\n No phone calls please.  Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates"}, {"title": "Data Engineer", "company_name": "Anika Systems", "location": "Leesburg, VA (Remote)", "link": "/jobs/view/3787766893/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=mApL9JnN%2B%2Fr2%2BYUPlxRYOA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787766893", "description": "About the job\nAnika Systems is a fast growing, woman-owned small business that specializes in providing innovative IT solutions for federal government agencies. Our expertise lies in accelerating delivery in Data and Analytics, Intelligent Automation, Application Development, and IT Modernization. We are currently expanding our Federal team and are seeking a passionate and talented Data Engineer. This opportunity is 100% remote.\n\nMust be a U.S. Citizen with the ability to obtain and maintain a government suitability clearance\n\nResponsibilities\n\nDesigns, develops, builds, analyzes, evaluates, and installs database management systems to include database modeling and design, relational database architecture, metadata, and repository creation and configuration management.\nDefines and oversees database organizations, standards, controls, procedures, and documentation. Provides technical consulting in the definition, design, and creation of a database environment.\nAdvises applications development staff and users on data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nDesigns and implements databases with respect to access methods, access time, batch processes, device allocation, validation checks, organization, protection and security, documentation, and statistical methods.\nUses data mapping, data mining, and data transformational analysis tools to design and develop databases.\nDetermines data storage and optimum storage requirements.\nPrepares system requirements, source analysis, and process analyses and designs throughout the database implementation.\n\nRequired Skills And Experience\n\nBA/BS and 1 year of relevant experience\nExperience with DataBricks, SQL, Python\nApplications development experience with data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nExcellent communication skills (both oral and written) with process-oriented organizational skills to ensure project success\n\nDesired Skills And Experience\n\nDemonstrated successful development experience with the full life cycle of government database management systems including database modeling and design, relational database architecture, metadata and repository creation, and configuration management.\n\nPowered by JazzHR\n\nTlKWIorucJ"}, {"title": "Remote Work - Need Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3774712978/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=r3Bp%2F0qcOD2OpC5jpbUOcA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774712978", "description": "About the job\nTitle : Data Engineer\n\nLocation : Remote\n\nDuration : 6+ months to start (long term contract (work order are extended 2x year))\n\nPlease Fill out the skill matrix: Technology Some Awareness (1) Novice (2) Intermediate (3) Advanced (4) Expert (5) SQL Tableau Desktop /Creator Tableau API Integration AWS - S3 Glue AWS - Athena Redshift AWS - Lambda AWS - SNS/SQS msExcel\n\nProfile: Data Analyst will work closely with Architects and leads in the Interflow team, key business stakeholders including program staff to support the organizations effort in the areas of data Analysis and information management. Role should have strong experience in writing SQL queries to extract data for reports. Candidate should have experience working with large set of data. As these reports are very critical for the company and we are looking for an expert who is capable of handling, consuming and extracting large sets of data for analysis and reports.\n\nMust Haves\n\nAWS - Athena\nAWS - S3\nGlue\nhands-on development of reporting applications\nMS Excel\nRedShift\nSQL\nTableau Desktop /Creator\n\nNice\n\nAWS - Lambda\nAWS - SNS/SQS\nTableau API Integration\n\nSkills\n\nBA/BS required (major in an analytical field desired) \nA minimum 10+ years work-related experience. \nExperience in hands-on development of reporting applications for the Web in a professional environment \nThe Ethos of continuous improvement and interest in learning new things.\nStrong analytical thinking and structured problem-solving ability\nAbility to handle multiple projects and assignments simultaneously and effectively in a cross-functional team environment. \nVersed on the agile methodology and best practices. \nExcellent interpersonal and collaboration skill with the ability to work with a diverse set of colleagues, across functions, from different organizations, disciplines, etc. \nSelf-starter, ability to set priorities, work independently and attain goals"}, {"title": "Data Engineer", "company_name": "Team Remotely Inc", "location": "Houston, TX (Remote)", "link": "/jobs/view/3786249829/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=78lY5Jo%2FfyRPC6TQYCdvww%3D%3D&trk=flagship3_search_srp_jobs", "id": "3786249829", "description": "About the job\nThis is a remote position.\n\n Data Engineer (US/Canada Residents Only, 1 year experience, remote)\n\nTeam Remotely Inc. is a staffing and recruitment agency that offers a comprehensive solution for talent acquisition, including sourcing, vetting, pay rolling, and managing talent. Whether you need contract staffing, direct hire, direct sourcing, talent pools, or diversity initiatives, our model can support your hiring strategy.\n\n Hiring Type: Full-Time\n\n Base Salary: $55K-$70K Per Annum.\n\nResponsibilities\n\nDesigning user interface changes for web-based DB applications. \nReviewing application requirements and interface designs. \nDeveloping and implementing highly responsive user interface components using react concepts. \nWriting application interface codes using JavaScript following react.js workflows. \nTroubleshooting interface software and debugging application codes. \nDeveloping and implementing front-end architecture to support user interface concepts. \nMonitoring and improving front-end performance. \n\nQualifications And Experience\n\nBachelor\u2019s degree in computer science, information technology, or a similar field. \nPrevious experience working as a react.js developer. \nIn-depth knowledge of JavaScript, CSS, HTML, jQuery, and front-end languages. \nKnowledge of REACT tools including React.js, Webpack, Enzyme, Redux, and Flux. \nExperience with user interface design. \nKnowledge of performance testing frameworks, including Mocha and Jest. \nExperience with browser-based debugging and performance testing software. \nTroubleshooting skills. \nProject management skills. \nProblem-solving skills. \nVerbal communication skills. \n\n Why work with Team Remotely?\n\nTeam Remotely Inc. is a staffing platform offering a seamless experience for employers and candidates. Employers can post job openings and specify their requirements, while candidates can create profiles and upload resumes.\n\nThe team of Team Remotely continuously learns and adapts based on previous successful placements, constantly improving its matching capabilities. This ensures that the recommendations provided by Team Remotely are tailored and accurate, increasing the likelihood of a successful match between employers and candidates. By providing intelligent and data-driven solutions, they strive to enhance the efficiency and effectiveness of the hiring process, ultimately helping companies find the best talent and individuals find their dream jobs."}, {"title": "Data Engineer", "company_name": "ZAG Technical Services", "location": "United States (Remote)", "link": "/jobs/view/3771403464/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=Hf0VefrKXBFIL5MtHmY0Pw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3771403464", "description": "About the job\nWe are looking for a tech savvy Data Engineer to contribute to the building, operating, and scaling of next generation data platforms and tools that will power data-driven analytics capabilities throughout the ZAG products and services portfolio, spanning areas such as business intelligence, reporting, and data analytics.\n\nThe ideal team member is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. Must be comfortable supporting the data needs of multiple clients and teams, systems, and products within the modernization of ecosystems. The right-fit person will be excited by the prospect of optimizing or even re-designing data processes to support next generation of advanced analytics capabilities, which in-turn translates to implementing strategic enhancements to maintain competitive advantage in the industry.\n\nThe successful team member will leverage their proficiency to\u2026\n\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\nBuild the analytics processes required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Data Management and cloud based \u2018big data\u2019 technologies.\nBuild analytics tools that utilize technologies to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.\nCollaborate with client teams to identify and solution regional and local requirements within the context of the standard global business model for enterprise analytics.\nParticipates in meetings, discussions, strategy sessions where changes, improvements, and enhancements are proposed, evaluated, and approved. \n\n\nThe exceptional team member will have or demonstrate progressive experiences in\u2026 \n\nBachelor\u2019s degree or higher in Computer Science or related field\nExperience working with BI and data warehousing tools\nExperience working with enterprise data, where security is paramount and data governance is critical\nAbility to work independently and in a team environment, and effectively engage all levels of the organization\nUnderstanding of distributed systems driving large-scale data processing and analytics with a successful history of manipulating, processing, and extracting value from large, disconnected datasets.\nAbility to communicate effectively (listening, presenting, and questioning)\nStrong organizational, written, and communication skills\n\n\nAbout ZAG\n\nZAG Technical Services empowers our clients\u2019 success through intelligent business solutions that increase employee productivity, network security and team collaboration. We aim to ensure that our client\u2019s IT investments drive ROI, provide a competitive advantage and help businesses grow. We do this by living our core set of values: Integrity Foremost, Client-Centered, Accountable Always, Teamwork Throughout, and Exceptional is the Goal.\n\nWhat ZAG Has To Offer\n\nCompetitive compensation package \nBenefits including medical, dental, vision, 401K and life insurance \n\n\nZAG Technical Services is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all of our employees.\n\nZAG Technical Services is an E-Verify participant and will provide the Social Security Administration (SSA) and, if necessary, the Department of Homeland Security (DHS), with information from each new employee\u2019s Form I-9 to confirm work authorization."}, {"title": "Junior Data Engineer", "company_name": "HireMeFast LLC - Career Accelerator - Land A Job", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3786249797/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D&trk=flagship3_search_srp_jobs&lipi=urn%3Ali%3Apage%3Ad_flagship3_search_srp_jobs%3BQExogrHBQoSF9zZm%2B9kufw%3D%3D&lici=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D", "id": "3786249797", "description": "About the job\nThis is a remote position.\n\n Job Title: Junior Data Engineer\n\n Employment Type: Full-Time\n\n Salary: $64,000 - $76,000 per annum\n\n Experience Required:  Minimum 1 year of project experience\n\nAbout us: HireMeFast is a leading staffing and recruitment agency specializing in connecting businesses with top-tier talent across various industries. Our mission is to bridge the gap between exceptional candidates and organizations needing their skills, expertise, and unique qualities. Our team of experienced and dedicated recruitment specialists utilizes innovative sourcing strategies, and a vast network to identify and attract top talent. We conduct comprehensive procedures to ensure that only the most qualified candidates are presented to our clients.\n\nPosition Summary\n\nJoin the fast-paced, innovative, and collaborative environment focused on providing an AIOps platform that enhances the intelligence of the CVS Health infrastructure. Work closely with subject matter experts and colleagues to build and scale out machine learning and AI solutions that will detect, predict, and recommend solutions to correct issues before system impact and enhance efficiency, reliability, and performance of CVS Health\u2019s IT operations.\n\nKey Responsibilities include:\n\nData pipeline development: Designed, implemented, and managed data pipelines for extracting, transforming, and loading data from various sources into data lakes for processing, analytics, and correlation. \nData modeling: Create and maintain data models ensuring data quality, scalability, and efficiency \nDevelop and automate processes to clean, transform, and prepare data for analytics, ensuring data accuracy and consistency \nData Integration: Integrate data from disparate sources, both structured and unstructured to provide a unified view of key infrastructure platform and application data \nUtilize big data technologies such as Kafka to process and analyze large volumes of data efficiently \nImplement data security measures to protect sensitive information and ensure compliance with data and privacy regulation \nCreate/maintain documentation for data processes, data flows, and system configurations \nPerformance Optimization- Monitor and optimize data pipelines and systems for performance, scalability and cost-effectiveness \n\nCharacteristics of this role:\n\nTeam Player: Willing to teach, share knowledge, and work with others to make the team successful. \nCommunication: Exceptional verbal, written, organizational, presentation, and communication skills. \nCreativity: Ability to take written and verbal requirements and come up with other innovative ideas. \nAttention to detail: Systematically and accurately research future solutions and current problems. \nStrong work ethic: The innate drive to do work extremely well. \nPassion: A drive to deliver better products and services than expected to customers. \n\nRequired Qualifications\n\n2+ years of programming experience in languages such as Python, Java, SQL \n2+ years of experience with ETL tools and database management (relational, non-relational) \n2+ years of experience in data modeling techniques and tools to design efficient scalable data structures \nSkills in data quality assessment, data cleansing, and data validation \n\nQualifications\n\nKnowledge of big data technologies and cloud platforms \nExperience with technologies like PySpark, Databricks, and Azure Synapse. \n\nEducation\n\nBachelor\u2019s degree in Computer Science, Information Technology, or related field, or equivalent working experience\n\nWhy HireMeFast LLC?\n\nAt HireMeFast, we understand that finding the right individuals to join your team is crucial for success and growth of your organization. We are committed to streamlining the hiring process for our clients, ensuring they have access to a diverse pool of highly qualified candidates who are a perfect fit for their specific needs."}, {"title": "Data Engineer  - Need candidates from Texas ( REMOTE )", "company_name": "Pulivarthi Group (PG)", "location": "United States (Remote)", "link": "/jobs/view/3770685212/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=9gdMwuif%2BpB%2BCQYvQi3imQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3770685212", "description": "About the job\nFollow us on Linkedin: https://www.linkedin.com/company/pulivarthigroup/\n\nPulivarthi Group LLC is a Global Staffing & IT Technology Solutions company, with our prime focus of providing world class solutions to our customers with the right talent. We combine the expertise of our team and the culture of your company to help you with the solution that is affordable and innovative using high quality standards and technologies.\n\nWe\u2019ve served some of the largest healthcare, financial services, and government entities in the U.S.\n\nData Engineer - FT only - Azure SQL with data lakes experience\n\nData Engineer -\n\nthe resource requirement for this Data Engineer role needed more clarification.\n\nOur customer is requesting a resource who knows Azure CI/CD (Continuous Integration / Continuous Deployment) to build and integrate their DevOps pipeline.\n\nOnce the DevOps pipelines are mapped, they need the Data Engineer to connect to all the Data Sources, primarily their Data Lake.\n\nThe Customer (based in Houston) is interested in a resource in a time zone not too far removed from the Central Time Zone to allow real time collaboration."}, {"title": "Data Engineer", "company_name": "Otter Products", "location": "Fort Collins, CO (Remote)", "link": "/jobs/view/3766728302/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=77U3U7CAi1PxgDmpI3%2BiNQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3766728302", "description": "About the job\nOverview \n\nOtter Products is currently recruiting for a Data Engineer to join our Data team! You can be based in our Fort Collins, CO office with flexibility to work a portion of your time remotely, or 100% Remote in the U.S.\n\nThe Data Engineer applies professional experience, concepts, and company objectives toward building and operationalizing the minimally inclusive data necessary for the enterprise data initiatives following industry standard practices and tools. The bulk of the Data Engineer's work would be in designing, managing and optimizing data pipelines, then moving these data pipelines effectively into production for key data and analytics consumers like business/data analysts, data scientists or other stakeholder that needs curated data for data and analytics use cases across the enterprise. The Data Engineer also ensures compliance with data governance and data security requirements while creating, improving and operationalizing these integrated and reusable data pipelines to enable faster data access, integrated data reuse and vastly improved time-to-solution for data and analytics initiatives. Additionally, the Data Engineer will be expected to collaborate with other data team members and data consumers working on the models and algorithms developed by them toward optimization for data quality, security, and governance to put them into production leading to potentially large productivity gains.\n\n About Otter Products \n\nOtter Products, we grow to give. From our founder's garage in 1998 to the global technology leader we are today, Otter Products continues to drive growth through innovation.\n\nThrough our industry-leading brands - OtterBox, Liviri and OtterCares - we provide our partners the number-one selling and most trusted products in our categories. Our philanthropic spirit is the foundation on which we foster our partner relationships, allowing us to grow and to give - together.\n\nBy way of our charitable arm, the OtterCares Foundation, we support our communities and invest in our future through education that inspires kids to change the world.\n\nAnd even as our global community of Otters continues to grow, our founder's core values are still at the heart of everything we do. We measure our success by our ability to give back to our communities and strengthen opportunities for all.\n\nFor more information visit otterproducts.com\n\n Responsibilities \n\nComplete analysis, design and development of BI solutions using Azure SQL Server\nFamiliarity with Data Warehouse concepts\nExperience coding the extraction, transformation, and loading (ETL) processes\nDatabase development primarily in SSIS, Data Factory and SQL\nPartner with the business to determine end user database/reporting needs and requirements\nGenerate ad hoc reports using Power BI or SSRS\nCollaborate with other developers to create and implement best approach solution(s)\nTroubleshoot Azure tools, systems, and software\nReview queries for performance issues, making changes as needed\nCollaborate with team to performance-tune Azure application as necessary\nAssist with the analysis and extraction of relevant information from large amounts of historical business data to feed data science initiatives\nParticipate and support the design and documentation of processes for large scale data analyses, model development, model validation and model implementation\nSupport and maintain a positive safety culture by following all safety policies and procedures and actively contributing to a safe working environment\nOther duties as required\n\n Qualifications \n\nBachelor's degree required. Degree in Computer Science or Mathematics preferred\nMinimum of four years of experience in an IT or analytical role required. Experience in database development, report writing and/or statistics preferred.\n\n EEO \n\nOtter Products, LLC is an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, age, marital status, pregnancy, sex, sexual orientation, gender, gender identity or expression, national origin, disability, veteran status, or any other characteristic or status protected by law.\n\n For US Based Roles Only - Compensation Range Minimum \n\nUSD $90,000.00/Yr.\n\n For US Based Roles Only - Compensation Range Maximum \n\nUSD $120,000.00/Yr.\n\n Additional Total Rewards \n\nProfit Sharing Program Eligible, Benefits Eligible - Full Time- check out otterproducts.com/careers/why for more info"}, {"title": "Software Engineer, Distributed Systems", "company_name": "Hightouch", "location": "United States (Remote)", "link": "/jobs/view/3628572539/?eBP=CwEAAAGMd4QO8APODyE_-mMOPhYyQGg5ZjVbPwq_3V7DK0B2bLAf10pcedM9S2T71X5i4TiJuEwOzWBbSiTKbLuk-VD-_Y-dN5uyf8YpYkHYqEYFxySMS9zr2hRjNrxX9gXbxssh35TFK9YTNV2WeH2Ncb675JgFbJTgNWSZySFC6rrdJGmdSDTr2q5d3VoxORz3GjsDbzjeyZQCvQ07RqYw7oujWSgslycs2LMCY_-N0NEj6e0PvbaYKiZ_5IW34bGVSYLXpHwqa8oOcNttYZ90UA3E_7sDSFzABdIVlfrp5zzERrIYy5sh2MIInxHdDoKgQw8VmOn5uM4S0tXUl-i7LnN0_W0NfPBmLiVDNuveIOm4MfDNPxfe5QKPk6FUAVpvgbcoJn9yRyE&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=rph%2FT6Q6Vkp6YoggJf9cJQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3628572539", "description": "About the job\nAbout Hightouch\n\nHightouch\u2019s mission is to empower everyone to take action on their data. Through our Reverse ETL platform, business and data users can seamlessly sync data from where it resides, such as warehouses and databases, to where it is needed, including operational systems and SaaS tools. Traditionally, acting on data has required engineering time and bandwidth, and left most business users stuck with charts and reports that are unable to take automated action on their data. With Hightouch, every business user, without writing any code, can activate data to streamline critical processes, improve marketing performance, and scale operations.\n\nOur team operates with a focus on making a meaningful impact for our customers. We believe in approaching challenges with a first principles thinking mindset, moving quickly and embracing our value of efficient execution, and treating each other with compassion and kindness. We look for team members that are strong communicators, have a growth mindset, and are motivated and persistent in achieving our goals.\n\nHundreds of companies use Hightouch, including Spotify, Ramp, Retool, NBA, Plaid, and Betterment. We\u2019re based in San Francisco, are remote-friendly, and backed by leading investors such as Amplify Partners, ICONIQ Growth, Bain Capital Ventures, Y-Combinator, and Afore Capital.\n\nAbout The Role\n\nWe are looking for a distributed systems engineer to work on the systems that power the syncing engine our customers and other engineering teams rely on. This presents an exciting challenge where you can apply your expertise in distributed systems, performance optimization, and troubleshooting to push the boundaries of what is possible and meaningful improve our ability to move massive amounts of customer data. This role also provides a unique opportunity to work on a multi-cloud and multi-region infrastructure that supports a global customer base.\n\nWe believe in enabling our engineers to do their best work for our customers by giving them extremely high levels of ownership and autonomy. This comes in different forms: you will own and deliver projects from start to finish, you will work directly with customers to solve their hardest scaling problems, and you will have a lot of influence over what we work on as a team and company.\n\nSome of the problems we\u2019ll be working on include:\n\nSync Speed: Customers want to sync a lot of data to important destinations like Facebook and Snapchat, which requires us to analyze every part of our syncing process and find where we can optimize to sync data more quickly\nStreaming Syncing: We currently sync data from sources in batch only, but envision a world where we also do syncs in real-time and support streaming sources like webhooks and queues\nScalability and Reliability: As part of our rapid growth, we\u2019re always evaluating current future bottlenecks in scaling and reliability, and architecting for the next order of magnitude of growth\nPersonalization API: Our Personalization API product provides a low latency caching layer on top of data warehouses that enable customers to use their data warehouse for real-time personalization use cases with \nMulti-Region and Multi-Cloud: Supporting our multi-region and multi-cloud backend, including extending it to launch Hightouch on in new regions to support data residency requirements of our global customer base\n\nWe are looking for talented, intellectually curious, and motivated individuals who are interested in tackling the problems above. This is a senior role, but we focus on impact and potential for growth more than years of experience. The salary range for this position is $170,000 - $240,000 USD per year, which is location independent in accordance with our remote-first policy.\n\nAbout You\n\nYou are an engineer with a passion for solving hard technical problems that generate real value for customers. You\u2019re motivated by high ownership and are comfortable in a fast-paced, startup environment.\n\nYou have experience and comfort with distributed systems and high-scale systems or the ability and desire to build this experience quickly. This experience can take different forms:\n\nYou\u2019ve built syncing engines that process and move terabytes of data per day in a scalable way\nYou\u2019ve created event collection or data streaming services that power company critical systems\nYou\u2019ve owned systems that have undergone orders of magnitude of growth and helped them scale accordingly\nYou have relevant skills and are a fast learner who is excited to take on the challenges we face\n\nInterview Process\n\nOur goal with the interview process is to balance speed with giving both parties opportunities to assess whether there is a strong mutual fit. We will ask you questions, but we want you to ask us questions! Our technical interviews focus on how you design systems because we believe this is the best way for us to see how you work and for you to see how we collaborate. We don\u2019t ask you to write code to solve technical brainteasers that don\u2019t appear in your day to day job.\n\nRecruiter screen [30m]: Introductory call with our recruiting team to get to know each other and see if the role could be a good mutual fit.\nSystems design screen [45m]: Designing a data processing feature end-to-end.\nHiring manager interview [30m]: Chat with hiring manager about past experiences and future operating preferences to assess fit on company values and operating principles.\nSystems design interview [90m]: Work with the interviewer to architect a system at a conceptual level. The problem will be at a pretty high level - and have both product and customer requirements as well as technical."}, {"title": "Entry-Level Data Scientist Engineer", "company_name": "Phoenix Recruitment LLC", "location": "Texas City, TX (Remote)", "link": "/jobs/view/3788727625/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=8iwtytHsCu4wMwJ4fj3pxw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3788727625", "description": "About the job\nThis is a remote position.\n\nTitle - Entry-Level Data Scientist Engineer, 1 year of project experience\n\nEmployment Type: Full-time\n\nBase Salary: $60K-$70K\n\nPhoenix Recruitment offers a variety of recruiting services to assist both employers and employees. They specialize in marketing open positions, recruiting, and helping employers to find qualified candidates across various industries. Phoenix Recruitment has expertise in streamlining the hiring process. They can help ensure that the process is efficient, well organized, and compliant with relevant regulations.\n\nSkills and Abilities:\n\nStrong knowledge of R or Python for data analysis and modeling. \nProficiency in statistical programs such as R, SAS, MATLAB, or Python. \nFamiliarity with spreadsheets (VBA) and database applications (Access, Oracle, SQL, or equivalent technology). \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \nAbility to learn new methods quickly and work under deadlines. \nExcellent teamwork and communication skills. \nStrong analytical and problem-solving abilities. \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \n\nPreferred:\n\nKnowledge of actuarial concepts and life, health, and/or annuity products. \nExperience with statistical modeling techniques such as GLM, Decision Trees, Time Series, Regression, etc. \nFamiliarity with Microsoft DeployR. \nExposure to insurance risk analysis. \nBasic experience in computational finance, econometrics, statistics, and math. \nKnowledge of SQL and VBA. \nFamiliarity with R or Python for predictive modeling \n\nWhy Phoenix Recruitment LLC?\n\nPhoenix Recruitment often has an extensive network of employers and candidates. This network allows them to tap into a pool of qualified candidates and connect them with suitable job opportunities. They can also leverage their connections to help employers find the right talent efficiently. Outsourcing the recruitment process to a specialized agency can save your time and resources, avoid delays, reduce administrative burdens, and increase the chances of finding the right fit for your organization."}, {"title": "Data Engineer - Remote Work", "company_name": "BairesDev", "location": "United States (Remote)", "link": "/jobs/view/3783162297/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=OtoCrAAHZFomVup04fy35g%3D%3D&trk=flagship3_search_srp_jobs", "id": "3783162297", "description": "About the job\nWho We are\n\nBairesDev is proud to be the fastest-growing company in America. With people in five continents and world-class clients, we are only as strong as the multicultural teams at the heart of our business. To consistently deliver the highest quality solutions to our clients, we only hire the Top 1% of the best talents and nurture their professional growth on exciting projects.\n\nWe are looking for a Data Engineer to join our Development team and participate in different projects made up of multicultural teams distributed throughout the world.\n\nWhat You Will Do\n\n Interview other Data Scientists and Data engineers to vet their technical abilities.\n Input data from those interviews into the product and help develop a predictive, fair, and enjoyable solution.\n Work together with developers, tech leads, and solution architects to build applications.\n Improve existing structures by adding new functionalities or proposing technological updates so that as a result, the impact of your contribution is significant in the core of each business.\n Improve the user experience on scalable and high availability platforms, contributing to the key differential of each business.\n\nHere\u2019s What We Are Looking For\n\n 5+ years of experience working as a developer.\n 3+ years of experience working as a Data Scientist / Data Engineer.\n Proficient with analysis, troubleshooting, and problem-solving.\n Hands-on experience with managing data loads and data quality.\n Advanced English level.\n\nHow we do make your work (and your life) easier:\n\n 100% remote work.\n Hardware setup for you to work from home.\n Flexible hours - make your schedule.\n Paid parental leave, vacation & holidays.\n Diverse and multicultural work environment.\n An innovative environment with the structure and resources of a leading multinational.\n Excellent compensation \u2014 well above the market average.\n Here you can grow at the speed of your learning curve.\n\nOur people work remotely but with a consistent and robust culture that promotes diversity and teamwork. To continue being the leading software development company in Latin America, we want to ensure that every BairesDev member gets the best growth and professional development opportunities in a diverse, welcoming, and innovative environment.\n\nEvery BairesDev team member brings something unique to our company.\n\nWe want to hear your story. Apply now!"}, {"title": "Data Engineer (Remote)", "company_name": "Cascade Debt", "location": "United States (Remote)", "link": "/jobs/view/3755336571/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=pmUtn1j2zyPr25CNPh2RNw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3755336571", "description": "About the job\nDo you want to work in a small team where you can make a real difference in a company? Would you embrace the challenge of building a high impact platform that is used globally? Then we want to speak with you!\n\nAbout Cascade\n\nCascade is a fintech startup backed by Canadian and US investors that empowers companies to grow by democratizing access to institutional debt. We are building tools that modernize how companies raise and manage debt, lowering the barriers to entry to the $7 trillion specialty finance and alternative credit market for companies around the world. Founded just last year, we are gearing up for our next phase of technical development and are seeking a talented Data Engineer to lead the way.\n\nAbout The Role\n\nAs a Data Engineer you\u2019ll be an early member of a growing team building a pioneering platform in the debt infrastructure space. You will design, build, and launch efficient, scalable, and reliable data pipelines to move and transform data .\n\nWhat You\u2019ll Do\n\n Building databases, data lakes and data ingestion pipelines to integrate customer databases and datasets to our systems \n Ingesting financial datasets from external customers, then u pdating and maintaining accurate and complete data mappings to ensure that our product s are displaying high quality data \n M onitoring and alerting across our data pipelines in order to make sure that our data ingests are reliable and correct \n Translat ing business goals and stakeholder requirements into new data flows and deliver analytical insights ensuring that the data flows create real business value continuously \n\nSkills And Experience\n\n 2 + years experience in a data engineering or data analyst role \n E xperience with relational SQL and NoSQL databases, including PostgreSQL , MySQL, MariaDB, MongoDB, Bigtable, etc. \n E xperience working with ETL technologies , such as Databricks, Fivetran , or dbt \n Proficiency in writing SQL queries and knowledge of analytical data warehouses such as RedShift, BigQuery , and Snowflake \n Some experience with CI/CD automation such as GitHub Actions \n Developing APIs and integrating with 3rd party APIs \n Bonus: experience in fintech / SaaS / credit analysis \n\nPeople Who Thrive At Cascade Are\n\n Self-starters, who take the initiative to tackle challenges in a remote work environment \n Good communicators, who can operate without ego to discuss, learn, grow, and help others do the same \n Passionate about creating great digital experiences for users \n Problem solvers who are not satisfied with the status quo \n\nBenefits\n\nRemote: we are a remote-first company and are very flexible on hours as long as things get done \nHome-office: there\u2019s a $1000 USD home office allowance to set yourself up \nEquity: we expect you to have an owner-mentality, and have the equity plan to match \nBenefits: health, dental, vision, and more \nPerks: we offer free lunches weekly and off-site trips \nJob satisfaction: we offer autonomy, ample opportunities for mastery, and an opportunity to make a difference for companies around the world"}, {"title": "Data Engineer", "company_name": "ISC (Integrated Specialty Coverages, LLC)", "location": "Carlsbad, CA (Remote)", "link": "/jobs/view/3778293528/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ew7%2FfEuXSujs5IGabdqUyw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3778293528", "description": "About the job\nAbout Integrated Specialty Coverages\n\nIntegrated Specialty Coverages LLC (ISC) is a growth stage technology and data-driven insurance company leading innovation in the market.\n\nBacked by one of the leading private equity firms, KKR, and led by a forward-thinking management team, ISC is combining the worlds of insurance and technology to create an Insurtech powerhouse. As a leading online distributor of insurance products for a range of industries and \"Main Street USA\", we are looking for the right people to help us in our mission of achieving exponential growth. We strive to be the number one place to go for brokers and agents to source insurance. To accomplish this, we're building a digitally focused team that deeply understands the intersection between user experience, data, and AI/ML to optimize the way we engage with our customers and partners.\n\nJob Summary\n\nThe Data Engineer role will be the technical liaison between multiple groups including a data science team, the engineering team, product management, and business stakeholders. You do not need any insurance knowledge prior, however, you must quickly dive deep into the insurance world and ask questions to become a subject matter expert. You will be responsible for building a data platform to facilitate the data science team. You must be a self-starter that can build out features such as a data pipeline from scratch. There will be support from both engineering and data science for any buildout.\n\nJob Responsibilities\n\nCreate and maintain optimal data pipeline architecture. \nAssemble large, complex data sets that meet functional / non-functional business requirements. \nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. \nBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using AWS technologies, SQL, and Python. \nWork with stakeholders including the Executive, Product, Data Science, and Engineering teams to assist with data-related technical issues and support their data infrastructure needs. \nWork with data science and analytics teams to strive for greater functionality in our data systems. \n\nMinimum Qualifications:\n\nAdvanced working SQL knowledge and experience working with relational databases, strong query authoring (SQL) as well as working familiarity with a variety of databases (Snowflake, Redshift, MySQL, MSSQL, etc.)\nExperience building and optimizing data pipelines, architecture and data sets. \nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. \nBuild processes supporting data transformation, data structures, metadata, dependency, and workload management. \nA successful history of transforming, processing and extracting value from large disconnected, datasets from a variety of data sources (Flat files, Excel, databases, etc.)\nStrong analytic skills related to working with unstructured datasets. \nStrong project management and organizational skills. \nExperience supporting and working with cross-functional teams in a dynamic environment. \nAbility to mentor/guide/collaborate with other team members. \n\nWe are looking for a candidate with 3-5 years of experience in a Data Engineer role. They should also have experience using the following software/tools:\n\nExperience with relational and MPP databases, including Snowflake and MySQL. \nExperience developing software in an agile environment from the requirements stage to production. \nExperience with version control: git\nExperience with container technologies: Docker\nExperience with data pipeline and workflow management tools: Airflow, Jenkins, AWS Glue, Azkaban, Luigi, etc. \nExperience with AWS cloud services: EC2, ECS, Batch, S3, EMR, RDS, Redshift\nExperience with other cloud services: Snowflake, Airflow\nExperience with object-oriented/object function scripting languages: Python, Java, C++, etc. \nExperience with data modeling and data warehouse design\nExperience with data visualization tools (PowerBI, QuickSight)\n\nThe starting annual pay scale for this position is listed below. Actual starting pay will be based on factors such as skills, qualifications, training, and experience. In addition, the company offers comprehensive benefits including medical, dental and vision insurance, 401(k) plan with match, paid time off, and other benefits.\n\nISC's salary ranges are determined by role and level. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations and could be higher or lower based on a multitude of factors, including job-related skills, experience, and relevant education or training.\n\nNational Pay Range\n\n$100,000\u2014$110,000 USD\n\nBenefits of Working at ISC\n\nCompetitive vacation and flexible working arrangements\nComprehensive and inclusive health benefits\nA variety of professional development and mentorship opportunities\nChoice of technology whether at home or in the office\n\nISC's Ownership Behaviors\n\nDo the Right Thing *Be Relentless. Pursue Excellence. *Personal Responsibility & Own Outcomes\nTry Fast. Learn Fast. Fail Fast. Think Big. *Build Extraordinary Owners & Join Forces *Grit & Determination\n\nApplicants with disabilities may contact ISC HR department via e-mail, and other means to request and arrange accommodation. If you need assistance to accommodate a disability, you may request accommodation at any time. Please contact ISC at HR@ISCMGA.com.\n\nISC believes in creating long-term relationships by being responsive and relevant and by consistently delivering value to our community of customers. Specifically, with our employees, we focus on attracting, developing, and retaining the best talent for our business, challenging our people, demonstrating a \"can-do\" attitude, and fostering a collaborative and mutually supportive environment. ISC is an equal opportunity employer and a participant in the US federal E-Verify program (US).\n\nDiversity creates a healthier atmosphere: All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, gender, gender identity, sexual orientation, marital status, medical condition, genetic information, mental or physical disability, military or veteran status, or any other characteristic protected by local, state, or Federal law."}, {"title": "Data Engineer -Remote", "company_name": "RIT Solutions, Inc.", "location": "Denver, CO (Remote)", "link": "/jobs/view/3768007496/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=1hQH8EDaII1eJ7nQ9BACQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3768007496", "description": "About the job\nDuration: 6+ months CTH\n\nMust have a valid LinkedIn profile\n\nRequired Skills\n\n 3-6 years of experience (no more than 6 years)\n Start up environment experience\n Python, SQL, and DBT"}, {"title": "Big Data Engineer - W2", "company_name": "Megan Soft Inc", "location": "United States (Remote)", "link": "/jobs/view/3674845652/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fCAxh4N%2F4YZJ5aEIASaeeA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3674845652", "description": "About the job\nTitle: Big Data Engineer - W2\n\nLocation: Dearborn, MI (Remote)\n\nDuration: 12+ Months\n\nDirect Client: Ford Motors\n\nMust have skills: Should be able to handle GCP requirements\n\n\"Translates business requirements to build repeatable, sustainable, efficient, coded processes that can be productionized by Software Engineers and readily modified by other Data Engineer Creates POC processes in Dev/QA and works with Software Engineers to productionize and align best coding and process practices that maximize efficiency, speed, stability, system resources and capabilities Awareness of and compliance with data privacy, security, legal and contractual guidelines Incorporates data quality and privacy checks/alerts to minimize bad data being consumed by end users, models and dashboards, and to protect customer data; revises checks for new data issues Validates data products and pipelines are functioning as expected following system or application upgrades, source changes, etc. Works closely with Architects if required to align systems, tools and applications being utilized with business use case and performance requirements Provides data product support and maintenance Establishes, tracks and monitors KPIs related to specific data products and deliverable Participate in iteration cadence activities (Iteration Planning, task estimation, daily scrum, demos, etc.). Will be a part of on-call team to resolve critical production support tickets during business hours and off hours.\n\nSkills Required\n\nExperience with Big Data Technologies (Hadoop , ETL Tool, Spark. Hive etc..) Someone who understands Cloud and has experience working with Cloud Platforms (preferably GCP) Should be able to handle GCP requirements - ingesting into GCS/ Big Query, working with GCP components Hands-on in Data Flow, Cloud Storage (GCS), Big Query and GitHub. Proficient in Python and SQL.\n\nSkills Preferred\n\nManages ETL: uses programming and tools for data ingestion, configures pipelines, applies transformations and decoding, integrates and fuses data, moves and securely deliver Experience NoSQL databases\n\nAdditional Information\n\nIf the candidate is remote only, please indicate \"\"100% Remote\"\" under candidate's name on resume. Other similar schedule notations can be \"\"Local Candidate\"\" or \"\"Hybrid Candidate.\"\" ***HRA Test Required***\""}, {"title": "Remote Opportunity: Quantexa Data Engineer", "company_name": "SPAR Information Systems LLC", "location": "United States (Remote)", "link": "/jobs/view/3680073191/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ioykVCZSOQ7BK5%2FwRQHOWg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3680073191", "description": "About the job\nHello All, Hope you are doing great!! Please go through the job descriptions and let me know your interest. Role: Quantexa Data engineer  Location: Remote Duration: Long Term Contract Job Description: 1. End to End usage of Quantexa including guiding our client team on how to use from both front end and data ingestion Reason they are asking for Certification 2. Resolve Entity issues with data 3. Knowledge of Scala and Hive which requires coding experience using parquet 4. Advanced SQL knowledge requiring writing SQL queries 5. Experience having worked with GCP data lake 6. Quantexa certification required\n\nThanks & Regards,\n\nSatnam Singh\n\nDirect: 201 623 3660\n\nEmail : Satnam.singh@sparinfosys.com"}, {"title": "Remote Work - Need Staff Data Engineer - USC only", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3746283980/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=CvIuLBNO%2BPMGi4IXVRkP0A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3746283980", "description": "About the job\n100% Remote, client is in NY\n\nLook for strong job history, Python, Machine Learning, SQL, 10/10 comms, etc. \n\nMUST be an Engineer \n\nSummary\n\nWe're looking for an experienced Data Engineer to join our client's growing team of top notched IT professionals. You will work closely across multiple groups including Editorial and Audio, Marketing, Advertising, etc. to achieve their goals and objectives. The Data Engineering team builds tools throughout the stack while sharing knowledge across these departments.\n\nAs the company grows, they're looking for Data Engineer who will help solidify and expand our pipelines and maintain their data warehouse. Their business model is based on performing complex and very detailed analyses of how our products perform with their subscribers. You'll be responsible for working alongside analysts, product, marketing, and other teams to define new data collection components and measurement schemes that support each new product and feature.\n\nHow Can Add Value To Their Mission\n\nCreate and maintain data pipelines to provide insights and drive business decisions\nEstablish data warehousing strategy (ex. Kimball, Data Vault, etc.)\nMaintain data infrastructure on our AWS accounts\nCollaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.\nWrite unit/integration tests, contributes to engineering wiki, and documents work\nImplement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it\n\nWhat You'll Need To Succeed\n\n7+ years of industry experience measuring product performance and user behavior\nExperience working with a variety of data management technologies, including RedShift, Kinesis/Kafka, Glue, Spark, Postgres, Airflow, dbt, and others.\nExperience implementing BI reporting tools such as Looker\nExperience interfacing with engineers, product managers and analysts to understand data needs\nKnowledge of a variety of measurement beacons, SDKs, APIs including Google Analytics, Amplitude, Braze, Stripe, email service providers\nA commitment to building secure, resilient, fault-tolerant architectures, with clear documentation and procedures in place for support\nA focus on accuracy and detail as you build, ensuring data pipelines produce clear, predictable results\nUnderstanding of the typical metrics a subscription and advertising-supported business needs to measure success\nExperience with Client techniques as applied to behavioral segmentation or anomaly identification is a definite plus\nFamiliarity using developer tools that increase productivity and facilitate the development of resilient code (eg. Docker, CircleCI, Serverless) is a plus"}, {"title": "ENTRY LEVEL DATA ENGINEER (REMOTE)", "company_name": "SynergisticIT", "location": "Jacksonville, FL (Remote)", "link": "/jobs/view/3767587909/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=om7L3CnowiC4PLIcpZD70A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3767587909", "description": "About the job\nSYNERGISTICIT wants every candidate to know that the Job Market is Challenging and to stand out, you need to have exceptional skills and technologies and that's where we come in to make sure you get the attention which you need\n\nSynergisticIT understands the complex nature of the job market and how difficult it can be to secure a position, especially for fresh graduates. Therefore, we assist and help tech-savvies to convert their passions into professions. We go above and beyond to keep you working in your niche.\n\nAs we focus on long-term success, we provide complete career development solutions. From job search to upskilling portfolio and interview preparation, we can guide you at every step of your career.\n\nSynergisticIT spares no efforts to connect you with a large network of tech giants, including Google, Apple, PayPal, Dell, Cisco, Client, etc. Presently, we are actively looking for ENTRY LEVEL DATA ENGINEER a driven mindset. Get the right opportunity and gain experience in building web-centric solutions on Java.\n\nWho Should Apply : Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or anyone looking to make their career in IT Industry\n\nWe also assist in filing for STEM extension and H1b and Green card filing.\n\nCandidates who are serious about their future in the IT Industry and have set big goals for themselves.\n\nCandidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. We also offer Skill Enhancement Programs if the candidates are missing skills or experience which our clients need with great outcomes\n\nCandidates can benefit from skill enhancement if they fall into the below categories. If they are qualified with enough skills then no need for skill enhancement\n\nCandidates Who Lack Experience\n\nHave had a break in careers\n\nLack Technical Competency\n\ncandidates who want to get employed and make a career in the Tech Industry \n\nPlease Also Check The Below Links\n\nhttps://www.synergisticit.com/candidate-outcomes/\n\nhttps://www.synergisticit.com/java-track/\n\nhttps://www.synergisticit.com/data-science-track/\n\nhttps://www.synergisticit.com/which-is-the-best-option-for-tech-job-seekers-staffing-companies-consulting-companies-bootcamps-or-synergisticit/\n\nhttps://www.synergisticit.com/contact-us/\n\nIf the skills are not a match candidates can opt for Skill enhancement. Or their resume can be sent out to clients to see if responses are achievable\n\nREQUIRED SKILLS For Java/Software Programmers\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Core Java , javascript , C++ or software programming\nSpring boot, Microservices and REST API's experience\nExcellent written and verbal communication skills\n\nFor data Science/Machine learning\n\nRequired Skills\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Statistics, Python, data visualization tools\nExcellent written and verbal communication skills\n\nPreferred skills: NLP, Text mining, Tableau, Time series analysis\n\nTechnical skills are required by clients for selection even if its Junior or entry level position each additional Technical skill helps a candidate's resume to be picked by clients over other job seekers.\n\nClients hire candidates with the right technical skills which they need and reject candidates who lack the required technical skills.\n\nNo third party candidates or c2c candidates\n\nPlease apply to the posting\n\nNo phone calls please. Shortlisted candidates would be reached out"}, {"title": "Data Engineer - Remote | WFH", "company_name": "Get It Recruit - Information Technology", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3782241984/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=BgBf1kSAAX%2BSuw6inJsIVw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3782241984", "description": "About the job\nJoin an innovative team at a leading company in the engineering and manufacturing industry, where we are committed to leveraging data for continuous improvement and operational excellence. This hybrid position allows you to enjoy the flexibility of remote work on Mondays and Fridays.\n\nWhy Join Us\n\nAt our company, we prioritize your well-being and professional growth. We offer a comprehensive benefits package, including excellent health benefits, a 6% 401(k) match, generous paid vacation and sick time, 12 paid holidays, and more.\n\nRole Overview\n\nAs a Data Engineer, you will play a pivotal role in transforming raw data into actionable insights, driving improvements in engineering and manufacturing processes, enhancing product quality, and optimizing operational efficiency. Collaborate with dynamic cross-functional teams to extract, analyze, and visualize data, influencing strategic decisions through your valuable insights.\n\nKey Responsibilities\n\nData Collection and Integration:\n\nGather, clean, and integrate data from diverse sources, including production databases, sensor data, and external datasets.\n\nData Analysis\n\nConduct exploratory data analysis to identify patterns, trends, and anomalies.\n\nPerform statistical analysis and hypothesis testing to derive meaningful insights.\n\nData Visualization\n\nCreate informative and visually appealing dashboards and reports using tools like Tableau, Power BI, or Python libraries.\n\nPredictive Modeling\n\nDevelop predictive models to forecast manufacturing performance, quality, and operational outcomes.\n\nRoot Cause Analysis\n\nInvestigate production issues and quality concerns by analyzing data to identify root causes and recommend corrective actions.\n\nPerformance Monitoring\n\nEstablish and maintain key performance indicators (KPIs) to track manufacturing and engineering metrics.\n\nGenerate regular reports to communicate performance trends to stakeholders.\n\nCross-functional Collaboration\n\nCollaborate with engineering, manufacturing, and quality assurance teams to understand their data needs and provide analytical support.\n\nContinuous Improvement\n\nSuggest process improvements and optimizations based on data analysis.\n\nData Security And Compliance\n\nEnsure data security and compliance with relevant regulations, such as GDPR or industry-specific standards.\n\nInfrastructure And Development\n\nBuild and maintain the infrastructure for collecting, storing, and processing data.\n\nDesign, develop, and manage data pipelines, ETL processes, and data warehouses.\n\nRequired Experience\n\nProven experience as a Sr. Data Analyst and/or Data Engineer in an engineering or manufacturing environment.\n\nProficiency in data analysis tools and programming languages, such as Python, R, SQL, and Excel.\n\nStrong data visualization skills with tools like Tableau or Power BI.\n\nExcellent knowledge of statistical analysis and machine learning techniques.\n\nAbility to communicate complex findings and insights effectively to both technical and non-technical stakeholders.\n\nStrong problem-solving skills and attention to detail.\n\nFamiliarity with manufacturing processes and quality control is a plus.\n\nExperience with enterprise ERPs (SAP S4HANA, Oracle EBS), CRMs (Salesforce, Hubspot), and IoT.\n\nProficient in working with cloud computing platforms including AWS, GCP, and Azure.\n\nExpertise in configuring and optimizing cloud-based data solutions for scalability, reliability, and performance.\n\nBachelor's Degree in Data Science, Statistics, Engineering, or Computer Science Required.\n\nApply Now and be part of a team shaping the future of data-driven innovation in engineering and manufacturing!\n\nEmployment Type: Full-Time"}, {"title": "Entry Level Software Engineer - Data Backend Engineer (Remote - Canada)", "company_name": "Yelp", "location": "United States (Remote)", "link": "/jobs/view/3752881757/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fankeoxifvhpTzOYvowHYQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3752881757", "description": "About the job\nYelp engineering culture is driven by our values: we\u2019re a cooperative team that values individual authenticity and encourages creative solutions to problems. All new engineers deploy working code their first week, and we strive to broaden individual impact with support from managers, mentors, and teams. At the end of the day, we\u2019re all about helping our users, growing as engineers, and having fun in a collaborative environment.\n Interested in exploring data and finding innovative ways to collect it, curate it, and make it easily accessible for others to use? How about doing that at an enormous scale? Yelp\u2019s datasets contain billions of interactions between users and local businesses around the globe. If working with our vast amount of data sounds exciting, come and join us!\n\nThe Marketplace Data Observability team is responsible for managing, governing, and monitoring all of Yelp\u2019s Ads data to protect our revenue stream and also support offline analysis that aids in making long term company decisions. We build tools that help us to better understand our data quality, and we strive to uncover data issues before they impact our consumers.\n\nAs a Data Backend Engineer on this team, you will be responsible for building elegant and scalable data products that serve critical, up-to-date, structured information to support different types of analytics within Yelp. As a core contributor to our growing data modeling and data warehousing engineering efforts, you will help design and own mission-critical data flow pipelines and datastores to enable decisions including effective A/B testing and company investments.\n This opportunity is fully remote and does not require you to be located in any particular area in Canada. We welcome applicants from throughout Canada. We\u2019d love to have you apply, even if you don\u2019t feel you meet every single requirement in this posting. At Yelp, we\u2019re looking for great people, not just those who simply check off all the boxes.\n  Build systems that can effectively store and crunch terabytes of data.\nDesign and develop data models for efficient data storage, retrieval, and reporting.\nCreate and maintain conceptual, logical, and physical data models using industry-standard modeling tools.\nCollaborate with cross-functional teams, including engineers, data analysts, business analysts, and data scientists, to understand data requirements and translate them into effective data models.\nParticipate in data integration efforts, including ETL processes and data migration.\nStay up-to-date with industry best practices, emerging technologies, and trends related to data warehousing.\nSupport on-call rotations as needed to operate the team.\n  Understanding of high performing and scalable data systems.\nExperience in building and orchestrating ETL pipelines.\nExperience with Data Lake or Data Warehouse landscape.\nA hunger for tracking down root causes and fixing them in systematic ways.\nAbility to communicate effectively to technical and non-technical cohorts alike. \nExposure to some of the following technologies: Python, AWS Redshift, AWS Athena / Apache Presto, Big Data technologies (e.g S3, Hadoop, Hive, Spark, Flink, Kafka etc), NoSQL systems like Cassandra, DBT."}, {"title": "Data Engineer (Associate Consultant), June 2024", "company_name": "UDig", "location": "Nashville, TN (Remote)", "link": "/jobs/view/3716039960/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=XUjo40kU55HnDCqJ%2F3FdPg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3716039960", "description": "About the job\nCan UDig It? \n\nUDig designs, builds, and implements technology solutions that deliver on business objectives. People join our team because they are excited to have an impact on the future of our organization, passionate about delivering meaningful results, and energized by solving complex problems. Oh, and they like working with other driven, smart, and authentic people!\n\nUDig is proud to be recognized as a \"Best Places to Work\" in Virginia and Tennessee. We're committed to fostering a transparent environment where leadership is accessible, professional development opportunities are abundant, and you are empowered to achieve your career goals. You'll collaborate, develop deep relationships with teammates, and accelerate your success while having fun!\n\nIf you're graduating in May 2024 and excited by the idea of impacting organizations by building data solutions to achieve business goals, you should consider joining our growing technology team.\n\nHow Things Go Down\n\nAs a new, entry-level teammate, you'll take part in our \"Breaking Ground\" training program designed to help you quickly get up-to-speed on UDig's delivery approach within a project team. The program covers:\n\nUDig delivery process and methodologies such as gathering requirements and delivering projects using agile\nTechnical guidance and instruction on the best practices from Data leaders at UDig\nDelivering executive and technical presentations\nCoaching to prepare you to for success on your first client project at UDig\n\nA typical day might entail:\n\nProject standups and check-ins\nDesign and development of data pipelines, BI reports, and more\nPlanning and participating in project demos\nContributing to internal UDig projects\nCareer and job growth through UDig-provided training\nCreating long-lasting friendships with teammates through fun activities as well as your projects (including social events, winning a prize for guessing the weight of pumpkin, or sharing experience on cool tech side projects)\n\nUDig believes it is important for Associate Consultants going through our Breaking Ground program to reside in one of our major markets (Richmond, VA or Nashville, TN metro area) for at least the first year of employment.\n\nWant to learn more? Hear directly from our teammates by checking out our 'A Day in the Life' blog series: https://www.udig.com/digging-in/breaking-ground-2023/\n\nSound Interesting? Here's the Technical Piece\n\nBelow are a few boxes you should be able to check if\u202fyou're interested: Don't check every box? Go ahead and apply!\n\n1+ year of experience with relational database management systems such as Microsoft SQL Server, MySQL, or Oracle\nUnderstanding of the full lifecycle of design, development, and implementation of data solutions including architecture design, development, testing\nSome experience in ETL development or reporting is a plus\nAbility to present technical ideas and high-level concepts and solutions to internal and external team members with varying degrees of technical knowledge\nBachelor's Degree in in computer science, information technology, or any science or business discipline\nAbility to take your work seriously but also seriously enjoy a good time with your UDig colleagues\n\nWhat's in It For You? \n\nAt UDig, you will build your career alongside talented and experienced developers while gaining experience and having fun! We provide continuous challenges and professional development opportunities. Additionally, we offer: \n\nCompetitive salary, merit reviews, and career advancement paths\nFlexible, hybrid environment\nIndividual $1500 Training Budget\nRegular team building and social activities (virtual and/or in-person)\nTransparent culture with strong communication and access to leadership\nGreat benefits like\nGenerous Paid Time Off, company holidays, and parental leave\nMultiple Single and Family Health Insurance plans to choose from\nDental & Vision coverage\nShort-Term and Long-Term disability\nOptional accident and critical illness coverage\nMatching 401(k)\nand more!"}, {"title": "Expression of Interest: Data Engineer", "company_name": "Fingerprint for Success (F4S)", "location": "New York, NY (Remote)", "link": "/jobs/view/3787769880/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=l%2FEDpQqUNmCttGdIyTk7Cg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787769880", "description": "About the job\nWe are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.\n\nThe F4S Talent Pool is a pilot project designed to:\n\nHelp job seekers get discovered by our partners based on their anticipated hiring needs.\nProvide optional support and resources for job seekers in their career endeavors.\nHelp individuals understand, and bring out the best in themselves and each other.\n\nThe F4S Talent Pool process:\n\nOnce you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.\n\nAbout Fingerprint For Success (F4S)\n\nBacked by 20+ years of research, F4S\u2019s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high performing teams.\n\nKeep in mind, joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your F4S work style assessment to match the hiring needs of our partners.\n\nYour feedback is a gift! Write to us via:\n\nPowered by JazzHR\n\noWuPJ9lwq9"}, {"title": "Data Engineer Seattle, WA (remote)", "company_name": "Conch Technologies, Inc", "location": "United States (Remote)", "link": "/jobs/view/3765052211/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=SBETI78UMutCHuRp5FnP%2FA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3765052211", "description": "About the job\nHi,\n\nGreetings from Conch Technologies Inc\n\nData Engineering Lead \u2013 With Databricks and Unity Catalog\n\nLocation \u2013 Seattle, WA (remote)\n\nDuration \u2013 12 months\n\nNeed: Design and implement data architecture and strategy, leveraging technologies such as Databricks, Unity Catalog, Privacera, and Collibra.\n\nAs a Data Engineering Lead, you will play a pivotal role in defining and executing technical deliverables while assessing complexities and levels of effort. You'll engage with both vendor and internal teams to resolve technical challenges and ensure seamless collaboration. Your leadership will extend to planning and prioritization with the SCRUM master, facilitating effective coordination between onsite and offshore teams, and driving feature prioritization through collaboration with product managers.\n\nKey Responsibilities\n\n Lead the definition and breakdown of technical deliverables, assessing complexities and levels of effort.\n Collaborate with vendor and internal teams to address technical issues.\n Drive planning and prioritization in conjunction with the SCRUM master.\n Coordinate with onsite and offshore teams for timely project delivery.\n Collaborate with product managers to prioritize features, communicate with stakeholders, and clarify requirements.\n Collect and manage user-reported issues, ensuring prompt resolution and communication.\n Define comprehensive test plans, oversee their execution, and document results.\n\n--\n\nWith Regards,\n\nNagesh G\n\nMobile: 408-381-5645\n\nDesk: 901-313-3066\n\nEmail: nagesh@conchtech.com \n\nWeb: www.conchtech.com"}, {"title": "Remote Work - Need Azure Fabric Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3759325599/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=C3iVjmcR9wjUip1nNjUiCg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3759325599", "description": "About the job\nThe client is placing heavy emphasis on Microsoft and Azure Fabric experience\n\nInformation Management Engineer \u2013 Take the identified information management sources and assist migration to the Client Data Lake, Azure Fabric. The engineer will transform information management (de-dup, clean and enrich).\n\nTake the identified data sources and bring them into our Data Lake, Azure Fabric. Once the data is in, the engineer will need to be able to transform data (de-dup, clean and enrich the data). We are focused on using a medallion data model for data architecture. The tools we are working with are: Delta Parquet files, python, spark, notebooks, T-SQL, Fabric lakehouse/warehouse, Dataflows Gen2, Data Pipelines, Direct Lake, shortcuts, ADF."}, {"title": "Data Engineer", "company_name": "Catalist", "location": "Washington, DC (Remote)", "link": "/jobs/view/3744951279/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ui1Fk6ICmnPNqAlZDZAcwA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3744951279", "description": "About the job\nFor over 17 years, Catalist has been a leader in civic data and data science innovation. Our mission is to provide progressive organizations with the data, software, and services needed to better identify, understand, and communicate with the people they need to engage and mobilize. Our clients include the largest, most influential organizations in the U.S. active in civic engagement, advocacy, and political campaigns.\n\nCatalist is home to a dedicated, creative team of technologists, data scientists, and campaign experts committed to using our talents and technology to nurture a vibrant and growing progressive community.\n\nAs a Data Engineer at Catalist, you will work closely with the Analytics and Technology departments to design, build, support, and maintain various data pipelines and processes with an end goal of providing data and intelligence to the progressive community. This role primarily involves translating data architecture designs into functional processes, code, and systems.\n\nThe ideal candidate will be a highly motivated individual with excellent technical skills, a strong desire to learn new skills, and an interest in progressive politics. Catalist values creativity and problem-solving. Our work is on the cutting edge of data-driven politics, and your support will help Democratic candidates and progressive organizations conduct successful advocacy and electoral campaigns.\n\nThis position reports to the Deputy Chief Data Officer. The Data Engineer is a part of a growing Data team that supports all underlying work at Catalist.\n\nThis position is included in our CWA bargaining unit.\n\nPrinciple Duties & Responsibilities\n\nDevelop scalable, production processes, code, and systems for data ingest, transformation, modeling, and reporting across a variety of platforms\nTranslate mock-ups and designs into functional processes, code, and systems\nCreate architecture designs when needed\nProvide quality assurance and testing on processes, code, systems, and products\nBecome an internal subject matter expert on various datasets and support other Catalist departments/teams on usage of those datasets\nExecute ad-hoc data and database maintenance tasks as requiredProject manage cross-departmental efforts, with direct responsibility for stakeholder engagement, management, and execution of technical elements\n\n\nRequirements\n\nBS or BA in a technical field, or relevant experience\n1-2 years of experience working with SQL databases\nExperience with data cleaning or ETL processes\nExperience with distributed computing systems and/or distributed datastores (particularly the Hadoop ecosystem)\nExperience managing projects\nFamiliarity with Catalist data, progressive politics, voter files, and/or commercial data\nBackground check required\n\n\nPreferred Skills & Abilities\n\nWillingness to be a problem solver and produce results in a fast paced environment\nAbility to be creative and personable, and articulate ideas clearly\nExperience working with SQL databases\nProficiency with Python or another object-oriented programming language (R, Java, Scala, etc\u2026)\nExperience working in cloud environments (AWS, GCP, etc.)\nExperience working in command line environments such as Bash\nExperience with a version control tool such as git or github\n\n\nBenefits\n\nMedical, Dental, Vision, Prescription Drug\n\nCatalist offers Medical, Dental, Vision, and Prescription Drug coverage for eligible staff and their eligible dependents. Catalist\u2019s Medical plan is a comprehensive PPO program including Prescription Drug coverage with 85% of the premium paid by Catalist. Dental and Vision coverage is provided at no cost to employees.\n\nGroup Term Life Insurance and Long-Term & Short-Term Disability Coverage\n\nGroup Term Life Insurance and Long-Term and Short-Term Disability coverage is available for eligible staff. These benefits are provided at no cost to Catalist employees.\n\n401(k) Safe Harbor Plan\n\nA 401(k) Safe Harbor Plan is available to eligible staff with a 3% contribution from Catalist from the date of hire. Employees may contribute pre-tax or post-tax from their salary up to the legal limits set forth by the IRS.\n\nMedical and Dependent Care Flexible Spending Accounts (FSAs)\n\nCatalist offers an FSA Program that gives eligible staff the ability to pay out-of-pocket medical/dental/vision/child care expenses from pre-tax earnings.\n\nTransit Benefits\n\nCatalist also makes available a Transit benefit FSA program to eligible employees using pre-tax contributions with a company match.\n\nProfessional Development and Remote Work Expenses\n\nEligible employees may be reimbursed up to $750 each year for professional development / education and remote work expenses.\n\nStudent Loan PayDown or SaveUp\n\nCatalist offers a Student Loan PayDown and College SaveUp benefit for eligible staff.\n\nVacation, Personal Leave, Sick Leave Benefits\n\nCatalist offers generous vacation benefits to all eligible staff. Eligible employees also receive:\n\n 14 Paid Holidays\n Personal Days\n Sick Leave\n Parental Leave\n\n\nHybrid Office/Remote Work\n\nCertain positions at Catalist are eligible for Office/Remote Hybrid or full Remote status."}, {"title": "Remote Entry Level Data Analyst/Engineer", "company_name": "SynergisticIT", "location": "White Plains, NY (Remote)", "link": "/jobs/view/3774171423/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=q54sk1%2FRww0z%2BC%2BE8e0jQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774171423", "description": "About the job\nAt SynergisticIT, we're all about making connections. Whatever IT goals you have, our software programmers can help achieve those. Our Software development teams can take up turnkey projects and execute them in an effective and efficient manner. If you are looking to source talent our recruiters will find the ideal IT talent for your company. What's the secret to our success? Well, it all starts with taking quality time to listen to each client's specific needs. After we have a thorough grasp of your IT goals, we can better customize our Developments as per your specific needs. We can also tailor make recruiting programs to exceed your expectations. Since our founding in 2010, SynergisticIT's strategies have earned the company an enviable position in the software development, IT staffing and IT skill enhancement fields. SynergisticIT continues to work with hundreds of satisfied American clients with our software programmers working on our projects and after gaining hands on experience on cutting edge technologies moving to contribute their skills to great clients like Apple, Google, Client, Ebay, Paypal, Kroger, the Walt Disney Company and hundreds more. If you are tired of working with inefficient programmers who take a lot of time to ramp up we want you to try us. Our software programmers can hit the ground running and get you the maximum return on your investment. You have already tried the rest its time you tried the best. SynergisticIT - Home of the Best Data Scientists and Software Programmers in the Bay Area.\n\n Why Us ? \n\nSynergisticIT has a proven track record of successfully skill enhancement and staffing IT employees for some of the world's most iconic brands. Our team takes the time to fully understand every client's needs so we could best meet your IT staffing requirements. The knowledgeable staff at SynergisticIT is always more than happy to work with clients to ensure they reach their software development goals. Besides staffing, SynergisticIT is also committed to helping young IT professionals advance their career with a robust upskill program . Everyone who goes through SynergisticIT's program learns all the skills necessary to succeed in many IT fields ranging from Java to Machine Learning. Additionally, everyone trained at SynergisticIT has been through extensive mock and technical interview screenings to bolster their career prospects. Last, but certainly not least, SynergisticIT takes great care to respect the privacy considerations for every client. All companies who work with SynergisticIT can rest assured their confidential data is protected using the most up-to-date encryption technologies. SynergisticIT also complies with all the latest NDA agreements.\n\n REQUIRED SKILLS For Java /Software Programmers \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Time series analysis \n\n We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2022 and at Gartner Data Analytics Summit (Florida)-2023 \n\nOracle CloudWorld Event (OCW) Las Vegas 2022 | SynergisticIT - YouTube\n\n https://www.youtube.com/watch?v=OAFOhcGy9Z8 \n\n https://www.youtube.com/watch?v=EmO7NrWHkLM \n\n https://www.youtube.com/watch?v=NVBU9RYZ6UI \n\n https://www.youtube.com/watch?v=Yy74yvjatVg \n\nSynergisticIT at Gartner Data and Analytics Summit 2023 - YouTube\n\nFor preparing for interviews please visit  https://www.synergisticit.com/interview-questions/ \n\n We are looking for the right matching candidates for our clients \n\n Please apply via the job posting \n\n REQUIRED SKILLS For Java /Full Stack/Software Programmer \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning Positions \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Tensorflow \n\n If you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team. \n\n No phone calls please.  Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates"}, {"title": "Data Engineer", "company_name": "Anika Systems", "location": "Leesburg, VA (Remote)", "link": "/jobs/view/3787766893/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=mApL9JnN%2B%2Fr2%2BYUPlxRYOA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787766893", "description": "About the job\nAnika Systems is a fast growing, woman-owned small business that specializes in providing innovative IT solutions for federal government agencies. Our expertise lies in accelerating delivery in Data and Analytics, Intelligent Automation, Application Development, and IT Modernization. We are currently expanding our Federal team and are seeking a passionate and talented Data Engineer. This opportunity is 100% remote.\n\nMust be a U.S. Citizen with the ability to obtain and maintain a government suitability clearance\n\nResponsibilities\n\nDesigns, develops, builds, analyzes, evaluates, and installs database management systems to include database modeling and design, relational database architecture, metadata, and repository creation and configuration management.\nDefines and oversees database organizations, standards, controls, procedures, and documentation. Provides technical consulting in the definition, design, and creation of a database environment.\nAdvises applications development staff and users on data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nDesigns and implements databases with respect to access methods, access time, batch processes, device allocation, validation checks, organization, protection and security, documentation, and statistical methods.\nUses data mapping, data mining, and data transformational analysis tools to design and develop databases.\nDetermines data storage and optimum storage requirements.\nPrepares system requirements, source analysis, and process analyses and designs throughout the database implementation.\n\nRequired Skills And Experience\n\nBA/BS and 1 year of relevant experience\nExperience with DataBricks, SQL, Python\nApplications development experience with data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nExcellent communication skills (both oral and written) with process-oriented organizational skills to ensure project success\n\nDesired Skills And Experience\n\nDemonstrated successful development experience with the full life cycle of government database management systems including database modeling and design, relational database architecture, metadata and repository creation, and configuration management.\n\nPowered by JazzHR\n\nTlKWIorucJ"}, {"title": "Remote Work - Need Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3774712978/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=r3Bp%2F0qcOD2OpC5jpbUOcA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774712978", "description": "About the job\nTitle : Data Engineer\n\nLocation : Remote\n\nDuration : 6+ months to start (long term contract (work order are extended 2x year))\n\nPlease Fill out the skill matrix: Technology Some Awareness (1) Novice (2) Intermediate (3) Advanced (4) Expert (5) SQL Tableau Desktop /Creator Tableau API Integration AWS - S3 Glue AWS - Athena Redshift AWS - Lambda AWS - SNS/SQS msExcel\n\nProfile: Data Analyst will work closely with Architects and leads in the Interflow team, key business stakeholders including program staff to support the organizations effort in the areas of data Analysis and information management. Role should have strong experience in writing SQL queries to extract data for reports. Candidate should have experience working with large set of data. As these reports are very critical for the company and we are looking for an expert who is capable of handling, consuming and extracting large sets of data for analysis and reports.\n\nMust Haves\n\nAWS - Athena\nAWS - S3\nGlue\nhands-on development of reporting applications\nMS Excel\nRedShift\nSQL\nTableau Desktop /Creator\n\nNice\n\nAWS - Lambda\nAWS - SNS/SQS\nTableau API Integration\n\nSkills\n\nBA/BS required (major in an analytical field desired) \nA minimum 10+ years work-related experience. \nExperience in hands-on development of reporting applications for the Web in a professional environment \nThe Ethos of continuous improvement and interest in learning new things.\nStrong analytical thinking and structured problem-solving ability\nAbility to handle multiple projects and assignments simultaneously and effectively in a cross-functional team environment. \nVersed on the agile methodology and best practices. \nExcellent interpersonal and collaboration skill with the ability to work with a diverse set of colleagues, across functions, from different organizations, disciplines, etc. \nSelf-starter, ability to set priorities, work independently and attain goals"}, {"title": "Data Engineer", "company_name": "Team Remotely Inc", "location": "Houston, TX (Remote)", "link": "/jobs/view/3786249829/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=78lY5Jo%2FfyRPC6TQYCdvww%3D%3D&trk=flagship3_search_srp_jobs", "id": "3786249829", "description": "About the job\nThis is a remote position.\n\n Data Engineer (US/Canada Residents Only, 1 year experience, remote)\n\nTeam Remotely Inc. is a staffing and recruitment agency that offers a comprehensive solution for talent acquisition, including sourcing, vetting, pay rolling, and managing talent. Whether you need contract staffing, direct hire, direct sourcing, talent pools, or diversity initiatives, our model can support your hiring strategy.\n\n Hiring Type: Full-Time\n\n Base Salary: $55K-$70K Per Annum.\n\nResponsibilities\n\nDesigning user interface changes for web-based DB applications. \nReviewing application requirements and interface designs. \nDeveloping and implementing highly responsive user interface components using react concepts. \nWriting application interface codes using JavaScript following react.js workflows. \nTroubleshooting interface software and debugging application codes. \nDeveloping and implementing front-end architecture to support user interface concepts. \nMonitoring and improving front-end performance. \n\nQualifications And Experience\n\nBachelor\u2019s degree in computer science, information technology, or a similar field. \nPrevious experience working as a react.js developer. \nIn-depth knowledge of JavaScript, CSS, HTML, jQuery, and front-end languages. \nKnowledge of REACT tools including React.js, Webpack, Enzyme, Redux, and Flux. \nExperience with user interface design. \nKnowledge of performance testing frameworks, including Mocha and Jest. \nExperience with browser-based debugging and performance testing software. \nTroubleshooting skills. \nProject management skills. \nProblem-solving skills. \nVerbal communication skills. \n\n Why work with Team Remotely?\n\nTeam Remotely Inc. is a staffing platform offering a seamless experience for employers and candidates. Employers can post job openings and specify their requirements, while candidates can create profiles and upload resumes.\n\nThe team of Team Remotely continuously learns and adapts based on previous successful placements, constantly improving its matching capabilities. This ensures that the recommendations provided by Team Remotely are tailored and accurate, increasing the likelihood of a successful match between employers and candidates. By providing intelligent and data-driven solutions, they strive to enhance the efficiency and effectiveness of the hiring process, ultimately helping companies find the best talent and individuals find their dream jobs."}, {"title": "Data Engineer", "company_name": "ZAG Technical Services", "location": "United States (Remote)", "link": "/jobs/view/3771403464/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=Hf0VefrKXBFIL5MtHmY0Pw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3771403464", "description": "About the job\nWe are looking for a tech savvy Data Engineer to contribute to the building, operating, and scaling of next generation data platforms and tools that will power data-driven analytics capabilities throughout the ZAG products and services portfolio, spanning areas such as business intelligence, reporting, and data analytics.\n\nThe ideal team member is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. Must be comfortable supporting the data needs of multiple clients and teams, systems, and products within the modernization of ecosystems. The right-fit person will be excited by the prospect of optimizing or even re-designing data processes to support next generation of advanced analytics capabilities, which in-turn translates to implementing strategic enhancements to maintain competitive advantage in the industry.\n\nThe successful team member will leverage their proficiency to\u2026\n\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\nBuild the analytics processes required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Data Management and cloud based \u2018big data\u2019 technologies.\nBuild analytics tools that utilize technologies to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.\nCollaborate with client teams to identify and solution regional and local requirements within the context of the standard global business model for enterprise analytics.\nParticipates in meetings, discussions, strategy sessions where changes, improvements, and enhancements are proposed, evaluated, and approved. \n\n\nThe exceptional team member will have or demonstrate progressive experiences in\u2026 \n\nBachelor\u2019s degree or higher in Computer Science or related field\nExperience working with BI and data warehousing tools\nExperience working with enterprise data, where security is paramount and data governance is critical\nAbility to work independently and in a team environment, and effectively engage all levels of the organization\nUnderstanding of distributed systems driving large-scale data processing and analytics with a successful history of manipulating, processing, and extracting value from large, disconnected datasets.\nAbility to communicate effectively (listening, presenting, and questioning)\nStrong organizational, written, and communication skills\n\n\nAbout ZAG\n\nZAG Technical Services empowers our clients\u2019 success through intelligent business solutions that increase employee productivity, network security and team collaboration. We aim to ensure that our client\u2019s IT investments drive ROI, provide a competitive advantage and help businesses grow. We do this by living our core set of values: Integrity Foremost, Client-Centered, Accountable Always, Teamwork Throughout, and Exceptional is the Goal.\n\nWhat ZAG Has To Offer\n\nCompetitive compensation package \nBenefits including medical, dental, vision, 401K and life insurance \n\n\nZAG Technical Services is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all of our employees.\n\nZAG Technical Services is an E-Verify participant and will provide the Social Security Administration (SSA) and, if necessary, the Department of Homeland Security (DHS), with information from each new employee\u2019s Form I-9 to confirm work authorization."}, {"title": "Junior Data Engineer", "company_name": "HireMeFast LLC - Career Accelerator - Land A Job", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3786249797/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D&trk=flagship3_search_srp_jobs&lipi=urn%3Ali%3Apage%3Ad_flagship3_search_srp_jobs%3BQExogrHBQoSF9zZm%2B9kufw%3D%3D&lici=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D", "id": "3786249797", "description": "About the job\nThis is a remote position.\n\n Job Title: Junior Data Engineer\n\n Employment Type: Full-Time\n\n Salary: $64,000 - $76,000 per annum\n\n Experience Required:  Minimum 1 year of project experience\n\nAbout us: HireMeFast is a leading staffing and recruitment agency specializing in connecting businesses with top-tier talent across various industries. Our mission is to bridge the gap between exceptional candidates and organizations needing their skills, expertise, and unique qualities. Our team of experienced and dedicated recruitment specialists utilizes innovative sourcing strategies, and a vast network to identify and attract top talent. We conduct comprehensive procedures to ensure that only the most qualified candidates are presented to our clients.\n\nPosition Summary\n\nJoin the fast-paced, innovative, and collaborative environment focused on providing an AIOps platform that enhances the intelligence of the CVS Health infrastructure. Work closely with subject matter experts and colleagues to build and scale out machine learning and AI solutions that will detect, predict, and recommend solutions to correct issues before system impact and enhance efficiency, reliability, and performance of CVS Health\u2019s IT operations.\n\nKey Responsibilities include:\n\nData pipeline development: Designed, implemented, and managed data pipelines for extracting, transforming, and loading data from various sources into data lakes for processing, analytics, and correlation. \nData modeling: Create and maintain data models ensuring data quality, scalability, and efficiency \nDevelop and automate processes to clean, transform, and prepare data for analytics, ensuring data accuracy and consistency \nData Integration: Integrate data from disparate sources, both structured and unstructured to provide a unified view of key infrastructure platform and application data \nUtilize big data technologies such as Kafka to process and analyze large volumes of data efficiently \nImplement data security measures to protect sensitive information and ensure compliance with data and privacy regulation \nCreate/maintain documentation for data processes, data flows, and system configurations \nPerformance Optimization- Monitor and optimize data pipelines and systems for performance, scalability and cost-effectiveness \n\nCharacteristics of this role:\n\nTeam Player: Willing to teach, share knowledge, and work with others to make the team successful. \nCommunication: Exceptional verbal, written, organizational, presentation, and communication skills. \nCreativity: Ability to take written and verbal requirements and come up with other innovative ideas. \nAttention to detail: Systematically and accurately research future solutions and current problems. \nStrong work ethic: The innate drive to do work extremely well. \nPassion: A drive to deliver better products and services than expected to customers. \n\nRequired Qualifications\n\n2+ years of programming experience in languages such as Python, Java, SQL \n2+ years of experience with ETL tools and database management (relational, non-relational) \n2+ years of experience in data modeling techniques and tools to design efficient scalable data structures \nSkills in data quality assessment, data cleansing, and data validation \n\nQualifications\n\nKnowledge of big data technologies and cloud platforms \nExperience with technologies like PySpark, Databricks, and Azure Synapse. \n\nEducation\n\nBachelor\u2019s degree in Computer Science, Information Technology, or related field, or equivalent working experience\n\nWhy HireMeFast LLC?\n\nAt HireMeFast, we understand that finding the right individuals to join your team is crucial for success and growth of your organization. We are committed to streamlining the hiring process for our clients, ensuring they have access to a diverse pool of highly qualified candidates who are a perfect fit for their specific needs."}, {"title": "Data Engineer  - Need candidates from Texas ( REMOTE )", "company_name": "Pulivarthi Group (PG)", "location": "United States (Remote)", "link": "/jobs/view/3770685212/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=9gdMwuif%2BpB%2BCQYvQi3imQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3770685212", "description": "About the job\nFollow us on Linkedin: https://www.linkedin.com/company/pulivarthigroup/\n\nPulivarthi Group LLC is a Global Staffing & IT Technology Solutions company, with our prime focus of providing world class solutions to our customers with the right talent. We combine the expertise of our team and the culture of your company to help you with the solution that is affordable and innovative using high quality standards and technologies.\n\nWe\u2019ve served some of the largest healthcare, financial services, and government entities in the U.S.\n\nData Engineer - FT only - Azure SQL with data lakes experience\n\nData Engineer -\n\nthe resource requirement for this Data Engineer role needed more clarification.\n\nOur customer is requesting a resource who knows Azure CI/CD (Continuous Integration / Continuous Deployment) to build and integrate their DevOps pipeline.\n\nOnce the DevOps pipelines are mapped, they need the Data Engineer to connect to all the Data Sources, primarily their Data Lake.\n\nThe Customer (based in Houston) is interested in a resource in a time zone not too far removed from the Central Time Zone to allow real time collaboration."}, {"title": "Data Engineer", "company_name": "Otter Products", "location": "Fort Collins, CO (Remote)", "link": "/jobs/view/3766728302/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=77U3U7CAi1PxgDmpI3%2BiNQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3766728302", "description": "About the job\nOverview \n\nOtter Products is currently recruiting for a Data Engineer to join our Data team! You can be based in our Fort Collins, CO office with flexibility to work a portion of your time remotely, or 100% Remote in the U.S.\n\nThe Data Engineer applies professional experience, concepts, and company objectives toward building and operationalizing the minimally inclusive data necessary for the enterprise data initiatives following industry standard practices and tools. The bulk of the Data Engineer's work would be in designing, managing and optimizing data pipelines, then moving these data pipelines effectively into production for key data and analytics consumers like business/data analysts, data scientists or other stakeholder that needs curated data for data and analytics use cases across the enterprise. The Data Engineer also ensures compliance with data governance and data security requirements while creating, improving and operationalizing these integrated and reusable data pipelines to enable faster data access, integrated data reuse and vastly improved time-to-solution for data and analytics initiatives. Additionally, the Data Engineer will be expected to collaborate with other data team members and data consumers working on the models and algorithms developed by them toward optimization for data quality, security, and governance to put them into production leading to potentially large productivity gains.\n\n About Otter Products \n\nOtter Products, we grow to give. From our founder's garage in 1998 to the global technology leader we are today, Otter Products continues to drive growth through innovation.\n\nThrough our industry-leading brands - OtterBox, Liviri and OtterCares - we provide our partners the number-one selling and most trusted products in our categories. Our philanthropic spirit is the foundation on which we foster our partner relationships, allowing us to grow and to give - together.\n\nBy way of our charitable arm, the OtterCares Foundation, we support our communities and invest in our future through education that inspires kids to change the world.\n\nAnd even as our global community of Otters continues to grow, our founder's core values are still at the heart of everything we do. We measure our success by our ability to give back to our communities and strengthen opportunities for all.\n\nFor more information visit otterproducts.com\n\n Responsibilities \n\nComplete analysis, design and development of BI solutions using Azure SQL Server\nFamiliarity with Data Warehouse concepts\nExperience coding the extraction, transformation, and loading (ETL) processes\nDatabase development primarily in SSIS, Data Factory and SQL\nPartner with the business to determine end user database/reporting needs and requirements\nGenerate ad hoc reports using Power BI or SSRS\nCollaborate with other developers to create and implement best approach solution(s)\nTroubleshoot Azure tools, systems, and software\nReview queries for performance issues, making changes as needed\nCollaborate with team to performance-tune Azure application as necessary\nAssist with the analysis and extraction of relevant information from large amounts of historical business data to feed data science initiatives\nParticipate and support the design and documentation of processes for large scale data analyses, model development, model validation and model implementation\nSupport and maintain a positive safety culture by following all safety policies and procedures and actively contributing to a safe working environment\nOther duties as required\n\n Qualifications \n\nBachelor's degree required. Degree in Computer Science or Mathematics preferred\nMinimum of four years of experience in an IT or analytical role required. Experience in database development, report writing and/or statistics preferred.\n\n EEO \n\nOtter Products, LLC is an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, age, marital status, pregnancy, sex, sexual orientation, gender, gender identity or expression, national origin, disability, veteran status, or any other characteristic or status protected by law.\n\n For US Based Roles Only - Compensation Range Minimum \n\nUSD $90,000.00/Yr.\n\n For US Based Roles Only - Compensation Range Maximum \n\nUSD $120,000.00/Yr.\n\n Additional Total Rewards \n\nProfit Sharing Program Eligible, Benefits Eligible - Full Time- check out otterproducts.com/careers/why for more info"}, {"title": "Software Engineer, Distributed Systems", "company_name": "Hightouch", "location": "United States (Remote)", "link": "/jobs/view/3628572539/?eBP=CwEAAAGMd4QO8APODyE_-mMOPhYyQGg5ZjVbPwq_3V7DK0B2bLAf10pcedM9S2T71X5i4TiJuEwOzWBbSiTKbLuk-VD-_Y-dN5uyf8YpYkHYqEYFxySMS9zr2hRjNrxX9gXbxssh35TFK9YTNV2WeH2Ncb675JgFbJTgNWSZySFC6rrdJGmdSDTr2q5d3VoxORz3GjsDbzjeyZQCvQ07RqYw7oujWSgslycs2LMCY_-N0NEj6e0PvbaYKiZ_5IW34bGVSYLXpHwqa8oOcNttYZ90UA3E_7sDSFzABdIVlfrp5zzERrIYy5sh2MIInxHdDoKgQw8VmOn5uM4S0tXUl-i7LnN0_W0NfPBmLiVDNuveIOm4MfDNPxfe5QKPk6FUAVpvgbcoJn9yRyE&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=rph%2FT6Q6Vkp6YoggJf9cJQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3628572539", "description": "About the job\nAbout Hightouch\n\nHightouch\u2019s mission is to empower everyone to take action on their data. Through our Reverse ETL platform, business and data users can seamlessly sync data from where it resides, such as warehouses and databases, to where it is needed, including operational systems and SaaS tools. Traditionally, acting on data has required engineering time and bandwidth, and left most business users stuck with charts and reports that are unable to take automated action on their data. With Hightouch, every business user, without writing any code, can activate data to streamline critical processes, improve marketing performance, and scale operations.\n\nOur team operates with a focus on making a meaningful impact for our customers. We believe in approaching challenges with a first principles thinking mindset, moving quickly and embracing our value of efficient execution, and treating each other with compassion and kindness. We look for team members that are strong communicators, have a growth mindset, and are motivated and persistent in achieving our goals.\n\nHundreds of companies use Hightouch, including Spotify, Ramp, Retool, NBA, Plaid, and Betterment. We\u2019re based in San Francisco, are remote-friendly, and backed by leading investors such as Amplify Partners, ICONIQ Growth, Bain Capital Ventures, Y-Combinator, and Afore Capital.\n\nAbout The Role\n\nWe are looking for a distributed systems engineer to work on the systems that power the syncing engine our customers and other engineering teams rely on. This presents an exciting challenge where you can apply your expertise in distributed systems, performance optimization, and troubleshooting to push the boundaries of what is possible and meaningful improve our ability to move massive amounts of customer data. This role also provides a unique opportunity to work on a multi-cloud and multi-region infrastructure that supports a global customer base.\n\nWe believe in enabling our engineers to do their best work for our customers by giving them extremely high levels of ownership and autonomy. This comes in different forms: you will own and deliver projects from start to finish, you will work directly with customers to solve their hardest scaling problems, and you will have a lot of influence over what we work on as a team and company.\n\nSome of the problems we\u2019ll be working on include:\n\nSync Speed: Customers want to sync a lot of data to important destinations like Facebook and Snapchat, which requires us to analyze every part of our syncing process and find where we can optimize to sync data more quickly\nStreaming Syncing: We currently sync data from sources in batch only, but envision a world where we also do syncs in real-time and support streaming sources like webhooks and queues\nScalability and Reliability: As part of our rapid growth, we\u2019re always evaluating current future bottlenecks in scaling and reliability, and architecting for the next order of magnitude of growth\nPersonalization API: Our Personalization API product provides a low latency caching layer on top of data warehouses that enable customers to use their data warehouse for real-time personalization use cases with \nMulti-Region and Multi-Cloud: Supporting our multi-region and multi-cloud backend, including extending it to launch Hightouch on in new regions to support data residency requirements of our global customer base\n\nWe are looking for talented, intellectually curious, and motivated individuals who are interested in tackling the problems above. This is a senior role, but we focus on impact and potential for growth more than years of experience. The salary range for this position is $170,000 - $240,000 USD per year, which is location independent in accordance with our remote-first policy.\n\nAbout You\n\nYou are an engineer with a passion for solving hard technical problems that generate real value for customers. You\u2019re motivated by high ownership and are comfortable in a fast-paced, startup environment.\n\nYou have experience and comfort with distributed systems and high-scale systems or the ability and desire to build this experience quickly. This experience can take different forms:\n\nYou\u2019ve built syncing engines that process and move terabytes of data per day in a scalable way\nYou\u2019ve created event collection or data streaming services that power company critical systems\nYou\u2019ve owned systems that have undergone orders of magnitude of growth and helped them scale accordingly\nYou have relevant skills and are a fast learner who is excited to take on the challenges we face\n\nInterview Process\n\nOur goal with the interview process is to balance speed with giving both parties opportunities to assess whether there is a strong mutual fit. We will ask you questions, but we want you to ask us questions! Our technical interviews focus on how you design systems because we believe this is the best way for us to see how you work and for you to see how we collaborate. We don\u2019t ask you to write code to solve technical brainteasers that don\u2019t appear in your day to day job.\n\nRecruiter screen [30m]: Introductory call with our recruiting team to get to know each other and see if the role could be a good mutual fit.\nSystems design screen [45m]: Designing a data processing feature end-to-end.\nHiring manager interview [30m]: Chat with hiring manager about past experiences and future operating preferences to assess fit on company values and operating principles.\nSystems design interview [90m]: Work with the interviewer to architect a system at a conceptual level. The problem will be at a pretty high level - and have both product and customer requirements as well as technical."}, {"title": "Entry-Level Data Scientist Engineer", "company_name": "Phoenix Recruitment LLC", "location": "Texas City, TX (Remote)", "link": "/jobs/view/3788727625/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=8iwtytHsCu4wMwJ4fj3pxw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3788727625", "description": "About the job\nThis is a remote position.\n\nTitle - Entry-Level Data Scientist Engineer, 1 year of project experience\n\nEmployment Type: Full-time\n\nBase Salary: $60K-$70K\n\nPhoenix Recruitment offers a variety of recruiting services to assist both employers and employees. They specialize in marketing open positions, recruiting, and helping employers to find qualified candidates across various industries. Phoenix Recruitment has expertise in streamlining the hiring process. They can help ensure that the process is efficient, well organized, and compliant with relevant regulations.\n\nSkills and Abilities:\n\nStrong knowledge of R or Python for data analysis and modeling. \nProficiency in statistical programs such as R, SAS, MATLAB, or Python. \nFamiliarity with spreadsheets (VBA) and database applications (Access, Oracle, SQL, or equivalent technology). \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \nAbility to learn new methods quickly and work under deadlines. \nExcellent teamwork and communication skills. \nStrong analytical and problem-solving abilities. \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \n\nPreferred:\n\nKnowledge of actuarial concepts and life, health, and/or annuity products. \nExperience with statistical modeling techniques such as GLM, Decision Trees, Time Series, Regression, etc. \nFamiliarity with Microsoft DeployR. \nExposure to insurance risk analysis. \nBasic experience in computational finance, econometrics, statistics, and math. \nKnowledge of SQL and VBA. \nFamiliarity with R or Python for predictive modeling \n\nWhy Phoenix Recruitment LLC?\n\nPhoenix Recruitment often has an extensive network of employers and candidates. This network allows them to tap into a pool of qualified candidates and connect them with suitable job opportunities. They can also leverage their connections to help employers find the right talent efficiently. Outsourcing the recruitment process to a specialized agency can save your time and resources, avoid delays, reduce administrative burdens, and increase the chances of finding the right fit for your organization."}, {"title": "Data Engineer - Remote Work", "company_name": "BairesDev", "location": "United States (Remote)", "link": "/jobs/view/3783162297/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=OtoCrAAHZFomVup04fy35g%3D%3D&trk=flagship3_search_srp_jobs", "id": "3783162297", "description": "About the job\nWho We are\n\nBairesDev is proud to be the fastest-growing company in America. With people in five continents and world-class clients, we are only as strong as the multicultural teams at the heart of our business. To consistently deliver the highest quality solutions to our clients, we only hire the Top 1% of the best talents and nurture their professional growth on exciting projects.\n\nWe are looking for a Data Engineer to join our Development team and participate in different projects made up of multicultural teams distributed throughout the world.\n\nWhat You Will Do\n\n Interview other Data Scientists and Data engineers to vet their technical abilities.\n Input data from those interviews into the product and help develop a predictive, fair, and enjoyable solution.\n Work together with developers, tech leads, and solution architects to build applications.\n Improve existing structures by adding new functionalities or proposing technological updates so that as a result, the impact of your contribution is significant in the core of each business.\n Improve the user experience on scalable and high availability platforms, contributing to the key differential of each business.\n\nHere\u2019s What We Are Looking For\n\n 5+ years of experience working as a developer.\n 3+ years of experience working as a Data Scientist / Data Engineer.\n Proficient with analysis, troubleshooting, and problem-solving.\n Hands-on experience with managing data loads and data quality.\n Advanced English level.\n\nHow we do make your work (and your life) easier:\n\n 100% remote work.\n Hardware setup for you to work from home.\n Flexible hours - make your schedule.\n Paid parental leave, vacation & holidays.\n Diverse and multicultural work environment.\n An innovative environment with the structure and resources of a leading multinational.\n Excellent compensation \u2014 well above the market average.\n Here you can grow at the speed of your learning curve.\n\nOur people work remotely but with a consistent and robust culture that promotes diversity and teamwork. To continue being the leading software development company in Latin America, we want to ensure that every BairesDev member gets the best growth and professional development opportunities in a diverse, welcoming, and innovative environment.\n\nEvery BairesDev team member brings something unique to our company.\n\nWe want to hear your story. Apply now!"}, {"title": "Data Engineer (Remote)", "company_name": "Cascade Debt", "location": "United States (Remote)", "link": "/jobs/view/3755336571/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=pmUtn1j2zyPr25CNPh2RNw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3755336571", "description": "About the job\nDo you want to work in a small team where you can make a real difference in a company? Would you embrace the challenge of building a high impact platform that is used globally? Then we want to speak with you!\n\nAbout Cascade\n\nCascade is a fintech startup backed by Canadian and US investors that empowers companies to grow by democratizing access to institutional debt. We are building tools that modernize how companies raise and manage debt, lowering the barriers to entry to the $7 trillion specialty finance and alternative credit market for companies around the world. Founded just last year, we are gearing up for our next phase of technical development and are seeking a talented Data Engineer to lead the way.\n\nAbout The Role\n\nAs a Data Engineer you\u2019ll be an early member of a growing team building a pioneering platform in the debt infrastructure space. You will design, build, and launch efficient, scalable, and reliable data pipelines to move and transform data .\n\nWhat You\u2019ll Do\n\n Building databases, data lakes and data ingestion pipelines to integrate customer databases and datasets to our systems \n Ingesting financial datasets from external customers, then u pdating and maintaining accurate and complete data mappings to ensure that our product s are displaying high quality data \n M onitoring and alerting across our data pipelines in order to make sure that our data ingests are reliable and correct \n Translat ing business goals and stakeholder requirements into new data flows and deliver analytical insights ensuring that the data flows create real business value continuously \n\nSkills And Experience\n\n 2 + years experience in a data engineering or data analyst role \n E xperience with relational SQL and NoSQL databases, including PostgreSQL , MySQL, MariaDB, MongoDB, Bigtable, etc. \n E xperience working with ETL technologies , such as Databricks, Fivetran , or dbt \n Proficiency in writing SQL queries and knowledge of analytical data warehouses such as RedShift, BigQuery , and Snowflake \n Some experience with CI/CD automation such as GitHub Actions \n Developing APIs and integrating with 3rd party APIs \n Bonus: experience in fintech / SaaS / credit analysis \n\nPeople Who Thrive At Cascade Are\n\n Self-starters, who take the initiative to tackle challenges in a remote work environment \n Good communicators, who can operate without ego to discuss, learn, grow, and help others do the same \n Passionate about creating great digital experiences for users \n Problem solvers who are not satisfied with the status quo \n\nBenefits\n\nRemote: we are a remote-first company and are very flexible on hours as long as things get done \nHome-office: there\u2019s a $1000 USD home office allowance to set yourself up \nEquity: we expect you to have an owner-mentality, and have the equity plan to match \nBenefits: health, dental, vision, and more \nPerks: we offer free lunches weekly and off-site trips \nJob satisfaction: we offer autonomy, ample opportunities for mastery, and an opportunity to make a difference for companies around the world"}, {"title": "Data Engineer", "company_name": "ISC (Integrated Specialty Coverages, LLC)", "location": "Carlsbad, CA (Remote)", "link": "/jobs/view/3778293528/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ew7%2FfEuXSujs5IGabdqUyw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3778293528", "description": "About the job\nAbout Integrated Specialty Coverages\n\nIntegrated Specialty Coverages LLC (ISC) is a growth stage technology and data-driven insurance company leading innovation in the market.\n\nBacked by one of the leading private equity firms, KKR, and led by a forward-thinking management team, ISC is combining the worlds of insurance and technology to create an Insurtech powerhouse. As a leading online distributor of insurance products for a range of industries and \"Main Street USA\", we are looking for the right people to help us in our mission of achieving exponential growth. We strive to be the number one place to go for brokers and agents to source insurance. To accomplish this, we're building a digitally focused team that deeply understands the intersection between user experience, data, and AI/ML to optimize the way we engage with our customers and partners.\n\nJob Summary\n\nThe Data Engineer role will be the technical liaison between multiple groups including a data science team, the engineering team, product management, and business stakeholders. You do not need any insurance knowledge prior, however, you must quickly dive deep into the insurance world and ask questions to become a subject matter expert. You will be responsible for building a data platform to facilitate the data science team. You must be a self-starter that can build out features such as a data pipeline from scratch. There will be support from both engineering and data science for any buildout.\n\nJob Responsibilities\n\nCreate and maintain optimal data pipeline architecture. \nAssemble large, complex data sets that meet functional / non-functional business requirements. \nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. \nBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using AWS technologies, SQL, and Python. \nWork with stakeholders including the Executive, Product, Data Science, and Engineering teams to assist with data-related technical issues and support their data infrastructure needs. \nWork with data science and analytics teams to strive for greater functionality in our data systems. \n\nMinimum Qualifications:\n\nAdvanced working SQL knowledge and experience working with relational databases, strong query authoring (SQL) as well as working familiarity with a variety of databases (Snowflake, Redshift, MySQL, MSSQL, etc.)\nExperience building and optimizing data pipelines, architecture and data sets. \nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. \nBuild processes supporting data transformation, data structures, metadata, dependency, and workload management. \nA successful history of transforming, processing and extracting value from large disconnected, datasets from a variety of data sources (Flat files, Excel, databases, etc.)\nStrong analytic skills related to working with unstructured datasets. \nStrong project management and organizational skills. \nExperience supporting and working with cross-functional teams in a dynamic environment. \nAbility to mentor/guide/collaborate with other team members. \n\nWe are looking for a candidate with 3-5 years of experience in a Data Engineer role. They should also have experience using the following software/tools:\n\nExperience with relational and MPP databases, including Snowflake and MySQL. \nExperience developing software in an agile environment from the requirements stage to production. \nExperience with version control: git\nExperience with container technologies: Docker\nExperience with data pipeline and workflow management tools: Airflow, Jenkins, AWS Glue, Azkaban, Luigi, etc. \nExperience with AWS cloud services: EC2, ECS, Batch, S3, EMR, RDS, Redshift\nExperience with other cloud services: Snowflake, Airflow\nExperience with object-oriented/object function scripting languages: Python, Java, C++, etc. \nExperience with data modeling and data warehouse design\nExperience with data visualization tools (PowerBI, QuickSight)\n\nThe starting annual pay scale for this position is listed below. Actual starting pay will be based on factors such as skills, qualifications, training, and experience. In addition, the company offers comprehensive benefits including medical, dental and vision insurance, 401(k) plan with match, paid time off, and other benefits.\n\nISC's salary ranges are determined by role and level. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations and could be higher or lower based on a multitude of factors, including job-related skills, experience, and relevant education or training.\n\nNational Pay Range\n\n$100,000\u2014$110,000 USD\n\nBenefits of Working at ISC\n\nCompetitive vacation and flexible working arrangements\nComprehensive and inclusive health benefits\nA variety of professional development and mentorship opportunities\nChoice of technology whether at home or in the office\n\nISC's Ownership Behaviors\n\nDo the Right Thing *Be Relentless. Pursue Excellence. *Personal Responsibility & Own Outcomes\nTry Fast. Learn Fast. Fail Fast. Think Big. *Build Extraordinary Owners & Join Forces *Grit & Determination\n\nApplicants with disabilities may contact ISC HR department via e-mail, and other means to request and arrange accommodation. If you need assistance to accommodate a disability, you may request accommodation at any time. Please contact ISC at HR@ISCMGA.com.\n\nISC believes in creating long-term relationships by being responsive and relevant and by consistently delivering value to our community of customers. Specifically, with our employees, we focus on attracting, developing, and retaining the best talent for our business, challenging our people, demonstrating a \"can-do\" attitude, and fostering a collaborative and mutually supportive environment. ISC is an equal opportunity employer and a participant in the US federal E-Verify program (US).\n\nDiversity creates a healthier atmosphere: All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, gender, gender identity, sexual orientation, marital status, medical condition, genetic information, mental or physical disability, military or veteran status, or any other characteristic protected by local, state, or Federal law."}, {"title": "Data Engineer -Remote", "company_name": "RIT Solutions, Inc.", "location": "Denver, CO (Remote)", "link": "/jobs/view/3768007496/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=1hQH8EDaII1eJ7nQ9BACQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3768007496", "description": "About the job\nDuration: 6+ months CTH\n\nMust have a valid LinkedIn profile\n\nRequired Skills\n\n 3-6 years of experience (no more than 6 years)\n Start up environment experience\n Python, SQL, and DBT"}, {"title": "Big Data Engineer - W2", "company_name": "Megan Soft Inc", "location": "United States (Remote)", "link": "/jobs/view/3674845652/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fCAxh4N%2F4YZJ5aEIASaeeA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3674845652", "description": "About the job\nTitle: Big Data Engineer - W2\n\nLocation: Dearborn, MI (Remote)\n\nDuration: 12+ Months\n\nDirect Client: Ford Motors\n\nMust have skills: Should be able to handle GCP requirements\n\n\"Translates business requirements to build repeatable, sustainable, efficient, coded processes that can be productionized by Software Engineers and readily modified by other Data Engineer Creates POC processes in Dev/QA and works with Software Engineers to productionize and align best coding and process practices that maximize efficiency, speed, stability, system resources and capabilities Awareness of and compliance with data privacy, security, legal and contractual guidelines Incorporates data quality and privacy checks/alerts to minimize bad data being consumed by end users, models and dashboards, and to protect customer data; revises checks for new data issues Validates data products and pipelines are functioning as expected following system or application upgrades, source changes, etc. Works closely with Architects if required to align systems, tools and applications being utilized with business use case and performance requirements Provides data product support and maintenance Establishes, tracks and monitors KPIs related to specific data products and deliverable Participate in iteration cadence activities (Iteration Planning, task estimation, daily scrum, demos, etc.). Will be a part of on-call team to resolve critical production support tickets during business hours and off hours.\n\nSkills Required\n\nExperience with Big Data Technologies (Hadoop , ETL Tool, Spark. Hive etc..) Someone who understands Cloud and has experience working with Cloud Platforms (preferably GCP) Should be able to handle GCP requirements - ingesting into GCS/ Big Query, working with GCP components Hands-on in Data Flow, Cloud Storage (GCS), Big Query and GitHub. Proficient in Python and SQL.\n\nSkills Preferred\n\nManages ETL: uses programming and tools for data ingestion, configures pipelines, applies transformations and decoding, integrates and fuses data, moves and securely deliver Experience NoSQL databases\n\nAdditional Information\n\nIf the candidate is remote only, please indicate \"\"100% Remote\"\" under candidate's name on resume. Other similar schedule notations can be \"\"Local Candidate\"\" or \"\"Hybrid Candidate.\"\" ***HRA Test Required***\""}, {"title": "Remote Opportunity: Quantexa Data Engineer", "company_name": "SPAR Information Systems LLC", "location": "United States (Remote)", "link": "/jobs/view/3680073191/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ioykVCZSOQ7BK5%2FwRQHOWg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3680073191", "description": "About the job\nHello All, Hope you are doing great!! Please go through the job descriptions and let me know your interest. Role: Quantexa Data engineer  Location: Remote Duration: Long Term Contract Job Description: 1. End to End usage of Quantexa including guiding our client team on how to use from both front end and data ingestion Reason they are asking for Certification 2. Resolve Entity issues with data 3. Knowledge of Scala and Hive which requires coding experience using parquet 4. Advanced SQL knowledge requiring writing SQL queries 5. Experience having worked with GCP data lake 6. Quantexa certification required\n\nThanks & Regards,\n\nSatnam Singh\n\nDirect: 201 623 3660\n\nEmail : Satnam.singh@sparinfosys.com"}, {"title": "Remote Work - Need Staff Data Engineer - USC only", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3746283980/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=CvIuLBNO%2BPMGi4IXVRkP0A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3746283980", "description": "About the job\n100% Remote, client is in NY\n\nLook for strong job history, Python, Machine Learning, SQL, 10/10 comms, etc. \n\nMUST be an Engineer \n\nSummary\n\nWe're looking for an experienced Data Engineer to join our client's growing team of top notched IT professionals. You will work closely across multiple groups including Editorial and Audio, Marketing, Advertising, etc. to achieve their goals and objectives. The Data Engineering team builds tools throughout the stack while sharing knowledge across these departments.\n\nAs the company grows, they're looking for Data Engineer who will help solidify and expand our pipelines and maintain their data warehouse. Their business model is based on performing complex and very detailed analyses of how our products perform with their subscribers. You'll be responsible for working alongside analysts, product, marketing, and other teams to define new data collection components and measurement schemes that support each new product and feature.\n\nHow Can Add Value To Their Mission\n\nCreate and maintain data pipelines to provide insights and drive business decisions\nEstablish data warehousing strategy (ex. Kimball, Data Vault, etc.)\nMaintain data infrastructure on our AWS accounts\nCollaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.\nWrite unit/integration tests, contributes to engineering wiki, and documents work\nImplement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it\n\nWhat You'll Need To Succeed\n\n7+ years of industry experience measuring product performance and user behavior\nExperience working with a variety of data management technologies, including RedShift, Kinesis/Kafka, Glue, Spark, Postgres, Airflow, dbt, and others.\nExperience implementing BI reporting tools such as Looker\nExperience interfacing with engineers, product managers and analysts to understand data needs\nKnowledge of a variety of measurement beacons, SDKs, APIs including Google Analytics, Amplitude, Braze, Stripe, email service providers\nA commitment to building secure, resilient, fault-tolerant architectures, with clear documentation and procedures in place for support\nA focus on accuracy and detail as you build, ensuring data pipelines produce clear, predictable results\nUnderstanding of the typical metrics a subscription and advertising-supported business needs to measure success\nExperience with Client techniques as applied to behavioral segmentation or anomaly identification is a definite plus\nFamiliarity using developer tools that increase productivity and facilitate the development of resilient code (eg. Docker, CircleCI, Serverless) is a plus"}, {"title": "ENTRY LEVEL DATA ENGINEER (REMOTE)", "company_name": "SynergisticIT", "location": "Jacksonville, FL (Remote)", "link": "/jobs/view/3767587909/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=om7L3CnowiC4PLIcpZD70A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3767587909", "description": "About the job\nSYNERGISTICIT wants every candidate to know that the Job Market is Challenging and to stand out, you need to have exceptional skills and technologies and that's where we come in to make sure you get the attention which you need\n\nSynergisticIT understands the complex nature of the job market and how difficult it can be to secure a position, especially for fresh graduates. Therefore, we assist and help tech-savvies to convert their passions into professions. We go above and beyond to keep you working in your niche.\n\nAs we focus on long-term success, we provide complete career development solutions. From job search to upskilling portfolio and interview preparation, we can guide you at every step of your career.\n\nSynergisticIT spares no efforts to connect you with a large network of tech giants, including Google, Apple, PayPal, Dell, Cisco, Client, etc. Presently, we are actively looking for ENTRY LEVEL DATA ENGINEER a driven mindset. Get the right opportunity and gain experience in building web-centric solutions on Java.\n\nWho Should Apply : Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or anyone looking to make their career in IT Industry\n\nWe also assist in filing for STEM extension and H1b and Green card filing.\n\nCandidates who are serious about their future in the IT Industry and have set big goals for themselves.\n\nCandidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. We also offer Skill Enhancement Programs if the candidates are missing skills or experience which our clients need with great outcomes\n\nCandidates can benefit from skill enhancement if they fall into the below categories. If they are qualified with enough skills then no need for skill enhancement\n\nCandidates Who Lack Experience\n\nHave had a break in careers\n\nLack Technical Competency\n\ncandidates who want to get employed and make a career in the Tech Industry \n\nPlease Also Check The Below Links\n\nhttps://www.synergisticit.com/candidate-outcomes/\n\nhttps://www.synergisticit.com/java-track/\n\nhttps://www.synergisticit.com/data-science-track/\n\nhttps://www.synergisticit.com/which-is-the-best-option-for-tech-job-seekers-staffing-companies-consulting-companies-bootcamps-or-synergisticit/\n\nhttps://www.synergisticit.com/contact-us/\n\nIf the skills are not a match candidates can opt for Skill enhancement. Or their resume can be sent out to clients to see if responses are achievable\n\nREQUIRED SKILLS For Java/Software Programmers\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Core Java , javascript , C++ or software programming\nSpring boot, Microservices and REST API's experience\nExcellent written and verbal communication skills\n\nFor data Science/Machine learning\n\nRequired Skills\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Statistics, Python, data visualization tools\nExcellent written and verbal communication skills\n\nPreferred skills: NLP, Text mining, Tableau, Time series analysis\n\nTechnical skills are required by clients for selection even if its Junior or entry level position each additional Technical skill helps a candidate's resume to be picked by clients over other job seekers.\n\nClients hire candidates with the right technical skills which they need and reject candidates who lack the required technical skills.\n\nNo third party candidates or c2c candidates\n\nPlease apply to the posting\n\nNo phone calls please. Shortlisted candidates would be reached out"}, {"title": "Data Engineer - Remote | WFH", "company_name": "Get It Recruit - Information Technology", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3782241984/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=BgBf1kSAAX%2BSuw6inJsIVw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3782241984", "description": "About the job\nJoin an innovative team at a leading company in the engineering and manufacturing industry, where we are committed to leveraging data for continuous improvement and operational excellence. This hybrid position allows you to enjoy the flexibility of remote work on Mondays and Fridays.\n\nWhy Join Us\n\nAt our company, we prioritize your well-being and professional growth. We offer a comprehensive benefits package, including excellent health benefits, a 6% 401(k) match, generous paid vacation and sick time, 12 paid holidays, and more.\n\nRole Overview\n\nAs a Data Engineer, you will play a pivotal role in transforming raw data into actionable insights, driving improvements in engineering and manufacturing processes, enhancing product quality, and optimizing operational efficiency. Collaborate with dynamic cross-functional teams to extract, analyze, and visualize data, influencing strategic decisions through your valuable insights.\n\nKey Responsibilities\n\nData Collection and Integration:\n\nGather, clean, and integrate data from diverse sources, including production databases, sensor data, and external datasets.\n\nData Analysis\n\nConduct exploratory data analysis to identify patterns, trends, and anomalies.\n\nPerform statistical analysis and hypothesis testing to derive meaningful insights.\n\nData Visualization\n\nCreate informative and visually appealing dashboards and reports using tools like Tableau, Power BI, or Python libraries.\n\nPredictive Modeling\n\nDevelop predictive models to forecast manufacturing performance, quality, and operational outcomes.\n\nRoot Cause Analysis\n\nInvestigate production issues and quality concerns by analyzing data to identify root causes and recommend corrective actions.\n\nPerformance Monitoring\n\nEstablish and maintain key performance indicators (KPIs) to track manufacturing and engineering metrics.\n\nGenerate regular reports to communicate performance trends to stakeholders.\n\nCross-functional Collaboration\n\nCollaborate with engineering, manufacturing, and quality assurance teams to understand their data needs and provide analytical support.\n\nContinuous Improvement\n\nSuggest process improvements and optimizations based on data analysis.\n\nData Security And Compliance\n\nEnsure data security and compliance with relevant regulations, such as GDPR or industry-specific standards.\n\nInfrastructure And Development\n\nBuild and maintain the infrastructure for collecting, storing, and processing data.\n\nDesign, develop, and manage data pipelines, ETL processes, and data warehouses.\n\nRequired Experience\n\nProven experience as a Sr. Data Analyst and/or Data Engineer in an engineering or manufacturing environment.\n\nProficiency in data analysis tools and programming languages, such as Python, R, SQL, and Excel.\n\nStrong data visualization skills with tools like Tableau or Power BI.\n\nExcellent knowledge of statistical analysis and machine learning techniques.\n\nAbility to communicate complex findings and insights effectively to both technical and non-technical stakeholders.\n\nStrong problem-solving skills and attention to detail.\n\nFamiliarity with manufacturing processes and quality control is a plus.\n\nExperience with enterprise ERPs (SAP S4HANA, Oracle EBS), CRMs (Salesforce, Hubspot), and IoT.\n\nProficient in working with cloud computing platforms including AWS, GCP, and Azure.\n\nExpertise in configuring and optimizing cloud-based data solutions for scalability, reliability, and performance.\n\nBachelor's Degree in Data Science, Statistics, Engineering, or Computer Science Required.\n\nApply Now and be part of a team shaping the future of data-driven innovation in engineering and manufacturing!\n\nEmployment Type: Full-Time"}, {"title": "Entry Level Software Engineer - Data Backend Engineer (Remote - Canada)", "company_name": "Yelp", "location": "United States (Remote)", "link": "/jobs/view/3752881757/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fankeoxifvhpTzOYvowHYQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3752881757", "description": "About the job\nYelp engineering culture is driven by our values: we\u2019re a cooperative team that values individual authenticity and encourages creative solutions to problems. All new engineers deploy working code their first week, and we strive to broaden individual impact with support from managers, mentors, and teams. At the end of the day, we\u2019re all about helping our users, growing as engineers, and having fun in a collaborative environment.\n Interested in exploring data and finding innovative ways to collect it, curate it, and make it easily accessible for others to use? How about doing that at an enormous scale? Yelp\u2019s datasets contain billions of interactions between users and local businesses around the globe. If working with our vast amount of data sounds exciting, come and join us!\n\nThe Marketplace Data Observability team is responsible for managing, governing, and monitoring all of Yelp\u2019s Ads data to protect our revenue stream and also support offline analysis that aids in making long term company decisions. We build tools that help us to better understand our data quality, and we strive to uncover data issues before they impact our consumers.\n\nAs a Data Backend Engineer on this team, you will be responsible for building elegant and scalable data products that serve critical, up-to-date, structured information to support different types of analytics within Yelp. As a core contributor to our growing data modeling and data warehousing engineering efforts, you will help design and own mission-critical data flow pipelines and datastores to enable decisions including effective A/B testing and company investments.\n This opportunity is fully remote and does not require you to be located in any particular area in Canada. We welcome applicants from throughout Canada. We\u2019d love to have you apply, even if you don\u2019t feel you meet every single requirement in this posting. At Yelp, we\u2019re looking for great people, not just those who simply check off all the boxes.\n  Build systems that can effectively store and crunch terabytes of data.\nDesign and develop data models for efficient data storage, retrieval, and reporting.\nCreate and maintain conceptual, logical, and physical data models using industry-standard modeling tools.\nCollaborate with cross-functional teams, including engineers, data analysts, business analysts, and data scientists, to understand data requirements and translate them into effective data models.\nParticipate in data integration efforts, including ETL processes and data migration.\nStay up-to-date with industry best practices, emerging technologies, and trends related to data warehousing.\nSupport on-call rotations as needed to operate the team.\n  Understanding of high performing and scalable data systems.\nExperience in building and orchestrating ETL pipelines.\nExperience with Data Lake or Data Warehouse landscape.\nA hunger for tracking down root causes and fixing them in systematic ways.\nAbility to communicate effectively to technical and non-technical cohorts alike. \nExposure to some of the following technologies: Python, AWS Redshift, AWS Athena / Apache Presto, Big Data technologies (e.g S3, Hadoop, Hive, Spark, Flink, Kafka etc), NoSQL systems like Cassandra, DBT."}, {"title": "Data Engineer (Associate Consultant), June 2024", "company_name": "UDig", "location": "Nashville, TN (Remote)", "link": "/jobs/view/3716039960/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=XUjo40kU55HnDCqJ%2F3FdPg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3716039960", "description": "About the job\nCan UDig It? \n\nUDig designs, builds, and implements technology solutions that deliver on business objectives. People join our team because they are excited to have an impact on the future of our organization, passionate about delivering meaningful results, and energized by solving complex problems. Oh, and they like working with other driven, smart, and authentic people!\n\nUDig is proud to be recognized as a \"Best Places to Work\" in Virginia and Tennessee. We're committed to fostering a transparent environment where leadership is accessible, professional development opportunities are abundant, and you are empowered to achieve your career goals. You'll collaborate, develop deep relationships with teammates, and accelerate your success while having fun!\n\nIf you're graduating in May 2024 and excited by the idea of impacting organizations by building data solutions to achieve business goals, you should consider joining our growing technology team.\n\nHow Things Go Down\n\nAs a new, entry-level teammate, you'll take part in our \"Breaking Ground\" training program designed to help you quickly get up-to-speed on UDig's delivery approach within a project team. The program covers:\n\nUDig delivery process and methodologies such as gathering requirements and delivering projects using agile\nTechnical guidance and instruction on the best practices from Data leaders at UDig\nDelivering executive and technical presentations\nCoaching to prepare you to for success on your first client project at UDig\n\nA typical day might entail:\n\nProject standups and check-ins\nDesign and development of data pipelines, BI reports, and more\nPlanning and participating in project demos\nContributing to internal UDig projects\nCareer and job growth through UDig-provided training\nCreating long-lasting friendships with teammates through fun activities as well as your projects (including social events, winning a prize for guessing the weight of pumpkin, or sharing experience on cool tech side projects)\n\nUDig believes it is important for Associate Consultants going through our Breaking Ground program to reside in one of our major markets (Richmond, VA or Nashville, TN metro area) for at least the first year of employment.\n\nWant to learn more? Hear directly from our teammates by checking out our 'A Day in the Life' blog series: https://www.udig.com/digging-in/breaking-ground-2023/\n\nSound Interesting? Here's the Technical Piece\n\nBelow are a few boxes you should be able to check if\u202fyou're interested: Don't check every box? Go ahead and apply!\n\n1+ year of experience with relational database management systems such as Microsoft SQL Server, MySQL, or Oracle\nUnderstanding of the full lifecycle of design, development, and implementation of data solutions including architecture design, development, testing\nSome experience in ETL development or reporting is a plus\nAbility to present technical ideas and high-level concepts and solutions to internal and external team members with varying degrees of technical knowledge\nBachelor's Degree in in computer science, information technology, or any science or business discipline\nAbility to take your work seriously but also seriously enjoy a good time with your UDig colleagues\n\nWhat's in It For You? \n\nAt UDig, you will build your career alongside talented and experienced developers while gaining experience and having fun! We provide continuous challenges and professional development opportunities. Additionally, we offer: \n\nCompetitive salary, merit reviews, and career advancement paths\nFlexible, hybrid environment\nIndividual $1500 Training Budget\nRegular team building and social activities (virtual and/or in-person)\nTransparent culture with strong communication and access to leadership\nGreat benefits like\nGenerous Paid Time Off, company holidays, and parental leave\nMultiple Single and Family Health Insurance plans to choose from\nDental & Vision coverage\nShort-Term and Long-Term disability\nOptional accident and critical illness coverage\nMatching 401(k)\nand more!"}, {"title": "Expression of Interest: Data Engineer", "company_name": "Fingerprint for Success (F4S)", "location": "New York, NY (Remote)", "link": "/jobs/view/3787769880/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=l%2FEDpQqUNmCttGdIyTk7Cg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787769880", "description": "About the job\nWe are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.\n\nThe F4S Talent Pool is a pilot project designed to:\n\nHelp job seekers get discovered by our partners based on their anticipated hiring needs.\nProvide optional support and resources for job seekers in their career endeavors.\nHelp individuals understand, and bring out the best in themselves and each other.\n\nThe F4S Talent Pool process:\n\nOnce you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.\n\nAbout Fingerprint For Success (F4S)\n\nBacked by 20+ years of research, F4S\u2019s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high performing teams.\n\nKeep in mind, joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your F4S work style assessment to match the hiring needs of our partners.\n\nYour feedback is a gift! Write to us via:\n\nPowered by JazzHR\n\noWuPJ9lwq9"}, {"title": "Data Engineer Seattle, WA (remote)", "company_name": "Conch Technologies, Inc", "location": "United States (Remote)", "link": "/jobs/view/3765052211/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=SBETI78UMutCHuRp5FnP%2FA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3765052211", "description": "About the job\nHi,\n\nGreetings from Conch Technologies Inc\n\nData Engineering Lead \u2013 With Databricks and Unity Catalog\n\nLocation \u2013 Seattle, WA (remote)\n\nDuration \u2013 12 months\n\nNeed: Design and implement data architecture and strategy, leveraging technologies such as Databricks, Unity Catalog, Privacera, and Collibra.\n\nAs a Data Engineering Lead, you will play a pivotal role in defining and executing technical deliverables while assessing complexities and levels of effort. You'll engage with both vendor and internal teams to resolve technical challenges and ensure seamless collaboration. Your leadership will extend to planning and prioritization with the SCRUM master, facilitating effective coordination between onsite and offshore teams, and driving feature prioritization through collaboration with product managers.\n\nKey Responsibilities\n\n Lead the definition and breakdown of technical deliverables, assessing complexities and levels of effort.\n Collaborate with vendor and internal teams to address technical issues.\n Drive planning and prioritization in conjunction with the SCRUM master.\n Coordinate with onsite and offshore teams for timely project delivery.\n Collaborate with product managers to prioritize features, communicate with stakeholders, and clarify requirements.\n Collect and manage user-reported issues, ensuring prompt resolution and communication.\n Define comprehensive test plans, oversee their execution, and document results.\n\n--\n\nWith Regards,\n\nNagesh G\n\nMobile: 408-381-5645\n\nDesk: 901-313-3066\n\nEmail: nagesh@conchtech.com \n\nWeb: www.conchtech.com"}, {"title": "Remote Work - Need Azure Fabric Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3759325599/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=C3iVjmcR9wjUip1nNjUiCg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3759325599", "description": "About the job\nThe client is placing heavy emphasis on Microsoft and Azure Fabric experience\n\nInformation Management Engineer \u2013 Take the identified information management sources and assist migration to the Client Data Lake, Azure Fabric. The engineer will transform information management (de-dup, clean and enrich).\n\nTake the identified data sources and bring them into our Data Lake, Azure Fabric. Once the data is in, the engineer will need to be able to transform data (de-dup, clean and enrich the data). We are focused on using a medallion data model for data architecture. The tools we are working with are: Delta Parquet files, python, spark, notebooks, T-SQL, Fabric lakehouse/warehouse, Dataflows Gen2, Data Pipelines, Direct Lake, shortcuts, ADF."}, {"title": "Data Engineer", "company_name": "Catalist", "location": "Washington, DC (Remote)", "link": "/jobs/view/3744951279/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ui1Fk6ICmnPNqAlZDZAcwA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3744951279", "description": "About the job\nFor over 17 years, Catalist has been a leader in civic data and data science innovation. Our mission is to provide progressive organizations with the data, software, and services needed to better identify, understand, and communicate with the people they need to engage and mobilize. Our clients include the largest, most influential organizations in the U.S. active in civic engagement, advocacy, and political campaigns.\n\nCatalist is home to a dedicated, creative team of technologists, data scientists, and campaign experts committed to using our talents and technology to nurture a vibrant and growing progressive community.\n\nAs a Data Engineer at Catalist, you will work closely with the Analytics and Technology departments to design, build, support, and maintain various data pipelines and processes with an end goal of providing data and intelligence to the progressive community. This role primarily involves translating data architecture designs into functional processes, code, and systems.\n\nThe ideal candidate will be a highly motivated individual with excellent technical skills, a strong desire to learn new skills, and an interest in progressive politics. Catalist values creativity and problem-solving. Our work is on the cutting edge of data-driven politics, and your support will help Democratic candidates and progressive organizations conduct successful advocacy and electoral campaigns.\n\nThis position reports to the Deputy Chief Data Officer. The Data Engineer is a part of a growing Data team that supports all underlying work at Catalist.\n\nThis position is included in our CWA bargaining unit.\n\nPrinciple Duties & Responsibilities\n\nDevelop scalable, production processes, code, and systems for data ingest, transformation, modeling, and reporting across a variety of platforms\nTranslate mock-ups and designs into functional processes, code, and systems\nCreate architecture designs when needed\nProvide quality assurance and testing on processes, code, systems, and products\nBecome an internal subject matter expert on various datasets and support other Catalist departments/teams on usage of those datasets\nExecute ad-hoc data and database maintenance tasks as requiredProject manage cross-departmental efforts, with direct responsibility for stakeholder engagement, management, and execution of technical elements\n\n\nRequirements\n\nBS or BA in a technical field, or relevant experience\n1-2 years of experience working with SQL databases\nExperience with data cleaning or ETL processes\nExperience with distributed computing systems and/or distributed datastores (particularly the Hadoop ecosystem)\nExperience managing projects\nFamiliarity with Catalist data, progressive politics, voter files, and/or commercial data\nBackground check required\n\n\nPreferred Skills & Abilities\n\nWillingness to be a problem solver and produce results in a fast paced environment\nAbility to be creative and personable, and articulate ideas clearly\nExperience working with SQL databases\nProficiency with Python or another object-oriented programming language (R, Java, Scala, etc\u2026)\nExperience working in cloud environments (AWS, GCP, etc.)\nExperience working in command line environments such as Bash\nExperience with a version control tool such as git or github\n\n\nBenefits\n\nMedical, Dental, Vision, Prescription Drug\n\nCatalist offers Medical, Dental, Vision, and Prescription Drug coverage for eligible staff and their eligible dependents. Catalist\u2019s Medical plan is a comprehensive PPO program including Prescription Drug coverage with 85% of the premium paid by Catalist. Dental and Vision coverage is provided at no cost to employees.\n\nGroup Term Life Insurance and Long-Term & Short-Term Disability Coverage\n\nGroup Term Life Insurance and Long-Term and Short-Term Disability coverage is available for eligible staff. These benefits are provided at no cost to Catalist employees.\n\n401(k) Safe Harbor Plan\n\nA 401(k) Safe Harbor Plan is available to eligible staff with a 3% contribution from Catalist from the date of hire. Employees may contribute pre-tax or post-tax from their salary up to the legal limits set forth by the IRS.\n\nMedical and Dependent Care Flexible Spending Accounts (FSAs)\n\nCatalist offers an FSA Program that gives eligible staff the ability to pay out-of-pocket medical/dental/vision/child care expenses from pre-tax earnings.\n\nTransit Benefits\n\nCatalist also makes available a Transit benefit FSA program to eligible employees using pre-tax contributions with a company match.\n\nProfessional Development and Remote Work Expenses\n\nEligible employees may be reimbursed up to $750 each year for professional development / education and remote work expenses.\n\nStudent Loan PayDown or SaveUp\n\nCatalist offers a Student Loan PayDown and College SaveUp benefit for eligible staff.\n\nVacation, Personal Leave, Sick Leave Benefits\n\nCatalist offers generous vacation benefits to all eligible staff. Eligible employees also receive:\n\n 14 Paid Holidays\n Personal Days\n Sick Leave\n Parental Leave\n\n\nHybrid Office/Remote Work\n\nCertain positions at Catalist are eligible for Office/Remote Hybrid or full Remote status."}, {"title": "Remote Entry Level Data Analyst/Engineer", "company_name": "SynergisticIT", "location": "White Plains, NY (Remote)", "link": "/jobs/view/3774171423/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=q54sk1%2FRww0z%2BC%2BE8e0jQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774171423", "description": "About the job\nAt SynergisticIT, we're all about making connections. Whatever IT goals you have, our software programmers can help achieve those. Our Software development teams can take up turnkey projects and execute them in an effective and efficient manner. If you are looking to source talent our recruiters will find the ideal IT talent for your company. What's the secret to our success? Well, it all starts with taking quality time to listen to each client's specific needs. After we have a thorough grasp of your IT goals, we can better customize our Developments as per your specific needs. We can also tailor make recruiting programs to exceed your expectations. Since our founding in 2010, SynergisticIT's strategies have earned the company an enviable position in the software development, IT staffing and IT skill enhancement fields. SynergisticIT continues to work with hundreds of satisfied American clients with our software programmers working on our projects and after gaining hands on experience on cutting edge technologies moving to contribute their skills to great clients like Apple, Google, Client, Ebay, Paypal, Kroger, the Walt Disney Company and hundreds more. If you are tired of working with inefficient programmers who take a lot of time to ramp up we want you to try us. Our software programmers can hit the ground running and get you the maximum return on your investment. You have already tried the rest its time you tried the best. SynergisticIT - Home of the Best Data Scientists and Software Programmers in the Bay Area.\n\n Why Us ? \n\nSynergisticIT has a proven track record of successfully skill enhancement and staffing IT employees for some of the world's most iconic brands. Our team takes the time to fully understand every client's needs so we could best meet your IT staffing requirements. The knowledgeable staff at SynergisticIT is always more than happy to work with clients to ensure they reach their software development goals. Besides staffing, SynergisticIT is also committed to helping young IT professionals advance their career with a robust upskill program . Everyone who goes through SynergisticIT's program learns all the skills necessary to succeed in many IT fields ranging from Java to Machine Learning. Additionally, everyone trained at SynergisticIT has been through extensive mock and technical interview screenings to bolster their career prospects. Last, but certainly not least, SynergisticIT takes great care to respect the privacy considerations for every client. All companies who work with SynergisticIT can rest assured their confidential data is protected using the most up-to-date encryption technologies. SynergisticIT also complies with all the latest NDA agreements.\n\n REQUIRED SKILLS For Java /Software Programmers \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Time series analysis \n\n We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2022 and at Gartner Data Analytics Summit (Florida)-2023 \n\nOracle CloudWorld Event (OCW) Las Vegas 2022 | SynergisticIT - YouTube\n\n https://www.youtube.com/watch?v=OAFOhcGy9Z8 \n\n https://www.youtube.com/watch?v=EmO7NrWHkLM \n\n https://www.youtube.com/watch?v=NVBU9RYZ6UI \n\n https://www.youtube.com/watch?v=Yy74yvjatVg \n\nSynergisticIT at Gartner Data and Analytics Summit 2023 - YouTube\n\nFor preparing for interviews please visit  https://www.synergisticit.com/interview-questions/ \n\n We are looking for the right matching candidates for our clients \n\n Please apply via the job posting \n\n REQUIRED SKILLS For Java /Full Stack/Software Programmer \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning Positions \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Tensorflow \n\n If you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team. \n\n No phone calls please.  Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates"}, {"title": "Data Engineer", "company_name": "Anika Systems", "location": "Leesburg, VA (Remote)", "link": "/jobs/view/3787766893/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=mApL9JnN%2B%2Fr2%2BYUPlxRYOA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787766893", "description": "About the job\nAnika Systems is a fast growing, woman-owned small business that specializes in providing innovative IT solutions for federal government agencies. Our expertise lies in accelerating delivery in Data and Analytics, Intelligent Automation, Application Development, and IT Modernization. We are currently expanding our Federal team and are seeking a passionate and talented Data Engineer. This opportunity is 100% remote.\n\nMust be a U.S. Citizen with the ability to obtain and maintain a government suitability clearance\n\nResponsibilities\n\nDesigns, develops, builds, analyzes, evaluates, and installs database management systems to include database modeling and design, relational database architecture, metadata, and repository creation and configuration management.\nDefines and oversees database organizations, standards, controls, procedures, and documentation. Provides technical consulting in the definition, design, and creation of a database environment.\nAdvises applications development staff and users on data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nDesigns and implements databases with respect to access methods, access time, batch processes, device allocation, validation checks, organization, protection and security, documentation, and statistical methods.\nUses data mapping, data mining, and data transformational analysis tools to design and develop databases.\nDetermines data storage and optimum storage requirements.\nPrepares system requirements, source analysis, and process analyses and designs throughout the database implementation.\n\nRequired Skills And Experience\n\nBA/BS and 1 year of relevant experience\nExperience with DataBricks, SQL, Python\nApplications development experience with data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nExcellent communication skills (both oral and written) with process-oriented organizational skills to ensure project success\n\nDesired Skills And Experience\n\nDemonstrated successful development experience with the full life cycle of government database management systems including database modeling and design, relational database architecture, metadata and repository creation, and configuration management.\n\nPowered by JazzHR\n\nTlKWIorucJ"}, {"title": "Remote Work - Need Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3774712978/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=r3Bp%2F0qcOD2OpC5jpbUOcA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774712978", "description": "About the job\nTitle : Data Engineer\n\nLocation : Remote\n\nDuration : 6+ months to start (long term contract (work order are extended 2x year))\n\nPlease Fill out the skill matrix: Technology Some Awareness (1) Novice (2) Intermediate (3) Advanced (4) Expert (5) SQL Tableau Desktop /Creator Tableau API Integration AWS - S3 Glue AWS - Athena Redshift AWS - Lambda AWS - SNS/SQS msExcel\n\nProfile: Data Analyst will work closely with Architects and leads in the Interflow team, key business stakeholders including program staff to support the organizations effort in the areas of data Analysis and information management. Role should have strong experience in writing SQL queries to extract data for reports. Candidate should have experience working with large set of data. As these reports are very critical for the company and we are looking for an expert who is capable of handling, consuming and extracting large sets of data for analysis and reports.\n\nMust Haves\n\nAWS - Athena\nAWS - S3\nGlue\nhands-on development of reporting applications\nMS Excel\nRedShift\nSQL\nTableau Desktop /Creator\n\nNice\n\nAWS - Lambda\nAWS - SNS/SQS\nTableau API Integration\n\nSkills\n\nBA/BS required (major in an analytical field desired) \nA minimum 10+ years work-related experience. \nExperience in hands-on development of reporting applications for the Web in a professional environment \nThe Ethos of continuous improvement and interest in learning new things.\nStrong analytical thinking and structured problem-solving ability\nAbility to handle multiple projects and assignments simultaneously and effectively in a cross-functional team environment. \nVersed on the agile methodology and best practices. \nExcellent interpersonal and collaboration skill with the ability to work with a diverse set of colleagues, across functions, from different organizations, disciplines, etc. \nSelf-starter, ability to set priorities, work independently and attain goals"}, {"title": "Data Engineer", "company_name": "Team Remotely Inc", "location": "Houston, TX (Remote)", "link": "/jobs/view/3786249829/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=78lY5Jo%2FfyRPC6TQYCdvww%3D%3D&trk=flagship3_search_srp_jobs", "id": "3786249829", "description": "About the job\nThis is a remote position.\n\n Data Engineer (US/Canada Residents Only, 1 year experience, remote)\n\nTeam Remotely Inc. is a staffing and recruitment agency that offers a comprehensive solution for talent acquisition, including sourcing, vetting, pay rolling, and managing talent. Whether you need contract staffing, direct hire, direct sourcing, talent pools, or diversity initiatives, our model can support your hiring strategy.\n\n Hiring Type: Full-Time\n\n Base Salary: $55K-$70K Per Annum.\n\nResponsibilities\n\nDesigning user interface changes for web-based DB applications. \nReviewing application requirements and interface designs. \nDeveloping and implementing highly responsive user interface components using react concepts. \nWriting application interface codes using JavaScript following react.js workflows. \nTroubleshooting interface software and debugging application codes. \nDeveloping and implementing front-end architecture to support user interface concepts. \nMonitoring and improving front-end performance. \n\nQualifications And Experience\n\nBachelor\u2019s degree in computer science, information technology, or a similar field. \nPrevious experience working as a react.js developer. \nIn-depth knowledge of JavaScript, CSS, HTML, jQuery, and front-end languages. \nKnowledge of REACT tools including React.js, Webpack, Enzyme, Redux, and Flux. \nExperience with user interface design. \nKnowledge of performance testing frameworks, including Mocha and Jest. \nExperience with browser-based debugging and performance testing software. \nTroubleshooting skills. \nProject management skills. \nProblem-solving skills. \nVerbal communication skills. \n\n Why work with Team Remotely?\n\nTeam Remotely Inc. is a staffing platform offering a seamless experience for employers and candidates. Employers can post job openings and specify their requirements, while candidates can create profiles and upload resumes.\n\nThe team of Team Remotely continuously learns and adapts based on previous successful placements, constantly improving its matching capabilities. This ensures that the recommendations provided by Team Remotely are tailored and accurate, increasing the likelihood of a successful match between employers and candidates. By providing intelligent and data-driven solutions, they strive to enhance the efficiency and effectiveness of the hiring process, ultimately helping companies find the best talent and individuals find their dream jobs."}, {"title": "Data Engineer", "company_name": "ZAG Technical Services", "location": "United States (Remote)", "link": "/jobs/view/3771403464/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=Hf0VefrKXBFIL5MtHmY0Pw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3771403464", "description": "About the job\nWe are looking for a tech savvy Data Engineer to contribute to the building, operating, and scaling of next generation data platforms and tools that will power data-driven analytics capabilities throughout the ZAG products and services portfolio, spanning areas such as business intelligence, reporting, and data analytics.\n\nThe ideal team member is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. Must be comfortable supporting the data needs of multiple clients and teams, systems, and products within the modernization of ecosystems. The right-fit person will be excited by the prospect of optimizing or even re-designing data processes to support next generation of advanced analytics capabilities, which in-turn translates to implementing strategic enhancements to maintain competitive advantage in the industry.\n\nThe successful team member will leverage their proficiency to\u2026\n\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\nBuild the analytics processes required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Data Management and cloud based \u2018big data\u2019 technologies.\nBuild analytics tools that utilize technologies to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.\nCollaborate with client teams to identify and solution regional and local requirements within the context of the standard global business model for enterprise analytics.\nParticipates in meetings, discussions, strategy sessions where changes, improvements, and enhancements are proposed, evaluated, and approved. \n\n\nThe exceptional team member will have or demonstrate progressive experiences in\u2026 \n\nBachelor\u2019s degree or higher in Computer Science or related field\nExperience working with BI and data warehousing tools\nExperience working with enterprise data, where security is paramount and data governance is critical\nAbility to work independently and in a team environment, and effectively engage all levels of the organization\nUnderstanding of distributed systems driving large-scale data processing and analytics with a successful history of manipulating, processing, and extracting value from large, disconnected datasets.\nAbility to communicate effectively (listening, presenting, and questioning)\nStrong organizational, written, and communication skills\n\n\nAbout ZAG\n\nZAG Technical Services empowers our clients\u2019 success through intelligent business solutions that increase employee productivity, network security and team collaboration. We aim to ensure that our client\u2019s IT investments drive ROI, provide a competitive advantage and help businesses grow. We do this by living our core set of values: Integrity Foremost, Client-Centered, Accountable Always, Teamwork Throughout, and Exceptional is the Goal.\n\nWhat ZAG Has To Offer\n\nCompetitive compensation package \nBenefits including medical, dental, vision, 401K and life insurance \n\n\nZAG Technical Services is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all of our employees.\n\nZAG Technical Services is an E-Verify participant and will provide the Social Security Administration (SSA) and, if necessary, the Department of Homeland Security (DHS), with information from each new employee\u2019s Form I-9 to confirm work authorization."}, {"title": "Junior Data Engineer", "company_name": "HireMeFast LLC - Career Accelerator - Land A Job", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3786249797/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D&trk=flagship3_search_srp_jobs&lipi=urn%3Ali%3Apage%3Ad_flagship3_search_srp_jobs%3BQExogrHBQoSF9zZm%2B9kufw%3D%3D&lici=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D", "id": "3786249797", "description": "About the job\nThis is a remote position.\n\n Job Title: Junior Data Engineer\n\n Employment Type: Full-Time\n\n Salary: $64,000 - $76,000 per annum\n\n Experience Required:  Minimum 1 year of project experience\n\nAbout us: HireMeFast is a leading staffing and recruitment agency specializing in connecting businesses with top-tier talent across various industries. Our mission is to bridge the gap between exceptional candidates and organizations needing their skills, expertise, and unique qualities. Our team of experienced and dedicated recruitment specialists utilizes innovative sourcing strategies, and a vast network to identify and attract top talent. We conduct comprehensive procedures to ensure that only the most qualified candidates are presented to our clients.\n\nPosition Summary\n\nJoin the fast-paced, innovative, and collaborative environment focused on providing an AIOps platform that enhances the intelligence of the CVS Health infrastructure. Work closely with subject matter experts and colleagues to build and scale out machine learning and AI solutions that will detect, predict, and recommend solutions to correct issues before system impact and enhance efficiency, reliability, and performance of CVS Health\u2019s IT operations.\n\nKey Responsibilities include:\n\nData pipeline development: Designed, implemented, and managed data pipelines for extracting, transforming, and loading data from various sources into data lakes for processing, analytics, and correlation. \nData modeling: Create and maintain data models ensuring data quality, scalability, and efficiency \nDevelop and automate processes to clean, transform, and prepare data for analytics, ensuring data accuracy and consistency \nData Integration: Integrate data from disparate sources, both structured and unstructured to provide a unified view of key infrastructure platform and application data \nUtilize big data technologies such as Kafka to process and analyze large volumes of data efficiently \nImplement data security measures to protect sensitive information and ensure compliance with data and privacy regulation \nCreate/maintain documentation for data processes, data flows, and system configurations \nPerformance Optimization- Monitor and optimize data pipelines and systems for performance, scalability and cost-effectiveness \n\nCharacteristics of this role:\n\nTeam Player: Willing to teach, share knowledge, and work with others to make the team successful. \nCommunication: Exceptional verbal, written, organizational, presentation, and communication skills. \nCreativity: Ability to take written and verbal requirements and come up with other innovative ideas. \nAttention to detail: Systematically and accurately research future solutions and current problems. \nStrong work ethic: The innate drive to do work extremely well. \nPassion: A drive to deliver better products and services than expected to customers. \n\nRequired Qualifications\n\n2+ years of programming experience in languages such as Python, Java, SQL \n2+ years of experience with ETL tools and database management (relational, non-relational) \n2+ years of experience in data modeling techniques and tools to design efficient scalable data structures \nSkills in data quality assessment, data cleansing, and data validation \n\nQualifications\n\nKnowledge of big data technologies and cloud platforms \nExperience with technologies like PySpark, Databricks, and Azure Synapse. \n\nEducation\n\nBachelor\u2019s degree in Computer Science, Information Technology, or related field, or equivalent working experience\n\nWhy HireMeFast LLC?\n\nAt HireMeFast, we understand that finding the right individuals to join your team is crucial for success and growth of your organization. We are committed to streamlining the hiring process for our clients, ensuring they have access to a diverse pool of highly qualified candidates who are a perfect fit for their specific needs."}, {"title": "Data Engineer  - Need candidates from Texas ( REMOTE )", "company_name": "Pulivarthi Group (PG)", "location": "United States (Remote)", "link": "/jobs/view/3770685212/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=9gdMwuif%2BpB%2BCQYvQi3imQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3770685212", "description": "About the job\nFollow us on Linkedin: https://www.linkedin.com/company/pulivarthigroup/\n\nPulivarthi Group LLC is a Global Staffing & IT Technology Solutions company, with our prime focus of providing world class solutions to our customers with the right talent. We combine the expertise of our team and the culture of your company to help you with the solution that is affordable and innovative using high quality standards and technologies.\n\nWe\u2019ve served some of the largest healthcare, financial services, and government entities in the U.S.\n\nData Engineer - FT only - Azure SQL with data lakes experience\n\nData Engineer -\n\nthe resource requirement for this Data Engineer role needed more clarification.\n\nOur customer is requesting a resource who knows Azure CI/CD (Continuous Integration / Continuous Deployment) to build and integrate their DevOps pipeline.\n\nOnce the DevOps pipelines are mapped, they need the Data Engineer to connect to all the Data Sources, primarily their Data Lake.\n\nThe Customer (based in Houston) is interested in a resource in a time zone not too far removed from the Central Time Zone to allow real time collaboration."}, {"title": "Data Engineer", "company_name": "Otter Products", "location": "Fort Collins, CO (Remote)", "link": "/jobs/view/3766728302/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=77U3U7CAi1PxgDmpI3%2BiNQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3766728302", "description": "About the job\nOverview \n\nOtter Products is currently recruiting for a Data Engineer to join our Data team! You can be based in our Fort Collins, CO office with flexibility to work a portion of your time remotely, or 100% Remote in the U.S.\n\nThe Data Engineer applies professional experience, concepts, and company objectives toward building and operationalizing the minimally inclusive data necessary for the enterprise data initiatives following industry standard practices and tools. The bulk of the Data Engineer's work would be in designing, managing and optimizing data pipelines, then moving these data pipelines effectively into production for key data and analytics consumers like business/data analysts, data scientists or other stakeholder that needs curated data for data and analytics use cases across the enterprise. The Data Engineer also ensures compliance with data governance and data security requirements while creating, improving and operationalizing these integrated and reusable data pipelines to enable faster data access, integrated data reuse and vastly improved time-to-solution for data and analytics initiatives. Additionally, the Data Engineer will be expected to collaborate with other data team members and data consumers working on the models and algorithms developed by them toward optimization for data quality, security, and governance to put them into production leading to potentially large productivity gains.\n\n About Otter Products \n\nOtter Products, we grow to give. From our founder's garage in 1998 to the global technology leader we are today, Otter Products continues to drive growth through innovation.\n\nThrough our industry-leading brands - OtterBox, Liviri and OtterCares - we provide our partners the number-one selling and most trusted products in our categories. Our philanthropic spirit is the foundation on which we foster our partner relationships, allowing us to grow and to give - together.\n\nBy way of our charitable arm, the OtterCares Foundation, we support our communities and invest in our future through education that inspires kids to change the world.\n\nAnd even as our global community of Otters continues to grow, our founder's core values are still at the heart of everything we do. We measure our success by our ability to give back to our communities and strengthen opportunities for all.\n\nFor more information visit otterproducts.com\n\n Responsibilities \n\nComplete analysis, design and development of BI solutions using Azure SQL Server\nFamiliarity with Data Warehouse concepts\nExperience coding the extraction, transformation, and loading (ETL) processes\nDatabase development primarily in SSIS, Data Factory and SQL\nPartner with the business to determine end user database/reporting needs and requirements\nGenerate ad hoc reports using Power BI or SSRS\nCollaborate with other developers to create and implement best approach solution(s)\nTroubleshoot Azure tools, systems, and software\nReview queries for performance issues, making changes as needed\nCollaborate with team to performance-tune Azure application as necessary\nAssist with the analysis and extraction of relevant information from large amounts of historical business data to feed data science initiatives\nParticipate and support the design and documentation of processes for large scale data analyses, model development, model validation and model implementation\nSupport and maintain a positive safety culture by following all safety policies and procedures and actively contributing to a safe working environment\nOther duties as required\n\n Qualifications \n\nBachelor's degree required. Degree in Computer Science or Mathematics preferred\nMinimum of four years of experience in an IT or analytical role required. Experience in database development, report writing and/or statistics preferred.\n\n EEO \n\nOtter Products, LLC is an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, age, marital status, pregnancy, sex, sexual orientation, gender, gender identity or expression, national origin, disability, veteran status, or any other characteristic or status protected by law.\n\n For US Based Roles Only - Compensation Range Minimum \n\nUSD $90,000.00/Yr.\n\n For US Based Roles Only - Compensation Range Maximum \n\nUSD $120,000.00/Yr.\n\n Additional Total Rewards \n\nProfit Sharing Program Eligible, Benefits Eligible - Full Time- check out otterproducts.com/careers/why for more info"}, {"title": "Software Engineer, Distributed Systems", "company_name": "Hightouch", "location": "United States (Remote)", "link": "/jobs/view/3628572539/?eBP=CwEAAAGMd4QO8APODyE_-mMOPhYyQGg5ZjVbPwq_3V7DK0B2bLAf10pcedM9S2T71X5i4TiJuEwOzWBbSiTKbLuk-VD-_Y-dN5uyf8YpYkHYqEYFxySMS9zr2hRjNrxX9gXbxssh35TFK9YTNV2WeH2Ncb675JgFbJTgNWSZySFC6rrdJGmdSDTr2q5d3VoxORz3GjsDbzjeyZQCvQ07RqYw7oujWSgslycs2LMCY_-N0NEj6e0PvbaYKiZ_5IW34bGVSYLXpHwqa8oOcNttYZ90UA3E_7sDSFzABdIVlfrp5zzERrIYy5sh2MIInxHdDoKgQw8VmOn5uM4S0tXUl-i7LnN0_W0NfPBmLiVDNuveIOm4MfDNPxfe5QKPk6FUAVpvgbcoJn9yRyE&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=rph%2FT6Q6Vkp6YoggJf9cJQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3628572539", "description": "About the job\nAbout Hightouch\n\nHightouch\u2019s mission is to empower everyone to take action on their data. Through our Reverse ETL platform, business and data users can seamlessly sync data from where it resides, such as warehouses and databases, to where it is needed, including operational systems and SaaS tools. Traditionally, acting on data has required engineering time and bandwidth, and left most business users stuck with charts and reports that are unable to take automated action on their data. With Hightouch, every business user, without writing any code, can activate data to streamline critical processes, improve marketing performance, and scale operations.\n\nOur team operates with a focus on making a meaningful impact for our customers. We believe in approaching challenges with a first principles thinking mindset, moving quickly and embracing our value of efficient execution, and treating each other with compassion and kindness. We look for team members that are strong communicators, have a growth mindset, and are motivated and persistent in achieving our goals.\n\nHundreds of companies use Hightouch, including Spotify, Ramp, Retool, NBA, Plaid, and Betterment. We\u2019re based in San Francisco, are remote-friendly, and backed by leading investors such as Amplify Partners, ICONIQ Growth, Bain Capital Ventures, Y-Combinator, and Afore Capital.\n\nAbout The Role\n\nWe are looking for a distributed systems engineer to work on the systems that power the syncing engine our customers and other engineering teams rely on. This presents an exciting challenge where you can apply your expertise in distributed systems, performance optimization, and troubleshooting to push the boundaries of what is possible and meaningful improve our ability to move massive amounts of customer data. This role also provides a unique opportunity to work on a multi-cloud and multi-region infrastructure that supports a global customer base.\n\nWe believe in enabling our engineers to do their best work for our customers by giving them extremely high levels of ownership and autonomy. This comes in different forms: you will own and deliver projects from start to finish, you will work directly with customers to solve their hardest scaling problems, and you will have a lot of influence over what we work on as a team and company.\n\nSome of the problems we\u2019ll be working on include:\n\nSync Speed: Customers want to sync a lot of data to important destinations like Facebook and Snapchat, which requires us to analyze every part of our syncing process and find where we can optimize to sync data more quickly\nStreaming Syncing: We currently sync data from sources in batch only, but envision a world where we also do syncs in real-time and support streaming sources like webhooks and queues\nScalability and Reliability: As part of our rapid growth, we\u2019re always evaluating current future bottlenecks in scaling and reliability, and architecting for the next order of magnitude of growth\nPersonalization API: Our Personalization API product provides a low latency caching layer on top of data warehouses that enable customers to use their data warehouse for real-time personalization use cases with \nMulti-Region and Multi-Cloud: Supporting our multi-region and multi-cloud backend, including extending it to launch Hightouch on in new regions to support data residency requirements of our global customer base\n\nWe are looking for talented, intellectually curious, and motivated individuals who are interested in tackling the problems above. This is a senior role, but we focus on impact and potential for growth more than years of experience. The salary range for this position is $170,000 - $240,000 USD per year, which is location independent in accordance with our remote-first policy.\n\nAbout You\n\nYou are an engineer with a passion for solving hard technical problems that generate real value for customers. You\u2019re motivated by high ownership and are comfortable in a fast-paced, startup environment.\n\nYou have experience and comfort with distributed systems and high-scale systems or the ability and desire to build this experience quickly. This experience can take different forms:\n\nYou\u2019ve built syncing engines that process and move terabytes of data per day in a scalable way\nYou\u2019ve created event collection or data streaming services that power company critical systems\nYou\u2019ve owned systems that have undergone orders of magnitude of growth and helped them scale accordingly\nYou have relevant skills and are a fast learner who is excited to take on the challenges we face\n\nInterview Process\n\nOur goal with the interview process is to balance speed with giving both parties opportunities to assess whether there is a strong mutual fit. We will ask you questions, but we want you to ask us questions! Our technical interviews focus on how you design systems because we believe this is the best way for us to see how you work and for you to see how we collaborate. We don\u2019t ask you to write code to solve technical brainteasers that don\u2019t appear in your day to day job.\n\nRecruiter screen [30m]: Introductory call with our recruiting team to get to know each other and see if the role could be a good mutual fit.\nSystems design screen [45m]: Designing a data processing feature end-to-end.\nHiring manager interview [30m]: Chat with hiring manager about past experiences and future operating preferences to assess fit on company values and operating principles.\nSystems design interview [90m]: Work with the interviewer to architect a system at a conceptual level. The problem will be at a pretty high level - and have both product and customer requirements as well as technical."}, {"title": "Entry-Level Data Scientist Engineer", "company_name": "Phoenix Recruitment LLC", "location": "Texas City, TX (Remote)", "link": "/jobs/view/3788727625/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=8iwtytHsCu4wMwJ4fj3pxw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3788727625", "description": "About the job\nThis is a remote position.\n\nTitle - Entry-Level Data Scientist Engineer, 1 year of project experience\n\nEmployment Type: Full-time\n\nBase Salary: $60K-$70K\n\nPhoenix Recruitment offers a variety of recruiting services to assist both employers and employees. They specialize in marketing open positions, recruiting, and helping employers to find qualified candidates across various industries. Phoenix Recruitment has expertise in streamlining the hiring process. They can help ensure that the process is efficient, well organized, and compliant with relevant regulations.\n\nSkills and Abilities:\n\nStrong knowledge of R or Python for data analysis and modeling. \nProficiency in statistical programs such as R, SAS, MATLAB, or Python. \nFamiliarity with spreadsheets (VBA) and database applications (Access, Oracle, SQL, or equivalent technology). \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \nAbility to learn new methods quickly and work under deadlines. \nExcellent teamwork and communication skills. \nStrong analytical and problem-solving abilities. \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \n\nPreferred:\n\nKnowledge of actuarial concepts and life, health, and/or annuity products. \nExperience with statistical modeling techniques such as GLM, Decision Trees, Time Series, Regression, etc. \nFamiliarity with Microsoft DeployR. \nExposure to insurance risk analysis. \nBasic experience in computational finance, econometrics, statistics, and math. \nKnowledge of SQL and VBA. \nFamiliarity with R or Python for predictive modeling \n\nWhy Phoenix Recruitment LLC?\n\nPhoenix Recruitment often has an extensive network of employers and candidates. This network allows them to tap into a pool of qualified candidates and connect them with suitable job opportunities. They can also leverage their connections to help employers find the right talent efficiently. Outsourcing the recruitment process to a specialized agency can save your time and resources, avoid delays, reduce administrative burdens, and increase the chances of finding the right fit for your organization."}, {"title": "Data Engineer - Remote Work", "company_name": "BairesDev", "location": "United States (Remote)", "link": "/jobs/view/3783162297/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=OtoCrAAHZFomVup04fy35g%3D%3D&trk=flagship3_search_srp_jobs", "id": "3783162297", "description": "About the job\nWho We are\n\nBairesDev is proud to be the fastest-growing company in America. With people in five continents and world-class clients, we are only as strong as the multicultural teams at the heart of our business. To consistently deliver the highest quality solutions to our clients, we only hire the Top 1% of the best talents and nurture their professional growth on exciting projects.\n\nWe are looking for a Data Engineer to join our Development team and participate in different projects made up of multicultural teams distributed throughout the world.\n\nWhat You Will Do\n\n Interview other Data Scientists and Data engineers to vet their technical abilities.\n Input data from those interviews into the product and help develop a predictive, fair, and enjoyable solution.\n Work together with developers, tech leads, and solution architects to build applications.\n Improve existing structures by adding new functionalities or proposing technological updates so that as a result, the impact of your contribution is significant in the core of each business.\n Improve the user experience on scalable and high availability platforms, contributing to the key differential of each business.\n\nHere\u2019s What We Are Looking For\n\n 5+ years of experience working as a developer.\n 3+ years of experience working as a Data Scientist / Data Engineer.\n Proficient with analysis, troubleshooting, and problem-solving.\n Hands-on experience with managing data loads and data quality.\n Advanced English level.\n\nHow we do make your work (and your life) easier:\n\n 100% remote work.\n Hardware setup for you to work from home.\n Flexible hours - make your schedule.\n Paid parental leave, vacation & holidays.\n Diverse and multicultural work environment.\n An innovative environment with the structure and resources of a leading multinational.\n Excellent compensation \u2014 well above the market average.\n Here you can grow at the speed of your learning curve.\n\nOur people work remotely but with a consistent and robust culture that promotes diversity and teamwork. To continue being the leading software development company in Latin America, we want to ensure that every BairesDev member gets the best growth and professional development opportunities in a diverse, welcoming, and innovative environment.\n\nEvery BairesDev team member brings something unique to our company.\n\nWe want to hear your story. Apply now!"}, {"title": "Data Engineer (Remote)", "company_name": "Cascade Debt", "location": "United States (Remote)", "link": "/jobs/view/3755336571/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=pmUtn1j2zyPr25CNPh2RNw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3755336571", "description": "About the job\nDo you want to work in a small team where you can make a real difference in a company? Would you embrace the challenge of building a high impact platform that is used globally? Then we want to speak with you!\n\nAbout Cascade\n\nCascade is a fintech startup backed by Canadian and US investors that empowers companies to grow by democratizing access to institutional debt. We are building tools that modernize how companies raise and manage debt, lowering the barriers to entry to the $7 trillion specialty finance and alternative credit market for companies around the world. Founded just last year, we are gearing up for our next phase of technical development and are seeking a talented Data Engineer to lead the way.\n\nAbout The Role\n\nAs a Data Engineer you\u2019ll be an early member of a growing team building a pioneering platform in the debt infrastructure space. You will design, build, and launch efficient, scalable, and reliable data pipelines to move and transform data .\n\nWhat You\u2019ll Do\n\n Building databases, data lakes and data ingestion pipelines to integrate customer databases and datasets to our systems \n Ingesting financial datasets from external customers, then u pdating and maintaining accurate and complete data mappings to ensure that our product s are displaying high quality data \n M onitoring and alerting across our data pipelines in order to make sure that our data ingests are reliable and correct \n Translat ing business goals and stakeholder requirements into new data flows and deliver analytical insights ensuring that the data flows create real business value continuously \n\nSkills And Experience\n\n 2 + years experience in a data engineering or data analyst role \n E xperience with relational SQL and NoSQL databases, including PostgreSQL , MySQL, MariaDB, MongoDB, Bigtable, etc. \n E xperience working with ETL technologies , such as Databricks, Fivetran , or dbt \n Proficiency in writing SQL queries and knowledge of analytical data warehouses such as RedShift, BigQuery , and Snowflake \n Some experience with CI/CD automation such as GitHub Actions \n Developing APIs and integrating with 3rd party APIs \n Bonus: experience in fintech / SaaS / credit analysis \n\nPeople Who Thrive At Cascade Are\n\n Self-starters, who take the initiative to tackle challenges in a remote work environment \n Good communicators, who can operate without ego to discuss, learn, grow, and help others do the same \n Passionate about creating great digital experiences for users \n Problem solvers who are not satisfied with the status quo \n\nBenefits\n\nRemote: we are a remote-first company and are very flexible on hours as long as things get done \nHome-office: there\u2019s a $1000 USD home office allowance to set yourself up \nEquity: we expect you to have an owner-mentality, and have the equity plan to match \nBenefits: health, dental, vision, and more \nPerks: we offer free lunches weekly and off-site trips \nJob satisfaction: we offer autonomy, ample opportunities for mastery, and an opportunity to make a difference for companies around the world"}, {"title": "Data Engineer", "company_name": "ISC (Integrated Specialty Coverages, LLC)", "location": "Carlsbad, CA (Remote)", "link": "/jobs/view/3778293528/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ew7%2FfEuXSujs5IGabdqUyw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3778293528", "description": "About the job\nAbout Integrated Specialty Coverages\n\nIntegrated Specialty Coverages LLC (ISC) is a growth stage technology and data-driven insurance company leading innovation in the market.\n\nBacked by one of the leading private equity firms, KKR, and led by a forward-thinking management team, ISC is combining the worlds of insurance and technology to create an Insurtech powerhouse. As a leading online distributor of insurance products for a range of industries and \"Main Street USA\", we are looking for the right people to help us in our mission of achieving exponential growth. We strive to be the number one place to go for brokers and agents to source insurance. To accomplish this, we're building a digitally focused team that deeply understands the intersection between user experience, data, and AI/ML to optimize the way we engage with our customers and partners.\n\nJob Summary\n\nThe Data Engineer role will be the technical liaison between multiple groups including a data science team, the engineering team, product management, and business stakeholders. You do not need any insurance knowledge prior, however, you must quickly dive deep into the insurance world and ask questions to become a subject matter expert. You will be responsible for building a data platform to facilitate the data science team. You must be a self-starter that can build out features such as a data pipeline from scratch. There will be support from both engineering and data science for any buildout.\n\nJob Responsibilities\n\nCreate and maintain optimal data pipeline architecture. \nAssemble large, complex data sets that meet functional / non-functional business requirements. \nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. \nBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using AWS technologies, SQL, and Python. \nWork with stakeholders including the Executive, Product, Data Science, and Engineering teams to assist with data-related technical issues and support their data infrastructure needs. \nWork with data science and analytics teams to strive for greater functionality in our data systems. \n\nMinimum Qualifications:\n\nAdvanced working SQL knowledge and experience working with relational databases, strong query authoring (SQL) as well as working familiarity with a variety of databases (Snowflake, Redshift, MySQL, MSSQL, etc.)\nExperience building and optimizing data pipelines, architecture and data sets. \nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. \nBuild processes supporting data transformation, data structures, metadata, dependency, and workload management. \nA successful history of transforming, processing and extracting value from large disconnected, datasets from a variety of data sources (Flat files, Excel, databases, etc.)\nStrong analytic skills related to working with unstructured datasets. \nStrong project management and organizational skills. \nExperience supporting and working with cross-functional teams in a dynamic environment. \nAbility to mentor/guide/collaborate with other team members. \n\nWe are looking for a candidate with 3-5 years of experience in a Data Engineer role. They should also have experience using the following software/tools:\n\nExperience with relational and MPP databases, including Snowflake and MySQL. \nExperience developing software in an agile environment from the requirements stage to production. \nExperience with version control: git\nExperience with container technologies: Docker\nExperience with data pipeline and workflow management tools: Airflow, Jenkins, AWS Glue, Azkaban, Luigi, etc. \nExperience with AWS cloud services: EC2, ECS, Batch, S3, EMR, RDS, Redshift\nExperience with other cloud services: Snowflake, Airflow\nExperience with object-oriented/object function scripting languages: Python, Java, C++, etc. \nExperience with data modeling and data warehouse design\nExperience with data visualization tools (PowerBI, QuickSight)\n\nThe starting annual pay scale for this position is listed below. Actual starting pay will be based on factors such as skills, qualifications, training, and experience. In addition, the company offers comprehensive benefits including medical, dental and vision insurance, 401(k) plan with match, paid time off, and other benefits.\n\nISC's salary ranges are determined by role and level. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations and could be higher or lower based on a multitude of factors, including job-related skills, experience, and relevant education or training.\n\nNational Pay Range\n\n$100,000\u2014$110,000 USD\n\nBenefits of Working at ISC\n\nCompetitive vacation and flexible working arrangements\nComprehensive and inclusive health benefits\nA variety of professional development and mentorship opportunities\nChoice of technology whether at home or in the office\n\nISC's Ownership Behaviors\n\nDo the Right Thing *Be Relentless. Pursue Excellence. *Personal Responsibility & Own Outcomes\nTry Fast. Learn Fast. Fail Fast. Think Big. *Build Extraordinary Owners & Join Forces *Grit & Determination\n\nApplicants with disabilities may contact ISC HR department via e-mail, and other means to request and arrange accommodation. If you need assistance to accommodate a disability, you may request accommodation at any time. Please contact ISC at HR@ISCMGA.com.\n\nISC believes in creating long-term relationships by being responsive and relevant and by consistently delivering value to our community of customers. Specifically, with our employees, we focus on attracting, developing, and retaining the best talent for our business, challenging our people, demonstrating a \"can-do\" attitude, and fostering a collaborative and mutually supportive environment. ISC is an equal opportunity employer and a participant in the US federal E-Verify program (US).\n\nDiversity creates a healthier atmosphere: All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, gender, gender identity, sexual orientation, marital status, medical condition, genetic information, mental or physical disability, military or veteran status, or any other characteristic protected by local, state, or Federal law."}, {"title": "Data Engineer -Remote", "company_name": "RIT Solutions, Inc.", "location": "Denver, CO (Remote)", "link": "/jobs/view/3768007496/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=1hQH8EDaII1eJ7nQ9BACQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3768007496", "description": "About the job\nDuration: 6+ months CTH\n\nMust have a valid LinkedIn profile\n\nRequired Skills\n\n 3-6 years of experience (no more than 6 years)\n Start up environment experience\n Python, SQL, and DBT"}, {"title": "Big Data Engineer - W2", "company_name": "Megan Soft Inc", "location": "United States (Remote)", "link": "/jobs/view/3674845652/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fCAxh4N%2F4YZJ5aEIASaeeA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3674845652", "description": "About the job\nTitle: Big Data Engineer - W2\n\nLocation: Dearborn, MI (Remote)\n\nDuration: 12+ Months\n\nDirect Client: Ford Motors\n\nMust have skills: Should be able to handle GCP requirements\n\n\"Translates business requirements to build repeatable, sustainable, efficient, coded processes that can be productionized by Software Engineers and readily modified by other Data Engineer Creates POC processes in Dev/QA and works with Software Engineers to productionize and align best coding and process practices that maximize efficiency, speed, stability, system resources and capabilities Awareness of and compliance with data privacy, security, legal and contractual guidelines Incorporates data quality and privacy checks/alerts to minimize bad data being consumed by end users, models and dashboards, and to protect customer data; revises checks for new data issues Validates data products and pipelines are functioning as expected following system or application upgrades, source changes, etc. Works closely with Architects if required to align systems, tools and applications being utilized with business use case and performance requirements Provides data product support and maintenance Establishes, tracks and monitors KPIs related to specific data products and deliverable Participate in iteration cadence activities (Iteration Planning, task estimation, daily scrum, demos, etc.). Will be a part of on-call team to resolve critical production support tickets during business hours and off hours.\n\nSkills Required\n\nExperience with Big Data Technologies (Hadoop , ETL Tool, Spark. Hive etc..) Someone who understands Cloud and has experience working with Cloud Platforms (preferably GCP) Should be able to handle GCP requirements - ingesting into GCS/ Big Query, working with GCP components Hands-on in Data Flow, Cloud Storage (GCS), Big Query and GitHub. Proficient in Python and SQL.\n\nSkills Preferred\n\nManages ETL: uses programming and tools for data ingestion, configures pipelines, applies transformations and decoding, integrates and fuses data, moves and securely deliver Experience NoSQL databases\n\nAdditional Information\n\nIf the candidate is remote only, please indicate \"\"100% Remote\"\" under candidate's name on resume. Other similar schedule notations can be \"\"Local Candidate\"\" or \"\"Hybrid Candidate.\"\" ***HRA Test Required***\""}, {"title": "Remote Opportunity: Quantexa Data Engineer", "company_name": "SPAR Information Systems LLC", "location": "United States (Remote)", "link": "/jobs/view/3680073191/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ioykVCZSOQ7BK5%2FwRQHOWg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3680073191", "description": "About the job\nHello All, Hope you are doing great!! Please go through the job descriptions and let me know your interest. Role: Quantexa Data engineer  Location: Remote Duration: Long Term Contract Job Description: 1. End to End usage of Quantexa including guiding our client team on how to use from both front end and data ingestion Reason they are asking for Certification 2. Resolve Entity issues with data 3. Knowledge of Scala and Hive which requires coding experience using parquet 4. Advanced SQL knowledge requiring writing SQL queries 5. Experience having worked with GCP data lake 6. Quantexa certification required\n\nThanks & Regards,\n\nSatnam Singh\n\nDirect: 201 623 3660\n\nEmail : Satnam.singh@sparinfosys.com"}, {"title": "Remote Work - Need Staff Data Engineer - USC only", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3746283980/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=CvIuLBNO%2BPMGi4IXVRkP0A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3746283980", "description": "About the job\n100% Remote, client is in NY\n\nLook for strong job history, Python, Machine Learning, SQL, 10/10 comms, etc. \n\nMUST be an Engineer \n\nSummary\n\nWe're looking for an experienced Data Engineer to join our client's growing team of top notched IT professionals. You will work closely across multiple groups including Editorial and Audio, Marketing, Advertising, etc. to achieve their goals and objectives. The Data Engineering team builds tools throughout the stack while sharing knowledge across these departments.\n\nAs the company grows, they're looking for Data Engineer who will help solidify and expand our pipelines and maintain their data warehouse. Their business model is based on performing complex and very detailed analyses of how our products perform with their subscribers. You'll be responsible for working alongside analysts, product, marketing, and other teams to define new data collection components and measurement schemes that support each new product and feature.\n\nHow Can Add Value To Their Mission\n\nCreate and maintain data pipelines to provide insights and drive business decisions\nEstablish data warehousing strategy (ex. Kimball, Data Vault, etc.)\nMaintain data infrastructure on our AWS accounts\nCollaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.\nWrite unit/integration tests, contributes to engineering wiki, and documents work\nImplement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it\n\nWhat You'll Need To Succeed\n\n7+ years of industry experience measuring product performance and user behavior\nExperience working with a variety of data management technologies, including RedShift, Kinesis/Kafka, Glue, Spark, Postgres, Airflow, dbt, and others.\nExperience implementing BI reporting tools such as Looker\nExperience interfacing with engineers, product managers and analysts to understand data needs\nKnowledge of a variety of measurement beacons, SDKs, APIs including Google Analytics, Amplitude, Braze, Stripe, email service providers\nA commitment to building secure, resilient, fault-tolerant architectures, with clear documentation and procedures in place for support\nA focus on accuracy and detail as you build, ensuring data pipelines produce clear, predictable results\nUnderstanding of the typical metrics a subscription and advertising-supported business needs to measure success\nExperience with Client techniques as applied to behavioral segmentation or anomaly identification is a definite plus\nFamiliarity using developer tools that increase productivity and facilitate the development of resilient code (eg. Docker, CircleCI, Serverless) is a plus"}, {"title": "ENTRY LEVEL DATA ENGINEER (REMOTE)", "company_name": "SynergisticIT", "location": "Jacksonville, FL (Remote)", "link": "/jobs/view/3767587909/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=om7L3CnowiC4PLIcpZD70A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3767587909", "description": "About the job\nSYNERGISTICIT wants every candidate to know that the Job Market is Challenging and to stand out, you need to have exceptional skills and technologies and that's where we come in to make sure you get the attention which you need\n\nSynergisticIT understands the complex nature of the job market and how difficult it can be to secure a position, especially for fresh graduates. Therefore, we assist and help tech-savvies to convert their passions into professions. We go above and beyond to keep you working in your niche.\n\nAs we focus on long-term success, we provide complete career development solutions. From job search to upskilling portfolio and interview preparation, we can guide you at every step of your career.\n\nSynergisticIT spares no efforts to connect you with a large network of tech giants, including Google, Apple, PayPal, Dell, Cisco, Client, etc. Presently, we are actively looking for ENTRY LEVEL DATA ENGINEER a driven mindset. Get the right opportunity and gain experience in building web-centric solutions on Java.\n\nWho Should Apply : Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or anyone looking to make their career in IT Industry\n\nWe also assist in filing for STEM extension and H1b and Green card filing.\n\nCandidates who are serious about their future in the IT Industry and have set big goals for themselves.\n\nCandidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. We also offer Skill Enhancement Programs if the candidates are missing skills or experience which our clients need with great outcomes\n\nCandidates can benefit from skill enhancement if they fall into the below categories. If they are qualified with enough skills then no need for skill enhancement\n\nCandidates Who Lack Experience\n\nHave had a break in careers\n\nLack Technical Competency\n\ncandidates who want to get employed and make a career in the Tech Industry \n\nPlease Also Check The Below Links\n\nhttps://www.synergisticit.com/candidate-outcomes/\n\nhttps://www.synergisticit.com/java-track/\n\nhttps://www.synergisticit.com/data-science-track/\n\nhttps://www.synergisticit.com/which-is-the-best-option-for-tech-job-seekers-staffing-companies-consulting-companies-bootcamps-or-synergisticit/\n\nhttps://www.synergisticit.com/contact-us/\n\nIf the skills are not a match candidates can opt for Skill enhancement. Or their resume can be sent out to clients to see if responses are achievable\n\nREQUIRED SKILLS For Java/Software Programmers\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Core Java , javascript , C++ or software programming\nSpring boot, Microservices and REST API's experience\nExcellent written and verbal communication skills\n\nFor data Science/Machine learning\n\nRequired Skills\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Statistics, Python, data visualization tools\nExcellent written and verbal communication skills\n\nPreferred skills: NLP, Text mining, Tableau, Time series analysis\n\nTechnical skills are required by clients for selection even if its Junior or entry level position each additional Technical skill helps a candidate's resume to be picked by clients over other job seekers.\n\nClients hire candidates with the right technical skills which they need and reject candidates who lack the required technical skills.\n\nNo third party candidates or c2c candidates\n\nPlease apply to the posting\n\nNo phone calls please. Shortlisted candidates would be reached out"}, {"title": "Data Engineer - Remote | WFH", "company_name": "Get It Recruit - Information Technology", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3782241984/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=BgBf1kSAAX%2BSuw6inJsIVw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3782241984", "description": "About the job\nJoin an innovative team at a leading company in the engineering and manufacturing industry, where we are committed to leveraging data for continuous improvement and operational excellence. This hybrid position allows you to enjoy the flexibility of remote work on Mondays and Fridays.\n\nWhy Join Us\n\nAt our company, we prioritize your well-being and professional growth. We offer a comprehensive benefits package, including excellent health benefits, a 6% 401(k) match, generous paid vacation and sick time, 12 paid holidays, and more.\n\nRole Overview\n\nAs a Data Engineer, you will play a pivotal role in transforming raw data into actionable insights, driving improvements in engineering and manufacturing processes, enhancing product quality, and optimizing operational efficiency. Collaborate with dynamic cross-functional teams to extract, analyze, and visualize data, influencing strategic decisions through your valuable insights.\n\nKey Responsibilities\n\nData Collection and Integration:\n\nGather, clean, and integrate data from diverse sources, including production databases, sensor data, and external datasets.\n\nData Analysis\n\nConduct exploratory data analysis to identify patterns, trends, and anomalies.\n\nPerform statistical analysis and hypothesis testing to derive meaningful insights.\n\nData Visualization\n\nCreate informative and visually appealing dashboards and reports using tools like Tableau, Power BI, or Python libraries.\n\nPredictive Modeling\n\nDevelop predictive models to forecast manufacturing performance, quality, and operational outcomes.\n\nRoot Cause Analysis\n\nInvestigate production issues and quality concerns by analyzing data to identify root causes and recommend corrective actions.\n\nPerformance Monitoring\n\nEstablish and maintain key performance indicators (KPIs) to track manufacturing and engineering metrics.\n\nGenerate regular reports to communicate performance trends to stakeholders.\n\nCross-functional Collaboration\n\nCollaborate with engineering, manufacturing, and quality assurance teams to understand their data needs and provide analytical support.\n\nContinuous Improvement\n\nSuggest process improvements and optimizations based on data analysis.\n\nData Security And Compliance\n\nEnsure data security and compliance with relevant regulations, such as GDPR or industry-specific standards.\n\nInfrastructure And Development\n\nBuild and maintain the infrastructure for collecting, storing, and processing data.\n\nDesign, develop, and manage data pipelines, ETL processes, and data warehouses.\n\nRequired Experience\n\nProven experience as a Sr. Data Analyst and/or Data Engineer in an engineering or manufacturing environment.\n\nProficiency in data analysis tools and programming languages, such as Python, R, SQL, and Excel.\n\nStrong data visualization skills with tools like Tableau or Power BI.\n\nExcellent knowledge of statistical analysis and machine learning techniques.\n\nAbility to communicate complex findings and insights effectively to both technical and non-technical stakeholders.\n\nStrong problem-solving skills and attention to detail.\n\nFamiliarity with manufacturing processes and quality control is a plus.\n\nExperience with enterprise ERPs (SAP S4HANA, Oracle EBS), CRMs (Salesforce, Hubspot), and IoT.\n\nProficient in working with cloud computing platforms including AWS, GCP, and Azure.\n\nExpertise in configuring and optimizing cloud-based data solutions for scalability, reliability, and performance.\n\nBachelor's Degree in Data Science, Statistics, Engineering, or Computer Science Required.\n\nApply Now and be part of a team shaping the future of data-driven innovation in engineering and manufacturing!\n\nEmployment Type: Full-Time"}, {"title": "Entry Level Software Engineer - Data Backend Engineer (Remote - Canada)", "company_name": "Yelp", "location": "United States (Remote)", "link": "/jobs/view/3752881757/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fankeoxifvhpTzOYvowHYQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3752881757", "description": "About the job\nYelp engineering culture is driven by our values: we\u2019re a cooperative team that values individual authenticity and encourages creative solutions to problems. All new engineers deploy working code their first week, and we strive to broaden individual impact with support from managers, mentors, and teams. At the end of the day, we\u2019re all about helping our users, growing as engineers, and having fun in a collaborative environment.\n Interested in exploring data and finding innovative ways to collect it, curate it, and make it easily accessible for others to use? How about doing that at an enormous scale? Yelp\u2019s datasets contain billions of interactions between users and local businesses around the globe. If working with our vast amount of data sounds exciting, come and join us!\n\nThe Marketplace Data Observability team is responsible for managing, governing, and monitoring all of Yelp\u2019s Ads data to protect our revenue stream and also support offline analysis that aids in making long term company decisions. We build tools that help us to better understand our data quality, and we strive to uncover data issues before they impact our consumers.\n\nAs a Data Backend Engineer on this team, you will be responsible for building elegant and scalable data products that serve critical, up-to-date, structured information to support different types of analytics within Yelp. As a core contributor to our growing data modeling and data warehousing engineering efforts, you will help design and own mission-critical data flow pipelines and datastores to enable decisions including effective A/B testing and company investments.\n This opportunity is fully remote and does not require you to be located in any particular area in Canada. We welcome applicants from throughout Canada. We\u2019d love to have you apply, even if you don\u2019t feel you meet every single requirement in this posting. At Yelp, we\u2019re looking for great people, not just those who simply check off all the boxes.\n  Build systems that can effectively store and crunch terabytes of data.\nDesign and develop data models for efficient data storage, retrieval, and reporting.\nCreate and maintain conceptual, logical, and physical data models using industry-standard modeling tools.\nCollaborate with cross-functional teams, including engineers, data analysts, business analysts, and data scientists, to understand data requirements and translate them into effective data models.\nParticipate in data integration efforts, including ETL processes and data migration.\nStay up-to-date with industry best practices, emerging technologies, and trends related to data warehousing.\nSupport on-call rotations as needed to operate the team.\n  Understanding of high performing and scalable data systems.\nExperience in building and orchestrating ETL pipelines.\nExperience with Data Lake or Data Warehouse landscape.\nA hunger for tracking down root causes and fixing them in systematic ways.\nAbility to communicate effectively to technical and non-technical cohorts alike. \nExposure to some of the following technologies: Python, AWS Redshift, AWS Athena / Apache Presto, Big Data technologies (e.g S3, Hadoop, Hive, Spark, Flink, Kafka etc), NoSQL systems like Cassandra, DBT."}, {"title": "Data Engineer (Associate Consultant), June 2024", "company_name": "UDig", "location": "Nashville, TN (Remote)", "link": "/jobs/view/3716039960/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=XUjo40kU55HnDCqJ%2F3FdPg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3716039960", "description": "About the job\nCan UDig It? \n\nUDig designs, builds, and implements technology solutions that deliver on business objectives. People join our team because they are excited to have an impact on the future of our organization, passionate about delivering meaningful results, and energized by solving complex problems. Oh, and they like working with other driven, smart, and authentic people!\n\nUDig is proud to be recognized as a \"Best Places to Work\" in Virginia and Tennessee. We're committed to fostering a transparent environment where leadership is accessible, professional development opportunities are abundant, and you are empowered to achieve your career goals. You'll collaborate, develop deep relationships with teammates, and accelerate your success while having fun!\n\nIf you're graduating in May 2024 and excited by the idea of impacting organizations by building data solutions to achieve business goals, you should consider joining our growing technology team.\n\nHow Things Go Down\n\nAs a new, entry-level teammate, you'll take part in our \"Breaking Ground\" training program designed to help you quickly get up-to-speed on UDig's delivery approach within a project team. The program covers:\n\nUDig delivery process and methodologies such as gathering requirements and delivering projects using agile\nTechnical guidance and instruction on the best practices from Data leaders at UDig\nDelivering executive and technical presentations\nCoaching to prepare you to for success on your first client project at UDig\n\nA typical day might entail:\n\nProject standups and check-ins\nDesign and development of data pipelines, BI reports, and more\nPlanning and participating in project demos\nContributing to internal UDig projects\nCareer and job growth through UDig-provided training\nCreating long-lasting friendships with teammates through fun activities as well as your projects (including social events, winning a prize for guessing the weight of pumpkin, or sharing experience on cool tech side projects)\n\nUDig believes it is important for Associate Consultants going through our Breaking Ground program to reside in one of our major markets (Richmond, VA or Nashville, TN metro area) for at least the first year of employment.\n\nWant to learn more? Hear directly from our teammates by checking out our 'A Day in the Life' blog series: https://www.udig.com/digging-in/breaking-ground-2023/\n\nSound Interesting? Here's the Technical Piece\n\nBelow are a few boxes you should be able to check if\u202fyou're interested: Don't check every box? Go ahead and apply!\n\n1+ year of experience with relational database management systems such as Microsoft SQL Server, MySQL, or Oracle\nUnderstanding of the full lifecycle of design, development, and implementation of data solutions including architecture design, development, testing\nSome experience in ETL development or reporting is a plus\nAbility to present technical ideas and high-level concepts and solutions to internal and external team members with varying degrees of technical knowledge\nBachelor's Degree in in computer science, information technology, or any science or business discipline\nAbility to take your work seriously but also seriously enjoy a good time with your UDig colleagues\n\nWhat's in It For You? \n\nAt UDig, you will build your career alongside talented and experienced developers while gaining experience and having fun! We provide continuous challenges and professional development opportunities. Additionally, we offer: \n\nCompetitive salary, merit reviews, and career advancement paths\nFlexible, hybrid environment\nIndividual $1500 Training Budget\nRegular team building and social activities (virtual and/or in-person)\nTransparent culture with strong communication and access to leadership\nGreat benefits like\nGenerous Paid Time Off, company holidays, and parental leave\nMultiple Single and Family Health Insurance plans to choose from\nDental & Vision coverage\nShort-Term and Long-Term disability\nOptional accident and critical illness coverage\nMatching 401(k)\nand more!"}, {"title": "Expression of Interest: Data Engineer", "company_name": "Fingerprint for Success (F4S)", "location": "New York, NY (Remote)", "link": "/jobs/view/3787769880/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=l%2FEDpQqUNmCttGdIyTk7Cg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787769880", "description": "About the job\nWe are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.\n\nThe F4S Talent Pool is a pilot project designed to:\n\nHelp job seekers get discovered by our partners based on their anticipated hiring needs.\nProvide optional support and resources for job seekers in their career endeavors.\nHelp individuals understand, and bring out the best in themselves and each other.\n\nThe F4S Talent Pool process:\n\nOnce you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.\n\nAbout Fingerprint For Success (F4S)\n\nBacked by 20+ years of research, F4S\u2019s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high performing teams.\n\nKeep in mind, joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your F4S work style assessment to match the hiring needs of our partners.\n\nYour feedback is a gift! Write to us via:\n\nPowered by JazzHR\n\noWuPJ9lwq9"}, {"title": "Data Engineer Seattle, WA (remote)", "company_name": "Conch Technologies, Inc", "location": "United States (Remote)", "link": "/jobs/view/3765052211/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=SBETI78UMutCHuRp5FnP%2FA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3765052211", "description": "About the job\nHi,\n\nGreetings from Conch Technologies Inc\n\nData Engineering Lead \u2013 With Databricks and Unity Catalog\n\nLocation \u2013 Seattle, WA (remote)\n\nDuration \u2013 12 months\n\nNeed: Design and implement data architecture and strategy, leveraging technologies such as Databricks, Unity Catalog, Privacera, and Collibra.\n\nAs a Data Engineering Lead, you will play a pivotal role in defining and executing technical deliverables while assessing complexities and levels of effort. You'll engage with both vendor and internal teams to resolve technical challenges and ensure seamless collaboration. Your leadership will extend to planning and prioritization with the SCRUM master, facilitating effective coordination between onsite and offshore teams, and driving feature prioritization through collaboration with product managers.\n\nKey Responsibilities\n\n Lead the definition and breakdown of technical deliverables, assessing complexities and levels of effort.\n Collaborate with vendor and internal teams to address technical issues.\n Drive planning and prioritization in conjunction with the SCRUM master.\n Coordinate with onsite and offshore teams for timely project delivery.\n Collaborate with product managers to prioritize features, communicate with stakeholders, and clarify requirements.\n Collect and manage user-reported issues, ensuring prompt resolution and communication.\n Define comprehensive test plans, oversee their execution, and document results.\n\n--\n\nWith Regards,\n\nNagesh G\n\nMobile: 408-381-5645\n\nDesk: 901-313-3066\n\nEmail: nagesh@conchtech.com \n\nWeb: www.conchtech.com"}, {"title": "Remote Work - Need Azure Fabric Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3759325599/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=C3iVjmcR9wjUip1nNjUiCg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3759325599", "description": "About the job\nThe client is placing heavy emphasis on Microsoft and Azure Fabric experience\n\nInformation Management Engineer \u2013 Take the identified information management sources and assist migration to the Client Data Lake, Azure Fabric. The engineer will transform information management (de-dup, clean and enrich).\n\nTake the identified data sources and bring them into our Data Lake, Azure Fabric. Once the data is in, the engineer will need to be able to transform data (de-dup, clean and enrich the data). We are focused on using a medallion data model for data architecture. The tools we are working with are: Delta Parquet files, python, spark, notebooks, T-SQL, Fabric lakehouse/warehouse, Dataflows Gen2, Data Pipelines, Direct Lake, shortcuts, ADF."}, {"title": "Data Engineer", "company_name": "Catalist", "location": "Washington, DC (Remote)", "link": "/jobs/view/3744951279/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ui1Fk6ICmnPNqAlZDZAcwA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3744951279", "description": "About the job\nFor over 17 years, Catalist has been a leader in civic data and data science innovation. Our mission is to provide progressive organizations with the data, software, and services needed to better identify, understand, and communicate with the people they need to engage and mobilize. Our clients include the largest, most influential organizations in the U.S. active in civic engagement, advocacy, and political campaigns.\n\nCatalist is home to a dedicated, creative team of technologists, data scientists, and campaign experts committed to using our talents and technology to nurture a vibrant and growing progressive community.\n\nAs a Data Engineer at Catalist, you will work closely with the Analytics and Technology departments to design, build, support, and maintain various data pipelines and processes with an end goal of providing data and intelligence to the progressive community. This role primarily involves translating data architecture designs into functional processes, code, and systems.\n\nThe ideal candidate will be a highly motivated individual with excellent technical skills, a strong desire to learn new skills, and an interest in progressive politics. Catalist values creativity and problem-solving. Our work is on the cutting edge of data-driven politics, and your support will help Democratic candidates and progressive organizations conduct successful advocacy and electoral campaigns.\n\nThis position reports to the Deputy Chief Data Officer. The Data Engineer is a part of a growing Data team that supports all underlying work at Catalist.\n\nThis position is included in our CWA bargaining unit.\n\nPrinciple Duties & Responsibilities\n\nDevelop scalable, production processes, code, and systems for data ingest, transformation, modeling, and reporting across a variety of platforms\nTranslate mock-ups and designs into functional processes, code, and systems\nCreate architecture designs when needed\nProvide quality assurance and testing on processes, code, systems, and products\nBecome an internal subject matter expert on various datasets and support other Catalist departments/teams on usage of those datasets\nExecute ad-hoc data and database maintenance tasks as requiredProject manage cross-departmental efforts, with direct responsibility for stakeholder engagement, management, and execution of technical elements\n\n\nRequirements\n\nBS or BA in a technical field, or relevant experience\n1-2 years of experience working with SQL databases\nExperience with data cleaning or ETL processes\nExperience with distributed computing systems and/or distributed datastores (particularly the Hadoop ecosystem)\nExperience managing projects\nFamiliarity with Catalist data, progressive politics, voter files, and/or commercial data\nBackground check required\n\n\nPreferred Skills & Abilities\n\nWillingness to be a problem solver and produce results in a fast paced environment\nAbility to be creative and personable, and articulate ideas clearly\nExperience working with SQL databases\nProficiency with Python or another object-oriented programming language (R, Java, Scala, etc\u2026)\nExperience working in cloud environments (AWS, GCP, etc.)\nExperience working in command line environments such as Bash\nExperience with a version control tool such as git or github\n\n\nBenefits\n\nMedical, Dental, Vision, Prescription Drug\n\nCatalist offers Medical, Dental, Vision, and Prescription Drug coverage for eligible staff and their eligible dependents. Catalist\u2019s Medical plan is a comprehensive PPO program including Prescription Drug coverage with 85% of the premium paid by Catalist. Dental and Vision coverage is provided at no cost to employees.\n\nGroup Term Life Insurance and Long-Term & Short-Term Disability Coverage\n\nGroup Term Life Insurance and Long-Term and Short-Term Disability coverage is available for eligible staff. These benefits are provided at no cost to Catalist employees.\n\n401(k) Safe Harbor Plan\n\nA 401(k) Safe Harbor Plan is available to eligible staff with a 3% contribution from Catalist from the date of hire. Employees may contribute pre-tax or post-tax from their salary up to the legal limits set forth by the IRS.\n\nMedical and Dependent Care Flexible Spending Accounts (FSAs)\n\nCatalist offers an FSA Program that gives eligible staff the ability to pay out-of-pocket medical/dental/vision/child care expenses from pre-tax earnings.\n\nTransit Benefits\n\nCatalist also makes available a Transit benefit FSA program to eligible employees using pre-tax contributions with a company match.\n\nProfessional Development and Remote Work Expenses\n\nEligible employees may be reimbursed up to $750 each year for professional development / education and remote work expenses.\n\nStudent Loan PayDown or SaveUp\n\nCatalist offers a Student Loan PayDown and College SaveUp benefit for eligible staff.\n\nVacation, Personal Leave, Sick Leave Benefits\n\nCatalist offers generous vacation benefits to all eligible staff. Eligible employees also receive:\n\n 14 Paid Holidays\n Personal Days\n Sick Leave\n Parental Leave\n\n\nHybrid Office/Remote Work\n\nCertain positions at Catalist are eligible for Office/Remote Hybrid or full Remote status."}, {"title": "Remote Entry Level Data Analyst/Engineer", "company_name": "SynergisticIT", "location": "White Plains, NY (Remote)", "link": "/jobs/view/3774171423/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=q54sk1%2FRww0z%2BC%2BE8e0jQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774171423", "description": "About the job\nAt SynergisticIT, we're all about making connections. Whatever IT goals you have, our software programmers can help achieve those. Our Software development teams can take up turnkey projects and execute them in an effective and efficient manner. If you are looking to source talent our recruiters will find the ideal IT talent for your company. What's the secret to our success? Well, it all starts with taking quality time to listen to each client's specific needs. After we have a thorough grasp of your IT goals, we can better customize our Developments as per your specific needs. We can also tailor make recruiting programs to exceed your expectations. Since our founding in 2010, SynergisticIT's strategies have earned the company an enviable position in the software development, IT staffing and IT skill enhancement fields. SynergisticIT continues to work with hundreds of satisfied American clients with our software programmers working on our projects and after gaining hands on experience on cutting edge technologies moving to contribute their skills to great clients like Apple, Google, Client, Ebay, Paypal, Kroger, the Walt Disney Company and hundreds more. If you are tired of working with inefficient programmers who take a lot of time to ramp up we want you to try us. Our software programmers can hit the ground running and get you the maximum return on your investment. You have already tried the rest its time you tried the best. SynergisticIT - Home of the Best Data Scientists and Software Programmers in the Bay Area.\n\n Why Us ? \n\nSynergisticIT has a proven track record of successfully skill enhancement and staffing IT employees for some of the world's most iconic brands. Our team takes the time to fully understand every client's needs so we could best meet your IT staffing requirements. The knowledgeable staff at SynergisticIT is always more than happy to work with clients to ensure they reach their software development goals. Besides staffing, SynergisticIT is also committed to helping young IT professionals advance their career with a robust upskill program . Everyone who goes through SynergisticIT's program learns all the skills necessary to succeed in many IT fields ranging from Java to Machine Learning. Additionally, everyone trained at SynergisticIT has been through extensive mock and technical interview screenings to bolster their career prospects. Last, but certainly not least, SynergisticIT takes great care to respect the privacy considerations for every client. All companies who work with SynergisticIT can rest assured their confidential data is protected using the most up-to-date encryption technologies. SynergisticIT also complies with all the latest NDA agreements.\n\n REQUIRED SKILLS For Java /Software Programmers \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Time series analysis \n\n We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2022 and at Gartner Data Analytics Summit (Florida)-2023 \n\nOracle CloudWorld Event (OCW) Las Vegas 2022 | SynergisticIT - YouTube\n\n https://www.youtube.com/watch?v=OAFOhcGy9Z8 \n\n https://www.youtube.com/watch?v=EmO7NrWHkLM \n\n https://www.youtube.com/watch?v=NVBU9RYZ6UI \n\n https://www.youtube.com/watch?v=Yy74yvjatVg \n\nSynergisticIT at Gartner Data and Analytics Summit 2023 - YouTube\n\nFor preparing for interviews please visit  https://www.synergisticit.com/interview-questions/ \n\n We are looking for the right matching candidates for our clients \n\n Please apply via the job posting \n\n REQUIRED SKILLS For Java /Full Stack/Software Programmer \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning Positions \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Tensorflow \n\n If you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team. \n\n No phone calls please.  Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates"}, {"title": "Data Engineer", "company_name": "Anika Systems", "location": "Leesburg, VA (Remote)", "link": "/jobs/view/3787766893/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=mApL9JnN%2B%2Fr2%2BYUPlxRYOA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787766893", "description": "About the job\nAnika Systems is a fast growing, woman-owned small business that specializes in providing innovative IT solutions for federal government agencies. Our expertise lies in accelerating delivery in Data and Analytics, Intelligent Automation, Application Development, and IT Modernization. We are currently expanding our Federal team and are seeking a passionate and talented Data Engineer. This opportunity is 100% remote.\n\nMust be a U.S. Citizen with the ability to obtain and maintain a government suitability clearance\n\nResponsibilities\n\nDesigns, develops, builds, analyzes, evaluates, and installs database management systems to include database modeling and design, relational database architecture, metadata, and repository creation and configuration management.\nDefines and oversees database organizations, standards, controls, procedures, and documentation. Provides technical consulting in the definition, design, and creation of a database environment.\nAdvises applications development staff and users on data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nDesigns and implements databases with respect to access methods, access time, batch processes, device allocation, validation checks, organization, protection and security, documentation, and statistical methods.\nUses data mapping, data mining, and data transformational analysis tools to design and develop databases.\nDetermines data storage and optimum storage requirements.\nPrepares system requirements, source analysis, and process analyses and designs throughout the database implementation.\n\nRequired Skills And Experience\n\nBA/BS and 1 year of relevant experience\nExperience with DataBricks, SQL, Python\nApplications development experience with data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nExcellent communication skills (both oral and written) with process-oriented organizational skills to ensure project success\n\nDesired Skills And Experience\n\nDemonstrated successful development experience with the full life cycle of government database management systems including database modeling and design, relational database architecture, metadata and repository creation, and configuration management.\n\nPowered by JazzHR\n\nTlKWIorucJ"}, {"title": "Remote Work - Need Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3774712978/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=r3Bp%2F0qcOD2OpC5jpbUOcA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774712978", "description": "About the job\nTitle : Data Engineer\n\nLocation : Remote\n\nDuration : 6+ months to start (long term contract (work order are extended 2x year))\n\nPlease Fill out the skill matrix: Technology Some Awareness (1) Novice (2) Intermediate (3) Advanced (4) Expert (5) SQL Tableau Desktop /Creator Tableau API Integration AWS - S3 Glue AWS - Athena Redshift AWS - Lambda AWS - SNS/SQS msExcel\n\nProfile: Data Analyst will work closely with Architects and leads in the Interflow team, key business stakeholders including program staff to support the organizations effort in the areas of data Analysis and information management. Role should have strong experience in writing SQL queries to extract data for reports. Candidate should have experience working with large set of data. As these reports are very critical for the company and we are looking for an expert who is capable of handling, consuming and extracting large sets of data for analysis and reports.\n\nMust Haves\n\nAWS - Athena\nAWS - S3\nGlue\nhands-on development of reporting applications\nMS Excel\nRedShift\nSQL\nTableau Desktop /Creator\n\nNice\n\nAWS - Lambda\nAWS - SNS/SQS\nTableau API Integration\n\nSkills\n\nBA/BS required (major in an analytical field desired) \nA minimum 10+ years work-related experience. \nExperience in hands-on development of reporting applications for the Web in a professional environment \nThe Ethos of continuous improvement and interest in learning new things.\nStrong analytical thinking and structured problem-solving ability\nAbility to handle multiple projects and assignments simultaneously and effectively in a cross-functional team environment. \nVersed on the agile methodology and best practices. \nExcellent interpersonal and collaboration skill with the ability to work with a diverse set of colleagues, across functions, from different organizations, disciplines, etc. \nSelf-starter, ability to set priorities, work independently and attain goals"}, {"title": "Data Engineer", "company_name": "Team Remotely Inc", "location": "Houston, TX (Remote)", "link": "/jobs/view/3786249829/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=78lY5Jo%2FfyRPC6TQYCdvww%3D%3D&trk=flagship3_search_srp_jobs", "id": "3786249829", "description": "About the job\nThis is a remote position.\n\n Data Engineer (US/Canada Residents Only, 1 year experience, remote)\n\nTeam Remotely Inc. is a staffing and recruitment agency that offers a comprehensive solution for talent acquisition, including sourcing, vetting, pay rolling, and managing talent. Whether you need contract staffing, direct hire, direct sourcing, talent pools, or diversity initiatives, our model can support your hiring strategy.\n\n Hiring Type: Full-Time\n\n Base Salary: $55K-$70K Per Annum.\n\nResponsibilities\n\nDesigning user interface changes for web-based DB applications. \nReviewing application requirements and interface designs. \nDeveloping and implementing highly responsive user interface components using react concepts. \nWriting application interface codes using JavaScript following react.js workflows. \nTroubleshooting interface software and debugging application codes. \nDeveloping and implementing front-end architecture to support user interface concepts. \nMonitoring and improving front-end performance. \n\nQualifications And Experience\n\nBachelor\u2019s degree in computer science, information technology, or a similar field. \nPrevious experience working as a react.js developer. \nIn-depth knowledge of JavaScript, CSS, HTML, jQuery, and front-end languages. \nKnowledge of REACT tools including React.js, Webpack, Enzyme, Redux, and Flux. \nExperience with user interface design. \nKnowledge of performance testing frameworks, including Mocha and Jest. \nExperience with browser-based debugging and performance testing software. \nTroubleshooting skills. \nProject management skills. \nProblem-solving skills. \nVerbal communication skills. \n\n Why work with Team Remotely?\n\nTeam Remotely Inc. is a staffing platform offering a seamless experience for employers and candidates. Employers can post job openings and specify their requirements, while candidates can create profiles and upload resumes.\n\nThe team of Team Remotely continuously learns and adapts based on previous successful placements, constantly improving its matching capabilities. This ensures that the recommendations provided by Team Remotely are tailored and accurate, increasing the likelihood of a successful match between employers and candidates. By providing intelligent and data-driven solutions, they strive to enhance the efficiency and effectiveness of the hiring process, ultimately helping companies find the best talent and individuals find their dream jobs."}, {"title": "Data Engineer", "company_name": "ZAG Technical Services", "location": "United States (Remote)", "link": "/jobs/view/3771403464/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=Hf0VefrKXBFIL5MtHmY0Pw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3771403464", "description": "About the job\nWe are looking for a tech savvy Data Engineer to contribute to the building, operating, and scaling of next generation data platforms and tools that will power data-driven analytics capabilities throughout the ZAG products and services portfolio, spanning areas such as business intelligence, reporting, and data analytics.\n\nThe ideal team member is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. Must be comfortable supporting the data needs of multiple clients and teams, systems, and products within the modernization of ecosystems. The right-fit person will be excited by the prospect of optimizing or even re-designing data processes to support next generation of advanced analytics capabilities, which in-turn translates to implementing strategic enhancements to maintain competitive advantage in the industry.\n\nThe successful team member will leverage their proficiency to\u2026\n\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\nBuild the analytics processes required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Data Management and cloud based \u2018big data\u2019 technologies.\nBuild analytics tools that utilize technologies to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.\nCollaborate with client teams to identify and solution regional and local requirements within the context of the standard global business model for enterprise analytics.\nParticipates in meetings, discussions, strategy sessions where changes, improvements, and enhancements are proposed, evaluated, and approved. \n\n\nThe exceptional team member will have or demonstrate progressive experiences in\u2026 \n\nBachelor\u2019s degree or higher in Computer Science or related field\nExperience working with BI and data warehousing tools\nExperience working with enterprise data, where security is paramount and data governance is critical\nAbility to work independently and in a team environment, and effectively engage all levels of the organization\nUnderstanding of distributed systems driving large-scale data processing and analytics with a successful history of manipulating, processing, and extracting value from large, disconnected datasets.\nAbility to communicate effectively (listening, presenting, and questioning)\nStrong organizational, written, and communication skills\n\n\nAbout ZAG\n\nZAG Technical Services empowers our clients\u2019 success through intelligent business solutions that increase employee productivity, network security and team collaboration. We aim to ensure that our client\u2019s IT investments drive ROI, provide a competitive advantage and help businesses grow. We do this by living our core set of values: Integrity Foremost, Client-Centered, Accountable Always, Teamwork Throughout, and Exceptional is the Goal.\n\nWhat ZAG Has To Offer\n\nCompetitive compensation package \nBenefits including medical, dental, vision, 401K and life insurance \n\n\nZAG Technical Services is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all of our employees.\n\nZAG Technical Services is an E-Verify participant and will provide the Social Security Administration (SSA) and, if necessary, the Department of Homeland Security (DHS), with information from each new employee\u2019s Form I-9 to confirm work authorization."}, {"title": "Junior Data Engineer", "company_name": "HireMeFast LLC - Career Accelerator - Land A Job", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3786249797/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D&trk=flagship3_search_srp_jobs&lipi=urn%3Ali%3Apage%3Ad_flagship3_search_srp_jobs%3BQExogrHBQoSF9zZm%2B9kufw%3D%3D&lici=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D", "id": "3786249797", "description": "About the job\nThis is a remote position.\n\n Job Title: Junior Data Engineer\n\n Employment Type: Full-Time\n\n Salary: $64,000 - $76,000 per annum\n\n Experience Required:  Minimum 1 year of project experience\n\nAbout us: HireMeFast is a leading staffing and recruitment agency specializing in connecting businesses with top-tier talent across various industries. Our mission is to bridge the gap between exceptional candidates and organizations needing their skills, expertise, and unique qualities. Our team of experienced and dedicated recruitment specialists utilizes innovative sourcing strategies, and a vast network to identify and attract top talent. We conduct comprehensive procedures to ensure that only the most qualified candidates are presented to our clients.\n\nPosition Summary\n\nJoin the fast-paced, innovative, and collaborative environment focused on providing an AIOps platform that enhances the intelligence of the CVS Health infrastructure. Work closely with subject matter experts and colleagues to build and scale out machine learning and AI solutions that will detect, predict, and recommend solutions to correct issues before system impact and enhance efficiency, reliability, and performance of CVS Health\u2019s IT operations.\n\nKey Responsibilities include:\n\nData pipeline development: Designed, implemented, and managed data pipelines for extracting, transforming, and loading data from various sources into data lakes for processing, analytics, and correlation. \nData modeling: Create and maintain data models ensuring data quality, scalability, and efficiency \nDevelop and automate processes to clean, transform, and prepare data for analytics, ensuring data accuracy and consistency \nData Integration: Integrate data from disparate sources, both structured and unstructured to provide a unified view of key infrastructure platform and application data \nUtilize big data technologies such as Kafka to process and analyze large volumes of data efficiently \nImplement data security measures to protect sensitive information and ensure compliance with data and privacy regulation \nCreate/maintain documentation for data processes, data flows, and system configurations \nPerformance Optimization- Monitor and optimize data pipelines and systems for performance, scalability and cost-effectiveness \n\nCharacteristics of this role:\n\nTeam Player: Willing to teach, share knowledge, and work with others to make the team successful. \nCommunication: Exceptional verbal, written, organizational, presentation, and communication skills. \nCreativity: Ability to take written and verbal requirements and come up with other innovative ideas. \nAttention to detail: Systematically and accurately research future solutions and current problems. \nStrong work ethic: The innate drive to do work extremely well. \nPassion: A drive to deliver better products and services than expected to customers. \n\nRequired Qualifications\n\n2+ years of programming experience in languages such as Python, Java, SQL \n2+ years of experience with ETL tools and database management (relational, non-relational) \n2+ years of experience in data modeling techniques and tools to design efficient scalable data structures \nSkills in data quality assessment, data cleansing, and data validation \n\nQualifications\n\nKnowledge of big data technologies and cloud platforms \nExperience with technologies like PySpark, Databricks, and Azure Synapse. \n\nEducation\n\nBachelor\u2019s degree in Computer Science, Information Technology, or related field, or equivalent working experience\n\nWhy HireMeFast LLC?\n\nAt HireMeFast, we understand that finding the right individuals to join your team is crucial for success and growth of your organization. We are committed to streamlining the hiring process for our clients, ensuring they have access to a diverse pool of highly qualified candidates who are a perfect fit for their specific needs."}, {"title": "Data Engineer  - Need candidates from Texas ( REMOTE )", "company_name": "Pulivarthi Group (PG)", "location": "United States (Remote)", "link": "/jobs/view/3770685212/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=9gdMwuif%2BpB%2BCQYvQi3imQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3770685212", "description": "About the job\nFollow us on Linkedin: https://www.linkedin.com/company/pulivarthigroup/\n\nPulivarthi Group LLC is a Global Staffing & IT Technology Solutions company, with our prime focus of providing world class solutions to our customers with the right talent. We combine the expertise of our team and the culture of your company to help you with the solution that is affordable and innovative using high quality standards and technologies.\n\nWe\u2019ve served some of the largest healthcare, financial services, and government entities in the U.S.\n\nData Engineer - FT only - Azure SQL with data lakes experience\n\nData Engineer -\n\nthe resource requirement for this Data Engineer role needed more clarification.\n\nOur customer is requesting a resource who knows Azure CI/CD (Continuous Integration / Continuous Deployment) to build and integrate their DevOps pipeline.\n\nOnce the DevOps pipelines are mapped, they need the Data Engineer to connect to all the Data Sources, primarily their Data Lake.\n\nThe Customer (based in Houston) is interested in a resource in a time zone not too far removed from the Central Time Zone to allow real time collaboration."}, {"title": "Data Engineer", "company_name": "Otter Products", "location": "Fort Collins, CO (Remote)", "link": "/jobs/view/3766728302/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=77U3U7CAi1PxgDmpI3%2BiNQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3766728302", "description": "About the job\nOverview \n\nOtter Products is currently recruiting for a Data Engineer to join our Data team! You can be based in our Fort Collins, CO office with flexibility to work a portion of your time remotely, or 100% Remote in the U.S.\n\nThe Data Engineer applies professional experience, concepts, and company objectives toward building and operationalizing the minimally inclusive data necessary for the enterprise data initiatives following industry standard practices and tools. The bulk of the Data Engineer's work would be in designing, managing and optimizing data pipelines, then moving these data pipelines effectively into production for key data and analytics consumers like business/data analysts, data scientists or other stakeholder that needs curated data for data and analytics use cases across the enterprise. The Data Engineer also ensures compliance with data governance and data security requirements while creating, improving and operationalizing these integrated and reusable data pipelines to enable faster data access, integrated data reuse and vastly improved time-to-solution for data and analytics initiatives. Additionally, the Data Engineer will be expected to collaborate with other data team members and data consumers working on the models and algorithms developed by them toward optimization for data quality, security, and governance to put them into production leading to potentially large productivity gains.\n\n About Otter Products \n\nOtter Products, we grow to give. From our founder's garage in 1998 to the global technology leader we are today, Otter Products continues to drive growth through innovation.\n\nThrough our industry-leading brands - OtterBox, Liviri and OtterCares - we provide our partners the number-one selling and most trusted products in our categories. Our philanthropic spirit is the foundation on which we foster our partner relationships, allowing us to grow and to give - together.\n\nBy way of our charitable arm, the OtterCares Foundation, we support our communities and invest in our future through education that inspires kids to change the world.\n\nAnd even as our global community of Otters continues to grow, our founder's core values are still at the heart of everything we do. We measure our success by our ability to give back to our communities and strengthen opportunities for all.\n\nFor more information visit otterproducts.com\n\n Responsibilities \n\nComplete analysis, design and development of BI solutions using Azure SQL Server\nFamiliarity with Data Warehouse concepts\nExperience coding the extraction, transformation, and loading (ETL) processes\nDatabase development primarily in SSIS, Data Factory and SQL\nPartner with the business to determine end user database/reporting needs and requirements\nGenerate ad hoc reports using Power BI or SSRS\nCollaborate with other developers to create and implement best approach solution(s)\nTroubleshoot Azure tools, systems, and software\nReview queries for performance issues, making changes as needed\nCollaborate with team to performance-tune Azure application as necessary\nAssist with the analysis and extraction of relevant information from large amounts of historical business data to feed data science initiatives\nParticipate and support the design and documentation of processes for large scale data analyses, model development, model validation and model implementation\nSupport and maintain a positive safety culture by following all safety policies and procedures and actively contributing to a safe working environment\nOther duties as required\n\n Qualifications \n\nBachelor's degree required. Degree in Computer Science or Mathematics preferred\nMinimum of four years of experience in an IT or analytical role required. Experience in database development, report writing and/or statistics preferred.\n\n EEO \n\nOtter Products, LLC is an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, age, marital status, pregnancy, sex, sexual orientation, gender, gender identity or expression, national origin, disability, veteran status, or any other characteristic or status protected by law.\n\n For US Based Roles Only - Compensation Range Minimum \n\nUSD $90,000.00/Yr.\n\n For US Based Roles Only - Compensation Range Maximum \n\nUSD $120,000.00/Yr.\n\n Additional Total Rewards \n\nProfit Sharing Program Eligible, Benefits Eligible - Full Time- check out otterproducts.com/careers/why for more info"}, {"title": "Software Engineer, Distributed Systems", "company_name": "Hightouch", "location": "United States (Remote)", "link": "/jobs/view/3628572539/?eBP=CwEAAAGMd4QO8APODyE_-mMOPhYyQGg5ZjVbPwq_3V7DK0B2bLAf10pcedM9S2T71X5i4TiJuEwOzWBbSiTKbLuk-VD-_Y-dN5uyf8YpYkHYqEYFxySMS9zr2hRjNrxX9gXbxssh35TFK9YTNV2WeH2Ncb675JgFbJTgNWSZySFC6rrdJGmdSDTr2q5d3VoxORz3GjsDbzjeyZQCvQ07RqYw7oujWSgslycs2LMCY_-N0NEj6e0PvbaYKiZ_5IW34bGVSYLXpHwqa8oOcNttYZ90UA3E_7sDSFzABdIVlfrp5zzERrIYy5sh2MIInxHdDoKgQw8VmOn5uM4S0tXUl-i7LnN0_W0NfPBmLiVDNuveIOm4MfDNPxfe5QKPk6FUAVpvgbcoJn9yRyE&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=rph%2FT6Q6Vkp6YoggJf9cJQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3628572539", "description": "About the job\nAbout Hightouch\n\nHightouch\u2019s mission is to empower everyone to take action on their data. Through our Reverse ETL platform, business and data users can seamlessly sync data from where it resides, such as warehouses and databases, to where it is needed, including operational systems and SaaS tools. Traditionally, acting on data has required engineering time and bandwidth, and left most business users stuck with charts and reports that are unable to take automated action on their data. With Hightouch, every business user, without writing any code, can activate data to streamline critical processes, improve marketing performance, and scale operations.\n\nOur team operates with a focus on making a meaningful impact for our customers. We believe in approaching challenges with a first principles thinking mindset, moving quickly and embracing our value of efficient execution, and treating each other with compassion and kindness. We look for team members that are strong communicators, have a growth mindset, and are motivated and persistent in achieving our goals.\n\nHundreds of companies use Hightouch, including Spotify, Ramp, Retool, NBA, Plaid, and Betterment. We\u2019re based in San Francisco, are remote-friendly, and backed by leading investors such as Amplify Partners, ICONIQ Growth, Bain Capital Ventures, Y-Combinator, and Afore Capital.\n\nAbout The Role\n\nWe are looking for a distributed systems engineer to work on the systems that power the syncing engine our customers and other engineering teams rely on. This presents an exciting challenge where you can apply your expertise in distributed systems, performance optimization, and troubleshooting to push the boundaries of what is possible and meaningful improve our ability to move massive amounts of customer data. This role also provides a unique opportunity to work on a multi-cloud and multi-region infrastructure that supports a global customer base.\n\nWe believe in enabling our engineers to do their best work for our customers by giving them extremely high levels of ownership and autonomy. This comes in different forms: you will own and deliver projects from start to finish, you will work directly with customers to solve their hardest scaling problems, and you will have a lot of influence over what we work on as a team and company.\n\nSome of the problems we\u2019ll be working on include:\n\nSync Speed: Customers want to sync a lot of data to important destinations like Facebook and Snapchat, which requires us to analyze every part of our syncing process and find where we can optimize to sync data more quickly\nStreaming Syncing: We currently sync data from sources in batch only, but envision a world where we also do syncs in real-time and support streaming sources like webhooks and queues\nScalability and Reliability: As part of our rapid growth, we\u2019re always evaluating current future bottlenecks in scaling and reliability, and architecting for the next order of magnitude of growth\nPersonalization API: Our Personalization API product provides a low latency caching layer on top of data warehouses that enable customers to use their data warehouse for real-time personalization use cases with \nMulti-Region and Multi-Cloud: Supporting our multi-region and multi-cloud backend, including extending it to launch Hightouch on in new regions to support data residency requirements of our global customer base\n\nWe are looking for talented, intellectually curious, and motivated individuals who are interested in tackling the problems above. This is a senior role, but we focus on impact and potential for growth more than years of experience. The salary range for this position is $170,000 - $240,000 USD per year, which is location independent in accordance with our remote-first policy.\n\nAbout You\n\nYou are an engineer with a passion for solving hard technical problems that generate real value for customers. You\u2019re motivated by high ownership and are comfortable in a fast-paced, startup environment.\n\nYou have experience and comfort with distributed systems and high-scale systems or the ability and desire to build this experience quickly. This experience can take different forms:\n\nYou\u2019ve built syncing engines that process and move terabytes of data per day in a scalable way\nYou\u2019ve created event collection or data streaming services that power company critical systems\nYou\u2019ve owned systems that have undergone orders of magnitude of growth and helped them scale accordingly\nYou have relevant skills and are a fast learner who is excited to take on the challenges we face\n\nInterview Process\n\nOur goal with the interview process is to balance speed with giving both parties opportunities to assess whether there is a strong mutual fit. We will ask you questions, but we want you to ask us questions! Our technical interviews focus on how you design systems because we believe this is the best way for us to see how you work and for you to see how we collaborate. We don\u2019t ask you to write code to solve technical brainteasers that don\u2019t appear in your day to day job.\n\nRecruiter screen [30m]: Introductory call with our recruiting team to get to know each other and see if the role could be a good mutual fit.\nSystems design screen [45m]: Designing a data processing feature end-to-end.\nHiring manager interview [30m]: Chat with hiring manager about past experiences and future operating preferences to assess fit on company values and operating principles.\nSystems design interview [90m]: Work with the interviewer to architect a system at a conceptual level. The problem will be at a pretty high level - and have both product and customer requirements as well as technical."}, {"title": "Entry-Level Data Scientist Engineer", "company_name": "Phoenix Recruitment LLC", "location": "Texas City, TX (Remote)", "link": "/jobs/view/3788727625/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=8iwtytHsCu4wMwJ4fj3pxw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3788727625", "description": "About the job\nThis is a remote position.\n\nTitle - Entry-Level Data Scientist Engineer, 1 year of project experience\n\nEmployment Type: Full-time\n\nBase Salary: $60K-$70K\n\nPhoenix Recruitment offers a variety of recruiting services to assist both employers and employees. They specialize in marketing open positions, recruiting, and helping employers to find qualified candidates across various industries. Phoenix Recruitment has expertise in streamlining the hiring process. They can help ensure that the process is efficient, well organized, and compliant with relevant regulations.\n\nSkills and Abilities:\n\nStrong knowledge of R or Python for data analysis and modeling. \nProficiency in statistical programs such as R, SAS, MATLAB, or Python. \nFamiliarity with spreadsheets (VBA) and database applications (Access, Oracle, SQL, or equivalent technology). \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \nAbility to learn new methods quickly and work under deadlines. \nExcellent teamwork and communication skills. \nStrong analytical and problem-solving abilities. \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \n\nPreferred:\n\nKnowledge of actuarial concepts and life, health, and/or annuity products. \nExperience with statistical modeling techniques such as GLM, Decision Trees, Time Series, Regression, etc. \nFamiliarity with Microsoft DeployR. \nExposure to insurance risk analysis. \nBasic experience in computational finance, econometrics, statistics, and math. \nKnowledge of SQL and VBA. \nFamiliarity with R or Python for predictive modeling \n\nWhy Phoenix Recruitment LLC?\n\nPhoenix Recruitment often has an extensive network of employers and candidates. This network allows them to tap into a pool of qualified candidates and connect them with suitable job opportunities. They can also leverage their connections to help employers find the right talent efficiently. Outsourcing the recruitment process to a specialized agency can save your time and resources, avoid delays, reduce administrative burdens, and increase the chances of finding the right fit for your organization."}, {"title": "Data Engineer - Remote Work", "company_name": "BairesDev", "location": "United States (Remote)", "link": "/jobs/view/3783162297/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=OtoCrAAHZFomVup04fy35g%3D%3D&trk=flagship3_search_srp_jobs", "id": "3783162297", "description": "About the job\nWho We are\n\nBairesDev is proud to be the fastest-growing company in America. With people in five continents and world-class clients, we are only as strong as the multicultural teams at the heart of our business. To consistently deliver the highest quality solutions to our clients, we only hire the Top 1% of the best talents and nurture their professional growth on exciting projects.\n\nWe are looking for a Data Engineer to join our Development team and participate in different projects made up of multicultural teams distributed throughout the world.\n\nWhat You Will Do\n\n Interview other Data Scientists and Data engineers to vet their technical abilities.\n Input data from those interviews into the product and help develop a predictive, fair, and enjoyable solution.\n Work together with developers, tech leads, and solution architects to build applications.\n Improve existing structures by adding new functionalities or proposing technological updates so that as a result, the impact of your contribution is significant in the core of each business.\n Improve the user experience on scalable and high availability platforms, contributing to the key differential of each business.\n\nHere\u2019s What We Are Looking For\n\n 5+ years of experience working as a developer.\n 3+ years of experience working as a Data Scientist / Data Engineer.\n Proficient with analysis, troubleshooting, and problem-solving.\n Hands-on experience with managing data loads and data quality.\n Advanced English level.\n\nHow we do make your work (and your life) easier:\n\n 100% remote work.\n Hardware setup for you to work from home.\n Flexible hours - make your schedule.\n Paid parental leave, vacation & holidays.\n Diverse and multicultural work environment.\n An innovative environment with the structure and resources of a leading multinational.\n Excellent compensation \u2014 well above the market average.\n Here you can grow at the speed of your learning curve.\n\nOur people work remotely but with a consistent and robust culture that promotes diversity and teamwork. To continue being the leading software development company in Latin America, we want to ensure that every BairesDev member gets the best growth and professional development opportunities in a diverse, welcoming, and innovative environment.\n\nEvery BairesDev team member brings something unique to our company.\n\nWe want to hear your story. Apply now!"}, {"title": "Data Engineer (Remote)", "company_name": "Cascade Debt", "location": "United States (Remote)", "link": "/jobs/view/3755336571/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=pmUtn1j2zyPr25CNPh2RNw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3755336571", "description": "About the job\nDo you want to work in a small team where you can make a real difference in a company? Would you embrace the challenge of building a high impact platform that is used globally? Then we want to speak with you!\n\nAbout Cascade\n\nCascade is a fintech startup backed by Canadian and US investors that empowers companies to grow by democratizing access to institutional debt. We are building tools that modernize how companies raise and manage debt, lowering the barriers to entry to the $7 trillion specialty finance and alternative credit market for companies around the world. Founded just last year, we are gearing up for our next phase of technical development and are seeking a talented Data Engineer to lead the way.\n\nAbout The Role\n\nAs a Data Engineer you\u2019ll be an early member of a growing team building a pioneering platform in the debt infrastructure space. You will design, build, and launch efficient, scalable, and reliable data pipelines to move and transform data .\n\nWhat You\u2019ll Do\n\n Building databases, data lakes and data ingestion pipelines to integrate customer databases and datasets to our systems \n Ingesting financial datasets from external customers, then u pdating and maintaining accurate and complete data mappings to ensure that our product s are displaying high quality data \n M onitoring and alerting across our data pipelines in order to make sure that our data ingests are reliable and correct \n Translat ing business goals and stakeholder requirements into new data flows and deliver analytical insights ensuring that the data flows create real business value continuously \n\nSkills And Experience\n\n 2 + years experience in a data engineering or data analyst role \n E xperience with relational SQL and NoSQL databases, including PostgreSQL , MySQL, MariaDB, MongoDB, Bigtable, etc. \n E xperience working with ETL technologies , such as Databricks, Fivetran , or dbt \n Proficiency in writing SQL queries and knowledge of analytical data warehouses such as RedShift, BigQuery , and Snowflake \n Some experience with CI/CD automation such as GitHub Actions \n Developing APIs and integrating with 3rd party APIs \n Bonus: experience in fintech / SaaS / credit analysis \n\nPeople Who Thrive At Cascade Are\n\n Self-starters, who take the initiative to tackle challenges in a remote work environment \n Good communicators, who can operate without ego to discuss, learn, grow, and help others do the same \n Passionate about creating great digital experiences for users \n Problem solvers who are not satisfied with the status quo \n\nBenefits\n\nRemote: we are a remote-first company and are very flexible on hours as long as things get done \nHome-office: there\u2019s a $1000 USD home office allowance to set yourself up \nEquity: we expect you to have an owner-mentality, and have the equity plan to match \nBenefits: health, dental, vision, and more \nPerks: we offer free lunches weekly and off-site trips \nJob satisfaction: we offer autonomy, ample opportunities for mastery, and an opportunity to make a difference for companies around the world"}, {"title": "Data Engineer", "company_name": "ISC (Integrated Specialty Coverages, LLC)", "location": "Carlsbad, CA (Remote)", "link": "/jobs/view/3778293528/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ew7%2FfEuXSujs5IGabdqUyw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3778293528", "description": "About the job\nAbout Integrated Specialty Coverages\n\nIntegrated Specialty Coverages LLC (ISC) is a growth stage technology and data-driven insurance company leading innovation in the market.\n\nBacked by one of the leading private equity firms, KKR, and led by a forward-thinking management team, ISC is combining the worlds of insurance and technology to create an Insurtech powerhouse. As a leading online distributor of insurance products for a range of industries and \"Main Street USA\", we are looking for the right people to help us in our mission of achieving exponential growth. We strive to be the number one place to go for brokers and agents to source insurance. To accomplish this, we're building a digitally focused team that deeply understands the intersection between user experience, data, and AI/ML to optimize the way we engage with our customers and partners.\n\nJob Summary\n\nThe Data Engineer role will be the technical liaison between multiple groups including a data science team, the engineering team, product management, and business stakeholders. You do not need any insurance knowledge prior, however, you must quickly dive deep into the insurance world and ask questions to become a subject matter expert. You will be responsible for building a data platform to facilitate the data science team. You must be a self-starter that can build out features such as a data pipeline from scratch. There will be support from both engineering and data science for any buildout.\n\nJob Responsibilities\n\nCreate and maintain optimal data pipeline architecture. \nAssemble large, complex data sets that meet functional / non-functional business requirements. \nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. \nBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using AWS technologies, SQL, and Python. \nWork with stakeholders including the Executive, Product, Data Science, and Engineering teams to assist with data-related technical issues and support their data infrastructure needs. \nWork with data science and analytics teams to strive for greater functionality in our data systems. \n\nMinimum Qualifications:\n\nAdvanced working SQL knowledge and experience working with relational databases, strong query authoring (SQL) as well as working familiarity with a variety of databases (Snowflake, Redshift, MySQL, MSSQL, etc.)\nExperience building and optimizing data pipelines, architecture and data sets. \nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. \nBuild processes supporting data transformation, data structures, metadata, dependency, and workload management. \nA successful history of transforming, processing and extracting value from large disconnected, datasets from a variety of data sources (Flat files, Excel, databases, etc.)\nStrong analytic skills related to working with unstructured datasets. \nStrong project management and organizational skills. \nExperience supporting and working with cross-functional teams in a dynamic environment. \nAbility to mentor/guide/collaborate with other team members. \n\nWe are looking for a candidate with 3-5 years of experience in a Data Engineer role. They should also have experience using the following software/tools:\n\nExperience with relational and MPP databases, including Snowflake and MySQL. \nExperience developing software in an agile environment from the requirements stage to production. \nExperience with version control: git\nExperience with container technologies: Docker\nExperience with data pipeline and workflow management tools: Airflow, Jenkins, AWS Glue, Azkaban, Luigi, etc. \nExperience with AWS cloud services: EC2, ECS, Batch, S3, EMR, RDS, Redshift\nExperience with other cloud services: Snowflake, Airflow\nExperience with object-oriented/object function scripting languages: Python, Java, C++, etc. \nExperience with data modeling and data warehouse design\nExperience with data visualization tools (PowerBI, QuickSight)\n\nThe starting annual pay scale for this position is listed below. Actual starting pay will be based on factors such as skills, qualifications, training, and experience. In addition, the company offers comprehensive benefits including medical, dental and vision insurance, 401(k) plan with match, paid time off, and other benefits.\n\nISC's salary ranges are determined by role and level. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations and could be higher or lower based on a multitude of factors, including job-related skills, experience, and relevant education or training.\n\nNational Pay Range\n\n$100,000\u2014$110,000 USD\n\nBenefits of Working at ISC\n\nCompetitive vacation and flexible working arrangements\nComprehensive and inclusive health benefits\nA variety of professional development and mentorship opportunities\nChoice of technology whether at home or in the office\n\nISC's Ownership Behaviors\n\nDo the Right Thing *Be Relentless. Pursue Excellence. *Personal Responsibility & Own Outcomes\nTry Fast. Learn Fast. Fail Fast. Think Big. *Build Extraordinary Owners & Join Forces *Grit & Determination\n\nApplicants with disabilities may contact ISC HR department via e-mail, and other means to request and arrange accommodation. If you need assistance to accommodate a disability, you may request accommodation at any time. Please contact ISC at HR@ISCMGA.com.\n\nISC believes in creating long-term relationships by being responsive and relevant and by consistently delivering value to our community of customers. Specifically, with our employees, we focus on attracting, developing, and retaining the best talent for our business, challenging our people, demonstrating a \"can-do\" attitude, and fostering a collaborative and mutually supportive environment. ISC is an equal opportunity employer and a participant in the US federal E-Verify program (US).\n\nDiversity creates a healthier atmosphere: All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, gender, gender identity, sexual orientation, marital status, medical condition, genetic information, mental or physical disability, military or veteran status, or any other characteristic protected by local, state, or Federal law."}, {"title": "Data Engineer -Remote", "company_name": "RIT Solutions, Inc.", "location": "Denver, CO (Remote)", "link": "/jobs/view/3768007496/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=1hQH8EDaII1eJ7nQ9BACQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3768007496", "description": "About the job\nDuration: 6+ months CTH\n\nMust have a valid LinkedIn profile\n\nRequired Skills\n\n 3-6 years of experience (no more than 6 years)\n Start up environment experience\n Python, SQL, and DBT"}, {"title": "Big Data Engineer - W2", "company_name": "Megan Soft Inc", "location": "United States (Remote)", "link": "/jobs/view/3674845652/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fCAxh4N%2F4YZJ5aEIASaeeA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3674845652", "description": "About the job\nTitle: Big Data Engineer - W2\n\nLocation: Dearborn, MI (Remote)\n\nDuration: 12+ Months\n\nDirect Client: Ford Motors\n\nMust have skills: Should be able to handle GCP requirements\n\n\"Translates business requirements to build repeatable, sustainable, efficient, coded processes that can be productionized by Software Engineers and readily modified by other Data Engineer Creates POC processes in Dev/QA and works with Software Engineers to productionize and align best coding and process practices that maximize efficiency, speed, stability, system resources and capabilities Awareness of and compliance with data privacy, security, legal and contractual guidelines Incorporates data quality and privacy checks/alerts to minimize bad data being consumed by end users, models and dashboards, and to protect customer data; revises checks for new data issues Validates data products and pipelines are functioning as expected following system or application upgrades, source changes, etc. Works closely with Architects if required to align systems, tools and applications being utilized with business use case and performance requirements Provides data product support and maintenance Establishes, tracks and monitors KPIs related to specific data products and deliverable Participate in iteration cadence activities (Iteration Planning, task estimation, daily scrum, demos, etc.). Will be a part of on-call team to resolve critical production support tickets during business hours and off hours.\n\nSkills Required\n\nExperience with Big Data Technologies (Hadoop , ETL Tool, Spark. Hive etc..) Someone who understands Cloud and has experience working with Cloud Platforms (preferably GCP) Should be able to handle GCP requirements - ingesting into GCS/ Big Query, working with GCP components Hands-on in Data Flow, Cloud Storage (GCS), Big Query and GitHub. Proficient in Python and SQL.\n\nSkills Preferred\n\nManages ETL: uses programming and tools for data ingestion, configures pipelines, applies transformations and decoding, integrates and fuses data, moves and securely deliver Experience NoSQL databases\n\nAdditional Information\n\nIf the candidate is remote only, please indicate \"\"100% Remote\"\" under candidate's name on resume. Other similar schedule notations can be \"\"Local Candidate\"\" or \"\"Hybrid Candidate.\"\" ***HRA Test Required***\""}, {"title": "Remote Opportunity: Quantexa Data Engineer", "company_name": "SPAR Information Systems LLC", "location": "United States (Remote)", "link": "/jobs/view/3680073191/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ioykVCZSOQ7BK5%2FwRQHOWg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3680073191", "description": "About the job\nHello All, Hope you are doing great!! Please go through the job descriptions and let me know your interest. Role: Quantexa Data engineer  Location: Remote Duration: Long Term Contract Job Description: 1. End to End usage of Quantexa including guiding our client team on how to use from both front end and data ingestion Reason they are asking for Certification 2. Resolve Entity issues with data 3. Knowledge of Scala and Hive which requires coding experience using parquet 4. Advanced SQL knowledge requiring writing SQL queries 5. Experience having worked with GCP data lake 6. Quantexa certification required\n\nThanks & Regards,\n\nSatnam Singh\n\nDirect: 201 623 3660\n\nEmail : Satnam.singh@sparinfosys.com"}, {"title": "Remote Work - Need Staff Data Engineer - USC only", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3746283980/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=CvIuLBNO%2BPMGi4IXVRkP0A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3746283980", "description": "About the job\n100% Remote, client is in NY\n\nLook for strong job history, Python, Machine Learning, SQL, 10/10 comms, etc. \n\nMUST be an Engineer \n\nSummary\n\nWe're looking for an experienced Data Engineer to join our client's growing team of top notched IT professionals. You will work closely across multiple groups including Editorial and Audio, Marketing, Advertising, etc. to achieve their goals and objectives. The Data Engineering team builds tools throughout the stack while sharing knowledge across these departments.\n\nAs the company grows, they're looking for Data Engineer who will help solidify and expand our pipelines and maintain their data warehouse. Their business model is based on performing complex and very detailed analyses of how our products perform with their subscribers. You'll be responsible for working alongside analysts, product, marketing, and other teams to define new data collection components and measurement schemes that support each new product and feature.\n\nHow Can Add Value To Their Mission\n\nCreate and maintain data pipelines to provide insights and drive business decisions\nEstablish data warehousing strategy (ex. Kimball, Data Vault, etc.)\nMaintain data infrastructure on our AWS accounts\nCollaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.\nWrite unit/integration tests, contributes to engineering wiki, and documents work\nImplement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it\n\nWhat You'll Need To Succeed\n\n7+ years of industry experience measuring product performance and user behavior\nExperience working with a variety of data management technologies, including RedShift, Kinesis/Kafka, Glue, Spark, Postgres, Airflow, dbt, and others.\nExperience implementing BI reporting tools such as Looker\nExperience interfacing with engineers, product managers and analysts to understand data needs\nKnowledge of a variety of measurement beacons, SDKs, APIs including Google Analytics, Amplitude, Braze, Stripe, email service providers\nA commitment to building secure, resilient, fault-tolerant architectures, with clear documentation and procedures in place for support\nA focus on accuracy and detail as you build, ensuring data pipelines produce clear, predictable results\nUnderstanding of the typical metrics a subscription and advertising-supported business needs to measure success\nExperience with Client techniques as applied to behavioral segmentation or anomaly identification is a definite plus\nFamiliarity using developer tools that increase productivity and facilitate the development of resilient code (eg. Docker, CircleCI, Serverless) is a plus"}, {"title": "ENTRY LEVEL DATA ENGINEER (REMOTE)", "company_name": "SynergisticIT", "location": "Jacksonville, FL (Remote)", "link": "/jobs/view/3767587909/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=om7L3CnowiC4PLIcpZD70A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3767587909", "description": "About the job\nSYNERGISTICIT wants every candidate to know that the Job Market is Challenging and to stand out, you need to have exceptional skills and technologies and that's where we come in to make sure you get the attention which you need\n\nSynergisticIT understands the complex nature of the job market and how difficult it can be to secure a position, especially for fresh graduates. Therefore, we assist and help tech-savvies to convert their passions into professions. We go above and beyond to keep you working in your niche.\n\nAs we focus on long-term success, we provide complete career development solutions. From job search to upskilling portfolio and interview preparation, we can guide you at every step of your career.\n\nSynergisticIT spares no efforts to connect you with a large network of tech giants, including Google, Apple, PayPal, Dell, Cisco, Client, etc. Presently, we are actively looking for ENTRY LEVEL DATA ENGINEER a driven mindset. Get the right opportunity and gain experience in building web-centric solutions on Java.\n\nWho Should Apply : Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or anyone looking to make their career in IT Industry\n\nWe also assist in filing for STEM extension and H1b and Green card filing.\n\nCandidates who are serious about their future in the IT Industry and have set big goals for themselves.\n\nCandidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. We also offer Skill Enhancement Programs if the candidates are missing skills or experience which our clients need with great outcomes\n\nCandidates can benefit from skill enhancement if they fall into the below categories. If they are qualified with enough skills then no need for skill enhancement\n\nCandidates Who Lack Experience\n\nHave had a break in careers\n\nLack Technical Competency\n\ncandidates who want to get employed and make a career in the Tech Industry \n\nPlease Also Check The Below Links\n\nhttps://www.synergisticit.com/candidate-outcomes/\n\nhttps://www.synergisticit.com/java-track/\n\nhttps://www.synergisticit.com/data-science-track/\n\nhttps://www.synergisticit.com/which-is-the-best-option-for-tech-job-seekers-staffing-companies-consulting-companies-bootcamps-or-synergisticit/\n\nhttps://www.synergisticit.com/contact-us/\n\nIf the skills are not a match candidates can opt for Skill enhancement. Or their resume can be sent out to clients to see if responses are achievable\n\nREQUIRED SKILLS For Java/Software Programmers\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Core Java , javascript , C++ or software programming\nSpring boot, Microservices and REST API's experience\nExcellent written and verbal communication skills\n\nFor data Science/Machine learning\n\nRequired Skills\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Statistics, Python, data visualization tools\nExcellent written and verbal communication skills\n\nPreferred skills: NLP, Text mining, Tableau, Time series analysis\n\nTechnical skills are required by clients for selection even if its Junior or entry level position each additional Technical skill helps a candidate's resume to be picked by clients over other job seekers.\n\nClients hire candidates with the right technical skills which they need and reject candidates who lack the required technical skills.\n\nNo third party candidates or c2c candidates\n\nPlease apply to the posting\n\nNo phone calls please. Shortlisted candidates would be reached out"}, {"title": "Data Engineer - Remote | WFH", "company_name": "Get It Recruit - Information Technology", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3782241984/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=BgBf1kSAAX%2BSuw6inJsIVw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3782241984", "description": "About the job\nJoin an innovative team at a leading company in the engineering and manufacturing industry, where we are committed to leveraging data for continuous improvement and operational excellence. This hybrid position allows you to enjoy the flexibility of remote work on Mondays and Fridays.\n\nWhy Join Us\n\nAt our company, we prioritize your well-being and professional growth. We offer a comprehensive benefits package, including excellent health benefits, a 6% 401(k) match, generous paid vacation and sick time, 12 paid holidays, and more.\n\nRole Overview\n\nAs a Data Engineer, you will play a pivotal role in transforming raw data into actionable insights, driving improvements in engineering and manufacturing processes, enhancing product quality, and optimizing operational efficiency. Collaborate with dynamic cross-functional teams to extract, analyze, and visualize data, influencing strategic decisions through your valuable insights.\n\nKey Responsibilities\n\nData Collection and Integration:\n\nGather, clean, and integrate data from diverse sources, including production databases, sensor data, and external datasets.\n\nData Analysis\n\nConduct exploratory data analysis to identify patterns, trends, and anomalies.\n\nPerform statistical analysis and hypothesis testing to derive meaningful insights.\n\nData Visualization\n\nCreate informative and visually appealing dashboards and reports using tools like Tableau, Power BI, or Python libraries.\n\nPredictive Modeling\n\nDevelop predictive models to forecast manufacturing performance, quality, and operational outcomes.\n\nRoot Cause Analysis\n\nInvestigate production issues and quality concerns by analyzing data to identify root causes and recommend corrective actions.\n\nPerformance Monitoring\n\nEstablish and maintain key performance indicators (KPIs) to track manufacturing and engineering metrics.\n\nGenerate regular reports to communicate performance trends to stakeholders.\n\nCross-functional Collaboration\n\nCollaborate with engineering, manufacturing, and quality assurance teams to understand their data needs and provide analytical support.\n\nContinuous Improvement\n\nSuggest process improvements and optimizations based on data analysis.\n\nData Security And Compliance\n\nEnsure data security and compliance with relevant regulations, such as GDPR or industry-specific standards.\n\nInfrastructure And Development\n\nBuild and maintain the infrastructure for collecting, storing, and processing data.\n\nDesign, develop, and manage data pipelines, ETL processes, and data warehouses.\n\nRequired Experience\n\nProven experience as a Sr. Data Analyst and/or Data Engineer in an engineering or manufacturing environment.\n\nProficiency in data analysis tools and programming languages, such as Python, R, SQL, and Excel.\n\nStrong data visualization skills with tools like Tableau or Power BI.\n\nExcellent knowledge of statistical analysis and machine learning techniques.\n\nAbility to communicate complex findings and insights effectively to both technical and non-technical stakeholders.\n\nStrong problem-solving skills and attention to detail.\n\nFamiliarity with manufacturing processes and quality control is a plus.\n\nExperience with enterprise ERPs (SAP S4HANA, Oracle EBS), CRMs (Salesforce, Hubspot), and IoT.\n\nProficient in working with cloud computing platforms including AWS, GCP, and Azure.\n\nExpertise in configuring and optimizing cloud-based data solutions for scalability, reliability, and performance.\n\nBachelor's Degree in Data Science, Statistics, Engineering, or Computer Science Required.\n\nApply Now and be part of a team shaping the future of data-driven innovation in engineering and manufacturing!\n\nEmployment Type: Full-Time"}, {"title": "Entry Level Software Engineer - Data Backend Engineer (Remote - Canada)", "company_name": "Yelp", "location": "United States (Remote)", "link": "/jobs/view/3752881757/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fankeoxifvhpTzOYvowHYQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3752881757", "description": "About the job\nYelp engineering culture is driven by our values: we\u2019re a cooperative team that values individual authenticity and encourages creative solutions to problems. All new engineers deploy working code their first week, and we strive to broaden individual impact with support from managers, mentors, and teams. At the end of the day, we\u2019re all about helping our users, growing as engineers, and having fun in a collaborative environment.\n Interested in exploring data and finding innovative ways to collect it, curate it, and make it easily accessible for others to use? How about doing that at an enormous scale? Yelp\u2019s datasets contain billions of interactions between users and local businesses around the globe. If working with our vast amount of data sounds exciting, come and join us!\n\nThe Marketplace Data Observability team is responsible for managing, governing, and monitoring all of Yelp\u2019s Ads data to protect our revenue stream and also support offline analysis that aids in making long term company decisions. We build tools that help us to better understand our data quality, and we strive to uncover data issues before they impact our consumers.\n\nAs a Data Backend Engineer on this team, you will be responsible for building elegant and scalable data products that serve critical, up-to-date, structured information to support different types of analytics within Yelp. As a core contributor to our growing data modeling and data warehousing engineering efforts, you will help design and own mission-critical data flow pipelines and datastores to enable decisions including effective A/B testing and company investments.\n This opportunity is fully remote and does not require you to be located in any particular area in Canada. We welcome applicants from throughout Canada. We\u2019d love to have you apply, even if you don\u2019t feel you meet every single requirement in this posting. At Yelp, we\u2019re looking for great people, not just those who simply check off all the boxes.\n  Build systems that can effectively store and crunch terabytes of data.\nDesign and develop data models for efficient data storage, retrieval, and reporting.\nCreate and maintain conceptual, logical, and physical data models using industry-standard modeling tools.\nCollaborate with cross-functional teams, including engineers, data analysts, business analysts, and data scientists, to understand data requirements and translate them into effective data models.\nParticipate in data integration efforts, including ETL processes and data migration.\nStay up-to-date with industry best practices, emerging technologies, and trends related to data warehousing.\nSupport on-call rotations as needed to operate the team.\n  Understanding of high performing and scalable data systems.\nExperience in building and orchestrating ETL pipelines.\nExperience with Data Lake or Data Warehouse landscape.\nA hunger for tracking down root causes and fixing them in systematic ways.\nAbility to communicate effectively to technical and non-technical cohorts alike. \nExposure to some of the following technologies: Python, AWS Redshift, AWS Athena / Apache Presto, Big Data technologies (e.g S3, Hadoop, Hive, Spark, Flink, Kafka etc), NoSQL systems like Cassandra, DBT."}, {"title": "Data Engineer (Associate Consultant), June 2024", "company_name": "UDig", "location": "Nashville, TN (Remote)", "link": "/jobs/view/3716039960/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=XUjo40kU55HnDCqJ%2F3FdPg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3716039960", "description": "About the job\nCan UDig It? \n\nUDig designs, builds, and implements technology solutions that deliver on business objectives. People join our team because they are excited to have an impact on the future of our organization, passionate about delivering meaningful results, and energized by solving complex problems. Oh, and they like working with other driven, smart, and authentic people!\n\nUDig is proud to be recognized as a \"Best Places to Work\" in Virginia and Tennessee. We're committed to fostering a transparent environment where leadership is accessible, professional development opportunities are abundant, and you are empowered to achieve your career goals. You'll collaborate, develop deep relationships with teammates, and accelerate your success while having fun!\n\nIf you're graduating in May 2024 and excited by the idea of impacting organizations by building data solutions to achieve business goals, you should consider joining our growing technology team.\n\nHow Things Go Down\n\nAs a new, entry-level teammate, you'll take part in our \"Breaking Ground\" training program designed to help you quickly get up-to-speed on UDig's delivery approach within a project team. The program covers:\n\nUDig delivery process and methodologies such as gathering requirements and delivering projects using agile\nTechnical guidance and instruction on the best practices from Data leaders at UDig\nDelivering executive and technical presentations\nCoaching to prepare you to for success on your first client project at UDig\n\nA typical day might entail:\n\nProject standups and check-ins\nDesign and development of data pipelines, BI reports, and more\nPlanning and participating in project demos\nContributing to internal UDig projects\nCareer and job growth through UDig-provided training\nCreating long-lasting friendships with teammates through fun activities as well as your projects (including social events, winning a prize for guessing the weight of pumpkin, or sharing experience on cool tech side projects)\n\nUDig believes it is important for Associate Consultants going through our Breaking Ground program to reside in one of our major markets (Richmond, VA or Nashville, TN metro area) for at least the first year of employment.\n\nWant to learn more? Hear directly from our teammates by checking out our 'A Day in the Life' blog series: https://www.udig.com/digging-in/breaking-ground-2023/\n\nSound Interesting? Here's the Technical Piece\n\nBelow are a few boxes you should be able to check if\u202fyou're interested: Don't check every box? Go ahead and apply!\n\n1+ year of experience with relational database management systems such as Microsoft SQL Server, MySQL, or Oracle\nUnderstanding of the full lifecycle of design, development, and implementation of data solutions including architecture design, development, testing\nSome experience in ETL development or reporting is a plus\nAbility to present technical ideas and high-level concepts and solutions to internal and external team members with varying degrees of technical knowledge\nBachelor's Degree in in computer science, information technology, or any science or business discipline\nAbility to take your work seriously but also seriously enjoy a good time with your UDig colleagues\n\nWhat's in It For You? \n\nAt UDig, you will build your career alongside talented and experienced developers while gaining experience and having fun! We provide continuous challenges and professional development opportunities. Additionally, we offer: \n\nCompetitive salary, merit reviews, and career advancement paths\nFlexible, hybrid environment\nIndividual $1500 Training Budget\nRegular team building and social activities (virtual and/or in-person)\nTransparent culture with strong communication and access to leadership\nGreat benefits like\nGenerous Paid Time Off, company holidays, and parental leave\nMultiple Single and Family Health Insurance plans to choose from\nDental & Vision coverage\nShort-Term and Long-Term disability\nOptional accident and critical illness coverage\nMatching 401(k)\nand more!"}, {"title": "Expression of Interest: Data Engineer", "company_name": "Fingerprint for Success (F4S)", "location": "New York, NY (Remote)", "link": "/jobs/view/3787769880/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=l%2FEDpQqUNmCttGdIyTk7Cg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787769880", "description": "About the job\nWe are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.\n\nThe F4S Talent Pool is a pilot project designed to:\n\nHelp job seekers get discovered by our partners based on their anticipated hiring needs.\nProvide optional support and resources for job seekers in their career endeavors.\nHelp individuals understand, and bring out the best in themselves and each other.\n\nThe F4S Talent Pool process:\n\nOnce you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.\n\nAbout Fingerprint For Success (F4S)\n\nBacked by 20+ years of research, F4S\u2019s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high performing teams.\n\nKeep in mind, joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your F4S work style assessment to match the hiring needs of our partners.\n\nYour feedback is a gift! Write to us via:\n\nPowered by JazzHR\n\noWuPJ9lwq9"}, {"title": "Data Engineer Seattle, WA (remote)", "company_name": "Conch Technologies, Inc", "location": "United States (Remote)", "link": "/jobs/view/3765052211/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=SBETI78UMutCHuRp5FnP%2FA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3765052211", "description": "About the job\nHi,\n\nGreetings from Conch Technologies Inc\n\nData Engineering Lead \u2013 With Databricks and Unity Catalog\n\nLocation \u2013 Seattle, WA (remote)\n\nDuration \u2013 12 months\n\nNeed: Design and implement data architecture and strategy, leveraging technologies such as Databricks, Unity Catalog, Privacera, and Collibra.\n\nAs a Data Engineering Lead, you will play a pivotal role in defining and executing technical deliverables while assessing complexities and levels of effort. You'll engage with both vendor and internal teams to resolve technical challenges and ensure seamless collaboration. Your leadership will extend to planning and prioritization with the SCRUM master, facilitating effective coordination between onsite and offshore teams, and driving feature prioritization through collaboration with product managers.\n\nKey Responsibilities\n\n Lead the definition and breakdown of technical deliverables, assessing complexities and levels of effort.\n Collaborate with vendor and internal teams to address technical issues.\n Drive planning and prioritization in conjunction with the SCRUM master.\n Coordinate with onsite and offshore teams for timely project delivery.\n Collaborate with product managers to prioritize features, communicate with stakeholders, and clarify requirements.\n Collect and manage user-reported issues, ensuring prompt resolution and communication.\n Define comprehensive test plans, oversee their execution, and document results.\n\n--\n\nWith Regards,\n\nNagesh G\n\nMobile: 408-381-5645\n\nDesk: 901-313-3066\n\nEmail: nagesh@conchtech.com \n\nWeb: www.conchtech.com"}, {"title": "Remote Work - Need Azure Fabric Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3759325599/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=C3iVjmcR9wjUip1nNjUiCg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3759325599", "description": "About the job\nThe client is placing heavy emphasis on Microsoft and Azure Fabric experience\n\nInformation Management Engineer \u2013 Take the identified information management sources and assist migration to the Client Data Lake, Azure Fabric. The engineer will transform information management (de-dup, clean and enrich).\n\nTake the identified data sources and bring them into our Data Lake, Azure Fabric. Once the data is in, the engineer will need to be able to transform data (de-dup, clean and enrich the data). We are focused on using a medallion data model for data architecture. The tools we are working with are: Delta Parquet files, python, spark, notebooks, T-SQL, Fabric lakehouse/warehouse, Dataflows Gen2, Data Pipelines, Direct Lake, shortcuts, ADF."}, {"title": "Data Engineer", "company_name": "Catalist", "location": "Washington, DC (Remote)", "link": "/jobs/view/3744951279/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ui1Fk6ICmnPNqAlZDZAcwA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3744951279", "description": "About the job\nFor over 17 years, Catalist has been a leader in civic data and data science innovation. Our mission is to provide progressive organizations with the data, software, and services needed to better identify, understand, and communicate with the people they need to engage and mobilize. Our clients include the largest, most influential organizations in the U.S. active in civic engagement, advocacy, and political campaigns.\n\nCatalist is home to a dedicated, creative team of technologists, data scientists, and campaign experts committed to using our talents and technology to nurture a vibrant and growing progressive community.\n\nAs a Data Engineer at Catalist, you will work closely with the Analytics and Technology departments to design, build, support, and maintain various data pipelines and processes with an end goal of providing data and intelligence to the progressive community. This role primarily involves translating data architecture designs into functional processes, code, and systems.\n\nThe ideal candidate will be a highly motivated individual with excellent technical skills, a strong desire to learn new skills, and an interest in progressive politics. Catalist values creativity and problem-solving. Our work is on the cutting edge of data-driven politics, and your support will help Democratic candidates and progressive organizations conduct successful advocacy and electoral campaigns.\n\nThis position reports to the Deputy Chief Data Officer. The Data Engineer is a part of a growing Data team that supports all underlying work at Catalist.\n\nThis position is included in our CWA bargaining unit.\n\nPrinciple Duties & Responsibilities\n\nDevelop scalable, production processes, code, and systems for data ingest, transformation, modeling, and reporting across a variety of platforms\nTranslate mock-ups and designs into functional processes, code, and systems\nCreate architecture designs when needed\nProvide quality assurance and testing on processes, code, systems, and products\nBecome an internal subject matter expert on various datasets and support other Catalist departments/teams on usage of those datasets\nExecute ad-hoc data and database maintenance tasks as requiredProject manage cross-departmental efforts, with direct responsibility for stakeholder engagement, management, and execution of technical elements\n\n\nRequirements\n\nBS or BA in a technical field, or relevant experience\n1-2 years of experience working with SQL databases\nExperience with data cleaning or ETL processes\nExperience with distributed computing systems and/or distributed datastores (particularly the Hadoop ecosystem)\nExperience managing projects\nFamiliarity with Catalist data, progressive politics, voter files, and/or commercial data\nBackground check required\n\n\nPreferred Skills & Abilities\n\nWillingness to be a problem solver and produce results in a fast paced environment\nAbility to be creative and personable, and articulate ideas clearly\nExperience working with SQL databases\nProficiency with Python or another object-oriented programming language (R, Java, Scala, etc\u2026)\nExperience working in cloud environments (AWS, GCP, etc.)\nExperience working in command line environments such as Bash\nExperience with a version control tool such as git or github\n\n\nBenefits\n\nMedical, Dental, Vision, Prescription Drug\n\nCatalist offers Medical, Dental, Vision, and Prescription Drug coverage for eligible staff and their eligible dependents. Catalist\u2019s Medical plan is a comprehensive PPO program including Prescription Drug coverage with 85% of the premium paid by Catalist. Dental and Vision coverage is provided at no cost to employees.\n\nGroup Term Life Insurance and Long-Term & Short-Term Disability Coverage\n\nGroup Term Life Insurance and Long-Term and Short-Term Disability coverage is available for eligible staff. These benefits are provided at no cost to Catalist employees.\n\n401(k) Safe Harbor Plan\n\nA 401(k) Safe Harbor Plan is available to eligible staff with a 3% contribution from Catalist from the date of hire. Employees may contribute pre-tax or post-tax from their salary up to the legal limits set forth by the IRS.\n\nMedical and Dependent Care Flexible Spending Accounts (FSAs)\n\nCatalist offers an FSA Program that gives eligible staff the ability to pay out-of-pocket medical/dental/vision/child care expenses from pre-tax earnings.\n\nTransit Benefits\n\nCatalist also makes available a Transit benefit FSA program to eligible employees using pre-tax contributions with a company match.\n\nProfessional Development and Remote Work Expenses\n\nEligible employees may be reimbursed up to $750 each year for professional development / education and remote work expenses.\n\nStudent Loan PayDown or SaveUp\n\nCatalist offers a Student Loan PayDown and College SaveUp benefit for eligible staff.\n\nVacation, Personal Leave, Sick Leave Benefits\n\nCatalist offers generous vacation benefits to all eligible staff. Eligible employees also receive:\n\n 14 Paid Holidays\n Personal Days\n Sick Leave\n Parental Leave\n\n\nHybrid Office/Remote Work\n\nCertain positions at Catalist are eligible for Office/Remote Hybrid or full Remote status."}, {"title": "Remote Entry Level Data Analyst/Engineer", "company_name": "SynergisticIT", "location": "White Plains, NY (Remote)", "link": "/jobs/view/3774171423/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=q54sk1%2FRww0z%2BC%2BE8e0jQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774171423", "description": "About the job\nAt SynergisticIT, we're all about making connections. Whatever IT goals you have, our software programmers can help achieve those. Our Software development teams can take up turnkey projects and execute them in an effective and efficient manner. If you are looking to source talent our recruiters will find the ideal IT talent for your company. What's the secret to our success? Well, it all starts with taking quality time to listen to each client's specific needs. After we have a thorough grasp of your IT goals, we can better customize our Developments as per your specific needs. We can also tailor make recruiting programs to exceed your expectations. Since our founding in 2010, SynergisticIT's strategies have earned the company an enviable position in the software development, IT staffing and IT skill enhancement fields. SynergisticIT continues to work with hundreds of satisfied American clients with our software programmers working on our projects and after gaining hands on experience on cutting edge technologies moving to contribute their skills to great clients like Apple, Google, Client, Ebay, Paypal, Kroger, the Walt Disney Company and hundreds more. If you are tired of working with inefficient programmers who take a lot of time to ramp up we want you to try us. Our software programmers can hit the ground running and get you the maximum return on your investment. You have already tried the rest its time you tried the best. SynergisticIT - Home of the Best Data Scientists and Software Programmers in the Bay Area.\n\n Why Us ? \n\nSynergisticIT has a proven track record of successfully skill enhancement and staffing IT employees for some of the world's most iconic brands. Our team takes the time to fully understand every client's needs so we could best meet your IT staffing requirements. The knowledgeable staff at SynergisticIT is always more than happy to work with clients to ensure they reach their software development goals. Besides staffing, SynergisticIT is also committed to helping young IT professionals advance their career with a robust upskill program . Everyone who goes through SynergisticIT's program learns all the skills necessary to succeed in many IT fields ranging from Java to Machine Learning. Additionally, everyone trained at SynergisticIT has been through extensive mock and technical interview screenings to bolster their career prospects. Last, but certainly not least, SynergisticIT takes great care to respect the privacy considerations for every client. All companies who work with SynergisticIT can rest assured their confidential data is protected using the most up-to-date encryption technologies. SynergisticIT also complies with all the latest NDA agreements.\n\n REQUIRED SKILLS For Java /Software Programmers \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Time series analysis \n\n We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2022 and at Gartner Data Analytics Summit (Florida)-2023 \n\nOracle CloudWorld Event (OCW) Las Vegas 2022 | SynergisticIT - YouTube\n\n https://www.youtube.com/watch?v=OAFOhcGy9Z8 \n\n https://www.youtube.com/watch?v=EmO7NrWHkLM \n\n https://www.youtube.com/watch?v=NVBU9RYZ6UI \n\n https://www.youtube.com/watch?v=Yy74yvjatVg \n\nSynergisticIT at Gartner Data and Analytics Summit 2023 - YouTube\n\nFor preparing for interviews please visit  https://www.synergisticit.com/interview-questions/ \n\n We are looking for the right matching candidates for our clients \n\n Please apply via the job posting \n\n REQUIRED SKILLS For Java /Full Stack/Software Programmer \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning Positions \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Tensorflow \n\n If you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team. \n\n No phone calls please.  Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates"}, {"title": "Data Engineer", "company_name": "Anika Systems", "location": "Leesburg, VA (Remote)", "link": "/jobs/view/3787766893/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=mApL9JnN%2B%2Fr2%2BYUPlxRYOA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787766893", "description": "About the job\nAnika Systems is a fast growing, woman-owned small business that specializes in providing innovative IT solutions for federal government agencies. Our expertise lies in accelerating delivery in Data and Analytics, Intelligent Automation, Application Development, and IT Modernization. We are currently expanding our Federal team and are seeking a passionate and talented Data Engineer. This opportunity is 100% remote.\n\nMust be a U.S. Citizen with the ability to obtain and maintain a government suitability clearance\n\nResponsibilities\n\nDesigns, develops, builds, analyzes, evaluates, and installs database management systems to include database modeling and design, relational database architecture, metadata, and repository creation and configuration management.\nDefines and oversees database organizations, standards, controls, procedures, and documentation. Provides technical consulting in the definition, design, and creation of a database environment.\nAdvises applications development staff and users on data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nDesigns and implements databases with respect to access methods, access time, batch processes, device allocation, validation checks, organization, protection and security, documentation, and statistical methods.\nUses data mapping, data mining, and data transformational analysis tools to design and develop databases.\nDetermines data storage and optimum storage requirements.\nPrepares system requirements, source analysis, and process analyses and designs throughout the database implementation.\n\nRequired Skills And Experience\n\nBA/BS and 1 year of relevant experience\nExperience with DataBricks, SQL, Python\nApplications development experience with data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nExcellent communication skills (both oral and written) with process-oriented organizational skills to ensure project success\n\nDesired Skills And Experience\n\nDemonstrated successful development experience with the full life cycle of government database management systems including database modeling and design, relational database architecture, metadata and repository creation, and configuration management.\n\nPowered by JazzHR\n\nTlKWIorucJ"}, {"title": "Remote Work - Need Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3774712978/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=r3Bp%2F0qcOD2OpC5jpbUOcA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774712978", "description": "About the job\nTitle : Data Engineer\n\nLocation : Remote\n\nDuration : 6+ months to start (long term contract (work order are extended 2x year))\n\nPlease Fill out the skill matrix: Technology Some Awareness (1) Novice (2) Intermediate (3) Advanced (4) Expert (5) SQL Tableau Desktop /Creator Tableau API Integration AWS - S3 Glue AWS - Athena Redshift AWS - Lambda AWS - SNS/SQS msExcel\n\nProfile: Data Analyst will work closely with Architects and leads in the Interflow team, key business stakeholders including program staff to support the organizations effort in the areas of data Analysis and information management. Role should have strong experience in writing SQL queries to extract data for reports. Candidate should have experience working with large set of data. As these reports are very critical for the company and we are looking for an expert who is capable of handling, consuming and extracting large sets of data for analysis and reports.\n\nMust Haves\n\nAWS - Athena\nAWS - S3\nGlue\nhands-on development of reporting applications\nMS Excel\nRedShift\nSQL\nTableau Desktop /Creator\n\nNice\n\nAWS - Lambda\nAWS - SNS/SQS\nTableau API Integration\n\nSkills\n\nBA/BS required (major in an analytical field desired) \nA minimum 10+ years work-related experience. \nExperience in hands-on development of reporting applications for the Web in a professional environment \nThe Ethos of continuous improvement and interest in learning new things.\nStrong analytical thinking and structured problem-solving ability\nAbility to handle multiple projects and assignments simultaneously and effectively in a cross-functional team environment. \nVersed on the agile methodology and best practices. \nExcellent interpersonal and collaboration skill with the ability to work with a diverse set of colleagues, across functions, from different organizations, disciplines, etc. \nSelf-starter, ability to set priorities, work independently and attain goals"}, {"title": "Data Engineer", "company_name": "Team Remotely Inc", "location": "Houston, TX (Remote)", "link": "/jobs/view/3786249829/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=78lY5Jo%2FfyRPC6TQYCdvww%3D%3D&trk=flagship3_search_srp_jobs", "id": "3786249829", "description": "About the job\nThis is a remote position.\n\n Data Engineer (US/Canada Residents Only, 1 year experience, remote)\n\nTeam Remotely Inc. is a staffing and recruitment agency that offers a comprehensive solution for talent acquisition, including sourcing, vetting, pay rolling, and managing talent. Whether you need contract staffing, direct hire, direct sourcing, talent pools, or diversity initiatives, our model can support your hiring strategy.\n\n Hiring Type: Full-Time\n\n Base Salary: $55K-$70K Per Annum.\n\nResponsibilities\n\nDesigning user interface changes for web-based DB applications. \nReviewing application requirements and interface designs. \nDeveloping and implementing highly responsive user interface components using react concepts. \nWriting application interface codes using JavaScript following react.js workflows. \nTroubleshooting interface software and debugging application codes. \nDeveloping and implementing front-end architecture to support user interface concepts. \nMonitoring and improving front-end performance. \n\nQualifications And Experience\n\nBachelor\u2019s degree in computer science, information technology, or a similar field. \nPrevious experience working as a react.js developer. \nIn-depth knowledge of JavaScript, CSS, HTML, jQuery, and front-end languages. \nKnowledge of REACT tools including React.js, Webpack, Enzyme, Redux, and Flux. \nExperience with user interface design. \nKnowledge of performance testing frameworks, including Mocha and Jest. \nExperience with browser-based debugging and performance testing software. \nTroubleshooting skills. \nProject management skills. \nProblem-solving skills. \nVerbal communication skills. \n\n Why work with Team Remotely?\n\nTeam Remotely Inc. is a staffing platform offering a seamless experience for employers and candidates. Employers can post job openings and specify their requirements, while candidates can create profiles and upload resumes.\n\nThe team of Team Remotely continuously learns and adapts based on previous successful placements, constantly improving its matching capabilities. This ensures that the recommendations provided by Team Remotely are tailored and accurate, increasing the likelihood of a successful match between employers and candidates. By providing intelligent and data-driven solutions, they strive to enhance the efficiency and effectiveness of the hiring process, ultimately helping companies find the best talent and individuals find their dream jobs."}, {"title": "Data Engineer", "company_name": "ZAG Technical Services", "location": "United States (Remote)", "link": "/jobs/view/3771403464/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=Hf0VefrKXBFIL5MtHmY0Pw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3771403464", "description": "About the job\nWe are looking for a tech savvy Data Engineer to contribute to the building, operating, and scaling of next generation data platforms and tools that will power data-driven analytics capabilities throughout the ZAG products and services portfolio, spanning areas such as business intelligence, reporting, and data analytics.\n\nThe ideal team member is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. Must be comfortable supporting the data needs of multiple clients and teams, systems, and products within the modernization of ecosystems. The right-fit person will be excited by the prospect of optimizing or even re-designing data processes to support next generation of advanced analytics capabilities, which in-turn translates to implementing strategic enhancements to maintain competitive advantage in the industry.\n\nThe successful team member will leverage their proficiency to\u2026\n\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\nBuild the analytics processes required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Data Management and cloud based \u2018big data\u2019 technologies.\nBuild analytics tools that utilize technologies to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.\nCollaborate with client teams to identify and solution regional and local requirements within the context of the standard global business model for enterprise analytics.\nParticipates in meetings, discussions, strategy sessions where changes, improvements, and enhancements are proposed, evaluated, and approved. \n\n\nThe exceptional team member will have or demonstrate progressive experiences in\u2026 \n\nBachelor\u2019s degree or higher in Computer Science or related field\nExperience working with BI and data warehousing tools\nExperience working with enterprise data, where security is paramount and data governance is critical\nAbility to work independently and in a team environment, and effectively engage all levels of the organization\nUnderstanding of distributed systems driving large-scale data processing and analytics with a successful history of manipulating, processing, and extracting value from large, disconnected datasets.\nAbility to communicate effectively (listening, presenting, and questioning)\nStrong organizational, written, and communication skills\n\n\nAbout ZAG\n\nZAG Technical Services empowers our clients\u2019 success through intelligent business solutions that increase employee productivity, network security and team collaboration. We aim to ensure that our client\u2019s IT investments drive ROI, provide a competitive advantage and help businesses grow. We do this by living our core set of values: Integrity Foremost, Client-Centered, Accountable Always, Teamwork Throughout, and Exceptional is the Goal.\n\nWhat ZAG Has To Offer\n\nCompetitive compensation package \nBenefits including medical, dental, vision, 401K and life insurance \n\n\nZAG Technical Services is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all of our employees.\n\nZAG Technical Services is an E-Verify participant and will provide the Social Security Administration (SSA) and, if necessary, the Department of Homeland Security (DHS), with information from each new employee\u2019s Form I-9 to confirm work authorization."}, {"title": "Junior Data Engineer", "company_name": "HireMeFast LLC - Career Accelerator - Land A Job", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3786249797/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D&trk=flagship3_search_srp_jobs&lipi=urn%3Ali%3Apage%3Ad_flagship3_search_srp_jobs%3BQExogrHBQoSF9zZm%2B9kufw%3D%3D&lici=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D", "id": "3786249797", "description": "About the job\nThis is a remote position.\n\n Job Title: Junior Data Engineer\n\n Employment Type: Full-Time\n\n Salary: $64,000 - $76,000 per annum\n\n Experience Required:  Minimum 1 year of project experience\n\nAbout us: HireMeFast is a leading staffing and recruitment agency specializing in connecting businesses with top-tier talent across various industries. Our mission is to bridge the gap between exceptional candidates and organizations needing their skills, expertise, and unique qualities. Our team of experienced and dedicated recruitment specialists utilizes innovative sourcing strategies, and a vast network to identify and attract top talent. We conduct comprehensive procedures to ensure that only the most qualified candidates are presented to our clients.\n\nPosition Summary\n\nJoin the fast-paced, innovative, and collaborative environment focused on providing an AIOps platform that enhances the intelligence of the CVS Health infrastructure. Work closely with subject matter experts and colleagues to build and scale out machine learning and AI solutions that will detect, predict, and recommend solutions to correct issues before system impact and enhance efficiency, reliability, and performance of CVS Health\u2019s IT operations.\n\nKey Responsibilities include:\n\nData pipeline development: Designed, implemented, and managed data pipelines for extracting, transforming, and loading data from various sources into data lakes for processing, analytics, and correlation. \nData modeling: Create and maintain data models ensuring data quality, scalability, and efficiency \nDevelop and automate processes to clean, transform, and prepare data for analytics, ensuring data accuracy and consistency \nData Integration: Integrate data from disparate sources, both structured and unstructured to provide a unified view of key infrastructure platform and application data \nUtilize big data technologies such as Kafka to process and analyze large volumes of data efficiently \nImplement data security measures to protect sensitive information and ensure compliance with data and privacy regulation \nCreate/maintain documentation for data processes, data flows, and system configurations \nPerformance Optimization- Monitor and optimize data pipelines and systems for performance, scalability and cost-effectiveness \n\nCharacteristics of this role:\n\nTeam Player: Willing to teach, share knowledge, and work with others to make the team successful. \nCommunication: Exceptional verbal, written, organizational, presentation, and communication skills. \nCreativity: Ability to take written and verbal requirements and come up with other innovative ideas. \nAttention to detail: Systematically and accurately research future solutions and current problems. \nStrong work ethic: The innate drive to do work extremely well. \nPassion: A drive to deliver better products and services than expected to customers. \n\nRequired Qualifications\n\n2+ years of programming experience in languages such as Python, Java, SQL \n2+ years of experience with ETL tools and database management (relational, non-relational) \n2+ years of experience in data modeling techniques and tools to design efficient scalable data structures \nSkills in data quality assessment, data cleansing, and data validation \n\nQualifications\n\nKnowledge of big data technologies and cloud platforms \nExperience with technologies like PySpark, Databricks, and Azure Synapse. \n\nEducation\n\nBachelor\u2019s degree in Computer Science, Information Technology, or related field, or equivalent working experience\n\nWhy HireMeFast LLC?\n\nAt HireMeFast, we understand that finding the right individuals to join your team is crucial for success and growth of your organization. We are committed to streamlining the hiring process for our clients, ensuring they have access to a diverse pool of highly qualified candidates who are a perfect fit for their specific needs."}, {"title": "Data Engineer  - Need candidates from Texas ( REMOTE )", "company_name": "Pulivarthi Group (PG)", "location": "United States (Remote)", "link": "/jobs/view/3770685212/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=9gdMwuif%2BpB%2BCQYvQi3imQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3770685212", "description": "About the job\nFollow us on Linkedin: https://www.linkedin.com/company/pulivarthigroup/\n\nPulivarthi Group LLC is a Global Staffing & IT Technology Solutions company, with our prime focus of providing world class solutions to our customers with the right talent. We combine the expertise of our team and the culture of your company to help you with the solution that is affordable and innovative using high quality standards and technologies.\n\nWe\u2019ve served some of the largest healthcare, financial services, and government entities in the U.S.\n\nData Engineer - FT only - Azure SQL with data lakes experience\n\nData Engineer -\n\nthe resource requirement for this Data Engineer role needed more clarification.\n\nOur customer is requesting a resource who knows Azure CI/CD (Continuous Integration / Continuous Deployment) to build and integrate their DevOps pipeline.\n\nOnce the DevOps pipelines are mapped, they need the Data Engineer to connect to all the Data Sources, primarily their Data Lake.\n\nThe Customer (based in Houston) is interested in a resource in a time zone not too far removed from the Central Time Zone to allow real time collaboration."}, {"title": "Data Engineer", "company_name": "Otter Products", "location": "Fort Collins, CO (Remote)", "link": "/jobs/view/3766728302/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=77U3U7CAi1PxgDmpI3%2BiNQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3766728302", "description": "About the job\nOverview \n\nOtter Products is currently recruiting for a Data Engineer to join our Data team! You can be based in our Fort Collins, CO office with flexibility to work a portion of your time remotely, or 100% Remote in the U.S.\n\nThe Data Engineer applies professional experience, concepts, and company objectives toward building and operationalizing the minimally inclusive data necessary for the enterprise data initiatives following industry standard practices and tools. The bulk of the Data Engineer's work would be in designing, managing and optimizing data pipelines, then moving these data pipelines effectively into production for key data and analytics consumers like business/data analysts, data scientists or other stakeholder that needs curated data for data and analytics use cases across the enterprise. The Data Engineer also ensures compliance with data governance and data security requirements while creating, improving and operationalizing these integrated and reusable data pipelines to enable faster data access, integrated data reuse and vastly improved time-to-solution for data and analytics initiatives. Additionally, the Data Engineer will be expected to collaborate with other data team members and data consumers working on the models and algorithms developed by them toward optimization for data quality, security, and governance to put them into production leading to potentially large productivity gains.\n\n About Otter Products \n\nOtter Products, we grow to give. From our founder's garage in 1998 to the global technology leader we are today, Otter Products continues to drive growth through innovation.\n\nThrough our industry-leading brands - OtterBox, Liviri and OtterCares - we provide our partners the number-one selling and most trusted products in our categories. Our philanthropic spirit is the foundation on which we foster our partner relationships, allowing us to grow and to give - together.\n\nBy way of our charitable arm, the OtterCares Foundation, we support our communities and invest in our future through education that inspires kids to change the world.\n\nAnd even as our global community of Otters continues to grow, our founder's core values are still at the heart of everything we do. We measure our success by our ability to give back to our communities and strengthen opportunities for all.\n\nFor more information visit otterproducts.com\n\n Responsibilities \n\nComplete analysis, design and development of BI solutions using Azure SQL Server\nFamiliarity with Data Warehouse concepts\nExperience coding the extraction, transformation, and loading (ETL) processes\nDatabase development primarily in SSIS, Data Factory and SQL\nPartner with the business to determine end user database/reporting needs and requirements\nGenerate ad hoc reports using Power BI or SSRS\nCollaborate with other developers to create and implement best approach solution(s)\nTroubleshoot Azure tools, systems, and software\nReview queries for performance issues, making changes as needed\nCollaborate with team to performance-tune Azure application as necessary\nAssist with the analysis and extraction of relevant information from large amounts of historical business data to feed data science initiatives\nParticipate and support the design and documentation of processes for large scale data analyses, model development, model validation and model implementation\nSupport and maintain a positive safety culture by following all safety policies and procedures and actively contributing to a safe working environment\nOther duties as required\n\n Qualifications \n\nBachelor's degree required. Degree in Computer Science or Mathematics preferred\nMinimum of four years of experience in an IT or analytical role required. Experience in database development, report writing and/or statistics preferred.\n\n EEO \n\nOtter Products, LLC is an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, age, marital status, pregnancy, sex, sexual orientation, gender, gender identity or expression, national origin, disability, veteran status, or any other characteristic or status protected by law.\n\n For US Based Roles Only - Compensation Range Minimum \n\nUSD $90,000.00/Yr.\n\n For US Based Roles Only - Compensation Range Maximum \n\nUSD $120,000.00/Yr.\n\n Additional Total Rewards \n\nProfit Sharing Program Eligible, Benefits Eligible - Full Time- check out otterproducts.com/careers/why for more info"}, {"title": "Software Engineer, Distributed Systems", "company_name": "Hightouch", "location": "United States (Remote)", "link": "/jobs/view/3628572539/?eBP=CwEAAAGMd4QO8APODyE_-mMOPhYyQGg5ZjVbPwq_3V7DK0B2bLAf10pcedM9S2T71X5i4TiJuEwOzWBbSiTKbLuk-VD-_Y-dN5uyf8YpYkHYqEYFxySMS9zr2hRjNrxX9gXbxssh35TFK9YTNV2WeH2Ncb675JgFbJTgNWSZySFC6rrdJGmdSDTr2q5d3VoxORz3GjsDbzjeyZQCvQ07RqYw7oujWSgslycs2LMCY_-N0NEj6e0PvbaYKiZ_5IW34bGVSYLXpHwqa8oOcNttYZ90UA3E_7sDSFzABdIVlfrp5zzERrIYy5sh2MIInxHdDoKgQw8VmOn5uM4S0tXUl-i7LnN0_W0NfPBmLiVDNuveIOm4MfDNPxfe5QKPk6FUAVpvgbcoJn9yRyE&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=rph%2FT6Q6Vkp6YoggJf9cJQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3628572539", "description": "About the job\nAbout Hightouch\n\nHightouch\u2019s mission is to empower everyone to take action on their data. Through our Reverse ETL platform, business and data users can seamlessly sync data from where it resides, such as warehouses and databases, to where it is needed, including operational systems and SaaS tools. Traditionally, acting on data has required engineering time and bandwidth, and left most business users stuck with charts and reports that are unable to take automated action on their data. With Hightouch, every business user, without writing any code, can activate data to streamline critical processes, improve marketing performance, and scale operations.\n\nOur team operates with a focus on making a meaningful impact for our customers. We believe in approaching challenges with a first principles thinking mindset, moving quickly and embracing our value of efficient execution, and treating each other with compassion and kindness. We look for team members that are strong communicators, have a growth mindset, and are motivated and persistent in achieving our goals.\n\nHundreds of companies use Hightouch, including Spotify, Ramp, Retool, NBA, Plaid, and Betterment. We\u2019re based in San Francisco, are remote-friendly, and backed by leading investors such as Amplify Partners, ICONIQ Growth, Bain Capital Ventures, Y-Combinator, and Afore Capital.\n\nAbout The Role\n\nWe are looking for a distributed systems engineer to work on the systems that power the syncing engine our customers and other engineering teams rely on. This presents an exciting challenge where you can apply your expertise in distributed systems, performance optimization, and troubleshooting to push the boundaries of what is possible and meaningful improve our ability to move massive amounts of customer data. This role also provides a unique opportunity to work on a multi-cloud and multi-region infrastructure that supports a global customer base.\n\nWe believe in enabling our engineers to do their best work for our customers by giving them extremely high levels of ownership and autonomy. This comes in different forms: you will own and deliver projects from start to finish, you will work directly with customers to solve their hardest scaling problems, and you will have a lot of influence over what we work on as a team and company.\n\nSome of the problems we\u2019ll be working on include:\n\nSync Speed: Customers want to sync a lot of data to important destinations like Facebook and Snapchat, which requires us to analyze every part of our syncing process and find where we can optimize to sync data more quickly\nStreaming Syncing: We currently sync data from sources in batch only, but envision a world where we also do syncs in real-time and support streaming sources like webhooks and queues\nScalability and Reliability: As part of our rapid growth, we\u2019re always evaluating current future bottlenecks in scaling and reliability, and architecting for the next order of magnitude of growth\nPersonalization API: Our Personalization API product provides a low latency caching layer on top of data warehouses that enable customers to use their data warehouse for real-time personalization use cases with \nMulti-Region and Multi-Cloud: Supporting our multi-region and multi-cloud backend, including extending it to launch Hightouch on in new regions to support data residency requirements of our global customer base\n\nWe are looking for talented, intellectually curious, and motivated individuals who are interested in tackling the problems above. This is a senior role, but we focus on impact and potential for growth more than years of experience. The salary range for this position is $170,000 - $240,000 USD per year, which is location independent in accordance with our remote-first policy.\n\nAbout You\n\nYou are an engineer with a passion for solving hard technical problems that generate real value for customers. You\u2019re motivated by high ownership and are comfortable in a fast-paced, startup environment.\n\nYou have experience and comfort with distributed systems and high-scale systems or the ability and desire to build this experience quickly. This experience can take different forms:\n\nYou\u2019ve built syncing engines that process and move terabytes of data per day in a scalable way\nYou\u2019ve created event collection or data streaming services that power company critical systems\nYou\u2019ve owned systems that have undergone orders of magnitude of growth and helped them scale accordingly\nYou have relevant skills and are a fast learner who is excited to take on the challenges we face\n\nInterview Process\n\nOur goal with the interview process is to balance speed with giving both parties opportunities to assess whether there is a strong mutual fit. We will ask you questions, but we want you to ask us questions! Our technical interviews focus on how you design systems because we believe this is the best way for us to see how you work and for you to see how we collaborate. We don\u2019t ask you to write code to solve technical brainteasers that don\u2019t appear in your day to day job.\n\nRecruiter screen [30m]: Introductory call with our recruiting team to get to know each other and see if the role could be a good mutual fit.\nSystems design screen [45m]: Designing a data processing feature end-to-end.\nHiring manager interview [30m]: Chat with hiring manager about past experiences and future operating preferences to assess fit on company values and operating principles.\nSystems design interview [90m]: Work with the interviewer to architect a system at a conceptual level. The problem will be at a pretty high level - and have both product and customer requirements as well as technical."}, {"title": "Entry-Level Data Scientist Engineer", "company_name": "Phoenix Recruitment LLC", "location": "Texas City, TX (Remote)", "link": "/jobs/view/3788727625/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=8iwtytHsCu4wMwJ4fj3pxw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3788727625", "description": "About the job\nThis is a remote position.\n\nTitle - Entry-Level Data Scientist Engineer, 1 year of project experience\n\nEmployment Type: Full-time\n\nBase Salary: $60K-$70K\n\nPhoenix Recruitment offers a variety of recruiting services to assist both employers and employees. They specialize in marketing open positions, recruiting, and helping employers to find qualified candidates across various industries. Phoenix Recruitment has expertise in streamlining the hiring process. They can help ensure that the process is efficient, well organized, and compliant with relevant regulations.\n\nSkills and Abilities:\n\nStrong knowledge of R or Python for data analysis and modeling. \nProficiency in statistical programs such as R, SAS, MATLAB, or Python. \nFamiliarity with spreadsheets (VBA) and database applications (Access, Oracle, SQL, or equivalent technology). \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \nAbility to learn new methods quickly and work under deadlines. \nExcellent teamwork and communication skills. \nStrong analytical and problem-solving abilities. \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \n\nPreferred:\n\nKnowledge of actuarial concepts and life, health, and/or annuity products. \nExperience with statistical modeling techniques such as GLM, Decision Trees, Time Series, Regression, etc. \nFamiliarity with Microsoft DeployR. \nExposure to insurance risk analysis. \nBasic experience in computational finance, econometrics, statistics, and math. \nKnowledge of SQL and VBA. \nFamiliarity with R or Python for predictive modeling \n\nWhy Phoenix Recruitment LLC?\n\nPhoenix Recruitment often has an extensive network of employers and candidates. This network allows them to tap into a pool of qualified candidates and connect them with suitable job opportunities. They can also leverage their connections to help employers find the right talent efficiently. Outsourcing the recruitment process to a specialized agency can save your time and resources, avoid delays, reduce administrative burdens, and increase the chances of finding the right fit for your organization."}, {"title": "Data Engineer - Remote Work", "company_name": "BairesDev", "location": "United States (Remote)", "link": "/jobs/view/3783162297/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=OtoCrAAHZFomVup04fy35g%3D%3D&trk=flagship3_search_srp_jobs", "id": "3783162297", "description": "About the job\nWho We are\n\nBairesDev is proud to be the fastest-growing company in America. With people in five continents and world-class clients, we are only as strong as the multicultural teams at the heart of our business. To consistently deliver the highest quality solutions to our clients, we only hire the Top 1% of the best talents and nurture their professional growth on exciting projects.\n\nWe are looking for a Data Engineer to join our Development team and participate in different projects made up of multicultural teams distributed throughout the world.\n\nWhat You Will Do\n\n Interview other Data Scientists and Data engineers to vet their technical abilities.\n Input data from those interviews into the product and help develop a predictive, fair, and enjoyable solution.\n Work together with developers, tech leads, and solution architects to build applications.\n Improve existing structures by adding new functionalities or proposing technological updates so that as a result, the impact of your contribution is significant in the core of each business.\n Improve the user experience on scalable and high availability platforms, contributing to the key differential of each business.\n\nHere\u2019s What We Are Looking For\n\n 5+ years of experience working as a developer.\n 3+ years of experience working as a Data Scientist / Data Engineer.\n Proficient with analysis, troubleshooting, and problem-solving.\n Hands-on experience with managing data loads and data quality.\n Advanced English level.\n\nHow we do make your work (and your life) easier:\n\n 100% remote work.\n Hardware setup for you to work from home.\n Flexible hours - make your schedule.\n Paid parental leave, vacation & holidays.\n Diverse and multicultural work environment.\n An innovative environment with the structure and resources of a leading multinational.\n Excellent compensation \u2014 well above the market average.\n Here you can grow at the speed of your learning curve.\n\nOur people work remotely but with a consistent and robust culture that promotes diversity and teamwork. To continue being the leading software development company in Latin America, we want to ensure that every BairesDev member gets the best growth and professional development opportunities in a diverse, welcoming, and innovative environment.\n\nEvery BairesDev team member brings something unique to our company.\n\nWe want to hear your story. Apply now!"}, {"title": "Data Engineer (Remote)", "company_name": "Cascade Debt", "location": "United States (Remote)", "link": "/jobs/view/3755336571/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=pmUtn1j2zyPr25CNPh2RNw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3755336571", "description": "About the job\nDo you want to work in a small team where you can make a real difference in a company? Would you embrace the challenge of building a high impact platform that is used globally? Then we want to speak with you!\n\nAbout Cascade\n\nCascade is a fintech startup backed by Canadian and US investors that empowers companies to grow by democratizing access to institutional debt. We are building tools that modernize how companies raise and manage debt, lowering the barriers to entry to the $7 trillion specialty finance and alternative credit market for companies around the world. Founded just last year, we are gearing up for our next phase of technical development and are seeking a talented Data Engineer to lead the way.\n\nAbout The Role\n\nAs a Data Engineer you\u2019ll be an early member of a growing team building a pioneering platform in the debt infrastructure space. You will design, build, and launch efficient, scalable, and reliable data pipelines to move and transform data .\n\nWhat You\u2019ll Do\n\n Building databases, data lakes and data ingestion pipelines to integrate customer databases and datasets to our systems \n Ingesting financial datasets from external customers, then u pdating and maintaining accurate and complete data mappings to ensure that our product s are displaying high quality data \n M onitoring and alerting across our data pipelines in order to make sure that our data ingests are reliable and correct \n Translat ing business goals and stakeholder requirements into new data flows and deliver analytical insights ensuring that the data flows create real business value continuously \n\nSkills And Experience\n\n 2 + years experience in a data engineering or data analyst role \n E xperience with relational SQL and NoSQL databases, including PostgreSQL , MySQL, MariaDB, MongoDB, Bigtable, etc. \n E xperience working with ETL technologies , such as Databricks, Fivetran , or dbt \n Proficiency in writing SQL queries and knowledge of analytical data warehouses such as RedShift, BigQuery , and Snowflake \n Some experience with CI/CD automation such as GitHub Actions \n Developing APIs and integrating with 3rd party APIs \n Bonus: experience in fintech / SaaS / credit analysis \n\nPeople Who Thrive At Cascade Are\n\n Self-starters, who take the initiative to tackle challenges in a remote work environment \n Good communicators, who can operate without ego to discuss, learn, grow, and help others do the same \n Passionate about creating great digital experiences for users \n Problem solvers who are not satisfied with the status quo \n\nBenefits\n\nRemote: we are a remote-first company and are very flexible on hours as long as things get done \nHome-office: there\u2019s a $1000 USD home office allowance to set yourself up \nEquity: we expect you to have an owner-mentality, and have the equity plan to match \nBenefits: health, dental, vision, and more \nPerks: we offer free lunches weekly and off-site trips \nJob satisfaction: we offer autonomy, ample opportunities for mastery, and an opportunity to make a difference for companies around the world"}, {"title": "Data Engineer", "company_name": "ISC (Integrated Specialty Coverages, LLC)", "location": "Carlsbad, CA (Remote)", "link": "/jobs/view/3778293528/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ew7%2FfEuXSujs5IGabdqUyw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3778293528", "description": "About the job\nAbout Integrated Specialty Coverages\n\nIntegrated Specialty Coverages LLC (ISC) is a growth stage technology and data-driven insurance company leading innovation in the market.\n\nBacked by one of the leading private equity firms, KKR, and led by a forward-thinking management team, ISC is combining the worlds of insurance and technology to create an Insurtech powerhouse. As a leading online distributor of insurance products for a range of industries and \"Main Street USA\", we are looking for the right people to help us in our mission of achieving exponential growth. We strive to be the number one place to go for brokers and agents to source insurance. To accomplish this, we're building a digitally focused team that deeply understands the intersection between user experience, data, and AI/ML to optimize the way we engage with our customers and partners.\n\nJob Summary\n\nThe Data Engineer role will be the technical liaison between multiple groups including a data science team, the engineering team, product management, and business stakeholders. You do not need any insurance knowledge prior, however, you must quickly dive deep into the insurance world and ask questions to become a subject matter expert. You will be responsible for building a data platform to facilitate the data science team. You must be a self-starter that can build out features such as a data pipeline from scratch. There will be support from both engineering and data science for any buildout.\n\nJob Responsibilities\n\nCreate and maintain optimal data pipeline architecture. \nAssemble large, complex data sets that meet functional / non-functional business requirements. \nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. \nBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using AWS technologies, SQL, and Python. \nWork with stakeholders including the Executive, Product, Data Science, and Engineering teams to assist with data-related technical issues and support their data infrastructure needs. \nWork with data science and analytics teams to strive for greater functionality in our data systems. \n\nMinimum Qualifications:\n\nAdvanced working SQL knowledge and experience working with relational databases, strong query authoring (SQL) as well as working familiarity with a variety of databases (Snowflake, Redshift, MySQL, MSSQL, etc.)\nExperience building and optimizing data pipelines, architecture and data sets. \nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. \nBuild processes supporting data transformation, data structures, metadata, dependency, and workload management. \nA successful history of transforming, processing and extracting value from large disconnected, datasets from a variety of data sources (Flat files, Excel, databases, etc.)\nStrong analytic skills related to working with unstructured datasets. \nStrong project management and organizational skills. \nExperience supporting and working with cross-functional teams in a dynamic environment. \nAbility to mentor/guide/collaborate with other team members. \n\nWe are looking for a candidate with 3-5 years of experience in a Data Engineer role. They should also have experience using the following software/tools:\n\nExperience with relational and MPP databases, including Snowflake and MySQL. \nExperience developing software in an agile environment from the requirements stage to production. \nExperience with version control: git\nExperience with container technologies: Docker\nExperience with data pipeline and workflow management tools: Airflow, Jenkins, AWS Glue, Azkaban, Luigi, etc. \nExperience with AWS cloud services: EC2, ECS, Batch, S3, EMR, RDS, Redshift\nExperience with other cloud services: Snowflake, Airflow\nExperience with object-oriented/object function scripting languages: Python, Java, C++, etc. \nExperience with data modeling and data warehouse design\nExperience with data visualization tools (PowerBI, QuickSight)\n\nThe starting annual pay scale for this position is listed below. Actual starting pay will be based on factors such as skills, qualifications, training, and experience. In addition, the company offers comprehensive benefits including medical, dental and vision insurance, 401(k) plan with match, paid time off, and other benefits.\n\nISC's salary ranges are determined by role and level. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations and could be higher or lower based on a multitude of factors, including job-related skills, experience, and relevant education or training.\n\nNational Pay Range\n\n$100,000\u2014$110,000 USD\n\nBenefits of Working at ISC\n\nCompetitive vacation and flexible working arrangements\nComprehensive and inclusive health benefits\nA variety of professional development and mentorship opportunities\nChoice of technology whether at home or in the office\n\nISC's Ownership Behaviors\n\nDo the Right Thing *Be Relentless. Pursue Excellence. *Personal Responsibility & Own Outcomes\nTry Fast. Learn Fast. Fail Fast. Think Big. *Build Extraordinary Owners & Join Forces *Grit & Determination\n\nApplicants with disabilities may contact ISC HR department via e-mail, and other means to request and arrange accommodation. If you need assistance to accommodate a disability, you may request accommodation at any time. Please contact ISC at HR@ISCMGA.com.\n\nISC believes in creating long-term relationships by being responsive and relevant and by consistently delivering value to our community of customers. Specifically, with our employees, we focus on attracting, developing, and retaining the best talent for our business, challenging our people, demonstrating a \"can-do\" attitude, and fostering a collaborative and mutually supportive environment. ISC is an equal opportunity employer and a participant in the US federal E-Verify program (US).\n\nDiversity creates a healthier atmosphere: All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, gender, gender identity, sexual orientation, marital status, medical condition, genetic information, mental or physical disability, military or veteran status, or any other characteristic protected by local, state, or Federal law."}, {"title": "Data Engineer -Remote", "company_name": "RIT Solutions, Inc.", "location": "Denver, CO (Remote)", "link": "/jobs/view/3768007496/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=1hQH8EDaII1eJ7nQ9BACQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3768007496", "description": "About the job\nDuration: 6+ months CTH\n\nMust have a valid LinkedIn profile\n\nRequired Skills\n\n 3-6 years of experience (no more than 6 years)\n Start up environment experience\n Python, SQL, and DBT"}, {"title": "Big Data Engineer - W2", "company_name": "Megan Soft Inc", "location": "United States (Remote)", "link": "/jobs/view/3674845652/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fCAxh4N%2F4YZJ5aEIASaeeA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3674845652", "description": "About the job\nTitle: Big Data Engineer - W2\n\nLocation: Dearborn, MI (Remote)\n\nDuration: 12+ Months\n\nDirect Client: Ford Motors\n\nMust have skills: Should be able to handle GCP requirements\n\n\"Translates business requirements to build repeatable, sustainable, efficient, coded processes that can be productionized by Software Engineers and readily modified by other Data Engineer Creates POC processes in Dev/QA and works with Software Engineers to productionize and align best coding and process practices that maximize efficiency, speed, stability, system resources and capabilities Awareness of and compliance with data privacy, security, legal and contractual guidelines Incorporates data quality and privacy checks/alerts to minimize bad data being consumed by end users, models and dashboards, and to protect customer data; revises checks for new data issues Validates data products and pipelines are functioning as expected following system or application upgrades, source changes, etc. Works closely with Architects if required to align systems, tools and applications being utilized with business use case and performance requirements Provides data product support and maintenance Establishes, tracks and monitors KPIs related to specific data products and deliverable Participate in iteration cadence activities (Iteration Planning, task estimation, daily scrum, demos, etc.). Will be a part of on-call team to resolve critical production support tickets during business hours and off hours.\n\nSkills Required\n\nExperience with Big Data Technologies (Hadoop , ETL Tool, Spark. Hive etc..) Someone who understands Cloud and has experience working with Cloud Platforms (preferably GCP) Should be able to handle GCP requirements - ingesting into GCS/ Big Query, working with GCP components Hands-on in Data Flow, Cloud Storage (GCS), Big Query and GitHub. Proficient in Python and SQL.\n\nSkills Preferred\n\nManages ETL: uses programming and tools for data ingestion, configures pipelines, applies transformations and decoding, integrates and fuses data, moves and securely deliver Experience NoSQL databases\n\nAdditional Information\n\nIf the candidate is remote only, please indicate \"\"100% Remote\"\" under candidate's name on resume. Other similar schedule notations can be \"\"Local Candidate\"\" or \"\"Hybrid Candidate.\"\" ***HRA Test Required***\""}, {"title": "Remote Opportunity: Quantexa Data Engineer", "company_name": "SPAR Information Systems LLC", "location": "United States (Remote)", "link": "/jobs/view/3680073191/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ioykVCZSOQ7BK5%2FwRQHOWg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3680073191", "description": "About the job\nHello All, Hope you are doing great!! Please go through the job descriptions and let me know your interest. Role: Quantexa Data engineer  Location: Remote Duration: Long Term Contract Job Description: 1. End to End usage of Quantexa including guiding our client team on how to use from both front end and data ingestion Reason they are asking for Certification 2. Resolve Entity issues with data 3. Knowledge of Scala and Hive which requires coding experience using parquet 4. Advanced SQL knowledge requiring writing SQL queries 5. Experience having worked with GCP data lake 6. Quantexa certification required\n\nThanks & Regards,\n\nSatnam Singh\n\nDirect: 201 623 3660\n\nEmail : Satnam.singh@sparinfosys.com"}, {"title": "Remote Work - Need Staff Data Engineer - USC only", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3746283980/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=CvIuLBNO%2BPMGi4IXVRkP0A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3746283980", "description": "About the job\n100% Remote, client is in NY\n\nLook for strong job history, Python, Machine Learning, SQL, 10/10 comms, etc. \n\nMUST be an Engineer \n\nSummary\n\nWe're looking for an experienced Data Engineer to join our client's growing team of top notched IT professionals. You will work closely across multiple groups including Editorial and Audio, Marketing, Advertising, etc. to achieve their goals and objectives. The Data Engineering team builds tools throughout the stack while sharing knowledge across these departments.\n\nAs the company grows, they're looking for Data Engineer who will help solidify and expand our pipelines and maintain their data warehouse. Their business model is based on performing complex and very detailed analyses of how our products perform with their subscribers. You'll be responsible for working alongside analysts, product, marketing, and other teams to define new data collection components and measurement schemes that support each new product and feature.\n\nHow Can Add Value To Their Mission\n\nCreate and maintain data pipelines to provide insights and drive business decisions\nEstablish data warehousing strategy (ex. Kimball, Data Vault, etc.)\nMaintain data infrastructure on our AWS accounts\nCollaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.\nWrite unit/integration tests, contributes to engineering wiki, and documents work\nImplement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it\n\nWhat You'll Need To Succeed\n\n7+ years of industry experience measuring product performance and user behavior\nExperience working with a variety of data management technologies, including RedShift, Kinesis/Kafka, Glue, Spark, Postgres, Airflow, dbt, and others.\nExperience implementing BI reporting tools such as Looker\nExperience interfacing with engineers, product managers and analysts to understand data needs\nKnowledge of a variety of measurement beacons, SDKs, APIs including Google Analytics, Amplitude, Braze, Stripe, email service providers\nA commitment to building secure, resilient, fault-tolerant architectures, with clear documentation and procedures in place for support\nA focus on accuracy and detail as you build, ensuring data pipelines produce clear, predictable results\nUnderstanding of the typical metrics a subscription and advertising-supported business needs to measure success\nExperience with Client techniques as applied to behavioral segmentation or anomaly identification is a definite plus\nFamiliarity using developer tools that increase productivity and facilitate the development of resilient code (eg. Docker, CircleCI, Serverless) is a plus"}, {"title": "ENTRY LEVEL DATA ENGINEER (REMOTE)", "company_name": "SynergisticIT", "location": "Jacksonville, FL (Remote)", "link": "/jobs/view/3767587909/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=om7L3CnowiC4PLIcpZD70A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3767587909", "description": "About the job\nSYNERGISTICIT wants every candidate to know that the Job Market is Challenging and to stand out, you need to have exceptional skills and technologies and that's where we come in to make sure you get the attention which you need\n\nSynergisticIT understands the complex nature of the job market and how difficult it can be to secure a position, especially for fresh graduates. Therefore, we assist and help tech-savvies to convert their passions into professions. We go above and beyond to keep you working in your niche.\n\nAs we focus on long-term success, we provide complete career development solutions. From job search to upskilling portfolio and interview preparation, we can guide you at every step of your career.\n\nSynergisticIT spares no efforts to connect you with a large network of tech giants, including Google, Apple, PayPal, Dell, Cisco, Client, etc. Presently, we are actively looking for ENTRY LEVEL DATA ENGINEER a driven mindset. Get the right opportunity and gain experience in building web-centric solutions on Java.\n\nWho Should Apply : Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or anyone looking to make their career in IT Industry\n\nWe also assist in filing for STEM extension and H1b and Green card filing.\n\nCandidates who are serious about their future in the IT Industry and have set big goals for themselves.\n\nCandidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. We also offer Skill Enhancement Programs if the candidates are missing skills or experience which our clients need with great outcomes\n\nCandidates can benefit from skill enhancement if they fall into the below categories. If they are qualified with enough skills then no need for skill enhancement\n\nCandidates Who Lack Experience\n\nHave had a break in careers\n\nLack Technical Competency\n\ncandidates who want to get employed and make a career in the Tech Industry \n\nPlease Also Check The Below Links\n\nhttps://www.synergisticit.com/candidate-outcomes/\n\nhttps://www.synergisticit.com/java-track/\n\nhttps://www.synergisticit.com/data-science-track/\n\nhttps://www.synergisticit.com/which-is-the-best-option-for-tech-job-seekers-staffing-companies-consulting-companies-bootcamps-or-synergisticit/\n\nhttps://www.synergisticit.com/contact-us/\n\nIf the skills are not a match candidates can opt for Skill enhancement. Or their resume can be sent out to clients to see if responses are achievable\n\nREQUIRED SKILLS For Java/Software Programmers\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Core Java , javascript , C++ or software programming\nSpring boot, Microservices and REST API's experience\nExcellent written and verbal communication skills\n\nFor data Science/Machine learning\n\nRequired Skills\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Statistics, Python, data visualization tools\nExcellent written and verbal communication skills\n\nPreferred skills: NLP, Text mining, Tableau, Time series analysis\n\nTechnical skills are required by clients for selection even if its Junior or entry level position each additional Technical skill helps a candidate's resume to be picked by clients over other job seekers.\n\nClients hire candidates with the right technical skills which they need and reject candidates who lack the required technical skills.\n\nNo third party candidates or c2c candidates\n\nPlease apply to the posting\n\nNo phone calls please. Shortlisted candidates would be reached out"}, {"title": "Data Engineer - Remote | WFH", "company_name": "Get It Recruit - Information Technology", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3782241984/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=BgBf1kSAAX%2BSuw6inJsIVw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3782241984", "description": "About the job\nJoin an innovative team at a leading company in the engineering and manufacturing industry, where we are committed to leveraging data for continuous improvement and operational excellence. This hybrid position allows you to enjoy the flexibility of remote work on Mondays and Fridays.\n\nWhy Join Us\n\nAt our company, we prioritize your well-being and professional growth. We offer a comprehensive benefits package, including excellent health benefits, a 6% 401(k) match, generous paid vacation and sick time, 12 paid holidays, and more.\n\nRole Overview\n\nAs a Data Engineer, you will play a pivotal role in transforming raw data into actionable insights, driving improvements in engineering and manufacturing processes, enhancing product quality, and optimizing operational efficiency. Collaborate with dynamic cross-functional teams to extract, analyze, and visualize data, influencing strategic decisions through your valuable insights.\n\nKey Responsibilities\n\nData Collection and Integration:\n\nGather, clean, and integrate data from diverse sources, including production databases, sensor data, and external datasets.\n\nData Analysis\n\nConduct exploratory data analysis to identify patterns, trends, and anomalies.\n\nPerform statistical analysis and hypothesis testing to derive meaningful insights.\n\nData Visualization\n\nCreate informative and visually appealing dashboards and reports using tools like Tableau, Power BI, or Python libraries.\n\nPredictive Modeling\n\nDevelop predictive models to forecast manufacturing performance, quality, and operational outcomes.\n\nRoot Cause Analysis\n\nInvestigate production issues and quality concerns by analyzing data to identify root causes and recommend corrective actions.\n\nPerformance Monitoring\n\nEstablish and maintain key performance indicators (KPIs) to track manufacturing and engineering metrics.\n\nGenerate regular reports to communicate performance trends to stakeholders.\n\nCross-functional Collaboration\n\nCollaborate with engineering, manufacturing, and quality assurance teams to understand their data needs and provide analytical support.\n\nContinuous Improvement\n\nSuggest process improvements and optimizations based on data analysis.\n\nData Security And Compliance\n\nEnsure data security and compliance with relevant regulations, such as GDPR or industry-specific standards.\n\nInfrastructure And Development\n\nBuild and maintain the infrastructure for collecting, storing, and processing data.\n\nDesign, develop, and manage data pipelines, ETL processes, and data warehouses.\n\nRequired Experience\n\nProven experience as a Sr. Data Analyst and/or Data Engineer in an engineering or manufacturing environment.\n\nProficiency in data analysis tools and programming languages, such as Python, R, SQL, and Excel.\n\nStrong data visualization skills with tools like Tableau or Power BI.\n\nExcellent knowledge of statistical analysis and machine learning techniques.\n\nAbility to communicate complex findings and insights effectively to both technical and non-technical stakeholders.\n\nStrong problem-solving skills and attention to detail.\n\nFamiliarity with manufacturing processes and quality control is a plus.\n\nExperience with enterprise ERPs (SAP S4HANA, Oracle EBS), CRMs (Salesforce, Hubspot), and IoT.\n\nProficient in working with cloud computing platforms including AWS, GCP, and Azure.\n\nExpertise in configuring and optimizing cloud-based data solutions for scalability, reliability, and performance.\n\nBachelor's Degree in Data Science, Statistics, Engineering, or Computer Science Required.\n\nApply Now and be part of a team shaping the future of data-driven innovation in engineering and manufacturing!\n\nEmployment Type: Full-Time"}, {"title": "Entry Level Software Engineer - Data Backend Engineer (Remote - Canada)", "company_name": "Yelp", "location": "United States (Remote)", "link": "/jobs/view/3752881757/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fankeoxifvhpTzOYvowHYQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3752881757", "description": "About the job\nYelp engineering culture is driven by our values: we\u2019re a cooperative team that values individual authenticity and encourages creative solutions to problems. All new engineers deploy working code their first week, and we strive to broaden individual impact with support from managers, mentors, and teams. At the end of the day, we\u2019re all about helping our users, growing as engineers, and having fun in a collaborative environment.\n Interested in exploring data and finding innovative ways to collect it, curate it, and make it easily accessible for others to use? How about doing that at an enormous scale? Yelp\u2019s datasets contain billions of interactions between users and local businesses around the globe. If working with our vast amount of data sounds exciting, come and join us!\n\nThe Marketplace Data Observability team is responsible for managing, governing, and monitoring all of Yelp\u2019s Ads data to protect our revenue stream and also support offline analysis that aids in making long term company decisions. We build tools that help us to better understand our data quality, and we strive to uncover data issues before they impact our consumers.\n\nAs a Data Backend Engineer on this team, you will be responsible for building elegant and scalable data products that serve critical, up-to-date, structured information to support different types of analytics within Yelp. As a core contributor to our growing data modeling and data warehousing engineering efforts, you will help design and own mission-critical data flow pipelines and datastores to enable decisions including effective A/B testing and company investments.\n This opportunity is fully remote and does not require you to be located in any particular area in Canada. We welcome applicants from throughout Canada. We\u2019d love to have you apply, even if you don\u2019t feel you meet every single requirement in this posting. At Yelp, we\u2019re looking for great people, not just those who simply check off all the boxes.\n  Build systems that can effectively store and crunch terabytes of data.\nDesign and develop data models for efficient data storage, retrieval, and reporting.\nCreate and maintain conceptual, logical, and physical data models using industry-standard modeling tools.\nCollaborate with cross-functional teams, including engineers, data analysts, business analysts, and data scientists, to understand data requirements and translate them into effective data models.\nParticipate in data integration efforts, including ETL processes and data migration.\nStay up-to-date with industry best practices, emerging technologies, and trends related to data warehousing.\nSupport on-call rotations as needed to operate the team.\n  Understanding of high performing and scalable data systems.\nExperience in building and orchestrating ETL pipelines.\nExperience with Data Lake or Data Warehouse landscape.\nA hunger for tracking down root causes and fixing them in systematic ways.\nAbility to communicate effectively to technical and non-technical cohorts alike. \nExposure to some of the following technologies: Python, AWS Redshift, AWS Athena / Apache Presto, Big Data technologies (e.g S3, Hadoop, Hive, Spark, Flink, Kafka etc), NoSQL systems like Cassandra, DBT."}, {"title": "Data Engineer (Associate Consultant), June 2024", "company_name": "UDig", "location": "Nashville, TN (Remote)", "link": "/jobs/view/3716039960/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=XUjo40kU55HnDCqJ%2F3FdPg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3716039960", "description": "About the job\nCan UDig It? \n\nUDig designs, builds, and implements technology solutions that deliver on business objectives. People join our team because they are excited to have an impact on the future of our organization, passionate about delivering meaningful results, and energized by solving complex problems. Oh, and they like working with other driven, smart, and authentic people!\n\nUDig is proud to be recognized as a \"Best Places to Work\" in Virginia and Tennessee. We're committed to fostering a transparent environment where leadership is accessible, professional development opportunities are abundant, and you are empowered to achieve your career goals. You'll collaborate, develop deep relationships with teammates, and accelerate your success while having fun!\n\nIf you're graduating in May 2024 and excited by the idea of impacting organizations by building data solutions to achieve business goals, you should consider joining our growing technology team.\n\nHow Things Go Down\n\nAs a new, entry-level teammate, you'll take part in our \"Breaking Ground\" training program designed to help you quickly get up-to-speed on UDig's delivery approach within a project team. The program covers:\n\nUDig delivery process and methodologies such as gathering requirements and delivering projects using agile\nTechnical guidance and instruction on the best practices from Data leaders at UDig\nDelivering executive and technical presentations\nCoaching to prepare you to for success on your first client project at UDig\n\nA typical day might entail:\n\nProject standups and check-ins\nDesign and development of data pipelines, BI reports, and more\nPlanning and participating in project demos\nContributing to internal UDig projects\nCareer and job growth through UDig-provided training\nCreating long-lasting friendships with teammates through fun activities as well as your projects (including social events, winning a prize for guessing the weight of pumpkin, or sharing experience on cool tech side projects)\n\nUDig believes it is important for Associate Consultants going through our Breaking Ground program to reside in one of our major markets (Richmond, VA or Nashville, TN metro area) for at least the first year of employment.\n\nWant to learn more? Hear directly from our teammates by checking out our 'A Day in the Life' blog series: https://www.udig.com/digging-in/breaking-ground-2023/\n\nSound Interesting? Here's the Technical Piece\n\nBelow are a few boxes you should be able to check if\u202fyou're interested: Don't check every box? Go ahead and apply!\n\n1+ year of experience with relational database management systems such as Microsoft SQL Server, MySQL, or Oracle\nUnderstanding of the full lifecycle of design, development, and implementation of data solutions including architecture design, development, testing\nSome experience in ETL development or reporting is a plus\nAbility to present technical ideas and high-level concepts and solutions to internal and external team members with varying degrees of technical knowledge\nBachelor's Degree in in computer science, information technology, or any science or business discipline\nAbility to take your work seriously but also seriously enjoy a good time with your UDig colleagues\n\nWhat's in It For You? \n\nAt UDig, you will build your career alongside talented and experienced developers while gaining experience and having fun! We provide continuous challenges and professional development opportunities. Additionally, we offer: \n\nCompetitive salary, merit reviews, and career advancement paths\nFlexible, hybrid environment\nIndividual $1500 Training Budget\nRegular team building and social activities (virtual and/or in-person)\nTransparent culture with strong communication and access to leadership\nGreat benefits like\nGenerous Paid Time Off, company holidays, and parental leave\nMultiple Single and Family Health Insurance plans to choose from\nDental & Vision coverage\nShort-Term and Long-Term disability\nOptional accident and critical illness coverage\nMatching 401(k)\nand more!"}, {"title": "Expression of Interest: Data Engineer", "company_name": "Fingerprint for Success (F4S)", "location": "New York, NY (Remote)", "link": "/jobs/view/3787769880/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=l%2FEDpQqUNmCttGdIyTk7Cg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787769880", "description": "About the job\nWe are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.\n\nThe F4S Talent Pool is a pilot project designed to:\n\nHelp job seekers get discovered by our partners based on their anticipated hiring needs.\nProvide optional support and resources for job seekers in their career endeavors.\nHelp individuals understand, and bring out the best in themselves and each other.\n\nThe F4S Talent Pool process:\n\nOnce you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.\n\nAbout Fingerprint For Success (F4S)\n\nBacked by 20+ years of research, F4S\u2019s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high performing teams.\n\nKeep in mind, joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your F4S work style assessment to match the hiring needs of our partners.\n\nYour feedback is a gift! Write to us via:\n\nPowered by JazzHR\n\noWuPJ9lwq9"}, {"title": "Data Engineer Seattle, WA (remote)", "company_name": "Conch Technologies, Inc", "location": "United States (Remote)", "link": "/jobs/view/3765052211/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=SBETI78UMutCHuRp5FnP%2FA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3765052211", "description": "About the job\nHi,\n\nGreetings from Conch Technologies Inc\n\nData Engineering Lead \u2013 With Databricks and Unity Catalog\n\nLocation \u2013 Seattle, WA (remote)\n\nDuration \u2013 12 months\n\nNeed: Design and implement data architecture and strategy, leveraging technologies such as Databricks, Unity Catalog, Privacera, and Collibra.\n\nAs a Data Engineering Lead, you will play a pivotal role in defining and executing technical deliverables while assessing complexities and levels of effort. You'll engage with both vendor and internal teams to resolve technical challenges and ensure seamless collaboration. Your leadership will extend to planning and prioritization with the SCRUM master, facilitating effective coordination between onsite and offshore teams, and driving feature prioritization through collaboration with product managers.\n\nKey Responsibilities\n\n Lead the definition and breakdown of technical deliverables, assessing complexities and levels of effort.\n Collaborate with vendor and internal teams to address technical issues.\n Drive planning and prioritization in conjunction with the SCRUM master.\n Coordinate with onsite and offshore teams for timely project delivery.\n Collaborate with product managers to prioritize features, communicate with stakeholders, and clarify requirements.\n Collect and manage user-reported issues, ensuring prompt resolution and communication.\n Define comprehensive test plans, oversee their execution, and document results.\n\n--\n\nWith Regards,\n\nNagesh G\n\nMobile: 408-381-5645\n\nDesk: 901-313-3066\n\nEmail: nagesh@conchtech.com \n\nWeb: www.conchtech.com"}, {"title": "Remote Work - Need Azure Fabric Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3759325599/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=C3iVjmcR9wjUip1nNjUiCg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3759325599", "description": "About the job\nThe client is placing heavy emphasis on Microsoft and Azure Fabric experience\n\nInformation Management Engineer \u2013 Take the identified information management sources and assist migration to the Client Data Lake, Azure Fabric. The engineer will transform information management (de-dup, clean and enrich).\n\nTake the identified data sources and bring them into our Data Lake, Azure Fabric. Once the data is in, the engineer will need to be able to transform data (de-dup, clean and enrich the data). We are focused on using a medallion data model for data architecture. The tools we are working with are: Delta Parquet files, python, spark, notebooks, T-SQL, Fabric lakehouse/warehouse, Dataflows Gen2, Data Pipelines, Direct Lake, shortcuts, ADF."}, {"title": "Data Engineer", "company_name": "Catalist", "location": "Washington, DC (Remote)", "link": "/jobs/view/3744951279/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ui1Fk6ICmnPNqAlZDZAcwA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3744951279", "description": "About the job\nFor over 17 years, Catalist has been a leader in civic data and data science innovation. Our mission is to provide progressive organizations with the data, software, and services needed to better identify, understand, and communicate with the people they need to engage and mobilize. Our clients include the largest, most influential organizations in the U.S. active in civic engagement, advocacy, and political campaigns.\n\nCatalist is home to a dedicated, creative team of technologists, data scientists, and campaign experts committed to using our talents and technology to nurture a vibrant and growing progressive community.\n\nAs a Data Engineer at Catalist, you will work closely with the Analytics and Technology departments to design, build, support, and maintain various data pipelines and processes with an end goal of providing data and intelligence to the progressive community. This role primarily involves translating data architecture designs into functional processes, code, and systems.\n\nThe ideal candidate will be a highly motivated individual with excellent technical skills, a strong desire to learn new skills, and an interest in progressive politics. Catalist values creativity and problem-solving. Our work is on the cutting edge of data-driven politics, and your support will help Democratic candidates and progressive organizations conduct successful advocacy and electoral campaigns.\n\nThis position reports to the Deputy Chief Data Officer. The Data Engineer is a part of a growing Data team that supports all underlying work at Catalist.\n\nThis position is included in our CWA bargaining unit.\n\nPrinciple Duties & Responsibilities\n\nDevelop scalable, production processes, code, and systems for data ingest, transformation, modeling, and reporting across a variety of platforms\nTranslate mock-ups and designs into functional processes, code, and systems\nCreate architecture designs when needed\nProvide quality assurance and testing on processes, code, systems, and products\nBecome an internal subject matter expert on various datasets and support other Catalist departments/teams on usage of those datasets\nExecute ad-hoc data and database maintenance tasks as requiredProject manage cross-departmental efforts, with direct responsibility for stakeholder engagement, management, and execution of technical elements\n\n\nRequirements\n\nBS or BA in a technical field, or relevant experience\n1-2 years of experience working with SQL databases\nExperience with data cleaning or ETL processes\nExperience with distributed computing systems and/or distributed datastores (particularly the Hadoop ecosystem)\nExperience managing projects\nFamiliarity with Catalist data, progressive politics, voter files, and/or commercial data\nBackground check required\n\n\nPreferred Skills & Abilities\n\nWillingness to be a problem solver and produce results in a fast paced environment\nAbility to be creative and personable, and articulate ideas clearly\nExperience working with SQL databases\nProficiency with Python or another object-oriented programming language (R, Java, Scala, etc\u2026)\nExperience working in cloud environments (AWS, GCP, etc.)\nExperience working in command line environments such as Bash\nExperience with a version control tool such as git or github\n\n\nBenefits\n\nMedical, Dental, Vision, Prescription Drug\n\nCatalist offers Medical, Dental, Vision, and Prescription Drug coverage for eligible staff and their eligible dependents. Catalist\u2019s Medical plan is a comprehensive PPO program including Prescription Drug coverage with 85% of the premium paid by Catalist. Dental and Vision coverage is provided at no cost to employees.\n\nGroup Term Life Insurance and Long-Term & Short-Term Disability Coverage\n\nGroup Term Life Insurance and Long-Term and Short-Term Disability coverage is available for eligible staff. These benefits are provided at no cost to Catalist employees.\n\n401(k) Safe Harbor Plan\n\nA 401(k) Safe Harbor Plan is available to eligible staff with a 3% contribution from Catalist from the date of hire. Employees may contribute pre-tax or post-tax from their salary up to the legal limits set forth by the IRS.\n\nMedical and Dependent Care Flexible Spending Accounts (FSAs)\n\nCatalist offers an FSA Program that gives eligible staff the ability to pay out-of-pocket medical/dental/vision/child care expenses from pre-tax earnings.\n\nTransit Benefits\n\nCatalist also makes available a Transit benefit FSA program to eligible employees using pre-tax contributions with a company match.\n\nProfessional Development and Remote Work Expenses\n\nEligible employees may be reimbursed up to $750 each year for professional development / education and remote work expenses.\n\nStudent Loan PayDown or SaveUp\n\nCatalist offers a Student Loan PayDown and College SaveUp benefit for eligible staff.\n\nVacation, Personal Leave, Sick Leave Benefits\n\nCatalist offers generous vacation benefits to all eligible staff. Eligible employees also receive:\n\n 14 Paid Holidays\n Personal Days\n Sick Leave\n Parental Leave\n\n\nHybrid Office/Remote Work\n\nCertain positions at Catalist are eligible for Office/Remote Hybrid or full Remote status."}, {"title": "Remote Entry Level Data Analyst/Engineer", "company_name": "SynergisticIT", "location": "White Plains, NY (Remote)", "link": "/jobs/view/3774171423/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=q54sk1%2FRww0z%2BC%2BE8e0jQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774171423", "description": "About the job\nAt SynergisticIT, we're all about making connections. Whatever IT goals you have, our software programmers can help achieve those. Our Software development teams can take up turnkey projects and execute them in an effective and efficient manner. If you are looking to source talent our recruiters will find the ideal IT talent for your company. What's the secret to our success? Well, it all starts with taking quality time to listen to each client's specific needs. After we have a thorough grasp of your IT goals, we can better customize our Developments as per your specific needs. We can also tailor make recruiting programs to exceed your expectations. Since our founding in 2010, SynergisticIT's strategies have earned the company an enviable position in the software development, IT staffing and IT skill enhancement fields. SynergisticIT continues to work with hundreds of satisfied American clients with our software programmers working on our projects and after gaining hands on experience on cutting edge technologies moving to contribute their skills to great clients like Apple, Google, Client, Ebay, Paypal, Kroger, the Walt Disney Company and hundreds more. If you are tired of working with inefficient programmers who take a lot of time to ramp up we want you to try us. Our software programmers can hit the ground running and get you the maximum return on your investment. You have already tried the rest its time you tried the best. SynergisticIT - Home of the Best Data Scientists and Software Programmers in the Bay Area.\n\n Why Us ? \n\nSynergisticIT has a proven track record of successfully skill enhancement and staffing IT employees for some of the world's most iconic brands. Our team takes the time to fully understand every client's needs so we could best meet your IT staffing requirements. The knowledgeable staff at SynergisticIT is always more than happy to work with clients to ensure they reach their software development goals. Besides staffing, SynergisticIT is also committed to helping young IT professionals advance their career with a robust upskill program . Everyone who goes through SynergisticIT's program learns all the skills necessary to succeed in many IT fields ranging from Java to Machine Learning. Additionally, everyone trained at SynergisticIT has been through extensive mock and technical interview screenings to bolster their career prospects. Last, but certainly not least, SynergisticIT takes great care to respect the privacy considerations for every client. All companies who work with SynergisticIT can rest assured their confidential data is protected using the most up-to-date encryption technologies. SynergisticIT also complies with all the latest NDA agreements.\n\n REQUIRED SKILLS For Java /Software Programmers \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Time series analysis \n\n We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2022 and at Gartner Data Analytics Summit (Florida)-2023 \n\nOracle CloudWorld Event (OCW) Las Vegas 2022 | SynergisticIT - YouTube\n\n https://www.youtube.com/watch?v=OAFOhcGy9Z8 \n\n https://www.youtube.com/watch?v=EmO7NrWHkLM \n\n https://www.youtube.com/watch?v=NVBU9RYZ6UI \n\n https://www.youtube.com/watch?v=Yy74yvjatVg \n\nSynergisticIT at Gartner Data and Analytics Summit 2023 - YouTube\n\nFor preparing for interviews please visit  https://www.synergisticit.com/interview-questions/ \n\n We are looking for the right matching candidates for our clients \n\n Please apply via the job posting \n\n REQUIRED SKILLS For Java /Full Stack/Software Programmer \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning Positions \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Tensorflow \n\n If you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team. \n\n No phone calls please.  Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates"}, {"title": "Data Engineer", "company_name": "Anika Systems", "location": "Leesburg, VA (Remote)", "link": "/jobs/view/3787766893/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=mApL9JnN%2B%2Fr2%2BYUPlxRYOA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787766893", "description": "About the job\nAnika Systems is a fast growing, woman-owned small business that specializes in providing innovative IT solutions for federal government agencies. Our expertise lies in accelerating delivery in Data and Analytics, Intelligent Automation, Application Development, and IT Modernization. We are currently expanding our Federal team and are seeking a passionate and talented Data Engineer. This opportunity is 100% remote.\n\nMust be a U.S. Citizen with the ability to obtain and maintain a government suitability clearance\n\nResponsibilities\n\nDesigns, develops, builds, analyzes, evaluates, and installs database management systems to include database modeling and design, relational database architecture, metadata, and repository creation and configuration management.\nDefines and oversees database organizations, standards, controls, procedures, and documentation. Provides technical consulting in the definition, design, and creation of a database environment.\nAdvises applications development staff and users on data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nDesigns and implements databases with respect to access methods, access time, batch processes, device allocation, validation checks, organization, protection and security, documentation, and statistical methods.\nUses data mapping, data mining, and data transformational analysis tools to design and develop databases.\nDetermines data storage and optimum storage requirements.\nPrepares system requirements, source analysis, and process analyses and designs throughout the database implementation.\n\nRequired Skills And Experience\n\nBA/BS and 1 year of relevant experience\nExperience with DataBricks, SQL, Python\nApplications development experience with data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nExcellent communication skills (both oral and written) with process-oriented organizational skills to ensure project success\n\nDesired Skills And Experience\n\nDemonstrated successful development experience with the full life cycle of government database management systems including database modeling and design, relational database architecture, metadata and repository creation, and configuration management.\n\nPowered by JazzHR\n\nTlKWIorucJ"}, {"title": "Remote Work - Need Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3774712978/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=r3Bp%2F0qcOD2OpC5jpbUOcA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774712978", "description": "About the job\nTitle : Data Engineer\n\nLocation : Remote\n\nDuration : 6+ months to start (long term contract (work order are extended 2x year))\n\nPlease Fill out the skill matrix: Technology Some Awareness (1) Novice (2) Intermediate (3) Advanced (4) Expert (5) SQL Tableau Desktop /Creator Tableau API Integration AWS - S3 Glue AWS - Athena Redshift AWS - Lambda AWS - SNS/SQS msExcel\n\nProfile: Data Analyst will work closely with Architects and leads in the Interflow team, key business stakeholders including program staff to support the organizations effort in the areas of data Analysis and information management. Role should have strong experience in writing SQL queries to extract data for reports. Candidate should have experience working with large set of data. As these reports are very critical for the company and we are looking for an expert who is capable of handling, consuming and extracting large sets of data for analysis and reports.\n\nMust Haves\n\nAWS - Athena\nAWS - S3\nGlue\nhands-on development of reporting applications\nMS Excel\nRedShift\nSQL\nTableau Desktop /Creator\n\nNice\n\nAWS - Lambda\nAWS - SNS/SQS\nTableau API Integration\n\nSkills\n\nBA/BS required (major in an analytical field desired) \nA minimum 10+ years work-related experience. \nExperience in hands-on development of reporting applications for the Web in a professional environment \nThe Ethos of continuous improvement and interest in learning new things.\nStrong analytical thinking and structured problem-solving ability\nAbility to handle multiple projects and assignments simultaneously and effectively in a cross-functional team environment. \nVersed on the agile methodology and best practices. \nExcellent interpersonal and collaboration skill with the ability to work with a diverse set of colleagues, across functions, from different organizations, disciplines, etc. \nSelf-starter, ability to set priorities, work independently and attain goals"}, {"title": "Data Engineer", "company_name": "Team Remotely Inc", "location": "Houston, TX (Remote)", "link": "/jobs/view/3786249829/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=78lY5Jo%2FfyRPC6TQYCdvww%3D%3D&trk=flagship3_search_srp_jobs", "id": "3786249829", "description": "About the job\nThis is a remote position.\n\n Data Engineer (US/Canada Residents Only, 1 year experience, remote)\n\nTeam Remotely Inc. is a staffing and recruitment agency that offers a comprehensive solution for talent acquisition, including sourcing, vetting, pay rolling, and managing talent. Whether you need contract staffing, direct hire, direct sourcing, talent pools, or diversity initiatives, our model can support your hiring strategy.\n\n Hiring Type: Full-Time\n\n Base Salary: $55K-$70K Per Annum.\n\nResponsibilities\n\nDesigning user interface changes for web-based DB applications. \nReviewing application requirements and interface designs. \nDeveloping and implementing highly responsive user interface components using react concepts. \nWriting application interface codes using JavaScript following react.js workflows. \nTroubleshooting interface software and debugging application codes. \nDeveloping and implementing front-end architecture to support user interface concepts. \nMonitoring and improving front-end performance. \n\nQualifications And Experience\n\nBachelor\u2019s degree in computer science, information technology, or a similar field. \nPrevious experience working as a react.js developer. \nIn-depth knowledge of JavaScript, CSS, HTML, jQuery, and front-end languages. \nKnowledge of REACT tools including React.js, Webpack, Enzyme, Redux, and Flux. \nExperience with user interface design. \nKnowledge of performance testing frameworks, including Mocha and Jest. \nExperience with browser-based debugging and performance testing software. \nTroubleshooting skills. \nProject management skills. \nProblem-solving skills. \nVerbal communication skills. \n\n Why work with Team Remotely?\n\nTeam Remotely Inc. is a staffing platform offering a seamless experience for employers and candidates. Employers can post job openings and specify their requirements, while candidates can create profiles and upload resumes.\n\nThe team of Team Remotely continuously learns and adapts based on previous successful placements, constantly improving its matching capabilities. This ensures that the recommendations provided by Team Remotely are tailored and accurate, increasing the likelihood of a successful match between employers and candidates. By providing intelligent and data-driven solutions, they strive to enhance the efficiency and effectiveness of the hiring process, ultimately helping companies find the best talent and individuals find their dream jobs."}, {"title": "Data Engineer", "company_name": "ZAG Technical Services", "location": "United States (Remote)", "link": "/jobs/view/3771403464/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=Hf0VefrKXBFIL5MtHmY0Pw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3771403464", "description": "About the job\nWe are looking for a tech savvy Data Engineer to contribute to the building, operating, and scaling of next generation data platforms and tools that will power data-driven analytics capabilities throughout the ZAG products and services portfolio, spanning areas such as business intelligence, reporting, and data analytics.\n\nThe ideal team member is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. Must be comfortable supporting the data needs of multiple clients and teams, systems, and products within the modernization of ecosystems. The right-fit person will be excited by the prospect of optimizing or even re-designing data processes to support next generation of advanced analytics capabilities, which in-turn translates to implementing strategic enhancements to maintain competitive advantage in the industry.\n\nThe successful team member will leverage their proficiency to\u2026\n\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\nBuild the analytics processes required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Data Management and cloud based \u2018big data\u2019 technologies.\nBuild analytics tools that utilize technologies to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.\nCollaborate with client teams to identify and solution regional and local requirements within the context of the standard global business model for enterprise analytics.\nParticipates in meetings, discussions, strategy sessions where changes, improvements, and enhancements are proposed, evaluated, and approved. \n\n\nThe exceptional team member will have or demonstrate progressive experiences in\u2026 \n\nBachelor\u2019s degree or higher in Computer Science or related field\nExperience working with BI and data warehousing tools\nExperience working with enterprise data, where security is paramount and data governance is critical\nAbility to work independently and in a team environment, and effectively engage all levels of the organization\nUnderstanding of distributed systems driving large-scale data processing and analytics with a successful history of manipulating, processing, and extracting value from large, disconnected datasets.\nAbility to communicate effectively (listening, presenting, and questioning)\nStrong organizational, written, and communication skills\n\n\nAbout ZAG\n\nZAG Technical Services empowers our clients\u2019 success through intelligent business solutions that increase employee productivity, network security and team collaboration. We aim to ensure that our client\u2019s IT investments drive ROI, provide a competitive advantage and help businesses grow. We do this by living our core set of values: Integrity Foremost, Client-Centered, Accountable Always, Teamwork Throughout, and Exceptional is the Goal.\n\nWhat ZAG Has To Offer\n\nCompetitive compensation package \nBenefits including medical, dental, vision, 401K and life insurance \n\n\nZAG Technical Services is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all of our employees.\n\nZAG Technical Services is an E-Verify participant and will provide the Social Security Administration (SSA) and, if necessary, the Department of Homeland Security (DHS), with information from each new employee\u2019s Form I-9 to confirm work authorization."}, {"title": "Junior Data Engineer", "company_name": "HireMeFast LLC - Career Accelerator - Land A Job", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3786249797/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D&trk=flagship3_search_srp_jobs&lipi=urn%3Ali%3Apage%3Ad_flagship3_search_srp_jobs%3BQExogrHBQoSF9zZm%2B9kufw%3D%3D&lici=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D", "id": "3786249797", "description": "About the job\nThis is a remote position.\n\n Job Title: Junior Data Engineer\n\n Employment Type: Full-Time\n\n Salary: $64,000 - $76,000 per annum\n\n Experience Required:  Minimum 1 year of project experience\n\nAbout us: HireMeFast is a leading staffing and recruitment agency specializing in connecting businesses with top-tier talent across various industries. Our mission is to bridge the gap between exceptional candidates and organizations needing their skills, expertise, and unique qualities. Our team of experienced and dedicated recruitment specialists utilizes innovative sourcing strategies, and a vast network to identify and attract top talent. We conduct comprehensive procedures to ensure that only the most qualified candidates are presented to our clients.\n\nPosition Summary\n\nJoin the fast-paced, innovative, and collaborative environment focused on providing an AIOps platform that enhances the intelligence of the CVS Health infrastructure. Work closely with subject matter experts and colleagues to build and scale out machine learning and AI solutions that will detect, predict, and recommend solutions to correct issues before system impact and enhance efficiency, reliability, and performance of CVS Health\u2019s IT operations.\n\nKey Responsibilities include:\n\nData pipeline development: Designed, implemented, and managed data pipelines for extracting, transforming, and loading data from various sources into data lakes for processing, analytics, and correlation. \nData modeling: Create and maintain data models ensuring data quality, scalability, and efficiency \nDevelop and automate processes to clean, transform, and prepare data for analytics, ensuring data accuracy and consistency \nData Integration: Integrate data from disparate sources, both structured and unstructured to provide a unified view of key infrastructure platform and application data \nUtilize big data technologies such as Kafka to process and analyze large volumes of data efficiently \nImplement data security measures to protect sensitive information and ensure compliance with data and privacy regulation \nCreate/maintain documentation for data processes, data flows, and system configurations \nPerformance Optimization- Monitor and optimize data pipelines and systems for performance, scalability and cost-effectiveness \n\nCharacteristics of this role:\n\nTeam Player: Willing to teach, share knowledge, and work with others to make the team successful. \nCommunication: Exceptional verbal, written, organizational, presentation, and communication skills. \nCreativity: Ability to take written and verbal requirements and come up with other innovative ideas. \nAttention to detail: Systematically and accurately research future solutions and current problems. \nStrong work ethic: The innate drive to do work extremely well. \nPassion: A drive to deliver better products and services than expected to customers. \n\nRequired Qualifications\n\n2+ years of programming experience in languages such as Python, Java, SQL \n2+ years of experience with ETL tools and database management (relational, non-relational) \n2+ years of experience in data modeling techniques and tools to design efficient scalable data structures \nSkills in data quality assessment, data cleansing, and data validation \n\nQualifications\n\nKnowledge of big data technologies and cloud platforms \nExperience with technologies like PySpark, Databricks, and Azure Synapse. \n\nEducation\n\nBachelor\u2019s degree in Computer Science, Information Technology, or related field, or equivalent working experience\n\nWhy HireMeFast LLC?\n\nAt HireMeFast, we understand that finding the right individuals to join your team is crucial for success and growth of your organization. We are committed to streamlining the hiring process for our clients, ensuring they have access to a diverse pool of highly qualified candidates who are a perfect fit for their specific needs."}, {"title": "Data Engineer  - Need candidates from Texas ( REMOTE )", "company_name": "Pulivarthi Group (PG)", "location": "United States (Remote)", "link": "/jobs/view/3770685212/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=9gdMwuif%2BpB%2BCQYvQi3imQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3770685212", "description": "About the job\nFollow us on Linkedin: https://www.linkedin.com/company/pulivarthigroup/\n\nPulivarthi Group LLC is a Global Staffing & IT Technology Solutions company, with our prime focus of providing world class solutions to our customers with the right talent. We combine the expertise of our team and the culture of your company to help you with the solution that is affordable and innovative using high quality standards and technologies.\n\nWe\u2019ve served some of the largest healthcare, financial services, and government entities in the U.S.\n\nData Engineer - FT only - Azure SQL with data lakes experience\n\nData Engineer -\n\nthe resource requirement for this Data Engineer role needed more clarification.\n\nOur customer is requesting a resource who knows Azure CI/CD (Continuous Integration / Continuous Deployment) to build and integrate their DevOps pipeline.\n\nOnce the DevOps pipelines are mapped, they need the Data Engineer to connect to all the Data Sources, primarily their Data Lake.\n\nThe Customer (based in Houston) is interested in a resource in a time zone not too far removed from the Central Time Zone to allow real time collaboration."}, {"title": "Data Engineer", "company_name": "Otter Products", "location": "Fort Collins, CO (Remote)", "link": "/jobs/view/3766728302/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=77U3U7CAi1PxgDmpI3%2BiNQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3766728302", "description": "About the job\nOverview \n\nOtter Products is currently recruiting for a Data Engineer to join our Data team! You can be based in our Fort Collins, CO office with flexibility to work a portion of your time remotely, or 100% Remote in the U.S.\n\nThe Data Engineer applies professional experience, concepts, and company objectives toward building and operationalizing the minimally inclusive data necessary for the enterprise data initiatives following industry standard practices and tools. The bulk of the Data Engineer's work would be in designing, managing and optimizing data pipelines, then moving these data pipelines effectively into production for key data and analytics consumers like business/data analysts, data scientists or other stakeholder that needs curated data for data and analytics use cases across the enterprise. The Data Engineer also ensures compliance with data governance and data security requirements while creating, improving and operationalizing these integrated and reusable data pipelines to enable faster data access, integrated data reuse and vastly improved time-to-solution for data and analytics initiatives. Additionally, the Data Engineer will be expected to collaborate with other data team members and data consumers working on the models and algorithms developed by them toward optimization for data quality, security, and governance to put them into production leading to potentially large productivity gains.\n\n About Otter Products \n\nOtter Products, we grow to give. From our founder's garage in 1998 to the global technology leader we are today, Otter Products continues to drive growth through innovation.\n\nThrough our industry-leading brands - OtterBox, Liviri and OtterCares - we provide our partners the number-one selling and most trusted products in our categories. Our philanthropic spirit is the foundation on which we foster our partner relationships, allowing us to grow and to give - together.\n\nBy way of our charitable arm, the OtterCares Foundation, we support our communities and invest in our future through education that inspires kids to change the world.\n\nAnd even as our global community of Otters continues to grow, our founder's core values are still at the heart of everything we do. We measure our success by our ability to give back to our communities and strengthen opportunities for all.\n\nFor more information visit otterproducts.com\n\n Responsibilities \n\nComplete analysis, design and development of BI solutions using Azure SQL Server\nFamiliarity with Data Warehouse concepts\nExperience coding the extraction, transformation, and loading (ETL) processes\nDatabase development primarily in SSIS, Data Factory and SQL\nPartner with the business to determine end user database/reporting needs and requirements\nGenerate ad hoc reports using Power BI or SSRS\nCollaborate with other developers to create and implement best approach solution(s)\nTroubleshoot Azure tools, systems, and software\nReview queries for performance issues, making changes as needed\nCollaborate with team to performance-tune Azure application as necessary\nAssist with the analysis and extraction of relevant information from large amounts of historical business data to feed data science initiatives\nParticipate and support the design and documentation of processes for large scale data analyses, model development, model validation and model implementation\nSupport and maintain a positive safety culture by following all safety policies and procedures and actively contributing to a safe working environment\nOther duties as required\n\n Qualifications \n\nBachelor's degree required. Degree in Computer Science or Mathematics preferred\nMinimum of four years of experience in an IT or analytical role required. Experience in database development, report writing and/or statistics preferred.\n\n EEO \n\nOtter Products, LLC is an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, age, marital status, pregnancy, sex, sexual orientation, gender, gender identity or expression, national origin, disability, veteran status, or any other characteristic or status protected by law.\n\n For US Based Roles Only - Compensation Range Minimum \n\nUSD $90,000.00/Yr.\n\n For US Based Roles Only - Compensation Range Maximum \n\nUSD $120,000.00/Yr.\n\n Additional Total Rewards \n\nProfit Sharing Program Eligible, Benefits Eligible - Full Time- check out otterproducts.com/careers/why for more info"}, {"title": "Software Engineer, Distributed Systems", "company_name": "Hightouch", "location": "United States (Remote)", "link": "/jobs/view/3628572539/?eBP=CwEAAAGMd4QO8APODyE_-mMOPhYyQGg5ZjVbPwq_3V7DK0B2bLAf10pcedM9S2T71X5i4TiJuEwOzWBbSiTKbLuk-VD-_Y-dN5uyf8YpYkHYqEYFxySMS9zr2hRjNrxX9gXbxssh35TFK9YTNV2WeH2Ncb675JgFbJTgNWSZySFC6rrdJGmdSDTr2q5d3VoxORz3GjsDbzjeyZQCvQ07RqYw7oujWSgslycs2LMCY_-N0NEj6e0PvbaYKiZ_5IW34bGVSYLXpHwqa8oOcNttYZ90UA3E_7sDSFzABdIVlfrp5zzERrIYy5sh2MIInxHdDoKgQw8VmOn5uM4S0tXUl-i7LnN0_W0NfPBmLiVDNuveIOm4MfDNPxfe5QKPk6FUAVpvgbcoJn9yRyE&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=rph%2FT6Q6Vkp6YoggJf9cJQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3628572539", "description": "About the job\nAbout Hightouch\n\nHightouch\u2019s mission is to empower everyone to take action on their data. Through our Reverse ETL platform, business and data users can seamlessly sync data from where it resides, such as warehouses and databases, to where it is needed, including operational systems and SaaS tools. Traditionally, acting on data has required engineering time and bandwidth, and left most business users stuck with charts and reports that are unable to take automated action on their data. With Hightouch, every business user, without writing any code, can activate data to streamline critical processes, improve marketing performance, and scale operations.\n\nOur team operates with a focus on making a meaningful impact for our customers. We believe in approaching challenges with a first principles thinking mindset, moving quickly and embracing our value of efficient execution, and treating each other with compassion and kindness. We look for team members that are strong communicators, have a growth mindset, and are motivated and persistent in achieving our goals.\n\nHundreds of companies use Hightouch, including Spotify, Ramp, Retool, NBA, Plaid, and Betterment. We\u2019re based in San Francisco, are remote-friendly, and backed by leading investors such as Amplify Partners, ICONIQ Growth, Bain Capital Ventures, Y-Combinator, and Afore Capital.\n\nAbout The Role\n\nWe are looking for a distributed systems engineer to work on the systems that power the syncing engine our customers and other engineering teams rely on. This presents an exciting challenge where you can apply your expertise in distributed systems, performance optimization, and troubleshooting to push the boundaries of what is possible and meaningful improve our ability to move massive amounts of customer data. This role also provides a unique opportunity to work on a multi-cloud and multi-region infrastructure that supports a global customer base.\n\nWe believe in enabling our engineers to do their best work for our customers by giving them extremely high levels of ownership and autonomy. This comes in different forms: you will own and deliver projects from start to finish, you will work directly with customers to solve their hardest scaling problems, and you will have a lot of influence over what we work on as a team and company.\n\nSome of the problems we\u2019ll be working on include:\n\nSync Speed: Customers want to sync a lot of data to important destinations like Facebook and Snapchat, which requires us to analyze every part of our syncing process and find where we can optimize to sync data more quickly\nStreaming Syncing: We currently sync data from sources in batch only, but envision a world where we also do syncs in real-time and support streaming sources like webhooks and queues\nScalability and Reliability: As part of our rapid growth, we\u2019re always evaluating current future bottlenecks in scaling and reliability, and architecting for the next order of magnitude of growth\nPersonalization API: Our Personalization API product provides a low latency caching layer on top of data warehouses that enable customers to use their data warehouse for real-time personalization use cases with \nMulti-Region and Multi-Cloud: Supporting our multi-region and multi-cloud backend, including extending it to launch Hightouch on in new regions to support data residency requirements of our global customer base\n\nWe are looking for talented, intellectually curious, and motivated individuals who are interested in tackling the problems above. This is a senior role, but we focus on impact and potential for growth more than years of experience. The salary range for this position is $170,000 - $240,000 USD per year, which is location independent in accordance with our remote-first policy.\n\nAbout You\n\nYou are an engineer with a passion for solving hard technical problems that generate real value for customers. You\u2019re motivated by high ownership and are comfortable in a fast-paced, startup environment.\n\nYou have experience and comfort with distributed systems and high-scale systems or the ability and desire to build this experience quickly. This experience can take different forms:\n\nYou\u2019ve built syncing engines that process and move terabytes of data per day in a scalable way\nYou\u2019ve created event collection or data streaming services that power company critical systems\nYou\u2019ve owned systems that have undergone orders of magnitude of growth and helped them scale accordingly\nYou have relevant skills and are a fast learner who is excited to take on the challenges we face\n\nInterview Process\n\nOur goal with the interview process is to balance speed with giving both parties opportunities to assess whether there is a strong mutual fit. We will ask you questions, but we want you to ask us questions! Our technical interviews focus on how you design systems because we believe this is the best way for us to see how you work and for you to see how we collaborate. We don\u2019t ask you to write code to solve technical brainteasers that don\u2019t appear in your day to day job.\n\nRecruiter screen [30m]: Introductory call with our recruiting team to get to know each other and see if the role could be a good mutual fit.\nSystems design screen [45m]: Designing a data processing feature end-to-end.\nHiring manager interview [30m]: Chat with hiring manager about past experiences and future operating preferences to assess fit on company values and operating principles.\nSystems design interview [90m]: Work with the interviewer to architect a system at a conceptual level. The problem will be at a pretty high level - and have both product and customer requirements as well as technical."}, {"title": "Entry-Level Data Scientist Engineer", "company_name": "Phoenix Recruitment LLC", "location": "Texas City, TX (Remote)", "link": "/jobs/view/3788727625/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=8iwtytHsCu4wMwJ4fj3pxw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3788727625", "description": "About the job\nThis is a remote position.\n\nTitle - Entry-Level Data Scientist Engineer, 1 year of project experience\n\nEmployment Type: Full-time\n\nBase Salary: $60K-$70K\n\nPhoenix Recruitment offers a variety of recruiting services to assist both employers and employees. They specialize in marketing open positions, recruiting, and helping employers to find qualified candidates across various industries. Phoenix Recruitment has expertise in streamlining the hiring process. They can help ensure that the process is efficient, well organized, and compliant with relevant regulations.\n\nSkills and Abilities:\n\nStrong knowledge of R or Python for data analysis and modeling. \nProficiency in statistical programs such as R, SAS, MATLAB, or Python. \nFamiliarity with spreadsheets (VBA) and database applications (Access, Oracle, SQL, or equivalent technology). \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \nAbility to learn new methods quickly and work under deadlines. \nExcellent teamwork and communication skills. \nStrong analytical and problem-solving abilities. \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \n\nPreferred:\n\nKnowledge of actuarial concepts and life, health, and/or annuity products. \nExperience with statistical modeling techniques such as GLM, Decision Trees, Time Series, Regression, etc. \nFamiliarity with Microsoft DeployR. \nExposure to insurance risk analysis. \nBasic experience in computational finance, econometrics, statistics, and math. \nKnowledge of SQL and VBA. \nFamiliarity with R or Python for predictive modeling \n\nWhy Phoenix Recruitment LLC?\n\nPhoenix Recruitment often has an extensive network of employers and candidates. This network allows them to tap into a pool of qualified candidates and connect them with suitable job opportunities. They can also leverage their connections to help employers find the right talent efficiently. Outsourcing the recruitment process to a specialized agency can save your time and resources, avoid delays, reduce administrative burdens, and increase the chances of finding the right fit for your organization."}, {"title": "Data Engineer - Remote Work", "company_name": "BairesDev", "location": "United States (Remote)", "link": "/jobs/view/3783162297/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=OtoCrAAHZFomVup04fy35g%3D%3D&trk=flagship3_search_srp_jobs", "id": "3783162297", "description": "About the job\nWho We are\n\nBairesDev is proud to be the fastest-growing company in America. With people in five continents and world-class clients, we are only as strong as the multicultural teams at the heart of our business. To consistently deliver the highest quality solutions to our clients, we only hire the Top 1% of the best talents and nurture their professional growth on exciting projects.\n\nWe are looking for a Data Engineer to join our Development team and participate in different projects made up of multicultural teams distributed throughout the world.\n\nWhat You Will Do\n\n Interview other Data Scientists and Data engineers to vet their technical abilities.\n Input data from those interviews into the product and help develop a predictive, fair, and enjoyable solution.\n Work together with developers, tech leads, and solution architects to build applications.\n Improve existing structures by adding new functionalities or proposing technological updates so that as a result, the impact of your contribution is significant in the core of each business.\n Improve the user experience on scalable and high availability platforms, contributing to the key differential of each business.\n\nHere\u2019s What We Are Looking For\n\n 5+ years of experience working as a developer.\n 3+ years of experience working as a Data Scientist / Data Engineer.\n Proficient with analysis, troubleshooting, and problem-solving.\n Hands-on experience with managing data loads and data quality.\n Advanced English level.\n\nHow we do make your work (and your life) easier:\n\n 100% remote work.\n Hardware setup for you to work from home.\n Flexible hours - make your schedule.\n Paid parental leave, vacation & holidays.\n Diverse and multicultural work environment.\n An innovative environment with the structure and resources of a leading multinational.\n Excellent compensation \u2014 well above the market average.\n Here you can grow at the speed of your learning curve.\n\nOur people work remotely but with a consistent and robust culture that promotes diversity and teamwork. To continue being the leading software development company in Latin America, we want to ensure that every BairesDev member gets the best growth and professional development opportunities in a diverse, welcoming, and innovative environment.\n\nEvery BairesDev team member brings something unique to our company.\n\nWe want to hear your story. Apply now!"}, {"title": "Data Engineer (Remote)", "company_name": "Cascade Debt", "location": "United States (Remote)", "link": "/jobs/view/3755336571/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=pmUtn1j2zyPr25CNPh2RNw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3755336571", "description": "About the job\nDo you want to work in a small team where you can make a real difference in a company? Would you embrace the challenge of building a high impact platform that is used globally? Then we want to speak with you!\n\nAbout Cascade\n\nCascade is a fintech startup backed by Canadian and US investors that empowers companies to grow by democratizing access to institutional debt. We are building tools that modernize how companies raise and manage debt, lowering the barriers to entry to the $7 trillion specialty finance and alternative credit market for companies around the world. Founded just last year, we are gearing up for our next phase of technical development and are seeking a talented Data Engineer to lead the way.\n\nAbout The Role\n\nAs a Data Engineer you\u2019ll be an early member of a growing team building a pioneering platform in the debt infrastructure space. You will design, build, and launch efficient, scalable, and reliable data pipelines to move and transform data .\n\nWhat You\u2019ll Do\n\n Building databases, data lakes and data ingestion pipelines to integrate customer databases and datasets to our systems \n Ingesting financial datasets from external customers, then u pdating and maintaining accurate and complete data mappings to ensure that our product s are displaying high quality data \n M onitoring and alerting across our data pipelines in order to make sure that our data ingests are reliable and correct \n Translat ing business goals and stakeholder requirements into new data flows and deliver analytical insights ensuring that the data flows create real business value continuously \n\nSkills And Experience\n\n 2 + years experience in a data engineering or data analyst role \n E xperience with relational SQL and NoSQL databases, including PostgreSQL , MySQL, MariaDB, MongoDB, Bigtable, etc. \n E xperience working with ETL technologies , such as Databricks, Fivetran , or dbt \n Proficiency in writing SQL queries and knowledge of analytical data warehouses such as RedShift, BigQuery , and Snowflake \n Some experience with CI/CD automation such as GitHub Actions \n Developing APIs and integrating with 3rd party APIs \n Bonus: experience in fintech / SaaS / credit analysis \n\nPeople Who Thrive At Cascade Are\n\n Self-starters, who take the initiative to tackle challenges in a remote work environment \n Good communicators, who can operate without ego to discuss, learn, grow, and help others do the same \n Passionate about creating great digital experiences for users \n Problem solvers who are not satisfied with the status quo \n\nBenefits\n\nRemote: we are a remote-first company and are very flexible on hours as long as things get done \nHome-office: there\u2019s a $1000 USD home office allowance to set yourself up \nEquity: we expect you to have an owner-mentality, and have the equity plan to match \nBenefits: health, dental, vision, and more \nPerks: we offer free lunches weekly and off-site trips \nJob satisfaction: we offer autonomy, ample opportunities for mastery, and an opportunity to make a difference for companies around the world"}, {"title": "Data Engineer", "company_name": "ISC (Integrated Specialty Coverages, LLC)", "location": "Carlsbad, CA (Remote)", "link": "/jobs/view/3778293528/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ew7%2FfEuXSujs5IGabdqUyw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3778293528", "description": "About the job\nAbout Integrated Specialty Coverages\n\nIntegrated Specialty Coverages LLC (ISC) is a growth stage technology and data-driven insurance company leading innovation in the market.\n\nBacked by one of the leading private equity firms, KKR, and led by a forward-thinking management team, ISC is combining the worlds of insurance and technology to create an Insurtech powerhouse. As a leading online distributor of insurance products for a range of industries and \"Main Street USA\", we are looking for the right people to help us in our mission of achieving exponential growth. We strive to be the number one place to go for brokers and agents to source insurance. To accomplish this, we're building a digitally focused team that deeply understands the intersection between user experience, data, and AI/ML to optimize the way we engage with our customers and partners.\n\nJob Summary\n\nThe Data Engineer role will be the technical liaison between multiple groups including a data science team, the engineering team, product management, and business stakeholders. You do not need any insurance knowledge prior, however, you must quickly dive deep into the insurance world and ask questions to become a subject matter expert. You will be responsible for building a data platform to facilitate the data science team. You must be a self-starter that can build out features such as a data pipeline from scratch. There will be support from both engineering and data science for any buildout.\n\nJob Responsibilities\n\nCreate and maintain optimal data pipeline architecture. \nAssemble large, complex data sets that meet functional / non-functional business requirements. \nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. \nBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using AWS technologies, SQL, and Python. \nWork with stakeholders including the Executive, Product, Data Science, and Engineering teams to assist with data-related technical issues and support their data infrastructure needs. \nWork with data science and analytics teams to strive for greater functionality in our data systems. \n\nMinimum Qualifications:\n\nAdvanced working SQL knowledge and experience working with relational databases, strong query authoring (SQL) as well as working familiarity with a variety of databases (Snowflake, Redshift, MySQL, MSSQL, etc.)\nExperience building and optimizing data pipelines, architecture and data sets. \nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. \nBuild processes supporting data transformation, data structures, metadata, dependency, and workload management. \nA successful history of transforming, processing and extracting value from large disconnected, datasets from a variety of data sources (Flat files, Excel, databases, etc.)\nStrong analytic skills related to working with unstructured datasets. \nStrong project management and organizational skills. \nExperience supporting and working with cross-functional teams in a dynamic environment. \nAbility to mentor/guide/collaborate with other team members. \n\nWe are looking for a candidate with 3-5 years of experience in a Data Engineer role. They should also have experience using the following software/tools:\n\nExperience with relational and MPP databases, including Snowflake and MySQL. \nExperience developing software in an agile environment from the requirements stage to production. \nExperience with version control: git\nExperience with container technologies: Docker\nExperience with data pipeline and workflow management tools: Airflow, Jenkins, AWS Glue, Azkaban, Luigi, etc. \nExperience with AWS cloud services: EC2, ECS, Batch, S3, EMR, RDS, Redshift\nExperience with other cloud services: Snowflake, Airflow\nExperience with object-oriented/object function scripting languages: Python, Java, C++, etc. \nExperience with data modeling and data warehouse design\nExperience with data visualization tools (PowerBI, QuickSight)\n\nThe starting annual pay scale for this position is listed below. Actual starting pay will be based on factors such as skills, qualifications, training, and experience. In addition, the company offers comprehensive benefits including medical, dental and vision insurance, 401(k) plan with match, paid time off, and other benefits.\n\nISC's salary ranges are determined by role and level. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations and could be higher or lower based on a multitude of factors, including job-related skills, experience, and relevant education or training.\n\nNational Pay Range\n\n$100,000\u2014$110,000 USD\n\nBenefits of Working at ISC\n\nCompetitive vacation and flexible working arrangements\nComprehensive and inclusive health benefits\nA variety of professional development and mentorship opportunities\nChoice of technology whether at home or in the office\n\nISC's Ownership Behaviors\n\nDo the Right Thing *Be Relentless. Pursue Excellence. *Personal Responsibility & Own Outcomes\nTry Fast. Learn Fast. Fail Fast. Think Big. *Build Extraordinary Owners & Join Forces *Grit & Determination\n\nApplicants with disabilities may contact ISC HR department via e-mail, and other means to request and arrange accommodation. If you need assistance to accommodate a disability, you may request accommodation at any time. Please contact ISC at HR@ISCMGA.com.\n\nISC believes in creating long-term relationships by being responsive and relevant and by consistently delivering value to our community of customers. Specifically, with our employees, we focus on attracting, developing, and retaining the best talent for our business, challenging our people, demonstrating a \"can-do\" attitude, and fostering a collaborative and mutually supportive environment. ISC is an equal opportunity employer and a participant in the US federal E-Verify program (US).\n\nDiversity creates a healthier atmosphere: All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, gender, gender identity, sexual orientation, marital status, medical condition, genetic information, mental or physical disability, military or veteran status, or any other characteristic protected by local, state, or Federal law."}, {"title": "Data Engineer -Remote", "company_name": "RIT Solutions, Inc.", "location": "Denver, CO (Remote)", "link": "/jobs/view/3768007496/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=1hQH8EDaII1eJ7nQ9BACQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3768007496", "description": "About the job\nDuration: 6+ months CTH\n\nMust have a valid LinkedIn profile\n\nRequired Skills\n\n 3-6 years of experience (no more than 6 years)\n Start up environment experience\n Python, SQL, and DBT"}, {"title": "Big Data Engineer - W2", "company_name": "Megan Soft Inc", "location": "United States (Remote)", "link": "/jobs/view/3674845652/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fCAxh4N%2F4YZJ5aEIASaeeA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3674845652", "description": "About the job\nTitle: Big Data Engineer - W2\n\nLocation: Dearborn, MI (Remote)\n\nDuration: 12+ Months\n\nDirect Client: Ford Motors\n\nMust have skills: Should be able to handle GCP requirements\n\n\"Translates business requirements to build repeatable, sustainable, efficient, coded processes that can be productionized by Software Engineers and readily modified by other Data Engineer Creates POC processes in Dev/QA and works with Software Engineers to productionize and align best coding and process practices that maximize efficiency, speed, stability, system resources and capabilities Awareness of and compliance with data privacy, security, legal and contractual guidelines Incorporates data quality and privacy checks/alerts to minimize bad data being consumed by end users, models and dashboards, and to protect customer data; revises checks for new data issues Validates data products and pipelines are functioning as expected following system or application upgrades, source changes, etc. Works closely with Architects if required to align systems, tools and applications being utilized with business use case and performance requirements Provides data product support and maintenance Establishes, tracks and monitors KPIs related to specific data products and deliverable Participate in iteration cadence activities (Iteration Planning, task estimation, daily scrum, demos, etc.). Will be a part of on-call team to resolve critical production support tickets during business hours and off hours.\n\nSkills Required\n\nExperience with Big Data Technologies (Hadoop , ETL Tool, Spark. Hive etc..) Someone who understands Cloud and has experience working with Cloud Platforms (preferably GCP) Should be able to handle GCP requirements - ingesting into GCS/ Big Query, working with GCP components Hands-on in Data Flow, Cloud Storage (GCS), Big Query and GitHub. Proficient in Python and SQL.\n\nSkills Preferred\n\nManages ETL: uses programming and tools for data ingestion, configures pipelines, applies transformations and decoding, integrates and fuses data, moves and securely deliver Experience NoSQL databases\n\nAdditional Information\n\nIf the candidate is remote only, please indicate \"\"100% Remote\"\" under candidate's name on resume. Other similar schedule notations can be \"\"Local Candidate\"\" or \"\"Hybrid Candidate.\"\" ***HRA Test Required***\""}, {"title": "Remote Opportunity: Quantexa Data Engineer", "company_name": "SPAR Information Systems LLC", "location": "United States (Remote)", "link": "/jobs/view/3680073191/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ioykVCZSOQ7BK5%2FwRQHOWg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3680073191", "description": "About the job\nHello All, Hope you are doing great!! Please go through the job descriptions and let me know your interest. Role: Quantexa Data engineer  Location: Remote Duration: Long Term Contract Job Description: 1. End to End usage of Quantexa including guiding our client team on how to use from both front end and data ingestion Reason they are asking for Certification 2. Resolve Entity issues with data 3. Knowledge of Scala and Hive which requires coding experience using parquet 4. Advanced SQL knowledge requiring writing SQL queries 5. Experience having worked with GCP data lake 6. Quantexa certification required\n\nThanks & Regards,\n\nSatnam Singh\n\nDirect: 201 623 3660\n\nEmail : Satnam.singh@sparinfosys.com"}, {"title": "Remote Work - Need Staff Data Engineer - USC only", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3746283980/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=CvIuLBNO%2BPMGi4IXVRkP0A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3746283980", "description": "About the job\n100% Remote, client is in NY\n\nLook for strong job history, Python, Machine Learning, SQL, 10/10 comms, etc. \n\nMUST be an Engineer \n\nSummary\n\nWe're looking for an experienced Data Engineer to join our client's growing team of top notched IT professionals. You will work closely across multiple groups including Editorial and Audio, Marketing, Advertising, etc. to achieve their goals and objectives. The Data Engineering team builds tools throughout the stack while sharing knowledge across these departments.\n\nAs the company grows, they're looking for Data Engineer who will help solidify and expand our pipelines and maintain their data warehouse. Their business model is based on performing complex and very detailed analyses of how our products perform with their subscribers. You'll be responsible for working alongside analysts, product, marketing, and other teams to define new data collection components and measurement schemes that support each new product and feature.\n\nHow Can Add Value To Their Mission\n\nCreate and maintain data pipelines to provide insights and drive business decisions\nEstablish data warehousing strategy (ex. Kimball, Data Vault, etc.)\nMaintain data infrastructure on our AWS accounts\nCollaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.\nWrite unit/integration tests, contributes to engineering wiki, and documents work\nImplement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it\n\nWhat You'll Need To Succeed\n\n7+ years of industry experience measuring product performance and user behavior\nExperience working with a variety of data management technologies, including RedShift, Kinesis/Kafka, Glue, Spark, Postgres, Airflow, dbt, and others.\nExperience implementing BI reporting tools such as Looker\nExperience interfacing with engineers, product managers and analysts to understand data needs\nKnowledge of a variety of measurement beacons, SDKs, APIs including Google Analytics, Amplitude, Braze, Stripe, email service providers\nA commitment to building secure, resilient, fault-tolerant architectures, with clear documentation and procedures in place for support\nA focus on accuracy and detail as you build, ensuring data pipelines produce clear, predictable results\nUnderstanding of the typical metrics a subscription and advertising-supported business needs to measure success\nExperience with Client techniques as applied to behavioral segmentation or anomaly identification is a definite plus\nFamiliarity using developer tools that increase productivity and facilitate the development of resilient code (eg. Docker, CircleCI, Serverless) is a plus"}, {"title": "ENTRY LEVEL DATA ENGINEER (REMOTE)", "company_name": "SynergisticIT", "location": "Jacksonville, FL (Remote)", "link": "/jobs/view/3767587909/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=om7L3CnowiC4PLIcpZD70A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3767587909", "description": "About the job\nSYNERGISTICIT wants every candidate to know that the Job Market is Challenging and to stand out, you need to have exceptional skills and technologies and that's where we come in to make sure you get the attention which you need\n\nSynergisticIT understands the complex nature of the job market and how difficult it can be to secure a position, especially for fresh graduates. Therefore, we assist and help tech-savvies to convert their passions into professions. We go above and beyond to keep you working in your niche.\n\nAs we focus on long-term success, we provide complete career development solutions. From job search to upskilling portfolio and interview preparation, we can guide you at every step of your career.\n\nSynergisticIT spares no efforts to connect you with a large network of tech giants, including Google, Apple, PayPal, Dell, Cisco, Client, etc. Presently, we are actively looking for ENTRY LEVEL DATA ENGINEER a driven mindset. Get the right opportunity and gain experience in building web-centric solutions on Java.\n\nWho Should Apply : Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or anyone looking to make their career in IT Industry\n\nWe also assist in filing for STEM extension and H1b and Green card filing.\n\nCandidates who are serious about their future in the IT Industry and have set big goals for themselves.\n\nCandidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. We also offer Skill Enhancement Programs if the candidates are missing skills or experience which our clients need with great outcomes\n\nCandidates can benefit from skill enhancement if they fall into the below categories. If they are qualified with enough skills then no need for skill enhancement\n\nCandidates Who Lack Experience\n\nHave had a break in careers\n\nLack Technical Competency\n\ncandidates who want to get employed and make a career in the Tech Industry \n\nPlease Also Check The Below Links\n\nhttps://www.synergisticit.com/candidate-outcomes/\n\nhttps://www.synergisticit.com/java-track/\n\nhttps://www.synergisticit.com/data-science-track/\n\nhttps://www.synergisticit.com/which-is-the-best-option-for-tech-job-seekers-staffing-companies-consulting-companies-bootcamps-or-synergisticit/\n\nhttps://www.synergisticit.com/contact-us/\n\nIf the skills are not a match candidates can opt for Skill enhancement. Or their resume can be sent out to clients to see if responses are achievable\n\nREQUIRED SKILLS For Java/Software Programmers\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Core Java , javascript , C++ or software programming\nSpring boot, Microservices and REST API's experience\nExcellent written and verbal communication skills\n\nFor data Science/Machine learning\n\nRequired Skills\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Statistics, Python, data visualization tools\nExcellent written and verbal communication skills\n\nPreferred skills: NLP, Text mining, Tableau, Time series analysis\n\nTechnical skills are required by clients for selection even if its Junior or entry level position each additional Technical skill helps a candidate's resume to be picked by clients over other job seekers.\n\nClients hire candidates with the right technical skills which they need and reject candidates who lack the required technical skills.\n\nNo third party candidates or c2c candidates\n\nPlease apply to the posting\n\nNo phone calls please. Shortlisted candidates would be reached out"}, {"title": "Data Engineer - Remote | WFH", "company_name": "Get It Recruit - Information Technology", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3782241984/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=BgBf1kSAAX%2BSuw6inJsIVw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3782241984", "description": "About the job\nJoin an innovative team at a leading company in the engineering and manufacturing industry, where we are committed to leveraging data for continuous improvement and operational excellence. This hybrid position allows you to enjoy the flexibility of remote work on Mondays and Fridays.\n\nWhy Join Us\n\nAt our company, we prioritize your well-being and professional growth. We offer a comprehensive benefits package, including excellent health benefits, a 6% 401(k) match, generous paid vacation and sick time, 12 paid holidays, and more.\n\nRole Overview\n\nAs a Data Engineer, you will play a pivotal role in transforming raw data into actionable insights, driving improvements in engineering and manufacturing processes, enhancing product quality, and optimizing operational efficiency. Collaborate with dynamic cross-functional teams to extract, analyze, and visualize data, influencing strategic decisions through your valuable insights.\n\nKey Responsibilities\n\nData Collection and Integration:\n\nGather, clean, and integrate data from diverse sources, including production databases, sensor data, and external datasets.\n\nData Analysis\n\nConduct exploratory data analysis to identify patterns, trends, and anomalies.\n\nPerform statistical analysis and hypothesis testing to derive meaningful insights.\n\nData Visualization\n\nCreate informative and visually appealing dashboards and reports using tools like Tableau, Power BI, or Python libraries.\n\nPredictive Modeling\n\nDevelop predictive models to forecast manufacturing performance, quality, and operational outcomes.\n\nRoot Cause Analysis\n\nInvestigate production issues and quality concerns by analyzing data to identify root causes and recommend corrective actions.\n\nPerformance Monitoring\n\nEstablish and maintain key performance indicators (KPIs) to track manufacturing and engineering metrics.\n\nGenerate regular reports to communicate performance trends to stakeholders.\n\nCross-functional Collaboration\n\nCollaborate with engineering, manufacturing, and quality assurance teams to understand their data needs and provide analytical support.\n\nContinuous Improvement\n\nSuggest process improvements and optimizations based on data analysis.\n\nData Security And Compliance\n\nEnsure data security and compliance with relevant regulations, such as GDPR or industry-specific standards.\n\nInfrastructure And Development\n\nBuild and maintain the infrastructure for collecting, storing, and processing data.\n\nDesign, develop, and manage data pipelines, ETL processes, and data warehouses.\n\nRequired Experience\n\nProven experience as a Sr. Data Analyst and/or Data Engineer in an engineering or manufacturing environment.\n\nProficiency in data analysis tools and programming languages, such as Python, R, SQL, and Excel.\n\nStrong data visualization skills with tools like Tableau or Power BI.\n\nExcellent knowledge of statistical analysis and machine learning techniques.\n\nAbility to communicate complex findings and insights effectively to both technical and non-technical stakeholders.\n\nStrong problem-solving skills and attention to detail.\n\nFamiliarity with manufacturing processes and quality control is a plus.\n\nExperience with enterprise ERPs (SAP S4HANA, Oracle EBS), CRMs (Salesforce, Hubspot), and IoT.\n\nProficient in working with cloud computing platforms including AWS, GCP, and Azure.\n\nExpertise in configuring and optimizing cloud-based data solutions for scalability, reliability, and performance.\n\nBachelor's Degree in Data Science, Statistics, Engineering, or Computer Science Required.\n\nApply Now and be part of a team shaping the future of data-driven innovation in engineering and manufacturing!\n\nEmployment Type: Full-Time"}, {"title": "Entry Level Software Engineer - Data Backend Engineer (Remote - Canada)", "company_name": "Yelp", "location": "United States (Remote)", "link": "/jobs/view/3752881757/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fankeoxifvhpTzOYvowHYQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3752881757", "description": "About the job\nYelp engineering culture is driven by our values: we\u2019re a cooperative team that values individual authenticity and encourages creative solutions to problems. All new engineers deploy working code their first week, and we strive to broaden individual impact with support from managers, mentors, and teams. At the end of the day, we\u2019re all about helping our users, growing as engineers, and having fun in a collaborative environment.\n Interested in exploring data and finding innovative ways to collect it, curate it, and make it easily accessible for others to use? How about doing that at an enormous scale? Yelp\u2019s datasets contain billions of interactions between users and local businesses around the globe. If working with our vast amount of data sounds exciting, come and join us!\n\nThe Marketplace Data Observability team is responsible for managing, governing, and monitoring all of Yelp\u2019s Ads data to protect our revenue stream and also support offline analysis that aids in making long term company decisions. We build tools that help us to better understand our data quality, and we strive to uncover data issues before they impact our consumers.\n\nAs a Data Backend Engineer on this team, you will be responsible for building elegant and scalable data products that serve critical, up-to-date, structured information to support different types of analytics within Yelp. As a core contributor to our growing data modeling and data warehousing engineering efforts, you will help design and own mission-critical data flow pipelines and datastores to enable decisions including effective A/B testing and company investments.\n This opportunity is fully remote and does not require you to be located in any particular area in Canada. We welcome applicants from throughout Canada. We\u2019d love to have you apply, even if you don\u2019t feel you meet every single requirement in this posting. At Yelp, we\u2019re looking for great people, not just those who simply check off all the boxes.\n  Build systems that can effectively store and crunch terabytes of data.\nDesign and develop data models for efficient data storage, retrieval, and reporting.\nCreate and maintain conceptual, logical, and physical data models using industry-standard modeling tools.\nCollaborate with cross-functional teams, including engineers, data analysts, business analysts, and data scientists, to understand data requirements and translate them into effective data models.\nParticipate in data integration efforts, including ETL processes and data migration.\nStay up-to-date with industry best practices, emerging technologies, and trends related to data warehousing.\nSupport on-call rotations as needed to operate the team.\n  Understanding of high performing and scalable data systems.\nExperience in building and orchestrating ETL pipelines.\nExperience with Data Lake or Data Warehouse landscape.\nA hunger for tracking down root causes and fixing them in systematic ways.\nAbility to communicate effectively to technical and non-technical cohorts alike. \nExposure to some of the following technologies: Python, AWS Redshift, AWS Athena / Apache Presto, Big Data technologies (e.g S3, Hadoop, Hive, Spark, Flink, Kafka etc), NoSQL systems like Cassandra, DBT."}, {"title": "Data Engineer (Associate Consultant), June 2024", "company_name": "UDig", "location": "Nashville, TN (Remote)", "link": "/jobs/view/3716039960/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=XUjo40kU55HnDCqJ%2F3FdPg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3716039960", "description": "About the job\nCan UDig It? \n\nUDig designs, builds, and implements technology solutions that deliver on business objectives. People join our team because they are excited to have an impact on the future of our organization, passionate about delivering meaningful results, and energized by solving complex problems. Oh, and they like working with other driven, smart, and authentic people!\n\nUDig is proud to be recognized as a \"Best Places to Work\" in Virginia and Tennessee. We're committed to fostering a transparent environment where leadership is accessible, professional development opportunities are abundant, and you are empowered to achieve your career goals. You'll collaborate, develop deep relationships with teammates, and accelerate your success while having fun!\n\nIf you're graduating in May 2024 and excited by the idea of impacting organizations by building data solutions to achieve business goals, you should consider joining our growing technology team.\n\nHow Things Go Down\n\nAs a new, entry-level teammate, you'll take part in our \"Breaking Ground\" training program designed to help you quickly get up-to-speed on UDig's delivery approach within a project team. The program covers:\n\nUDig delivery process and methodologies such as gathering requirements and delivering projects using agile\nTechnical guidance and instruction on the best practices from Data leaders at UDig\nDelivering executive and technical presentations\nCoaching to prepare you to for success on your first client project at UDig\n\nA typical day might entail:\n\nProject standups and check-ins\nDesign and development of data pipelines, BI reports, and more\nPlanning and participating in project demos\nContributing to internal UDig projects\nCareer and job growth through UDig-provided training\nCreating long-lasting friendships with teammates through fun activities as well as your projects (including social events, winning a prize for guessing the weight of pumpkin, or sharing experience on cool tech side projects)\n\nUDig believes it is important for Associate Consultants going through our Breaking Ground program to reside in one of our major markets (Richmond, VA or Nashville, TN metro area) for at least the first year of employment.\n\nWant to learn more? Hear directly from our teammates by checking out our 'A Day in the Life' blog series: https://www.udig.com/digging-in/breaking-ground-2023/\n\nSound Interesting? Here's the Technical Piece\n\nBelow are a few boxes you should be able to check if\u202fyou're interested: Don't check every box? Go ahead and apply!\n\n1+ year of experience with relational database management systems such as Microsoft SQL Server, MySQL, or Oracle\nUnderstanding of the full lifecycle of design, development, and implementation of data solutions including architecture design, development, testing\nSome experience in ETL development or reporting is a plus\nAbility to present technical ideas and high-level concepts and solutions to internal and external team members with varying degrees of technical knowledge\nBachelor's Degree in in computer science, information technology, or any science or business discipline\nAbility to take your work seriously but also seriously enjoy a good time with your UDig colleagues\n\nWhat's in It For You? \n\nAt UDig, you will build your career alongside talented and experienced developers while gaining experience and having fun! We provide continuous challenges and professional development opportunities. Additionally, we offer: \n\nCompetitive salary, merit reviews, and career advancement paths\nFlexible, hybrid environment\nIndividual $1500 Training Budget\nRegular team building and social activities (virtual and/or in-person)\nTransparent culture with strong communication and access to leadership\nGreat benefits like\nGenerous Paid Time Off, company holidays, and parental leave\nMultiple Single and Family Health Insurance plans to choose from\nDental & Vision coverage\nShort-Term and Long-Term disability\nOptional accident and critical illness coverage\nMatching 401(k)\nand more!"}, {"title": "Expression of Interest: Data Engineer", "company_name": "Fingerprint for Success (F4S)", "location": "New York, NY (Remote)", "link": "/jobs/view/3787769880/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=l%2FEDpQqUNmCttGdIyTk7Cg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787769880", "description": "About the job\nWe are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.\n\nThe F4S Talent Pool is a pilot project designed to:\n\nHelp job seekers get discovered by our partners based on their anticipated hiring needs.\nProvide optional support and resources for job seekers in their career endeavors.\nHelp individuals understand, and bring out the best in themselves and each other.\n\nThe F4S Talent Pool process:\n\nOnce you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.\n\nAbout Fingerprint For Success (F4S)\n\nBacked by 20+ years of research, F4S\u2019s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high performing teams.\n\nKeep in mind, joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your F4S work style assessment to match the hiring needs of our partners.\n\nYour feedback is a gift! Write to us via:\n\nPowered by JazzHR\n\noWuPJ9lwq9"}, {"title": "Data Engineer Seattle, WA (remote)", "company_name": "Conch Technologies, Inc", "location": "United States (Remote)", "link": "/jobs/view/3765052211/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=SBETI78UMutCHuRp5FnP%2FA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3765052211", "description": "About the job\nHi,\n\nGreetings from Conch Technologies Inc\n\nData Engineering Lead \u2013 With Databricks and Unity Catalog\n\nLocation \u2013 Seattle, WA (remote)\n\nDuration \u2013 12 months\n\nNeed: Design and implement data architecture and strategy, leveraging technologies such as Databricks, Unity Catalog, Privacera, and Collibra.\n\nAs a Data Engineering Lead, you will play a pivotal role in defining and executing technical deliverables while assessing complexities and levels of effort. You'll engage with both vendor and internal teams to resolve technical challenges and ensure seamless collaboration. Your leadership will extend to planning and prioritization with the SCRUM master, facilitating effective coordination between onsite and offshore teams, and driving feature prioritization through collaboration with product managers.\n\nKey Responsibilities\n\n Lead the definition and breakdown of technical deliverables, assessing complexities and levels of effort.\n Collaborate with vendor and internal teams to address technical issues.\n Drive planning and prioritization in conjunction with the SCRUM master.\n Coordinate with onsite and offshore teams for timely project delivery.\n Collaborate with product managers to prioritize features, communicate with stakeholders, and clarify requirements.\n Collect and manage user-reported issues, ensuring prompt resolution and communication.\n Define comprehensive test plans, oversee their execution, and document results.\n\n--\n\nWith Regards,\n\nNagesh G\n\nMobile: 408-381-5645\n\nDesk: 901-313-3066\n\nEmail: nagesh@conchtech.com \n\nWeb: www.conchtech.com"}, {"title": "Remote Work - Need Azure Fabric Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3759325599/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=C3iVjmcR9wjUip1nNjUiCg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3759325599", "description": "About the job\nThe client is placing heavy emphasis on Microsoft and Azure Fabric experience\n\nInformation Management Engineer \u2013 Take the identified information management sources and assist migration to the Client Data Lake, Azure Fabric. The engineer will transform information management (de-dup, clean and enrich).\n\nTake the identified data sources and bring them into our Data Lake, Azure Fabric. Once the data is in, the engineer will need to be able to transform data (de-dup, clean and enrich the data). We are focused on using a medallion data model for data architecture. The tools we are working with are: Delta Parquet files, python, spark, notebooks, T-SQL, Fabric lakehouse/warehouse, Dataflows Gen2, Data Pipelines, Direct Lake, shortcuts, ADF."}, {"title": "Data Engineer", "company_name": "Catalist", "location": "Washington, DC (Remote)", "link": "/jobs/view/3744951279/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ui1Fk6ICmnPNqAlZDZAcwA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3744951279", "description": "About the job\nFor over 17 years, Catalist has been a leader in civic data and data science innovation. Our mission is to provide progressive organizations with the data, software, and services needed to better identify, understand, and communicate with the people they need to engage and mobilize. Our clients include the largest, most influential organizations in the U.S. active in civic engagement, advocacy, and political campaigns.\n\nCatalist is home to a dedicated, creative team of technologists, data scientists, and campaign experts committed to using our talents and technology to nurture a vibrant and growing progressive community.\n\nAs a Data Engineer at Catalist, you will work closely with the Analytics and Technology departments to design, build, support, and maintain various data pipelines and processes with an end goal of providing data and intelligence to the progressive community. This role primarily involves translating data architecture designs into functional processes, code, and systems.\n\nThe ideal candidate will be a highly motivated individual with excellent technical skills, a strong desire to learn new skills, and an interest in progressive politics. Catalist values creativity and problem-solving. Our work is on the cutting edge of data-driven politics, and your support will help Democratic candidates and progressive organizations conduct successful advocacy and electoral campaigns.\n\nThis position reports to the Deputy Chief Data Officer. The Data Engineer is a part of a growing Data team that supports all underlying work at Catalist.\n\nThis position is included in our CWA bargaining unit.\n\nPrinciple Duties & Responsibilities\n\nDevelop scalable, production processes, code, and systems for data ingest, transformation, modeling, and reporting across a variety of platforms\nTranslate mock-ups and designs into functional processes, code, and systems\nCreate architecture designs when needed\nProvide quality assurance and testing on processes, code, systems, and products\nBecome an internal subject matter expert on various datasets and support other Catalist departments/teams on usage of those datasets\nExecute ad-hoc data and database maintenance tasks as requiredProject manage cross-departmental efforts, with direct responsibility for stakeholder engagement, management, and execution of technical elements\n\n\nRequirements\n\nBS or BA in a technical field, or relevant experience\n1-2 years of experience working with SQL databases\nExperience with data cleaning or ETL processes\nExperience with distributed computing systems and/or distributed datastores (particularly the Hadoop ecosystem)\nExperience managing projects\nFamiliarity with Catalist data, progressive politics, voter files, and/or commercial data\nBackground check required\n\n\nPreferred Skills & Abilities\n\nWillingness to be a problem solver and produce results in a fast paced environment\nAbility to be creative and personable, and articulate ideas clearly\nExperience working with SQL databases\nProficiency with Python or another object-oriented programming language (R, Java, Scala, etc\u2026)\nExperience working in cloud environments (AWS, GCP, etc.)\nExperience working in command line environments such as Bash\nExperience with a version control tool such as git or github\n\n\nBenefits\n\nMedical, Dental, Vision, Prescription Drug\n\nCatalist offers Medical, Dental, Vision, and Prescription Drug coverage for eligible staff and their eligible dependents. Catalist\u2019s Medical plan is a comprehensive PPO program including Prescription Drug coverage with 85% of the premium paid by Catalist. Dental and Vision coverage is provided at no cost to employees.\n\nGroup Term Life Insurance and Long-Term & Short-Term Disability Coverage\n\nGroup Term Life Insurance and Long-Term and Short-Term Disability coverage is available for eligible staff. These benefits are provided at no cost to Catalist employees.\n\n401(k) Safe Harbor Plan\n\nA 401(k) Safe Harbor Plan is available to eligible staff with a 3% contribution from Catalist from the date of hire. Employees may contribute pre-tax or post-tax from their salary up to the legal limits set forth by the IRS.\n\nMedical and Dependent Care Flexible Spending Accounts (FSAs)\n\nCatalist offers an FSA Program that gives eligible staff the ability to pay out-of-pocket medical/dental/vision/child care expenses from pre-tax earnings.\n\nTransit Benefits\n\nCatalist also makes available a Transit benefit FSA program to eligible employees using pre-tax contributions with a company match.\n\nProfessional Development and Remote Work Expenses\n\nEligible employees may be reimbursed up to $750 each year for professional development / education and remote work expenses.\n\nStudent Loan PayDown or SaveUp\n\nCatalist offers a Student Loan PayDown and College SaveUp benefit for eligible staff.\n\nVacation, Personal Leave, Sick Leave Benefits\n\nCatalist offers generous vacation benefits to all eligible staff. Eligible employees also receive:\n\n 14 Paid Holidays\n Personal Days\n Sick Leave\n Parental Leave\n\n\nHybrid Office/Remote Work\n\nCertain positions at Catalist are eligible for Office/Remote Hybrid or full Remote status."}, {"title": "Remote Entry Level Data Analyst/Engineer", "company_name": "SynergisticIT", "location": "White Plains, NY (Remote)", "link": "/jobs/view/3774171423/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=q54sk1%2FRww0z%2BC%2BE8e0jQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774171423", "description": "About the job\nAt SynergisticIT, we're all about making connections. Whatever IT goals you have, our software programmers can help achieve those. Our Software development teams can take up turnkey projects and execute them in an effective and efficient manner. If you are looking to source talent our recruiters will find the ideal IT talent for your company. What's the secret to our success? Well, it all starts with taking quality time to listen to each client's specific needs. After we have a thorough grasp of your IT goals, we can better customize our Developments as per your specific needs. We can also tailor make recruiting programs to exceed your expectations. Since our founding in 2010, SynergisticIT's strategies have earned the company an enviable position in the software development, IT staffing and IT skill enhancement fields. SynergisticIT continues to work with hundreds of satisfied American clients with our software programmers working on our projects and after gaining hands on experience on cutting edge technologies moving to contribute their skills to great clients like Apple, Google, Client, Ebay, Paypal, Kroger, the Walt Disney Company and hundreds more. If you are tired of working with inefficient programmers who take a lot of time to ramp up we want you to try us. Our software programmers can hit the ground running and get you the maximum return on your investment. You have already tried the rest its time you tried the best. SynergisticIT - Home of the Best Data Scientists and Software Programmers in the Bay Area.\n\n Why Us ? \n\nSynergisticIT has a proven track record of successfully skill enhancement and staffing IT employees for some of the world's most iconic brands. Our team takes the time to fully understand every client's needs so we could best meet your IT staffing requirements. The knowledgeable staff at SynergisticIT is always more than happy to work with clients to ensure they reach their software development goals. Besides staffing, SynergisticIT is also committed to helping young IT professionals advance their career with a robust upskill program . Everyone who goes through SynergisticIT's program learns all the skills necessary to succeed in many IT fields ranging from Java to Machine Learning. Additionally, everyone trained at SynergisticIT has been through extensive mock and technical interview screenings to bolster their career prospects. Last, but certainly not least, SynergisticIT takes great care to respect the privacy considerations for every client. All companies who work with SynergisticIT can rest assured their confidential data is protected using the most up-to-date encryption technologies. SynergisticIT also complies with all the latest NDA agreements.\n\n REQUIRED SKILLS For Java /Software Programmers \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Time series analysis \n\n We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2022 and at Gartner Data Analytics Summit (Florida)-2023 \n\nOracle CloudWorld Event (OCW) Las Vegas 2022 | SynergisticIT - YouTube\n\n https://www.youtube.com/watch?v=OAFOhcGy9Z8 \n\n https://www.youtube.com/watch?v=EmO7NrWHkLM \n\n https://www.youtube.com/watch?v=NVBU9RYZ6UI \n\n https://www.youtube.com/watch?v=Yy74yvjatVg \n\nSynergisticIT at Gartner Data and Analytics Summit 2023 - YouTube\n\nFor preparing for interviews please visit  https://www.synergisticit.com/interview-questions/ \n\n We are looking for the right matching candidates for our clients \n\n Please apply via the job posting \n\n REQUIRED SKILLS For Java /Full Stack/Software Programmer \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning Positions \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Tensorflow \n\n If you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team. \n\n No phone calls please.  Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates"}, {"title": "Data Engineer", "company_name": "Anika Systems", "location": "Leesburg, VA (Remote)", "link": "/jobs/view/3787766893/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=mApL9JnN%2B%2Fr2%2BYUPlxRYOA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787766893", "description": "About the job\nAnika Systems is a fast growing, woman-owned small business that specializes in providing innovative IT solutions for federal government agencies. Our expertise lies in accelerating delivery in Data and Analytics, Intelligent Automation, Application Development, and IT Modernization. We are currently expanding our Federal team and are seeking a passionate and talented Data Engineer. This opportunity is 100% remote.\n\nMust be a U.S. Citizen with the ability to obtain and maintain a government suitability clearance\n\nResponsibilities\n\nDesigns, develops, builds, analyzes, evaluates, and installs database management systems to include database modeling and design, relational database architecture, metadata, and repository creation and configuration management.\nDefines and oversees database organizations, standards, controls, procedures, and documentation. Provides technical consulting in the definition, design, and creation of a database environment.\nAdvises applications development staff and users on data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nDesigns and implements databases with respect to access methods, access time, batch processes, device allocation, validation checks, organization, protection and security, documentation, and statistical methods.\nUses data mapping, data mining, and data transformational analysis tools to design and develop databases.\nDetermines data storage and optimum storage requirements.\nPrepares system requirements, source analysis, and process analyses and designs throughout the database implementation.\n\nRequired Skills And Experience\n\nBA/BS and 1 year of relevant experience\nExperience with DataBricks, SQL, Python\nApplications development experience with data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nExcellent communication skills (both oral and written) with process-oriented organizational skills to ensure project success\n\nDesired Skills And Experience\n\nDemonstrated successful development experience with the full life cycle of government database management systems including database modeling and design, relational database architecture, metadata and repository creation, and configuration management.\n\nPowered by JazzHR\n\nTlKWIorucJ"}, {"title": "Remote Work - Need Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3774712978/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=r3Bp%2F0qcOD2OpC5jpbUOcA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774712978", "description": "About the job\nTitle : Data Engineer\n\nLocation : Remote\n\nDuration : 6+ months to start (long term contract (work order are extended 2x year))\n\nPlease Fill out the skill matrix: Technology Some Awareness (1) Novice (2) Intermediate (3) Advanced (4) Expert (5) SQL Tableau Desktop /Creator Tableau API Integration AWS - S3 Glue AWS - Athena Redshift AWS - Lambda AWS - SNS/SQS msExcel\n\nProfile: Data Analyst will work closely with Architects and leads in the Interflow team, key business stakeholders including program staff to support the organizations effort in the areas of data Analysis and information management. Role should have strong experience in writing SQL queries to extract data for reports. Candidate should have experience working with large set of data. As these reports are very critical for the company and we are looking for an expert who is capable of handling, consuming and extracting large sets of data for analysis and reports.\n\nMust Haves\n\nAWS - Athena\nAWS - S3\nGlue\nhands-on development of reporting applications\nMS Excel\nRedShift\nSQL\nTableau Desktop /Creator\n\nNice\n\nAWS - Lambda\nAWS - SNS/SQS\nTableau API Integration\n\nSkills\n\nBA/BS required (major in an analytical field desired) \nA minimum 10+ years work-related experience. \nExperience in hands-on development of reporting applications for the Web in a professional environment \nThe Ethos of continuous improvement and interest in learning new things.\nStrong analytical thinking and structured problem-solving ability\nAbility to handle multiple projects and assignments simultaneously and effectively in a cross-functional team environment. \nVersed on the agile methodology and best practices. \nExcellent interpersonal and collaboration skill with the ability to work with a diverse set of colleagues, across functions, from different organizations, disciplines, etc. \nSelf-starter, ability to set priorities, work independently and attain goals"}, {"title": "Data Engineer", "company_name": "Team Remotely Inc", "location": "Houston, TX (Remote)", "link": "/jobs/view/3786249829/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=78lY5Jo%2FfyRPC6TQYCdvww%3D%3D&trk=flagship3_search_srp_jobs", "id": "3786249829", "description": "About the job\nThis is a remote position.\n\n Data Engineer (US/Canada Residents Only, 1 year experience, remote)\n\nTeam Remotely Inc. is a staffing and recruitment agency that offers a comprehensive solution for talent acquisition, including sourcing, vetting, pay rolling, and managing talent. Whether you need contract staffing, direct hire, direct sourcing, talent pools, or diversity initiatives, our model can support your hiring strategy.\n\n Hiring Type: Full-Time\n\n Base Salary: $55K-$70K Per Annum.\n\nResponsibilities\n\nDesigning user interface changes for web-based DB applications. \nReviewing application requirements and interface designs. \nDeveloping and implementing highly responsive user interface components using react concepts. \nWriting application interface codes using JavaScript following react.js workflows. \nTroubleshooting interface software and debugging application codes. \nDeveloping and implementing front-end architecture to support user interface concepts. \nMonitoring and improving front-end performance. \n\nQualifications And Experience\n\nBachelor\u2019s degree in computer science, information technology, or a similar field. \nPrevious experience working as a react.js developer. \nIn-depth knowledge of JavaScript, CSS, HTML, jQuery, and front-end languages. \nKnowledge of REACT tools including React.js, Webpack, Enzyme, Redux, and Flux. \nExperience with user interface design. \nKnowledge of performance testing frameworks, including Mocha and Jest. \nExperience with browser-based debugging and performance testing software. \nTroubleshooting skills. \nProject management skills. \nProblem-solving skills. \nVerbal communication skills. \n\n Why work with Team Remotely?\n\nTeam Remotely Inc. is a staffing platform offering a seamless experience for employers and candidates. Employers can post job openings and specify their requirements, while candidates can create profiles and upload resumes.\n\nThe team of Team Remotely continuously learns and adapts based on previous successful placements, constantly improving its matching capabilities. This ensures that the recommendations provided by Team Remotely are tailored and accurate, increasing the likelihood of a successful match between employers and candidates. By providing intelligent and data-driven solutions, they strive to enhance the efficiency and effectiveness of the hiring process, ultimately helping companies find the best talent and individuals find their dream jobs."}, {"title": "Data Engineer", "company_name": "ZAG Technical Services", "location": "United States (Remote)", "link": "/jobs/view/3771403464/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=Hf0VefrKXBFIL5MtHmY0Pw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3771403464", "description": "About the job\nWe are looking for a tech savvy Data Engineer to contribute to the building, operating, and scaling of next generation data platforms and tools that will power data-driven analytics capabilities throughout the ZAG products and services portfolio, spanning areas such as business intelligence, reporting, and data analytics.\n\nThe ideal team member is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. Must be comfortable supporting the data needs of multiple clients and teams, systems, and products within the modernization of ecosystems. The right-fit person will be excited by the prospect of optimizing or even re-designing data processes to support next generation of advanced analytics capabilities, which in-turn translates to implementing strategic enhancements to maintain competitive advantage in the industry.\n\nThe successful team member will leverage their proficiency to\u2026\n\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\nBuild the analytics processes required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Data Management and cloud based \u2018big data\u2019 technologies.\nBuild analytics tools that utilize technologies to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.\nCollaborate with client teams to identify and solution regional and local requirements within the context of the standard global business model for enterprise analytics.\nParticipates in meetings, discussions, strategy sessions where changes, improvements, and enhancements are proposed, evaluated, and approved. \n\n\nThe exceptional team member will have or demonstrate progressive experiences in\u2026 \n\nBachelor\u2019s degree or higher in Computer Science or related field\nExperience working with BI and data warehousing tools\nExperience working with enterprise data, where security is paramount and data governance is critical\nAbility to work independently and in a team environment, and effectively engage all levels of the organization\nUnderstanding of distributed systems driving large-scale data processing and analytics with a successful history of manipulating, processing, and extracting value from large, disconnected datasets.\nAbility to communicate effectively (listening, presenting, and questioning)\nStrong organizational, written, and communication skills\n\n\nAbout ZAG\n\nZAG Technical Services empowers our clients\u2019 success through intelligent business solutions that increase employee productivity, network security and team collaboration. We aim to ensure that our client\u2019s IT investments drive ROI, provide a competitive advantage and help businesses grow. We do this by living our core set of values: Integrity Foremost, Client-Centered, Accountable Always, Teamwork Throughout, and Exceptional is the Goal.\n\nWhat ZAG Has To Offer\n\nCompetitive compensation package \nBenefits including medical, dental, vision, 401K and life insurance \n\n\nZAG Technical Services is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all of our employees.\n\nZAG Technical Services is an E-Verify participant and will provide the Social Security Administration (SSA) and, if necessary, the Department of Homeland Security (DHS), with information from each new employee\u2019s Form I-9 to confirm work authorization."}, {"title": "Junior Data Engineer", "company_name": "HireMeFast LLC - Career Accelerator - Land A Job", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3786249797/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D&trk=flagship3_search_srp_jobs&lipi=urn%3Ali%3Apage%3Ad_flagship3_search_srp_jobs%3BQExogrHBQoSF9zZm%2B9kufw%3D%3D&lici=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D", "id": "3786249797", "description": "About the job\nThis is a remote position.\n\n Job Title: Junior Data Engineer\n\n Employment Type: Full-Time\n\n Salary: $64,000 - $76,000 per annum\n\n Experience Required:  Minimum 1 year of project experience\n\nAbout us: HireMeFast is a leading staffing and recruitment agency specializing in connecting businesses with top-tier talent across various industries. Our mission is to bridge the gap between exceptional candidates and organizations needing their skills, expertise, and unique qualities. Our team of experienced and dedicated recruitment specialists utilizes innovative sourcing strategies, and a vast network to identify and attract top talent. We conduct comprehensive procedures to ensure that only the most qualified candidates are presented to our clients.\n\nPosition Summary\n\nJoin the fast-paced, innovative, and collaborative environment focused on providing an AIOps platform that enhances the intelligence of the CVS Health infrastructure. Work closely with subject matter experts and colleagues to build and scale out machine learning and AI solutions that will detect, predict, and recommend solutions to correct issues before system impact and enhance efficiency, reliability, and performance of CVS Health\u2019s IT operations.\n\nKey Responsibilities include:\n\nData pipeline development: Designed, implemented, and managed data pipelines for extracting, transforming, and loading data from various sources into data lakes for processing, analytics, and correlation. \nData modeling: Create and maintain data models ensuring data quality, scalability, and efficiency \nDevelop and automate processes to clean, transform, and prepare data for analytics, ensuring data accuracy and consistency \nData Integration: Integrate data from disparate sources, both structured and unstructured to provide a unified view of key infrastructure platform and application data \nUtilize big data technologies such as Kafka to process and analyze large volumes of data efficiently \nImplement data security measures to protect sensitive information and ensure compliance with data and privacy regulation \nCreate/maintain documentation for data processes, data flows, and system configurations \nPerformance Optimization- Monitor and optimize data pipelines and systems for performance, scalability and cost-effectiveness \n\nCharacteristics of this role:\n\nTeam Player: Willing to teach, share knowledge, and work with others to make the team successful. \nCommunication: Exceptional verbal, written, organizational, presentation, and communication skills. \nCreativity: Ability to take written and verbal requirements and come up with other innovative ideas. \nAttention to detail: Systematically and accurately research future solutions and current problems. \nStrong work ethic: The innate drive to do work extremely well. \nPassion: A drive to deliver better products and services than expected to customers. \n\nRequired Qualifications\n\n2+ years of programming experience in languages such as Python, Java, SQL \n2+ years of experience with ETL tools and database management (relational, non-relational) \n2+ years of experience in data modeling techniques and tools to design efficient scalable data structures \nSkills in data quality assessment, data cleansing, and data validation \n\nQualifications\n\nKnowledge of big data technologies and cloud platforms \nExperience with technologies like PySpark, Databricks, and Azure Synapse. \n\nEducation\n\nBachelor\u2019s degree in Computer Science, Information Technology, or related field, or equivalent working experience\n\nWhy HireMeFast LLC?\n\nAt HireMeFast, we understand that finding the right individuals to join your team is crucial for success and growth of your organization. We are committed to streamlining the hiring process for our clients, ensuring they have access to a diverse pool of highly qualified candidates who are a perfect fit for their specific needs."}, {"title": "Data Engineer  - Need candidates from Texas ( REMOTE )", "company_name": "Pulivarthi Group (PG)", "location": "United States (Remote)", "link": "/jobs/view/3770685212/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=9gdMwuif%2BpB%2BCQYvQi3imQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3770685212", "description": "About the job\nFollow us on Linkedin: https://www.linkedin.com/company/pulivarthigroup/\n\nPulivarthi Group LLC is a Global Staffing & IT Technology Solutions company, with our prime focus of providing world class solutions to our customers with the right talent. We combine the expertise of our team and the culture of your company to help you with the solution that is affordable and innovative using high quality standards and technologies.\n\nWe\u2019ve served some of the largest healthcare, financial services, and government entities in the U.S.\n\nData Engineer - FT only - Azure SQL with data lakes experience\n\nData Engineer -\n\nthe resource requirement for this Data Engineer role needed more clarification.\n\nOur customer is requesting a resource who knows Azure CI/CD (Continuous Integration / Continuous Deployment) to build and integrate their DevOps pipeline.\n\nOnce the DevOps pipelines are mapped, they need the Data Engineer to connect to all the Data Sources, primarily their Data Lake.\n\nThe Customer (based in Houston) is interested in a resource in a time zone not too far removed from the Central Time Zone to allow real time collaboration."}, {"title": "Data Engineer", "company_name": "Otter Products", "location": "Fort Collins, CO (Remote)", "link": "/jobs/view/3766728302/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=77U3U7CAi1PxgDmpI3%2BiNQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3766728302", "description": "About the job\nOverview \n\nOtter Products is currently recruiting for a Data Engineer to join our Data team! You can be based in our Fort Collins, CO office with flexibility to work a portion of your time remotely, or 100% Remote in the U.S.\n\nThe Data Engineer applies professional experience, concepts, and company objectives toward building and operationalizing the minimally inclusive data necessary for the enterprise data initiatives following industry standard practices and tools. The bulk of the Data Engineer's work would be in designing, managing and optimizing data pipelines, then moving these data pipelines effectively into production for key data and analytics consumers like business/data analysts, data scientists or other stakeholder that needs curated data for data and analytics use cases across the enterprise. The Data Engineer also ensures compliance with data governance and data security requirements while creating, improving and operationalizing these integrated and reusable data pipelines to enable faster data access, integrated data reuse and vastly improved time-to-solution for data and analytics initiatives. Additionally, the Data Engineer will be expected to collaborate with other data team members and data consumers working on the models and algorithms developed by them toward optimization for data quality, security, and governance to put them into production leading to potentially large productivity gains.\n\n About Otter Products \n\nOtter Products, we grow to give. From our founder's garage in 1998 to the global technology leader we are today, Otter Products continues to drive growth through innovation.\n\nThrough our industry-leading brands - OtterBox, Liviri and OtterCares - we provide our partners the number-one selling and most trusted products in our categories. Our philanthropic spirit is the foundation on which we foster our partner relationships, allowing us to grow and to give - together.\n\nBy way of our charitable arm, the OtterCares Foundation, we support our communities and invest in our future through education that inspires kids to change the world.\n\nAnd even as our global community of Otters continues to grow, our founder's core values are still at the heart of everything we do. We measure our success by our ability to give back to our communities and strengthen opportunities for all.\n\nFor more information visit otterproducts.com\n\n Responsibilities \n\nComplete analysis, design and development of BI solutions using Azure SQL Server\nFamiliarity with Data Warehouse concepts\nExperience coding the extraction, transformation, and loading (ETL) processes\nDatabase development primarily in SSIS, Data Factory and SQL\nPartner with the business to determine end user database/reporting needs and requirements\nGenerate ad hoc reports using Power BI or SSRS\nCollaborate with other developers to create and implement best approach solution(s)\nTroubleshoot Azure tools, systems, and software\nReview queries for performance issues, making changes as needed\nCollaborate with team to performance-tune Azure application as necessary\nAssist with the analysis and extraction of relevant information from large amounts of historical business data to feed data science initiatives\nParticipate and support the design and documentation of processes for large scale data analyses, model development, model validation and model implementation\nSupport and maintain a positive safety culture by following all safety policies and procedures and actively contributing to a safe working environment\nOther duties as required\n\n Qualifications \n\nBachelor's degree required. Degree in Computer Science or Mathematics preferred\nMinimum of four years of experience in an IT or analytical role required. Experience in database development, report writing and/or statistics preferred.\n\n EEO \n\nOtter Products, LLC is an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, age, marital status, pregnancy, sex, sexual orientation, gender, gender identity or expression, national origin, disability, veteran status, or any other characteristic or status protected by law.\n\n For US Based Roles Only - Compensation Range Minimum \n\nUSD $90,000.00/Yr.\n\n For US Based Roles Only - Compensation Range Maximum \n\nUSD $120,000.00/Yr.\n\n Additional Total Rewards \n\nProfit Sharing Program Eligible, Benefits Eligible - Full Time- check out otterproducts.com/careers/why for more info"}, {"title": "Software Engineer, Distributed Systems", "company_name": "Hightouch", "location": "United States (Remote)", "link": "/jobs/view/3628572539/?eBP=CwEAAAGMd4QO8APODyE_-mMOPhYyQGg5ZjVbPwq_3V7DK0B2bLAf10pcedM9S2T71X5i4TiJuEwOzWBbSiTKbLuk-VD-_Y-dN5uyf8YpYkHYqEYFxySMS9zr2hRjNrxX9gXbxssh35TFK9YTNV2WeH2Ncb675JgFbJTgNWSZySFC6rrdJGmdSDTr2q5d3VoxORz3GjsDbzjeyZQCvQ07RqYw7oujWSgslycs2LMCY_-N0NEj6e0PvbaYKiZ_5IW34bGVSYLXpHwqa8oOcNttYZ90UA3E_7sDSFzABdIVlfrp5zzERrIYy5sh2MIInxHdDoKgQw8VmOn5uM4S0tXUl-i7LnN0_W0NfPBmLiVDNuveIOm4MfDNPxfe5QKPk6FUAVpvgbcoJn9yRyE&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=rph%2FT6Q6Vkp6YoggJf9cJQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3628572539", "description": "About the job\nAbout Hightouch\n\nHightouch\u2019s mission is to empower everyone to take action on their data. Through our Reverse ETL platform, business and data users can seamlessly sync data from where it resides, such as warehouses and databases, to where it is needed, including operational systems and SaaS tools. Traditionally, acting on data has required engineering time and bandwidth, and left most business users stuck with charts and reports that are unable to take automated action on their data. With Hightouch, every business user, without writing any code, can activate data to streamline critical processes, improve marketing performance, and scale operations.\n\nOur team operates with a focus on making a meaningful impact for our customers. We believe in approaching challenges with a first principles thinking mindset, moving quickly and embracing our value of efficient execution, and treating each other with compassion and kindness. We look for team members that are strong communicators, have a growth mindset, and are motivated and persistent in achieving our goals.\n\nHundreds of companies use Hightouch, including Spotify, Ramp, Retool, NBA, Plaid, and Betterment. We\u2019re based in San Francisco, are remote-friendly, and backed by leading investors such as Amplify Partners, ICONIQ Growth, Bain Capital Ventures, Y-Combinator, and Afore Capital.\n\nAbout The Role\n\nWe are looking for a distributed systems engineer to work on the systems that power the syncing engine our customers and other engineering teams rely on. This presents an exciting challenge where you can apply your expertise in distributed systems, performance optimization, and troubleshooting to push the boundaries of what is possible and meaningful improve our ability to move massive amounts of customer data. This role also provides a unique opportunity to work on a multi-cloud and multi-region infrastructure that supports a global customer base.\n\nWe believe in enabling our engineers to do their best work for our customers by giving them extremely high levels of ownership and autonomy. This comes in different forms: you will own and deliver projects from start to finish, you will work directly with customers to solve their hardest scaling problems, and you will have a lot of influence over what we work on as a team and company.\n\nSome of the problems we\u2019ll be working on include:\n\nSync Speed: Customers want to sync a lot of data to important destinations like Facebook and Snapchat, which requires us to analyze every part of our syncing process and find where we can optimize to sync data more quickly\nStreaming Syncing: We currently sync data from sources in batch only, but envision a world where we also do syncs in real-time and support streaming sources like webhooks and queues\nScalability and Reliability: As part of our rapid growth, we\u2019re always evaluating current future bottlenecks in scaling and reliability, and architecting for the next order of magnitude of growth\nPersonalization API: Our Personalization API product provides a low latency caching layer on top of data warehouses that enable customers to use their data warehouse for real-time personalization use cases with \nMulti-Region and Multi-Cloud: Supporting our multi-region and multi-cloud backend, including extending it to launch Hightouch on in new regions to support data residency requirements of our global customer base\n\nWe are looking for talented, intellectually curious, and motivated individuals who are interested in tackling the problems above. This is a senior role, but we focus on impact and potential for growth more than years of experience. The salary range for this position is $170,000 - $240,000 USD per year, which is location independent in accordance with our remote-first policy.\n\nAbout You\n\nYou are an engineer with a passion for solving hard technical problems that generate real value for customers. You\u2019re motivated by high ownership and are comfortable in a fast-paced, startup environment.\n\nYou have experience and comfort with distributed systems and high-scale systems or the ability and desire to build this experience quickly. This experience can take different forms:\n\nYou\u2019ve built syncing engines that process and move terabytes of data per day in a scalable way\nYou\u2019ve created event collection or data streaming services that power company critical systems\nYou\u2019ve owned systems that have undergone orders of magnitude of growth and helped them scale accordingly\nYou have relevant skills and are a fast learner who is excited to take on the challenges we face\n\nInterview Process\n\nOur goal with the interview process is to balance speed with giving both parties opportunities to assess whether there is a strong mutual fit. We will ask you questions, but we want you to ask us questions! Our technical interviews focus on how you design systems because we believe this is the best way for us to see how you work and for you to see how we collaborate. We don\u2019t ask you to write code to solve technical brainteasers that don\u2019t appear in your day to day job.\n\nRecruiter screen [30m]: Introductory call with our recruiting team to get to know each other and see if the role could be a good mutual fit.\nSystems design screen [45m]: Designing a data processing feature end-to-end.\nHiring manager interview [30m]: Chat with hiring manager about past experiences and future operating preferences to assess fit on company values and operating principles.\nSystems design interview [90m]: Work with the interviewer to architect a system at a conceptual level. The problem will be at a pretty high level - and have both product and customer requirements as well as technical."}, {"title": "Entry-Level Data Scientist Engineer", "company_name": "Phoenix Recruitment LLC", "location": "Texas City, TX (Remote)", "link": "/jobs/view/3788727625/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=8iwtytHsCu4wMwJ4fj3pxw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3788727625", "description": "About the job\nThis is a remote position.\n\nTitle - Entry-Level Data Scientist Engineer, 1 year of project experience\n\nEmployment Type: Full-time\n\nBase Salary: $60K-$70K\n\nPhoenix Recruitment offers a variety of recruiting services to assist both employers and employees. They specialize in marketing open positions, recruiting, and helping employers to find qualified candidates across various industries. Phoenix Recruitment has expertise in streamlining the hiring process. They can help ensure that the process is efficient, well organized, and compliant with relevant regulations.\n\nSkills and Abilities:\n\nStrong knowledge of R or Python for data analysis and modeling. \nProficiency in statistical programs such as R, SAS, MATLAB, or Python. \nFamiliarity with spreadsheets (VBA) and database applications (Access, Oracle, SQL, or equivalent technology). \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \nAbility to learn new methods quickly and work under deadlines. \nExcellent teamwork and communication skills. \nStrong analytical and problem-solving abilities. \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \n\nPreferred:\n\nKnowledge of actuarial concepts and life, health, and/or annuity products. \nExperience with statistical modeling techniques such as GLM, Decision Trees, Time Series, Regression, etc. \nFamiliarity with Microsoft DeployR. \nExposure to insurance risk analysis. \nBasic experience in computational finance, econometrics, statistics, and math. \nKnowledge of SQL and VBA. \nFamiliarity with R or Python for predictive modeling \n\nWhy Phoenix Recruitment LLC?\n\nPhoenix Recruitment often has an extensive network of employers and candidates. This network allows them to tap into a pool of qualified candidates and connect them with suitable job opportunities. They can also leverage their connections to help employers find the right talent efficiently. Outsourcing the recruitment process to a specialized agency can save your time and resources, avoid delays, reduce administrative burdens, and increase the chances of finding the right fit for your organization."}, {"title": "Data Engineer - Remote Work", "company_name": "BairesDev", "location": "United States (Remote)", "link": "/jobs/view/3783162297/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=OtoCrAAHZFomVup04fy35g%3D%3D&trk=flagship3_search_srp_jobs", "id": "3783162297", "description": "About the job\nWho We are\n\nBairesDev is proud to be the fastest-growing company in America. With people in five continents and world-class clients, we are only as strong as the multicultural teams at the heart of our business. To consistently deliver the highest quality solutions to our clients, we only hire the Top 1% of the best talents and nurture their professional growth on exciting projects.\n\nWe are looking for a Data Engineer to join our Development team and participate in different projects made up of multicultural teams distributed throughout the world.\n\nWhat You Will Do\n\n Interview other Data Scientists and Data engineers to vet their technical abilities.\n Input data from those interviews into the product and help develop a predictive, fair, and enjoyable solution.\n Work together with developers, tech leads, and solution architects to build applications.\n Improve existing structures by adding new functionalities or proposing technological updates so that as a result, the impact of your contribution is significant in the core of each business.\n Improve the user experience on scalable and high availability platforms, contributing to the key differential of each business.\n\nHere\u2019s What We Are Looking For\n\n 5+ years of experience working as a developer.\n 3+ years of experience working as a Data Scientist / Data Engineer.\n Proficient with analysis, troubleshooting, and problem-solving.\n Hands-on experience with managing data loads and data quality.\n Advanced English level.\n\nHow we do make your work (and your life) easier:\n\n 100% remote work.\n Hardware setup for you to work from home.\n Flexible hours - make your schedule.\n Paid parental leave, vacation & holidays.\n Diverse and multicultural work environment.\n An innovative environment with the structure and resources of a leading multinational.\n Excellent compensation \u2014 well above the market average.\n Here you can grow at the speed of your learning curve.\n\nOur people work remotely but with a consistent and robust culture that promotes diversity and teamwork. To continue being the leading software development company in Latin America, we want to ensure that every BairesDev member gets the best growth and professional development opportunities in a diverse, welcoming, and innovative environment.\n\nEvery BairesDev team member brings something unique to our company.\n\nWe want to hear your story. Apply now!"}, {"title": "Data Engineer (Remote)", "company_name": "Cascade Debt", "location": "United States (Remote)", "link": "/jobs/view/3755336571/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=pmUtn1j2zyPr25CNPh2RNw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3755336571", "description": "About the job\nDo you want to work in a small team where you can make a real difference in a company? Would you embrace the challenge of building a high impact platform that is used globally? Then we want to speak with you!\n\nAbout Cascade\n\nCascade is a fintech startup backed by Canadian and US investors that empowers companies to grow by democratizing access to institutional debt. We are building tools that modernize how companies raise and manage debt, lowering the barriers to entry to the $7 trillion specialty finance and alternative credit market for companies around the world. Founded just last year, we are gearing up for our next phase of technical development and are seeking a talented Data Engineer to lead the way.\n\nAbout The Role\n\nAs a Data Engineer you\u2019ll be an early member of a growing team building a pioneering platform in the debt infrastructure space. You will design, build, and launch efficient, scalable, and reliable data pipelines to move and transform data .\n\nWhat You\u2019ll Do\n\n Building databases, data lakes and data ingestion pipelines to integrate customer databases and datasets to our systems \n Ingesting financial datasets from external customers, then u pdating and maintaining accurate and complete data mappings to ensure that our product s are displaying high quality data \n M onitoring and alerting across our data pipelines in order to make sure that our data ingests are reliable and correct \n Translat ing business goals and stakeholder requirements into new data flows and deliver analytical insights ensuring that the data flows create real business value continuously \n\nSkills And Experience\n\n 2 + years experience in a data engineering or data analyst role \n E xperience with relational SQL and NoSQL databases, including PostgreSQL , MySQL, MariaDB, MongoDB, Bigtable, etc. \n E xperience working with ETL technologies , such as Databricks, Fivetran , or dbt \n Proficiency in writing SQL queries and knowledge of analytical data warehouses such as RedShift, BigQuery , and Snowflake \n Some experience with CI/CD automation such as GitHub Actions \n Developing APIs and integrating with 3rd party APIs \n Bonus: experience in fintech / SaaS / credit analysis \n\nPeople Who Thrive At Cascade Are\n\n Self-starters, who take the initiative to tackle challenges in a remote work environment \n Good communicators, who can operate without ego to discuss, learn, grow, and help others do the same \n Passionate about creating great digital experiences for users \n Problem solvers who are not satisfied with the status quo \n\nBenefits\n\nRemote: we are a remote-first company and are very flexible on hours as long as things get done \nHome-office: there\u2019s a $1000 USD home office allowance to set yourself up \nEquity: we expect you to have an owner-mentality, and have the equity plan to match \nBenefits: health, dental, vision, and more \nPerks: we offer free lunches weekly and off-site trips \nJob satisfaction: we offer autonomy, ample opportunities for mastery, and an opportunity to make a difference for companies around the world"}, {"title": "Data Engineer", "company_name": "ISC (Integrated Specialty Coverages, LLC)", "location": "Carlsbad, CA (Remote)", "link": "/jobs/view/3778293528/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ew7%2FfEuXSujs5IGabdqUyw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3778293528", "description": "About the job\nAbout Integrated Specialty Coverages\n\nIntegrated Specialty Coverages LLC (ISC) is a growth stage technology and data-driven insurance company leading innovation in the market.\n\nBacked by one of the leading private equity firms, KKR, and led by a forward-thinking management team, ISC is combining the worlds of insurance and technology to create an Insurtech powerhouse. As a leading online distributor of insurance products for a range of industries and \"Main Street USA\", we are looking for the right people to help us in our mission of achieving exponential growth. We strive to be the number one place to go for brokers and agents to source insurance. To accomplish this, we're building a digitally focused team that deeply understands the intersection between user experience, data, and AI/ML to optimize the way we engage with our customers and partners.\n\nJob Summary\n\nThe Data Engineer role will be the technical liaison between multiple groups including a data science team, the engineering team, product management, and business stakeholders. You do not need any insurance knowledge prior, however, you must quickly dive deep into the insurance world and ask questions to become a subject matter expert. You will be responsible for building a data platform to facilitate the data science team. You must be a self-starter that can build out features such as a data pipeline from scratch. There will be support from both engineering and data science for any buildout.\n\nJob Responsibilities\n\nCreate and maintain optimal data pipeline architecture. \nAssemble large, complex data sets that meet functional / non-functional business requirements. \nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. \nBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using AWS technologies, SQL, and Python. \nWork with stakeholders including the Executive, Product, Data Science, and Engineering teams to assist with data-related technical issues and support their data infrastructure needs. \nWork with data science and analytics teams to strive for greater functionality in our data systems. \n\nMinimum Qualifications:\n\nAdvanced working SQL knowledge and experience working with relational databases, strong query authoring (SQL) as well as working familiarity with a variety of databases (Snowflake, Redshift, MySQL, MSSQL, etc.)\nExperience building and optimizing data pipelines, architecture and data sets. \nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. \nBuild processes supporting data transformation, data structures, metadata, dependency, and workload management. \nA successful history of transforming, processing and extracting value from large disconnected, datasets from a variety of data sources (Flat files, Excel, databases, etc.)\nStrong analytic skills related to working with unstructured datasets. \nStrong project management and organizational skills. \nExperience supporting and working with cross-functional teams in a dynamic environment. \nAbility to mentor/guide/collaborate with other team members. \n\nWe are looking for a candidate with 3-5 years of experience in a Data Engineer role. They should also have experience using the following software/tools:\n\nExperience with relational and MPP databases, including Snowflake and MySQL. \nExperience developing software in an agile environment from the requirements stage to production. \nExperience with version control: git\nExperience with container technologies: Docker\nExperience with data pipeline and workflow management tools: Airflow, Jenkins, AWS Glue, Azkaban, Luigi, etc. \nExperience with AWS cloud services: EC2, ECS, Batch, S3, EMR, RDS, Redshift\nExperience with other cloud services: Snowflake, Airflow\nExperience with object-oriented/object function scripting languages: Python, Java, C++, etc. \nExperience with data modeling and data warehouse design\nExperience with data visualization tools (PowerBI, QuickSight)\n\nThe starting annual pay scale for this position is listed below. Actual starting pay will be based on factors such as skills, qualifications, training, and experience. In addition, the company offers comprehensive benefits including medical, dental and vision insurance, 401(k) plan with match, paid time off, and other benefits.\n\nISC's salary ranges are determined by role and level. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations and could be higher or lower based on a multitude of factors, including job-related skills, experience, and relevant education or training.\n\nNational Pay Range\n\n$100,000\u2014$110,000 USD\n\nBenefits of Working at ISC\n\nCompetitive vacation and flexible working arrangements\nComprehensive and inclusive health benefits\nA variety of professional development and mentorship opportunities\nChoice of technology whether at home or in the office\n\nISC's Ownership Behaviors\n\nDo the Right Thing *Be Relentless. Pursue Excellence. *Personal Responsibility & Own Outcomes\nTry Fast. Learn Fast. Fail Fast. Think Big. *Build Extraordinary Owners & Join Forces *Grit & Determination\n\nApplicants with disabilities may contact ISC HR department via e-mail, and other means to request and arrange accommodation. If you need assistance to accommodate a disability, you may request accommodation at any time. Please contact ISC at HR@ISCMGA.com.\n\nISC believes in creating long-term relationships by being responsive and relevant and by consistently delivering value to our community of customers. Specifically, with our employees, we focus on attracting, developing, and retaining the best talent for our business, challenging our people, demonstrating a \"can-do\" attitude, and fostering a collaborative and mutually supportive environment. ISC is an equal opportunity employer and a participant in the US federal E-Verify program (US).\n\nDiversity creates a healthier atmosphere: All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, gender, gender identity, sexual orientation, marital status, medical condition, genetic information, mental or physical disability, military or veteran status, or any other characteristic protected by local, state, or Federal law."}, {"title": "Data Engineer -Remote", "company_name": "RIT Solutions, Inc.", "location": "Denver, CO (Remote)", "link": "/jobs/view/3768007496/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=1hQH8EDaII1eJ7nQ9BACQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3768007496", "description": "About the job\nDuration: 6+ months CTH\n\nMust have a valid LinkedIn profile\n\nRequired Skills\n\n 3-6 years of experience (no more than 6 years)\n Start up environment experience\n Python, SQL, and DBT"}, {"title": "Big Data Engineer - W2", "company_name": "Megan Soft Inc", "location": "United States (Remote)", "link": "/jobs/view/3674845652/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fCAxh4N%2F4YZJ5aEIASaeeA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3674845652", "description": "About the job\nTitle: Big Data Engineer - W2\n\nLocation: Dearborn, MI (Remote)\n\nDuration: 12+ Months\n\nDirect Client: Ford Motors\n\nMust have skills: Should be able to handle GCP requirements\n\n\"Translates business requirements to build repeatable, sustainable, efficient, coded processes that can be productionized by Software Engineers and readily modified by other Data Engineer Creates POC processes in Dev/QA and works with Software Engineers to productionize and align best coding and process practices that maximize efficiency, speed, stability, system resources and capabilities Awareness of and compliance with data privacy, security, legal and contractual guidelines Incorporates data quality and privacy checks/alerts to minimize bad data being consumed by end users, models and dashboards, and to protect customer data; revises checks for new data issues Validates data products and pipelines are functioning as expected following system or application upgrades, source changes, etc. Works closely with Architects if required to align systems, tools and applications being utilized with business use case and performance requirements Provides data product support and maintenance Establishes, tracks and monitors KPIs related to specific data products and deliverable Participate in iteration cadence activities (Iteration Planning, task estimation, daily scrum, demos, etc.). Will be a part of on-call team to resolve critical production support tickets during business hours and off hours.\n\nSkills Required\n\nExperience with Big Data Technologies (Hadoop , ETL Tool, Spark. Hive etc..) Someone who understands Cloud and has experience working with Cloud Platforms (preferably GCP) Should be able to handle GCP requirements - ingesting into GCS/ Big Query, working with GCP components Hands-on in Data Flow, Cloud Storage (GCS), Big Query and GitHub. Proficient in Python and SQL.\n\nSkills Preferred\n\nManages ETL: uses programming and tools for data ingestion, configures pipelines, applies transformations and decoding, integrates and fuses data, moves and securely deliver Experience NoSQL databases\n\nAdditional Information\n\nIf the candidate is remote only, please indicate \"\"100% Remote\"\" under candidate's name on resume. Other similar schedule notations can be \"\"Local Candidate\"\" or \"\"Hybrid Candidate.\"\" ***HRA Test Required***\""}, {"title": "Remote Opportunity: Quantexa Data Engineer", "company_name": "SPAR Information Systems LLC", "location": "United States (Remote)", "link": "/jobs/view/3680073191/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ioykVCZSOQ7BK5%2FwRQHOWg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3680073191", "description": "About the job\nHello All, Hope you are doing great!! Please go through the job descriptions and let me know your interest. Role: Quantexa Data engineer  Location: Remote Duration: Long Term Contract Job Description: 1. End to End usage of Quantexa including guiding our client team on how to use from both front end and data ingestion Reason they are asking for Certification 2. Resolve Entity issues with data 3. Knowledge of Scala and Hive which requires coding experience using parquet 4. Advanced SQL knowledge requiring writing SQL queries 5. Experience having worked with GCP data lake 6. Quantexa certification required\n\nThanks & Regards,\n\nSatnam Singh\n\nDirect: 201 623 3660\n\nEmail : Satnam.singh@sparinfosys.com"}, {"title": "Remote Work - Need Staff Data Engineer - USC only", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3746283980/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=CvIuLBNO%2BPMGi4IXVRkP0A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3746283980", "description": "About the job\n100% Remote, client is in NY\n\nLook for strong job history, Python, Machine Learning, SQL, 10/10 comms, etc. \n\nMUST be an Engineer \n\nSummary\n\nWe're looking for an experienced Data Engineer to join our client's growing team of top notched IT professionals. You will work closely across multiple groups including Editorial and Audio, Marketing, Advertising, etc. to achieve their goals and objectives. The Data Engineering team builds tools throughout the stack while sharing knowledge across these departments.\n\nAs the company grows, they're looking for Data Engineer who will help solidify and expand our pipelines and maintain their data warehouse. Their business model is based on performing complex and very detailed analyses of how our products perform with their subscribers. You'll be responsible for working alongside analysts, product, marketing, and other teams to define new data collection components and measurement schemes that support each new product and feature.\n\nHow Can Add Value To Their Mission\n\nCreate and maintain data pipelines to provide insights and drive business decisions\nEstablish data warehousing strategy (ex. Kimball, Data Vault, etc.)\nMaintain data infrastructure on our AWS accounts\nCollaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.\nWrite unit/integration tests, contributes to engineering wiki, and documents work\nImplement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it\n\nWhat You'll Need To Succeed\n\n7+ years of industry experience measuring product performance and user behavior\nExperience working with a variety of data management technologies, including RedShift, Kinesis/Kafka, Glue, Spark, Postgres, Airflow, dbt, and others.\nExperience implementing BI reporting tools such as Looker\nExperience interfacing with engineers, product managers and analysts to understand data needs\nKnowledge of a variety of measurement beacons, SDKs, APIs including Google Analytics, Amplitude, Braze, Stripe, email service providers\nA commitment to building secure, resilient, fault-tolerant architectures, with clear documentation and procedures in place for support\nA focus on accuracy and detail as you build, ensuring data pipelines produce clear, predictable results\nUnderstanding of the typical metrics a subscription and advertising-supported business needs to measure success\nExperience with Client techniques as applied to behavioral segmentation or anomaly identification is a definite plus\nFamiliarity using developer tools that increase productivity and facilitate the development of resilient code (eg. Docker, CircleCI, Serverless) is a plus"}, {"title": "ENTRY LEVEL DATA ENGINEER (REMOTE)", "company_name": "SynergisticIT", "location": "Jacksonville, FL (Remote)", "link": "/jobs/view/3767587909/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=om7L3CnowiC4PLIcpZD70A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3767587909", "description": "About the job\nSYNERGISTICIT wants every candidate to know that the Job Market is Challenging and to stand out, you need to have exceptional skills and technologies and that's where we come in to make sure you get the attention which you need\n\nSynergisticIT understands the complex nature of the job market and how difficult it can be to secure a position, especially for fresh graduates. Therefore, we assist and help tech-savvies to convert their passions into professions. We go above and beyond to keep you working in your niche.\n\nAs we focus on long-term success, we provide complete career development solutions. From job search to upskilling portfolio and interview preparation, we can guide you at every step of your career.\n\nSynergisticIT spares no efforts to connect you with a large network of tech giants, including Google, Apple, PayPal, Dell, Cisco, Client, etc. Presently, we are actively looking for ENTRY LEVEL DATA ENGINEER a driven mindset. Get the right opportunity and gain experience in building web-centric solutions on Java.\n\nWho Should Apply : Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or anyone looking to make their career in IT Industry\n\nWe also assist in filing for STEM extension and H1b and Green card filing.\n\nCandidates who are serious about their future in the IT Industry and have set big goals for themselves.\n\nCandidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. We also offer Skill Enhancement Programs if the candidates are missing skills or experience which our clients need with great outcomes\n\nCandidates can benefit from skill enhancement if they fall into the below categories. If they are qualified with enough skills then no need for skill enhancement\n\nCandidates Who Lack Experience\n\nHave had a break in careers\n\nLack Technical Competency\n\ncandidates who want to get employed and make a career in the Tech Industry \n\nPlease Also Check The Below Links\n\nhttps://www.synergisticit.com/candidate-outcomes/\n\nhttps://www.synergisticit.com/java-track/\n\nhttps://www.synergisticit.com/data-science-track/\n\nhttps://www.synergisticit.com/which-is-the-best-option-for-tech-job-seekers-staffing-companies-consulting-companies-bootcamps-or-synergisticit/\n\nhttps://www.synergisticit.com/contact-us/\n\nIf the skills are not a match candidates can opt for Skill enhancement. Or their resume can be sent out to clients to see if responses are achievable\n\nREQUIRED SKILLS For Java/Software Programmers\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Core Java , javascript , C++ or software programming\nSpring boot, Microservices and REST API's experience\nExcellent written and verbal communication skills\n\nFor data Science/Machine learning\n\nRequired Skills\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Statistics, Python, data visualization tools\nExcellent written and verbal communication skills\n\nPreferred skills: NLP, Text mining, Tableau, Time series analysis\n\nTechnical skills are required by clients for selection even if its Junior or entry level position each additional Technical skill helps a candidate's resume to be picked by clients over other job seekers.\n\nClients hire candidates with the right technical skills which they need and reject candidates who lack the required technical skills.\n\nNo third party candidates or c2c candidates\n\nPlease apply to the posting\n\nNo phone calls please. Shortlisted candidates would be reached out"}, {"title": "Data Engineer - Remote | WFH", "company_name": "Get It Recruit - Information Technology", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3782241984/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=BgBf1kSAAX%2BSuw6inJsIVw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3782241984", "description": "About the job\nJoin an innovative team at a leading company in the engineering and manufacturing industry, where we are committed to leveraging data for continuous improvement and operational excellence. This hybrid position allows you to enjoy the flexibility of remote work on Mondays and Fridays.\n\nWhy Join Us\n\nAt our company, we prioritize your well-being and professional growth. We offer a comprehensive benefits package, including excellent health benefits, a 6% 401(k) match, generous paid vacation and sick time, 12 paid holidays, and more.\n\nRole Overview\n\nAs a Data Engineer, you will play a pivotal role in transforming raw data into actionable insights, driving improvements in engineering and manufacturing processes, enhancing product quality, and optimizing operational efficiency. Collaborate with dynamic cross-functional teams to extract, analyze, and visualize data, influencing strategic decisions through your valuable insights.\n\nKey Responsibilities\n\nData Collection and Integration:\n\nGather, clean, and integrate data from diverse sources, including production databases, sensor data, and external datasets.\n\nData Analysis\n\nConduct exploratory data analysis to identify patterns, trends, and anomalies.\n\nPerform statistical analysis and hypothesis testing to derive meaningful insights.\n\nData Visualization\n\nCreate informative and visually appealing dashboards and reports using tools like Tableau, Power BI, or Python libraries.\n\nPredictive Modeling\n\nDevelop predictive models to forecast manufacturing performance, quality, and operational outcomes.\n\nRoot Cause Analysis\n\nInvestigate production issues and quality concerns by analyzing data to identify root causes and recommend corrective actions.\n\nPerformance Monitoring\n\nEstablish and maintain key performance indicators (KPIs) to track manufacturing and engineering metrics.\n\nGenerate regular reports to communicate performance trends to stakeholders.\n\nCross-functional Collaboration\n\nCollaborate with engineering, manufacturing, and quality assurance teams to understand their data needs and provide analytical support.\n\nContinuous Improvement\n\nSuggest process improvements and optimizations based on data analysis.\n\nData Security And Compliance\n\nEnsure data security and compliance with relevant regulations, such as GDPR or industry-specific standards.\n\nInfrastructure And Development\n\nBuild and maintain the infrastructure for collecting, storing, and processing data.\n\nDesign, develop, and manage data pipelines, ETL processes, and data warehouses.\n\nRequired Experience\n\nProven experience as a Sr. Data Analyst and/or Data Engineer in an engineering or manufacturing environment.\n\nProficiency in data analysis tools and programming languages, such as Python, R, SQL, and Excel.\n\nStrong data visualization skills with tools like Tableau or Power BI.\n\nExcellent knowledge of statistical analysis and machine learning techniques.\n\nAbility to communicate complex findings and insights effectively to both technical and non-technical stakeholders.\n\nStrong problem-solving skills and attention to detail.\n\nFamiliarity with manufacturing processes and quality control is a plus.\n\nExperience with enterprise ERPs (SAP S4HANA, Oracle EBS), CRMs (Salesforce, Hubspot), and IoT.\n\nProficient in working with cloud computing platforms including AWS, GCP, and Azure.\n\nExpertise in configuring and optimizing cloud-based data solutions for scalability, reliability, and performance.\n\nBachelor's Degree in Data Science, Statistics, Engineering, or Computer Science Required.\n\nApply Now and be part of a team shaping the future of data-driven innovation in engineering and manufacturing!\n\nEmployment Type: Full-Time"}, {"title": "Entry Level Software Engineer - Data Backend Engineer (Remote - Canada)", "company_name": "Yelp", "location": "United States (Remote)", "link": "/jobs/view/3752881757/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fankeoxifvhpTzOYvowHYQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3752881757", "description": "About the job\nYelp engineering culture is driven by our values: we\u2019re a cooperative team that values individual authenticity and encourages creative solutions to problems. All new engineers deploy working code their first week, and we strive to broaden individual impact with support from managers, mentors, and teams. At the end of the day, we\u2019re all about helping our users, growing as engineers, and having fun in a collaborative environment.\n Interested in exploring data and finding innovative ways to collect it, curate it, and make it easily accessible for others to use? How about doing that at an enormous scale? Yelp\u2019s datasets contain billions of interactions between users and local businesses around the globe. If working with our vast amount of data sounds exciting, come and join us!\n\nThe Marketplace Data Observability team is responsible for managing, governing, and monitoring all of Yelp\u2019s Ads data to protect our revenue stream and also support offline analysis that aids in making long term company decisions. We build tools that help us to better understand our data quality, and we strive to uncover data issues before they impact our consumers.\n\nAs a Data Backend Engineer on this team, you will be responsible for building elegant and scalable data products that serve critical, up-to-date, structured information to support different types of analytics within Yelp. As a core contributor to our growing data modeling and data warehousing engineering efforts, you will help design and own mission-critical data flow pipelines and datastores to enable decisions including effective A/B testing and company investments.\n This opportunity is fully remote and does not require you to be located in any particular area in Canada. We welcome applicants from throughout Canada. We\u2019d love to have you apply, even if you don\u2019t feel you meet every single requirement in this posting. At Yelp, we\u2019re looking for great people, not just those who simply check off all the boxes.\n  Build systems that can effectively store and crunch terabytes of data.\nDesign and develop data models for efficient data storage, retrieval, and reporting.\nCreate and maintain conceptual, logical, and physical data models using industry-standard modeling tools.\nCollaborate with cross-functional teams, including engineers, data analysts, business analysts, and data scientists, to understand data requirements and translate them into effective data models.\nParticipate in data integration efforts, including ETL processes and data migration.\nStay up-to-date with industry best practices, emerging technologies, and trends related to data warehousing.\nSupport on-call rotations as needed to operate the team.\n  Understanding of high performing and scalable data systems.\nExperience in building and orchestrating ETL pipelines.\nExperience with Data Lake or Data Warehouse landscape.\nA hunger for tracking down root causes and fixing them in systematic ways.\nAbility to communicate effectively to technical and non-technical cohorts alike. \nExposure to some of the following technologies: Python, AWS Redshift, AWS Athena / Apache Presto, Big Data technologies (e.g S3, Hadoop, Hive, Spark, Flink, Kafka etc), NoSQL systems like Cassandra, DBT."}, {"title": "Data Engineer (Associate Consultant), June 2024", "company_name": "UDig", "location": "Nashville, TN (Remote)", "link": "/jobs/view/3716039960/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=XUjo40kU55HnDCqJ%2F3FdPg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3716039960", "description": "About the job\nCan UDig It? \n\nUDig designs, builds, and implements technology solutions that deliver on business objectives. People join our team because they are excited to have an impact on the future of our organization, passionate about delivering meaningful results, and energized by solving complex problems. Oh, and they like working with other driven, smart, and authentic people!\n\nUDig is proud to be recognized as a \"Best Places to Work\" in Virginia and Tennessee. We're committed to fostering a transparent environment where leadership is accessible, professional development opportunities are abundant, and you are empowered to achieve your career goals. You'll collaborate, develop deep relationships with teammates, and accelerate your success while having fun!\n\nIf you're graduating in May 2024 and excited by the idea of impacting organizations by building data solutions to achieve business goals, you should consider joining our growing technology team.\n\nHow Things Go Down\n\nAs a new, entry-level teammate, you'll take part in our \"Breaking Ground\" training program designed to help you quickly get up-to-speed on UDig's delivery approach within a project team. The program covers:\n\nUDig delivery process and methodologies such as gathering requirements and delivering projects using agile\nTechnical guidance and instruction on the best practices from Data leaders at UDig\nDelivering executive and technical presentations\nCoaching to prepare you to for success on your first client project at UDig\n\nA typical day might entail:\n\nProject standups and check-ins\nDesign and development of data pipelines, BI reports, and more\nPlanning and participating in project demos\nContributing to internal UDig projects\nCareer and job growth through UDig-provided training\nCreating long-lasting friendships with teammates through fun activities as well as your projects (including social events, winning a prize for guessing the weight of pumpkin, or sharing experience on cool tech side projects)\n\nUDig believes it is important for Associate Consultants going through our Breaking Ground program to reside in one of our major markets (Richmond, VA or Nashville, TN metro area) for at least the first year of employment.\n\nWant to learn more? Hear directly from our teammates by checking out our 'A Day in the Life' blog series: https://www.udig.com/digging-in/breaking-ground-2023/\n\nSound Interesting? Here's the Technical Piece\n\nBelow are a few boxes you should be able to check if\u202fyou're interested: Don't check every box? Go ahead and apply!\n\n1+ year of experience with relational database management systems such as Microsoft SQL Server, MySQL, or Oracle\nUnderstanding of the full lifecycle of design, development, and implementation of data solutions including architecture design, development, testing\nSome experience in ETL development or reporting is a plus\nAbility to present technical ideas and high-level concepts and solutions to internal and external team members with varying degrees of technical knowledge\nBachelor's Degree in in computer science, information technology, or any science or business discipline\nAbility to take your work seriously but also seriously enjoy a good time with your UDig colleagues\n\nWhat's in It For You? \n\nAt UDig, you will build your career alongside talented and experienced developers while gaining experience and having fun! We provide continuous challenges and professional development opportunities. Additionally, we offer: \n\nCompetitive salary, merit reviews, and career advancement paths\nFlexible, hybrid environment\nIndividual $1500 Training Budget\nRegular team building and social activities (virtual and/or in-person)\nTransparent culture with strong communication and access to leadership\nGreat benefits like\nGenerous Paid Time Off, company holidays, and parental leave\nMultiple Single and Family Health Insurance plans to choose from\nDental & Vision coverage\nShort-Term and Long-Term disability\nOptional accident and critical illness coverage\nMatching 401(k)\nand more!"}, {"title": "Expression of Interest: Data Engineer", "company_name": "Fingerprint for Success (F4S)", "location": "New York, NY (Remote)", "link": "/jobs/view/3787769880/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=l%2FEDpQqUNmCttGdIyTk7Cg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787769880", "description": "About the job\nWe are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.\n\nThe F4S Talent Pool is a pilot project designed to:\n\nHelp job seekers get discovered by our partners based on their anticipated hiring needs.\nProvide optional support and resources for job seekers in their career endeavors.\nHelp individuals understand, and bring out the best in themselves and each other.\n\nThe F4S Talent Pool process:\n\nOnce you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.\n\nAbout Fingerprint For Success (F4S)\n\nBacked by 20+ years of research, F4S\u2019s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high performing teams.\n\nKeep in mind, joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your F4S work style assessment to match the hiring needs of our partners.\n\nYour feedback is a gift! Write to us via:\n\nPowered by JazzHR\n\noWuPJ9lwq9"}, {"title": "Data Engineer Seattle, WA (remote)", "company_name": "Conch Technologies, Inc", "location": "United States (Remote)", "link": "/jobs/view/3765052211/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=SBETI78UMutCHuRp5FnP%2FA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3765052211", "description": "About the job\nHi,\n\nGreetings from Conch Technologies Inc\n\nData Engineering Lead \u2013 With Databricks and Unity Catalog\n\nLocation \u2013 Seattle, WA (remote)\n\nDuration \u2013 12 months\n\nNeed: Design and implement data architecture and strategy, leveraging technologies such as Databricks, Unity Catalog, Privacera, and Collibra.\n\nAs a Data Engineering Lead, you will play a pivotal role in defining and executing technical deliverables while assessing complexities and levels of effort. You'll engage with both vendor and internal teams to resolve technical challenges and ensure seamless collaboration. Your leadership will extend to planning and prioritization with the SCRUM master, facilitating effective coordination between onsite and offshore teams, and driving feature prioritization through collaboration with product managers.\n\nKey Responsibilities\n\n Lead the definition and breakdown of technical deliverables, assessing complexities and levels of effort.\n Collaborate with vendor and internal teams to address technical issues.\n Drive planning and prioritization in conjunction with the SCRUM master.\n Coordinate with onsite and offshore teams for timely project delivery.\n Collaborate with product managers to prioritize features, communicate with stakeholders, and clarify requirements.\n Collect and manage user-reported issues, ensuring prompt resolution and communication.\n Define comprehensive test plans, oversee their execution, and document results.\n\n--\n\nWith Regards,\n\nNagesh G\n\nMobile: 408-381-5645\n\nDesk: 901-313-3066\n\nEmail: nagesh@conchtech.com \n\nWeb: www.conchtech.com"}, {"title": "Remote Work - Need Azure Fabric Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3759325599/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=C3iVjmcR9wjUip1nNjUiCg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3759325599", "description": "About the job\nThe client is placing heavy emphasis on Microsoft and Azure Fabric experience\n\nInformation Management Engineer \u2013 Take the identified information management sources and assist migration to the Client Data Lake, Azure Fabric. The engineer will transform information management (de-dup, clean and enrich).\n\nTake the identified data sources and bring them into our Data Lake, Azure Fabric. Once the data is in, the engineer will need to be able to transform data (de-dup, clean and enrich the data). We are focused on using a medallion data model for data architecture. The tools we are working with are: Delta Parquet files, python, spark, notebooks, T-SQL, Fabric lakehouse/warehouse, Dataflows Gen2, Data Pipelines, Direct Lake, shortcuts, ADF."}, {"title": "Data Engineer", "company_name": "Catalist", "location": "Washington, DC (Remote)", "link": "/jobs/view/3744951279/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ui1Fk6ICmnPNqAlZDZAcwA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3744951279", "description": "About the job\nFor over 17 years, Catalist has been a leader in civic data and data science innovation. Our mission is to provide progressive organizations with the data, software, and services needed to better identify, understand, and communicate with the people they need to engage and mobilize. Our clients include the largest, most influential organizations in the U.S. active in civic engagement, advocacy, and political campaigns.\n\nCatalist is home to a dedicated, creative team of technologists, data scientists, and campaign experts committed to using our talents and technology to nurture a vibrant and growing progressive community.\n\nAs a Data Engineer at Catalist, you will work closely with the Analytics and Technology departments to design, build, support, and maintain various data pipelines and processes with an end goal of providing data and intelligence to the progressive community. This role primarily involves translating data architecture designs into functional processes, code, and systems.\n\nThe ideal candidate will be a highly motivated individual with excellent technical skills, a strong desire to learn new skills, and an interest in progressive politics. Catalist values creativity and problem-solving. Our work is on the cutting edge of data-driven politics, and your support will help Democratic candidates and progressive organizations conduct successful advocacy and electoral campaigns.\n\nThis position reports to the Deputy Chief Data Officer. The Data Engineer is a part of a growing Data team that supports all underlying work at Catalist.\n\nThis position is included in our CWA bargaining unit.\n\nPrinciple Duties & Responsibilities\n\nDevelop scalable, production processes, code, and systems for data ingest, transformation, modeling, and reporting across a variety of platforms\nTranslate mock-ups and designs into functional processes, code, and systems\nCreate architecture designs when needed\nProvide quality assurance and testing on processes, code, systems, and products\nBecome an internal subject matter expert on various datasets and support other Catalist departments/teams on usage of those datasets\nExecute ad-hoc data and database maintenance tasks as requiredProject manage cross-departmental efforts, with direct responsibility for stakeholder engagement, management, and execution of technical elements\n\n\nRequirements\n\nBS or BA in a technical field, or relevant experience\n1-2 years of experience working with SQL databases\nExperience with data cleaning or ETL processes\nExperience with distributed computing systems and/or distributed datastores (particularly the Hadoop ecosystem)\nExperience managing projects\nFamiliarity with Catalist data, progressive politics, voter files, and/or commercial data\nBackground check required\n\n\nPreferred Skills & Abilities\n\nWillingness to be a problem solver and produce results in a fast paced environment\nAbility to be creative and personable, and articulate ideas clearly\nExperience working with SQL databases\nProficiency with Python or another object-oriented programming language (R, Java, Scala, etc\u2026)\nExperience working in cloud environments (AWS, GCP, etc.)\nExperience working in command line environments such as Bash\nExperience with a version control tool such as git or github\n\n\nBenefits\n\nMedical, Dental, Vision, Prescription Drug\n\nCatalist offers Medical, Dental, Vision, and Prescription Drug coverage for eligible staff and their eligible dependents. Catalist\u2019s Medical plan is a comprehensive PPO program including Prescription Drug coverage with 85% of the premium paid by Catalist. Dental and Vision coverage is provided at no cost to employees.\n\nGroup Term Life Insurance and Long-Term & Short-Term Disability Coverage\n\nGroup Term Life Insurance and Long-Term and Short-Term Disability coverage is available for eligible staff. These benefits are provided at no cost to Catalist employees.\n\n401(k) Safe Harbor Plan\n\nA 401(k) Safe Harbor Plan is available to eligible staff with a 3% contribution from Catalist from the date of hire. Employees may contribute pre-tax or post-tax from their salary up to the legal limits set forth by the IRS.\n\nMedical and Dependent Care Flexible Spending Accounts (FSAs)\n\nCatalist offers an FSA Program that gives eligible staff the ability to pay out-of-pocket medical/dental/vision/child care expenses from pre-tax earnings.\n\nTransit Benefits\n\nCatalist also makes available a Transit benefit FSA program to eligible employees using pre-tax contributions with a company match.\n\nProfessional Development and Remote Work Expenses\n\nEligible employees may be reimbursed up to $750 each year for professional development / education and remote work expenses.\n\nStudent Loan PayDown or SaveUp\n\nCatalist offers a Student Loan PayDown and College SaveUp benefit for eligible staff.\n\nVacation, Personal Leave, Sick Leave Benefits\n\nCatalist offers generous vacation benefits to all eligible staff. Eligible employees also receive:\n\n 14 Paid Holidays\n Personal Days\n Sick Leave\n Parental Leave\n\n\nHybrid Office/Remote Work\n\nCertain positions at Catalist are eligible for Office/Remote Hybrid or full Remote status."}, {"title": "Remote Entry Level Data Analyst/Engineer", "company_name": "SynergisticIT", "location": "White Plains, NY (Remote)", "link": "/jobs/view/3774171423/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=q54sk1%2FRww0z%2BC%2BE8e0jQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774171423", "description": "About the job\nAt SynergisticIT, we're all about making connections. Whatever IT goals you have, our software programmers can help achieve those. Our Software development teams can take up turnkey projects and execute them in an effective and efficient manner. If you are looking to source talent our recruiters will find the ideal IT talent for your company. What's the secret to our success? Well, it all starts with taking quality time to listen to each client's specific needs. After we have a thorough grasp of your IT goals, we can better customize our Developments as per your specific needs. We can also tailor make recruiting programs to exceed your expectations. Since our founding in 2010, SynergisticIT's strategies have earned the company an enviable position in the software development, IT staffing and IT skill enhancement fields. SynergisticIT continues to work with hundreds of satisfied American clients with our software programmers working on our projects and after gaining hands on experience on cutting edge technologies moving to contribute their skills to great clients like Apple, Google, Client, Ebay, Paypal, Kroger, the Walt Disney Company and hundreds more. If you are tired of working with inefficient programmers who take a lot of time to ramp up we want you to try us. Our software programmers can hit the ground running and get you the maximum return on your investment. You have already tried the rest its time you tried the best. SynergisticIT - Home of the Best Data Scientists and Software Programmers in the Bay Area.\n\n Why Us ? \n\nSynergisticIT has a proven track record of successfully skill enhancement and staffing IT employees for some of the world's most iconic brands. Our team takes the time to fully understand every client's needs so we could best meet your IT staffing requirements. The knowledgeable staff at SynergisticIT is always more than happy to work with clients to ensure they reach their software development goals. Besides staffing, SynergisticIT is also committed to helping young IT professionals advance their career with a robust upskill program . Everyone who goes through SynergisticIT's program learns all the skills necessary to succeed in many IT fields ranging from Java to Machine Learning. Additionally, everyone trained at SynergisticIT has been through extensive mock and technical interview screenings to bolster their career prospects. Last, but certainly not least, SynergisticIT takes great care to respect the privacy considerations for every client. All companies who work with SynergisticIT can rest assured their confidential data is protected using the most up-to-date encryption technologies. SynergisticIT also complies with all the latest NDA agreements.\n\n REQUIRED SKILLS For Java /Software Programmers \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Time series analysis \n\n We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2022 and at Gartner Data Analytics Summit (Florida)-2023 \n\nOracle CloudWorld Event (OCW) Las Vegas 2022 | SynergisticIT - YouTube\n\n https://www.youtube.com/watch?v=OAFOhcGy9Z8 \n\n https://www.youtube.com/watch?v=EmO7NrWHkLM \n\n https://www.youtube.com/watch?v=NVBU9RYZ6UI \n\n https://www.youtube.com/watch?v=Yy74yvjatVg \n\nSynergisticIT at Gartner Data and Analytics Summit 2023 - YouTube\n\nFor preparing for interviews please visit  https://www.synergisticit.com/interview-questions/ \n\n We are looking for the right matching candidates for our clients \n\n Please apply via the job posting \n\n REQUIRED SKILLS For Java /Full Stack/Software Programmer \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning Positions \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Tensorflow \n\n If you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team. \n\n No phone calls please.  Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates"}, {"title": "Data Engineer", "company_name": "Anika Systems", "location": "Leesburg, VA (Remote)", "link": "/jobs/view/3787766893/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=mApL9JnN%2B%2Fr2%2BYUPlxRYOA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787766893", "description": "About the job\nAnika Systems is a fast growing, woman-owned small business that specializes in providing innovative IT solutions for federal government agencies. Our expertise lies in accelerating delivery in Data and Analytics, Intelligent Automation, Application Development, and IT Modernization. We are currently expanding our Federal team and are seeking a passionate and talented Data Engineer. This opportunity is 100% remote.\n\nMust be a U.S. Citizen with the ability to obtain and maintain a government suitability clearance\n\nResponsibilities\n\nDesigns, develops, builds, analyzes, evaluates, and installs database management systems to include database modeling and design, relational database architecture, metadata, and repository creation and configuration management.\nDefines and oversees database organizations, standards, controls, procedures, and documentation. Provides technical consulting in the definition, design, and creation of a database environment.\nAdvises applications development staff and users on data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nDesigns and implements databases with respect to access methods, access time, batch processes, device allocation, validation checks, organization, protection and security, documentation, and statistical methods.\nUses data mapping, data mining, and data transformational analysis tools to design and develop databases.\nDetermines data storage and optimum storage requirements.\nPrepares system requirements, source analysis, and process analyses and designs throughout the database implementation.\n\nRequired Skills And Experience\n\nBA/BS and 1 year of relevant experience\nExperience with DataBricks, SQL, Python\nApplications development experience with data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nExcellent communication skills (both oral and written) with process-oriented organizational skills to ensure project success\n\nDesired Skills And Experience\n\nDemonstrated successful development experience with the full life cycle of government database management systems including database modeling and design, relational database architecture, metadata and repository creation, and configuration management.\n\nPowered by JazzHR\n\nTlKWIorucJ"}, {"title": "Remote Work - Need Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3774712978/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=r3Bp%2F0qcOD2OpC5jpbUOcA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774712978", "description": "About the job\nTitle : Data Engineer\n\nLocation : Remote\n\nDuration : 6+ months to start (long term contract (work order are extended 2x year))\n\nPlease Fill out the skill matrix: Technology Some Awareness (1) Novice (2) Intermediate (3) Advanced (4) Expert (5) SQL Tableau Desktop /Creator Tableau API Integration AWS - S3 Glue AWS - Athena Redshift AWS - Lambda AWS - SNS/SQS msExcel\n\nProfile: Data Analyst will work closely with Architects and leads in the Interflow team, key business stakeholders including program staff to support the organizations effort in the areas of data Analysis and information management. Role should have strong experience in writing SQL queries to extract data for reports. Candidate should have experience working with large set of data. As these reports are very critical for the company and we are looking for an expert who is capable of handling, consuming and extracting large sets of data for analysis and reports.\n\nMust Haves\n\nAWS - Athena\nAWS - S3\nGlue\nhands-on development of reporting applications\nMS Excel\nRedShift\nSQL\nTableau Desktop /Creator\n\nNice\n\nAWS - Lambda\nAWS - SNS/SQS\nTableau API Integration\n\nSkills\n\nBA/BS required (major in an analytical field desired) \nA minimum 10+ years work-related experience. \nExperience in hands-on development of reporting applications for the Web in a professional environment \nThe Ethos of continuous improvement and interest in learning new things.\nStrong analytical thinking and structured problem-solving ability\nAbility to handle multiple projects and assignments simultaneously and effectively in a cross-functional team environment. \nVersed on the agile methodology and best practices. \nExcellent interpersonal and collaboration skill with the ability to work with a diverse set of colleagues, across functions, from different organizations, disciplines, etc. \nSelf-starter, ability to set priorities, work independently and attain goals"}, {"title": "Data Engineer", "company_name": "Team Remotely Inc", "location": "Houston, TX (Remote)", "link": "/jobs/view/3786249829/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=78lY5Jo%2FfyRPC6TQYCdvww%3D%3D&trk=flagship3_search_srp_jobs", "id": "3786249829", "description": "About the job\nThis is a remote position.\n\n Data Engineer (US/Canada Residents Only, 1 year experience, remote)\n\nTeam Remotely Inc. is a staffing and recruitment agency that offers a comprehensive solution for talent acquisition, including sourcing, vetting, pay rolling, and managing talent. Whether you need contract staffing, direct hire, direct sourcing, talent pools, or diversity initiatives, our model can support your hiring strategy.\n\n Hiring Type: Full-Time\n\n Base Salary: $55K-$70K Per Annum.\n\nResponsibilities\n\nDesigning user interface changes for web-based DB applications. \nReviewing application requirements and interface designs. \nDeveloping and implementing highly responsive user interface components using react concepts. \nWriting application interface codes using JavaScript following react.js workflows. \nTroubleshooting interface software and debugging application codes. \nDeveloping and implementing front-end architecture to support user interface concepts. \nMonitoring and improving front-end performance. \n\nQualifications And Experience\n\nBachelor\u2019s degree in computer science, information technology, or a similar field. \nPrevious experience working as a react.js developer. \nIn-depth knowledge of JavaScript, CSS, HTML, jQuery, and front-end languages. \nKnowledge of REACT tools including React.js, Webpack, Enzyme, Redux, and Flux. \nExperience with user interface design. \nKnowledge of performance testing frameworks, including Mocha and Jest. \nExperience with browser-based debugging and performance testing software. \nTroubleshooting skills. \nProject management skills. \nProblem-solving skills. \nVerbal communication skills. \n\n Why work with Team Remotely?\n\nTeam Remotely Inc. is a staffing platform offering a seamless experience for employers and candidates. Employers can post job openings and specify their requirements, while candidates can create profiles and upload resumes.\n\nThe team of Team Remotely continuously learns and adapts based on previous successful placements, constantly improving its matching capabilities. This ensures that the recommendations provided by Team Remotely are tailored and accurate, increasing the likelihood of a successful match between employers and candidates. By providing intelligent and data-driven solutions, they strive to enhance the efficiency and effectiveness of the hiring process, ultimately helping companies find the best talent and individuals find their dream jobs."}, {"title": "Data Engineer", "company_name": "ZAG Technical Services", "location": "United States (Remote)", "link": "/jobs/view/3771403464/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=Hf0VefrKXBFIL5MtHmY0Pw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3771403464", "description": "About the job\nWe are looking for a tech savvy Data Engineer to contribute to the building, operating, and scaling of next generation data platforms and tools that will power data-driven analytics capabilities throughout the ZAG products and services portfolio, spanning areas such as business intelligence, reporting, and data analytics.\n\nThe ideal team member is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. Must be comfortable supporting the data needs of multiple clients and teams, systems, and products within the modernization of ecosystems. The right-fit person will be excited by the prospect of optimizing or even re-designing data processes to support next generation of advanced analytics capabilities, which in-turn translates to implementing strategic enhancements to maintain competitive advantage in the industry.\n\nThe successful team member will leverage their proficiency to\u2026\n\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\nBuild the analytics processes required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Data Management and cloud based \u2018big data\u2019 technologies.\nBuild analytics tools that utilize technologies to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.\nCollaborate with client teams to identify and solution regional and local requirements within the context of the standard global business model for enterprise analytics.\nParticipates in meetings, discussions, strategy sessions where changes, improvements, and enhancements are proposed, evaluated, and approved. \n\n\nThe exceptional team member will have or demonstrate progressive experiences in\u2026 \n\nBachelor\u2019s degree or higher in Computer Science or related field\nExperience working with BI and data warehousing tools\nExperience working with enterprise data, where security is paramount and data governance is critical\nAbility to work independently and in a team environment, and effectively engage all levels of the organization\nUnderstanding of distributed systems driving large-scale data processing and analytics with a successful history of manipulating, processing, and extracting value from large, disconnected datasets.\nAbility to communicate effectively (listening, presenting, and questioning)\nStrong organizational, written, and communication skills\n\n\nAbout ZAG\n\nZAG Technical Services empowers our clients\u2019 success through intelligent business solutions that increase employee productivity, network security and team collaboration. We aim to ensure that our client\u2019s IT investments drive ROI, provide a competitive advantage and help businesses grow. We do this by living our core set of values: Integrity Foremost, Client-Centered, Accountable Always, Teamwork Throughout, and Exceptional is the Goal.\n\nWhat ZAG Has To Offer\n\nCompetitive compensation package \nBenefits including medical, dental, vision, 401K and life insurance \n\n\nZAG Technical Services is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all of our employees.\n\nZAG Technical Services is an E-Verify participant and will provide the Social Security Administration (SSA) and, if necessary, the Department of Homeland Security (DHS), with information from each new employee\u2019s Form I-9 to confirm work authorization."}, {"title": "Junior Data Engineer", "company_name": "HireMeFast LLC - Career Accelerator - Land A Job", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3786249797/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D&trk=flagship3_search_srp_jobs&lipi=urn%3Ali%3Apage%3Ad_flagship3_search_srp_jobs%3BQExogrHBQoSF9zZm%2B9kufw%3D%3D&lici=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D", "id": "3786249797", "description": "About the job\nThis is a remote position.\n\n Job Title: Junior Data Engineer\n\n Employment Type: Full-Time\n\n Salary: $64,000 - $76,000 per annum\n\n Experience Required:  Minimum 1 year of project experience\n\nAbout us: HireMeFast is a leading staffing and recruitment agency specializing in connecting businesses with top-tier talent across various industries. Our mission is to bridge the gap between exceptional candidates and organizations needing their skills, expertise, and unique qualities. Our team of experienced and dedicated recruitment specialists utilizes innovative sourcing strategies, and a vast network to identify and attract top talent. We conduct comprehensive procedures to ensure that only the most qualified candidates are presented to our clients.\n\nPosition Summary\n\nJoin the fast-paced, innovative, and collaborative environment focused on providing an AIOps platform that enhances the intelligence of the CVS Health infrastructure. Work closely with subject matter experts and colleagues to build and scale out machine learning and AI solutions that will detect, predict, and recommend solutions to correct issues before system impact and enhance efficiency, reliability, and performance of CVS Health\u2019s IT operations.\n\nKey Responsibilities include:\n\nData pipeline development: Designed, implemented, and managed data pipelines for extracting, transforming, and loading data from various sources into data lakes for processing, analytics, and correlation. \nData modeling: Create and maintain data models ensuring data quality, scalability, and efficiency \nDevelop and automate processes to clean, transform, and prepare data for analytics, ensuring data accuracy and consistency \nData Integration: Integrate data from disparate sources, both structured and unstructured to provide a unified view of key infrastructure platform and application data \nUtilize big data technologies such as Kafka to process and analyze large volumes of data efficiently \nImplement data security measures to protect sensitive information and ensure compliance with data and privacy regulation \nCreate/maintain documentation for data processes, data flows, and system configurations \nPerformance Optimization- Monitor and optimize data pipelines and systems for performance, scalability and cost-effectiveness \n\nCharacteristics of this role:\n\nTeam Player: Willing to teach, share knowledge, and work with others to make the team successful. \nCommunication: Exceptional verbal, written, organizational, presentation, and communication skills. \nCreativity: Ability to take written and verbal requirements and come up with other innovative ideas. \nAttention to detail: Systematically and accurately research future solutions and current problems. \nStrong work ethic: The innate drive to do work extremely well. \nPassion: A drive to deliver better products and services than expected to customers. \n\nRequired Qualifications\n\n2+ years of programming experience in languages such as Python, Java, SQL \n2+ years of experience with ETL tools and database management (relational, non-relational) \n2+ years of experience in data modeling techniques and tools to design efficient scalable data structures \nSkills in data quality assessment, data cleansing, and data validation \n\nQualifications\n\nKnowledge of big data technologies and cloud platforms \nExperience with technologies like PySpark, Databricks, and Azure Synapse. \n\nEducation\n\nBachelor\u2019s degree in Computer Science, Information Technology, or related field, or equivalent working experience\n\nWhy HireMeFast LLC?\n\nAt HireMeFast, we understand that finding the right individuals to join your team is crucial for success and growth of your organization. We are committed to streamlining the hiring process for our clients, ensuring they have access to a diverse pool of highly qualified candidates who are a perfect fit for their specific needs."}, {"title": "Data Engineer  - Need candidates from Texas ( REMOTE )", "company_name": "Pulivarthi Group (PG)", "location": "United States (Remote)", "link": "/jobs/view/3770685212/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=9gdMwuif%2BpB%2BCQYvQi3imQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3770685212", "description": "About the job\nFollow us on Linkedin: https://www.linkedin.com/company/pulivarthigroup/\n\nPulivarthi Group LLC is a Global Staffing & IT Technology Solutions company, with our prime focus of providing world class solutions to our customers with the right talent. We combine the expertise of our team and the culture of your company to help you with the solution that is affordable and innovative using high quality standards and technologies.\n\nWe\u2019ve served some of the largest healthcare, financial services, and government entities in the U.S.\n\nData Engineer - FT only - Azure SQL with data lakes experience\n\nData Engineer -\n\nthe resource requirement for this Data Engineer role needed more clarification.\n\nOur customer is requesting a resource who knows Azure CI/CD (Continuous Integration / Continuous Deployment) to build and integrate their DevOps pipeline.\n\nOnce the DevOps pipelines are mapped, they need the Data Engineer to connect to all the Data Sources, primarily their Data Lake.\n\nThe Customer (based in Houston) is interested in a resource in a time zone not too far removed from the Central Time Zone to allow real time collaboration."}, {"title": "Data Engineer", "company_name": "Otter Products", "location": "Fort Collins, CO (Remote)", "link": "/jobs/view/3766728302/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=77U3U7CAi1PxgDmpI3%2BiNQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3766728302", "description": "About the job\nOverview \n\nOtter Products is currently recruiting for a Data Engineer to join our Data team! You can be based in our Fort Collins, CO office with flexibility to work a portion of your time remotely, or 100% Remote in the U.S.\n\nThe Data Engineer applies professional experience, concepts, and company objectives toward building and operationalizing the minimally inclusive data necessary for the enterprise data initiatives following industry standard practices and tools. The bulk of the Data Engineer's work would be in designing, managing and optimizing data pipelines, then moving these data pipelines effectively into production for key data and analytics consumers like business/data analysts, data scientists or other stakeholder that needs curated data for data and analytics use cases across the enterprise. The Data Engineer also ensures compliance with data governance and data security requirements while creating, improving and operationalizing these integrated and reusable data pipelines to enable faster data access, integrated data reuse and vastly improved time-to-solution for data and analytics initiatives. Additionally, the Data Engineer will be expected to collaborate with other data team members and data consumers working on the models and algorithms developed by them toward optimization for data quality, security, and governance to put them into production leading to potentially large productivity gains.\n\n About Otter Products \n\nOtter Products, we grow to give. From our founder's garage in 1998 to the global technology leader we are today, Otter Products continues to drive growth through innovation.\n\nThrough our industry-leading brands - OtterBox, Liviri and OtterCares - we provide our partners the number-one selling and most trusted products in our categories. Our philanthropic spirit is the foundation on which we foster our partner relationships, allowing us to grow and to give - together.\n\nBy way of our charitable arm, the OtterCares Foundation, we support our communities and invest in our future through education that inspires kids to change the world.\n\nAnd even as our global community of Otters continues to grow, our founder's core values are still at the heart of everything we do. We measure our success by our ability to give back to our communities and strengthen opportunities for all.\n\nFor more information visit otterproducts.com\n\n Responsibilities \n\nComplete analysis, design and development of BI solutions using Azure SQL Server\nFamiliarity with Data Warehouse concepts\nExperience coding the extraction, transformation, and loading (ETL) processes\nDatabase development primarily in SSIS, Data Factory and SQL\nPartner with the business to determine end user database/reporting needs and requirements\nGenerate ad hoc reports using Power BI or SSRS\nCollaborate with other developers to create and implement best approach solution(s)\nTroubleshoot Azure tools, systems, and software\nReview queries for performance issues, making changes as needed\nCollaborate with team to performance-tune Azure application as necessary\nAssist with the analysis and extraction of relevant information from large amounts of historical business data to feed data science initiatives\nParticipate and support the design and documentation of processes for large scale data analyses, model development, model validation and model implementation\nSupport and maintain a positive safety culture by following all safety policies and procedures and actively contributing to a safe working environment\nOther duties as required\n\n Qualifications \n\nBachelor's degree required. Degree in Computer Science or Mathematics preferred\nMinimum of four years of experience in an IT or analytical role required. Experience in database development, report writing and/or statistics preferred.\n\n EEO \n\nOtter Products, LLC is an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, age, marital status, pregnancy, sex, sexual orientation, gender, gender identity or expression, national origin, disability, veteran status, or any other characteristic or status protected by law.\n\n For US Based Roles Only - Compensation Range Minimum \n\nUSD $90,000.00/Yr.\n\n For US Based Roles Only - Compensation Range Maximum \n\nUSD $120,000.00/Yr.\n\n Additional Total Rewards \n\nProfit Sharing Program Eligible, Benefits Eligible - Full Time- check out otterproducts.com/careers/why for more info"}, {"title": "Software Engineer, Distributed Systems", "company_name": "Hightouch", "location": "United States (Remote)", "link": "/jobs/view/3628572539/?eBP=CwEAAAGMd4QO8APODyE_-mMOPhYyQGg5ZjVbPwq_3V7DK0B2bLAf10pcedM9S2T71X5i4TiJuEwOzWBbSiTKbLuk-VD-_Y-dN5uyf8YpYkHYqEYFxySMS9zr2hRjNrxX9gXbxssh35TFK9YTNV2WeH2Ncb675JgFbJTgNWSZySFC6rrdJGmdSDTr2q5d3VoxORz3GjsDbzjeyZQCvQ07RqYw7oujWSgslycs2LMCY_-N0NEj6e0PvbaYKiZ_5IW34bGVSYLXpHwqa8oOcNttYZ90UA3E_7sDSFzABdIVlfrp5zzERrIYy5sh2MIInxHdDoKgQw8VmOn5uM4S0tXUl-i7LnN0_W0NfPBmLiVDNuveIOm4MfDNPxfe5QKPk6FUAVpvgbcoJn9yRyE&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=rph%2FT6Q6Vkp6YoggJf9cJQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3628572539", "description": "About the job\nAbout Hightouch\n\nHightouch\u2019s mission is to empower everyone to take action on their data. Through our Reverse ETL platform, business and data users can seamlessly sync data from where it resides, such as warehouses and databases, to where it is needed, including operational systems and SaaS tools. Traditionally, acting on data has required engineering time and bandwidth, and left most business users stuck with charts and reports that are unable to take automated action on their data. With Hightouch, every business user, without writing any code, can activate data to streamline critical processes, improve marketing performance, and scale operations.\n\nOur team operates with a focus on making a meaningful impact for our customers. We believe in approaching challenges with a first principles thinking mindset, moving quickly and embracing our value of efficient execution, and treating each other with compassion and kindness. We look for team members that are strong communicators, have a growth mindset, and are motivated and persistent in achieving our goals.\n\nHundreds of companies use Hightouch, including Spotify, Ramp, Retool, NBA, Plaid, and Betterment. We\u2019re based in San Francisco, are remote-friendly, and backed by leading investors such as Amplify Partners, ICONIQ Growth, Bain Capital Ventures, Y-Combinator, and Afore Capital.\n\nAbout The Role\n\nWe are looking for a distributed systems engineer to work on the systems that power the syncing engine our customers and other engineering teams rely on. This presents an exciting challenge where you can apply your expertise in distributed systems, performance optimization, and troubleshooting to push the boundaries of what is possible and meaningful improve our ability to move massive amounts of customer data. This role also provides a unique opportunity to work on a multi-cloud and multi-region infrastructure that supports a global customer base.\n\nWe believe in enabling our engineers to do their best work for our customers by giving them extremely high levels of ownership and autonomy. This comes in different forms: you will own and deliver projects from start to finish, you will work directly with customers to solve their hardest scaling problems, and you will have a lot of influence over what we work on as a team and company.\n\nSome of the problems we\u2019ll be working on include:\n\nSync Speed: Customers want to sync a lot of data to important destinations like Facebook and Snapchat, which requires us to analyze every part of our syncing process and find where we can optimize to sync data more quickly\nStreaming Syncing: We currently sync data from sources in batch only, but envision a world where we also do syncs in real-time and support streaming sources like webhooks and queues\nScalability and Reliability: As part of our rapid growth, we\u2019re always evaluating current future bottlenecks in scaling and reliability, and architecting for the next order of magnitude of growth\nPersonalization API: Our Personalization API product provides a low latency caching layer on top of data warehouses that enable customers to use their data warehouse for real-time personalization use cases with \nMulti-Region and Multi-Cloud: Supporting our multi-region and multi-cloud backend, including extending it to launch Hightouch on in new regions to support data residency requirements of our global customer base\n\nWe are looking for talented, intellectually curious, and motivated individuals who are interested in tackling the problems above. This is a senior role, but we focus on impact and potential for growth more than years of experience. The salary range for this position is $170,000 - $240,000 USD per year, which is location independent in accordance with our remote-first policy.\n\nAbout You\n\nYou are an engineer with a passion for solving hard technical problems that generate real value for customers. You\u2019re motivated by high ownership and are comfortable in a fast-paced, startup environment.\n\nYou have experience and comfort with distributed systems and high-scale systems or the ability and desire to build this experience quickly. This experience can take different forms:\n\nYou\u2019ve built syncing engines that process and move terabytes of data per day in a scalable way\nYou\u2019ve created event collection or data streaming services that power company critical systems\nYou\u2019ve owned systems that have undergone orders of magnitude of growth and helped them scale accordingly\nYou have relevant skills and are a fast learner who is excited to take on the challenges we face\n\nInterview Process\n\nOur goal with the interview process is to balance speed with giving both parties opportunities to assess whether there is a strong mutual fit. We will ask you questions, but we want you to ask us questions! Our technical interviews focus on how you design systems because we believe this is the best way for us to see how you work and for you to see how we collaborate. We don\u2019t ask you to write code to solve technical brainteasers that don\u2019t appear in your day to day job.\n\nRecruiter screen [30m]: Introductory call with our recruiting team to get to know each other and see if the role could be a good mutual fit.\nSystems design screen [45m]: Designing a data processing feature end-to-end.\nHiring manager interview [30m]: Chat with hiring manager about past experiences and future operating preferences to assess fit on company values and operating principles.\nSystems design interview [90m]: Work with the interviewer to architect a system at a conceptual level. The problem will be at a pretty high level - and have both product and customer requirements as well as technical."}, {"title": "Entry-Level Data Scientist Engineer", "company_name": "Phoenix Recruitment LLC", "location": "Texas City, TX (Remote)", "link": "/jobs/view/3788727625/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=8iwtytHsCu4wMwJ4fj3pxw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3788727625", "description": "About the job\nThis is a remote position.\n\nTitle - Entry-Level Data Scientist Engineer, 1 year of project experience\n\nEmployment Type: Full-time\n\nBase Salary: $60K-$70K\n\nPhoenix Recruitment offers a variety of recruiting services to assist both employers and employees. They specialize in marketing open positions, recruiting, and helping employers to find qualified candidates across various industries. Phoenix Recruitment has expertise in streamlining the hiring process. They can help ensure that the process is efficient, well organized, and compliant with relevant regulations.\n\nSkills and Abilities:\n\nStrong knowledge of R or Python for data analysis and modeling. \nProficiency in statistical programs such as R, SAS, MATLAB, or Python. \nFamiliarity with spreadsheets (VBA) and database applications (Access, Oracle, SQL, or equivalent technology). \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \nAbility to learn new methods quickly and work under deadlines. \nExcellent teamwork and communication skills. \nStrong analytical and problem-solving abilities. \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \n\nPreferred:\n\nKnowledge of actuarial concepts and life, health, and/or annuity products. \nExperience with statistical modeling techniques such as GLM, Decision Trees, Time Series, Regression, etc. \nFamiliarity with Microsoft DeployR. \nExposure to insurance risk analysis. \nBasic experience in computational finance, econometrics, statistics, and math. \nKnowledge of SQL and VBA. \nFamiliarity with R or Python for predictive modeling \n\nWhy Phoenix Recruitment LLC?\n\nPhoenix Recruitment often has an extensive network of employers and candidates. This network allows them to tap into a pool of qualified candidates and connect them with suitable job opportunities. They can also leverage their connections to help employers find the right talent efficiently. Outsourcing the recruitment process to a specialized agency can save your time and resources, avoid delays, reduce administrative burdens, and increase the chances of finding the right fit for your organization."}, {"title": "Data Engineer - Remote Work", "company_name": "BairesDev", "location": "United States (Remote)", "link": "/jobs/view/3783162297/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=OtoCrAAHZFomVup04fy35g%3D%3D&trk=flagship3_search_srp_jobs", "id": "3783162297", "description": "About the job\nWho We are\n\nBairesDev is proud to be the fastest-growing company in America. With people in five continents and world-class clients, we are only as strong as the multicultural teams at the heart of our business. To consistently deliver the highest quality solutions to our clients, we only hire the Top 1% of the best talents and nurture their professional growth on exciting projects.\n\nWe are looking for a Data Engineer to join our Development team and participate in different projects made up of multicultural teams distributed throughout the world.\n\nWhat You Will Do\n\n Interview other Data Scientists and Data engineers to vet their technical abilities.\n Input data from those interviews into the product and help develop a predictive, fair, and enjoyable solution.\n Work together with developers, tech leads, and solution architects to build applications.\n Improve existing structures by adding new functionalities or proposing technological updates so that as a result, the impact of your contribution is significant in the core of each business.\n Improve the user experience on scalable and high availability platforms, contributing to the key differential of each business.\n\nHere\u2019s What We Are Looking For\n\n 5+ years of experience working as a developer.\n 3+ years of experience working as a Data Scientist / Data Engineer.\n Proficient with analysis, troubleshooting, and problem-solving.\n Hands-on experience with managing data loads and data quality.\n Advanced English level.\n\nHow we do make your work (and your life) easier:\n\n 100% remote work.\n Hardware setup for you to work from home.\n Flexible hours - make your schedule.\n Paid parental leave, vacation & holidays.\n Diverse and multicultural work environment.\n An innovative environment with the structure and resources of a leading multinational.\n Excellent compensation \u2014 well above the market average.\n Here you can grow at the speed of your learning curve.\n\nOur people work remotely but with a consistent and robust culture that promotes diversity and teamwork. To continue being the leading software development company in Latin America, we want to ensure that every BairesDev member gets the best growth and professional development opportunities in a diverse, welcoming, and innovative environment.\n\nEvery BairesDev team member brings something unique to our company.\n\nWe want to hear your story. Apply now!"}, {"title": "Data Engineer (Remote)", "company_name": "Cascade Debt", "location": "United States (Remote)", "link": "/jobs/view/3755336571/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=pmUtn1j2zyPr25CNPh2RNw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3755336571", "description": "About the job\nDo you want to work in a small team where you can make a real difference in a company? Would you embrace the challenge of building a high impact platform that is used globally? Then we want to speak with you!\n\nAbout Cascade\n\nCascade is a fintech startup backed by Canadian and US investors that empowers companies to grow by democratizing access to institutional debt. We are building tools that modernize how companies raise and manage debt, lowering the barriers to entry to the $7 trillion specialty finance and alternative credit market for companies around the world. Founded just last year, we are gearing up for our next phase of technical development and are seeking a talented Data Engineer to lead the way.\n\nAbout The Role\n\nAs a Data Engineer you\u2019ll be an early member of a growing team building a pioneering platform in the debt infrastructure space. You will design, build, and launch efficient, scalable, and reliable data pipelines to move and transform data .\n\nWhat You\u2019ll Do\n\n Building databases, data lakes and data ingestion pipelines to integrate customer databases and datasets to our systems \n Ingesting financial datasets from external customers, then u pdating and maintaining accurate and complete data mappings to ensure that our product s are displaying high quality data \n M onitoring and alerting across our data pipelines in order to make sure that our data ingests are reliable and correct \n Translat ing business goals and stakeholder requirements into new data flows and deliver analytical insights ensuring that the data flows create real business value continuously \n\nSkills And Experience\n\n 2 + years experience in a data engineering or data analyst role \n E xperience with relational SQL and NoSQL databases, including PostgreSQL , MySQL, MariaDB, MongoDB, Bigtable, etc. \n E xperience working with ETL technologies , such as Databricks, Fivetran , or dbt \n Proficiency in writing SQL queries and knowledge of analytical data warehouses such as RedShift, BigQuery , and Snowflake \n Some experience with CI/CD automation such as GitHub Actions \n Developing APIs and integrating with 3rd party APIs \n Bonus: experience in fintech / SaaS / credit analysis \n\nPeople Who Thrive At Cascade Are\n\n Self-starters, who take the initiative to tackle challenges in a remote work environment \n Good communicators, who can operate without ego to discuss, learn, grow, and help others do the same \n Passionate about creating great digital experiences for users \n Problem solvers who are not satisfied with the status quo \n\nBenefits\n\nRemote: we are a remote-first company and are very flexible on hours as long as things get done \nHome-office: there\u2019s a $1000 USD home office allowance to set yourself up \nEquity: we expect you to have an owner-mentality, and have the equity plan to match \nBenefits: health, dental, vision, and more \nPerks: we offer free lunches weekly and off-site trips \nJob satisfaction: we offer autonomy, ample opportunities for mastery, and an opportunity to make a difference for companies around the world"}, {"title": "Data Engineer", "company_name": "ISC (Integrated Specialty Coverages, LLC)", "location": "Carlsbad, CA (Remote)", "link": "/jobs/view/3778293528/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ew7%2FfEuXSujs5IGabdqUyw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3778293528", "description": "About the job\nAbout Integrated Specialty Coverages\n\nIntegrated Specialty Coverages LLC (ISC) is a growth stage technology and data-driven insurance company leading innovation in the market.\n\nBacked by one of the leading private equity firms, KKR, and led by a forward-thinking management team, ISC is combining the worlds of insurance and technology to create an Insurtech powerhouse. As a leading online distributor of insurance products for a range of industries and \"Main Street USA\", we are looking for the right people to help us in our mission of achieving exponential growth. We strive to be the number one place to go for brokers and agents to source insurance. To accomplish this, we're building a digitally focused team that deeply understands the intersection between user experience, data, and AI/ML to optimize the way we engage with our customers and partners.\n\nJob Summary\n\nThe Data Engineer role will be the technical liaison between multiple groups including a data science team, the engineering team, product management, and business stakeholders. You do not need any insurance knowledge prior, however, you must quickly dive deep into the insurance world and ask questions to become a subject matter expert. You will be responsible for building a data platform to facilitate the data science team. You must be a self-starter that can build out features such as a data pipeline from scratch. There will be support from both engineering and data science for any buildout.\n\nJob Responsibilities\n\nCreate and maintain optimal data pipeline architecture. \nAssemble large, complex data sets that meet functional / non-functional business requirements. \nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. \nBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using AWS technologies, SQL, and Python. \nWork with stakeholders including the Executive, Product, Data Science, and Engineering teams to assist with data-related technical issues and support their data infrastructure needs. \nWork with data science and analytics teams to strive for greater functionality in our data systems. \n\nMinimum Qualifications:\n\nAdvanced working SQL knowledge and experience working with relational databases, strong query authoring (SQL) as well as working familiarity with a variety of databases (Snowflake, Redshift, MySQL, MSSQL, etc.)\nExperience building and optimizing data pipelines, architecture and data sets. \nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. \nBuild processes supporting data transformation, data structures, metadata, dependency, and workload management. \nA successful history of transforming, processing and extracting value from large disconnected, datasets from a variety of data sources (Flat files, Excel, databases, etc.)\nStrong analytic skills related to working with unstructured datasets. \nStrong project management and organizational skills. \nExperience supporting and working with cross-functional teams in a dynamic environment. \nAbility to mentor/guide/collaborate with other team members. \n\nWe are looking for a candidate with 3-5 years of experience in a Data Engineer role. They should also have experience using the following software/tools:\n\nExperience with relational and MPP databases, including Snowflake and MySQL. \nExperience developing software in an agile environment from the requirements stage to production. \nExperience with version control: git\nExperience with container technologies: Docker\nExperience with data pipeline and workflow management tools: Airflow, Jenkins, AWS Glue, Azkaban, Luigi, etc. \nExperience with AWS cloud services: EC2, ECS, Batch, S3, EMR, RDS, Redshift\nExperience with other cloud services: Snowflake, Airflow\nExperience with object-oriented/object function scripting languages: Python, Java, C++, etc. \nExperience with data modeling and data warehouse design\nExperience with data visualization tools (PowerBI, QuickSight)\n\nThe starting annual pay scale for this position is listed below. Actual starting pay will be based on factors such as skills, qualifications, training, and experience. In addition, the company offers comprehensive benefits including medical, dental and vision insurance, 401(k) plan with match, paid time off, and other benefits.\n\nISC's salary ranges are determined by role and level. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations and could be higher or lower based on a multitude of factors, including job-related skills, experience, and relevant education or training.\n\nNational Pay Range\n\n$100,000\u2014$110,000 USD\n\nBenefits of Working at ISC\n\nCompetitive vacation and flexible working arrangements\nComprehensive and inclusive health benefits\nA variety of professional development and mentorship opportunities\nChoice of technology whether at home or in the office\n\nISC's Ownership Behaviors\n\nDo the Right Thing *Be Relentless. Pursue Excellence. *Personal Responsibility & Own Outcomes\nTry Fast. Learn Fast. Fail Fast. Think Big. *Build Extraordinary Owners & Join Forces *Grit & Determination\n\nApplicants with disabilities may contact ISC HR department via e-mail, and other means to request and arrange accommodation. If you need assistance to accommodate a disability, you may request accommodation at any time. Please contact ISC at HR@ISCMGA.com.\n\nISC believes in creating long-term relationships by being responsive and relevant and by consistently delivering value to our community of customers. Specifically, with our employees, we focus on attracting, developing, and retaining the best talent for our business, challenging our people, demonstrating a \"can-do\" attitude, and fostering a collaborative and mutually supportive environment. ISC is an equal opportunity employer and a participant in the US federal E-Verify program (US).\n\nDiversity creates a healthier atmosphere: All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, gender, gender identity, sexual orientation, marital status, medical condition, genetic information, mental or physical disability, military or veteran status, or any other characteristic protected by local, state, or Federal law."}, {"title": "Data Engineer -Remote", "company_name": "RIT Solutions, Inc.", "location": "Denver, CO (Remote)", "link": "/jobs/view/3768007496/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=1hQH8EDaII1eJ7nQ9BACQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3768007496", "description": "About the job\nDuration: 6+ months CTH\n\nMust have a valid LinkedIn profile\n\nRequired Skills\n\n 3-6 years of experience (no more than 6 years)\n Start up environment experience\n Python, SQL, and DBT"}, {"title": "Big Data Engineer - W2", "company_name": "Megan Soft Inc", "location": "United States (Remote)", "link": "/jobs/view/3674845652/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fCAxh4N%2F4YZJ5aEIASaeeA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3674845652", "description": "About the job\nTitle: Big Data Engineer - W2\n\nLocation: Dearborn, MI (Remote)\n\nDuration: 12+ Months\n\nDirect Client: Ford Motors\n\nMust have skills: Should be able to handle GCP requirements\n\n\"Translates business requirements to build repeatable, sustainable, efficient, coded processes that can be productionized by Software Engineers and readily modified by other Data Engineer Creates POC processes in Dev/QA and works with Software Engineers to productionize and align best coding and process practices that maximize efficiency, speed, stability, system resources and capabilities Awareness of and compliance with data privacy, security, legal and contractual guidelines Incorporates data quality and privacy checks/alerts to minimize bad data being consumed by end users, models and dashboards, and to protect customer data; revises checks for new data issues Validates data products and pipelines are functioning as expected following system or application upgrades, source changes, etc. Works closely with Architects if required to align systems, tools and applications being utilized with business use case and performance requirements Provides data product support and maintenance Establishes, tracks and monitors KPIs related to specific data products and deliverable Participate in iteration cadence activities (Iteration Planning, task estimation, daily scrum, demos, etc.). Will be a part of on-call team to resolve critical production support tickets during business hours and off hours.\n\nSkills Required\n\nExperience with Big Data Technologies (Hadoop , ETL Tool, Spark. Hive etc..) Someone who understands Cloud and has experience working with Cloud Platforms (preferably GCP) Should be able to handle GCP requirements - ingesting into GCS/ Big Query, working with GCP components Hands-on in Data Flow, Cloud Storage (GCS), Big Query and GitHub. Proficient in Python and SQL.\n\nSkills Preferred\n\nManages ETL: uses programming and tools for data ingestion, configures pipelines, applies transformations and decoding, integrates and fuses data, moves and securely deliver Experience NoSQL databases\n\nAdditional Information\n\nIf the candidate is remote only, please indicate \"\"100% Remote\"\" under candidate's name on resume. Other similar schedule notations can be \"\"Local Candidate\"\" or \"\"Hybrid Candidate.\"\" ***HRA Test Required***\""}, {"title": "Remote Opportunity: Quantexa Data Engineer", "company_name": "SPAR Information Systems LLC", "location": "United States (Remote)", "link": "/jobs/view/3680073191/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ioykVCZSOQ7BK5%2FwRQHOWg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3680073191", "description": "About the job\nHello All, Hope you are doing great!! Please go through the job descriptions and let me know your interest. Role: Quantexa Data engineer  Location: Remote Duration: Long Term Contract Job Description: 1. End to End usage of Quantexa including guiding our client team on how to use from both front end and data ingestion Reason they are asking for Certification 2. Resolve Entity issues with data 3. Knowledge of Scala and Hive which requires coding experience using parquet 4. Advanced SQL knowledge requiring writing SQL queries 5. Experience having worked with GCP data lake 6. Quantexa certification required\n\nThanks & Regards,\n\nSatnam Singh\n\nDirect: 201 623 3660\n\nEmail : Satnam.singh@sparinfosys.com"}, {"title": "Remote Work - Need Staff Data Engineer - USC only", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3746283980/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=CvIuLBNO%2BPMGi4IXVRkP0A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3746283980", "description": "About the job\n100% Remote, client is in NY\n\nLook for strong job history, Python, Machine Learning, SQL, 10/10 comms, etc. \n\nMUST be an Engineer \n\nSummary\n\nWe're looking for an experienced Data Engineer to join our client's growing team of top notched IT professionals. You will work closely across multiple groups including Editorial and Audio, Marketing, Advertising, etc. to achieve their goals and objectives. The Data Engineering team builds tools throughout the stack while sharing knowledge across these departments.\n\nAs the company grows, they're looking for Data Engineer who will help solidify and expand our pipelines and maintain their data warehouse. Their business model is based on performing complex and very detailed analyses of how our products perform with their subscribers. You'll be responsible for working alongside analysts, product, marketing, and other teams to define new data collection components and measurement schemes that support each new product and feature.\n\nHow Can Add Value To Their Mission\n\nCreate and maintain data pipelines to provide insights and drive business decisions\nEstablish data warehousing strategy (ex. Kimball, Data Vault, etc.)\nMaintain data infrastructure on our AWS accounts\nCollaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.\nWrite unit/integration tests, contributes to engineering wiki, and documents work\nImplement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it\n\nWhat You'll Need To Succeed\n\n7+ years of industry experience measuring product performance and user behavior\nExperience working with a variety of data management technologies, including RedShift, Kinesis/Kafka, Glue, Spark, Postgres, Airflow, dbt, and others.\nExperience implementing BI reporting tools such as Looker\nExperience interfacing with engineers, product managers and analysts to understand data needs\nKnowledge of a variety of measurement beacons, SDKs, APIs including Google Analytics, Amplitude, Braze, Stripe, email service providers\nA commitment to building secure, resilient, fault-tolerant architectures, with clear documentation and procedures in place for support\nA focus on accuracy and detail as you build, ensuring data pipelines produce clear, predictable results\nUnderstanding of the typical metrics a subscription and advertising-supported business needs to measure success\nExperience with Client techniques as applied to behavioral segmentation or anomaly identification is a definite plus\nFamiliarity using developer tools that increase productivity and facilitate the development of resilient code (eg. Docker, CircleCI, Serverless) is a plus"}, {"title": "ENTRY LEVEL DATA ENGINEER (REMOTE)", "company_name": "SynergisticIT", "location": "Jacksonville, FL (Remote)", "link": "/jobs/view/3767587909/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=om7L3CnowiC4PLIcpZD70A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3767587909", "description": "About the job\nSYNERGISTICIT wants every candidate to know that the Job Market is Challenging and to stand out, you need to have exceptional skills and technologies and that's where we come in to make sure you get the attention which you need\n\nSynergisticIT understands the complex nature of the job market and how difficult it can be to secure a position, especially for fresh graduates. Therefore, we assist and help tech-savvies to convert their passions into professions. We go above and beyond to keep you working in your niche.\n\nAs we focus on long-term success, we provide complete career development solutions. From job search to upskilling portfolio and interview preparation, we can guide you at every step of your career.\n\nSynergisticIT spares no efforts to connect you with a large network of tech giants, including Google, Apple, PayPal, Dell, Cisco, Client, etc. Presently, we are actively looking for ENTRY LEVEL DATA ENGINEER a driven mindset. Get the right opportunity and gain experience in building web-centric solutions on Java.\n\nWho Should Apply : Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or anyone looking to make their career in IT Industry\n\nWe also assist in filing for STEM extension and H1b and Green card filing.\n\nCandidates who are serious about their future in the IT Industry and have set big goals for themselves.\n\nCandidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. We also offer Skill Enhancement Programs if the candidates are missing skills or experience which our clients need with great outcomes\n\nCandidates can benefit from skill enhancement if they fall into the below categories. If they are qualified with enough skills then no need for skill enhancement\n\nCandidates Who Lack Experience\n\nHave had a break in careers\n\nLack Technical Competency\n\ncandidates who want to get employed and make a career in the Tech Industry \n\nPlease Also Check The Below Links\n\nhttps://www.synergisticit.com/candidate-outcomes/\n\nhttps://www.synergisticit.com/java-track/\n\nhttps://www.synergisticit.com/data-science-track/\n\nhttps://www.synergisticit.com/which-is-the-best-option-for-tech-job-seekers-staffing-companies-consulting-companies-bootcamps-or-synergisticit/\n\nhttps://www.synergisticit.com/contact-us/\n\nIf the skills are not a match candidates can opt for Skill enhancement. Or their resume can be sent out to clients to see if responses are achievable\n\nREQUIRED SKILLS For Java/Software Programmers\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Core Java , javascript , C++ or software programming\nSpring boot, Microservices and REST API's experience\nExcellent written and verbal communication skills\n\nFor data Science/Machine learning\n\nRequired Skills\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Statistics, Python, data visualization tools\nExcellent written and verbal communication skills\n\nPreferred skills: NLP, Text mining, Tableau, Time series analysis\n\nTechnical skills are required by clients for selection even if its Junior or entry level position each additional Technical skill helps a candidate's resume to be picked by clients over other job seekers.\n\nClients hire candidates with the right technical skills which they need and reject candidates who lack the required technical skills.\n\nNo third party candidates or c2c candidates\n\nPlease apply to the posting\n\nNo phone calls please. Shortlisted candidates would be reached out"}, {"title": "Data Engineer - Remote | WFH", "company_name": "Get It Recruit - Information Technology", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3782241984/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=BgBf1kSAAX%2BSuw6inJsIVw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3782241984", "description": "About the job\nJoin an innovative team at a leading company in the engineering and manufacturing industry, where we are committed to leveraging data for continuous improvement and operational excellence. This hybrid position allows you to enjoy the flexibility of remote work on Mondays and Fridays.\n\nWhy Join Us\n\nAt our company, we prioritize your well-being and professional growth. We offer a comprehensive benefits package, including excellent health benefits, a 6% 401(k) match, generous paid vacation and sick time, 12 paid holidays, and more.\n\nRole Overview\n\nAs a Data Engineer, you will play a pivotal role in transforming raw data into actionable insights, driving improvements in engineering and manufacturing processes, enhancing product quality, and optimizing operational efficiency. Collaborate with dynamic cross-functional teams to extract, analyze, and visualize data, influencing strategic decisions through your valuable insights.\n\nKey Responsibilities\n\nData Collection and Integration:\n\nGather, clean, and integrate data from diverse sources, including production databases, sensor data, and external datasets.\n\nData Analysis\n\nConduct exploratory data analysis to identify patterns, trends, and anomalies.\n\nPerform statistical analysis and hypothesis testing to derive meaningful insights.\n\nData Visualization\n\nCreate informative and visually appealing dashboards and reports using tools like Tableau, Power BI, or Python libraries.\n\nPredictive Modeling\n\nDevelop predictive models to forecast manufacturing performance, quality, and operational outcomes.\n\nRoot Cause Analysis\n\nInvestigate production issues and quality concerns by analyzing data to identify root causes and recommend corrective actions.\n\nPerformance Monitoring\n\nEstablish and maintain key performance indicators (KPIs) to track manufacturing and engineering metrics.\n\nGenerate regular reports to communicate performance trends to stakeholders.\n\nCross-functional Collaboration\n\nCollaborate with engineering, manufacturing, and quality assurance teams to understand their data needs and provide analytical support.\n\nContinuous Improvement\n\nSuggest process improvements and optimizations based on data analysis.\n\nData Security And Compliance\n\nEnsure data security and compliance with relevant regulations, such as GDPR or industry-specific standards.\n\nInfrastructure And Development\n\nBuild and maintain the infrastructure for collecting, storing, and processing data.\n\nDesign, develop, and manage data pipelines, ETL processes, and data warehouses.\n\nRequired Experience\n\nProven experience as a Sr. Data Analyst and/or Data Engineer in an engineering or manufacturing environment.\n\nProficiency in data analysis tools and programming languages, such as Python, R, SQL, and Excel.\n\nStrong data visualization skills with tools like Tableau or Power BI.\n\nExcellent knowledge of statistical analysis and machine learning techniques.\n\nAbility to communicate complex findings and insights effectively to both technical and non-technical stakeholders.\n\nStrong problem-solving skills and attention to detail.\n\nFamiliarity with manufacturing processes and quality control is a plus.\n\nExperience with enterprise ERPs (SAP S4HANA, Oracle EBS), CRMs (Salesforce, Hubspot), and IoT.\n\nProficient in working with cloud computing platforms including AWS, GCP, and Azure.\n\nExpertise in configuring and optimizing cloud-based data solutions for scalability, reliability, and performance.\n\nBachelor's Degree in Data Science, Statistics, Engineering, or Computer Science Required.\n\nApply Now and be part of a team shaping the future of data-driven innovation in engineering and manufacturing!\n\nEmployment Type: Full-Time"}, {"title": "Entry Level Software Engineer - Data Backend Engineer (Remote - Canada)", "company_name": "Yelp", "location": "United States (Remote)", "link": "/jobs/view/3752881757/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fankeoxifvhpTzOYvowHYQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3752881757", "description": "About the job\nYelp engineering culture is driven by our values: we\u2019re a cooperative team that values individual authenticity and encourages creative solutions to problems. All new engineers deploy working code their first week, and we strive to broaden individual impact with support from managers, mentors, and teams. At the end of the day, we\u2019re all about helping our users, growing as engineers, and having fun in a collaborative environment.\n Interested in exploring data and finding innovative ways to collect it, curate it, and make it easily accessible for others to use? How about doing that at an enormous scale? Yelp\u2019s datasets contain billions of interactions between users and local businesses around the globe. If working with our vast amount of data sounds exciting, come and join us!\n\nThe Marketplace Data Observability team is responsible for managing, governing, and monitoring all of Yelp\u2019s Ads data to protect our revenue stream and also support offline analysis that aids in making long term company decisions. We build tools that help us to better understand our data quality, and we strive to uncover data issues before they impact our consumers.\n\nAs a Data Backend Engineer on this team, you will be responsible for building elegant and scalable data products that serve critical, up-to-date, structured information to support different types of analytics within Yelp. As a core contributor to our growing data modeling and data warehousing engineering efforts, you will help design and own mission-critical data flow pipelines and datastores to enable decisions including effective A/B testing and company investments.\n This opportunity is fully remote and does not require you to be located in any particular area in Canada. We welcome applicants from throughout Canada. We\u2019d love to have you apply, even if you don\u2019t feel you meet every single requirement in this posting. At Yelp, we\u2019re looking for great people, not just those who simply check off all the boxes.\n  Build systems that can effectively store and crunch terabytes of data.\nDesign and develop data models for efficient data storage, retrieval, and reporting.\nCreate and maintain conceptual, logical, and physical data models using industry-standard modeling tools.\nCollaborate with cross-functional teams, including engineers, data analysts, business analysts, and data scientists, to understand data requirements and translate them into effective data models.\nParticipate in data integration efforts, including ETL processes and data migration.\nStay up-to-date with industry best practices, emerging technologies, and trends related to data warehousing.\nSupport on-call rotations as needed to operate the team.\n  Understanding of high performing and scalable data systems.\nExperience in building and orchestrating ETL pipelines.\nExperience with Data Lake or Data Warehouse landscape.\nA hunger for tracking down root causes and fixing them in systematic ways.\nAbility to communicate effectively to technical and non-technical cohorts alike. \nExposure to some of the following technologies: Python, AWS Redshift, AWS Athena / Apache Presto, Big Data technologies (e.g S3, Hadoop, Hive, Spark, Flink, Kafka etc), NoSQL systems like Cassandra, DBT."}, {"title": "Data Engineer (Associate Consultant), June 2024", "company_name": "UDig", "location": "Nashville, TN (Remote)", "link": "/jobs/view/3716039960/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=XUjo40kU55HnDCqJ%2F3FdPg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3716039960", "description": "About the job\nCan UDig It? \n\nUDig designs, builds, and implements technology solutions that deliver on business objectives. People join our team because they are excited to have an impact on the future of our organization, passionate about delivering meaningful results, and energized by solving complex problems. Oh, and they like working with other driven, smart, and authentic people!\n\nUDig is proud to be recognized as a \"Best Places to Work\" in Virginia and Tennessee. We're committed to fostering a transparent environment where leadership is accessible, professional development opportunities are abundant, and you are empowered to achieve your career goals. You'll collaborate, develop deep relationships with teammates, and accelerate your success while having fun!\n\nIf you're graduating in May 2024 and excited by the idea of impacting organizations by building data solutions to achieve business goals, you should consider joining our growing technology team.\n\nHow Things Go Down\n\nAs a new, entry-level teammate, you'll take part in our \"Breaking Ground\" training program designed to help you quickly get up-to-speed on UDig's delivery approach within a project team. The program covers:\n\nUDig delivery process and methodologies such as gathering requirements and delivering projects using agile\nTechnical guidance and instruction on the best practices from Data leaders at UDig\nDelivering executive and technical presentations\nCoaching to prepare you to for success on your first client project at UDig\n\nA typical day might entail:\n\nProject standups and check-ins\nDesign and development of data pipelines, BI reports, and more\nPlanning and participating in project demos\nContributing to internal UDig projects\nCareer and job growth through UDig-provided training\nCreating long-lasting friendships with teammates through fun activities as well as your projects (including social events, winning a prize for guessing the weight of pumpkin, or sharing experience on cool tech side projects)\n\nUDig believes it is important for Associate Consultants going through our Breaking Ground program to reside in one of our major markets (Richmond, VA or Nashville, TN metro area) for at least the first year of employment.\n\nWant to learn more? Hear directly from our teammates by checking out our 'A Day in the Life' blog series: https://www.udig.com/digging-in/breaking-ground-2023/\n\nSound Interesting? Here's the Technical Piece\n\nBelow are a few boxes you should be able to check if\u202fyou're interested: Don't check every box? Go ahead and apply!\n\n1+ year of experience with relational database management systems such as Microsoft SQL Server, MySQL, or Oracle\nUnderstanding of the full lifecycle of design, development, and implementation of data solutions including architecture design, development, testing\nSome experience in ETL development or reporting is a plus\nAbility to present technical ideas and high-level concepts and solutions to internal and external team members with varying degrees of technical knowledge\nBachelor's Degree in in computer science, information technology, or any science or business discipline\nAbility to take your work seriously but also seriously enjoy a good time with your UDig colleagues\n\nWhat's in It For You? \n\nAt UDig, you will build your career alongside talented and experienced developers while gaining experience and having fun! We provide continuous challenges and professional development opportunities. Additionally, we offer: \n\nCompetitive salary, merit reviews, and career advancement paths\nFlexible, hybrid environment\nIndividual $1500 Training Budget\nRegular team building and social activities (virtual and/or in-person)\nTransparent culture with strong communication and access to leadership\nGreat benefits like\nGenerous Paid Time Off, company holidays, and parental leave\nMultiple Single and Family Health Insurance plans to choose from\nDental & Vision coverage\nShort-Term and Long-Term disability\nOptional accident and critical illness coverage\nMatching 401(k)\nand more!"}, {"title": "Expression of Interest: Data Engineer", "company_name": "Fingerprint for Success (F4S)", "location": "New York, NY (Remote)", "link": "/jobs/view/3787769880/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=l%2FEDpQqUNmCttGdIyTk7Cg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787769880", "description": "About the job\nWe are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.\n\nThe F4S Talent Pool is a pilot project designed to:\n\nHelp job seekers get discovered by our partners based on their anticipated hiring needs.\nProvide optional support and resources for job seekers in their career endeavors.\nHelp individuals understand, and bring out the best in themselves and each other.\n\nThe F4S Talent Pool process:\n\nOnce you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.\n\nAbout Fingerprint For Success (F4S)\n\nBacked by 20+ years of research, F4S\u2019s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high performing teams.\n\nKeep in mind, joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your F4S work style assessment to match the hiring needs of our partners.\n\nYour feedback is a gift! Write to us via:\n\nPowered by JazzHR\n\noWuPJ9lwq9"}, {"title": "Data Engineer Seattle, WA (remote)", "company_name": "Conch Technologies, Inc", "location": "United States (Remote)", "link": "/jobs/view/3765052211/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=SBETI78UMutCHuRp5FnP%2FA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3765052211", "description": "About the job\nHi,\n\nGreetings from Conch Technologies Inc\n\nData Engineering Lead \u2013 With Databricks and Unity Catalog\n\nLocation \u2013 Seattle, WA (remote)\n\nDuration \u2013 12 months\n\nNeed: Design and implement data architecture and strategy, leveraging technologies such as Databricks, Unity Catalog, Privacera, and Collibra.\n\nAs a Data Engineering Lead, you will play a pivotal role in defining and executing technical deliverables while assessing complexities and levels of effort. You'll engage with both vendor and internal teams to resolve technical challenges and ensure seamless collaboration. Your leadership will extend to planning and prioritization with the SCRUM master, facilitating effective coordination between onsite and offshore teams, and driving feature prioritization through collaboration with product managers.\n\nKey Responsibilities\n\n Lead the definition and breakdown of technical deliverables, assessing complexities and levels of effort.\n Collaborate with vendor and internal teams to address technical issues.\n Drive planning and prioritization in conjunction with the SCRUM master.\n Coordinate with onsite and offshore teams for timely project delivery.\n Collaborate with product managers to prioritize features, communicate with stakeholders, and clarify requirements.\n Collect and manage user-reported issues, ensuring prompt resolution and communication.\n Define comprehensive test plans, oversee their execution, and document results.\n\n--\n\nWith Regards,\n\nNagesh G\n\nMobile: 408-381-5645\n\nDesk: 901-313-3066\n\nEmail: nagesh@conchtech.com \n\nWeb: www.conchtech.com"}, {"title": "Remote Work - Need Azure Fabric Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3759325599/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=C3iVjmcR9wjUip1nNjUiCg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3759325599", "description": "About the job\nThe client is placing heavy emphasis on Microsoft and Azure Fabric experience\n\nInformation Management Engineer \u2013 Take the identified information management sources and assist migration to the Client Data Lake, Azure Fabric. The engineer will transform information management (de-dup, clean and enrich).\n\nTake the identified data sources and bring them into our Data Lake, Azure Fabric. Once the data is in, the engineer will need to be able to transform data (de-dup, clean and enrich the data). We are focused on using a medallion data model for data architecture. The tools we are working with are: Delta Parquet files, python, spark, notebooks, T-SQL, Fabric lakehouse/warehouse, Dataflows Gen2, Data Pipelines, Direct Lake, shortcuts, ADF."}, {"title": "Data Engineer", "company_name": "Catalist", "location": "Washington, DC (Remote)", "link": "/jobs/view/3744951279/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ui1Fk6ICmnPNqAlZDZAcwA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3744951279", "description": "About the job\nFor over 17 years, Catalist has been a leader in civic data and data science innovation. Our mission is to provide progressive organizations with the data, software, and services needed to better identify, understand, and communicate with the people they need to engage and mobilize. Our clients include the largest, most influential organizations in the U.S. active in civic engagement, advocacy, and political campaigns.\n\nCatalist is home to a dedicated, creative team of technologists, data scientists, and campaign experts committed to using our talents and technology to nurture a vibrant and growing progressive community.\n\nAs a Data Engineer at Catalist, you will work closely with the Analytics and Technology departments to design, build, support, and maintain various data pipelines and processes with an end goal of providing data and intelligence to the progressive community. This role primarily involves translating data architecture designs into functional processes, code, and systems.\n\nThe ideal candidate will be a highly motivated individual with excellent technical skills, a strong desire to learn new skills, and an interest in progressive politics. Catalist values creativity and problem-solving. Our work is on the cutting edge of data-driven politics, and your support will help Democratic candidates and progressive organizations conduct successful advocacy and electoral campaigns.\n\nThis position reports to the Deputy Chief Data Officer. The Data Engineer is a part of a growing Data team that supports all underlying work at Catalist.\n\nThis position is included in our CWA bargaining unit.\n\nPrinciple Duties & Responsibilities\n\nDevelop scalable, production processes, code, and systems for data ingest, transformation, modeling, and reporting across a variety of platforms\nTranslate mock-ups and designs into functional processes, code, and systems\nCreate architecture designs when needed\nProvide quality assurance and testing on processes, code, systems, and products\nBecome an internal subject matter expert on various datasets and support other Catalist departments/teams on usage of those datasets\nExecute ad-hoc data and database maintenance tasks as requiredProject manage cross-departmental efforts, with direct responsibility for stakeholder engagement, management, and execution of technical elements\n\n\nRequirements\n\nBS or BA in a technical field, or relevant experience\n1-2 years of experience working with SQL databases\nExperience with data cleaning or ETL processes\nExperience with distributed computing systems and/or distributed datastores (particularly the Hadoop ecosystem)\nExperience managing projects\nFamiliarity with Catalist data, progressive politics, voter files, and/or commercial data\nBackground check required\n\n\nPreferred Skills & Abilities\n\nWillingness to be a problem solver and produce results in a fast paced environment\nAbility to be creative and personable, and articulate ideas clearly\nExperience working with SQL databases\nProficiency with Python or another object-oriented programming language (R, Java, Scala, etc\u2026)\nExperience working in cloud environments (AWS, GCP, etc.)\nExperience working in command line environments such as Bash\nExperience with a version control tool such as git or github\n\n\nBenefits\n\nMedical, Dental, Vision, Prescription Drug\n\nCatalist offers Medical, Dental, Vision, and Prescription Drug coverage for eligible staff and their eligible dependents. Catalist\u2019s Medical plan is a comprehensive PPO program including Prescription Drug coverage with 85% of the premium paid by Catalist. Dental and Vision coverage is provided at no cost to employees.\n\nGroup Term Life Insurance and Long-Term & Short-Term Disability Coverage\n\nGroup Term Life Insurance and Long-Term and Short-Term Disability coverage is available for eligible staff. These benefits are provided at no cost to Catalist employees.\n\n401(k) Safe Harbor Plan\n\nA 401(k) Safe Harbor Plan is available to eligible staff with a 3% contribution from Catalist from the date of hire. Employees may contribute pre-tax or post-tax from their salary up to the legal limits set forth by the IRS.\n\nMedical and Dependent Care Flexible Spending Accounts (FSAs)\n\nCatalist offers an FSA Program that gives eligible staff the ability to pay out-of-pocket medical/dental/vision/child care expenses from pre-tax earnings.\n\nTransit Benefits\n\nCatalist also makes available a Transit benefit FSA program to eligible employees using pre-tax contributions with a company match.\n\nProfessional Development and Remote Work Expenses\n\nEligible employees may be reimbursed up to $750 each year for professional development / education and remote work expenses.\n\nStudent Loan PayDown or SaveUp\n\nCatalist offers a Student Loan PayDown and College SaveUp benefit for eligible staff.\n\nVacation, Personal Leave, Sick Leave Benefits\n\nCatalist offers generous vacation benefits to all eligible staff. Eligible employees also receive:\n\n 14 Paid Holidays\n Personal Days\n Sick Leave\n Parental Leave\n\n\nHybrid Office/Remote Work\n\nCertain positions at Catalist are eligible for Office/Remote Hybrid or full Remote status."}, {"title": "Remote Entry Level Data Analyst/Engineer", "company_name": "SynergisticIT", "location": "White Plains, NY (Remote)", "link": "/jobs/view/3774171423/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=q54sk1%2FRww0z%2BC%2BE8e0jQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774171423", "description": "About the job\nAt SynergisticIT, we're all about making connections. Whatever IT goals you have, our software programmers can help achieve those. Our Software development teams can take up turnkey projects and execute them in an effective and efficient manner. If you are looking to source talent our recruiters will find the ideal IT talent for your company. What's the secret to our success? Well, it all starts with taking quality time to listen to each client's specific needs. After we have a thorough grasp of your IT goals, we can better customize our Developments as per your specific needs. We can also tailor make recruiting programs to exceed your expectations. Since our founding in 2010, SynergisticIT's strategies have earned the company an enviable position in the software development, IT staffing and IT skill enhancement fields. SynergisticIT continues to work with hundreds of satisfied American clients with our software programmers working on our projects and after gaining hands on experience on cutting edge technologies moving to contribute their skills to great clients like Apple, Google, Client, Ebay, Paypal, Kroger, the Walt Disney Company and hundreds more. If you are tired of working with inefficient programmers who take a lot of time to ramp up we want you to try us. Our software programmers can hit the ground running and get you the maximum return on your investment. You have already tried the rest its time you tried the best. SynergisticIT - Home of the Best Data Scientists and Software Programmers in the Bay Area.\n\n Why Us ? \n\nSynergisticIT has a proven track record of successfully skill enhancement and staffing IT employees for some of the world's most iconic brands. Our team takes the time to fully understand every client's needs so we could best meet your IT staffing requirements. The knowledgeable staff at SynergisticIT is always more than happy to work with clients to ensure they reach their software development goals. Besides staffing, SynergisticIT is also committed to helping young IT professionals advance their career with a robust upskill program . Everyone who goes through SynergisticIT's program learns all the skills necessary to succeed in many IT fields ranging from Java to Machine Learning. Additionally, everyone trained at SynergisticIT has been through extensive mock and technical interview screenings to bolster their career prospects. Last, but certainly not least, SynergisticIT takes great care to respect the privacy considerations for every client. All companies who work with SynergisticIT can rest assured their confidential data is protected using the most up-to-date encryption technologies. SynergisticIT also complies with all the latest NDA agreements.\n\n REQUIRED SKILLS For Java /Software Programmers \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Time series analysis \n\n We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2022 and at Gartner Data Analytics Summit (Florida)-2023 \n\nOracle CloudWorld Event (OCW) Las Vegas 2022 | SynergisticIT - YouTube\n\n https://www.youtube.com/watch?v=OAFOhcGy9Z8 \n\n https://www.youtube.com/watch?v=EmO7NrWHkLM \n\n https://www.youtube.com/watch?v=NVBU9RYZ6UI \n\n https://www.youtube.com/watch?v=Yy74yvjatVg \n\nSynergisticIT at Gartner Data and Analytics Summit 2023 - YouTube\n\nFor preparing for interviews please visit  https://www.synergisticit.com/interview-questions/ \n\n We are looking for the right matching candidates for our clients \n\n Please apply via the job posting \n\n REQUIRED SKILLS For Java /Full Stack/Software Programmer \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning Positions \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Tensorflow \n\n If you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team. \n\n No phone calls please.  Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates"}, {"title": "Data Engineer", "company_name": "Anika Systems", "location": "Leesburg, VA (Remote)", "link": "/jobs/view/3787766893/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=mApL9JnN%2B%2Fr2%2BYUPlxRYOA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787766893", "description": "About the job\nAnika Systems is a fast growing, woman-owned small business that specializes in providing innovative IT solutions for federal government agencies. Our expertise lies in accelerating delivery in Data and Analytics, Intelligent Automation, Application Development, and IT Modernization. We are currently expanding our Federal team and are seeking a passionate and talented Data Engineer. This opportunity is 100% remote.\n\nMust be a U.S. Citizen with the ability to obtain and maintain a government suitability clearance\n\nResponsibilities\n\nDesigns, develops, builds, analyzes, evaluates, and installs database management systems to include database modeling and design, relational database architecture, metadata, and repository creation and configuration management.\nDefines and oversees database organizations, standards, controls, procedures, and documentation. Provides technical consulting in the definition, design, and creation of a database environment.\nAdvises applications development staff and users on data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nDesigns and implements databases with respect to access methods, access time, batch processes, device allocation, validation checks, organization, protection and security, documentation, and statistical methods.\nUses data mapping, data mining, and data transformational analysis tools to design and develop databases.\nDetermines data storage and optimum storage requirements.\nPrepares system requirements, source analysis, and process analyses and designs throughout the database implementation.\n\nRequired Skills And Experience\n\nBA/BS and 1 year of relevant experience\nExperience with DataBricks, SQL, Python\nApplications development experience with data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nExcellent communication skills (both oral and written) with process-oriented organizational skills to ensure project success\n\nDesired Skills And Experience\n\nDemonstrated successful development experience with the full life cycle of government database management systems including database modeling and design, relational database architecture, metadata and repository creation, and configuration management.\n\nPowered by JazzHR\n\nTlKWIorucJ"}, {"title": "Remote Work - Need Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3774712978/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=r3Bp%2F0qcOD2OpC5jpbUOcA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774712978", "description": "About the job\nTitle : Data Engineer\n\nLocation : Remote\n\nDuration : 6+ months to start (long term contract (work order are extended 2x year))\n\nPlease Fill out the skill matrix: Technology Some Awareness (1) Novice (2) Intermediate (3) Advanced (4) Expert (5) SQL Tableau Desktop /Creator Tableau API Integration AWS - S3 Glue AWS - Athena Redshift AWS - Lambda AWS - SNS/SQS msExcel\n\nProfile: Data Analyst will work closely with Architects and leads in the Interflow team, key business stakeholders including program staff to support the organizations effort in the areas of data Analysis and information management. Role should have strong experience in writing SQL queries to extract data for reports. Candidate should have experience working with large set of data. As these reports are very critical for the company and we are looking for an expert who is capable of handling, consuming and extracting large sets of data for analysis and reports.\n\nMust Haves\n\nAWS - Athena\nAWS - S3\nGlue\nhands-on development of reporting applications\nMS Excel\nRedShift\nSQL\nTableau Desktop /Creator\n\nNice\n\nAWS - Lambda\nAWS - SNS/SQS\nTableau API Integration\n\nSkills\n\nBA/BS required (major in an analytical field desired) \nA minimum 10+ years work-related experience. \nExperience in hands-on development of reporting applications for the Web in a professional environment \nThe Ethos of continuous improvement and interest in learning new things.\nStrong analytical thinking and structured problem-solving ability\nAbility to handle multiple projects and assignments simultaneously and effectively in a cross-functional team environment. \nVersed on the agile methodology and best practices. \nExcellent interpersonal and collaboration skill with the ability to work with a diverse set of colleagues, across functions, from different organizations, disciplines, etc. \nSelf-starter, ability to set priorities, work independently and attain goals"}, {"title": "Data Engineer", "company_name": "Team Remotely Inc", "location": "Houston, TX (Remote)", "link": "/jobs/view/3786249829/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=78lY5Jo%2FfyRPC6TQYCdvww%3D%3D&trk=flagship3_search_srp_jobs", "id": "3786249829", "description": "About the job\nThis is a remote position.\n\n Data Engineer (US/Canada Residents Only, 1 year experience, remote)\n\nTeam Remotely Inc. is a staffing and recruitment agency that offers a comprehensive solution for talent acquisition, including sourcing, vetting, pay rolling, and managing talent. Whether you need contract staffing, direct hire, direct sourcing, talent pools, or diversity initiatives, our model can support your hiring strategy.\n\n Hiring Type: Full-Time\n\n Base Salary: $55K-$70K Per Annum.\n\nResponsibilities\n\nDesigning user interface changes for web-based DB applications. \nReviewing application requirements and interface designs. \nDeveloping and implementing highly responsive user interface components using react concepts. \nWriting application interface codes using JavaScript following react.js workflows. \nTroubleshooting interface software and debugging application codes. \nDeveloping and implementing front-end architecture to support user interface concepts. \nMonitoring and improving front-end performance. \n\nQualifications And Experience\n\nBachelor\u2019s degree in computer science, information technology, or a similar field. \nPrevious experience working as a react.js developer. \nIn-depth knowledge of JavaScript, CSS, HTML, jQuery, and front-end languages. \nKnowledge of REACT tools including React.js, Webpack, Enzyme, Redux, and Flux. \nExperience with user interface design. \nKnowledge of performance testing frameworks, including Mocha and Jest. \nExperience with browser-based debugging and performance testing software. \nTroubleshooting skills. \nProject management skills. \nProblem-solving skills. \nVerbal communication skills. \n\n Why work with Team Remotely?\n\nTeam Remotely Inc. is a staffing platform offering a seamless experience for employers and candidates. Employers can post job openings and specify their requirements, while candidates can create profiles and upload resumes.\n\nThe team of Team Remotely continuously learns and adapts based on previous successful placements, constantly improving its matching capabilities. This ensures that the recommendations provided by Team Remotely are tailored and accurate, increasing the likelihood of a successful match between employers and candidates. By providing intelligent and data-driven solutions, they strive to enhance the efficiency and effectiveness of the hiring process, ultimately helping companies find the best talent and individuals find their dream jobs."}, {"title": "Data Engineer", "company_name": "ZAG Technical Services", "location": "United States (Remote)", "link": "/jobs/view/3771403464/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=Hf0VefrKXBFIL5MtHmY0Pw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3771403464", "description": "About the job\nWe are looking for a tech savvy Data Engineer to contribute to the building, operating, and scaling of next generation data platforms and tools that will power data-driven analytics capabilities throughout the ZAG products and services portfolio, spanning areas such as business intelligence, reporting, and data analytics.\n\nThe ideal team member is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. Must be comfortable supporting the data needs of multiple clients and teams, systems, and products within the modernization of ecosystems. The right-fit person will be excited by the prospect of optimizing or even re-designing data processes to support next generation of advanced analytics capabilities, which in-turn translates to implementing strategic enhancements to maintain competitive advantage in the industry.\n\nThe successful team member will leverage their proficiency to\u2026\n\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\nBuild the analytics processes required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Data Management and cloud based \u2018big data\u2019 technologies.\nBuild analytics tools that utilize technologies to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.\nCollaborate with client teams to identify and solution regional and local requirements within the context of the standard global business model for enterprise analytics.\nParticipates in meetings, discussions, strategy sessions where changes, improvements, and enhancements are proposed, evaluated, and approved. \n\n\nThe exceptional team member will have or demonstrate progressive experiences in\u2026 \n\nBachelor\u2019s degree or higher in Computer Science or related field\nExperience working with BI and data warehousing tools\nExperience working with enterprise data, where security is paramount and data governance is critical\nAbility to work independently and in a team environment, and effectively engage all levels of the organization\nUnderstanding of distributed systems driving large-scale data processing and analytics with a successful history of manipulating, processing, and extracting value from large, disconnected datasets.\nAbility to communicate effectively (listening, presenting, and questioning)\nStrong organizational, written, and communication skills\n\n\nAbout ZAG\n\nZAG Technical Services empowers our clients\u2019 success through intelligent business solutions that increase employee productivity, network security and team collaboration. We aim to ensure that our client\u2019s IT investments drive ROI, provide a competitive advantage and help businesses grow. We do this by living our core set of values: Integrity Foremost, Client-Centered, Accountable Always, Teamwork Throughout, and Exceptional is the Goal.\n\nWhat ZAG Has To Offer\n\nCompetitive compensation package \nBenefits including medical, dental, vision, 401K and life insurance \n\n\nZAG Technical Services is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all of our employees.\n\nZAG Technical Services is an E-Verify participant and will provide the Social Security Administration (SSA) and, if necessary, the Department of Homeland Security (DHS), with information from each new employee\u2019s Form I-9 to confirm work authorization."}, {"title": "Junior Data Engineer", "company_name": "HireMeFast LLC - Career Accelerator - Land A Job", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3786249797/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D&trk=flagship3_search_srp_jobs&lipi=urn%3Ali%3Apage%3Ad_flagship3_search_srp_jobs%3BQExogrHBQoSF9zZm%2B9kufw%3D%3D&lici=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D", "id": "3786249797", "description": "About the job\nThis is a remote position.\n\n Job Title: Junior Data Engineer\n\n Employment Type: Full-Time\n\n Salary: $64,000 - $76,000 per annum\n\n Experience Required:  Minimum 1 year of project experience\n\nAbout us: HireMeFast is a leading staffing and recruitment agency specializing in connecting businesses with top-tier talent across various industries. Our mission is to bridge the gap between exceptional candidates and organizations needing their skills, expertise, and unique qualities. Our team of experienced and dedicated recruitment specialists utilizes innovative sourcing strategies, and a vast network to identify and attract top talent. We conduct comprehensive procedures to ensure that only the most qualified candidates are presented to our clients.\n\nPosition Summary\n\nJoin the fast-paced, innovative, and collaborative environment focused on providing an AIOps platform that enhances the intelligence of the CVS Health infrastructure. Work closely with subject matter experts and colleagues to build and scale out machine learning and AI solutions that will detect, predict, and recommend solutions to correct issues before system impact and enhance efficiency, reliability, and performance of CVS Health\u2019s IT operations.\n\nKey Responsibilities include:\n\nData pipeline development: Designed, implemented, and managed data pipelines for extracting, transforming, and loading data from various sources into data lakes for processing, analytics, and correlation. \nData modeling: Create and maintain data models ensuring data quality, scalability, and efficiency \nDevelop and automate processes to clean, transform, and prepare data for analytics, ensuring data accuracy and consistency \nData Integration: Integrate data from disparate sources, both structured and unstructured to provide a unified view of key infrastructure platform and application data \nUtilize big data technologies such as Kafka to process and analyze large volumes of data efficiently \nImplement data security measures to protect sensitive information and ensure compliance with data and privacy regulation \nCreate/maintain documentation for data processes, data flows, and system configurations \nPerformance Optimization- Monitor and optimize data pipelines and systems for performance, scalability and cost-effectiveness \n\nCharacteristics of this role:\n\nTeam Player: Willing to teach, share knowledge, and work with others to make the team successful. \nCommunication: Exceptional verbal, written, organizational, presentation, and communication skills. \nCreativity: Ability to take written and verbal requirements and come up with other innovative ideas. \nAttention to detail: Systematically and accurately research future solutions and current problems. \nStrong work ethic: The innate drive to do work extremely well. \nPassion: A drive to deliver better products and services than expected to customers. \n\nRequired Qualifications\n\n2+ years of programming experience in languages such as Python, Java, SQL \n2+ years of experience with ETL tools and database management (relational, non-relational) \n2+ years of experience in data modeling techniques and tools to design efficient scalable data structures \nSkills in data quality assessment, data cleansing, and data validation \n\nQualifications\n\nKnowledge of big data technologies and cloud platforms \nExperience with technologies like PySpark, Databricks, and Azure Synapse. \n\nEducation\n\nBachelor\u2019s degree in Computer Science, Information Technology, or related field, or equivalent working experience\n\nWhy HireMeFast LLC?\n\nAt HireMeFast, we understand that finding the right individuals to join your team is crucial for success and growth of your organization. We are committed to streamlining the hiring process for our clients, ensuring they have access to a diverse pool of highly qualified candidates who are a perfect fit for their specific needs."}, {"title": "Data Engineer  - Need candidates from Texas ( REMOTE )", "company_name": "Pulivarthi Group (PG)", "location": "United States (Remote)", "link": "/jobs/view/3770685212/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=9gdMwuif%2BpB%2BCQYvQi3imQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3770685212", "description": "About the job\nFollow us on Linkedin: https://www.linkedin.com/company/pulivarthigroup/\n\nPulivarthi Group LLC is a Global Staffing & IT Technology Solutions company, with our prime focus of providing world class solutions to our customers with the right talent. We combine the expertise of our team and the culture of your company to help you with the solution that is affordable and innovative using high quality standards and technologies.\n\nWe\u2019ve served some of the largest healthcare, financial services, and government entities in the U.S.\n\nData Engineer - FT only - Azure SQL with data lakes experience\n\nData Engineer -\n\nthe resource requirement for this Data Engineer role needed more clarification.\n\nOur customer is requesting a resource who knows Azure CI/CD (Continuous Integration / Continuous Deployment) to build and integrate their DevOps pipeline.\n\nOnce the DevOps pipelines are mapped, they need the Data Engineer to connect to all the Data Sources, primarily their Data Lake.\n\nThe Customer (based in Houston) is interested in a resource in a time zone not too far removed from the Central Time Zone to allow real time collaboration."}, {"title": "Data Engineer", "company_name": "Otter Products", "location": "Fort Collins, CO (Remote)", "link": "/jobs/view/3766728302/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=77U3U7CAi1PxgDmpI3%2BiNQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3766728302", "description": "About the job\nOverview \n\nOtter Products is currently recruiting for a Data Engineer to join our Data team! You can be based in our Fort Collins, CO office with flexibility to work a portion of your time remotely, or 100% Remote in the U.S.\n\nThe Data Engineer applies professional experience, concepts, and company objectives toward building and operationalizing the minimally inclusive data necessary for the enterprise data initiatives following industry standard practices and tools. The bulk of the Data Engineer's work would be in designing, managing and optimizing data pipelines, then moving these data pipelines effectively into production for key data and analytics consumers like business/data analysts, data scientists or other stakeholder that needs curated data for data and analytics use cases across the enterprise. The Data Engineer also ensures compliance with data governance and data security requirements while creating, improving and operationalizing these integrated and reusable data pipelines to enable faster data access, integrated data reuse and vastly improved time-to-solution for data and analytics initiatives. Additionally, the Data Engineer will be expected to collaborate with other data team members and data consumers working on the models and algorithms developed by them toward optimization for data quality, security, and governance to put them into production leading to potentially large productivity gains.\n\n About Otter Products \n\nOtter Products, we grow to give. From our founder's garage in 1998 to the global technology leader we are today, Otter Products continues to drive growth through innovation.\n\nThrough our industry-leading brands - OtterBox, Liviri and OtterCares - we provide our partners the number-one selling and most trusted products in our categories. Our philanthropic spirit is the foundation on which we foster our partner relationships, allowing us to grow and to give - together.\n\nBy way of our charitable arm, the OtterCares Foundation, we support our communities and invest in our future through education that inspires kids to change the world.\n\nAnd even as our global community of Otters continues to grow, our founder's core values are still at the heart of everything we do. We measure our success by our ability to give back to our communities and strengthen opportunities for all.\n\nFor more information visit otterproducts.com\n\n Responsibilities \n\nComplete analysis, design and development of BI solutions using Azure SQL Server\nFamiliarity with Data Warehouse concepts\nExperience coding the extraction, transformation, and loading (ETL) processes\nDatabase development primarily in SSIS, Data Factory and SQL\nPartner with the business to determine end user database/reporting needs and requirements\nGenerate ad hoc reports using Power BI or SSRS\nCollaborate with other developers to create and implement best approach solution(s)\nTroubleshoot Azure tools, systems, and software\nReview queries for performance issues, making changes as needed\nCollaborate with team to performance-tune Azure application as necessary\nAssist with the analysis and extraction of relevant information from large amounts of historical business data to feed data science initiatives\nParticipate and support the design and documentation of processes for large scale data analyses, model development, model validation and model implementation\nSupport and maintain a positive safety culture by following all safety policies and procedures and actively contributing to a safe working environment\nOther duties as required\n\n Qualifications \n\nBachelor's degree required. Degree in Computer Science or Mathematics preferred\nMinimum of four years of experience in an IT or analytical role required. Experience in database development, report writing and/or statistics preferred.\n\n EEO \n\nOtter Products, LLC is an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, age, marital status, pregnancy, sex, sexual orientation, gender, gender identity or expression, national origin, disability, veteran status, or any other characteristic or status protected by law.\n\n For US Based Roles Only - Compensation Range Minimum \n\nUSD $90,000.00/Yr.\n\n For US Based Roles Only - Compensation Range Maximum \n\nUSD $120,000.00/Yr.\n\n Additional Total Rewards \n\nProfit Sharing Program Eligible, Benefits Eligible - Full Time- check out otterproducts.com/careers/why for more info"}, {"title": "Software Engineer, Distributed Systems", "company_name": "Hightouch", "location": "United States (Remote)", "link": "/jobs/view/3628572539/?eBP=CwEAAAGMd4QO8APODyE_-mMOPhYyQGg5ZjVbPwq_3V7DK0B2bLAf10pcedM9S2T71X5i4TiJuEwOzWBbSiTKbLuk-VD-_Y-dN5uyf8YpYkHYqEYFxySMS9zr2hRjNrxX9gXbxssh35TFK9YTNV2WeH2Ncb675JgFbJTgNWSZySFC6rrdJGmdSDTr2q5d3VoxORz3GjsDbzjeyZQCvQ07RqYw7oujWSgslycs2LMCY_-N0NEj6e0PvbaYKiZ_5IW34bGVSYLXpHwqa8oOcNttYZ90UA3E_7sDSFzABdIVlfrp5zzERrIYy5sh2MIInxHdDoKgQw8VmOn5uM4S0tXUl-i7LnN0_W0NfPBmLiVDNuveIOm4MfDNPxfe5QKPk6FUAVpvgbcoJn9yRyE&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=rph%2FT6Q6Vkp6YoggJf9cJQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3628572539", "description": "About the job\nAbout Hightouch\n\nHightouch\u2019s mission is to empower everyone to take action on their data. Through our Reverse ETL platform, business and data users can seamlessly sync data from where it resides, such as warehouses and databases, to where it is needed, including operational systems and SaaS tools. Traditionally, acting on data has required engineering time and bandwidth, and left most business users stuck with charts and reports that are unable to take automated action on their data. With Hightouch, every business user, without writing any code, can activate data to streamline critical processes, improve marketing performance, and scale operations.\n\nOur team operates with a focus on making a meaningful impact for our customers. We believe in approaching challenges with a first principles thinking mindset, moving quickly and embracing our value of efficient execution, and treating each other with compassion and kindness. We look for team members that are strong communicators, have a growth mindset, and are motivated and persistent in achieving our goals.\n\nHundreds of companies use Hightouch, including Spotify, Ramp, Retool, NBA, Plaid, and Betterment. We\u2019re based in San Francisco, are remote-friendly, and backed by leading investors such as Amplify Partners, ICONIQ Growth, Bain Capital Ventures, Y-Combinator, and Afore Capital.\n\nAbout The Role\n\nWe are looking for a distributed systems engineer to work on the systems that power the syncing engine our customers and other engineering teams rely on. This presents an exciting challenge where you can apply your expertise in distributed systems, performance optimization, and troubleshooting to push the boundaries of what is possible and meaningful improve our ability to move massive amounts of customer data. This role also provides a unique opportunity to work on a multi-cloud and multi-region infrastructure that supports a global customer base.\n\nWe believe in enabling our engineers to do their best work for our customers by giving them extremely high levels of ownership and autonomy. This comes in different forms: you will own and deliver projects from start to finish, you will work directly with customers to solve their hardest scaling problems, and you will have a lot of influence over what we work on as a team and company.\n\nSome of the problems we\u2019ll be working on include:\n\nSync Speed: Customers want to sync a lot of data to important destinations like Facebook and Snapchat, which requires us to analyze every part of our syncing process and find where we can optimize to sync data more quickly\nStreaming Syncing: We currently sync data from sources in batch only, but envision a world where we also do syncs in real-time and support streaming sources like webhooks and queues\nScalability and Reliability: As part of our rapid growth, we\u2019re always evaluating current future bottlenecks in scaling and reliability, and architecting for the next order of magnitude of growth\nPersonalization API: Our Personalization API product provides a low latency caching layer on top of data warehouses that enable customers to use their data warehouse for real-time personalization use cases with \nMulti-Region and Multi-Cloud: Supporting our multi-region and multi-cloud backend, including extending it to launch Hightouch on in new regions to support data residency requirements of our global customer base\n\nWe are looking for talented, intellectually curious, and motivated individuals who are interested in tackling the problems above. This is a senior role, but we focus on impact and potential for growth more than years of experience. The salary range for this position is $170,000 - $240,000 USD per year, which is location independent in accordance with our remote-first policy.\n\nAbout You\n\nYou are an engineer with a passion for solving hard technical problems that generate real value for customers. You\u2019re motivated by high ownership and are comfortable in a fast-paced, startup environment.\n\nYou have experience and comfort with distributed systems and high-scale systems or the ability and desire to build this experience quickly. This experience can take different forms:\n\nYou\u2019ve built syncing engines that process and move terabytes of data per day in a scalable way\nYou\u2019ve created event collection or data streaming services that power company critical systems\nYou\u2019ve owned systems that have undergone orders of magnitude of growth and helped them scale accordingly\nYou have relevant skills and are a fast learner who is excited to take on the challenges we face\n\nInterview Process\n\nOur goal with the interview process is to balance speed with giving both parties opportunities to assess whether there is a strong mutual fit. We will ask you questions, but we want you to ask us questions! Our technical interviews focus on how you design systems because we believe this is the best way for us to see how you work and for you to see how we collaborate. We don\u2019t ask you to write code to solve technical brainteasers that don\u2019t appear in your day to day job.\n\nRecruiter screen [30m]: Introductory call with our recruiting team to get to know each other and see if the role could be a good mutual fit.\nSystems design screen [45m]: Designing a data processing feature end-to-end.\nHiring manager interview [30m]: Chat with hiring manager about past experiences and future operating preferences to assess fit on company values and operating principles.\nSystems design interview [90m]: Work with the interviewer to architect a system at a conceptual level. The problem will be at a pretty high level - and have both product and customer requirements as well as technical."}, {"title": "Entry-Level Data Scientist Engineer", "company_name": "Phoenix Recruitment LLC", "location": "Texas City, TX (Remote)", "link": "/jobs/view/3788727625/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=8iwtytHsCu4wMwJ4fj3pxw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3788727625", "description": "About the job\nThis is a remote position.\n\nTitle - Entry-Level Data Scientist Engineer, 1 year of project experience\n\nEmployment Type: Full-time\n\nBase Salary: $60K-$70K\n\nPhoenix Recruitment offers a variety of recruiting services to assist both employers and employees. They specialize in marketing open positions, recruiting, and helping employers to find qualified candidates across various industries. Phoenix Recruitment has expertise in streamlining the hiring process. They can help ensure that the process is efficient, well organized, and compliant with relevant regulations.\n\nSkills and Abilities:\n\nStrong knowledge of R or Python for data analysis and modeling. \nProficiency in statistical programs such as R, SAS, MATLAB, or Python. \nFamiliarity with spreadsheets (VBA) and database applications (Access, Oracle, SQL, or equivalent technology). \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \nAbility to learn new methods quickly and work under deadlines. \nExcellent teamwork and communication skills. \nStrong analytical and problem-solving abilities. \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \n\nPreferred:\n\nKnowledge of actuarial concepts and life, health, and/or annuity products. \nExperience with statistical modeling techniques such as GLM, Decision Trees, Time Series, Regression, etc. \nFamiliarity with Microsoft DeployR. \nExposure to insurance risk analysis. \nBasic experience in computational finance, econometrics, statistics, and math. \nKnowledge of SQL and VBA. \nFamiliarity with R or Python for predictive modeling \n\nWhy Phoenix Recruitment LLC?\n\nPhoenix Recruitment often has an extensive network of employers and candidates. This network allows them to tap into a pool of qualified candidates and connect them with suitable job opportunities. They can also leverage their connections to help employers find the right talent efficiently. Outsourcing the recruitment process to a specialized agency can save your time and resources, avoid delays, reduce administrative burdens, and increase the chances of finding the right fit for your organization."}, {"title": "Data Engineer - Remote Work", "company_name": "BairesDev", "location": "United States (Remote)", "link": "/jobs/view/3783162297/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=OtoCrAAHZFomVup04fy35g%3D%3D&trk=flagship3_search_srp_jobs", "id": "3783162297", "description": "About the job\nWho We are\n\nBairesDev is proud to be the fastest-growing company in America. With people in five continents and world-class clients, we are only as strong as the multicultural teams at the heart of our business. To consistently deliver the highest quality solutions to our clients, we only hire the Top 1% of the best talents and nurture their professional growth on exciting projects.\n\nWe are looking for a Data Engineer to join our Development team and participate in different projects made up of multicultural teams distributed throughout the world.\n\nWhat You Will Do\n\n Interview other Data Scientists and Data engineers to vet their technical abilities.\n Input data from those interviews into the product and help develop a predictive, fair, and enjoyable solution.\n Work together with developers, tech leads, and solution architects to build applications.\n Improve existing structures by adding new functionalities or proposing technological updates so that as a result, the impact of your contribution is significant in the core of each business.\n Improve the user experience on scalable and high availability platforms, contributing to the key differential of each business.\n\nHere\u2019s What We Are Looking For\n\n 5+ years of experience working as a developer.\n 3+ years of experience working as a Data Scientist / Data Engineer.\n Proficient with analysis, troubleshooting, and problem-solving.\n Hands-on experience with managing data loads and data quality.\n Advanced English level.\n\nHow we do make your work (and your life) easier:\n\n 100% remote work.\n Hardware setup for you to work from home.\n Flexible hours - make your schedule.\n Paid parental leave, vacation & holidays.\n Diverse and multicultural work environment.\n An innovative environment with the structure and resources of a leading multinational.\n Excellent compensation \u2014 well above the market average.\n Here you can grow at the speed of your learning curve.\n\nOur people work remotely but with a consistent and robust culture that promotes diversity and teamwork. To continue being the leading software development company in Latin America, we want to ensure that every BairesDev member gets the best growth and professional development opportunities in a diverse, welcoming, and innovative environment.\n\nEvery BairesDev team member brings something unique to our company.\n\nWe want to hear your story. Apply now!"}, {"title": "Data Engineer (Remote)", "company_name": "Cascade Debt", "location": "United States (Remote)", "link": "/jobs/view/3755336571/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=pmUtn1j2zyPr25CNPh2RNw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3755336571", "description": "About the job\nDo you want to work in a small team where you can make a real difference in a company? Would you embrace the challenge of building a high impact platform that is used globally? Then we want to speak with you!\n\nAbout Cascade\n\nCascade is a fintech startup backed by Canadian and US investors that empowers companies to grow by democratizing access to institutional debt. We are building tools that modernize how companies raise and manage debt, lowering the barriers to entry to the $7 trillion specialty finance and alternative credit market for companies around the world. Founded just last year, we are gearing up for our next phase of technical development and are seeking a talented Data Engineer to lead the way.\n\nAbout The Role\n\nAs a Data Engineer you\u2019ll be an early member of a growing team building a pioneering platform in the debt infrastructure space. You will design, build, and launch efficient, scalable, and reliable data pipelines to move and transform data .\n\nWhat You\u2019ll Do\n\n Building databases, data lakes and data ingestion pipelines to integrate customer databases and datasets to our systems \n Ingesting financial datasets from external customers, then u pdating and maintaining accurate and complete data mappings to ensure that our product s are displaying high quality data \n M onitoring and alerting across our data pipelines in order to make sure that our data ingests are reliable and correct \n Translat ing business goals and stakeholder requirements into new data flows and deliver analytical insights ensuring that the data flows create real business value continuously \n\nSkills And Experience\n\n 2 + years experience in a data engineering or data analyst role \n E xperience with relational SQL and NoSQL databases, including PostgreSQL , MySQL, MariaDB, MongoDB, Bigtable, etc. \n E xperience working with ETL technologies , such as Databricks, Fivetran , or dbt \n Proficiency in writing SQL queries and knowledge of analytical data warehouses such as RedShift, BigQuery , and Snowflake \n Some experience with CI/CD automation such as GitHub Actions \n Developing APIs and integrating with 3rd party APIs \n Bonus: experience in fintech / SaaS / credit analysis \n\nPeople Who Thrive At Cascade Are\n\n Self-starters, who take the initiative to tackle challenges in a remote work environment \n Good communicators, who can operate without ego to discuss, learn, grow, and help others do the same \n Passionate about creating great digital experiences for users \n Problem solvers who are not satisfied with the status quo \n\nBenefits\n\nRemote: we are a remote-first company and are very flexible on hours as long as things get done \nHome-office: there\u2019s a $1000 USD home office allowance to set yourself up \nEquity: we expect you to have an owner-mentality, and have the equity plan to match \nBenefits: health, dental, vision, and more \nPerks: we offer free lunches weekly and off-site trips \nJob satisfaction: we offer autonomy, ample opportunities for mastery, and an opportunity to make a difference for companies around the world"}, {"title": "Data Engineer", "company_name": "ISC (Integrated Specialty Coverages, LLC)", "location": "Carlsbad, CA (Remote)", "link": "/jobs/view/3778293528/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ew7%2FfEuXSujs5IGabdqUyw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3778293528", "description": "About the job\nAbout Integrated Specialty Coverages\n\nIntegrated Specialty Coverages LLC (ISC) is a growth stage technology and data-driven insurance company leading innovation in the market.\n\nBacked by one of the leading private equity firms, KKR, and led by a forward-thinking management team, ISC is combining the worlds of insurance and technology to create an Insurtech powerhouse. As a leading online distributor of insurance products for a range of industries and \"Main Street USA\", we are looking for the right people to help us in our mission of achieving exponential growth. We strive to be the number one place to go for brokers and agents to source insurance. To accomplish this, we're building a digitally focused team that deeply understands the intersection between user experience, data, and AI/ML to optimize the way we engage with our customers and partners.\n\nJob Summary\n\nThe Data Engineer role will be the technical liaison between multiple groups including a data science team, the engineering team, product management, and business stakeholders. You do not need any insurance knowledge prior, however, you must quickly dive deep into the insurance world and ask questions to become a subject matter expert. You will be responsible for building a data platform to facilitate the data science team. You must be a self-starter that can build out features such as a data pipeline from scratch. There will be support from both engineering and data science for any buildout.\n\nJob Responsibilities\n\nCreate and maintain optimal data pipeline architecture. \nAssemble large, complex data sets that meet functional / non-functional business requirements. \nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. \nBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using AWS technologies, SQL, and Python. \nWork with stakeholders including the Executive, Product, Data Science, and Engineering teams to assist with data-related technical issues and support their data infrastructure needs. \nWork with data science and analytics teams to strive for greater functionality in our data systems. \n\nMinimum Qualifications:\n\nAdvanced working SQL knowledge and experience working with relational databases, strong query authoring (SQL) as well as working familiarity with a variety of databases (Snowflake, Redshift, MySQL, MSSQL, etc.)\nExperience building and optimizing data pipelines, architecture and data sets. \nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. \nBuild processes supporting data transformation, data structures, metadata, dependency, and workload management. \nA successful history of transforming, processing and extracting value from large disconnected, datasets from a variety of data sources (Flat files, Excel, databases, etc.)\nStrong analytic skills related to working with unstructured datasets. \nStrong project management and organizational skills. \nExperience supporting and working with cross-functional teams in a dynamic environment. \nAbility to mentor/guide/collaborate with other team members. \n\nWe are looking for a candidate with 3-5 years of experience in a Data Engineer role. They should also have experience using the following software/tools:\n\nExperience with relational and MPP databases, including Snowflake and MySQL. \nExperience developing software in an agile environment from the requirements stage to production. \nExperience with version control: git\nExperience with container technologies: Docker\nExperience with data pipeline and workflow management tools: Airflow, Jenkins, AWS Glue, Azkaban, Luigi, etc. \nExperience with AWS cloud services: EC2, ECS, Batch, S3, EMR, RDS, Redshift\nExperience with other cloud services: Snowflake, Airflow\nExperience with object-oriented/object function scripting languages: Python, Java, C++, etc. \nExperience with data modeling and data warehouse design\nExperience with data visualization tools (PowerBI, QuickSight)\n\nThe starting annual pay scale for this position is listed below. Actual starting pay will be based on factors such as skills, qualifications, training, and experience. In addition, the company offers comprehensive benefits including medical, dental and vision insurance, 401(k) plan with match, paid time off, and other benefits.\n\nISC's salary ranges are determined by role and level. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations and could be higher or lower based on a multitude of factors, including job-related skills, experience, and relevant education or training.\n\nNational Pay Range\n\n$100,000\u2014$110,000 USD\n\nBenefits of Working at ISC\n\nCompetitive vacation and flexible working arrangements\nComprehensive and inclusive health benefits\nA variety of professional development and mentorship opportunities\nChoice of technology whether at home or in the office\n\nISC's Ownership Behaviors\n\nDo the Right Thing *Be Relentless. Pursue Excellence. *Personal Responsibility & Own Outcomes\nTry Fast. Learn Fast. Fail Fast. Think Big. *Build Extraordinary Owners & Join Forces *Grit & Determination\n\nApplicants with disabilities may contact ISC HR department via e-mail, and other means to request and arrange accommodation. If you need assistance to accommodate a disability, you may request accommodation at any time. Please contact ISC at HR@ISCMGA.com.\n\nISC believes in creating long-term relationships by being responsive and relevant and by consistently delivering value to our community of customers. Specifically, with our employees, we focus on attracting, developing, and retaining the best talent for our business, challenging our people, demonstrating a \"can-do\" attitude, and fostering a collaborative and mutually supportive environment. ISC is an equal opportunity employer and a participant in the US federal E-Verify program (US).\n\nDiversity creates a healthier atmosphere: All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, gender, gender identity, sexual orientation, marital status, medical condition, genetic information, mental or physical disability, military or veteran status, or any other characteristic protected by local, state, or Federal law."}, {"title": "Data Engineer -Remote", "company_name": "RIT Solutions, Inc.", "location": "Denver, CO (Remote)", "link": "/jobs/view/3768007496/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=1hQH8EDaII1eJ7nQ9BACQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3768007496", "description": "About the job\nDuration: 6+ months CTH\n\nMust have a valid LinkedIn profile\n\nRequired Skills\n\n 3-6 years of experience (no more than 6 years)\n Start up environment experience\n Python, SQL, and DBT"}, {"title": "Big Data Engineer - W2", "company_name": "Megan Soft Inc", "location": "United States (Remote)", "link": "/jobs/view/3674845652/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fCAxh4N%2F4YZJ5aEIASaeeA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3674845652", "description": "About the job\nTitle: Big Data Engineer - W2\n\nLocation: Dearborn, MI (Remote)\n\nDuration: 12+ Months\n\nDirect Client: Ford Motors\n\nMust have skills: Should be able to handle GCP requirements\n\n\"Translates business requirements to build repeatable, sustainable, efficient, coded processes that can be productionized by Software Engineers and readily modified by other Data Engineer Creates POC processes in Dev/QA and works with Software Engineers to productionize and align best coding and process practices that maximize efficiency, speed, stability, system resources and capabilities Awareness of and compliance with data privacy, security, legal and contractual guidelines Incorporates data quality and privacy checks/alerts to minimize bad data being consumed by end users, models and dashboards, and to protect customer data; revises checks for new data issues Validates data products and pipelines are functioning as expected following system or application upgrades, source changes, etc. Works closely with Architects if required to align systems, tools and applications being utilized with business use case and performance requirements Provides data product support and maintenance Establishes, tracks and monitors KPIs related to specific data products and deliverable Participate in iteration cadence activities (Iteration Planning, task estimation, daily scrum, demos, etc.). Will be a part of on-call team to resolve critical production support tickets during business hours and off hours.\n\nSkills Required\n\nExperience with Big Data Technologies (Hadoop , ETL Tool, Spark. Hive etc..) Someone who understands Cloud and has experience working with Cloud Platforms (preferably GCP) Should be able to handle GCP requirements - ingesting into GCS/ Big Query, working with GCP components Hands-on in Data Flow, Cloud Storage (GCS), Big Query and GitHub. Proficient in Python and SQL.\n\nSkills Preferred\n\nManages ETL: uses programming and tools for data ingestion, configures pipelines, applies transformations and decoding, integrates and fuses data, moves and securely deliver Experience NoSQL databases\n\nAdditional Information\n\nIf the candidate is remote only, please indicate \"\"100% Remote\"\" under candidate's name on resume. Other similar schedule notations can be \"\"Local Candidate\"\" or \"\"Hybrid Candidate.\"\" ***HRA Test Required***\""}, {"title": "Remote Opportunity: Quantexa Data Engineer", "company_name": "SPAR Information Systems LLC", "location": "United States (Remote)", "link": "/jobs/view/3680073191/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ioykVCZSOQ7BK5%2FwRQHOWg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3680073191", "description": "About the job\nHello All, Hope you are doing great!! Please go through the job descriptions and let me know your interest. Role: Quantexa Data engineer  Location: Remote Duration: Long Term Contract Job Description: 1. End to End usage of Quantexa including guiding our client team on how to use from both front end and data ingestion Reason they are asking for Certification 2. Resolve Entity issues with data 3. Knowledge of Scala and Hive which requires coding experience using parquet 4. Advanced SQL knowledge requiring writing SQL queries 5. Experience having worked with GCP data lake 6. Quantexa certification required\n\nThanks & Regards,\n\nSatnam Singh\n\nDirect: 201 623 3660\n\nEmail : Satnam.singh@sparinfosys.com"}, {"title": "Remote Work - Need Staff Data Engineer - USC only", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3746283980/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=CvIuLBNO%2BPMGi4IXVRkP0A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3746283980", "description": "About the job\n100% Remote, client is in NY\n\nLook for strong job history, Python, Machine Learning, SQL, 10/10 comms, etc. \n\nMUST be an Engineer \n\nSummary\n\nWe're looking for an experienced Data Engineer to join our client's growing team of top notched IT professionals. You will work closely across multiple groups including Editorial and Audio, Marketing, Advertising, etc. to achieve their goals and objectives. The Data Engineering team builds tools throughout the stack while sharing knowledge across these departments.\n\nAs the company grows, they're looking for Data Engineer who will help solidify and expand our pipelines and maintain their data warehouse. Their business model is based on performing complex and very detailed analyses of how our products perform with their subscribers. You'll be responsible for working alongside analysts, product, marketing, and other teams to define new data collection components and measurement schemes that support each new product and feature.\n\nHow Can Add Value To Their Mission\n\nCreate and maintain data pipelines to provide insights and drive business decisions\nEstablish data warehousing strategy (ex. Kimball, Data Vault, etc.)\nMaintain data infrastructure on our AWS accounts\nCollaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.\nWrite unit/integration tests, contributes to engineering wiki, and documents work\nImplement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it\n\nWhat You'll Need To Succeed\n\n7+ years of industry experience measuring product performance and user behavior\nExperience working with a variety of data management technologies, including RedShift, Kinesis/Kafka, Glue, Spark, Postgres, Airflow, dbt, and others.\nExperience implementing BI reporting tools such as Looker\nExperience interfacing with engineers, product managers and analysts to understand data needs\nKnowledge of a variety of measurement beacons, SDKs, APIs including Google Analytics, Amplitude, Braze, Stripe, email service providers\nA commitment to building secure, resilient, fault-tolerant architectures, with clear documentation and procedures in place for support\nA focus on accuracy and detail as you build, ensuring data pipelines produce clear, predictable results\nUnderstanding of the typical metrics a subscription and advertising-supported business needs to measure success\nExperience with Client techniques as applied to behavioral segmentation or anomaly identification is a definite plus\nFamiliarity using developer tools that increase productivity and facilitate the development of resilient code (eg. Docker, CircleCI, Serverless) is a plus"}, {"title": "ENTRY LEVEL DATA ENGINEER (REMOTE)", "company_name": "SynergisticIT", "location": "Jacksonville, FL (Remote)", "link": "/jobs/view/3767587909/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=om7L3CnowiC4PLIcpZD70A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3767587909", "description": "About the job\nSYNERGISTICIT wants every candidate to know that the Job Market is Challenging and to stand out, you need to have exceptional skills and technologies and that's where we come in to make sure you get the attention which you need\n\nSynergisticIT understands the complex nature of the job market and how difficult it can be to secure a position, especially for fresh graduates. Therefore, we assist and help tech-savvies to convert their passions into professions. We go above and beyond to keep you working in your niche.\n\nAs we focus on long-term success, we provide complete career development solutions. From job search to upskilling portfolio and interview preparation, we can guide you at every step of your career.\n\nSynergisticIT spares no efforts to connect you with a large network of tech giants, including Google, Apple, PayPal, Dell, Cisco, Client, etc. Presently, we are actively looking for ENTRY LEVEL DATA ENGINEER a driven mindset. Get the right opportunity and gain experience in building web-centric solutions on Java.\n\nWho Should Apply : Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or anyone looking to make their career in IT Industry\n\nWe also assist in filing for STEM extension and H1b and Green card filing.\n\nCandidates who are serious about their future in the IT Industry and have set big goals for themselves.\n\nCandidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. We also offer Skill Enhancement Programs if the candidates are missing skills or experience which our clients need with great outcomes\n\nCandidates can benefit from skill enhancement if they fall into the below categories. If they are qualified with enough skills then no need for skill enhancement\n\nCandidates Who Lack Experience\n\nHave had a break in careers\n\nLack Technical Competency\n\ncandidates who want to get employed and make a career in the Tech Industry \n\nPlease Also Check The Below Links\n\nhttps://www.synergisticit.com/candidate-outcomes/\n\nhttps://www.synergisticit.com/java-track/\n\nhttps://www.synergisticit.com/data-science-track/\n\nhttps://www.synergisticit.com/which-is-the-best-option-for-tech-job-seekers-staffing-companies-consulting-companies-bootcamps-or-synergisticit/\n\nhttps://www.synergisticit.com/contact-us/\n\nIf the skills are not a match candidates can opt for Skill enhancement. Or their resume can be sent out to clients to see if responses are achievable\n\nREQUIRED SKILLS For Java/Software Programmers\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Core Java , javascript , C++ or software programming\nSpring boot, Microservices and REST API's experience\nExcellent written and verbal communication skills\n\nFor data Science/Machine learning\n\nRequired Skills\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Statistics, Python, data visualization tools\nExcellent written and verbal communication skills\n\nPreferred skills: NLP, Text mining, Tableau, Time series analysis\n\nTechnical skills are required by clients for selection even if its Junior or entry level position each additional Technical skill helps a candidate's resume to be picked by clients over other job seekers.\n\nClients hire candidates with the right technical skills which they need and reject candidates who lack the required technical skills.\n\nNo third party candidates or c2c candidates\n\nPlease apply to the posting\n\nNo phone calls please. Shortlisted candidates would be reached out"}, {"title": "Data Engineer - Remote | WFH", "company_name": "Get It Recruit - Information Technology", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3782241984/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=BgBf1kSAAX%2BSuw6inJsIVw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3782241984", "description": "About the job\nJoin an innovative team at a leading company in the engineering and manufacturing industry, where we are committed to leveraging data for continuous improvement and operational excellence. This hybrid position allows you to enjoy the flexibility of remote work on Mondays and Fridays.\n\nWhy Join Us\n\nAt our company, we prioritize your well-being and professional growth. We offer a comprehensive benefits package, including excellent health benefits, a 6% 401(k) match, generous paid vacation and sick time, 12 paid holidays, and more.\n\nRole Overview\n\nAs a Data Engineer, you will play a pivotal role in transforming raw data into actionable insights, driving improvements in engineering and manufacturing processes, enhancing product quality, and optimizing operational efficiency. Collaborate with dynamic cross-functional teams to extract, analyze, and visualize data, influencing strategic decisions through your valuable insights.\n\nKey Responsibilities\n\nData Collection and Integration:\n\nGather, clean, and integrate data from diverse sources, including production databases, sensor data, and external datasets.\n\nData Analysis\n\nConduct exploratory data analysis to identify patterns, trends, and anomalies.\n\nPerform statistical analysis and hypothesis testing to derive meaningful insights.\n\nData Visualization\n\nCreate informative and visually appealing dashboards and reports using tools like Tableau, Power BI, or Python libraries.\n\nPredictive Modeling\n\nDevelop predictive models to forecast manufacturing performance, quality, and operational outcomes.\n\nRoot Cause Analysis\n\nInvestigate production issues and quality concerns by analyzing data to identify root causes and recommend corrective actions.\n\nPerformance Monitoring\n\nEstablish and maintain key performance indicators (KPIs) to track manufacturing and engineering metrics.\n\nGenerate regular reports to communicate performance trends to stakeholders.\n\nCross-functional Collaboration\n\nCollaborate with engineering, manufacturing, and quality assurance teams to understand their data needs and provide analytical support.\n\nContinuous Improvement\n\nSuggest process improvements and optimizations based on data analysis.\n\nData Security And Compliance\n\nEnsure data security and compliance with relevant regulations, such as GDPR or industry-specific standards.\n\nInfrastructure And Development\n\nBuild and maintain the infrastructure for collecting, storing, and processing data.\n\nDesign, develop, and manage data pipelines, ETL processes, and data warehouses.\n\nRequired Experience\n\nProven experience as a Sr. Data Analyst and/or Data Engineer in an engineering or manufacturing environment.\n\nProficiency in data analysis tools and programming languages, such as Python, R, SQL, and Excel.\n\nStrong data visualization skills with tools like Tableau or Power BI.\n\nExcellent knowledge of statistical analysis and machine learning techniques.\n\nAbility to communicate complex findings and insights effectively to both technical and non-technical stakeholders.\n\nStrong problem-solving skills and attention to detail.\n\nFamiliarity with manufacturing processes and quality control is a plus.\n\nExperience with enterprise ERPs (SAP S4HANA, Oracle EBS), CRMs (Salesforce, Hubspot), and IoT.\n\nProficient in working with cloud computing platforms including AWS, GCP, and Azure.\n\nExpertise in configuring and optimizing cloud-based data solutions for scalability, reliability, and performance.\n\nBachelor's Degree in Data Science, Statistics, Engineering, or Computer Science Required.\n\nApply Now and be part of a team shaping the future of data-driven innovation in engineering and manufacturing!\n\nEmployment Type: Full-Time"}, {"title": "Entry Level Software Engineer - Data Backend Engineer (Remote - Canada)", "company_name": "Yelp", "location": "United States (Remote)", "link": "/jobs/view/3752881757/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fankeoxifvhpTzOYvowHYQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3752881757", "description": "About the job\nYelp engineering culture is driven by our values: we\u2019re a cooperative team that values individual authenticity and encourages creative solutions to problems. All new engineers deploy working code their first week, and we strive to broaden individual impact with support from managers, mentors, and teams. At the end of the day, we\u2019re all about helping our users, growing as engineers, and having fun in a collaborative environment.\n Interested in exploring data and finding innovative ways to collect it, curate it, and make it easily accessible for others to use? How about doing that at an enormous scale? Yelp\u2019s datasets contain billions of interactions between users and local businesses around the globe. If working with our vast amount of data sounds exciting, come and join us!\n\nThe Marketplace Data Observability team is responsible for managing, governing, and monitoring all of Yelp\u2019s Ads data to protect our revenue stream and also support offline analysis that aids in making long term company decisions. We build tools that help us to better understand our data quality, and we strive to uncover data issues before they impact our consumers.\n\nAs a Data Backend Engineer on this team, you will be responsible for building elegant and scalable data products that serve critical, up-to-date, structured information to support different types of analytics within Yelp. As a core contributor to our growing data modeling and data warehousing engineering efforts, you will help design and own mission-critical data flow pipelines and datastores to enable decisions including effective A/B testing and company investments.\n This opportunity is fully remote and does not require you to be located in any particular area in Canada. We welcome applicants from throughout Canada. We\u2019d love to have you apply, even if you don\u2019t feel you meet every single requirement in this posting. At Yelp, we\u2019re looking for great people, not just those who simply check off all the boxes.\n  Build systems that can effectively store and crunch terabytes of data.\nDesign and develop data models for efficient data storage, retrieval, and reporting.\nCreate and maintain conceptual, logical, and physical data models using industry-standard modeling tools.\nCollaborate with cross-functional teams, including engineers, data analysts, business analysts, and data scientists, to understand data requirements and translate them into effective data models.\nParticipate in data integration efforts, including ETL processes and data migration.\nStay up-to-date with industry best practices, emerging technologies, and trends related to data warehousing.\nSupport on-call rotations as needed to operate the team.\n  Understanding of high performing and scalable data systems.\nExperience in building and orchestrating ETL pipelines.\nExperience with Data Lake or Data Warehouse landscape.\nA hunger for tracking down root causes and fixing them in systematic ways.\nAbility to communicate effectively to technical and non-technical cohorts alike. \nExposure to some of the following technologies: Python, AWS Redshift, AWS Athena / Apache Presto, Big Data technologies (e.g S3, Hadoop, Hive, Spark, Flink, Kafka etc), NoSQL systems like Cassandra, DBT."}, {"title": "Data Engineer (Associate Consultant), June 2024", "company_name": "UDig", "location": "Nashville, TN (Remote)", "link": "/jobs/view/3716039960/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=XUjo40kU55HnDCqJ%2F3FdPg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3716039960", "description": "About the job\nCan UDig It? \n\nUDig designs, builds, and implements technology solutions that deliver on business objectives. People join our team because they are excited to have an impact on the future of our organization, passionate about delivering meaningful results, and energized by solving complex problems. Oh, and they like working with other driven, smart, and authentic people!\n\nUDig is proud to be recognized as a \"Best Places to Work\" in Virginia and Tennessee. We're committed to fostering a transparent environment where leadership is accessible, professional development opportunities are abundant, and you are empowered to achieve your career goals. You'll collaborate, develop deep relationships with teammates, and accelerate your success while having fun!\n\nIf you're graduating in May 2024 and excited by the idea of impacting organizations by building data solutions to achieve business goals, you should consider joining our growing technology team.\n\nHow Things Go Down\n\nAs a new, entry-level teammate, you'll take part in our \"Breaking Ground\" training program designed to help you quickly get up-to-speed on UDig's delivery approach within a project team. The program covers:\n\nUDig delivery process and methodologies such as gathering requirements and delivering projects using agile\nTechnical guidance and instruction on the best practices from Data leaders at UDig\nDelivering executive and technical presentations\nCoaching to prepare you to for success on your first client project at UDig\n\nA typical day might entail:\n\nProject standups and check-ins\nDesign and development of data pipelines, BI reports, and more\nPlanning and participating in project demos\nContributing to internal UDig projects\nCareer and job growth through UDig-provided training\nCreating long-lasting friendships with teammates through fun activities as well as your projects (including social events, winning a prize for guessing the weight of pumpkin, or sharing experience on cool tech side projects)\n\nUDig believes it is important for Associate Consultants going through our Breaking Ground program to reside in one of our major markets (Richmond, VA or Nashville, TN metro area) for at least the first year of employment.\n\nWant to learn more? Hear directly from our teammates by checking out our 'A Day in the Life' blog series: https://www.udig.com/digging-in/breaking-ground-2023/\n\nSound Interesting? Here's the Technical Piece\n\nBelow are a few boxes you should be able to check if\u202fyou're interested: Don't check every box? Go ahead and apply!\n\n1+ year of experience with relational database management systems such as Microsoft SQL Server, MySQL, or Oracle\nUnderstanding of the full lifecycle of design, development, and implementation of data solutions including architecture design, development, testing\nSome experience in ETL development or reporting is a plus\nAbility to present technical ideas and high-level concepts and solutions to internal and external team members with varying degrees of technical knowledge\nBachelor's Degree in in computer science, information technology, or any science or business discipline\nAbility to take your work seriously but also seriously enjoy a good time with your UDig colleagues\n\nWhat's in It For You? \n\nAt UDig, you will build your career alongside talented and experienced developers while gaining experience and having fun! We provide continuous challenges and professional development opportunities. Additionally, we offer: \n\nCompetitive salary, merit reviews, and career advancement paths\nFlexible, hybrid environment\nIndividual $1500 Training Budget\nRegular team building and social activities (virtual and/or in-person)\nTransparent culture with strong communication and access to leadership\nGreat benefits like\nGenerous Paid Time Off, company holidays, and parental leave\nMultiple Single and Family Health Insurance plans to choose from\nDental & Vision coverage\nShort-Term and Long-Term disability\nOptional accident and critical illness coverage\nMatching 401(k)\nand more!"}, {"title": "Expression of Interest: Data Engineer", "company_name": "Fingerprint for Success (F4S)", "location": "New York, NY (Remote)", "link": "/jobs/view/3787769880/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=l%2FEDpQqUNmCttGdIyTk7Cg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787769880", "description": "About the job\nWe are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.\n\nThe F4S Talent Pool is a pilot project designed to:\n\nHelp job seekers get discovered by our partners based on their anticipated hiring needs.\nProvide optional support and resources for job seekers in their career endeavors.\nHelp individuals understand, and bring out the best in themselves and each other.\n\nThe F4S Talent Pool process:\n\nOnce you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.\n\nAbout Fingerprint For Success (F4S)\n\nBacked by 20+ years of research, F4S\u2019s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high performing teams.\n\nKeep in mind, joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your F4S work style assessment to match the hiring needs of our partners.\n\nYour feedback is a gift! Write to us via:\n\nPowered by JazzHR\n\noWuPJ9lwq9"}, {"title": "Data Engineer Seattle, WA (remote)", "company_name": "Conch Technologies, Inc", "location": "United States (Remote)", "link": "/jobs/view/3765052211/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=SBETI78UMutCHuRp5FnP%2FA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3765052211", "description": "About the job\nHi,\n\nGreetings from Conch Technologies Inc\n\nData Engineering Lead \u2013 With Databricks and Unity Catalog\n\nLocation \u2013 Seattle, WA (remote)\n\nDuration \u2013 12 months\n\nNeed: Design and implement data architecture and strategy, leveraging technologies such as Databricks, Unity Catalog, Privacera, and Collibra.\n\nAs a Data Engineering Lead, you will play a pivotal role in defining and executing technical deliverables while assessing complexities and levels of effort. You'll engage with both vendor and internal teams to resolve technical challenges and ensure seamless collaboration. Your leadership will extend to planning and prioritization with the SCRUM master, facilitating effective coordination between onsite and offshore teams, and driving feature prioritization through collaboration with product managers.\n\nKey Responsibilities\n\n Lead the definition and breakdown of technical deliverables, assessing complexities and levels of effort.\n Collaborate with vendor and internal teams to address technical issues.\n Drive planning and prioritization in conjunction with the SCRUM master.\n Coordinate with onsite and offshore teams for timely project delivery.\n Collaborate with product managers to prioritize features, communicate with stakeholders, and clarify requirements.\n Collect and manage user-reported issues, ensuring prompt resolution and communication.\n Define comprehensive test plans, oversee their execution, and document results.\n\n--\n\nWith Regards,\n\nNagesh G\n\nMobile: 408-381-5645\n\nDesk: 901-313-3066\n\nEmail: nagesh@conchtech.com \n\nWeb: www.conchtech.com"}, {"title": "Remote Work - Need Azure Fabric Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3759325599/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=C3iVjmcR9wjUip1nNjUiCg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3759325599", "description": "About the job\nThe client is placing heavy emphasis on Microsoft and Azure Fabric experience\n\nInformation Management Engineer \u2013 Take the identified information management sources and assist migration to the Client Data Lake, Azure Fabric. The engineer will transform information management (de-dup, clean and enrich).\n\nTake the identified data sources and bring them into our Data Lake, Azure Fabric. Once the data is in, the engineer will need to be able to transform data (de-dup, clean and enrich the data). We are focused on using a medallion data model for data architecture. The tools we are working with are: Delta Parquet files, python, spark, notebooks, T-SQL, Fabric lakehouse/warehouse, Dataflows Gen2, Data Pipelines, Direct Lake, shortcuts, ADF."}, {"title": "Data Engineer", "company_name": "Catalist", "location": "Washington, DC (Remote)", "link": "/jobs/view/3744951279/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ui1Fk6ICmnPNqAlZDZAcwA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3744951279", "description": "About the job\nFor over 17 years, Catalist has been a leader in civic data and data science innovation. Our mission is to provide progressive organizations with the data, software, and services needed to better identify, understand, and communicate with the people they need to engage and mobilize. Our clients include the largest, most influential organizations in the U.S. active in civic engagement, advocacy, and political campaigns.\n\nCatalist is home to a dedicated, creative team of technologists, data scientists, and campaign experts committed to using our talents and technology to nurture a vibrant and growing progressive community.\n\nAs a Data Engineer at Catalist, you will work closely with the Analytics and Technology departments to design, build, support, and maintain various data pipelines and processes with an end goal of providing data and intelligence to the progressive community. This role primarily involves translating data architecture designs into functional processes, code, and systems.\n\nThe ideal candidate will be a highly motivated individual with excellent technical skills, a strong desire to learn new skills, and an interest in progressive politics. Catalist values creativity and problem-solving. Our work is on the cutting edge of data-driven politics, and your support will help Democratic candidates and progressive organizations conduct successful advocacy and electoral campaigns.\n\nThis position reports to the Deputy Chief Data Officer. The Data Engineer is a part of a growing Data team that supports all underlying work at Catalist.\n\nThis position is included in our CWA bargaining unit.\n\nPrinciple Duties & Responsibilities\n\nDevelop scalable, production processes, code, and systems for data ingest, transformation, modeling, and reporting across a variety of platforms\nTranslate mock-ups and designs into functional processes, code, and systems\nCreate architecture designs when needed\nProvide quality assurance and testing on processes, code, systems, and products\nBecome an internal subject matter expert on various datasets and support other Catalist departments/teams on usage of those datasets\nExecute ad-hoc data and database maintenance tasks as requiredProject manage cross-departmental efforts, with direct responsibility for stakeholder engagement, management, and execution of technical elements\n\n\nRequirements\n\nBS or BA in a technical field, or relevant experience\n1-2 years of experience working with SQL databases\nExperience with data cleaning or ETL processes\nExperience with distributed computing systems and/or distributed datastores (particularly the Hadoop ecosystem)\nExperience managing projects\nFamiliarity with Catalist data, progressive politics, voter files, and/or commercial data\nBackground check required\n\n\nPreferred Skills & Abilities\n\nWillingness to be a problem solver and produce results in a fast paced environment\nAbility to be creative and personable, and articulate ideas clearly\nExperience working with SQL databases\nProficiency with Python or another object-oriented programming language (R, Java, Scala, etc\u2026)\nExperience working in cloud environments (AWS, GCP, etc.)\nExperience working in command line environments such as Bash\nExperience with a version control tool such as git or github\n\n\nBenefits\n\nMedical, Dental, Vision, Prescription Drug\n\nCatalist offers Medical, Dental, Vision, and Prescription Drug coverage for eligible staff and their eligible dependents. Catalist\u2019s Medical plan is a comprehensive PPO program including Prescription Drug coverage with 85% of the premium paid by Catalist. Dental and Vision coverage is provided at no cost to employees.\n\nGroup Term Life Insurance and Long-Term & Short-Term Disability Coverage\n\nGroup Term Life Insurance and Long-Term and Short-Term Disability coverage is available for eligible staff. These benefits are provided at no cost to Catalist employees.\n\n401(k) Safe Harbor Plan\n\nA 401(k) Safe Harbor Plan is available to eligible staff with a 3% contribution from Catalist from the date of hire. Employees may contribute pre-tax or post-tax from their salary up to the legal limits set forth by the IRS.\n\nMedical and Dependent Care Flexible Spending Accounts (FSAs)\n\nCatalist offers an FSA Program that gives eligible staff the ability to pay out-of-pocket medical/dental/vision/child care expenses from pre-tax earnings.\n\nTransit Benefits\n\nCatalist also makes available a Transit benefit FSA program to eligible employees using pre-tax contributions with a company match.\n\nProfessional Development and Remote Work Expenses\n\nEligible employees may be reimbursed up to $750 each year for professional development / education and remote work expenses.\n\nStudent Loan PayDown or SaveUp\n\nCatalist offers a Student Loan PayDown and College SaveUp benefit for eligible staff.\n\nVacation, Personal Leave, Sick Leave Benefits\n\nCatalist offers generous vacation benefits to all eligible staff. Eligible employees also receive:\n\n 14 Paid Holidays\n Personal Days\n Sick Leave\n Parental Leave\n\n\nHybrid Office/Remote Work\n\nCertain positions at Catalist are eligible for Office/Remote Hybrid or full Remote status."}, {"title": "Remote Entry Level Data Analyst/Engineer", "company_name": "SynergisticIT", "location": "White Plains, NY (Remote)", "link": "/jobs/view/3774171423/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=q54sk1%2FRww0z%2BC%2BE8e0jQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774171423", "description": "About the job\nAt SynergisticIT, we're all about making connections. Whatever IT goals you have, our software programmers can help achieve those. Our Software development teams can take up turnkey projects and execute them in an effective and efficient manner. If you are looking to source talent our recruiters will find the ideal IT talent for your company. What's the secret to our success? Well, it all starts with taking quality time to listen to each client's specific needs. After we have a thorough grasp of your IT goals, we can better customize our Developments as per your specific needs. We can also tailor make recruiting programs to exceed your expectations. Since our founding in 2010, SynergisticIT's strategies have earned the company an enviable position in the software development, IT staffing and IT skill enhancement fields. SynergisticIT continues to work with hundreds of satisfied American clients with our software programmers working on our projects and after gaining hands on experience on cutting edge technologies moving to contribute their skills to great clients like Apple, Google, Client, Ebay, Paypal, Kroger, the Walt Disney Company and hundreds more. If you are tired of working with inefficient programmers who take a lot of time to ramp up we want you to try us. Our software programmers can hit the ground running and get you the maximum return on your investment. You have already tried the rest its time you tried the best. SynergisticIT - Home of the Best Data Scientists and Software Programmers in the Bay Area.\n\n Why Us ? \n\nSynergisticIT has a proven track record of successfully skill enhancement and staffing IT employees for some of the world's most iconic brands. Our team takes the time to fully understand every client's needs so we could best meet your IT staffing requirements. The knowledgeable staff at SynergisticIT is always more than happy to work with clients to ensure they reach their software development goals. Besides staffing, SynergisticIT is also committed to helping young IT professionals advance their career with a robust upskill program . Everyone who goes through SynergisticIT's program learns all the skills necessary to succeed in many IT fields ranging from Java to Machine Learning. Additionally, everyone trained at SynergisticIT has been through extensive mock and technical interview screenings to bolster their career prospects. Last, but certainly not least, SynergisticIT takes great care to respect the privacy considerations for every client. All companies who work with SynergisticIT can rest assured their confidential data is protected using the most up-to-date encryption technologies. SynergisticIT also complies with all the latest NDA agreements.\n\n REQUIRED SKILLS For Java /Software Programmers \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Time series analysis \n\n We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2022 and at Gartner Data Analytics Summit (Florida)-2023 \n\nOracle CloudWorld Event (OCW) Las Vegas 2022 | SynergisticIT - YouTube\n\n https://www.youtube.com/watch?v=OAFOhcGy9Z8 \n\n https://www.youtube.com/watch?v=EmO7NrWHkLM \n\n https://www.youtube.com/watch?v=NVBU9RYZ6UI \n\n https://www.youtube.com/watch?v=Yy74yvjatVg \n\nSynergisticIT at Gartner Data and Analytics Summit 2023 - YouTube\n\nFor preparing for interviews please visit  https://www.synergisticit.com/interview-questions/ \n\n We are looking for the right matching candidates for our clients \n\n Please apply via the job posting \n\n REQUIRED SKILLS For Java /Full Stack/Software Programmer \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning Positions \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Tensorflow \n\n If you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team. \n\n No phone calls please.  Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates"}, {"title": "Data Engineer", "company_name": "Anika Systems", "location": "Leesburg, VA (Remote)", "link": "/jobs/view/3787766893/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=mApL9JnN%2B%2Fr2%2BYUPlxRYOA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787766893", "description": "About the job\nAnika Systems is a fast growing, woman-owned small business that specializes in providing innovative IT solutions for federal government agencies. Our expertise lies in accelerating delivery in Data and Analytics, Intelligent Automation, Application Development, and IT Modernization. We are currently expanding our Federal team and are seeking a passionate and talented Data Engineer. This opportunity is 100% remote.\n\nMust be a U.S. Citizen with the ability to obtain and maintain a government suitability clearance\n\nResponsibilities\n\nDesigns, develops, builds, analyzes, evaluates, and installs database management systems to include database modeling and design, relational database architecture, metadata, and repository creation and configuration management.\nDefines and oversees database organizations, standards, controls, procedures, and documentation. Provides technical consulting in the definition, design, and creation of a database environment.\nAdvises applications development staff and users on data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nDesigns and implements databases with respect to access methods, access time, batch processes, device allocation, validation checks, organization, protection and security, documentation, and statistical methods.\nUses data mapping, data mining, and data transformational analysis tools to design and develop databases.\nDetermines data storage and optimum storage requirements.\nPrepares system requirements, source analysis, and process analyses and designs throughout the database implementation.\n\nRequired Skills And Experience\n\nBA/BS and 1 year of relevant experience\nExperience with DataBricks, SQL, Python\nApplications development experience with data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nExcellent communication skills (both oral and written) with process-oriented organizational skills to ensure project success\n\nDesired Skills And Experience\n\nDemonstrated successful development experience with the full life cycle of government database management systems including database modeling and design, relational database architecture, metadata and repository creation, and configuration management.\n\nPowered by JazzHR\n\nTlKWIorucJ"}, {"title": "Remote Work - Need Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3774712978/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=r3Bp%2F0qcOD2OpC5jpbUOcA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774712978", "description": "About the job\nTitle : Data Engineer\n\nLocation : Remote\n\nDuration : 6+ months to start (long term contract (work order are extended 2x year))\n\nPlease Fill out the skill matrix: Technology Some Awareness (1) Novice (2) Intermediate (3) Advanced (4) Expert (5) SQL Tableau Desktop /Creator Tableau API Integration AWS - S3 Glue AWS - Athena Redshift AWS - Lambda AWS - SNS/SQS msExcel\n\nProfile: Data Analyst will work closely with Architects and leads in the Interflow team, key business stakeholders including program staff to support the organizations effort in the areas of data Analysis and information management. Role should have strong experience in writing SQL queries to extract data for reports. Candidate should have experience working with large set of data. As these reports are very critical for the company and we are looking for an expert who is capable of handling, consuming and extracting large sets of data for analysis and reports.\n\nMust Haves\n\nAWS - Athena\nAWS - S3\nGlue\nhands-on development of reporting applications\nMS Excel\nRedShift\nSQL\nTableau Desktop /Creator\n\nNice\n\nAWS - Lambda\nAWS - SNS/SQS\nTableau API Integration\n\nSkills\n\nBA/BS required (major in an analytical field desired) \nA minimum 10+ years work-related experience. \nExperience in hands-on development of reporting applications for the Web in a professional environment \nThe Ethos of continuous improvement and interest in learning new things.\nStrong analytical thinking and structured problem-solving ability\nAbility to handle multiple projects and assignments simultaneously and effectively in a cross-functional team environment. \nVersed on the agile methodology and best practices. \nExcellent interpersonal and collaboration skill with the ability to work with a diverse set of colleagues, across functions, from different organizations, disciplines, etc. \nSelf-starter, ability to set priorities, work independently and attain goals"}, {"title": "Data Engineer", "company_name": "Team Remotely Inc", "location": "Houston, TX (Remote)", "link": "/jobs/view/3786249829/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=78lY5Jo%2FfyRPC6TQYCdvww%3D%3D&trk=flagship3_search_srp_jobs", "id": "3786249829", "description": "About the job\nThis is a remote position.\n\n Data Engineer (US/Canada Residents Only, 1 year experience, remote)\n\nTeam Remotely Inc. is a staffing and recruitment agency that offers a comprehensive solution for talent acquisition, including sourcing, vetting, pay rolling, and managing talent. Whether you need contract staffing, direct hire, direct sourcing, talent pools, or diversity initiatives, our model can support your hiring strategy.\n\n Hiring Type: Full-Time\n\n Base Salary: $55K-$70K Per Annum.\n\nResponsibilities\n\nDesigning user interface changes for web-based DB applications. \nReviewing application requirements and interface designs. \nDeveloping and implementing highly responsive user interface components using react concepts. \nWriting application interface codes using JavaScript following react.js workflows. \nTroubleshooting interface software and debugging application codes. \nDeveloping and implementing front-end architecture to support user interface concepts. \nMonitoring and improving front-end performance. \n\nQualifications And Experience\n\nBachelor\u2019s degree in computer science, information technology, or a similar field. \nPrevious experience working as a react.js developer. \nIn-depth knowledge of JavaScript, CSS, HTML, jQuery, and front-end languages. \nKnowledge of REACT tools including React.js, Webpack, Enzyme, Redux, and Flux. \nExperience with user interface design. \nKnowledge of performance testing frameworks, including Mocha and Jest. \nExperience with browser-based debugging and performance testing software. \nTroubleshooting skills. \nProject management skills. \nProblem-solving skills. \nVerbal communication skills. \n\n Why work with Team Remotely?\n\nTeam Remotely Inc. is a staffing platform offering a seamless experience for employers and candidates. Employers can post job openings and specify their requirements, while candidates can create profiles and upload resumes.\n\nThe team of Team Remotely continuously learns and adapts based on previous successful placements, constantly improving its matching capabilities. This ensures that the recommendations provided by Team Remotely are tailored and accurate, increasing the likelihood of a successful match between employers and candidates. By providing intelligent and data-driven solutions, they strive to enhance the efficiency and effectiveness of the hiring process, ultimately helping companies find the best talent and individuals find their dream jobs."}, {"title": "Data Engineer", "company_name": "ZAG Technical Services", "location": "United States (Remote)", "link": "/jobs/view/3771403464/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=Hf0VefrKXBFIL5MtHmY0Pw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3771403464", "description": "About the job\nWe are looking for a tech savvy Data Engineer to contribute to the building, operating, and scaling of next generation data platforms and tools that will power data-driven analytics capabilities throughout the ZAG products and services portfolio, spanning areas such as business intelligence, reporting, and data analytics.\n\nThe ideal team member is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. Must be comfortable supporting the data needs of multiple clients and teams, systems, and products within the modernization of ecosystems. The right-fit person will be excited by the prospect of optimizing or even re-designing data processes to support next generation of advanced analytics capabilities, which in-turn translates to implementing strategic enhancements to maintain competitive advantage in the industry.\n\nThe successful team member will leverage their proficiency to\u2026\n\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\nBuild the analytics processes required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Data Management and cloud based \u2018big data\u2019 technologies.\nBuild analytics tools that utilize technologies to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.\nCollaborate with client teams to identify and solution regional and local requirements within the context of the standard global business model for enterprise analytics.\nParticipates in meetings, discussions, strategy sessions where changes, improvements, and enhancements are proposed, evaluated, and approved. \n\n\nThe exceptional team member will have or demonstrate progressive experiences in\u2026 \n\nBachelor\u2019s degree or higher in Computer Science or related field\nExperience working with BI and data warehousing tools\nExperience working with enterprise data, where security is paramount and data governance is critical\nAbility to work independently and in a team environment, and effectively engage all levels of the organization\nUnderstanding of distributed systems driving large-scale data processing and analytics with a successful history of manipulating, processing, and extracting value from large, disconnected datasets.\nAbility to communicate effectively (listening, presenting, and questioning)\nStrong organizational, written, and communication skills\n\n\nAbout ZAG\n\nZAG Technical Services empowers our clients\u2019 success through intelligent business solutions that increase employee productivity, network security and team collaboration. We aim to ensure that our client\u2019s IT investments drive ROI, provide a competitive advantage and help businesses grow. We do this by living our core set of values: Integrity Foremost, Client-Centered, Accountable Always, Teamwork Throughout, and Exceptional is the Goal.\n\nWhat ZAG Has To Offer\n\nCompetitive compensation package \nBenefits including medical, dental, vision, 401K and life insurance \n\n\nZAG Technical Services is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all of our employees.\n\nZAG Technical Services is an E-Verify participant and will provide the Social Security Administration (SSA) and, if necessary, the Department of Homeland Security (DHS), with information from each new employee\u2019s Form I-9 to confirm work authorization."}, {"title": "Junior Data Engineer", "company_name": "HireMeFast LLC - Career Accelerator - Land A Job", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3786249797/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D&trk=flagship3_search_srp_jobs&lipi=urn%3Ali%3Apage%3Ad_flagship3_search_srp_jobs%3BQExogrHBQoSF9zZm%2B9kufw%3D%3D&lici=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D", "id": "3786249797", "description": "About the job\nThis is a remote position.\n\n Job Title: Junior Data Engineer\n\n Employment Type: Full-Time\n\n Salary: $64,000 - $76,000 per annum\n\n Experience Required:  Minimum 1 year of project experience\n\nAbout us: HireMeFast is a leading staffing and recruitment agency specializing in connecting businesses with top-tier talent across various industries. Our mission is to bridge the gap between exceptional candidates and organizations needing their skills, expertise, and unique qualities. Our team of experienced and dedicated recruitment specialists utilizes innovative sourcing strategies, and a vast network to identify and attract top talent. We conduct comprehensive procedures to ensure that only the most qualified candidates are presented to our clients.\n\nPosition Summary\n\nJoin the fast-paced, innovative, and collaborative environment focused on providing an AIOps platform that enhances the intelligence of the CVS Health infrastructure. Work closely with subject matter experts and colleagues to build and scale out machine learning and AI solutions that will detect, predict, and recommend solutions to correct issues before system impact and enhance efficiency, reliability, and performance of CVS Health\u2019s IT operations.\n\nKey Responsibilities include:\n\nData pipeline development: Designed, implemented, and managed data pipelines for extracting, transforming, and loading data from various sources into data lakes for processing, analytics, and correlation. \nData modeling: Create and maintain data models ensuring data quality, scalability, and efficiency \nDevelop and automate processes to clean, transform, and prepare data for analytics, ensuring data accuracy and consistency \nData Integration: Integrate data from disparate sources, both structured and unstructured to provide a unified view of key infrastructure platform and application data \nUtilize big data technologies such as Kafka to process and analyze large volumes of data efficiently \nImplement data security measures to protect sensitive information and ensure compliance with data and privacy regulation \nCreate/maintain documentation for data processes, data flows, and system configurations \nPerformance Optimization- Monitor and optimize data pipelines and systems for performance, scalability and cost-effectiveness \n\nCharacteristics of this role:\n\nTeam Player: Willing to teach, share knowledge, and work with others to make the team successful. \nCommunication: Exceptional verbal, written, organizational, presentation, and communication skills. \nCreativity: Ability to take written and verbal requirements and come up with other innovative ideas. \nAttention to detail: Systematically and accurately research future solutions and current problems. \nStrong work ethic: The innate drive to do work extremely well. \nPassion: A drive to deliver better products and services than expected to customers. \n\nRequired Qualifications\n\n2+ years of programming experience in languages such as Python, Java, SQL \n2+ years of experience with ETL tools and database management (relational, non-relational) \n2+ years of experience in data modeling techniques and tools to design efficient scalable data structures \nSkills in data quality assessment, data cleansing, and data validation \n\nQualifications\n\nKnowledge of big data technologies and cloud platforms \nExperience with technologies like PySpark, Databricks, and Azure Synapse. \n\nEducation\n\nBachelor\u2019s degree in Computer Science, Information Technology, or related field, or equivalent working experience\n\nWhy HireMeFast LLC?\n\nAt HireMeFast, we understand that finding the right individuals to join your team is crucial for success and growth of your organization. We are committed to streamlining the hiring process for our clients, ensuring they have access to a diverse pool of highly qualified candidates who are a perfect fit for their specific needs."}, {"title": "Data Engineer  - Need candidates from Texas ( REMOTE )", "company_name": "Pulivarthi Group (PG)", "location": "United States (Remote)", "link": "/jobs/view/3770685212/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=9gdMwuif%2BpB%2BCQYvQi3imQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3770685212", "description": "About the job\nFollow us on Linkedin: https://www.linkedin.com/company/pulivarthigroup/\n\nPulivarthi Group LLC is a Global Staffing & IT Technology Solutions company, with our prime focus of providing world class solutions to our customers with the right talent. We combine the expertise of our team and the culture of your company to help you with the solution that is affordable and innovative using high quality standards and technologies.\n\nWe\u2019ve served some of the largest healthcare, financial services, and government entities in the U.S.\n\nData Engineer - FT only - Azure SQL with data lakes experience\n\nData Engineer -\n\nthe resource requirement for this Data Engineer role needed more clarification.\n\nOur customer is requesting a resource who knows Azure CI/CD (Continuous Integration / Continuous Deployment) to build and integrate their DevOps pipeline.\n\nOnce the DevOps pipelines are mapped, they need the Data Engineer to connect to all the Data Sources, primarily their Data Lake.\n\nThe Customer (based in Houston) is interested in a resource in a time zone not too far removed from the Central Time Zone to allow real time collaboration."}, {"title": "Data Engineer", "company_name": "Otter Products", "location": "Fort Collins, CO (Remote)", "link": "/jobs/view/3766728302/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=77U3U7CAi1PxgDmpI3%2BiNQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3766728302", "description": "About the job\nOverview \n\nOtter Products is currently recruiting for a Data Engineer to join our Data team! You can be based in our Fort Collins, CO office with flexibility to work a portion of your time remotely, or 100% Remote in the U.S.\n\nThe Data Engineer applies professional experience, concepts, and company objectives toward building and operationalizing the minimally inclusive data necessary for the enterprise data initiatives following industry standard practices and tools. The bulk of the Data Engineer's work would be in designing, managing and optimizing data pipelines, then moving these data pipelines effectively into production for key data and analytics consumers like business/data analysts, data scientists or other stakeholder that needs curated data for data and analytics use cases across the enterprise. The Data Engineer also ensures compliance with data governance and data security requirements while creating, improving and operationalizing these integrated and reusable data pipelines to enable faster data access, integrated data reuse and vastly improved time-to-solution for data and analytics initiatives. Additionally, the Data Engineer will be expected to collaborate with other data team members and data consumers working on the models and algorithms developed by them toward optimization for data quality, security, and governance to put them into production leading to potentially large productivity gains.\n\n About Otter Products \n\nOtter Products, we grow to give. From our founder's garage in 1998 to the global technology leader we are today, Otter Products continues to drive growth through innovation.\n\nThrough our industry-leading brands - OtterBox, Liviri and OtterCares - we provide our partners the number-one selling and most trusted products in our categories. Our philanthropic spirit is the foundation on which we foster our partner relationships, allowing us to grow and to give - together.\n\nBy way of our charitable arm, the OtterCares Foundation, we support our communities and invest in our future through education that inspires kids to change the world.\n\nAnd even as our global community of Otters continues to grow, our founder's core values are still at the heart of everything we do. We measure our success by our ability to give back to our communities and strengthen opportunities for all.\n\nFor more information visit otterproducts.com\n\n Responsibilities \n\nComplete analysis, design and development of BI solutions using Azure SQL Server\nFamiliarity with Data Warehouse concepts\nExperience coding the extraction, transformation, and loading (ETL) processes\nDatabase development primarily in SSIS, Data Factory and SQL\nPartner with the business to determine end user database/reporting needs and requirements\nGenerate ad hoc reports using Power BI or SSRS\nCollaborate with other developers to create and implement best approach solution(s)\nTroubleshoot Azure tools, systems, and software\nReview queries for performance issues, making changes as needed\nCollaborate with team to performance-tune Azure application as necessary\nAssist with the analysis and extraction of relevant information from large amounts of historical business data to feed data science initiatives\nParticipate and support the design and documentation of processes for large scale data analyses, model development, model validation and model implementation\nSupport and maintain a positive safety culture by following all safety policies and procedures and actively contributing to a safe working environment\nOther duties as required\n\n Qualifications \n\nBachelor's degree required. Degree in Computer Science or Mathematics preferred\nMinimum of four years of experience in an IT or analytical role required. Experience in database development, report writing and/or statistics preferred.\n\n EEO \n\nOtter Products, LLC is an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, age, marital status, pregnancy, sex, sexual orientation, gender, gender identity or expression, national origin, disability, veteran status, or any other characteristic or status protected by law.\n\n For US Based Roles Only - Compensation Range Minimum \n\nUSD $90,000.00/Yr.\n\n For US Based Roles Only - Compensation Range Maximum \n\nUSD $120,000.00/Yr.\n\n Additional Total Rewards \n\nProfit Sharing Program Eligible, Benefits Eligible - Full Time- check out otterproducts.com/careers/why for more info"}, {"title": "Software Engineer, Distributed Systems", "company_name": "Hightouch", "location": "United States (Remote)", "link": "/jobs/view/3628572539/?eBP=CwEAAAGMd4QO8APODyE_-mMOPhYyQGg5ZjVbPwq_3V7DK0B2bLAf10pcedM9S2T71X5i4TiJuEwOzWBbSiTKbLuk-VD-_Y-dN5uyf8YpYkHYqEYFxySMS9zr2hRjNrxX9gXbxssh35TFK9YTNV2WeH2Ncb675JgFbJTgNWSZySFC6rrdJGmdSDTr2q5d3VoxORz3GjsDbzjeyZQCvQ07RqYw7oujWSgslycs2LMCY_-N0NEj6e0PvbaYKiZ_5IW34bGVSYLXpHwqa8oOcNttYZ90UA3E_7sDSFzABdIVlfrp5zzERrIYy5sh2MIInxHdDoKgQw8VmOn5uM4S0tXUl-i7LnN0_W0NfPBmLiVDNuveIOm4MfDNPxfe5QKPk6FUAVpvgbcoJn9yRyE&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=rph%2FT6Q6Vkp6YoggJf9cJQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3628572539", "description": "About the job\nAbout Hightouch\n\nHightouch\u2019s mission is to empower everyone to take action on their data. Through our Reverse ETL platform, business and data users can seamlessly sync data from where it resides, such as warehouses and databases, to where it is needed, including operational systems and SaaS tools. Traditionally, acting on data has required engineering time and bandwidth, and left most business users stuck with charts and reports that are unable to take automated action on their data. With Hightouch, every business user, without writing any code, can activate data to streamline critical processes, improve marketing performance, and scale operations.\n\nOur team operates with a focus on making a meaningful impact for our customers. We believe in approaching challenges with a first principles thinking mindset, moving quickly and embracing our value of efficient execution, and treating each other with compassion and kindness. We look for team members that are strong communicators, have a growth mindset, and are motivated and persistent in achieving our goals.\n\nHundreds of companies use Hightouch, including Spotify, Ramp, Retool, NBA, Plaid, and Betterment. We\u2019re based in San Francisco, are remote-friendly, and backed by leading investors such as Amplify Partners, ICONIQ Growth, Bain Capital Ventures, Y-Combinator, and Afore Capital.\n\nAbout The Role\n\nWe are looking for a distributed systems engineer to work on the systems that power the syncing engine our customers and other engineering teams rely on. This presents an exciting challenge where you can apply your expertise in distributed systems, performance optimization, and troubleshooting to push the boundaries of what is possible and meaningful improve our ability to move massive amounts of customer data. This role also provides a unique opportunity to work on a multi-cloud and multi-region infrastructure that supports a global customer base.\n\nWe believe in enabling our engineers to do their best work for our customers by giving them extremely high levels of ownership and autonomy. This comes in different forms: you will own and deliver projects from start to finish, you will work directly with customers to solve their hardest scaling problems, and you will have a lot of influence over what we work on as a team and company.\n\nSome of the problems we\u2019ll be working on include:\n\nSync Speed: Customers want to sync a lot of data to important destinations like Facebook and Snapchat, which requires us to analyze every part of our syncing process and find where we can optimize to sync data more quickly\nStreaming Syncing: We currently sync data from sources in batch only, but envision a world where we also do syncs in real-time and support streaming sources like webhooks and queues\nScalability and Reliability: As part of our rapid growth, we\u2019re always evaluating current future bottlenecks in scaling and reliability, and architecting for the next order of magnitude of growth\nPersonalization API: Our Personalization API product provides a low latency caching layer on top of data warehouses that enable customers to use their data warehouse for real-time personalization use cases with \nMulti-Region and Multi-Cloud: Supporting our multi-region and multi-cloud backend, including extending it to launch Hightouch on in new regions to support data residency requirements of our global customer base\n\nWe are looking for talented, intellectually curious, and motivated individuals who are interested in tackling the problems above. This is a senior role, but we focus on impact and potential for growth more than years of experience. The salary range for this position is $170,000 - $240,000 USD per year, which is location independent in accordance with our remote-first policy.\n\nAbout You\n\nYou are an engineer with a passion for solving hard technical problems that generate real value for customers. You\u2019re motivated by high ownership and are comfortable in a fast-paced, startup environment.\n\nYou have experience and comfort with distributed systems and high-scale systems or the ability and desire to build this experience quickly. This experience can take different forms:\n\nYou\u2019ve built syncing engines that process and move terabytes of data per day in a scalable way\nYou\u2019ve created event collection or data streaming services that power company critical systems\nYou\u2019ve owned systems that have undergone orders of magnitude of growth and helped them scale accordingly\nYou have relevant skills and are a fast learner who is excited to take on the challenges we face\n\nInterview Process\n\nOur goal with the interview process is to balance speed with giving both parties opportunities to assess whether there is a strong mutual fit. We will ask you questions, but we want you to ask us questions! Our technical interviews focus on how you design systems because we believe this is the best way for us to see how you work and for you to see how we collaborate. We don\u2019t ask you to write code to solve technical brainteasers that don\u2019t appear in your day to day job.\n\nRecruiter screen [30m]: Introductory call with our recruiting team to get to know each other and see if the role could be a good mutual fit.\nSystems design screen [45m]: Designing a data processing feature end-to-end.\nHiring manager interview [30m]: Chat with hiring manager about past experiences and future operating preferences to assess fit on company values and operating principles.\nSystems design interview [90m]: Work with the interviewer to architect a system at a conceptual level. The problem will be at a pretty high level - and have both product and customer requirements as well as technical."}, {"title": "Entry-Level Data Scientist Engineer", "company_name": "Phoenix Recruitment LLC", "location": "Texas City, TX (Remote)", "link": "/jobs/view/3788727625/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=8iwtytHsCu4wMwJ4fj3pxw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3788727625", "description": "About the job\nThis is a remote position.\n\nTitle - Entry-Level Data Scientist Engineer, 1 year of project experience\n\nEmployment Type: Full-time\n\nBase Salary: $60K-$70K\n\nPhoenix Recruitment offers a variety of recruiting services to assist both employers and employees. They specialize in marketing open positions, recruiting, and helping employers to find qualified candidates across various industries. Phoenix Recruitment has expertise in streamlining the hiring process. They can help ensure that the process is efficient, well organized, and compliant with relevant regulations.\n\nSkills and Abilities:\n\nStrong knowledge of R or Python for data analysis and modeling. \nProficiency in statistical programs such as R, SAS, MATLAB, or Python. \nFamiliarity with spreadsheets (VBA) and database applications (Access, Oracle, SQL, or equivalent technology). \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \nAbility to learn new methods quickly and work under deadlines. \nExcellent teamwork and communication skills. \nStrong analytical and problem-solving abilities. \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \n\nPreferred:\n\nKnowledge of actuarial concepts and life, health, and/or annuity products. \nExperience with statistical modeling techniques such as GLM, Decision Trees, Time Series, Regression, etc. \nFamiliarity with Microsoft DeployR. \nExposure to insurance risk analysis. \nBasic experience in computational finance, econometrics, statistics, and math. \nKnowledge of SQL and VBA. \nFamiliarity with R or Python for predictive modeling \n\nWhy Phoenix Recruitment LLC?\n\nPhoenix Recruitment often has an extensive network of employers and candidates. This network allows them to tap into a pool of qualified candidates and connect them with suitable job opportunities. They can also leverage their connections to help employers find the right talent efficiently. Outsourcing the recruitment process to a specialized agency can save your time and resources, avoid delays, reduce administrative burdens, and increase the chances of finding the right fit for your organization."}, {"title": "Data Engineer - Remote Work", "company_name": "BairesDev", "location": "United States (Remote)", "link": "/jobs/view/3783162297/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=OtoCrAAHZFomVup04fy35g%3D%3D&trk=flagship3_search_srp_jobs", "id": "3783162297", "description": "About the job\nWho We are\n\nBairesDev is proud to be the fastest-growing company in America. With people in five continents and world-class clients, we are only as strong as the multicultural teams at the heart of our business. To consistently deliver the highest quality solutions to our clients, we only hire the Top 1% of the best talents and nurture their professional growth on exciting projects.\n\nWe are looking for a Data Engineer to join our Development team and participate in different projects made up of multicultural teams distributed throughout the world.\n\nWhat You Will Do\n\n Interview other Data Scientists and Data engineers to vet their technical abilities.\n Input data from those interviews into the product and help develop a predictive, fair, and enjoyable solution.\n Work together with developers, tech leads, and solution architects to build applications.\n Improve existing structures by adding new functionalities or proposing technological updates so that as a result, the impact of your contribution is significant in the core of each business.\n Improve the user experience on scalable and high availability platforms, contributing to the key differential of each business.\n\nHere\u2019s What We Are Looking For\n\n 5+ years of experience working as a developer.\n 3+ years of experience working as a Data Scientist / Data Engineer.\n Proficient with analysis, troubleshooting, and problem-solving.\n Hands-on experience with managing data loads and data quality.\n Advanced English level.\n\nHow we do make your work (and your life) easier:\n\n 100% remote work.\n Hardware setup for you to work from home.\n Flexible hours - make your schedule.\n Paid parental leave, vacation & holidays.\n Diverse and multicultural work environment.\n An innovative environment with the structure and resources of a leading multinational.\n Excellent compensation \u2014 well above the market average.\n Here you can grow at the speed of your learning curve.\n\nOur people work remotely but with a consistent and robust culture that promotes diversity and teamwork. To continue being the leading software development company in Latin America, we want to ensure that every BairesDev member gets the best growth and professional development opportunities in a diverse, welcoming, and innovative environment.\n\nEvery BairesDev team member brings something unique to our company.\n\nWe want to hear your story. Apply now!"}, {"title": "Data Engineer (Remote)", "company_name": "Cascade Debt", "location": "United States (Remote)", "link": "/jobs/view/3755336571/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=pmUtn1j2zyPr25CNPh2RNw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3755336571", "description": "About the job\nDo you want to work in a small team where you can make a real difference in a company? Would you embrace the challenge of building a high impact platform that is used globally? Then we want to speak with you!\n\nAbout Cascade\n\nCascade is a fintech startup backed by Canadian and US investors that empowers companies to grow by democratizing access to institutional debt. We are building tools that modernize how companies raise and manage debt, lowering the barriers to entry to the $7 trillion specialty finance and alternative credit market for companies around the world. Founded just last year, we are gearing up for our next phase of technical development and are seeking a talented Data Engineer to lead the way.\n\nAbout The Role\n\nAs a Data Engineer you\u2019ll be an early member of a growing team building a pioneering platform in the debt infrastructure space. You will design, build, and launch efficient, scalable, and reliable data pipelines to move and transform data .\n\nWhat You\u2019ll Do\n\n Building databases, data lakes and data ingestion pipelines to integrate customer databases and datasets to our systems \n Ingesting financial datasets from external customers, then u pdating and maintaining accurate and complete data mappings to ensure that our product s are displaying high quality data \n M onitoring and alerting across our data pipelines in order to make sure that our data ingests are reliable and correct \n Translat ing business goals and stakeholder requirements into new data flows and deliver analytical insights ensuring that the data flows create real business value continuously \n\nSkills And Experience\n\n 2 + years experience in a data engineering or data analyst role \n E xperience with relational SQL and NoSQL databases, including PostgreSQL , MySQL, MariaDB, MongoDB, Bigtable, etc. \n E xperience working with ETL technologies , such as Databricks, Fivetran , or dbt \n Proficiency in writing SQL queries and knowledge of analytical data warehouses such as RedShift, BigQuery , and Snowflake \n Some experience with CI/CD automation such as GitHub Actions \n Developing APIs and integrating with 3rd party APIs \n Bonus: experience in fintech / SaaS / credit analysis \n\nPeople Who Thrive At Cascade Are\n\n Self-starters, who take the initiative to tackle challenges in a remote work environment \n Good communicators, who can operate without ego to discuss, learn, grow, and help others do the same \n Passionate about creating great digital experiences for users \n Problem solvers who are not satisfied with the status quo \n\nBenefits\n\nRemote: we are a remote-first company and are very flexible on hours as long as things get done \nHome-office: there\u2019s a $1000 USD home office allowance to set yourself up \nEquity: we expect you to have an owner-mentality, and have the equity plan to match \nBenefits: health, dental, vision, and more \nPerks: we offer free lunches weekly and off-site trips \nJob satisfaction: we offer autonomy, ample opportunities for mastery, and an opportunity to make a difference for companies around the world"}, {"title": "Data Engineer", "company_name": "ISC (Integrated Specialty Coverages, LLC)", "location": "Carlsbad, CA (Remote)", "link": "/jobs/view/3778293528/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ew7%2FfEuXSujs5IGabdqUyw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3778293528", "description": "About the job\nAbout Integrated Specialty Coverages\n\nIntegrated Specialty Coverages LLC (ISC) is a growth stage technology and data-driven insurance company leading innovation in the market.\n\nBacked by one of the leading private equity firms, KKR, and led by a forward-thinking management team, ISC is combining the worlds of insurance and technology to create an Insurtech powerhouse. As a leading online distributor of insurance products for a range of industries and \"Main Street USA\", we are looking for the right people to help us in our mission of achieving exponential growth. We strive to be the number one place to go for brokers and agents to source insurance. To accomplish this, we're building a digitally focused team that deeply understands the intersection between user experience, data, and AI/ML to optimize the way we engage with our customers and partners.\n\nJob Summary\n\nThe Data Engineer role will be the technical liaison between multiple groups including a data science team, the engineering team, product management, and business stakeholders. You do not need any insurance knowledge prior, however, you must quickly dive deep into the insurance world and ask questions to become a subject matter expert. You will be responsible for building a data platform to facilitate the data science team. You must be a self-starter that can build out features such as a data pipeline from scratch. There will be support from both engineering and data science for any buildout.\n\nJob Responsibilities\n\nCreate and maintain optimal data pipeline architecture. \nAssemble large, complex data sets that meet functional / non-functional business requirements. \nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. \nBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using AWS technologies, SQL, and Python. \nWork with stakeholders including the Executive, Product, Data Science, and Engineering teams to assist with data-related technical issues and support their data infrastructure needs. \nWork with data science and analytics teams to strive for greater functionality in our data systems. \n\nMinimum Qualifications:\n\nAdvanced working SQL knowledge and experience working with relational databases, strong query authoring (SQL) as well as working familiarity with a variety of databases (Snowflake, Redshift, MySQL, MSSQL, etc.)\nExperience building and optimizing data pipelines, architecture and data sets. \nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. \nBuild processes supporting data transformation, data structures, metadata, dependency, and workload management. \nA successful history of transforming, processing and extracting value from large disconnected, datasets from a variety of data sources (Flat files, Excel, databases, etc.)\nStrong analytic skills related to working with unstructured datasets. \nStrong project management and organizational skills. \nExperience supporting and working with cross-functional teams in a dynamic environment. \nAbility to mentor/guide/collaborate with other team members. \n\nWe are looking for a candidate with 3-5 years of experience in a Data Engineer role. They should also have experience using the following software/tools:\n\nExperience with relational and MPP databases, including Snowflake and MySQL. \nExperience developing software in an agile environment from the requirements stage to production. \nExperience with version control: git\nExperience with container technologies: Docker\nExperience with data pipeline and workflow management tools: Airflow, Jenkins, AWS Glue, Azkaban, Luigi, etc. \nExperience with AWS cloud services: EC2, ECS, Batch, S3, EMR, RDS, Redshift\nExperience with other cloud services: Snowflake, Airflow\nExperience with object-oriented/object function scripting languages: Python, Java, C++, etc. \nExperience with data modeling and data warehouse design\nExperience with data visualization tools (PowerBI, QuickSight)\n\nThe starting annual pay scale for this position is listed below. Actual starting pay will be based on factors such as skills, qualifications, training, and experience. In addition, the company offers comprehensive benefits including medical, dental and vision insurance, 401(k) plan with match, paid time off, and other benefits.\n\nISC's salary ranges are determined by role and level. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations and could be higher or lower based on a multitude of factors, including job-related skills, experience, and relevant education or training.\n\nNational Pay Range\n\n$100,000\u2014$110,000 USD\n\nBenefits of Working at ISC\n\nCompetitive vacation and flexible working arrangements\nComprehensive and inclusive health benefits\nA variety of professional development and mentorship opportunities\nChoice of technology whether at home or in the office\n\nISC's Ownership Behaviors\n\nDo the Right Thing *Be Relentless. Pursue Excellence. *Personal Responsibility & Own Outcomes\nTry Fast. Learn Fast. Fail Fast. Think Big. *Build Extraordinary Owners & Join Forces *Grit & Determination\n\nApplicants with disabilities may contact ISC HR department via e-mail, and other means to request and arrange accommodation. If you need assistance to accommodate a disability, you may request accommodation at any time. Please contact ISC at HR@ISCMGA.com.\n\nISC believes in creating long-term relationships by being responsive and relevant and by consistently delivering value to our community of customers. Specifically, with our employees, we focus on attracting, developing, and retaining the best talent for our business, challenging our people, demonstrating a \"can-do\" attitude, and fostering a collaborative and mutually supportive environment. ISC is an equal opportunity employer and a participant in the US federal E-Verify program (US).\n\nDiversity creates a healthier atmosphere: All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, gender, gender identity, sexual orientation, marital status, medical condition, genetic information, mental or physical disability, military or veteran status, or any other characteristic protected by local, state, or Federal law."}, {"title": "Data Engineer -Remote", "company_name": "RIT Solutions, Inc.", "location": "Denver, CO (Remote)", "link": "/jobs/view/3768007496/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=1hQH8EDaII1eJ7nQ9BACQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3768007496", "description": "About the job\nDuration: 6+ months CTH\n\nMust have a valid LinkedIn profile\n\nRequired Skills\n\n 3-6 years of experience (no more than 6 years)\n Start up environment experience\n Python, SQL, and DBT"}, {"title": "Big Data Engineer - W2", "company_name": "Megan Soft Inc", "location": "United States (Remote)", "link": "/jobs/view/3674845652/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fCAxh4N%2F4YZJ5aEIASaeeA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3674845652", "description": "About the job\nTitle: Big Data Engineer - W2\n\nLocation: Dearborn, MI (Remote)\n\nDuration: 12+ Months\n\nDirect Client: Ford Motors\n\nMust have skills: Should be able to handle GCP requirements\n\n\"Translates business requirements to build repeatable, sustainable, efficient, coded processes that can be productionized by Software Engineers and readily modified by other Data Engineer Creates POC processes in Dev/QA and works with Software Engineers to productionize and align best coding and process practices that maximize efficiency, speed, stability, system resources and capabilities Awareness of and compliance with data privacy, security, legal and contractual guidelines Incorporates data quality and privacy checks/alerts to minimize bad data being consumed by end users, models and dashboards, and to protect customer data; revises checks for new data issues Validates data products and pipelines are functioning as expected following system or application upgrades, source changes, etc. Works closely with Architects if required to align systems, tools and applications being utilized with business use case and performance requirements Provides data product support and maintenance Establishes, tracks and monitors KPIs related to specific data products and deliverable Participate in iteration cadence activities (Iteration Planning, task estimation, daily scrum, demos, etc.). Will be a part of on-call team to resolve critical production support tickets during business hours and off hours.\n\nSkills Required\n\nExperience with Big Data Technologies (Hadoop , ETL Tool, Spark. Hive etc..) Someone who understands Cloud and has experience working with Cloud Platforms (preferably GCP) Should be able to handle GCP requirements - ingesting into GCS/ Big Query, working with GCP components Hands-on in Data Flow, Cloud Storage (GCS), Big Query and GitHub. Proficient in Python and SQL.\n\nSkills Preferred\n\nManages ETL: uses programming and tools for data ingestion, configures pipelines, applies transformations and decoding, integrates and fuses data, moves and securely deliver Experience NoSQL databases\n\nAdditional Information\n\nIf the candidate is remote only, please indicate \"\"100% Remote\"\" under candidate's name on resume. Other similar schedule notations can be \"\"Local Candidate\"\" or \"\"Hybrid Candidate.\"\" ***HRA Test Required***\""}, {"title": "Remote Opportunity: Quantexa Data Engineer", "company_name": "SPAR Information Systems LLC", "location": "United States (Remote)", "link": "/jobs/view/3680073191/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ioykVCZSOQ7BK5%2FwRQHOWg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3680073191", "description": "About the job\nHello All, Hope you are doing great!! Please go through the job descriptions and let me know your interest. Role: Quantexa Data engineer  Location: Remote Duration: Long Term Contract Job Description: 1. End to End usage of Quantexa including guiding our client team on how to use from both front end and data ingestion Reason they are asking for Certification 2. Resolve Entity issues with data 3. Knowledge of Scala and Hive which requires coding experience using parquet 4. Advanced SQL knowledge requiring writing SQL queries 5. Experience having worked with GCP data lake 6. Quantexa certification required\n\nThanks & Regards,\n\nSatnam Singh\n\nDirect: 201 623 3660\n\nEmail : Satnam.singh@sparinfosys.com"}, {"title": "Remote Work - Need Staff Data Engineer - USC only", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3746283980/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=CvIuLBNO%2BPMGi4IXVRkP0A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3746283980", "description": "About the job\n100% Remote, client is in NY\n\nLook for strong job history, Python, Machine Learning, SQL, 10/10 comms, etc. \n\nMUST be an Engineer \n\nSummary\n\nWe're looking for an experienced Data Engineer to join our client's growing team of top notched IT professionals. You will work closely across multiple groups including Editorial and Audio, Marketing, Advertising, etc. to achieve their goals and objectives. The Data Engineering team builds tools throughout the stack while sharing knowledge across these departments.\n\nAs the company grows, they're looking for Data Engineer who will help solidify and expand our pipelines and maintain their data warehouse. Their business model is based on performing complex and very detailed analyses of how our products perform with their subscribers. You'll be responsible for working alongside analysts, product, marketing, and other teams to define new data collection components and measurement schemes that support each new product and feature.\n\nHow Can Add Value To Their Mission\n\nCreate and maintain data pipelines to provide insights and drive business decisions\nEstablish data warehousing strategy (ex. Kimball, Data Vault, etc.)\nMaintain data infrastructure on our AWS accounts\nCollaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.\nWrite unit/integration tests, contributes to engineering wiki, and documents work\nImplement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it\n\nWhat You'll Need To Succeed\n\n7+ years of industry experience measuring product performance and user behavior\nExperience working with a variety of data management technologies, including RedShift, Kinesis/Kafka, Glue, Spark, Postgres, Airflow, dbt, and others.\nExperience implementing BI reporting tools such as Looker\nExperience interfacing with engineers, product managers and analysts to understand data needs\nKnowledge of a variety of measurement beacons, SDKs, APIs including Google Analytics, Amplitude, Braze, Stripe, email service providers\nA commitment to building secure, resilient, fault-tolerant architectures, with clear documentation and procedures in place for support\nA focus on accuracy and detail as you build, ensuring data pipelines produce clear, predictable results\nUnderstanding of the typical metrics a subscription and advertising-supported business needs to measure success\nExperience with Client techniques as applied to behavioral segmentation or anomaly identification is a definite plus\nFamiliarity using developer tools that increase productivity and facilitate the development of resilient code (eg. Docker, CircleCI, Serverless) is a plus"}, {"title": "ENTRY LEVEL DATA ENGINEER (REMOTE)", "company_name": "SynergisticIT", "location": "Jacksonville, FL (Remote)", "link": "/jobs/view/3767587909/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=om7L3CnowiC4PLIcpZD70A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3767587909", "description": "About the job\nSYNERGISTICIT wants every candidate to know that the Job Market is Challenging and to stand out, you need to have exceptional skills and technologies and that's where we come in to make sure you get the attention which you need\n\nSynergisticIT understands the complex nature of the job market and how difficult it can be to secure a position, especially for fresh graduates. Therefore, we assist and help tech-savvies to convert their passions into professions. We go above and beyond to keep you working in your niche.\n\nAs we focus on long-term success, we provide complete career development solutions. From job search to upskilling portfolio and interview preparation, we can guide you at every step of your career.\n\nSynergisticIT spares no efforts to connect you with a large network of tech giants, including Google, Apple, PayPal, Dell, Cisco, Client, etc. Presently, we are actively looking for ENTRY LEVEL DATA ENGINEER a driven mindset. Get the right opportunity and gain experience in building web-centric solutions on Java.\n\nWho Should Apply : Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or anyone looking to make their career in IT Industry\n\nWe also assist in filing for STEM extension and H1b and Green card filing.\n\nCandidates who are serious about their future in the IT Industry and have set big goals for themselves.\n\nCandidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. We also offer Skill Enhancement Programs if the candidates are missing skills or experience which our clients need with great outcomes\n\nCandidates can benefit from skill enhancement if they fall into the below categories. If they are qualified with enough skills then no need for skill enhancement\n\nCandidates Who Lack Experience\n\nHave had a break in careers\n\nLack Technical Competency\n\ncandidates who want to get employed and make a career in the Tech Industry \n\nPlease Also Check The Below Links\n\nhttps://www.synergisticit.com/candidate-outcomes/\n\nhttps://www.synergisticit.com/java-track/\n\nhttps://www.synergisticit.com/data-science-track/\n\nhttps://www.synergisticit.com/which-is-the-best-option-for-tech-job-seekers-staffing-companies-consulting-companies-bootcamps-or-synergisticit/\n\nhttps://www.synergisticit.com/contact-us/\n\nIf the skills are not a match candidates can opt for Skill enhancement. Or their resume can be sent out to clients to see if responses are achievable\n\nREQUIRED SKILLS For Java/Software Programmers\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Core Java , javascript , C++ or software programming\nSpring boot, Microservices and REST API's experience\nExcellent written and verbal communication skills\n\nFor data Science/Machine learning\n\nRequired Skills\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Statistics, Python, data visualization tools\nExcellent written and verbal communication skills\n\nPreferred skills: NLP, Text mining, Tableau, Time series analysis\n\nTechnical skills are required by clients for selection even if its Junior or entry level position each additional Technical skill helps a candidate's resume to be picked by clients over other job seekers.\n\nClients hire candidates with the right technical skills which they need and reject candidates who lack the required technical skills.\n\nNo third party candidates or c2c candidates\n\nPlease apply to the posting\n\nNo phone calls please. Shortlisted candidates would be reached out"}, {"title": "Data Engineer - Remote | WFH", "company_name": "Get It Recruit - Information Technology", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3782241984/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=BgBf1kSAAX%2BSuw6inJsIVw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3782241984", "description": "About the job\nJoin an innovative team at a leading company in the engineering and manufacturing industry, where we are committed to leveraging data for continuous improvement and operational excellence. This hybrid position allows you to enjoy the flexibility of remote work on Mondays and Fridays.\n\nWhy Join Us\n\nAt our company, we prioritize your well-being and professional growth. We offer a comprehensive benefits package, including excellent health benefits, a 6% 401(k) match, generous paid vacation and sick time, 12 paid holidays, and more.\n\nRole Overview\n\nAs a Data Engineer, you will play a pivotal role in transforming raw data into actionable insights, driving improvements in engineering and manufacturing processes, enhancing product quality, and optimizing operational efficiency. Collaborate with dynamic cross-functional teams to extract, analyze, and visualize data, influencing strategic decisions through your valuable insights.\n\nKey Responsibilities\n\nData Collection and Integration:\n\nGather, clean, and integrate data from diverse sources, including production databases, sensor data, and external datasets.\n\nData Analysis\n\nConduct exploratory data analysis to identify patterns, trends, and anomalies.\n\nPerform statistical analysis and hypothesis testing to derive meaningful insights.\n\nData Visualization\n\nCreate informative and visually appealing dashboards and reports using tools like Tableau, Power BI, or Python libraries.\n\nPredictive Modeling\n\nDevelop predictive models to forecast manufacturing performance, quality, and operational outcomes.\n\nRoot Cause Analysis\n\nInvestigate production issues and quality concerns by analyzing data to identify root causes and recommend corrective actions.\n\nPerformance Monitoring\n\nEstablish and maintain key performance indicators (KPIs) to track manufacturing and engineering metrics.\n\nGenerate regular reports to communicate performance trends to stakeholders.\n\nCross-functional Collaboration\n\nCollaborate with engineering, manufacturing, and quality assurance teams to understand their data needs and provide analytical support.\n\nContinuous Improvement\n\nSuggest process improvements and optimizations based on data analysis.\n\nData Security And Compliance\n\nEnsure data security and compliance with relevant regulations, such as GDPR or industry-specific standards.\n\nInfrastructure And Development\n\nBuild and maintain the infrastructure for collecting, storing, and processing data.\n\nDesign, develop, and manage data pipelines, ETL processes, and data warehouses.\n\nRequired Experience\n\nProven experience as a Sr. Data Analyst and/or Data Engineer in an engineering or manufacturing environment.\n\nProficiency in data analysis tools and programming languages, such as Python, R, SQL, and Excel.\n\nStrong data visualization skills with tools like Tableau or Power BI.\n\nExcellent knowledge of statistical analysis and machine learning techniques.\n\nAbility to communicate complex findings and insights effectively to both technical and non-technical stakeholders.\n\nStrong problem-solving skills and attention to detail.\n\nFamiliarity with manufacturing processes and quality control is a plus.\n\nExperience with enterprise ERPs (SAP S4HANA, Oracle EBS), CRMs (Salesforce, Hubspot), and IoT.\n\nProficient in working with cloud computing platforms including AWS, GCP, and Azure.\n\nExpertise in configuring and optimizing cloud-based data solutions for scalability, reliability, and performance.\n\nBachelor's Degree in Data Science, Statistics, Engineering, or Computer Science Required.\n\nApply Now and be part of a team shaping the future of data-driven innovation in engineering and manufacturing!\n\nEmployment Type: Full-Time"}, {"title": "Entry Level Software Engineer - Data Backend Engineer (Remote - Canada)", "company_name": "Yelp", "location": "United States (Remote)", "link": "/jobs/view/3752881757/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fankeoxifvhpTzOYvowHYQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3752881757", "description": "About the job\nYelp engineering culture is driven by our values: we\u2019re a cooperative team that values individual authenticity and encourages creative solutions to problems. All new engineers deploy working code their first week, and we strive to broaden individual impact with support from managers, mentors, and teams. At the end of the day, we\u2019re all about helping our users, growing as engineers, and having fun in a collaborative environment.\n Interested in exploring data and finding innovative ways to collect it, curate it, and make it easily accessible for others to use? How about doing that at an enormous scale? Yelp\u2019s datasets contain billions of interactions between users and local businesses around the globe. If working with our vast amount of data sounds exciting, come and join us!\n\nThe Marketplace Data Observability team is responsible for managing, governing, and monitoring all of Yelp\u2019s Ads data to protect our revenue stream and also support offline analysis that aids in making long term company decisions. We build tools that help us to better understand our data quality, and we strive to uncover data issues before they impact our consumers.\n\nAs a Data Backend Engineer on this team, you will be responsible for building elegant and scalable data products that serve critical, up-to-date, structured information to support different types of analytics within Yelp. As a core contributor to our growing data modeling and data warehousing engineering efforts, you will help design and own mission-critical data flow pipelines and datastores to enable decisions including effective A/B testing and company investments.\n This opportunity is fully remote and does not require you to be located in any particular area in Canada. We welcome applicants from throughout Canada. We\u2019d love to have you apply, even if you don\u2019t feel you meet every single requirement in this posting. At Yelp, we\u2019re looking for great people, not just those who simply check off all the boxes.\n  Build systems that can effectively store and crunch terabytes of data.\nDesign and develop data models for efficient data storage, retrieval, and reporting.\nCreate and maintain conceptual, logical, and physical data models using industry-standard modeling tools.\nCollaborate with cross-functional teams, including engineers, data analysts, business analysts, and data scientists, to understand data requirements and translate them into effective data models.\nParticipate in data integration efforts, including ETL processes and data migration.\nStay up-to-date with industry best practices, emerging technologies, and trends related to data warehousing.\nSupport on-call rotations as needed to operate the team.\n  Understanding of high performing and scalable data systems.\nExperience in building and orchestrating ETL pipelines.\nExperience with Data Lake or Data Warehouse landscape.\nA hunger for tracking down root causes and fixing them in systematic ways.\nAbility to communicate effectively to technical and non-technical cohorts alike. \nExposure to some of the following technologies: Python, AWS Redshift, AWS Athena / Apache Presto, Big Data technologies (e.g S3, Hadoop, Hive, Spark, Flink, Kafka etc), NoSQL systems like Cassandra, DBT."}, {"title": "Data Engineer (Associate Consultant), June 2024", "company_name": "UDig", "location": "Nashville, TN (Remote)", "link": "/jobs/view/3716039960/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=XUjo40kU55HnDCqJ%2F3FdPg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3716039960", "description": "About the job\nCan UDig It? \n\nUDig designs, builds, and implements technology solutions that deliver on business objectives. People join our team because they are excited to have an impact on the future of our organization, passionate about delivering meaningful results, and energized by solving complex problems. Oh, and they like working with other driven, smart, and authentic people!\n\nUDig is proud to be recognized as a \"Best Places to Work\" in Virginia and Tennessee. We're committed to fostering a transparent environment where leadership is accessible, professional development opportunities are abundant, and you are empowered to achieve your career goals. You'll collaborate, develop deep relationships with teammates, and accelerate your success while having fun!\n\nIf you're graduating in May 2024 and excited by the idea of impacting organizations by building data solutions to achieve business goals, you should consider joining our growing technology team.\n\nHow Things Go Down\n\nAs a new, entry-level teammate, you'll take part in our \"Breaking Ground\" training program designed to help you quickly get up-to-speed on UDig's delivery approach within a project team. The program covers:\n\nUDig delivery process and methodologies such as gathering requirements and delivering projects using agile\nTechnical guidance and instruction on the best practices from Data leaders at UDig\nDelivering executive and technical presentations\nCoaching to prepare you to for success on your first client project at UDig\n\nA typical day might entail:\n\nProject standups and check-ins\nDesign and development of data pipelines, BI reports, and more\nPlanning and participating in project demos\nContributing to internal UDig projects\nCareer and job growth through UDig-provided training\nCreating long-lasting friendships with teammates through fun activities as well as your projects (including social events, winning a prize for guessing the weight of pumpkin, or sharing experience on cool tech side projects)\n\nUDig believes it is important for Associate Consultants going through our Breaking Ground program to reside in one of our major markets (Richmond, VA or Nashville, TN metro area) for at least the first year of employment.\n\nWant to learn more? Hear directly from our teammates by checking out our 'A Day in the Life' blog series: https://www.udig.com/digging-in/breaking-ground-2023/\n\nSound Interesting? Here's the Technical Piece\n\nBelow are a few boxes you should be able to check if\u202fyou're interested: Don't check every box? Go ahead and apply!\n\n1+ year of experience with relational database management systems such as Microsoft SQL Server, MySQL, or Oracle\nUnderstanding of the full lifecycle of design, development, and implementation of data solutions including architecture design, development, testing\nSome experience in ETL development or reporting is a plus\nAbility to present technical ideas and high-level concepts and solutions to internal and external team members with varying degrees of technical knowledge\nBachelor's Degree in in computer science, information technology, or any science or business discipline\nAbility to take your work seriously but also seriously enjoy a good time with your UDig colleagues\n\nWhat's in It For You? \n\nAt UDig, you will build your career alongside talented and experienced developers while gaining experience and having fun! We provide continuous challenges and professional development opportunities. Additionally, we offer: \n\nCompetitive salary, merit reviews, and career advancement paths\nFlexible, hybrid environment\nIndividual $1500 Training Budget\nRegular team building and social activities (virtual and/or in-person)\nTransparent culture with strong communication and access to leadership\nGreat benefits like\nGenerous Paid Time Off, company holidays, and parental leave\nMultiple Single and Family Health Insurance plans to choose from\nDental & Vision coverage\nShort-Term and Long-Term disability\nOptional accident and critical illness coverage\nMatching 401(k)\nand more!"}, {"title": "Expression of Interest: Data Engineer", "company_name": "Fingerprint for Success (F4S)", "location": "New York, NY (Remote)", "link": "/jobs/view/3787769880/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=l%2FEDpQqUNmCttGdIyTk7Cg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787769880", "description": "About the job\nWe are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.\n\nThe F4S Talent Pool is a pilot project designed to:\n\nHelp job seekers get discovered by our partners based on their anticipated hiring needs.\nProvide optional support and resources for job seekers in their career endeavors.\nHelp individuals understand, and bring out the best in themselves and each other.\n\nThe F4S Talent Pool process:\n\nOnce you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.\n\nAbout Fingerprint For Success (F4S)\n\nBacked by 20+ years of research, F4S\u2019s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high performing teams.\n\nKeep in mind, joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your F4S work style assessment to match the hiring needs of our partners.\n\nYour feedback is a gift! Write to us via:\n\nPowered by JazzHR\n\noWuPJ9lwq9"}, {"title": "Data Engineer Seattle, WA (remote)", "company_name": "Conch Technologies, Inc", "location": "United States (Remote)", "link": "/jobs/view/3765052211/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=SBETI78UMutCHuRp5FnP%2FA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3765052211", "description": "About the job\nHi,\n\nGreetings from Conch Technologies Inc\n\nData Engineering Lead \u2013 With Databricks and Unity Catalog\n\nLocation \u2013 Seattle, WA (remote)\n\nDuration \u2013 12 months\n\nNeed: Design and implement data architecture and strategy, leveraging technologies such as Databricks, Unity Catalog, Privacera, and Collibra.\n\nAs a Data Engineering Lead, you will play a pivotal role in defining and executing technical deliverables while assessing complexities and levels of effort. You'll engage with both vendor and internal teams to resolve technical challenges and ensure seamless collaboration. Your leadership will extend to planning and prioritization with the SCRUM master, facilitating effective coordination between onsite and offshore teams, and driving feature prioritization through collaboration with product managers.\n\nKey Responsibilities\n\n Lead the definition and breakdown of technical deliverables, assessing complexities and levels of effort.\n Collaborate with vendor and internal teams to address technical issues.\n Drive planning and prioritization in conjunction with the SCRUM master.\n Coordinate with onsite and offshore teams for timely project delivery.\n Collaborate with product managers to prioritize features, communicate with stakeholders, and clarify requirements.\n Collect and manage user-reported issues, ensuring prompt resolution and communication.\n Define comprehensive test plans, oversee their execution, and document results.\n\n--\n\nWith Regards,\n\nNagesh G\n\nMobile: 408-381-5645\n\nDesk: 901-313-3066\n\nEmail: nagesh@conchtech.com \n\nWeb: www.conchtech.com"}, {"title": "Remote Work - Need Azure Fabric Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3759325599/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=C3iVjmcR9wjUip1nNjUiCg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3759325599", "description": "About the job\nThe client is placing heavy emphasis on Microsoft and Azure Fabric experience\n\nInformation Management Engineer \u2013 Take the identified information management sources and assist migration to the Client Data Lake, Azure Fabric. The engineer will transform information management (de-dup, clean and enrich).\n\nTake the identified data sources and bring them into our Data Lake, Azure Fabric. Once the data is in, the engineer will need to be able to transform data (de-dup, clean and enrich the data). We are focused on using a medallion data model for data architecture. The tools we are working with are: Delta Parquet files, python, spark, notebooks, T-SQL, Fabric lakehouse/warehouse, Dataflows Gen2, Data Pipelines, Direct Lake, shortcuts, ADF."}, {"title": "Data Engineer", "company_name": "Catalist", "location": "Washington, DC (Remote)", "link": "/jobs/view/3744951279/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ui1Fk6ICmnPNqAlZDZAcwA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3744951279", "description": "About the job\nFor over 17 years, Catalist has been a leader in civic data and data science innovation. Our mission is to provide progressive organizations with the data, software, and services needed to better identify, understand, and communicate with the people they need to engage and mobilize. Our clients include the largest, most influential organizations in the U.S. active in civic engagement, advocacy, and political campaigns.\n\nCatalist is home to a dedicated, creative team of technologists, data scientists, and campaign experts committed to using our talents and technology to nurture a vibrant and growing progressive community.\n\nAs a Data Engineer at Catalist, you will work closely with the Analytics and Technology departments to design, build, support, and maintain various data pipelines and processes with an end goal of providing data and intelligence to the progressive community. This role primarily involves translating data architecture designs into functional processes, code, and systems.\n\nThe ideal candidate will be a highly motivated individual with excellent technical skills, a strong desire to learn new skills, and an interest in progressive politics. Catalist values creativity and problem-solving. Our work is on the cutting edge of data-driven politics, and your support will help Democratic candidates and progressive organizations conduct successful advocacy and electoral campaigns.\n\nThis position reports to the Deputy Chief Data Officer. The Data Engineer is a part of a growing Data team that supports all underlying work at Catalist.\n\nThis position is included in our CWA bargaining unit.\n\nPrinciple Duties & Responsibilities\n\nDevelop scalable, production processes, code, and systems for data ingest, transformation, modeling, and reporting across a variety of platforms\nTranslate mock-ups and designs into functional processes, code, and systems\nCreate architecture designs when needed\nProvide quality assurance and testing on processes, code, systems, and products\nBecome an internal subject matter expert on various datasets and support other Catalist departments/teams on usage of those datasets\nExecute ad-hoc data and database maintenance tasks as requiredProject manage cross-departmental efforts, with direct responsibility for stakeholder engagement, management, and execution of technical elements\n\n\nRequirements\n\nBS or BA in a technical field, or relevant experience\n1-2 years of experience working with SQL databases\nExperience with data cleaning or ETL processes\nExperience with distributed computing systems and/or distributed datastores (particularly the Hadoop ecosystem)\nExperience managing projects\nFamiliarity with Catalist data, progressive politics, voter files, and/or commercial data\nBackground check required\n\n\nPreferred Skills & Abilities\n\nWillingness to be a problem solver and produce results in a fast paced environment\nAbility to be creative and personable, and articulate ideas clearly\nExperience working with SQL databases\nProficiency with Python or another object-oriented programming language (R, Java, Scala, etc\u2026)\nExperience working in cloud environments (AWS, GCP, etc.)\nExperience working in command line environments such as Bash\nExperience with a version control tool such as git or github\n\n\nBenefits\n\nMedical, Dental, Vision, Prescription Drug\n\nCatalist offers Medical, Dental, Vision, and Prescription Drug coverage for eligible staff and their eligible dependents. Catalist\u2019s Medical plan is a comprehensive PPO program including Prescription Drug coverage with 85% of the premium paid by Catalist. Dental and Vision coverage is provided at no cost to employees.\n\nGroup Term Life Insurance and Long-Term & Short-Term Disability Coverage\n\nGroup Term Life Insurance and Long-Term and Short-Term Disability coverage is available for eligible staff. These benefits are provided at no cost to Catalist employees.\n\n401(k) Safe Harbor Plan\n\nA 401(k) Safe Harbor Plan is available to eligible staff with a 3% contribution from Catalist from the date of hire. Employees may contribute pre-tax or post-tax from their salary up to the legal limits set forth by the IRS.\n\nMedical and Dependent Care Flexible Spending Accounts (FSAs)\n\nCatalist offers an FSA Program that gives eligible staff the ability to pay out-of-pocket medical/dental/vision/child care expenses from pre-tax earnings.\n\nTransit Benefits\n\nCatalist also makes available a Transit benefit FSA program to eligible employees using pre-tax contributions with a company match.\n\nProfessional Development and Remote Work Expenses\n\nEligible employees may be reimbursed up to $750 each year for professional development / education and remote work expenses.\n\nStudent Loan PayDown or SaveUp\n\nCatalist offers a Student Loan PayDown and College SaveUp benefit for eligible staff.\n\nVacation, Personal Leave, Sick Leave Benefits\n\nCatalist offers generous vacation benefits to all eligible staff. Eligible employees also receive:\n\n 14 Paid Holidays\n Personal Days\n Sick Leave\n Parental Leave\n\n\nHybrid Office/Remote Work\n\nCertain positions at Catalist are eligible for Office/Remote Hybrid or full Remote status."}, {"title": "Remote Entry Level Data Analyst/Engineer", "company_name": "SynergisticIT", "location": "White Plains, NY (Remote)", "link": "/jobs/view/3774171423/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=q54sk1%2FRww0z%2BC%2BE8e0jQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774171423", "description": "About the job\nAt SynergisticIT, we're all about making connections. Whatever IT goals you have, our software programmers can help achieve those. Our Software development teams can take up turnkey projects and execute them in an effective and efficient manner. If you are looking to source talent our recruiters will find the ideal IT talent for your company. What's the secret to our success? Well, it all starts with taking quality time to listen to each client's specific needs. After we have a thorough grasp of your IT goals, we can better customize our Developments as per your specific needs. We can also tailor make recruiting programs to exceed your expectations. Since our founding in 2010, SynergisticIT's strategies have earned the company an enviable position in the software development, IT staffing and IT skill enhancement fields. SynergisticIT continues to work with hundreds of satisfied American clients with our software programmers working on our projects and after gaining hands on experience on cutting edge technologies moving to contribute their skills to great clients like Apple, Google, Client, Ebay, Paypal, Kroger, the Walt Disney Company and hundreds more. If you are tired of working with inefficient programmers who take a lot of time to ramp up we want you to try us. Our software programmers can hit the ground running and get you the maximum return on your investment. You have already tried the rest its time you tried the best. SynergisticIT - Home of the Best Data Scientists and Software Programmers in the Bay Area.\n\n Why Us ? \n\nSynergisticIT has a proven track record of successfully skill enhancement and staffing IT employees for some of the world's most iconic brands. Our team takes the time to fully understand every client's needs so we could best meet your IT staffing requirements. The knowledgeable staff at SynergisticIT is always more than happy to work with clients to ensure they reach their software development goals. Besides staffing, SynergisticIT is also committed to helping young IT professionals advance their career with a robust upskill program . Everyone who goes through SynergisticIT's program learns all the skills necessary to succeed in many IT fields ranging from Java to Machine Learning. Additionally, everyone trained at SynergisticIT has been through extensive mock and technical interview screenings to bolster their career prospects. Last, but certainly not least, SynergisticIT takes great care to respect the privacy considerations for every client. All companies who work with SynergisticIT can rest assured their confidential data is protected using the most up-to-date encryption technologies. SynergisticIT also complies with all the latest NDA agreements.\n\n REQUIRED SKILLS For Java /Software Programmers \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Time series analysis \n\n We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2022 and at Gartner Data Analytics Summit (Florida)-2023 \n\nOracle CloudWorld Event (OCW) Las Vegas 2022 | SynergisticIT - YouTube\n\n https://www.youtube.com/watch?v=OAFOhcGy9Z8 \n\n https://www.youtube.com/watch?v=EmO7NrWHkLM \n\n https://www.youtube.com/watch?v=NVBU9RYZ6UI \n\n https://www.youtube.com/watch?v=Yy74yvjatVg \n\nSynergisticIT at Gartner Data and Analytics Summit 2023 - YouTube\n\nFor preparing for interviews please visit  https://www.synergisticit.com/interview-questions/ \n\n We are looking for the right matching candidates for our clients \n\n Please apply via the job posting \n\n REQUIRED SKILLS For Java /Full Stack/Software Programmer \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning Positions \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Tensorflow \n\n If you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team. \n\n No phone calls please.  Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates"}, {"title": "Data Engineer", "company_name": "Anika Systems", "location": "Leesburg, VA (Remote)", "link": "/jobs/view/3787766893/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=mApL9JnN%2B%2Fr2%2BYUPlxRYOA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787766893", "description": "About the job\nAnika Systems is a fast growing, woman-owned small business that specializes in providing innovative IT solutions for federal government agencies. Our expertise lies in accelerating delivery in Data and Analytics, Intelligent Automation, Application Development, and IT Modernization. We are currently expanding our Federal team and are seeking a passionate and talented Data Engineer. This opportunity is 100% remote.\n\nMust be a U.S. Citizen with the ability to obtain and maintain a government suitability clearance\n\nResponsibilities\n\nDesigns, develops, builds, analyzes, evaluates, and installs database management systems to include database modeling and design, relational database architecture, metadata, and repository creation and configuration management.\nDefines and oversees database organizations, standards, controls, procedures, and documentation. Provides technical consulting in the definition, design, and creation of a database environment.\nAdvises applications development staff and users on data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nDesigns and implements databases with respect to access methods, access time, batch processes, device allocation, validation checks, organization, protection and security, documentation, and statistical methods.\nUses data mapping, data mining, and data transformational analysis tools to design and develop databases.\nDetermines data storage and optimum storage requirements.\nPrepares system requirements, source analysis, and process analyses and designs throughout the database implementation.\n\nRequired Skills And Experience\n\nBA/BS and 1 year of relevant experience\nExperience with DataBricks, SQL, Python\nApplications development experience with data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nExcellent communication skills (both oral and written) with process-oriented organizational skills to ensure project success\n\nDesired Skills And Experience\n\nDemonstrated successful development experience with the full life cycle of government database management systems including database modeling and design, relational database architecture, metadata and repository creation, and configuration management.\n\nPowered by JazzHR\n\nTlKWIorucJ"}, {"title": "Remote Work - Need Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3774712978/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=r3Bp%2F0qcOD2OpC5jpbUOcA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774712978", "description": "About the job\nTitle : Data Engineer\n\nLocation : Remote\n\nDuration : 6+ months to start (long term contract (work order are extended 2x year))\n\nPlease Fill out the skill matrix: Technology Some Awareness (1) Novice (2) Intermediate (3) Advanced (4) Expert (5) SQL Tableau Desktop /Creator Tableau API Integration AWS - S3 Glue AWS - Athena Redshift AWS - Lambda AWS - SNS/SQS msExcel\n\nProfile: Data Analyst will work closely with Architects and leads in the Interflow team, key business stakeholders including program staff to support the organizations effort in the areas of data Analysis and information management. Role should have strong experience in writing SQL queries to extract data for reports. Candidate should have experience working with large set of data. As these reports are very critical for the company and we are looking for an expert who is capable of handling, consuming and extracting large sets of data for analysis and reports.\n\nMust Haves\n\nAWS - Athena\nAWS - S3\nGlue\nhands-on development of reporting applications\nMS Excel\nRedShift\nSQL\nTableau Desktop /Creator\n\nNice\n\nAWS - Lambda\nAWS - SNS/SQS\nTableau API Integration\n\nSkills\n\nBA/BS required (major in an analytical field desired) \nA minimum 10+ years work-related experience. \nExperience in hands-on development of reporting applications for the Web in a professional environment \nThe Ethos of continuous improvement and interest in learning new things.\nStrong analytical thinking and structured problem-solving ability\nAbility to handle multiple projects and assignments simultaneously and effectively in a cross-functional team environment. \nVersed on the agile methodology and best practices. \nExcellent interpersonal and collaboration skill with the ability to work with a diverse set of colleagues, across functions, from different organizations, disciplines, etc. \nSelf-starter, ability to set priorities, work independently and attain goals"}, {"title": "Data Engineer", "company_name": "Team Remotely Inc", "location": "Houston, TX (Remote)", "link": "/jobs/view/3786249829/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=78lY5Jo%2FfyRPC6TQYCdvww%3D%3D&trk=flagship3_search_srp_jobs", "id": "3786249829", "description": "About the job\nThis is a remote position.\n\n Data Engineer (US/Canada Residents Only, 1 year experience, remote)\n\nTeam Remotely Inc. is a staffing and recruitment agency that offers a comprehensive solution for talent acquisition, including sourcing, vetting, pay rolling, and managing talent. Whether you need contract staffing, direct hire, direct sourcing, talent pools, or diversity initiatives, our model can support your hiring strategy.\n\n Hiring Type: Full-Time\n\n Base Salary: $55K-$70K Per Annum.\n\nResponsibilities\n\nDesigning user interface changes for web-based DB applications. \nReviewing application requirements and interface designs. \nDeveloping and implementing highly responsive user interface components using react concepts. \nWriting application interface codes using JavaScript following react.js workflows. \nTroubleshooting interface software and debugging application codes. \nDeveloping and implementing front-end architecture to support user interface concepts. \nMonitoring and improving front-end performance. \n\nQualifications And Experience\n\nBachelor\u2019s degree in computer science, information technology, or a similar field. \nPrevious experience working as a react.js developer. \nIn-depth knowledge of JavaScript, CSS, HTML, jQuery, and front-end languages. \nKnowledge of REACT tools including React.js, Webpack, Enzyme, Redux, and Flux. \nExperience with user interface design. \nKnowledge of performance testing frameworks, including Mocha and Jest. \nExperience with browser-based debugging and performance testing software. \nTroubleshooting skills. \nProject management skills. \nProblem-solving skills. \nVerbal communication skills. \n\n Why work with Team Remotely?\n\nTeam Remotely Inc. is a staffing platform offering a seamless experience for employers and candidates. Employers can post job openings and specify their requirements, while candidates can create profiles and upload resumes.\n\nThe team of Team Remotely continuously learns and adapts based on previous successful placements, constantly improving its matching capabilities. This ensures that the recommendations provided by Team Remotely are tailored and accurate, increasing the likelihood of a successful match between employers and candidates. By providing intelligent and data-driven solutions, they strive to enhance the efficiency and effectiveness of the hiring process, ultimately helping companies find the best talent and individuals find their dream jobs."}, {"title": "Data Engineer", "company_name": "ZAG Technical Services", "location": "United States (Remote)", "link": "/jobs/view/3771403464/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=Hf0VefrKXBFIL5MtHmY0Pw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3771403464", "description": "About the job\nWe are looking for a tech savvy Data Engineer to contribute to the building, operating, and scaling of next generation data platforms and tools that will power data-driven analytics capabilities throughout the ZAG products and services portfolio, spanning areas such as business intelligence, reporting, and data analytics.\n\nThe ideal team member is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. Must be comfortable supporting the data needs of multiple clients and teams, systems, and products within the modernization of ecosystems. The right-fit person will be excited by the prospect of optimizing or even re-designing data processes to support next generation of advanced analytics capabilities, which in-turn translates to implementing strategic enhancements to maintain competitive advantage in the industry.\n\nThe successful team member will leverage their proficiency to\u2026\n\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\nBuild the analytics processes required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Data Management and cloud based \u2018big data\u2019 technologies.\nBuild analytics tools that utilize technologies to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.\nCollaborate with client teams to identify and solution regional and local requirements within the context of the standard global business model for enterprise analytics.\nParticipates in meetings, discussions, strategy sessions where changes, improvements, and enhancements are proposed, evaluated, and approved. \n\n\nThe exceptional team member will have or demonstrate progressive experiences in\u2026 \n\nBachelor\u2019s degree or higher in Computer Science or related field\nExperience working with BI and data warehousing tools\nExperience working with enterprise data, where security is paramount and data governance is critical\nAbility to work independently and in a team environment, and effectively engage all levels of the organization\nUnderstanding of distributed systems driving large-scale data processing and analytics with a successful history of manipulating, processing, and extracting value from large, disconnected datasets.\nAbility to communicate effectively (listening, presenting, and questioning)\nStrong organizational, written, and communication skills\n\n\nAbout ZAG\n\nZAG Technical Services empowers our clients\u2019 success through intelligent business solutions that increase employee productivity, network security and team collaboration. We aim to ensure that our client\u2019s IT investments drive ROI, provide a competitive advantage and help businesses grow. We do this by living our core set of values: Integrity Foremost, Client-Centered, Accountable Always, Teamwork Throughout, and Exceptional is the Goal.\n\nWhat ZAG Has To Offer\n\nCompetitive compensation package \nBenefits including medical, dental, vision, 401K and life insurance \n\n\nZAG Technical Services is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all of our employees.\n\nZAG Technical Services is an E-Verify participant and will provide the Social Security Administration (SSA) and, if necessary, the Department of Homeland Security (DHS), with information from each new employee\u2019s Form I-9 to confirm work authorization."}, {"title": "Junior Data Engineer", "company_name": "HireMeFast LLC - Career Accelerator - Land A Job", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3786249797/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D&trk=flagship3_search_srp_jobs&lipi=urn%3Ali%3Apage%3Ad_flagship3_search_srp_jobs%3BQExogrHBQoSF9zZm%2B9kufw%3D%3D&lici=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D", "id": "3786249797", "description": "About the job\nThis is a remote position.\n\n Job Title: Junior Data Engineer\n\n Employment Type: Full-Time\n\n Salary: $64,000 - $76,000 per annum\n\n Experience Required:  Minimum 1 year of project experience\n\nAbout us: HireMeFast is a leading staffing and recruitment agency specializing in connecting businesses with top-tier talent across various industries. Our mission is to bridge the gap between exceptional candidates and organizations needing their skills, expertise, and unique qualities. Our team of experienced and dedicated recruitment specialists utilizes innovative sourcing strategies, and a vast network to identify and attract top talent. We conduct comprehensive procedures to ensure that only the most qualified candidates are presented to our clients.\n\nPosition Summary\n\nJoin the fast-paced, innovative, and collaborative environment focused on providing an AIOps platform that enhances the intelligence of the CVS Health infrastructure. Work closely with subject matter experts and colleagues to build and scale out machine learning and AI solutions that will detect, predict, and recommend solutions to correct issues before system impact and enhance efficiency, reliability, and performance of CVS Health\u2019s IT operations.\n\nKey Responsibilities include:\n\nData pipeline development: Designed, implemented, and managed data pipelines for extracting, transforming, and loading data from various sources into data lakes for processing, analytics, and correlation. \nData modeling: Create and maintain data models ensuring data quality, scalability, and efficiency \nDevelop and automate processes to clean, transform, and prepare data for analytics, ensuring data accuracy and consistency \nData Integration: Integrate data from disparate sources, both structured and unstructured to provide a unified view of key infrastructure platform and application data \nUtilize big data technologies such as Kafka to process and analyze large volumes of data efficiently \nImplement data security measures to protect sensitive information and ensure compliance with data and privacy regulation \nCreate/maintain documentation for data processes, data flows, and system configurations \nPerformance Optimization- Monitor and optimize data pipelines and systems for performance, scalability and cost-effectiveness \n\nCharacteristics of this role:\n\nTeam Player: Willing to teach, share knowledge, and work with others to make the team successful. \nCommunication: Exceptional verbal, written, organizational, presentation, and communication skills. \nCreativity: Ability to take written and verbal requirements and come up with other innovative ideas. \nAttention to detail: Systematically and accurately research future solutions and current problems. \nStrong work ethic: The innate drive to do work extremely well. \nPassion: A drive to deliver better products and services than expected to customers. \n\nRequired Qualifications\n\n2+ years of programming experience in languages such as Python, Java, SQL \n2+ years of experience with ETL tools and database management (relational, non-relational) \n2+ years of experience in data modeling techniques and tools to design efficient scalable data structures \nSkills in data quality assessment, data cleansing, and data validation \n\nQualifications\n\nKnowledge of big data technologies and cloud platforms \nExperience with technologies like PySpark, Databricks, and Azure Synapse. \n\nEducation\n\nBachelor\u2019s degree in Computer Science, Information Technology, or related field, or equivalent working experience\n\nWhy HireMeFast LLC?\n\nAt HireMeFast, we understand that finding the right individuals to join your team is crucial for success and growth of your organization. We are committed to streamlining the hiring process for our clients, ensuring they have access to a diverse pool of highly qualified candidates who are a perfect fit for their specific needs."}, {"title": "Data Engineer  - Need candidates from Texas ( REMOTE )", "company_name": "Pulivarthi Group (PG)", "location": "United States (Remote)", "link": "/jobs/view/3770685212/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=9gdMwuif%2BpB%2BCQYvQi3imQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3770685212", "description": "About the job\nFollow us on Linkedin: https://www.linkedin.com/company/pulivarthigroup/\n\nPulivarthi Group LLC is a Global Staffing & IT Technology Solutions company, with our prime focus of providing world class solutions to our customers with the right talent. We combine the expertise of our team and the culture of your company to help you with the solution that is affordable and innovative using high quality standards and technologies.\n\nWe\u2019ve served some of the largest healthcare, financial services, and government entities in the U.S.\n\nData Engineer - FT only - Azure SQL with data lakes experience\n\nData Engineer -\n\nthe resource requirement for this Data Engineer role needed more clarification.\n\nOur customer is requesting a resource who knows Azure CI/CD (Continuous Integration / Continuous Deployment) to build and integrate their DevOps pipeline.\n\nOnce the DevOps pipelines are mapped, they need the Data Engineer to connect to all the Data Sources, primarily their Data Lake.\n\nThe Customer (based in Houston) is interested in a resource in a time zone not too far removed from the Central Time Zone to allow real time collaboration."}, {"title": "Data Engineer", "company_name": "Otter Products", "location": "Fort Collins, CO (Remote)", "link": "/jobs/view/3766728302/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=77U3U7CAi1PxgDmpI3%2BiNQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3766728302", "description": "About the job\nOverview \n\nOtter Products is currently recruiting for a Data Engineer to join our Data team! You can be based in our Fort Collins, CO office with flexibility to work a portion of your time remotely, or 100% Remote in the U.S.\n\nThe Data Engineer applies professional experience, concepts, and company objectives toward building and operationalizing the minimally inclusive data necessary for the enterprise data initiatives following industry standard practices and tools. The bulk of the Data Engineer's work would be in designing, managing and optimizing data pipelines, then moving these data pipelines effectively into production for key data and analytics consumers like business/data analysts, data scientists or other stakeholder that needs curated data for data and analytics use cases across the enterprise. The Data Engineer also ensures compliance with data governance and data security requirements while creating, improving and operationalizing these integrated and reusable data pipelines to enable faster data access, integrated data reuse and vastly improved time-to-solution for data and analytics initiatives. Additionally, the Data Engineer will be expected to collaborate with other data team members and data consumers working on the models and algorithms developed by them toward optimization for data quality, security, and governance to put them into production leading to potentially large productivity gains.\n\n About Otter Products \n\nOtter Products, we grow to give. From our founder's garage in 1998 to the global technology leader we are today, Otter Products continues to drive growth through innovation.\n\nThrough our industry-leading brands - OtterBox, Liviri and OtterCares - we provide our partners the number-one selling and most trusted products in our categories. Our philanthropic spirit is the foundation on which we foster our partner relationships, allowing us to grow and to give - together.\n\nBy way of our charitable arm, the OtterCares Foundation, we support our communities and invest in our future through education that inspires kids to change the world.\n\nAnd even as our global community of Otters continues to grow, our founder's core values are still at the heart of everything we do. We measure our success by our ability to give back to our communities and strengthen opportunities for all.\n\nFor more information visit otterproducts.com\n\n Responsibilities \n\nComplete analysis, design and development of BI solutions using Azure SQL Server\nFamiliarity with Data Warehouse concepts\nExperience coding the extraction, transformation, and loading (ETL) processes\nDatabase development primarily in SSIS, Data Factory and SQL\nPartner with the business to determine end user database/reporting needs and requirements\nGenerate ad hoc reports using Power BI or SSRS\nCollaborate with other developers to create and implement best approach solution(s)\nTroubleshoot Azure tools, systems, and software\nReview queries for performance issues, making changes as needed\nCollaborate with team to performance-tune Azure application as necessary\nAssist with the analysis and extraction of relevant information from large amounts of historical business data to feed data science initiatives\nParticipate and support the design and documentation of processes for large scale data analyses, model development, model validation and model implementation\nSupport and maintain a positive safety culture by following all safety policies and procedures and actively contributing to a safe working environment\nOther duties as required\n\n Qualifications \n\nBachelor's degree required. Degree in Computer Science or Mathematics preferred\nMinimum of four years of experience in an IT or analytical role required. Experience in database development, report writing and/or statistics preferred.\n\n EEO \n\nOtter Products, LLC is an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, age, marital status, pregnancy, sex, sexual orientation, gender, gender identity or expression, national origin, disability, veteran status, or any other characteristic or status protected by law.\n\n For US Based Roles Only - Compensation Range Minimum \n\nUSD $90,000.00/Yr.\n\n For US Based Roles Only - Compensation Range Maximum \n\nUSD $120,000.00/Yr.\n\n Additional Total Rewards \n\nProfit Sharing Program Eligible, Benefits Eligible - Full Time- check out otterproducts.com/careers/why for more info"}, {"title": "Software Engineer, Distributed Systems", "company_name": "Hightouch", "location": "United States (Remote)", "link": "/jobs/view/3628572539/?eBP=CwEAAAGMd4QO8APODyE_-mMOPhYyQGg5ZjVbPwq_3V7DK0B2bLAf10pcedM9S2T71X5i4TiJuEwOzWBbSiTKbLuk-VD-_Y-dN5uyf8YpYkHYqEYFxySMS9zr2hRjNrxX9gXbxssh35TFK9YTNV2WeH2Ncb675JgFbJTgNWSZySFC6rrdJGmdSDTr2q5d3VoxORz3GjsDbzjeyZQCvQ07RqYw7oujWSgslycs2LMCY_-N0NEj6e0PvbaYKiZ_5IW34bGVSYLXpHwqa8oOcNttYZ90UA3E_7sDSFzABdIVlfrp5zzERrIYy5sh2MIInxHdDoKgQw8VmOn5uM4S0tXUl-i7LnN0_W0NfPBmLiVDNuveIOm4MfDNPxfe5QKPk6FUAVpvgbcoJn9yRyE&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=rph%2FT6Q6Vkp6YoggJf9cJQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3628572539", "description": "About the job\nAbout Hightouch\n\nHightouch\u2019s mission is to empower everyone to take action on their data. Through our Reverse ETL platform, business and data users can seamlessly sync data from where it resides, such as warehouses and databases, to where it is needed, including operational systems and SaaS tools. Traditionally, acting on data has required engineering time and bandwidth, and left most business users stuck with charts and reports that are unable to take automated action on their data. With Hightouch, every business user, without writing any code, can activate data to streamline critical processes, improve marketing performance, and scale operations.\n\nOur team operates with a focus on making a meaningful impact for our customers. We believe in approaching challenges with a first principles thinking mindset, moving quickly and embracing our value of efficient execution, and treating each other with compassion and kindness. We look for team members that are strong communicators, have a growth mindset, and are motivated and persistent in achieving our goals.\n\nHundreds of companies use Hightouch, including Spotify, Ramp, Retool, NBA, Plaid, and Betterment. We\u2019re based in San Francisco, are remote-friendly, and backed by leading investors such as Amplify Partners, ICONIQ Growth, Bain Capital Ventures, Y-Combinator, and Afore Capital.\n\nAbout The Role\n\nWe are looking for a distributed systems engineer to work on the systems that power the syncing engine our customers and other engineering teams rely on. This presents an exciting challenge where you can apply your expertise in distributed systems, performance optimization, and troubleshooting to push the boundaries of what is possible and meaningful improve our ability to move massive amounts of customer data. This role also provides a unique opportunity to work on a multi-cloud and multi-region infrastructure that supports a global customer base.\n\nWe believe in enabling our engineers to do their best work for our customers by giving them extremely high levels of ownership and autonomy. This comes in different forms: you will own and deliver projects from start to finish, you will work directly with customers to solve their hardest scaling problems, and you will have a lot of influence over what we work on as a team and company.\n\nSome of the problems we\u2019ll be working on include:\n\nSync Speed: Customers want to sync a lot of data to important destinations like Facebook and Snapchat, which requires us to analyze every part of our syncing process and find where we can optimize to sync data more quickly\nStreaming Syncing: We currently sync data from sources in batch only, but envision a world where we also do syncs in real-time and support streaming sources like webhooks and queues\nScalability and Reliability: As part of our rapid growth, we\u2019re always evaluating current future bottlenecks in scaling and reliability, and architecting for the next order of magnitude of growth\nPersonalization API: Our Personalization API product provides a low latency caching layer on top of data warehouses that enable customers to use their data warehouse for real-time personalization use cases with \nMulti-Region and Multi-Cloud: Supporting our multi-region and multi-cloud backend, including extending it to launch Hightouch on in new regions to support data residency requirements of our global customer base\n\nWe are looking for talented, intellectually curious, and motivated individuals who are interested in tackling the problems above. This is a senior role, but we focus on impact and potential for growth more than years of experience. The salary range for this position is $170,000 - $240,000 USD per year, which is location independent in accordance with our remote-first policy.\n\nAbout You\n\nYou are an engineer with a passion for solving hard technical problems that generate real value for customers. You\u2019re motivated by high ownership and are comfortable in a fast-paced, startup environment.\n\nYou have experience and comfort with distributed systems and high-scale systems or the ability and desire to build this experience quickly. This experience can take different forms:\n\nYou\u2019ve built syncing engines that process and move terabytes of data per day in a scalable way\nYou\u2019ve created event collection or data streaming services that power company critical systems\nYou\u2019ve owned systems that have undergone orders of magnitude of growth and helped them scale accordingly\nYou have relevant skills and are a fast learner who is excited to take on the challenges we face\n\nInterview Process\n\nOur goal with the interview process is to balance speed with giving both parties opportunities to assess whether there is a strong mutual fit. We will ask you questions, but we want you to ask us questions! Our technical interviews focus on how you design systems because we believe this is the best way for us to see how you work and for you to see how we collaborate. We don\u2019t ask you to write code to solve technical brainteasers that don\u2019t appear in your day to day job.\n\nRecruiter screen [30m]: Introductory call with our recruiting team to get to know each other and see if the role could be a good mutual fit.\nSystems design screen [45m]: Designing a data processing feature end-to-end.\nHiring manager interview [30m]: Chat with hiring manager about past experiences and future operating preferences to assess fit on company values and operating principles.\nSystems design interview [90m]: Work with the interviewer to architect a system at a conceptual level. The problem will be at a pretty high level - and have both product and customer requirements as well as technical."}, {"title": "Entry-Level Data Scientist Engineer", "company_name": "Phoenix Recruitment LLC", "location": "Texas City, TX (Remote)", "link": "/jobs/view/3788727625/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=8iwtytHsCu4wMwJ4fj3pxw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3788727625", "description": "About the job\nThis is a remote position.\n\nTitle - Entry-Level Data Scientist Engineer, 1 year of project experience\n\nEmployment Type: Full-time\n\nBase Salary: $60K-$70K\n\nPhoenix Recruitment offers a variety of recruiting services to assist both employers and employees. They specialize in marketing open positions, recruiting, and helping employers to find qualified candidates across various industries. Phoenix Recruitment has expertise in streamlining the hiring process. They can help ensure that the process is efficient, well organized, and compliant with relevant regulations.\n\nSkills and Abilities:\n\nStrong knowledge of R or Python for data analysis and modeling. \nProficiency in statistical programs such as R, SAS, MATLAB, or Python. \nFamiliarity with spreadsheets (VBA) and database applications (Access, Oracle, SQL, or equivalent technology). \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \nAbility to learn new methods quickly and work under deadlines. \nExcellent teamwork and communication skills. \nStrong analytical and problem-solving abilities. \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \n\nPreferred:\n\nKnowledge of actuarial concepts and life, health, and/or annuity products. \nExperience with statistical modeling techniques such as GLM, Decision Trees, Time Series, Regression, etc. \nFamiliarity with Microsoft DeployR. \nExposure to insurance risk analysis. \nBasic experience in computational finance, econometrics, statistics, and math. \nKnowledge of SQL and VBA. \nFamiliarity with R or Python for predictive modeling \n\nWhy Phoenix Recruitment LLC?\n\nPhoenix Recruitment often has an extensive network of employers and candidates. This network allows them to tap into a pool of qualified candidates and connect them with suitable job opportunities. They can also leverage their connections to help employers find the right talent efficiently. Outsourcing the recruitment process to a specialized agency can save your time and resources, avoid delays, reduce administrative burdens, and increase the chances of finding the right fit for your organization."}, {"title": "Data Engineer - Remote Work", "company_name": "BairesDev", "location": "United States (Remote)", "link": "/jobs/view/3783162297/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=OtoCrAAHZFomVup04fy35g%3D%3D&trk=flagship3_search_srp_jobs", "id": "3783162297", "description": "About the job\nWho We are\n\nBairesDev is proud to be the fastest-growing company in America. With people in five continents and world-class clients, we are only as strong as the multicultural teams at the heart of our business. To consistently deliver the highest quality solutions to our clients, we only hire the Top 1% of the best talents and nurture their professional growth on exciting projects.\n\nWe are looking for a Data Engineer to join our Development team and participate in different projects made up of multicultural teams distributed throughout the world.\n\nWhat You Will Do\n\n Interview other Data Scientists and Data engineers to vet their technical abilities.\n Input data from those interviews into the product and help develop a predictive, fair, and enjoyable solution.\n Work together with developers, tech leads, and solution architects to build applications.\n Improve existing structures by adding new functionalities or proposing technological updates so that as a result, the impact of your contribution is significant in the core of each business.\n Improve the user experience on scalable and high availability platforms, contributing to the key differential of each business.\n\nHere\u2019s What We Are Looking For\n\n 5+ years of experience working as a developer.\n 3+ years of experience working as a Data Scientist / Data Engineer.\n Proficient with analysis, troubleshooting, and problem-solving.\n Hands-on experience with managing data loads and data quality.\n Advanced English level.\n\nHow we do make your work (and your life) easier:\n\n 100% remote work.\n Hardware setup for you to work from home.\n Flexible hours - make your schedule.\n Paid parental leave, vacation & holidays.\n Diverse and multicultural work environment.\n An innovative environment with the structure and resources of a leading multinational.\n Excellent compensation \u2014 well above the market average.\n Here you can grow at the speed of your learning curve.\n\nOur people work remotely but with a consistent and robust culture that promotes diversity and teamwork. To continue being the leading software development company in Latin America, we want to ensure that every BairesDev member gets the best growth and professional development opportunities in a diverse, welcoming, and innovative environment.\n\nEvery BairesDev team member brings something unique to our company.\n\nWe want to hear your story. Apply now!"}, {"title": "Data Engineer (Remote)", "company_name": "Cascade Debt", "location": "United States (Remote)", "link": "/jobs/view/3755336571/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=pmUtn1j2zyPr25CNPh2RNw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3755336571", "description": "About the job\nDo you want to work in a small team where you can make a real difference in a company? Would you embrace the challenge of building a high impact platform that is used globally? Then we want to speak with you!\n\nAbout Cascade\n\nCascade is a fintech startup backed by Canadian and US investors that empowers companies to grow by democratizing access to institutional debt. We are building tools that modernize how companies raise and manage debt, lowering the barriers to entry to the $7 trillion specialty finance and alternative credit market for companies around the world. Founded just last year, we are gearing up for our next phase of technical development and are seeking a talented Data Engineer to lead the way.\n\nAbout The Role\n\nAs a Data Engineer you\u2019ll be an early member of a growing team building a pioneering platform in the debt infrastructure space. You will design, build, and launch efficient, scalable, and reliable data pipelines to move and transform data .\n\nWhat You\u2019ll Do\n\n Building databases, data lakes and data ingestion pipelines to integrate customer databases and datasets to our systems \n Ingesting financial datasets from external customers, then u pdating and maintaining accurate and complete data mappings to ensure that our product s are displaying high quality data \n M onitoring and alerting across our data pipelines in order to make sure that our data ingests are reliable and correct \n Translat ing business goals and stakeholder requirements into new data flows and deliver analytical insights ensuring that the data flows create real business value continuously \n\nSkills And Experience\n\n 2 + years experience in a data engineering or data analyst role \n E xperience with relational SQL and NoSQL databases, including PostgreSQL , MySQL, MariaDB, MongoDB, Bigtable, etc. \n E xperience working with ETL technologies , such as Databricks, Fivetran , or dbt \n Proficiency in writing SQL queries and knowledge of analytical data warehouses such as RedShift, BigQuery , and Snowflake \n Some experience with CI/CD automation such as GitHub Actions \n Developing APIs and integrating with 3rd party APIs \n Bonus: experience in fintech / SaaS / credit analysis \n\nPeople Who Thrive At Cascade Are\n\n Self-starters, who take the initiative to tackle challenges in a remote work environment \n Good communicators, who can operate without ego to discuss, learn, grow, and help others do the same \n Passionate about creating great digital experiences for users \n Problem solvers who are not satisfied with the status quo \n\nBenefits\n\nRemote: we are a remote-first company and are very flexible on hours as long as things get done \nHome-office: there\u2019s a $1000 USD home office allowance to set yourself up \nEquity: we expect you to have an owner-mentality, and have the equity plan to match \nBenefits: health, dental, vision, and more \nPerks: we offer free lunches weekly and off-site trips \nJob satisfaction: we offer autonomy, ample opportunities for mastery, and an opportunity to make a difference for companies around the world"}, {"title": "Data Engineer", "company_name": "ISC (Integrated Specialty Coverages, LLC)", "location": "Carlsbad, CA (Remote)", "link": "/jobs/view/3778293528/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ew7%2FfEuXSujs5IGabdqUyw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3778293528", "description": "About the job\nAbout Integrated Specialty Coverages\n\nIntegrated Specialty Coverages LLC (ISC) is a growth stage technology and data-driven insurance company leading innovation in the market.\n\nBacked by one of the leading private equity firms, KKR, and led by a forward-thinking management team, ISC is combining the worlds of insurance and technology to create an Insurtech powerhouse. As a leading online distributor of insurance products for a range of industries and \"Main Street USA\", we are looking for the right people to help us in our mission of achieving exponential growth. We strive to be the number one place to go for brokers and agents to source insurance. To accomplish this, we're building a digitally focused team that deeply understands the intersection between user experience, data, and AI/ML to optimize the way we engage with our customers and partners.\n\nJob Summary\n\nThe Data Engineer role will be the technical liaison between multiple groups including a data science team, the engineering team, product management, and business stakeholders. You do not need any insurance knowledge prior, however, you must quickly dive deep into the insurance world and ask questions to become a subject matter expert. You will be responsible for building a data platform to facilitate the data science team. You must be a self-starter that can build out features such as a data pipeline from scratch. There will be support from both engineering and data science for any buildout.\n\nJob Responsibilities\n\nCreate and maintain optimal data pipeline architecture. \nAssemble large, complex data sets that meet functional / non-functional business requirements. \nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. \nBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using AWS technologies, SQL, and Python. \nWork with stakeholders including the Executive, Product, Data Science, and Engineering teams to assist with data-related technical issues and support their data infrastructure needs. \nWork with data science and analytics teams to strive for greater functionality in our data systems. \n\nMinimum Qualifications:\n\nAdvanced working SQL knowledge and experience working with relational databases, strong query authoring (SQL) as well as working familiarity with a variety of databases (Snowflake, Redshift, MySQL, MSSQL, etc.)\nExperience building and optimizing data pipelines, architecture and data sets. \nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. \nBuild processes supporting data transformation, data structures, metadata, dependency, and workload management. \nA successful history of transforming, processing and extracting value from large disconnected, datasets from a variety of data sources (Flat files, Excel, databases, etc.)\nStrong analytic skills related to working with unstructured datasets. \nStrong project management and organizational skills. \nExperience supporting and working with cross-functional teams in a dynamic environment. \nAbility to mentor/guide/collaborate with other team members. \n\nWe are looking for a candidate with 3-5 years of experience in a Data Engineer role. They should also have experience using the following software/tools:\n\nExperience with relational and MPP databases, including Snowflake and MySQL. \nExperience developing software in an agile environment from the requirements stage to production. \nExperience with version control: git\nExperience with container technologies: Docker\nExperience with data pipeline and workflow management tools: Airflow, Jenkins, AWS Glue, Azkaban, Luigi, etc. \nExperience with AWS cloud services: EC2, ECS, Batch, S3, EMR, RDS, Redshift\nExperience with other cloud services: Snowflake, Airflow\nExperience with object-oriented/object function scripting languages: Python, Java, C++, etc. \nExperience with data modeling and data warehouse design\nExperience with data visualization tools (PowerBI, QuickSight)\n\nThe starting annual pay scale for this position is listed below. Actual starting pay will be based on factors such as skills, qualifications, training, and experience. In addition, the company offers comprehensive benefits including medical, dental and vision insurance, 401(k) plan with match, paid time off, and other benefits.\n\nISC's salary ranges are determined by role and level. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations and could be higher or lower based on a multitude of factors, including job-related skills, experience, and relevant education or training.\n\nNational Pay Range\n\n$100,000\u2014$110,000 USD\n\nBenefits of Working at ISC\n\nCompetitive vacation and flexible working arrangements\nComprehensive and inclusive health benefits\nA variety of professional development and mentorship opportunities\nChoice of technology whether at home or in the office\n\nISC's Ownership Behaviors\n\nDo the Right Thing *Be Relentless. Pursue Excellence. *Personal Responsibility & Own Outcomes\nTry Fast. Learn Fast. Fail Fast. Think Big. *Build Extraordinary Owners & Join Forces *Grit & Determination\n\nApplicants with disabilities may contact ISC HR department via e-mail, and other means to request and arrange accommodation. If you need assistance to accommodate a disability, you may request accommodation at any time. Please contact ISC at HR@ISCMGA.com.\n\nISC believes in creating long-term relationships by being responsive and relevant and by consistently delivering value to our community of customers. Specifically, with our employees, we focus on attracting, developing, and retaining the best talent for our business, challenging our people, demonstrating a \"can-do\" attitude, and fostering a collaborative and mutually supportive environment. ISC is an equal opportunity employer and a participant in the US federal E-Verify program (US).\n\nDiversity creates a healthier atmosphere: All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, gender, gender identity, sexual orientation, marital status, medical condition, genetic information, mental or physical disability, military or veteran status, or any other characteristic protected by local, state, or Federal law."}, {"title": "Data Engineer -Remote", "company_name": "RIT Solutions, Inc.", "location": "Denver, CO (Remote)", "link": "/jobs/view/3768007496/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=1hQH8EDaII1eJ7nQ9BACQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3768007496", "description": "About the job\nDuration: 6+ months CTH\n\nMust have a valid LinkedIn profile\n\nRequired Skills\n\n 3-6 years of experience (no more than 6 years)\n Start up environment experience\n Python, SQL, and DBT"}, {"title": "Big Data Engineer - W2", "company_name": "Megan Soft Inc", "location": "United States (Remote)", "link": "/jobs/view/3674845652/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fCAxh4N%2F4YZJ5aEIASaeeA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3674845652", "description": "About the job\nTitle: Big Data Engineer - W2\n\nLocation: Dearborn, MI (Remote)\n\nDuration: 12+ Months\n\nDirect Client: Ford Motors\n\nMust have skills: Should be able to handle GCP requirements\n\n\"Translates business requirements to build repeatable, sustainable, efficient, coded processes that can be productionized by Software Engineers and readily modified by other Data Engineer Creates POC processes in Dev/QA and works with Software Engineers to productionize and align best coding and process practices that maximize efficiency, speed, stability, system resources and capabilities Awareness of and compliance with data privacy, security, legal and contractual guidelines Incorporates data quality and privacy checks/alerts to minimize bad data being consumed by end users, models and dashboards, and to protect customer data; revises checks for new data issues Validates data products and pipelines are functioning as expected following system or application upgrades, source changes, etc. Works closely with Architects if required to align systems, tools and applications being utilized with business use case and performance requirements Provides data product support and maintenance Establishes, tracks and monitors KPIs related to specific data products and deliverable Participate in iteration cadence activities (Iteration Planning, task estimation, daily scrum, demos, etc.). Will be a part of on-call team to resolve critical production support tickets during business hours and off hours.\n\nSkills Required\n\nExperience with Big Data Technologies (Hadoop , ETL Tool, Spark. Hive etc..) Someone who understands Cloud and has experience working with Cloud Platforms (preferably GCP) Should be able to handle GCP requirements - ingesting into GCS/ Big Query, working with GCP components Hands-on in Data Flow, Cloud Storage (GCS), Big Query and GitHub. Proficient in Python and SQL.\n\nSkills Preferred\n\nManages ETL: uses programming and tools for data ingestion, configures pipelines, applies transformations and decoding, integrates and fuses data, moves and securely deliver Experience NoSQL databases\n\nAdditional Information\n\nIf the candidate is remote only, please indicate \"\"100% Remote\"\" under candidate's name on resume. Other similar schedule notations can be \"\"Local Candidate\"\" or \"\"Hybrid Candidate.\"\" ***HRA Test Required***\""}, {"title": "Remote Opportunity: Quantexa Data Engineer", "company_name": "SPAR Information Systems LLC", "location": "United States (Remote)", "link": "/jobs/view/3680073191/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ioykVCZSOQ7BK5%2FwRQHOWg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3680073191", "description": "About the job\nHello All, Hope you are doing great!! Please go through the job descriptions and let me know your interest. Role: Quantexa Data engineer  Location: Remote Duration: Long Term Contract Job Description: 1. End to End usage of Quantexa including guiding our client team on how to use from both front end and data ingestion Reason they are asking for Certification 2. Resolve Entity issues with data 3. Knowledge of Scala and Hive which requires coding experience using parquet 4. Advanced SQL knowledge requiring writing SQL queries 5. Experience having worked with GCP data lake 6. Quantexa certification required\n\nThanks & Regards,\n\nSatnam Singh\n\nDirect: 201 623 3660\n\nEmail : Satnam.singh@sparinfosys.com"}, {"title": "Remote Work - Need Staff Data Engineer - USC only", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3746283980/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=CvIuLBNO%2BPMGi4IXVRkP0A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3746283980", "description": "About the job\n100% Remote, client is in NY\n\nLook for strong job history, Python, Machine Learning, SQL, 10/10 comms, etc. \n\nMUST be an Engineer \n\nSummary\n\nWe're looking for an experienced Data Engineer to join our client's growing team of top notched IT professionals. You will work closely across multiple groups including Editorial and Audio, Marketing, Advertising, etc. to achieve their goals and objectives. The Data Engineering team builds tools throughout the stack while sharing knowledge across these departments.\n\nAs the company grows, they're looking for Data Engineer who will help solidify and expand our pipelines and maintain their data warehouse. Their business model is based on performing complex and very detailed analyses of how our products perform with their subscribers. You'll be responsible for working alongside analysts, product, marketing, and other teams to define new data collection components and measurement schemes that support each new product and feature.\n\nHow Can Add Value To Their Mission\n\nCreate and maintain data pipelines to provide insights and drive business decisions\nEstablish data warehousing strategy (ex. Kimball, Data Vault, etc.)\nMaintain data infrastructure on our AWS accounts\nCollaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.\nWrite unit/integration tests, contributes to engineering wiki, and documents work\nImplement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it\n\nWhat You'll Need To Succeed\n\n7+ years of industry experience measuring product performance and user behavior\nExperience working with a variety of data management technologies, including RedShift, Kinesis/Kafka, Glue, Spark, Postgres, Airflow, dbt, and others.\nExperience implementing BI reporting tools such as Looker\nExperience interfacing with engineers, product managers and analysts to understand data needs\nKnowledge of a variety of measurement beacons, SDKs, APIs including Google Analytics, Amplitude, Braze, Stripe, email service providers\nA commitment to building secure, resilient, fault-tolerant architectures, with clear documentation and procedures in place for support\nA focus on accuracy and detail as you build, ensuring data pipelines produce clear, predictable results\nUnderstanding of the typical metrics a subscription and advertising-supported business needs to measure success\nExperience with Client techniques as applied to behavioral segmentation or anomaly identification is a definite plus\nFamiliarity using developer tools that increase productivity and facilitate the development of resilient code (eg. Docker, CircleCI, Serverless) is a plus"}, {"title": "ENTRY LEVEL DATA ENGINEER (REMOTE)", "company_name": "SynergisticIT", "location": "Jacksonville, FL (Remote)", "link": "/jobs/view/3767587909/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=om7L3CnowiC4PLIcpZD70A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3767587909", "description": "About the job\nSYNERGISTICIT wants every candidate to know that the Job Market is Challenging and to stand out, you need to have exceptional skills and technologies and that's where we come in to make sure you get the attention which you need\n\nSynergisticIT understands the complex nature of the job market and how difficult it can be to secure a position, especially for fresh graduates. Therefore, we assist and help tech-savvies to convert their passions into professions. We go above and beyond to keep you working in your niche.\n\nAs we focus on long-term success, we provide complete career development solutions. From job search to upskilling portfolio and interview preparation, we can guide you at every step of your career.\n\nSynergisticIT spares no efforts to connect you with a large network of tech giants, including Google, Apple, PayPal, Dell, Cisco, Client, etc. Presently, we are actively looking for ENTRY LEVEL DATA ENGINEER a driven mindset. Get the right opportunity and gain experience in building web-centric solutions on Java.\n\nWho Should Apply : Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or anyone looking to make their career in IT Industry\n\nWe also assist in filing for STEM extension and H1b and Green card filing.\n\nCandidates who are serious about their future in the IT Industry and have set big goals for themselves.\n\nCandidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. We also offer Skill Enhancement Programs if the candidates are missing skills or experience which our clients need with great outcomes\n\nCandidates can benefit from skill enhancement if they fall into the below categories. If they are qualified with enough skills then no need for skill enhancement\n\nCandidates Who Lack Experience\n\nHave had a break in careers\n\nLack Technical Competency\n\ncandidates who want to get employed and make a career in the Tech Industry \n\nPlease Also Check The Below Links\n\nhttps://www.synergisticit.com/candidate-outcomes/\n\nhttps://www.synergisticit.com/java-track/\n\nhttps://www.synergisticit.com/data-science-track/\n\nhttps://www.synergisticit.com/which-is-the-best-option-for-tech-job-seekers-staffing-companies-consulting-companies-bootcamps-or-synergisticit/\n\nhttps://www.synergisticit.com/contact-us/\n\nIf the skills are not a match candidates can opt for Skill enhancement. Or their resume can be sent out to clients to see if responses are achievable\n\nREQUIRED SKILLS For Java/Software Programmers\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Core Java , javascript , C++ or software programming\nSpring boot, Microservices and REST API's experience\nExcellent written and verbal communication skills\n\nFor data Science/Machine learning\n\nRequired Skills\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Statistics, Python, data visualization tools\nExcellent written and verbal communication skills\n\nPreferred skills: NLP, Text mining, Tableau, Time series analysis\n\nTechnical skills are required by clients for selection even if its Junior or entry level position each additional Technical skill helps a candidate's resume to be picked by clients over other job seekers.\n\nClients hire candidates with the right technical skills which they need and reject candidates who lack the required technical skills.\n\nNo third party candidates or c2c candidates\n\nPlease apply to the posting\n\nNo phone calls please. Shortlisted candidates would be reached out"}, {"title": "Data Engineer - Remote | WFH", "company_name": "Get It Recruit - Information Technology", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3782241984/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=BgBf1kSAAX%2BSuw6inJsIVw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3782241984", "description": "About the job\nJoin an innovative team at a leading company in the engineering and manufacturing industry, where we are committed to leveraging data for continuous improvement and operational excellence. This hybrid position allows you to enjoy the flexibility of remote work on Mondays and Fridays.\n\nWhy Join Us\n\nAt our company, we prioritize your well-being and professional growth. We offer a comprehensive benefits package, including excellent health benefits, a 6% 401(k) match, generous paid vacation and sick time, 12 paid holidays, and more.\n\nRole Overview\n\nAs a Data Engineer, you will play a pivotal role in transforming raw data into actionable insights, driving improvements in engineering and manufacturing processes, enhancing product quality, and optimizing operational efficiency. Collaborate with dynamic cross-functional teams to extract, analyze, and visualize data, influencing strategic decisions through your valuable insights.\n\nKey Responsibilities\n\nData Collection and Integration:\n\nGather, clean, and integrate data from diverse sources, including production databases, sensor data, and external datasets.\n\nData Analysis\n\nConduct exploratory data analysis to identify patterns, trends, and anomalies.\n\nPerform statistical analysis and hypothesis testing to derive meaningful insights.\n\nData Visualization\n\nCreate informative and visually appealing dashboards and reports using tools like Tableau, Power BI, or Python libraries.\n\nPredictive Modeling\n\nDevelop predictive models to forecast manufacturing performance, quality, and operational outcomes.\n\nRoot Cause Analysis\n\nInvestigate production issues and quality concerns by analyzing data to identify root causes and recommend corrective actions.\n\nPerformance Monitoring\n\nEstablish and maintain key performance indicators (KPIs) to track manufacturing and engineering metrics.\n\nGenerate regular reports to communicate performance trends to stakeholders.\n\nCross-functional Collaboration\n\nCollaborate with engineering, manufacturing, and quality assurance teams to understand their data needs and provide analytical support.\n\nContinuous Improvement\n\nSuggest process improvements and optimizations based on data analysis.\n\nData Security And Compliance\n\nEnsure data security and compliance with relevant regulations, such as GDPR or industry-specific standards.\n\nInfrastructure And Development\n\nBuild and maintain the infrastructure for collecting, storing, and processing data.\n\nDesign, develop, and manage data pipelines, ETL processes, and data warehouses.\n\nRequired Experience\n\nProven experience as a Sr. Data Analyst and/or Data Engineer in an engineering or manufacturing environment.\n\nProficiency in data analysis tools and programming languages, such as Python, R, SQL, and Excel.\n\nStrong data visualization skills with tools like Tableau or Power BI.\n\nExcellent knowledge of statistical analysis and machine learning techniques.\n\nAbility to communicate complex findings and insights effectively to both technical and non-technical stakeholders.\n\nStrong problem-solving skills and attention to detail.\n\nFamiliarity with manufacturing processes and quality control is a plus.\n\nExperience with enterprise ERPs (SAP S4HANA, Oracle EBS), CRMs (Salesforce, Hubspot), and IoT.\n\nProficient in working with cloud computing platforms including AWS, GCP, and Azure.\n\nExpertise in configuring and optimizing cloud-based data solutions for scalability, reliability, and performance.\n\nBachelor's Degree in Data Science, Statistics, Engineering, or Computer Science Required.\n\nApply Now and be part of a team shaping the future of data-driven innovation in engineering and manufacturing!\n\nEmployment Type: Full-Time"}, {"title": "Entry Level Software Engineer - Data Backend Engineer (Remote - Canada)", "company_name": "Yelp", "location": "United States (Remote)", "link": "/jobs/view/3752881757/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fankeoxifvhpTzOYvowHYQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3752881757", "description": "About the job\nYelp engineering culture is driven by our values: we\u2019re a cooperative team that values individual authenticity and encourages creative solutions to problems. All new engineers deploy working code their first week, and we strive to broaden individual impact with support from managers, mentors, and teams. At the end of the day, we\u2019re all about helping our users, growing as engineers, and having fun in a collaborative environment.\n Interested in exploring data and finding innovative ways to collect it, curate it, and make it easily accessible for others to use? How about doing that at an enormous scale? Yelp\u2019s datasets contain billions of interactions between users and local businesses around the globe. If working with our vast amount of data sounds exciting, come and join us!\n\nThe Marketplace Data Observability team is responsible for managing, governing, and monitoring all of Yelp\u2019s Ads data to protect our revenue stream and also support offline analysis that aids in making long term company decisions. We build tools that help us to better understand our data quality, and we strive to uncover data issues before they impact our consumers.\n\nAs a Data Backend Engineer on this team, you will be responsible for building elegant and scalable data products that serve critical, up-to-date, structured information to support different types of analytics within Yelp. As a core contributor to our growing data modeling and data warehousing engineering efforts, you will help design and own mission-critical data flow pipelines and datastores to enable decisions including effective A/B testing and company investments.\n This opportunity is fully remote and does not require you to be located in any particular area in Canada. We welcome applicants from throughout Canada. We\u2019d love to have you apply, even if you don\u2019t feel you meet every single requirement in this posting. At Yelp, we\u2019re looking for great people, not just those who simply check off all the boxes.\n  Build systems that can effectively store and crunch terabytes of data.\nDesign and develop data models for efficient data storage, retrieval, and reporting.\nCreate and maintain conceptual, logical, and physical data models using industry-standard modeling tools.\nCollaborate with cross-functional teams, including engineers, data analysts, business analysts, and data scientists, to understand data requirements and translate them into effective data models.\nParticipate in data integration efforts, including ETL processes and data migration.\nStay up-to-date with industry best practices, emerging technologies, and trends related to data warehousing.\nSupport on-call rotations as needed to operate the team.\n  Understanding of high performing and scalable data systems.\nExperience in building and orchestrating ETL pipelines.\nExperience with Data Lake or Data Warehouse landscape.\nA hunger for tracking down root causes and fixing them in systematic ways.\nAbility to communicate effectively to technical and non-technical cohorts alike. \nExposure to some of the following technologies: Python, AWS Redshift, AWS Athena / Apache Presto, Big Data technologies (e.g S3, Hadoop, Hive, Spark, Flink, Kafka etc), NoSQL systems like Cassandra, DBT."}, {"title": "Data Engineer (Associate Consultant), June 2024", "company_name": "UDig", "location": "Nashville, TN (Remote)", "link": "/jobs/view/3716039960/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=XUjo40kU55HnDCqJ%2F3FdPg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3716039960", "description": "About the job\nCan UDig It? \n\nUDig designs, builds, and implements technology solutions that deliver on business objectives. People join our team because they are excited to have an impact on the future of our organization, passionate about delivering meaningful results, and energized by solving complex problems. Oh, and they like working with other driven, smart, and authentic people!\n\nUDig is proud to be recognized as a \"Best Places to Work\" in Virginia and Tennessee. We're committed to fostering a transparent environment where leadership is accessible, professional development opportunities are abundant, and you are empowered to achieve your career goals. You'll collaborate, develop deep relationships with teammates, and accelerate your success while having fun!\n\nIf you're graduating in May 2024 and excited by the idea of impacting organizations by building data solutions to achieve business goals, you should consider joining our growing technology team.\n\nHow Things Go Down\n\nAs a new, entry-level teammate, you'll take part in our \"Breaking Ground\" training program designed to help you quickly get up-to-speed on UDig's delivery approach within a project team. The program covers:\n\nUDig delivery process and methodologies such as gathering requirements and delivering projects using agile\nTechnical guidance and instruction on the best practices from Data leaders at UDig\nDelivering executive and technical presentations\nCoaching to prepare you to for success on your first client project at UDig\n\nA typical day might entail:\n\nProject standups and check-ins\nDesign and development of data pipelines, BI reports, and more\nPlanning and participating in project demos\nContributing to internal UDig projects\nCareer and job growth through UDig-provided training\nCreating long-lasting friendships with teammates through fun activities as well as your projects (including social events, winning a prize for guessing the weight of pumpkin, or sharing experience on cool tech side projects)\n\nUDig believes it is important for Associate Consultants going through our Breaking Ground program to reside in one of our major markets (Richmond, VA or Nashville, TN metro area) for at least the first year of employment.\n\nWant to learn more? Hear directly from our teammates by checking out our 'A Day in the Life' blog series: https://www.udig.com/digging-in/breaking-ground-2023/\n\nSound Interesting? Here's the Technical Piece\n\nBelow are a few boxes you should be able to check if\u202fyou're interested: Don't check every box? Go ahead and apply!\n\n1+ year of experience with relational database management systems such as Microsoft SQL Server, MySQL, or Oracle\nUnderstanding of the full lifecycle of design, development, and implementation of data solutions including architecture design, development, testing\nSome experience in ETL development or reporting is a plus\nAbility to present technical ideas and high-level concepts and solutions to internal and external team members with varying degrees of technical knowledge\nBachelor's Degree in in computer science, information technology, or any science or business discipline\nAbility to take your work seriously but also seriously enjoy a good time with your UDig colleagues\n\nWhat's in It For You? \n\nAt UDig, you will build your career alongside talented and experienced developers while gaining experience and having fun! We provide continuous challenges and professional development opportunities. Additionally, we offer: \n\nCompetitive salary, merit reviews, and career advancement paths\nFlexible, hybrid environment\nIndividual $1500 Training Budget\nRegular team building and social activities (virtual and/or in-person)\nTransparent culture with strong communication and access to leadership\nGreat benefits like\nGenerous Paid Time Off, company holidays, and parental leave\nMultiple Single and Family Health Insurance plans to choose from\nDental & Vision coverage\nShort-Term and Long-Term disability\nOptional accident and critical illness coverage\nMatching 401(k)\nand more!"}, {"title": "Expression of Interest: Data Engineer", "company_name": "Fingerprint for Success (F4S)", "location": "New York, NY (Remote)", "link": "/jobs/view/3787769880/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=l%2FEDpQqUNmCttGdIyTk7Cg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787769880", "description": "About the job\nWe are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.\n\nThe F4S Talent Pool is a pilot project designed to:\n\nHelp job seekers get discovered by our partners based on their anticipated hiring needs.\nProvide optional support and resources for job seekers in their career endeavors.\nHelp individuals understand, and bring out the best in themselves and each other.\n\nThe F4S Talent Pool process:\n\nOnce you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.\n\nAbout Fingerprint For Success (F4S)\n\nBacked by 20+ years of research, F4S\u2019s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high performing teams.\n\nKeep in mind, joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your F4S work style assessment to match the hiring needs of our partners.\n\nYour feedback is a gift! Write to us via:\n\nPowered by JazzHR\n\noWuPJ9lwq9"}, {"title": "Data Engineer Seattle, WA (remote)", "company_name": "Conch Technologies, Inc", "location": "United States (Remote)", "link": "/jobs/view/3765052211/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=SBETI78UMutCHuRp5FnP%2FA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3765052211", "description": "About the job\nHi,\n\nGreetings from Conch Technologies Inc\n\nData Engineering Lead \u2013 With Databricks and Unity Catalog\n\nLocation \u2013 Seattle, WA (remote)\n\nDuration \u2013 12 months\n\nNeed: Design and implement data architecture and strategy, leveraging technologies such as Databricks, Unity Catalog, Privacera, and Collibra.\n\nAs a Data Engineering Lead, you will play a pivotal role in defining and executing technical deliverables while assessing complexities and levels of effort. You'll engage with both vendor and internal teams to resolve technical challenges and ensure seamless collaboration. Your leadership will extend to planning and prioritization with the SCRUM master, facilitating effective coordination between onsite and offshore teams, and driving feature prioritization through collaboration with product managers.\n\nKey Responsibilities\n\n Lead the definition and breakdown of technical deliverables, assessing complexities and levels of effort.\n Collaborate with vendor and internal teams to address technical issues.\n Drive planning and prioritization in conjunction with the SCRUM master.\n Coordinate with onsite and offshore teams for timely project delivery.\n Collaborate with product managers to prioritize features, communicate with stakeholders, and clarify requirements.\n Collect and manage user-reported issues, ensuring prompt resolution and communication.\n Define comprehensive test plans, oversee their execution, and document results.\n\n--\n\nWith Regards,\n\nNagesh G\n\nMobile: 408-381-5645\n\nDesk: 901-313-3066\n\nEmail: nagesh@conchtech.com \n\nWeb: www.conchtech.com"}, {"title": "Remote Work - Need Azure Fabric Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3759325599/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=C3iVjmcR9wjUip1nNjUiCg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3759325599", "description": "About the job\nThe client is placing heavy emphasis on Microsoft and Azure Fabric experience\n\nInformation Management Engineer \u2013 Take the identified information management sources and assist migration to the Client Data Lake, Azure Fabric. The engineer will transform information management (de-dup, clean and enrich).\n\nTake the identified data sources and bring them into our Data Lake, Azure Fabric. Once the data is in, the engineer will need to be able to transform data (de-dup, clean and enrich the data). We are focused on using a medallion data model for data architecture. The tools we are working with are: Delta Parquet files, python, spark, notebooks, T-SQL, Fabric lakehouse/warehouse, Dataflows Gen2, Data Pipelines, Direct Lake, shortcuts, ADF."}, {"title": "Data Engineer", "company_name": "Catalist", "location": "Washington, DC (Remote)", "link": "/jobs/view/3744951279/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ui1Fk6ICmnPNqAlZDZAcwA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3744951279", "description": "About the job\nFor over 17 years, Catalist has been a leader in civic data and data science innovation. Our mission is to provide progressive organizations with the data, software, and services needed to better identify, understand, and communicate with the people they need to engage and mobilize. Our clients include the largest, most influential organizations in the U.S. active in civic engagement, advocacy, and political campaigns.\n\nCatalist is home to a dedicated, creative team of technologists, data scientists, and campaign experts committed to using our talents and technology to nurture a vibrant and growing progressive community.\n\nAs a Data Engineer at Catalist, you will work closely with the Analytics and Technology departments to design, build, support, and maintain various data pipelines and processes with an end goal of providing data and intelligence to the progressive community. This role primarily involves translating data architecture designs into functional processes, code, and systems.\n\nThe ideal candidate will be a highly motivated individual with excellent technical skills, a strong desire to learn new skills, and an interest in progressive politics. Catalist values creativity and problem-solving. Our work is on the cutting edge of data-driven politics, and your support will help Democratic candidates and progressive organizations conduct successful advocacy and electoral campaigns.\n\nThis position reports to the Deputy Chief Data Officer. The Data Engineer is a part of a growing Data team that supports all underlying work at Catalist.\n\nThis position is included in our CWA bargaining unit.\n\nPrinciple Duties & Responsibilities\n\nDevelop scalable, production processes, code, and systems for data ingest, transformation, modeling, and reporting across a variety of platforms\nTranslate mock-ups and designs into functional processes, code, and systems\nCreate architecture designs when needed\nProvide quality assurance and testing on processes, code, systems, and products\nBecome an internal subject matter expert on various datasets and support other Catalist departments/teams on usage of those datasets\nExecute ad-hoc data and database maintenance tasks as requiredProject manage cross-departmental efforts, with direct responsibility for stakeholder engagement, management, and execution of technical elements\n\n\nRequirements\n\nBS or BA in a technical field, or relevant experience\n1-2 years of experience working with SQL databases\nExperience with data cleaning or ETL processes\nExperience with distributed computing systems and/or distributed datastores (particularly the Hadoop ecosystem)\nExperience managing projects\nFamiliarity with Catalist data, progressive politics, voter files, and/or commercial data\nBackground check required\n\n\nPreferred Skills & Abilities\n\nWillingness to be a problem solver and produce results in a fast paced environment\nAbility to be creative and personable, and articulate ideas clearly\nExperience working with SQL databases\nProficiency with Python or another object-oriented programming language (R, Java, Scala, etc\u2026)\nExperience working in cloud environments (AWS, GCP, etc.)\nExperience working in command line environments such as Bash\nExperience with a version control tool such as git or github\n\n\nBenefits\n\nMedical, Dental, Vision, Prescription Drug\n\nCatalist offers Medical, Dental, Vision, and Prescription Drug coverage for eligible staff and their eligible dependents. Catalist\u2019s Medical plan is a comprehensive PPO program including Prescription Drug coverage with 85% of the premium paid by Catalist. Dental and Vision coverage is provided at no cost to employees.\n\nGroup Term Life Insurance and Long-Term & Short-Term Disability Coverage\n\nGroup Term Life Insurance and Long-Term and Short-Term Disability coverage is available for eligible staff. These benefits are provided at no cost to Catalist employees.\n\n401(k) Safe Harbor Plan\n\nA 401(k) Safe Harbor Plan is available to eligible staff with a 3% contribution from Catalist from the date of hire. Employees may contribute pre-tax or post-tax from their salary up to the legal limits set forth by the IRS.\n\nMedical and Dependent Care Flexible Spending Accounts (FSAs)\n\nCatalist offers an FSA Program that gives eligible staff the ability to pay out-of-pocket medical/dental/vision/child care expenses from pre-tax earnings.\n\nTransit Benefits\n\nCatalist also makes available a Transit benefit FSA program to eligible employees using pre-tax contributions with a company match.\n\nProfessional Development and Remote Work Expenses\n\nEligible employees may be reimbursed up to $750 each year for professional development / education and remote work expenses.\n\nStudent Loan PayDown or SaveUp\n\nCatalist offers a Student Loan PayDown and College SaveUp benefit for eligible staff.\n\nVacation, Personal Leave, Sick Leave Benefits\n\nCatalist offers generous vacation benefits to all eligible staff. Eligible employees also receive:\n\n 14 Paid Holidays\n Personal Days\n Sick Leave\n Parental Leave\n\n\nHybrid Office/Remote Work\n\nCertain positions at Catalist are eligible for Office/Remote Hybrid or full Remote status."}, {"title": "Remote Entry Level Data Analyst/Engineer", "company_name": "SynergisticIT", "location": "White Plains, NY (Remote)", "link": "/jobs/view/3774171423/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=q54sk1%2FRww0z%2BC%2BE8e0jQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774171423", "description": "About the job\nAt SynergisticIT, we're all about making connections. Whatever IT goals you have, our software programmers can help achieve those. Our Software development teams can take up turnkey projects and execute them in an effective and efficient manner. If you are looking to source talent our recruiters will find the ideal IT talent for your company. What's the secret to our success? Well, it all starts with taking quality time to listen to each client's specific needs. After we have a thorough grasp of your IT goals, we can better customize our Developments as per your specific needs. We can also tailor make recruiting programs to exceed your expectations. Since our founding in 2010, SynergisticIT's strategies have earned the company an enviable position in the software development, IT staffing and IT skill enhancement fields. SynergisticIT continues to work with hundreds of satisfied American clients with our software programmers working on our projects and after gaining hands on experience on cutting edge technologies moving to contribute their skills to great clients like Apple, Google, Client, Ebay, Paypal, Kroger, the Walt Disney Company and hundreds more. If you are tired of working with inefficient programmers who take a lot of time to ramp up we want you to try us. Our software programmers can hit the ground running and get you the maximum return on your investment. You have already tried the rest its time you tried the best. SynergisticIT - Home of the Best Data Scientists and Software Programmers in the Bay Area.\n\n Why Us ? \n\nSynergisticIT has a proven track record of successfully skill enhancement and staffing IT employees for some of the world's most iconic brands. Our team takes the time to fully understand every client's needs so we could best meet your IT staffing requirements. The knowledgeable staff at SynergisticIT is always more than happy to work with clients to ensure they reach their software development goals. Besides staffing, SynergisticIT is also committed to helping young IT professionals advance their career with a robust upskill program . Everyone who goes through SynergisticIT's program learns all the skills necessary to succeed in many IT fields ranging from Java to Machine Learning. Additionally, everyone trained at SynergisticIT has been through extensive mock and technical interview screenings to bolster their career prospects. Last, but certainly not least, SynergisticIT takes great care to respect the privacy considerations for every client. All companies who work with SynergisticIT can rest assured their confidential data is protected using the most up-to-date encryption technologies. SynergisticIT also complies with all the latest NDA agreements.\n\n REQUIRED SKILLS For Java /Software Programmers \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Time series analysis \n\n We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2022 and at Gartner Data Analytics Summit (Florida)-2023 \n\nOracle CloudWorld Event (OCW) Las Vegas 2022 | SynergisticIT - YouTube\n\n https://www.youtube.com/watch?v=OAFOhcGy9Z8 \n\n https://www.youtube.com/watch?v=EmO7NrWHkLM \n\n https://www.youtube.com/watch?v=NVBU9RYZ6UI \n\n https://www.youtube.com/watch?v=Yy74yvjatVg \n\nSynergisticIT at Gartner Data and Analytics Summit 2023 - YouTube\n\nFor preparing for interviews please visit  https://www.synergisticit.com/interview-questions/ \n\n We are looking for the right matching candidates for our clients \n\n Please apply via the job posting \n\n REQUIRED SKILLS For Java /Full Stack/Software Programmer \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning Positions \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Tensorflow \n\n If you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team. \n\n No phone calls please.  Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates"}, {"title": "Data Engineer", "company_name": "Anika Systems", "location": "Leesburg, VA (Remote)", "link": "/jobs/view/3787766893/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=mApL9JnN%2B%2Fr2%2BYUPlxRYOA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787766893", "description": "About the job\nAnika Systems is a fast growing, woman-owned small business that specializes in providing innovative IT solutions for federal government agencies. Our expertise lies in accelerating delivery in Data and Analytics, Intelligent Automation, Application Development, and IT Modernization. We are currently expanding our Federal team and are seeking a passionate and talented Data Engineer. This opportunity is 100% remote.\n\nMust be a U.S. Citizen with the ability to obtain and maintain a government suitability clearance\n\nResponsibilities\n\nDesigns, develops, builds, analyzes, evaluates, and installs database management systems to include database modeling and design, relational database architecture, metadata, and repository creation and configuration management.\nDefines and oversees database organizations, standards, controls, procedures, and documentation. Provides technical consulting in the definition, design, and creation of a database environment.\nAdvises applications development staff and users on data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nDesigns and implements databases with respect to access methods, access time, batch processes, device allocation, validation checks, organization, protection and security, documentation, and statistical methods.\nUses data mapping, data mining, and data transformational analysis tools to design and develop databases.\nDetermines data storage and optimum storage requirements.\nPrepares system requirements, source analysis, and process analyses and designs throughout the database implementation.\n\nRequired Skills And Experience\n\nBA/BS and 1 year of relevant experience\nExperience with DataBricks, SQL, Python\nApplications development experience with data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nExcellent communication skills (both oral and written) with process-oriented organizational skills to ensure project success\n\nDesired Skills And Experience\n\nDemonstrated successful development experience with the full life cycle of government database management systems including database modeling and design, relational database architecture, metadata and repository creation, and configuration management.\n\nPowered by JazzHR\n\nTlKWIorucJ"}, {"title": "Remote Work - Need Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3774712978/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=r3Bp%2F0qcOD2OpC5jpbUOcA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774712978", "description": "About the job\nTitle : Data Engineer\n\nLocation : Remote\n\nDuration : 6+ months to start (long term contract (work order are extended 2x year))\n\nPlease Fill out the skill matrix: Technology Some Awareness (1) Novice (2) Intermediate (3) Advanced (4) Expert (5) SQL Tableau Desktop /Creator Tableau API Integration AWS - S3 Glue AWS - Athena Redshift AWS - Lambda AWS - SNS/SQS msExcel\n\nProfile: Data Analyst will work closely with Architects and leads in the Interflow team, key business stakeholders including program staff to support the organizations effort in the areas of data Analysis and information management. Role should have strong experience in writing SQL queries to extract data for reports. Candidate should have experience working with large set of data. As these reports are very critical for the company and we are looking for an expert who is capable of handling, consuming and extracting large sets of data for analysis and reports.\n\nMust Haves\n\nAWS - Athena\nAWS - S3\nGlue\nhands-on development of reporting applications\nMS Excel\nRedShift\nSQL\nTableau Desktop /Creator\n\nNice\n\nAWS - Lambda\nAWS - SNS/SQS\nTableau API Integration\n\nSkills\n\nBA/BS required (major in an analytical field desired) \nA minimum 10+ years work-related experience. \nExperience in hands-on development of reporting applications for the Web in a professional environment \nThe Ethos of continuous improvement and interest in learning new things.\nStrong analytical thinking and structured problem-solving ability\nAbility to handle multiple projects and assignments simultaneously and effectively in a cross-functional team environment. \nVersed on the agile methodology and best practices. \nExcellent interpersonal and collaboration skill with the ability to work with a diverse set of colleagues, across functions, from different organizations, disciplines, etc. \nSelf-starter, ability to set priorities, work independently and attain goals"}, {"title": "Data Engineer", "company_name": "Team Remotely Inc", "location": "Houston, TX (Remote)", "link": "/jobs/view/3786249829/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=78lY5Jo%2FfyRPC6TQYCdvww%3D%3D&trk=flagship3_search_srp_jobs", "id": "3786249829", "description": "About the job\nThis is a remote position.\n\n Data Engineer (US/Canada Residents Only, 1 year experience, remote)\n\nTeam Remotely Inc. is a staffing and recruitment agency that offers a comprehensive solution for talent acquisition, including sourcing, vetting, pay rolling, and managing talent. Whether you need contract staffing, direct hire, direct sourcing, talent pools, or diversity initiatives, our model can support your hiring strategy.\n\n Hiring Type: Full-Time\n\n Base Salary: $55K-$70K Per Annum.\n\nResponsibilities\n\nDesigning user interface changes for web-based DB applications. \nReviewing application requirements and interface designs. \nDeveloping and implementing highly responsive user interface components using react concepts. \nWriting application interface codes using JavaScript following react.js workflows. \nTroubleshooting interface software and debugging application codes. \nDeveloping and implementing front-end architecture to support user interface concepts. \nMonitoring and improving front-end performance. \n\nQualifications And Experience\n\nBachelor\u2019s degree in computer science, information technology, or a similar field. \nPrevious experience working as a react.js developer. \nIn-depth knowledge of JavaScript, CSS, HTML, jQuery, and front-end languages. \nKnowledge of REACT tools including React.js, Webpack, Enzyme, Redux, and Flux. \nExperience with user interface design. \nKnowledge of performance testing frameworks, including Mocha and Jest. \nExperience with browser-based debugging and performance testing software. \nTroubleshooting skills. \nProject management skills. \nProblem-solving skills. \nVerbal communication skills. \n\n Why work with Team Remotely?\n\nTeam Remotely Inc. is a staffing platform offering a seamless experience for employers and candidates. Employers can post job openings and specify their requirements, while candidates can create profiles and upload resumes.\n\nThe team of Team Remotely continuously learns and adapts based on previous successful placements, constantly improving its matching capabilities. This ensures that the recommendations provided by Team Remotely are tailored and accurate, increasing the likelihood of a successful match between employers and candidates. By providing intelligent and data-driven solutions, they strive to enhance the efficiency and effectiveness of the hiring process, ultimately helping companies find the best talent and individuals find their dream jobs."}, {"title": "Data Engineer", "company_name": "ZAG Technical Services", "location": "United States (Remote)", "link": "/jobs/view/3771403464/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=Hf0VefrKXBFIL5MtHmY0Pw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3771403464", "description": "About the job\nWe are looking for a tech savvy Data Engineer to contribute to the building, operating, and scaling of next generation data platforms and tools that will power data-driven analytics capabilities throughout the ZAG products and services portfolio, spanning areas such as business intelligence, reporting, and data analytics.\n\nThe ideal team member is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. Must be comfortable supporting the data needs of multiple clients and teams, systems, and products within the modernization of ecosystems. The right-fit person will be excited by the prospect of optimizing or even re-designing data processes to support next generation of advanced analytics capabilities, which in-turn translates to implementing strategic enhancements to maintain competitive advantage in the industry.\n\nThe successful team member will leverage their proficiency to\u2026\n\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\nBuild the analytics processes required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Data Management and cloud based \u2018big data\u2019 technologies.\nBuild analytics tools that utilize technologies to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.\nCollaborate with client teams to identify and solution regional and local requirements within the context of the standard global business model for enterprise analytics.\nParticipates in meetings, discussions, strategy sessions where changes, improvements, and enhancements are proposed, evaluated, and approved. \n\n\nThe exceptional team member will have or demonstrate progressive experiences in\u2026 \n\nBachelor\u2019s degree or higher in Computer Science or related field\nExperience working with BI and data warehousing tools\nExperience working with enterprise data, where security is paramount and data governance is critical\nAbility to work independently and in a team environment, and effectively engage all levels of the organization\nUnderstanding of distributed systems driving large-scale data processing and analytics with a successful history of manipulating, processing, and extracting value from large, disconnected datasets.\nAbility to communicate effectively (listening, presenting, and questioning)\nStrong organizational, written, and communication skills\n\n\nAbout ZAG\n\nZAG Technical Services empowers our clients\u2019 success through intelligent business solutions that increase employee productivity, network security and team collaboration. We aim to ensure that our client\u2019s IT investments drive ROI, provide a competitive advantage and help businesses grow. We do this by living our core set of values: Integrity Foremost, Client-Centered, Accountable Always, Teamwork Throughout, and Exceptional is the Goal.\n\nWhat ZAG Has To Offer\n\nCompetitive compensation package \nBenefits including medical, dental, vision, 401K and life insurance \n\n\nZAG Technical Services is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all of our employees.\n\nZAG Technical Services is an E-Verify participant and will provide the Social Security Administration (SSA) and, if necessary, the Department of Homeland Security (DHS), with information from each new employee\u2019s Form I-9 to confirm work authorization."}, {"title": "Junior Data Engineer", "company_name": "HireMeFast LLC - Career Accelerator - Land A Job", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3786249797/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D&trk=flagship3_search_srp_jobs&lipi=urn%3Ali%3Apage%3Ad_flagship3_search_srp_jobs%3BQExogrHBQoSF9zZm%2B9kufw%3D%3D&lici=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D", "id": "3786249797", "description": "About the job\nThis is a remote position.\n\n Job Title: Junior Data Engineer\n\n Employment Type: Full-Time\n\n Salary: $64,000 - $76,000 per annum\n\n Experience Required:  Minimum 1 year of project experience\n\nAbout us: HireMeFast is a leading staffing and recruitment agency specializing in connecting businesses with top-tier talent across various industries. Our mission is to bridge the gap between exceptional candidates and organizations needing their skills, expertise, and unique qualities. Our team of experienced and dedicated recruitment specialists utilizes innovative sourcing strategies, and a vast network to identify and attract top talent. We conduct comprehensive procedures to ensure that only the most qualified candidates are presented to our clients.\n\nPosition Summary\n\nJoin the fast-paced, innovative, and collaborative environment focused on providing an AIOps platform that enhances the intelligence of the CVS Health infrastructure. Work closely with subject matter experts and colleagues to build and scale out machine learning and AI solutions that will detect, predict, and recommend solutions to correct issues before system impact and enhance efficiency, reliability, and performance of CVS Health\u2019s IT operations.\n\nKey Responsibilities include:\n\nData pipeline development: Designed, implemented, and managed data pipelines for extracting, transforming, and loading data from various sources into data lakes for processing, analytics, and correlation. \nData modeling: Create and maintain data models ensuring data quality, scalability, and efficiency \nDevelop and automate processes to clean, transform, and prepare data for analytics, ensuring data accuracy and consistency \nData Integration: Integrate data from disparate sources, both structured and unstructured to provide a unified view of key infrastructure platform and application data \nUtilize big data technologies such as Kafka to process and analyze large volumes of data efficiently \nImplement data security measures to protect sensitive information and ensure compliance with data and privacy regulation \nCreate/maintain documentation for data processes, data flows, and system configurations \nPerformance Optimization- Monitor and optimize data pipelines and systems for performance, scalability and cost-effectiveness \n\nCharacteristics of this role:\n\nTeam Player: Willing to teach, share knowledge, and work with others to make the team successful. \nCommunication: Exceptional verbal, written, organizational, presentation, and communication skills. \nCreativity: Ability to take written and verbal requirements and come up with other innovative ideas. \nAttention to detail: Systematically and accurately research future solutions and current problems. \nStrong work ethic: The innate drive to do work extremely well. \nPassion: A drive to deliver better products and services than expected to customers. \n\nRequired Qualifications\n\n2+ years of programming experience in languages such as Python, Java, SQL \n2+ years of experience with ETL tools and database management (relational, non-relational) \n2+ years of experience in data modeling techniques and tools to design efficient scalable data structures \nSkills in data quality assessment, data cleansing, and data validation \n\nQualifications\n\nKnowledge of big data technologies and cloud platforms \nExperience with technologies like PySpark, Databricks, and Azure Synapse. \n\nEducation\n\nBachelor\u2019s degree in Computer Science, Information Technology, or related field, or equivalent working experience\n\nWhy HireMeFast LLC?\n\nAt HireMeFast, we understand that finding the right individuals to join your team is crucial for success and growth of your organization. We are committed to streamlining the hiring process for our clients, ensuring they have access to a diverse pool of highly qualified candidates who are a perfect fit for their specific needs."}, {"title": "Data Engineer  - Need candidates from Texas ( REMOTE )", "company_name": "Pulivarthi Group (PG)", "location": "United States (Remote)", "link": "/jobs/view/3770685212/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=9gdMwuif%2BpB%2BCQYvQi3imQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3770685212", "description": "About the job\nFollow us on Linkedin: https://www.linkedin.com/company/pulivarthigroup/\n\nPulivarthi Group LLC is a Global Staffing & IT Technology Solutions company, with our prime focus of providing world class solutions to our customers with the right talent. We combine the expertise of our team and the culture of your company to help you with the solution that is affordable and innovative using high quality standards and technologies.\n\nWe\u2019ve served some of the largest healthcare, financial services, and government entities in the U.S.\n\nData Engineer - FT only - Azure SQL with data lakes experience\n\nData Engineer -\n\nthe resource requirement for this Data Engineer role needed more clarification.\n\nOur customer is requesting a resource who knows Azure CI/CD (Continuous Integration / Continuous Deployment) to build and integrate their DevOps pipeline.\n\nOnce the DevOps pipelines are mapped, they need the Data Engineer to connect to all the Data Sources, primarily their Data Lake.\n\nThe Customer (based in Houston) is interested in a resource in a time zone not too far removed from the Central Time Zone to allow real time collaboration."}, {"title": "Data Engineer", "company_name": "Otter Products", "location": "Fort Collins, CO (Remote)", "link": "/jobs/view/3766728302/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=77U3U7CAi1PxgDmpI3%2BiNQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3766728302", "description": "About the job\nOverview \n\nOtter Products is currently recruiting for a Data Engineer to join our Data team! You can be based in our Fort Collins, CO office with flexibility to work a portion of your time remotely, or 100% Remote in the U.S.\n\nThe Data Engineer applies professional experience, concepts, and company objectives toward building and operationalizing the minimally inclusive data necessary for the enterprise data initiatives following industry standard practices and tools. The bulk of the Data Engineer's work would be in designing, managing and optimizing data pipelines, then moving these data pipelines effectively into production for key data and analytics consumers like business/data analysts, data scientists or other stakeholder that needs curated data for data and analytics use cases across the enterprise. The Data Engineer also ensures compliance with data governance and data security requirements while creating, improving and operationalizing these integrated and reusable data pipelines to enable faster data access, integrated data reuse and vastly improved time-to-solution for data and analytics initiatives. Additionally, the Data Engineer will be expected to collaborate with other data team members and data consumers working on the models and algorithms developed by them toward optimization for data quality, security, and governance to put them into production leading to potentially large productivity gains.\n\n About Otter Products \n\nOtter Products, we grow to give. From our founder's garage in 1998 to the global technology leader we are today, Otter Products continues to drive growth through innovation.\n\nThrough our industry-leading brands - OtterBox, Liviri and OtterCares - we provide our partners the number-one selling and most trusted products in our categories. Our philanthropic spirit is the foundation on which we foster our partner relationships, allowing us to grow and to give - together.\n\nBy way of our charitable arm, the OtterCares Foundation, we support our communities and invest in our future through education that inspires kids to change the world.\n\nAnd even as our global community of Otters continues to grow, our founder's core values are still at the heart of everything we do. We measure our success by our ability to give back to our communities and strengthen opportunities for all.\n\nFor more information visit otterproducts.com\n\n Responsibilities \n\nComplete analysis, design and development of BI solutions using Azure SQL Server\nFamiliarity with Data Warehouse concepts\nExperience coding the extraction, transformation, and loading (ETL) processes\nDatabase development primarily in SSIS, Data Factory and SQL\nPartner with the business to determine end user database/reporting needs and requirements\nGenerate ad hoc reports using Power BI or SSRS\nCollaborate with other developers to create and implement best approach solution(s)\nTroubleshoot Azure tools, systems, and software\nReview queries for performance issues, making changes as needed\nCollaborate with team to performance-tune Azure application as necessary\nAssist with the analysis and extraction of relevant information from large amounts of historical business data to feed data science initiatives\nParticipate and support the design and documentation of processes for large scale data analyses, model development, model validation and model implementation\nSupport and maintain a positive safety culture by following all safety policies and procedures and actively contributing to a safe working environment\nOther duties as required\n\n Qualifications \n\nBachelor's degree required. Degree in Computer Science or Mathematics preferred\nMinimum of four years of experience in an IT or analytical role required. Experience in database development, report writing and/or statistics preferred.\n\n EEO \n\nOtter Products, LLC is an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, age, marital status, pregnancy, sex, sexual orientation, gender, gender identity or expression, national origin, disability, veteran status, or any other characteristic or status protected by law.\n\n For US Based Roles Only - Compensation Range Minimum \n\nUSD $90,000.00/Yr.\n\n For US Based Roles Only - Compensation Range Maximum \n\nUSD $120,000.00/Yr.\n\n Additional Total Rewards \n\nProfit Sharing Program Eligible, Benefits Eligible - Full Time- check out otterproducts.com/careers/why for more info"}, {"title": "Software Engineer, Distributed Systems", "company_name": "Hightouch", "location": "United States (Remote)", "link": "/jobs/view/3628572539/?eBP=CwEAAAGMd4QO8APODyE_-mMOPhYyQGg5ZjVbPwq_3V7DK0B2bLAf10pcedM9S2T71X5i4TiJuEwOzWBbSiTKbLuk-VD-_Y-dN5uyf8YpYkHYqEYFxySMS9zr2hRjNrxX9gXbxssh35TFK9YTNV2WeH2Ncb675JgFbJTgNWSZySFC6rrdJGmdSDTr2q5d3VoxORz3GjsDbzjeyZQCvQ07RqYw7oujWSgslycs2LMCY_-N0NEj6e0PvbaYKiZ_5IW34bGVSYLXpHwqa8oOcNttYZ90UA3E_7sDSFzABdIVlfrp5zzERrIYy5sh2MIInxHdDoKgQw8VmOn5uM4S0tXUl-i7LnN0_W0NfPBmLiVDNuveIOm4MfDNPxfe5QKPk6FUAVpvgbcoJn9yRyE&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=rph%2FT6Q6Vkp6YoggJf9cJQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3628572539", "description": "About the job\nAbout Hightouch\n\nHightouch\u2019s mission is to empower everyone to take action on their data. Through our Reverse ETL platform, business and data users can seamlessly sync data from where it resides, such as warehouses and databases, to where it is needed, including operational systems and SaaS tools. Traditionally, acting on data has required engineering time and bandwidth, and left most business users stuck with charts and reports that are unable to take automated action on their data. With Hightouch, every business user, without writing any code, can activate data to streamline critical processes, improve marketing performance, and scale operations.\n\nOur team operates with a focus on making a meaningful impact for our customers. We believe in approaching challenges with a first principles thinking mindset, moving quickly and embracing our value of efficient execution, and treating each other with compassion and kindness. We look for team members that are strong communicators, have a growth mindset, and are motivated and persistent in achieving our goals.\n\nHundreds of companies use Hightouch, including Spotify, Ramp, Retool, NBA, Plaid, and Betterment. We\u2019re based in San Francisco, are remote-friendly, and backed by leading investors such as Amplify Partners, ICONIQ Growth, Bain Capital Ventures, Y-Combinator, and Afore Capital.\n\nAbout The Role\n\nWe are looking for a distributed systems engineer to work on the systems that power the syncing engine our customers and other engineering teams rely on. This presents an exciting challenge where you can apply your expertise in distributed systems, performance optimization, and troubleshooting to push the boundaries of what is possible and meaningful improve our ability to move massive amounts of customer data. This role also provides a unique opportunity to work on a multi-cloud and multi-region infrastructure that supports a global customer base.\n\nWe believe in enabling our engineers to do their best work for our customers by giving them extremely high levels of ownership and autonomy. This comes in different forms: you will own and deliver projects from start to finish, you will work directly with customers to solve their hardest scaling problems, and you will have a lot of influence over what we work on as a team and company.\n\nSome of the problems we\u2019ll be working on include:\n\nSync Speed: Customers want to sync a lot of data to important destinations like Facebook and Snapchat, which requires us to analyze every part of our syncing process and find where we can optimize to sync data more quickly\nStreaming Syncing: We currently sync data from sources in batch only, but envision a world where we also do syncs in real-time and support streaming sources like webhooks and queues\nScalability and Reliability: As part of our rapid growth, we\u2019re always evaluating current future bottlenecks in scaling and reliability, and architecting for the next order of magnitude of growth\nPersonalization API: Our Personalization API product provides a low latency caching layer on top of data warehouses that enable customers to use their data warehouse for real-time personalization use cases with \nMulti-Region and Multi-Cloud: Supporting our multi-region and multi-cloud backend, including extending it to launch Hightouch on in new regions to support data residency requirements of our global customer base\n\nWe are looking for talented, intellectually curious, and motivated individuals who are interested in tackling the problems above. This is a senior role, but we focus on impact and potential for growth more than years of experience. The salary range for this position is $170,000 - $240,000 USD per year, which is location independent in accordance with our remote-first policy.\n\nAbout You\n\nYou are an engineer with a passion for solving hard technical problems that generate real value for customers. You\u2019re motivated by high ownership and are comfortable in a fast-paced, startup environment.\n\nYou have experience and comfort with distributed systems and high-scale systems or the ability and desire to build this experience quickly. This experience can take different forms:\n\nYou\u2019ve built syncing engines that process and move terabytes of data per day in a scalable way\nYou\u2019ve created event collection or data streaming services that power company critical systems\nYou\u2019ve owned systems that have undergone orders of magnitude of growth and helped them scale accordingly\nYou have relevant skills and are a fast learner who is excited to take on the challenges we face\n\nInterview Process\n\nOur goal with the interview process is to balance speed with giving both parties opportunities to assess whether there is a strong mutual fit. We will ask you questions, but we want you to ask us questions! Our technical interviews focus on how you design systems because we believe this is the best way for us to see how you work and for you to see how we collaborate. We don\u2019t ask you to write code to solve technical brainteasers that don\u2019t appear in your day to day job.\n\nRecruiter screen [30m]: Introductory call with our recruiting team to get to know each other and see if the role could be a good mutual fit.\nSystems design screen [45m]: Designing a data processing feature end-to-end.\nHiring manager interview [30m]: Chat with hiring manager about past experiences and future operating preferences to assess fit on company values and operating principles.\nSystems design interview [90m]: Work with the interviewer to architect a system at a conceptual level. The problem will be at a pretty high level - and have both product and customer requirements as well as technical."}, {"title": "Entry-Level Data Scientist Engineer", "company_name": "Phoenix Recruitment LLC", "location": "Texas City, TX (Remote)", "link": "/jobs/view/3788727625/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=8iwtytHsCu4wMwJ4fj3pxw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3788727625", "description": "About the job\nThis is a remote position.\n\nTitle - Entry-Level Data Scientist Engineer, 1 year of project experience\n\nEmployment Type: Full-time\n\nBase Salary: $60K-$70K\n\nPhoenix Recruitment offers a variety of recruiting services to assist both employers and employees. They specialize in marketing open positions, recruiting, and helping employers to find qualified candidates across various industries. Phoenix Recruitment has expertise in streamlining the hiring process. They can help ensure that the process is efficient, well organized, and compliant with relevant regulations.\n\nSkills and Abilities:\n\nStrong knowledge of R or Python for data analysis and modeling. \nProficiency in statistical programs such as R, SAS, MATLAB, or Python. \nFamiliarity with spreadsheets (VBA) and database applications (Access, Oracle, SQL, or equivalent technology). \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \nAbility to learn new methods quickly and work under deadlines. \nExcellent teamwork and communication skills. \nStrong analytical and problem-solving abilities. \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \n\nPreferred:\n\nKnowledge of actuarial concepts and life, health, and/or annuity products. \nExperience with statistical modeling techniques such as GLM, Decision Trees, Time Series, Regression, etc. \nFamiliarity with Microsoft DeployR. \nExposure to insurance risk analysis. \nBasic experience in computational finance, econometrics, statistics, and math. \nKnowledge of SQL and VBA. \nFamiliarity with R or Python for predictive modeling \n\nWhy Phoenix Recruitment LLC?\n\nPhoenix Recruitment often has an extensive network of employers and candidates. This network allows them to tap into a pool of qualified candidates and connect them with suitable job opportunities. They can also leverage their connections to help employers find the right talent efficiently. Outsourcing the recruitment process to a specialized agency can save your time and resources, avoid delays, reduce administrative burdens, and increase the chances of finding the right fit for your organization."}, {"title": "Data Engineer - Remote Work", "company_name": "BairesDev", "location": "United States (Remote)", "link": "/jobs/view/3783162297/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=OtoCrAAHZFomVup04fy35g%3D%3D&trk=flagship3_search_srp_jobs", "id": "3783162297", "description": "About the job\nWho We are\n\nBairesDev is proud to be the fastest-growing company in America. With people in five continents and world-class clients, we are only as strong as the multicultural teams at the heart of our business. To consistently deliver the highest quality solutions to our clients, we only hire the Top 1% of the best talents and nurture their professional growth on exciting projects.\n\nWe are looking for a Data Engineer to join our Development team and participate in different projects made up of multicultural teams distributed throughout the world.\n\nWhat You Will Do\n\n Interview other Data Scientists and Data engineers to vet their technical abilities.\n Input data from those interviews into the product and help develop a predictive, fair, and enjoyable solution.\n Work together with developers, tech leads, and solution architects to build applications.\n Improve existing structures by adding new functionalities or proposing technological updates so that as a result, the impact of your contribution is significant in the core of each business.\n Improve the user experience on scalable and high availability platforms, contributing to the key differential of each business.\n\nHere\u2019s What We Are Looking For\n\n 5+ years of experience working as a developer.\n 3+ years of experience working as a Data Scientist / Data Engineer.\n Proficient with analysis, troubleshooting, and problem-solving.\n Hands-on experience with managing data loads and data quality.\n Advanced English level.\n\nHow we do make your work (and your life) easier:\n\n 100% remote work.\n Hardware setup for you to work from home.\n Flexible hours - make your schedule.\n Paid parental leave, vacation & holidays.\n Diverse and multicultural work environment.\n An innovative environment with the structure and resources of a leading multinational.\n Excellent compensation \u2014 well above the market average.\n Here you can grow at the speed of your learning curve.\n\nOur people work remotely but with a consistent and robust culture that promotes diversity and teamwork. To continue being the leading software development company in Latin America, we want to ensure that every BairesDev member gets the best growth and professional development opportunities in a diverse, welcoming, and innovative environment.\n\nEvery BairesDev team member brings something unique to our company.\n\nWe want to hear your story. Apply now!"}, {"title": "Data Engineer (Remote)", "company_name": "Cascade Debt", "location": "United States (Remote)", "link": "/jobs/view/3755336571/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=pmUtn1j2zyPr25CNPh2RNw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3755336571", "description": "About the job\nDo you want to work in a small team where you can make a real difference in a company? Would you embrace the challenge of building a high impact platform that is used globally? Then we want to speak with you!\n\nAbout Cascade\n\nCascade is a fintech startup backed by Canadian and US investors that empowers companies to grow by democratizing access to institutional debt. We are building tools that modernize how companies raise and manage debt, lowering the barriers to entry to the $7 trillion specialty finance and alternative credit market for companies around the world. Founded just last year, we are gearing up for our next phase of technical development and are seeking a talented Data Engineer to lead the way.\n\nAbout The Role\n\nAs a Data Engineer you\u2019ll be an early member of a growing team building a pioneering platform in the debt infrastructure space. You will design, build, and launch efficient, scalable, and reliable data pipelines to move and transform data .\n\nWhat You\u2019ll Do\n\n Building databases, data lakes and data ingestion pipelines to integrate customer databases and datasets to our systems \n Ingesting financial datasets from external customers, then u pdating and maintaining accurate and complete data mappings to ensure that our product s are displaying high quality data \n M onitoring and alerting across our data pipelines in order to make sure that our data ingests are reliable and correct \n Translat ing business goals and stakeholder requirements into new data flows and deliver analytical insights ensuring that the data flows create real business value continuously \n\nSkills And Experience\n\n 2 + years experience in a data engineering or data analyst role \n E xperience with relational SQL and NoSQL databases, including PostgreSQL , MySQL, MariaDB, MongoDB, Bigtable, etc. \n E xperience working with ETL technologies , such as Databricks, Fivetran , or dbt \n Proficiency in writing SQL queries and knowledge of analytical data warehouses such as RedShift, BigQuery , and Snowflake \n Some experience with CI/CD automation such as GitHub Actions \n Developing APIs and integrating with 3rd party APIs \n Bonus: experience in fintech / SaaS / credit analysis \n\nPeople Who Thrive At Cascade Are\n\n Self-starters, who take the initiative to tackle challenges in a remote work environment \n Good communicators, who can operate without ego to discuss, learn, grow, and help others do the same \n Passionate about creating great digital experiences for users \n Problem solvers who are not satisfied with the status quo \n\nBenefits\n\nRemote: we are a remote-first company and are very flexible on hours as long as things get done \nHome-office: there\u2019s a $1000 USD home office allowance to set yourself up \nEquity: we expect you to have an owner-mentality, and have the equity plan to match \nBenefits: health, dental, vision, and more \nPerks: we offer free lunches weekly and off-site trips \nJob satisfaction: we offer autonomy, ample opportunities for mastery, and an opportunity to make a difference for companies around the world"}, {"title": "Data Engineer", "company_name": "ISC (Integrated Specialty Coverages, LLC)", "location": "Carlsbad, CA (Remote)", "link": "/jobs/view/3778293528/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ew7%2FfEuXSujs5IGabdqUyw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3778293528", "description": "About the job\nAbout Integrated Specialty Coverages\n\nIntegrated Specialty Coverages LLC (ISC) is a growth stage technology and data-driven insurance company leading innovation in the market.\n\nBacked by one of the leading private equity firms, KKR, and led by a forward-thinking management team, ISC is combining the worlds of insurance and technology to create an Insurtech powerhouse. As a leading online distributor of insurance products for a range of industries and \"Main Street USA\", we are looking for the right people to help us in our mission of achieving exponential growth. We strive to be the number one place to go for brokers and agents to source insurance. To accomplish this, we're building a digitally focused team that deeply understands the intersection between user experience, data, and AI/ML to optimize the way we engage with our customers and partners.\n\nJob Summary\n\nThe Data Engineer role will be the technical liaison between multiple groups including a data science team, the engineering team, product management, and business stakeholders. You do not need any insurance knowledge prior, however, you must quickly dive deep into the insurance world and ask questions to become a subject matter expert. You will be responsible for building a data platform to facilitate the data science team. You must be a self-starter that can build out features such as a data pipeline from scratch. There will be support from both engineering and data science for any buildout.\n\nJob Responsibilities\n\nCreate and maintain optimal data pipeline architecture. \nAssemble large, complex data sets that meet functional / non-functional business requirements. \nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. \nBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using AWS technologies, SQL, and Python. \nWork with stakeholders including the Executive, Product, Data Science, and Engineering teams to assist with data-related technical issues and support their data infrastructure needs. \nWork with data science and analytics teams to strive for greater functionality in our data systems. \n\nMinimum Qualifications:\n\nAdvanced working SQL knowledge and experience working with relational databases, strong query authoring (SQL) as well as working familiarity with a variety of databases (Snowflake, Redshift, MySQL, MSSQL, etc.)\nExperience building and optimizing data pipelines, architecture and data sets. \nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. \nBuild processes supporting data transformation, data structures, metadata, dependency, and workload management. \nA successful history of transforming, processing and extracting value from large disconnected, datasets from a variety of data sources (Flat files, Excel, databases, etc.)\nStrong analytic skills related to working with unstructured datasets. \nStrong project management and organizational skills. \nExperience supporting and working with cross-functional teams in a dynamic environment. \nAbility to mentor/guide/collaborate with other team members. \n\nWe are looking for a candidate with 3-5 years of experience in a Data Engineer role. They should also have experience using the following software/tools:\n\nExperience with relational and MPP databases, including Snowflake and MySQL. \nExperience developing software in an agile environment from the requirements stage to production. \nExperience with version control: git\nExperience with container technologies: Docker\nExperience with data pipeline and workflow management tools: Airflow, Jenkins, AWS Glue, Azkaban, Luigi, etc. \nExperience with AWS cloud services: EC2, ECS, Batch, S3, EMR, RDS, Redshift\nExperience with other cloud services: Snowflake, Airflow\nExperience with object-oriented/object function scripting languages: Python, Java, C++, etc. \nExperience with data modeling and data warehouse design\nExperience with data visualization tools (PowerBI, QuickSight)\n\nThe starting annual pay scale for this position is listed below. Actual starting pay will be based on factors such as skills, qualifications, training, and experience. In addition, the company offers comprehensive benefits including medical, dental and vision insurance, 401(k) plan with match, paid time off, and other benefits.\n\nISC's salary ranges are determined by role and level. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations and could be higher or lower based on a multitude of factors, including job-related skills, experience, and relevant education or training.\n\nNational Pay Range\n\n$100,000\u2014$110,000 USD\n\nBenefits of Working at ISC\n\nCompetitive vacation and flexible working arrangements\nComprehensive and inclusive health benefits\nA variety of professional development and mentorship opportunities\nChoice of technology whether at home or in the office\n\nISC's Ownership Behaviors\n\nDo the Right Thing *Be Relentless. Pursue Excellence. *Personal Responsibility & Own Outcomes\nTry Fast. Learn Fast. Fail Fast. Think Big. *Build Extraordinary Owners & Join Forces *Grit & Determination\n\nApplicants with disabilities may contact ISC HR department via e-mail, and other means to request and arrange accommodation. If you need assistance to accommodate a disability, you may request accommodation at any time. Please contact ISC at HR@ISCMGA.com.\n\nISC believes in creating long-term relationships by being responsive and relevant and by consistently delivering value to our community of customers. Specifically, with our employees, we focus on attracting, developing, and retaining the best talent for our business, challenging our people, demonstrating a \"can-do\" attitude, and fostering a collaborative and mutually supportive environment. ISC is an equal opportunity employer and a participant in the US federal E-Verify program (US).\n\nDiversity creates a healthier atmosphere: All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, gender, gender identity, sexual orientation, marital status, medical condition, genetic information, mental or physical disability, military or veteran status, or any other characteristic protected by local, state, or Federal law."}, {"title": "Data Engineer -Remote", "company_name": "RIT Solutions, Inc.", "location": "Denver, CO (Remote)", "link": "/jobs/view/3768007496/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=1hQH8EDaII1eJ7nQ9BACQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3768007496", "description": "About the job\nDuration: 6+ months CTH\n\nMust have a valid LinkedIn profile\n\nRequired Skills\n\n 3-6 years of experience (no more than 6 years)\n Start up environment experience\n Python, SQL, and DBT"}, {"title": "Big Data Engineer - W2", "company_name": "Megan Soft Inc", "location": "United States (Remote)", "link": "/jobs/view/3674845652/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fCAxh4N%2F4YZJ5aEIASaeeA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3674845652", "description": "About the job\nTitle: Big Data Engineer - W2\n\nLocation: Dearborn, MI (Remote)\n\nDuration: 12+ Months\n\nDirect Client: Ford Motors\n\nMust have skills: Should be able to handle GCP requirements\n\n\"Translates business requirements to build repeatable, sustainable, efficient, coded processes that can be productionized by Software Engineers and readily modified by other Data Engineer Creates POC processes in Dev/QA and works with Software Engineers to productionize and align best coding and process practices that maximize efficiency, speed, stability, system resources and capabilities Awareness of and compliance with data privacy, security, legal and contractual guidelines Incorporates data quality and privacy checks/alerts to minimize bad data being consumed by end users, models and dashboards, and to protect customer data; revises checks for new data issues Validates data products and pipelines are functioning as expected following system or application upgrades, source changes, etc. Works closely with Architects if required to align systems, tools and applications being utilized with business use case and performance requirements Provides data product support and maintenance Establishes, tracks and monitors KPIs related to specific data products and deliverable Participate in iteration cadence activities (Iteration Planning, task estimation, daily scrum, demos, etc.). Will be a part of on-call team to resolve critical production support tickets during business hours and off hours.\n\nSkills Required\n\nExperience with Big Data Technologies (Hadoop , ETL Tool, Spark. Hive etc..) Someone who understands Cloud and has experience working with Cloud Platforms (preferably GCP) Should be able to handle GCP requirements - ingesting into GCS/ Big Query, working with GCP components Hands-on in Data Flow, Cloud Storage (GCS), Big Query and GitHub. Proficient in Python and SQL.\n\nSkills Preferred\n\nManages ETL: uses programming and tools for data ingestion, configures pipelines, applies transformations and decoding, integrates and fuses data, moves and securely deliver Experience NoSQL databases\n\nAdditional Information\n\nIf the candidate is remote only, please indicate \"\"100% Remote\"\" under candidate's name on resume. Other similar schedule notations can be \"\"Local Candidate\"\" or \"\"Hybrid Candidate.\"\" ***HRA Test Required***\""}, {"title": "Remote Opportunity: Quantexa Data Engineer", "company_name": "SPAR Information Systems LLC", "location": "United States (Remote)", "link": "/jobs/view/3680073191/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ioykVCZSOQ7BK5%2FwRQHOWg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3680073191", "description": "About the job\nHello All, Hope you are doing great!! Please go through the job descriptions and let me know your interest. Role: Quantexa Data engineer  Location: Remote Duration: Long Term Contract Job Description: 1. End to End usage of Quantexa including guiding our client team on how to use from both front end and data ingestion Reason they are asking for Certification 2. Resolve Entity issues with data 3. Knowledge of Scala and Hive which requires coding experience using parquet 4. Advanced SQL knowledge requiring writing SQL queries 5. Experience having worked with GCP data lake 6. Quantexa certification required\n\nThanks & Regards,\n\nSatnam Singh\n\nDirect: 201 623 3660\n\nEmail : Satnam.singh@sparinfosys.com"}, {"title": "Remote Work - Need Staff Data Engineer - USC only", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3746283980/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=CvIuLBNO%2BPMGi4IXVRkP0A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3746283980", "description": "About the job\n100% Remote, client is in NY\n\nLook for strong job history, Python, Machine Learning, SQL, 10/10 comms, etc. \n\nMUST be an Engineer \n\nSummary\n\nWe're looking for an experienced Data Engineer to join our client's growing team of top notched IT professionals. You will work closely across multiple groups including Editorial and Audio, Marketing, Advertising, etc. to achieve their goals and objectives. The Data Engineering team builds tools throughout the stack while sharing knowledge across these departments.\n\nAs the company grows, they're looking for Data Engineer who will help solidify and expand our pipelines and maintain their data warehouse. Their business model is based on performing complex and very detailed analyses of how our products perform with their subscribers. You'll be responsible for working alongside analysts, product, marketing, and other teams to define new data collection components and measurement schemes that support each new product and feature.\n\nHow Can Add Value To Their Mission\n\nCreate and maintain data pipelines to provide insights and drive business decisions\nEstablish data warehousing strategy (ex. Kimball, Data Vault, etc.)\nMaintain data infrastructure on our AWS accounts\nCollaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.\nWrite unit/integration tests, contributes to engineering wiki, and documents work\nImplement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it\n\nWhat You'll Need To Succeed\n\n7+ years of industry experience measuring product performance and user behavior\nExperience working with a variety of data management technologies, including RedShift, Kinesis/Kafka, Glue, Spark, Postgres, Airflow, dbt, and others.\nExperience implementing BI reporting tools such as Looker\nExperience interfacing with engineers, product managers and analysts to understand data needs\nKnowledge of a variety of measurement beacons, SDKs, APIs including Google Analytics, Amplitude, Braze, Stripe, email service providers\nA commitment to building secure, resilient, fault-tolerant architectures, with clear documentation and procedures in place for support\nA focus on accuracy and detail as you build, ensuring data pipelines produce clear, predictable results\nUnderstanding of the typical metrics a subscription and advertising-supported business needs to measure success\nExperience with Client techniques as applied to behavioral segmentation or anomaly identification is a definite plus\nFamiliarity using developer tools that increase productivity and facilitate the development of resilient code (eg. Docker, CircleCI, Serverless) is a plus"}, {"title": "ENTRY LEVEL DATA ENGINEER (REMOTE)", "company_name": "SynergisticIT", "location": "Jacksonville, FL (Remote)", "link": "/jobs/view/3767587909/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=om7L3CnowiC4PLIcpZD70A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3767587909", "description": "About the job\nSYNERGISTICIT wants every candidate to know that the Job Market is Challenging and to stand out, you need to have exceptional skills and technologies and that's where we come in to make sure you get the attention which you need\n\nSynergisticIT understands the complex nature of the job market and how difficult it can be to secure a position, especially for fresh graduates. Therefore, we assist and help tech-savvies to convert their passions into professions. We go above and beyond to keep you working in your niche.\n\nAs we focus on long-term success, we provide complete career development solutions. From job search to upskilling portfolio and interview preparation, we can guide you at every step of your career.\n\nSynergisticIT spares no efforts to connect you with a large network of tech giants, including Google, Apple, PayPal, Dell, Cisco, Client, etc. Presently, we are actively looking for ENTRY LEVEL DATA ENGINEER a driven mindset. Get the right opportunity and gain experience in building web-centric solutions on Java.\n\nWho Should Apply : Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or anyone looking to make their career in IT Industry\n\nWe also assist in filing for STEM extension and H1b and Green card filing.\n\nCandidates who are serious about their future in the IT Industry and have set big goals for themselves.\n\nCandidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. We also offer Skill Enhancement Programs if the candidates are missing skills or experience which our clients need with great outcomes\n\nCandidates can benefit from skill enhancement if they fall into the below categories. If they are qualified with enough skills then no need for skill enhancement\n\nCandidates Who Lack Experience\n\nHave had a break in careers\n\nLack Technical Competency\n\ncandidates who want to get employed and make a career in the Tech Industry \n\nPlease Also Check The Below Links\n\nhttps://www.synergisticit.com/candidate-outcomes/\n\nhttps://www.synergisticit.com/java-track/\n\nhttps://www.synergisticit.com/data-science-track/\n\nhttps://www.synergisticit.com/which-is-the-best-option-for-tech-job-seekers-staffing-companies-consulting-companies-bootcamps-or-synergisticit/\n\nhttps://www.synergisticit.com/contact-us/\n\nIf the skills are not a match candidates can opt for Skill enhancement. Or their resume can be sent out to clients to see if responses are achievable\n\nREQUIRED SKILLS For Java/Software Programmers\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Core Java , javascript , C++ or software programming\nSpring boot, Microservices and REST API's experience\nExcellent written and verbal communication skills\n\nFor data Science/Machine learning\n\nRequired Skills\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Statistics, Python, data visualization tools\nExcellent written and verbal communication skills\n\nPreferred skills: NLP, Text mining, Tableau, Time series analysis\n\nTechnical skills are required by clients for selection even if its Junior or entry level position each additional Technical skill helps a candidate's resume to be picked by clients over other job seekers.\n\nClients hire candidates with the right technical skills which they need and reject candidates who lack the required technical skills.\n\nNo third party candidates or c2c candidates\n\nPlease apply to the posting\n\nNo phone calls please. Shortlisted candidates would be reached out"}, {"title": "Data Engineer - Remote | WFH", "company_name": "Get It Recruit - Information Technology", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3782241984/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=BgBf1kSAAX%2BSuw6inJsIVw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3782241984", "description": "About the job\nJoin an innovative team at a leading company in the engineering and manufacturing industry, where we are committed to leveraging data for continuous improvement and operational excellence. This hybrid position allows you to enjoy the flexibility of remote work on Mondays and Fridays.\n\nWhy Join Us\n\nAt our company, we prioritize your well-being and professional growth. We offer a comprehensive benefits package, including excellent health benefits, a 6% 401(k) match, generous paid vacation and sick time, 12 paid holidays, and more.\n\nRole Overview\n\nAs a Data Engineer, you will play a pivotal role in transforming raw data into actionable insights, driving improvements in engineering and manufacturing processes, enhancing product quality, and optimizing operational efficiency. Collaborate with dynamic cross-functional teams to extract, analyze, and visualize data, influencing strategic decisions through your valuable insights.\n\nKey Responsibilities\n\nData Collection and Integration:\n\nGather, clean, and integrate data from diverse sources, including production databases, sensor data, and external datasets.\n\nData Analysis\n\nConduct exploratory data analysis to identify patterns, trends, and anomalies.\n\nPerform statistical analysis and hypothesis testing to derive meaningful insights.\n\nData Visualization\n\nCreate informative and visually appealing dashboards and reports using tools like Tableau, Power BI, or Python libraries.\n\nPredictive Modeling\n\nDevelop predictive models to forecast manufacturing performance, quality, and operational outcomes.\n\nRoot Cause Analysis\n\nInvestigate production issues and quality concerns by analyzing data to identify root causes and recommend corrective actions.\n\nPerformance Monitoring\n\nEstablish and maintain key performance indicators (KPIs) to track manufacturing and engineering metrics.\n\nGenerate regular reports to communicate performance trends to stakeholders.\n\nCross-functional Collaboration\n\nCollaborate with engineering, manufacturing, and quality assurance teams to understand their data needs and provide analytical support.\n\nContinuous Improvement\n\nSuggest process improvements and optimizations based on data analysis.\n\nData Security And Compliance\n\nEnsure data security and compliance with relevant regulations, such as GDPR or industry-specific standards.\n\nInfrastructure And Development\n\nBuild and maintain the infrastructure for collecting, storing, and processing data.\n\nDesign, develop, and manage data pipelines, ETL processes, and data warehouses.\n\nRequired Experience\n\nProven experience as a Sr. Data Analyst and/or Data Engineer in an engineering or manufacturing environment.\n\nProficiency in data analysis tools and programming languages, such as Python, R, SQL, and Excel.\n\nStrong data visualization skills with tools like Tableau or Power BI.\n\nExcellent knowledge of statistical analysis and machine learning techniques.\n\nAbility to communicate complex findings and insights effectively to both technical and non-technical stakeholders.\n\nStrong problem-solving skills and attention to detail.\n\nFamiliarity with manufacturing processes and quality control is a plus.\n\nExperience with enterprise ERPs (SAP S4HANA, Oracle EBS), CRMs (Salesforce, Hubspot), and IoT.\n\nProficient in working with cloud computing platforms including AWS, GCP, and Azure.\n\nExpertise in configuring and optimizing cloud-based data solutions for scalability, reliability, and performance.\n\nBachelor's Degree in Data Science, Statistics, Engineering, or Computer Science Required.\n\nApply Now and be part of a team shaping the future of data-driven innovation in engineering and manufacturing!\n\nEmployment Type: Full-Time"}, {"title": "Entry Level Software Engineer - Data Backend Engineer (Remote - Canada)", "company_name": "Yelp", "location": "United States (Remote)", "link": "/jobs/view/3752881757/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fankeoxifvhpTzOYvowHYQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3752881757", "description": "About the job\nYelp engineering culture is driven by our values: we\u2019re a cooperative team that values individual authenticity and encourages creative solutions to problems. All new engineers deploy working code their first week, and we strive to broaden individual impact with support from managers, mentors, and teams. At the end of the day, we\u2019re all about helping our users, growing as engineers, and having fun in a collaborative environment.\n Interested in exploring data and finding innovative ways to collect it, curate it, and make it easily accessible for others to use? How about doing that at an enormous scale? Yelp\u2019s datasets contain billions of interactions between users and local businesses around the globe. If working with our vast amount of data sounds exciting, come and join us!\n\nThe Marketplace Data Observability team is responsible for managing, governing, and monitoring all of Yelp\u2019s Ads data to protect our revenue stream and also support offline analysis that aids in making long term company decisions. We build tools that help us to better understand our data quality, and we strive to uncover data issues before they impact our consumers.\n\nAs a Data Backend Engineer on this team, you will be responsible for building elegant and scalable data products that serve critical, up-to-date, structured information to support different types of analytics within Yelp. As a core contributor to our growing data modeling and data warehousing engineering efforts, you will help design and own mission-critical data flow pipelines and datastores to enable decisions including effective A/B testing and company investments.\n This opportunity is fully remote and does not require you to be located in any particular area in Canada. We welcome applicants from throughout Canada. We\u2019d love to have you apply, even if you don\u2019t feel you meet every single requirement in this posting. At Yelp, we\u2019re looking for great people, not just those who simply check off all the boxes.\n  Build systems that can effectively store and crunch terabytes of data.\nDesign and develop data models for efficient data storage, retrieval, and reporting.\nCreate and maintain conceptual, logical, and physical data models using industry-standard modeling tools.\nCollaborate with cross-functional teams, including engineers, data analysts, business analysts, and data scientists, to understand data requirements and translate them into effective data models.\nParticipate in data integration efforts, including ETL processes and data migration.\nStay up-to-date with industry best practices, emerging technologies, and trends related to data warehousing.\nSupport on-call rotations as needed to operate the team.\n  Understanding of high performing and scalable data systems.\nExperience in building and orchestrating ETL pipelines.\nExperience with Data Lake or Data Warehouse landscape.\nA hunger for tracking down root causes and fixing them in systematic ways.\nAbility to communicate effectively to technical and non-technical cohorts alike. \nExposure to some of the following technologies: Python, AWS Redshift, AWS Athena / Apache Presto, Big Data technologies (e.g S3, Hadoop, Hive, Spark, Flink, Kafka etc), NoSQL systems like Cassandra, DBT."}, {"title": "Data Engineer (Associate Consultant), June 2024", "company_name": "UDig", "location": "Nashville, TN (Remote)", "link": "/jobs/view/3716039960/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=XUjo40kU55HnDCqJ%2F3FdPg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3716039960", "description": "About the job\nCan UDig It? \n\nUDig designs, builds, and implements technology solutions that deliver on business objectives. People join our team because they are excited to have an impact on the future of our organization, passionate about delivering meaningful results, and energized by solving complex problems. Oh, and they like working with other driven, smart, and authentic people!\n\nUDig is proud to be recognized as a \"Best Places to Work\" in Virginia and Tennessee. We're committed to fostering a transparent environment where leadership is accessible, professional development opportunities are abundant, and you are empowered to achieve your career goals. You'll collaborate, develop deep relationships with teammates, and accelerate your success while having fun!\n\nIf you're graduating in May 2024 and excited by the idea of impacting organizations by building data solutions to achieve business goals, you should consider joining our growing technology team.\n\nHow Things Go Down\n\nAs a new, entry-level teammate, you'll take part in our \"Breaking Ground\" training program designed to help you quickly get up-to-speed on UDig's delivery approach within a project team. The program covers:\n\nUDig delivery process and methodologies such as gathering requirements and delivering projects using agile\nTechnical guidance and instruction on the best practices from Data leaders at UDig\nDelivering executive and technical presentations\nCoaching to prepare you to for success on your first client project at UDig\n\nA typical day might entail:\n\nProject standups and check-ins\nDesign and development of data pipelines, BI reports, and more\nPlanning and participating in project demos\nContributing to internal UDig projects\nCareer and job growth through UDig-provided training\nCreating long-lasting friendships with teammates through fun activities as well as your projects (including social events, winning a prize for guessing the weight of pumpkin, or sharing experience on cool tech side projects)\n\nUDig believes it is important for Associate Consultants going through our Breaking Ground program to reside in one of our major markets (Richmond, VA or Nashville, TN metro area) for at least the first year of employment.\n\nWant to learn more? Hear directly from our teammates by checking out our 'A Day in the Life' blog series: https://www.udig.com/digging-in/breaking-ground-2023/\n\nSound Interesting? Here's the Technical Piece\n\nBelow are a few boxes you should be able to check if\u202fyou're interested: Don't check every box? Go ahead and apply!\n\n1+ year of experience with relational database management systems such as Microsoft SQL Server, MySQL, or Oracle\nUnderstanding of the full lifecycle of design, development, and implementation of data solutions including architecture design, development, testing\nSome experience in ETL development or reporting is a plus\nAbility to present technical ideas and high-level concepts and solutions to internal and external team members with varying degrees of technical knowledge\nBachelor's Degree in in computer science, information technology, or any science or business discipline\nAbility to take your work seriously but also seriously enjoy a good time with your UDig colleagues\n\nWhat's in It For You? \n\nAt UDig, you will build your career alongside talented and experienced developers while gaining experience and having fun! We provide continuous challenges and professional development opportunities. Additionally, we offer: \n\nCompetitive salary, merit reviews, and career advancement paths\nFlexible, hybrid environment\nIndividual $1500 Training Budget\nRegular team building and social activities (virtual and/or in-person)\nTransparent culture with strong communication and access to leadership\nGreat benefits like\nGenerous Paid Time Off, company holidays, and parental leave\nMultiple Single and Family Health Insurance plans to choose from\nDental & Vision coverage\nShort-Term and Long-Term disability\nOptional accident and critical illness coverage\nMatching 401(k)\nand more!"}, {"title": "Expression of Interest: Data Engineer", "company_name": "Fingerprint for Success (F4S)", "location": "New York, NY (Remote)", "link": "/jobs/view/3787769880/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=l%2FEDpQqUNmCttGdIyTk7Cg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787769880", "description": "About the job\nWe are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.\n\nThe F4S Talent Pool is a pilot project designed to:\n\nHelp job seekers get discovered by our partners based on their anticipated hiring needs.\nProvide optional support and resources for job seekers in their career endeavors.\nHelp individuals understand, and bring out the best in themselves and each other.\n\nThe F4S Talent Pool process:\n\nOnce you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.\n\nAbout Fingerprint For Success (F4S)\n\nBacked by 20+ years of research, F4S\u2019s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high performing teams.\n\nKeep in mind, joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your F4S work style assessment to match the hiring needs of our partners.\n\nYour feedback is a gift! Write to us via:\n\nPowered by JazzHR\n\noWuPJ9lwq9"}, {"title": "Data Engineer Seattle, WA (remote)", "company_name": "Conch Technologies, Inc", "location": "United States (Remote)", "link": "/jobs/view/3765052211/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=SBETI78UMutCHuRp5FnP%2FA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3765052211", "description": "About the job\nHi,\n\nGreetings from Conch Technologies Inc\n\nData Engineering Lead \u2013 With Databricks and Unity Catalog\n\nLocation \u2013 Seattle, WA (remote)\n\nDuration \u2013 12 months\n\nNeed: Design and implement data architecture and strategy, leveraging technologies such as Databricks, Unity Catalog, Privacera, and Collibra.\n\nAs a Data Engineering Lead, you will play a pivotal role in defining and executing technical deliverables while assessing complexities and levels of effort. You'll engage with both vendor and internal teams to resolve technical challenges and ensure seamless collaboration. Your leadership will extend to planning and prioritization with the SCRUM master, facilitating effective coordination between onsite and offshore teams, and driving feature prioritization through collaboration with product managers.\n\nKey Responsibilities\n\n Lead the definition and breakdown of technical deliverables, assessing complexities and levels of effort.\n Collaborate with vendor and internal teams to address technical issues.\n Drive planning and prioritization in conjunction with the SCRUM master.\n Coordinate with onsite and offshore teams for timely project delivery.\n Collaborate with product managers to prioritize features, communicate with stakeholders, and clarify requirements.\n Collect and manage user-reported issues, ensuring prompt resolution and communication.\n Define comprehensive test plans, oversee their execution, and document results.\n\n--\n\nWith Regards,\n\nNagesh G\n\nMobile: 408-381-5645\n\nDesk: 901-313-3066\n\nEmail: nagesh@conchtech.com \n\nWeb: www.conchtech.com"}, {"title": "Remote Work - Need Azure Fabric Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3759325599/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=C3iVjmcR9wjUip1nNjUiCg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3759325599", "description": "About the job\nThe client is placing heavy emphasis on Microsoft and Azure Fabric experience\n\nInformation Management Engineer \u2013 Take the identified information management sources and assist migration to the Client Data Lake, Azure Fabric. The engineer will transform information management (de-dup, clean and enrich).\n\nTake the identified data sources and bring them into our Data Lake, Azure Fabric. Once the data is in, the engineer will need to be able to transform data (de-dup, clean and enrich the data). We are focused on using a medallion data model for data architecture. The tools we are working with are: Delta Parquet files, python, spark, notebooks, T-SQL, Fabric lakehouse/warehouse, Dataflows Gen2, Data Pipelines, Direct Lake, shortcuts, ADF."}, {"title": "Data Engineer", "company_name": "Catalist", "location": "Washington, DC (Remote)", "link": "/jobs/view/3744951279/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ui1Fk6ICmnPNqAlZDZAcwA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3744951279", "description": "About the job\nFor over 17 years, Catalist has been a leader in civic data and data science innovation. Our mission is to provide progressive organizations with the data, software, and services needed to better identify, understand, and communicate with the people they need to engage and mobilize. Our clients include the largest, most influential organizations in the U.S. active in civic engagement, advocacy, and political campaigns.\n\nCatalist is home to a dedicated, creative team of technologists, data scientists, and campaign experts committed to using our talents and technology to nurture a vibrant and growing progressive community.\n\nAs a Data Engineer at Catalist, you will work closely with the Analytics and Technology departments to design, build, support, and maintain various data pipelines and processes with an end goal of providing data and intelligence to the progressive community. This role primarily involves translating data architecture designs into functional processes, code, and systems.\n\nThe ideal candidate will be a highly motivated individual with excellent technical skills, a strong desire to learn new skills, and an interest in progressive politics. Catalist values creativity and problem-solving. Our work is on the cutting edge of data-driven politics, and your support will help Democratic candidates and progressive organizations conduct successful advocacy and electoral campaigns.\n\nThis position reports to the Deputy Chief Data Officer. The Data Engineer is a part of a growing Data team that supports all underlying work at Catalist.\n\nThis position is included in our CWA bargaining unit.\n\nPrinciple Duties & Responsibilities\n\nDevelop scalable, production processes, code, and systems for data ingest, transformation, modeling, and reporting across a variety of platforms\nTranslate mock-ups and designs into functional processes, code, and systems\nCreate architecture designs when needed\nProvide quality assurance and testing on processes, code, systems, and products\nBecome an internal subject matter expert on various datasets and support other Catalist departments/teams on usage of those datasets\nExecute ad-hoc data and database maintenance tasks as requiredProject manage cross-departmental efforts, with direct responsibility for stakeholder engagement, management, and execution of technical elements\n\n\nRequirements\n\nBS or BA in a technical field, or relevant experience\n1-2 years of experience working with SQL databases\nExperience with data cleaning or ETL processes\nExperience with distributed computing systems and/or distributed datastores (particularly the Hadoop ecosystem)\nExperience managing projects\nFamiliarity with Catalist data, progressive politics, voter files, and/or commercial data\nBackground check required\n\n\nPreferred Skills & Abilities\n\nWillingness to be a problem solver and produce results in a fast paced environment\nAbility to be creative and personable, and articulate ideas clearly\nExperience working with SQL databases\nProficiency with Python or another object-oriented programming language (R, Java, Scala, etc\u2026)\nExperience working in cloud environments (AWS, GCP, etc.)\nExperience working in command line environments such as Bash\nExperience with a version control tool such as git or github\n\n\nBenefits\n\nMedical, Dental, Vision, Prescription Drug\n\nCatalist offers Medical, Dental, Vision, and Prescription Drug coverage for eligible staff and their eligible dependents. Catalist\u2019s Medical plan is a comprehensive PPO program including Prescription Drug coverage with 85% of the premium paid by Catalist. Dental and Vision coverage is provided at no cost to employees.\n\nGroup Term Life Insurance and Long-Term & Short-Term Disability Coverage\n\nGroup Term Life Insurance and Long-Term and Short-Term Disability coverage is available for eligible staff. These benefits are provided at no cost to Catalist employees.\n\n401(k) Safe Harbor Plan\n\nA 401(k) Safe Harbor Plan is available to eligible staff with a 3% contribution from Catalist from the date of hire. Employees may contribute pre-tax or post-tax from their salary up to the legal limits set forth by the IRS.\n\nMedical and Dependent Care Flexible Spending Accounts (FSAs)\n\nCatalist offers an FSA Program that gives eligible staff the ability to pay out-of-pocket medical/dental/vision/child care expenses from pre-tax earnings.\n\nTransit Benefits\n\nCatalist also makes available a Transit benefit FSA program to eligible employees using pre-tax contributions with a company match.\n\nProfessional Development and Remote Work Expenses\n\nEligible employees may be reimbursed up to $750 each year for professional development / education and remote work expenses.\n\nStudent Loan PayDown or SaveUp\n\nCatalist offers a Student Loan PayDown and College SaveUp benefit for eligible staff.\n\nVacation, Personal Leave, Sick Leave Benefits\n\nCatalist offers generous vacation benefits to all eligible staff. Eligible employees also receive:\n\n 14 Paid Holidays\n Personal Days\n Sick Leave\n Parental Leave\n\n\nHybrid Office/Remote Work\n\nCertain positions at Catalist are eligible for Office/Remote Hybrid or full Remote status."}, {"title": "Remote Entry Level Data Analyst/Engineer", "company_name": "SynergisticIT", "location": "White Plains, NY (Remote)", "link": "/jobs/view/3774171423/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=q54sk1%2FRww0z%2BC%2BE8e0jQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774171423", "description": "About the job\nAt SynergisticIT, we're all about making connections. Whatever IT goals you have, our software programmers can help achieve those. Our Software development teams can take up turnkey projects and execute them in an effective and efficient manner. If you are looking to source talent our recruiters will find the ideal IT talent for your company. What's the secret to our success? Well, it all starts with taking quality time to listen to each client's specific needs. After we have a thorough grasp of your IT goals, we can better customize our Developments as per your specific needs. We can also tailor make recruiting programs to exceed your expectations. Since our founding in 2010, SynergisticIT's strategies have earned the company an enviable position in the software development, IT staffing and IT skill enhancement fields. SynergisticIT continues to work with hundreds of satisfied American clients with our software programmers working on our projects and after gaining hands on experience on cutting edge technologies moving to contribute their skills to great clients like Apple, Google, Client, Ebay, Paypal, Kroger, the Walt Disney Company and hundreds more. If you are tired of working with inefficient programmers who take a lot of time to ramp up we want you to try us. Our software programmers can hit the ground running and get you the maximum return on your investment. You have already tried the rest its time you tried the best. SynergisticIT - Home of the Best Data Scientists and Software Programmers in the Bay Area.\n\n Why Us ? \n\nSynergisticIT has a proven track record of successfully skill enhancement and staffing IT employees for some of the world's most iconic brands. Our team takes the time to fully understand every client's needs so we could best meet your IT staffing requirements. The knowledgeable staff at SynergisticIT is always more than happy to work with clients to ensure they reach their software development goals. Besides staffing, SynergisticIT is also committed to helping young IT professionals advance their career with a robust upskill program . Everyone who goes through SynergisticIT's program learns all the skills necessary to succeed in many IT fields ranging from Java to Machine Learning. Additionally, everyone trained at SynergisticIT has been through extensive mock and technical interview screenings to bolster their career prospects. Last, but certainly not least, SynergisticIT takes great care to respect the privacy considerations for every client. All companies who work with SynergisticIT can rest assured their confidential data is protected using the most up-to-date encryption technologies. SynergisticIT also complies with all the latest NDA agreements.\n\n REQUIRED SKILLS For Java /Software Programmers \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Time series analysis \n\n We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2022 and at Gartner Data Analytics Summit (Florida)-2023 \n\nOracle CloudWorld Event (OCW) Las Vegas 2022 | SynergisticIT - YouTube\n\n https://www.youtube.com/watch?v=OAFOhcGy9Z8 \n\n https://www.youtube.com/watch?v=EmO7NrWHkLM \n\n https://www.youtube.com/watch?v=NVBU9RYZ6UI \n\n https://www.youtube.com/watch?v=Yy74yvjatVg \n\nSynergisticIT at Gartner Data and Analytics Summit 2023 - YouTube\n\nFor preparing for interviews please visit  https://www.synergisticit.com/interview-questions/ \n\n We are looking for the right matching candidates for our clients \n\n Please apply via the job posting \n\n REQUIRED SKILLS For Java /Full Stack/Software Programmer \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning Positions \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Tensorflow \n\n If you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team. \n\n No phone calls please.  Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates"}, {"title": "Data Engineer", "company_name": "Anika Systems", "location": "Leesburg, VA (Remote)", "link": "/jobs/view/3787766893/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=mApL9JnN%2B%2Fr2%2BYUPlxRYOA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787766893", "description": "About the job\nAnika Systems is a fast growing, woman-owned small business that specializes in providing innovative IT solutions for federal government agencies. Our expertise lies in accelerating delivery in Data and Analytics, Intelligent Automation, Application Development, and IT Modernization. We are currently expanding our Federal team and are seeking a passionate and talented Data Engineer. This opportunity is 100% remote.\n\nMust be a U.S. Citizen with the ability to obtain and maintain a government suitability clearance\n\nResponsibilities\n\nDesigns, develops, builds, analyzes, evaluates, and installs database management systems to include database modeling and design, relational database architecture, metadata, and repository creation and configuration management.\nDefines and oversees database organizations, standards, controls, procedures, and documentation. Provides technical consulting in the definition, design, and creation of a database environment.\nAdvises applications development staff and users on data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nDesigns and implements databases with respect to access methods, access time, batch processes, device allocation, validation checks, organization, protection and security, documentation, and statistical methods.\nUses data mapping, data mining, and data transformational analysis tools to design and develop databases.\nDetermines data storage and optimum storage requirements.\nPrepares system requirements, source analysis, and process analyses and designs throughout the database implementation.\n\nRequired Skills And Experience\n\nBA/BS and 1 year of relevant experience\nExperience with DataBricks, SQL, Python\nApplications development experience with data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nExcellent communication skills (both oral and written) with process-oriented organizational skills to ensure project success\n\nDesired Skills And Experience\n\nDemonstrated successful development experience with the full life cycle of government database management systems including database modeling and design, relational database architecture, metadata and repository creation, and configuration management.\n\nPowered by JazzHR\n\nTlKWIorucJ"}, {"title": "Remote Work - Need Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3774712978/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=r3Bp%2F0qcOD2OpC5jpbUOcA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774712978", "description": "About the job\nTitle : Data Engineer\n\nLocation : Remote\n\nDuration : 6+ months to start (long term contract (work order are extended 2x year))\n\nPlease Fill out the skill matrix: Technology Some Awareness (1) Novice (2) Intermediate (3) Advanced (4) Expert (5) SQL Tableau Desktop /Creator Tableau API Integration AWS - S3 Glue AWS - Athena Redshift AWS - Lambda AWS - SNS/SQS msExcel\n\nProfile: Data Analyst will work closely with Architects and leads in the Interflow team, key business stakeholders including program staff to support the organizations effort in the areas of data Analysis and information management. Role should have strong experience in writing SQL queries to extract data for reports. Candidate should have experience working with large set of data. As these reports are very critical for the company and we are looking for an expert who is capable of handling, consuming and extracting large sets of data for analysis and reports.\n\nMust Haves\n\nAWS - Athena\nAWS - S3\nGlue\nhands-on development of reporting applications\nMS Excel\nRedShift\nSQL\nTableau Desktop /Creator\n\nNice\n\nAWS - Lambda\nAWS - SNS/SQS\nTableau API Integration\n\nSkills\n\nBA/BS required (major in an analytical field desired) \nA minimum 10+ years work-related experience. \nExperience in hands-on development of reporting applications for the Web in a professional environment \nThe Ethos of continuous improvement and interest in learning new things.\nStrong analytical thinking and structured problem-solving ability\nAbility to handle multiple projects and assignments simultaneously and effectively in a cross-functional team environment. \nVersed on the agile methodology and best practices. \nExcellent interpersonal and collaboration skill with the ability to work with a diverse set of colleagues, across functions, from different organizations, disciplines, etc. \nSelf-starter, ability to set priorities, work independently and attain goals"}, {"title": "Data Engineer", "company_name": "Team Remotely Inc", "location": "Houston, TX (Remote)", "link": "/jobs/view/3786249829/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=78lY5Jo%2FfyRPC6TQYCdvww%3D%3D&trk=flagship3_search_srp_jobs", "id": "3786249829", "description": "About the job\nThis is a remote position.\n\n Data Engineer (US/Canada Residents Only, 1 year experience, remote)\n\nTeam Remotely Inc. is a staffing and recruitment agency that offers a comprehensive solution for talent acquisition, including sourcing, vetting, pay rolling, and managing talent. Whether you need contract staffing, direct hire, direct sourcing, talent pools, or diversity initiatives, our model can support your hiring strategy.\n\n Hiring Type: Full-Time\n\n Base Salary: $55K-$70K Per Annum.\n\nResponsibilities\n\nDesigning user interface changes for web-based DB applications. \nReviewing application requirements and interface designs. \nDeveloping and implementing highly responsive user interface components using react concepts. \nWriting application interface codes using JavaScript following react.js workflows. \nTroubleshooting interface software and debugging application codes. \nDeveloping and implementing front-end architecture to support user interface concepts. \nMonitoring and improving front-end performance. \n\nQualifications And Experience\n\nBachelor\u2019s degree in computer science, information technology, or a similar field. \nPrevious experience working as a react.js developer. \nIn-depth knowledge of JavaScript, CSS, HTML, jQuery, and front-end languages. \nKnowledge of REACT tools including React.js, Webpack, Enzyme, Redux, and Flux. \nExperience with user interface design. \nKnowledge of performance testing frameworks, including Mocha and Jest. \nExperience with browser-based debugging and performance testing software. \nTroubleshooting skills. \nProject management skills. \nProblem-solving skills. \nVerbal communication skills. \n\n Why work with Team Remotely?\n\nTeam Remotely Inc. is a staffing platform offering a seamless experience for employers and candidates. Employers can post job openings and specify their requirements, while candidates can create profiles and upload resumes.\n\nThe team of Team Remotely continuously learns and adapts based on previous successful placements, constantly improving its matching capabilities. This ensures that the recommendations provided by Team Remotely are tailored and accurate, increasing the likelihood of a successful match between employers and candidates. By providing intelligent and data-driven solutions, they strive to enhance the efficiency and effectiveness of the hiring process, ultimately helping companies find the best talent and individuals find their dream jobs."}, {"title": "Data Engineer", "company_name": "ZAG Technical Services", "location": "United States (Remote)", "link": "/jobs/view/3771403464/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=Hf0VefrKXBFIL5MtHmY0Pw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3771403464", "description": "About the job\nWe are looking for a tech savvy Data Engineer to contribute to the building, operating, and scaling of next generation data platforms and tools that will power data-driven analytics capabilities throughout the ZAG products and services portfolio, spanning areas such as business intelligence, reporting, and data analytics.\n\nThe ideal team member is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. Must be comfortable supporting the data needs of multiple clients and teams, systems, and products within the modernization of ecosystems. The right-fit person will be excited by the prospect of optimizing or even re-designing data processes to support next generation of advanced analytics capabilities, which in-turn translates to implementing strategic enhancements to maintain competitive advantage in the industry.\n\nThe successful team member will leverage their proficiency to\u2026\n\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\nBuild the analytics processes required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Data Management and cloud based \u2018big data\u2019 technologies.\nBuild analytics tools that utilize technologies to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.\nCollaborate with client teams to identify and solution regional and local requirements within the context of the standard global business model for enterprise analytics.\nParticipates in meetings, discussions, strategy sessions where changes, improvements, and enhancements are proposed, evaluated, and approved. \n\n\nThe exceptional team member will have or demonstrate progressive experiences in\u2026 \n\nBachelor\u2019s degree or higher in Computer Science or related field\nExperience working with BI and data warehousing tools\nExperience working with enterprise data, where security is paramount and data governance is critical\nAbility to work independently and in a team environment, and effectively engage all levels of the organization\nUnderstanding of distributed systems driving large-scale data processing and analytics with a successful history of manipulating, processing, and extracting value from large, disconnected datasets.\nAbility to communicate effectively (listening, presenting, and questioning)\nStrong organizational, written, and communication skills\n\n\nAbout ZAG\n\nZAG Technical Services empowers our clients\u2019 success through intelligent business solutions that increase employee productivity, network security and team collaboration. We aim to ensure that our client\u2019s IT investments drive ROI, provide a competitive advantage and help businesses grow. We do this by living our core set of values: Integrity Foremost, Client-Centered, Accountable Always, Teamwork Throughout, and Exceptional is the Goal.\n\nWhat ZAG Has To Offer\n\nCompetitive compensation package \nBenefits including medical, dental, vision, 401K and life insurance \n\n\nZAG Technical Services is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all of our employees.\n\nZAG Technical Services is an E-Verify participant and will provide the Social Security Administration (SSA) and, if necessary, the Department of Homeland Security (DHS), with information from each new employee\u2019s Form I-9 to confirm work authorization."}, {"title": "Junior Data Engineer", "company_name": "HireMeFast LLC - Career Accelerator - Land A Job", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3786249797/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D&trk=flagship3_search_srp_jobs&lipi=urn%3Ali%3Apage%3Ad_flagship3_search_srp_jobs%3BQExogrHBQoSF9zZm%2B9kufw%3D%3D&lici=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D", "id": "3786249797", "description": "About the job\nThis is a remote position.\n\n Job Title: Junior Data Engineer\n\n Employment Type: Full-Time\n\n Salary: $64,000 - $76,000 per annum\n\n Experience Required:  Minimum 1 year of project experience\n\nAbout us: HireMeFast is a leading staffing and recruitment agency specializing in connecting businesses with top-tier talent across various industries. Our mission is to bridge the gap between exceptional candidates and organizations needing their skills, expertise, and unique qualities. Our team of experienced and dedicated recruitment specialists utilizes innovative sourcing strategies, and a vast network to identify and attract top talent. We conduct comprehensive procedures to ensure that only the most qualified candidates are presented to our clients.\n\nPosition Summary\n\nJoin the fast-paced, innovative, and collaborative environment focused on providing an AIOps platform that enhances the intelligence of the CVS Health infrastructure. Work closely with subject matter experts and colleagues to build and scale out machine learning and AI solutions that will detect, predict, and recommend solutions to correct issues before system impact and enhance efficiency, reliability, and performance of CVS Health\u2019s IT operations.\n\nKey Responsibilities include:\n\nData pipeline development: Designed, implemented, and managed data pipelines for extracting, transforming, and loading data from various sources into data lakes for processing, analytics, and correlation. \nData modeling: Create and maintain data models ensuring data quality, scalability, and efficiency \nDevelop and automate processes to clean, transform, and prepare data for analytics, ensuring data accuracy and consistency \nData Integration: Integrate data from disparate sources, both structured and unstructured to provide a unified view of key infrastructure platform and application data \nUtilize big data technologies such as Kafka to process and analyze large volumes of data efficiently \nImplement data security measures to protect sensitive information and ensure compliance with data and privacy regulation \nCreate/maintain documentation for data processes, data flows, and system configurations \nPerformance Optimization- Monitor and optimize data pipelines and systems for performance, scalability and cost-effectiveness \n\nCharacteristics of this role:\n\nTeam Player: Willing to teach, share knowledge, and work with others to make the team successful. \nCommunication: Exceptional verbal, written, organizational, presentation, and communication skills. \nCreativity: Ability to take written and verbal requirements and come up with other innovative ideas. \nAttention to detail: Systematically and accurately research future solutions and current problems. \nStrong work ethic: The innate drive to do work extremely well. \nPassion: A drive to deliver better products and services than expected to customers. \n\nRequired Qualifications\n\n2+ years of programming experience in languages such as Python, Java, SQL \n2+ years of experience with ETL tools and database management (relational, non-relational) \n2+ years of experience in data modeling techniques and tools to design efficient scalable data structures \nSkills in data quality assessment, data cleansing, and data validation \n\nQualifications\n\nKnowledge of big data technologies and cloud platforms \nExperience with technologies like PySpark, Databricks, and Azure Synapse. \n\nEducation\n\nBachelor\u2019s degree in Computer Science, Information Technology, or related field, or equivalent working experience\n\nWhy HireMeFast LLC?\n\nAt HireMeFast, we understand that finding the right individuals to join your team is crucial for success and growth of your organization. We are committed to streamlining the hiring process for our clients, ensuring they have access to a diverse pool of highly qualified candidates who are a perfect fit for their specific needs."}, {"title": "Data Engineer  - Need candidates from Texas ( REMOTE )", "company_name": "Pulivarthi Group (PG)", "location": "United States (Remote)", "link": "/jobs/view/3770685212/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=9gdMwuif%2BpB%2BCQYvQi3imQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3770685212", "description": "About the job\nFollow us on Linkedin: https://www.linkedin.com/company/pulivarthigroup/\n\nPulivarthi Group LLC is a Global Staffing & IT Technology Solutions company, with our prime focus of providing world class solutions to our customers with the right talent. We combine the expertise of our team and the culture of your company to help you with the solution that is affordable and innovative using high quality standards and technologies.\n\nWe\u2019ve served some of the largest healthcare, financial services, and government entities in the U.S.\n\nData Engineer - FT only - Azure SQL with data lakes experience\n\nData Engineer -\n\nthe resource requirement for this Data Engineer role needed more clarification.\n\nOur customer is requesting a resource who knows Azure CI/CD (Continuous Integration / Continuous Deployment) to build and integrate their DevOps pipeline.\n\nOnce the DevOps pipelines are mapped, they need the Data Engineer to connect to all the Data Sources, primarily their Data Lake.\n\nThe Customer (based in Houston) is interested in a resource in a time zone not too far removed from the Central Time Zone to allow real time collaboration."}, {"title": "Data Engineer", "company_name": "Otter Products", "location": "Fort Collins, CO (Remote)", "link": "/jobs/view/3766728302/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=77U3U7CAi1PxgDmpI3%2BiNQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3766728302", "description": "About the job\nOverview \n\nOtter Products is currently recruiting for a Data Engineer to join our Data team! You can be based in our Fort Collins, CO office with flexibility to work a portion of your time remotely, or 100% Remote in the U.S.\n\nThe Data Engineer applies professional experience, concepts, and company objectives toward building and operationalizing the minimally inclusive data necessary for the enterprise data initiatives following industry standard practices and tools. The bulk of the Data Engineer's work would be in designing, managing and optimizing data pipelines, then moving these data pipelines effectively into production for key data and analytics consumers like business/data analysts, data scientists or other stakeholder that needs curated data for data and analytics use cases across the enterprise. The Data Engineer also ensures compliance with data governance and data security requirements while creating, improving and operationalizing these integrated and reusable data pipelines to enable faster data access, integrated data reuse and vastly improved time-to-solution for data and analytics initiatives. Additionally, the Data Engineer will be expected to collaborate with other data team members and data consumers working on the models and algorithms developed by them toward optimization for data quality, security, and governance to put them into production leading to potentially large productivity gains.\n\n About Otter Products \n\nOtter Products, we grow to give. From our founder's garage in 1998 to the global technology leader we are today, Otter Products continues to drive growth through innovation.\n\nThrough our industry-leading brands - OtterBox, Liviri and OtterCares - we provide our partners the number-one selling and most trusted products in our categories. Our philanthropic spirit is the foundation on which we foster our partner relationships, allowing us to grow and to give - together.\n\nBy way of our charitable arm, the OtterCares Foundation, we support our communities and invest in our future through education that inspires kids to change the world.\n\nAnd even as our global community of Otters continues to grow, our founder's core values are still at the heart of everything we do. We measure our success by our ability to give back to our communities and strengthen opportunities for all.\n\nFor more information visit otterproducts.com\n\n Responsibilities \n\nComplete analysis, design and development of BI solutions using Azure SQL Server\nFamiliarity with Data Warehouse concepts\nExperience coding the extraction, transformation, and loading (ETL) processes\nDatabase development primarily in SSIS, Data Factory and SQL\nPartner with the business to determine end user database/reporting needs and requirements\nGenerate ad hoc reports using Power BI or SSRS\nCollaborate with other developers to create and implement best approach solution(s)\nTroubleshoot Azure tools, systems, and software\nReview queries for performance issues, making changes as needed\nCollaborate with team to performance-tune Azure application as necessary\nAssist with the analysis and extraction of relevant information from large amounts of historical business data to feed data science initiatives\nParticipate and support the design and documentation of processes for large scale data analyses, model development, model validation and model implementation\nSupport and maintain a positive safety culture by following all safety policies and procedures and actively contributing to a safe working environment\nOther duties as required\n\n Qualifications \n\nBachelor's degree required. Degree in Computer Science or Mathematics preferred\nMinimum of four years of experience in an IT or analytical role required. Experience in database development, report writing and/or statistics preferred.\n\n EEO \n\nOtter Products, LLC is an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, age, marital status, pregnancy, sex, sexual orientation, gender, gender identity or expression, national origin, disability, veteran status, or any other characteristic or status protected by law.\n\n For US Based Roles Only - Compensation Range Minimum \n\nUSD $90,000.00/Yr.\n\n For US Based Roles Only - Compensation Range Maximum \n\nUSD $120,000.00/Yr.\n\n Additional Total Rewards \n\nProfit Sharing Program Eligible, Benefits Eligible - Full Time- check out otterproducts.com/careers/why for more info"}, {"title": "Software Engineer, Distributed Systems", "company_name": "Hightouch", "location": "United States (Remote)", "link": "/jobs/view/3628572539/?eBP=CwEAAAGMd4QO8APODyE_-mMOPhYyQGg5ZjVbPwq_3V7DK0B2bLAf10pcedM9S2T71X5i4TiJuEwOzWBbSiTKbLuk-VD-_Y-dN5uyf8YpYkHYqEYFxySMS9zr2hRjNrxX9gXbxssh35TFK9YTNV2WeH2Ncb675JgFbJTgNWSZySFC6rrdJGmdSDTr2q5d3VoxORz3GjsDbzjeyZQCvQ07RqYw7oujWSgslycs2LMCY_-N0NEj6e0PvbaYKiZ_5IW34bGVSYLXpHwqa8oOcNttYZ90UA3E_7sDSFzABdIVlfrp5zzERrIYy5sh2MIInxHdDoKgQw8VmOn5uM4S0tXUl-i7LnN0_W0NfPBmLiVDNuveIOm4MfDNPxfe5QKPk6FUAVpvgbcoJn9yRyE&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=rph%2FT6Q6Vkp6YoggJf9cJQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3628572539", "description": "About the job\nAbout Hightouch\n\nHightouch\u2019s mission is to empower everyone to take action on their data. Through our Reverse ETL platform, business and data users can seamlessly sync data from where it resides, such as warehouses and databases, to where it is needed, including operational systems and SaaS tools. Traditionally, acting on data has required engineering time and bandwidth, and left most business users stuck with charts and reports that are unable to take automated action on their data. With Hightouch, every business user, without writing any code, can activate data to streamline critical processes, improve marketing performance, and scale operations.\n\nOur team operates with a focus on making a meaningful impact for our customers. We believe in approaching challenges with a first principles thinking mindset, moving quickly and embracing our value of efficient execution, and treating each other with compassion and kindness. We look for team members that are strong communicators, have a growth mindset, and are motivated and persistent in achieving our goals.\n\nHundreds of companies use Hightouch, including Spotify, Ramp, Retool, NBA, Plaid, and Betterment. We\u2019re based in San Francisco, are remote-friendly, and backed by leading investors such as Amplify Partners, ICONIQ Growth, Bain Capital Ventures, Y-Combinator, and Afore Capital.\n\nAbout The Role\n\nWe are looking for a distributed systems engineer to work on the systems that power the syncing engine our customers and other engineering teams rely on. This presents an exciting challenge where you can apply your expertise in distributed systems, performance optimization, and troubleshooting to push the boundaries of what is possible and meaningful improve our ability to move massive amounts of customer data. This role also provides a unique opportunity to work on a multi-cloud and multi-region infrastructure that supports a global customer base.\n\nWe believe in enabling our engineers to do their best work for our customers by giving them extremely high levels of ownership and autonomy. This comes in different forms: you will own and deliver projects from start to finish, you will work directly with customers to solve their hardest scaling problems, and you will have a lot of influence over what we work on as a team and company.\n\nSome of the problems we\u2019ll be working on include:\n\nSync Speed: Customers want to sync a lot of data to important destinations like Facebook and Snapchat, which requires us to analyze every part of our syncing process and find where we can optimize to sync data more quickly\nStreaming Syncing: We currently sync data from sources in batch only, but envision a world where we also do syncs in real-time and support streaming sources like webhooks and queues\nScalability and Reliability: As part of our rapid growth, we\u2019re always evaluating current future bottlenecks in scaling and reliability, and architecting for the next order of magnitude of growth\nPersonalization API: Our Personalization API product provides a low latency caching layer on top of data warehouses that enable customers to use their data warehouse for real-time personalization use cases with \nMulti-Region and Multi-Cloud: Supporting our multi-region and multi-cloud backend, including extending it to launch Hightouch on in new regions to support data residency requirements of our global customer base\n\nWe are looking for talented, intellectually curious, and motivated individuals who are interested in tackling the problems above. This is a senior role, but we focus on impact and potential for growth more than years of experience. The salary range for this position is $170,000 - $240,000 USD per year, which is location independent in accordance with our remote-first policy.\n\nAbout You\n\nYou are an engineer with a passion for solving hard technical problems that generate real value for customers. You\u2019re motivated by high ownership and are comfortable in a fast-paced, startup environment.\n\nYou have experience and comfort with distributed systems and high-scale systems or the ability and desire to build this experience quickly. This experience can take different forms:\n\nYou\u2019ve built syncing engines that process and move terabytes of data per day in a scalable way\nYou\u2019ve created event collection or data streaming services that power company critical systems\nYou\u2019ve owned systems that have undergone orders of magnitude of growth and helped them scale accordingly\nYou have relevant skills and are a fast learner who is excited to take on the challenges we face\n\nInterview Process\n\nOur goal with the interview process is to balance speed with giving both parties opportunities to assess whether there is a strong mutual fit. We will ask you questions, but we want you to ask us questions! Our technical interviews focus on how you design systems because we believe this is the best way for us to see how you work and for you to see how we collaborate. We don\u2019t ask you to write code to solve technical brainteasers that don\u2019t appear in your day to day job.\n\nRecruiter screen [30m]: Introductory call with our recruiting team to get to know each other and see if the role could be a good mutual fit.\nSystems design screen [45m]: Designing a data processing feature end-to-end.\nHiring manager interview [30m]: Chat with hiring manager about past experiences and future operating preferences to assess fit on company values and operating principles.\nSystems design interview [90m]: Work with the interviewer to architect a system at a conceptual level. The problem will be at a pretty high level - and have both product and customer requirements as well as technical."}, {"title": "Entry-Level Data Scientist Engineer", "company_name": "Phoenix Recruitment LLC", "location": "Texas City, TX (Remote)", "link": "/jobs/view/3788727625/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=8iwtytHsCu4wMwJ4fj3pxw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3788727625", "description": "About the job\nThis is a remote position.\n\nTitle - Entry-Level Data Scientist Engineer, 1 year of project experience\n\nEmployment Type: Full-time\n\nBase Salary: $60K-$70K\n\nPhoenix Recruitment offers a variety of recruiting services to assist both employers and employees. They specialize in marketing open positions, recruiting, and helping employers to find qualified candidates across various industries. Phoenix Recruitment has expertise in streamlining the hiring process. They can help ensure that the process is efficient, well organized, and compliant with relevant regulations.\n\nSkills and Abilities:\n\nStrong knowledge of R or Python for data analysis and modeling. \nProficiency in statistical programs such as R, SAS, MATLAB, or Python. \nFamiliarity with spreadsheets (VBA) and database applications (Access, Oracle, SQL, or equivalent technology). \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \nAbility to learn new methods quickly and work under deadlines. \nExcellent teamwork and communication skills. \nStrong analytical and problem-solving abilities. \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \n\nPreferred:\n\nKnowledge of actuarial concepts and life, health, and/or annuity products. \nExperience with statistical modeling techniques such as GLM, Decision Trees, Time Series, Regression, etc. \nFamiliarity with Microsoft DeployR. \nExposure to insurance risk analysis. \nBasic experience in computational finance, econometrics, statistics, and math. \nKnowledge of SQL and VBA. \nFamiliarity with R or Python for predictive modeling \n\nWhy Phoenix Recruitment LLC?\n\nPhoenix Recruitment often has an extensive network of employers and candidates. This network allows them to tap into a pool of qualified candidates and connect them with suitable job opportunities. They can also leverage their connections to help employers find the right talent efficiently. Outsourcing the recruitment process to a specialized agency can save your time and resources, avoid delays, reduce administrative burdens, and increase the chances of finding the right fit for your organization."}, {"title": "Data Engineer - Remote Work", "company_name": "BairesDev", "location": "United States (Remote)", "link": "/jobs/view/3783162297/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=OtoCrAAHZFomVup04fy35g%3D%3D&trk=flagship3_search_srp_jobs", "id": "3783162297", "description": "About the job\nWho We are\n\nBairesDev is proud to be the fastest-growing company in America. With people in five continents and world-class clients, we are only as strong as the multicultural teams at the heart of our business. To consistently deliver the highest quality solutions to our clients, we only hire the Top 1% of the best talents and nurture their professional growth on exciting projects.\n\nWe are looking for a Data Engineer to join our Development team and participate in different projects made up of multicultural teams distributed throughout the world.\n\nWhat You Will Do\n\n Interview other Data Scientists and Data engineers to vet their technical abilities.\n Input data from those interviews into the product and help develop a predictive, fair, and enjoyable solution.\n Work together with developers, tech leads, and solution architects to build applications.\n Improve existing structures by adding new functionalities or proposing technological updates so that as a result, the impact of your contribution is significant in the core of each business.\n Improve the user experience on scalable and high availability platforms, contributing to the key differential of each business.\n\nHere\u2019s What We Are Looking For\n\n 5+ years of experience working as a developer.\n 3+ years of experience working as a Data Scientist / Data Engineer.\n Proficient with analysis, troubleshooting, and problem-solving.\n Hands-on experience with managing data loads and data quality.\n Advanced English level.\n\nHow we do make your work (and your life) easier:\n\n 100% remote work.\n Hardware setup for you to work from home.\n Flexible hours - make your schedule.\n Paid parental leave, vacation & holidays.\n Diverse and multicultural work environment.\n An innovative environment with the structure and resources of a leading multinational.\n Excellent compensation \u2014 well above the market average.\n Here you can grow at the speed of your learning curve.\n\nOur people work remotely but with a consistent and robust culture that promotes diversity and teamwork. To continue being the leading software development company in Latin America, we want to ensure that every BairesDev member gets the best growth and professional development opportunities in a diverse, welcoming, and innovative environment.\n\nEvery BairesDev team member brings something unique to our company.\n\nWe want to hear your story. Apply now!"}, {"title": "Data Engineer (Remote)", "company_name": "Cascade Debt", "location": "United States (Remote)", "link": "/jobs/view/3755336571/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=pmUtn1j2zyPr25CNPh2RNw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3755336571", "description": "About the job\nDo you want to work in a small team where you can make a real difference in a company? Would you embrace the challenge of building a high impact platform that is used globally? Then we want to speak with you!\n\nAbout Cascade\n\nCascade is a fintech startup backed by Canadian and US investors that empowers companies to grow by democratizing access to institutional debt. We are building tools that modernize how companies raise and manage debt, lowering the barriers to entry to the $7 trillion specialty finance and alternative credit market for companies around the world. Founded just last year, we are gearing up for our next phase of technical development and are seeking a talented Data Engineer to lead the way.\n\nAbout The Role\n\nAs a Data Engineer you\u2019ll be an early member of a growing team building a pioneering platform in the debt infrastructure space. You will design, build, and launch efficient, scalable, and reliable data pipelines to move and transform data .\n\nWhat You\u2019ll Do\n\n Building databases, data lakes and data ingestion pipelines to integrate customer databases and datasets to our systems \n Ingesting financial datasets from external customers, then u pdating and maintaining accurate and complete data mappings to ensure that our product s are displaying high quality data \n M onitoring and alerting across our data pipelines in order to make sure that our data ingests are reliable and correct \n Translat ing business goals and stakeholder requirements into new data flows and deliver analytical insights ensuring that the data flows create real business value continuously \n\nSkills And Experience\n\n 2 + years experience in a data engineering or data analyst role \n E xperience with relational SQL and NoSQL databases, including PostgreSQL , MySQL, MariaDB, MongoDB, Bigtable, etc. \n E xperience working with ETL technologies , such as Databricks, Fivetran , or dbt \n Proficiency in writing SQL queries and knowledge of analytical data warehouses such as RedShift, BigQuery , and Snowflake \n Some experience with CI/CD automation such as GitHub Actions \n Developing APIs and integrating with 3rd party APIs \n Bonus: experience in fintech / SaaS / credit analysis \n\nPeople Who Thrive At Cascade Are\n\n Self-starters, who take the initiative to tackle challenges in a remote work environment \n Good communicators, who can operate without ego to discuss, learn, grow, and help others do the same \n Passionate about creating great digital experiences for users \n Problem solvers who are not satisfied with the status quo \n\nBenefits\n\nRemote: we are a remote-first company and are very flexible on hours as long as things get done \nHome-office: there\u2019s a $1000 USD home office allowance to set yourself up \nEquity: we expect you to have an owner-mentality, and have the equity plan to match \nBenefits: health, dental, vision, and more \nPerks: we offer free lunches weekly and off-site trips \nJob satisfaction: we offer autonomy, ample opportunities for mastery, and an opportunity to make a difference for companies around the world"}, {"title": "Data Engineer", "company_name": "ISC (Integrated Specialty Coverages, LLC)", "location": "Carlsbad, CA (Remote)", "link": "/jobs/view/3778293528/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ew7%2FfEuXSujs5IGabdqUyw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3778293528", "description": "About the job\nAbout Integrated Specialty Coverages\n\nIntegrated Specialty Coverages LLC (ISC) is a growth stage technology and data-driven insurance company leading innovation in the market.\n\nBacked by one of the leading private equity firms, KKR, and led by a forward-thinking management team, ISC is combining the worlds of insurance and technology to create an Insurtech powerhouse. As a leading online distributor of insurance products for a range of industries and \"Main Street USA\", we are looking for the right people to help us in our mission of achieving exponential growth. We strive to be the number one place to go for brokers and agents to source insurance. To accomplish this, we're building a digitally focused team that deeply understands the intersection between user experience, data, and AI/ML to optimize the way we engage with our customers and partners.\n\nJob Summary\n\nThe Data Engineer role will be the technical liaison between multiple groups including a data science team, the engineering team, product management, and business stakeholders. You do not need any insurance knowledge prior, however, you must quickly dive deep into the insurance world and ask questions to become a subject matter expert. You will be responsible for building a data platform to facilitate the data science team. You must be a self-starter that can build out features such as a data pipeline from scratch. There will be support from both engineering and data science for any buildout.\n\nJob Responsibilities\n\nCreate and maintain optimal data pipeline architecture. \nAssemble large, complex data sets that meet functional / non-functional business requirements. \nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. \nBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using AWS technologies, SQL, and Python. \nWork with stakeholders including the Executive, Product, Data Science, and Engineering teams to assist with data-related technical issues and support their data infrastructure needs. \nWork with data science and analytics teams to strive for greater functionality in our data systems. \n\nMinimum Qualifications:\n\nAdvanced working SQL knowledge and experience working with relational databases, strong query authoring (SQL) as well as working familiarity with a variety of databases (Snowflake, Redshift, MySQL, MSSQL, etc.)\nExperience building and optimizing data pipelines, architecture and data sets. \nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. \nBuild processes supporting data transformation, data structures, metadata, dependency, and workload management. \nA successful history of transforming, processing and extracting value from large disconnected, datasets from a variety of data sources (Flat files, Excel, databases, etc.)\nStrong analytic skills related to working with unstructured datasets. \nStrong project management and organizational skills. \nExperience supporting and working with cross-functional teams in a dynamic environment. \nAbility to mentor/guide/collaborate with other team members. \n\nWe are looking for a candidate with 3-5 years of experience in a Data Engineer role. They should also have experience using the following software/tools:\n\nExperience with relational and MPP databases, including Snowflake and MySQL. \nExperience developing software in an agile environment from the requirements stage to production. \nExperience with version control: git\nExperience with container technologies: Docker\nExperience with data pipeline and workflow management tools: Airflow, Jenkins, AWS Glue, Azkaban, Luigi, etc. \nExperience with AWS cloud services: EC2, ECS, Batch, S3, EMR, RDS, Redshift\nExperience with other cloud services: Snowflake, Airflow\nExperience with object-oriented/object function scripting languages: Python, Java, C++, etc. \nExperience with data modeling and data warehouse design\nExperience with data visualization tools (PowerBI, QuickSight)\n\nThe starting annual pay scale for this position is listed below. Actual starting pay will be based on factors such as skills, qualifications, training, and experience. In addition, the company offers comprehensive benefits including medical, dental and vision insurance, 401(k) plan with match, paid time off, and other benefits.\n\nISC's salary ranges are determined by role and level. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations and could be higher or lower based on a multitude of factors, including job-related skills, experience, and relevant education or training.\n\nNational Pay Range\n\n$100,000\u2014$110,000 USD\n\nBenefits of Working at ISC\n\nCompetitive vacation and flexible working arrangements\nComprehensive and inclusive health benefits\nA variety of professional development and mentorship opportunities\nChoice of technology whether at home or in the office\n\nISC's Ownership Behaviors\n\nDo the Right Thing *Be Relentless. Pursue Excellence. *Personal Responsibility & Own Outcomes\nTry Fast. Learn Fast. Fail Fast. Think Big. *Build Extraordinary Owners & Join Forces *Grit & Determination\n\nApplicants with disabilities may contact ISC HR department via e-mail, and other means to request and arrange accommodation. If you need assistance to accommodate a disability, you may request accommodation at any time. Please contact ISC at HR@ISCMGA.com.\n\nISC believes in creating long-term relationships by being responsive and relevant and by consistently delivering value to our community of customers. Specifically, with our employees, we focus on attracting, developing, and retaining the best talent for our business, challenging our people, demonstrating a \"can-do\" attitude, and fostering a collaborative and mutually supportive environment. ISC is an equal opportunity employer and a participant in the US federal E-Verify program (US).\n\nDiversity creates a healthier atmosphere: All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, gender, gender identity, sexual orientation, marital status, medical condition, genetic information, mental or physical disability, military or veteran status, or any other characteristic protected by local, state, or Federal law."}, {"title": "Data Engineer -Remote", "company_name": "RIT Solutions, Inc.", "location": "Denver, CO (Remote)", "link": "/jobs/view/3768007496/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=1hQH8EDaII1eJ7nQ9BACQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3768007496", "description": "About the job\nDuration: 6+ months CTH\n\nMust have a valid LinkedIn profile\n\nRequired Skills\n\n 3-6 years of experience (no more than 6 years)\n Start up environment experience\n Python, SQL, and DBT"}, {"title": "Big Data Engineer - W2", "company_name": "Megan Soft Inc", "location": "United States (Remote)", "link": "/jobs/view/3674845652/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fCAxh4N%2F4YZJ5aEIASaeeA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3674845652", "description": "About the job\nTitle: Big Data Engineer - W2\n\nLocation: Dearborn, MI (Remote)\n\nDuration: 12+ Months\n\nDirect Client: Ford Motors\n\nMust have skills: Should be able to handle GCP requirements\n\n\"Translates business requirements to build repeatable, sustainable, efficient, coded processes that can be productionized by Software Engineers and readily modified by other Data Engineer Creates POC processes in Dev/QA and works with Software Engineers to productionize and align best coding and process practices that maximize efficiency, speed, stability, system resources and capabilities Awareness of and compliance with data privacy, security, legal and contractual guidelines Incorporates data quality and privacy checks/alerts to minimize bad data being consumed by end users, models and dashboards, and to protect customer data; revises checks for new data issues Validates data products and pipelines are functioning as expected following system or application upgrades, source changes, etc. Works closely with Architects if required to align systems, tools and applications being utilized with business use case and performance requirements Provides data product support and maintenance Establishes, tracks and monitors KPIs related to specific data products and deliverable Participate in iteration cadence activities (Iteration Planning, task estimation, daily scrum, demos, etc.). Will be a part of on-call team to resolve critical production support tickets during business hours and off hours.\n\nSkills Required\n\nExperience with Big Data Technologies (Hadoop , ETL Tool, Spark. Hive etc..) Someone who understands Cloud and has experience working with Cloud Platforms (preferably GCP) Should be able to handle GCP requirements - ingesting into GCS/ Big Query, working with GCP components Hands-on in Data Flow, Cloud Storage (GCS), Big Query and GitHub. Proficient in Python and SQL.\n\nSkills Preferred\n\nManages ETL: uses programming and tools for data ingestion, configures pipelines, applies transformations and decoding, integrates and fuses data, moves and securely deliver Experience NoSQL databases\n\nAdditional Information\n\nIf the candidate is remote only, please indicate \"\"100% Remote\"\" under candidate's name on resume. Other similar schedule notations can be \"\"Local Candidate\"\" or \"\"Hybrid Candidate.\"\" ***HRA Test Required***\""}, {"title": "Remote Opportunity: Quantexa Data Engineer", "company_name": "SPAR Information Systems LLC", "location": "United States (Remote)", "link": "/jobs/view/3680073191/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ioykVCZSOQ7BK5%2FwRQHOWg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3680073191", "description": "About the job\nHello All, Hope you are doing great!! Please go through the job descriptions and let me know your interest. Role: Quantexa Data engineer  Location: Remote Duration: Long Term Contract Job Description: 1. End to End usage of Quantexa including guiding our client team on how to use from both front end and data ingestion Reason they are asking for Certification 2. Resolve Entity issues with data 3. Knowledge of Scala and Hive which requires coding experience using parquet 4. Advanced SQL knowledge requiring writing SQL queries 5. Experience having worked with GCP data lake 6. Quantexa certification required\n\nThanks & Regards,\n\nSatnam Singh\n\nDirect: 201 623 3660\n\nEmail : Satnam.singh@sparinfosys.com"}, {"title": "Remote Work - Need Staff Data Engineer - USC only", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3746283980/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=CvIuLBNO%2BPMGi4IXVRkP0A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3746283980", "description": "About the job\n100% Remote, client is in NY\n\nLook for strong job history, Python, Machine Learning, SQL, 10/10 comms, etc. \n\nMUST be an Engineer \n\nSummary\n\nWe're looking for an experienced Data Engineer to join our client's growing team of top notched IT professionals. You will work closely across multiple groups including Editorial and Audio, Marketing, Advertising, etc. to achieve their goals and objectives. The Data Engineering team builds tools throughout the stack while sharing knowledge across these departments.\n\nAs the company grows, they're looking for Data Engineer who will help solidify and expand our pipelines and maintain their data warehouse. Their business model is based on performing complex and very detailed analyses of how our products perform with their subscribers. You'll be responsible for working alongside analysts, product, marketing, and other teams to define new data collection components and measurement schemes that support each new product and feature.\n\nHow Can Add Value To Their Mission\n\nCreate and maintain data pipelines to provide insights and drive business decisions\nEstablish data warehousing strategy (ex. Kimball, Data Vault, etc.)\nMaintain data infrastructure on our AWS accounts\nCollaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.\nWrite unit/integration tests, contributes to engineering wiki, and documents work\nImplement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it\n\nWhat You'll Need To Succeed\n\n7+ years of industry experience measuring product performance and user behavior\nExperience working with a variety of data management technologies, including RedShift, Kinesis/Kafka, Glue, Spark, Postgres, Airflow, dbt, and others.\nExperience implementing BI reporting tools such as Looker\nExperience interfacing with engineers, product managers and analysts to understand data needs\nKnowledge of a variety of measurement beacons, SDKs, APIs including Google Analytics, Amplitude, Braze, Stripe, email service providers\nA commitment to building secure, resilient, fault-tolerant architectures, with clear documentation and procedures in place for support\nA focus on accuracy and detail as you build, ensuring data pipelines produce clear, predictable results\nUnderstanding of the typical metrics a subscription and advertising-supported business needs to measure success\nExperience with Client techniques as applied to behavioral segmentation or anomaly identification is a definite plus\nFamiliarity using developer tools that increase productivity and facilitate the development of resilient code (eg. Docker, CircleCI, Serverless) is a plus"}, {"title": "ENTRY LEVEL DATA ENGINEER (REMOTE)", "company_name": "SynergisticIT", "location": "Jacksonville, FL (Remote)", "link": "/jobs/view/3767587909/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=om7L3CnowiC4PLIcpZD70A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3767587909", "description": "About the job\nSYNERGISTICIT wants every candidate to know that the Job Market is Challenging and to stand out, you need to have exceptional skills and technologies and that's where we come in to make sure you get the attention which you need\n\nSynergisticIT understands the complex nature of the job market and how difficult it can be to secure a position, especially for fresh graduates. Therefore, we assist and help tech-savvies to convert their passions into professions. We go above and beyond to keep you working in your niche.\n\nAs we focus on long-term success, we provide complete career development solutions. From job search to upskilling portfolio and interview preparation, we can guide you at every step of your career.\n\nSynergisticIT spares no efforts to connect you with a large network of tech giants, including Google, Apple, PayPal, Dell, Cisco, Client, etc. Presently, we are actively looking for ENTRY LEVEL DATA ENGINEER a driven mindset. Get the right opportunity and gain experience in building web-centric solutions on Java.\n\nWho Should Apply : Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or anyone looking to make their career in IT Industry\n\nWe also assist in filing for STEM extension and H1b and Green card filing.\n\nCandidates who are serious about their future in the IT Industry and have set big goals for themselves.\n\nCandidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. We also offer Skill Enhancement Programs if the candidates are missing skills or experience which our clients need with great outcomes\n\nCandidates can benefit from skill enhancement if they fall into the below categories. If they are qualified with enough skills then no need for skill enhancement\n\nCandidates Who Lack Experience\n\nHave had a break in careers\n\nLack Technical Competency\n\ncandidates who want to get employed and make a career in the Tech Industry \n\nPlease Also Check The Below Links\n\nhttps://www.synergisticit.com/candidate-outcomes/\n\nhttps://www.synergisticit.com/java-track/\n\nhttps://www.synergisticit.com/data-science-track/\n\nhttps://www.synergisticit.com/which-is-the-best-option-for-tech-job-seekers-staffing-companies-consulting-companies-bootcamps-or-synergisticit/\n\nhttps://www.synergisticit.com/contact-us/\n\nIf the skills are not a match candidates can opt for Skill enhancement. Or their resume can be sent out to clients to see if responses are achievable\n\nREQUIRED SKILLS For Java/Software Programmers\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Core Java , javascript , C++ or software programming\nSpring boot, Microservices and REST API's experience\nExcellent written and verbal communication skills\n\nFor data Science/Machine learning\n\nRequired Skills\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Statistics, Python, data visualization tools\nExcellent written and verbal communication skills\n\nPreferred skills: NLP, Text mining, Tableau, Time series analysis\n\nTechnical skills are required by clients for selection even if its Junior or entry level position each additional Technical skill helps a candidate's resume to be picked by clients over other job seekers.\n\nClients hire candidates with the right technical skills which they need and reject candidates who lack the required technical skills.\n\nNo third party candidates or c2c candidates\n\nPlease apply to the posting\n\nNo phone calls please. Shortlisted candidates would be reached out"}, {"title": "Data Engineer - Remote | WFH", "company_name": "Get It Recruit - Information Technology", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3782241984/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=BgBf1kSAAX%2BSuw6inJsIVw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3782241984", "description": "About the job\nJoin an innovative team at a leading company in the engineering and manufacturing industry, where we are committed to leveraging data for continuous improvement and operational excellence. This hybrid position allows you to enjoy the flexibility of remote work on Mondays and Fridays.\n\nWhy Join Us\n\nAt our company, we prioritize your well-being and professional growth. We offer a comprehensive benefits package, including excellent health benefits, a 6% 401(k) match, generous paid vacation and sick time, 12 paid holidays, and more.\n\nRole Overview\n\nAs a Data Engineer, you will play a pivotal role in transforming raw data into actionable insights, driving improvements in engineering and manufacturing processes, enhancing product quality, and optimizing operational efficiency. Collaborate with dynamic cross-functional teams to extract, analyze, and visualize data, influencing strategic decisions through your valuable insights.\n\nKey Responsibilities\n\nData Collection and Integration:\n\nGather, clean, and integrate data from diverse sources, including production databases, sensor data, and external datasets.\n\nData Analysis\n\nConduct exploratory data analysis to identify patterns, trends, and anomalies.\n\nPerform statistical analysis and hypothesis testing to derive meaningful insights.\n\nData Visualization\n\nCreate informative and visually appealing dashboards and reports using tools like Tableau, Power BI, or Python libraries.\n\nPredictive Modeling\n\nDevelop predictive models to forecast manufacturing performance, quality, and operational outcomes.\n\nRoot Cause Analysis\n\nInvestigate production issues and quality concerns by analyzing data to identify root causes and recommend corrective actions.\n\nPerformance Monitoring\n\nEstablish and maintain key performance indicators (KPIs) to track manufacturing and engineering metrics.\n\nGenerate regular reports to communicate performance trends to stakeholders.\n\nCross-functional Collaboration\n\nCollaborate with engineering, manufacturing, and quality assurance teams to understand their data needs and provide analytical support.\n\nContinuous Improvement\n\nSuggest process improvements and optimizations based on data analysis.\n\nData Security And Compliance\n\nEnsure data security and compliance with relevant regulations, such as GDPR or industry-specific standards.\n\nInfrastructure And Development\n\nBuild and maintain the infrastructure for collecting, storing, and processing data.\n\nDesign, develop, and manage data pipelines, ETL processes, and data warehouses.\n\nRequired Experience\n\nProven experience as a Sr. Data Analyst and/or Data Engineer in an engineering or manufacturing environment.\n\nProficiency in data analysis tools and programming languages, such as Python, R, SQL, and Excel.\n\nStrong data visualization skills with tools like Tableau or Power BI.\n\nExcellent knowledge of statistical analysis and machine learning techniques.\n\nAbility to communicate complex findings and insights effectively to both technical and non-technical stakeholders.\n\nStrong problem-solving skills and attention to detail.\n\nFamiliarity with manufacturing processes and quality control is a plus.\n\nExperience with enterprise ERPs (SAP S4HANA, Oracle EBS), CRMs (Salesforce, Hubspot), and IoT.\n\nProficient in working with cloud computing platforms including AWS, GCP, and Azure.\n\nExpertise in configuring and optimizing cloud-based data solutions for scalability, reliability, and performance.\n\nBachelor's Degree in Data Science, Statistics, Engineering, or Computer Science Required.\n\nApply Now and be part of a team shaping the future of data-driven innovation in engineering and manufacturing!\n\nEmployment Type: Full-Time"}, {"title": "Entry Level Software Engineer - Data Backend Engineer (Remote - Canada)", "company_name": "Yelp", "location": "United States (Remote)", "link": "/jobs/view/3752881757/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fankeoxifvhpTzOYvowHYQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3752881757", "description": "About the job\nYelp engineering culture is driven by our values: we\u2019re a cooperative team that values individual authenticity and encourages creative solutions to problems. All new engineers deploy working code their first week, and we strive to broaden individual impact with support from managers, mentors, and teams. At the end of the day, we\u2019re all about helping our users, growing as engineers, and having fun in a collaborative environment.\n Interested in exploring data and finding innovative ways to collect it, curate it, and make it easily accessible for others to use? How about doing that at an enormous scale? Yelp\u2019s datasets contain billions of interactions between users and local businesses around the globe. If working with our vast amount of data sounds exciting, come and join us!\n\nThe Marketplace Data Observability team is responsible for managing, governing, and monitoring all of Yelp\u2019s Ads data to protect our revenue stream and also support offline analysis that aids in making long term company decisions. We build tools that help us to better understand our data quality, and we strive to uncover data issues before they impact our consumers.\n\nAs a Data Backend Engineer on this team, you will be responsible for building elegant and scalable data products that serve critical, up-to-date, structured information to support different types of analytics within Yelp. As a core contributor to our growing data modeling and data warehousing engineering efforts, you will help design and own mission-critical data flow pipelines and datastores to enable decisions including effective A/B testing and company investments.\n This opportunity is fully remote and does not require you to be located in any particular area in Canada. We welcome applicants from throughout Canada. We\u2019d love to have you apply, even if you don\u2019t feel you meet every single requirement in this posting. At Yelp, we\u2019re looking for great people, not just those who simply check off all the boxes.\n  Build systems that can effectively store and crunch terabytes of data.\nDesign and develop data models for efficient data storage, retrieval, and reporting.\nCreate and maintain conceptual, logical, and physical data models using industry-standard modeling tools.\nCollaborate with cross-functional teams, including engineers, data analysts, business analysts, and data scientists, to understand data requirements and translate them into effective data models.\nParticipate in data integration efforts, including ETL processes and data migration.\nStay up-to-date with industry best practices, emerging technologies, and trends related to data warehousing.\nSupport on-call rotations as needed to operate the team.\n  Understanding of high performing and scalable data systems.\nExperience in building and orchestrating ETL pipelines.\nExperience with Data Lake or Data Warehouse landscape.\nA hunger for tracking down root causes and fixing them in systematic ways.\nAbility to communicate effectively to technical and non-technical cohorts alike. \nExposure to some of the following technologies: Python, AWS Redshift, AWS Athena / Apache Presto, Big Data technologies (e.g S3, Hadoop, Hive, Spark, Flink, Kafka etc), NoSQL systems like Cassandra, DBT."}, {"title": "Data Engineer (Associate Consultant), June 2024", "company_name": "UDig", "location": "Nashville, TN (Remote)", "link": "/jobs/view/3716039960/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=XUjo40kU55HnDCqJ%2F3FdPg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3716039960", "description": "About the job\nCan UDig It? \n\nUDig designs, builds, and implements technology solutions that deliver on business objectives. People join our team because they are excited to have an impact on the future of our organization, passionate about delivering meaningful results, and energized by solving complex problems. Oh, and they like working with other driven, smart, and authentic people!\n\nUDig is proud to be recognized as a \"Best Places to Work\" in Virginia and Tennessee. We're committed to fostering a transparent environment where leadership is accessible, professional development opportunities are abundant, and you are empowered to achieve your career goals. You'll collaborate, develop deep relationships with teammates, and accelerate your success while having fun!\n\nIf you're graduating in May 2024 and excited by the idea of impacting organizations by building data solutions to achieve business goals, you should consider joining our growing technology team.\n\nHow Things Go Down\n\nAs a new, entry-level teammate, you'll take part in our \"Breaking Ground\" training program designed to help you quickly get up-to-speed on UDig's delivery approach within a project team. The program covers:\n\nUDig delivery process and methodologies such as gathering requirements and delivering projects using agile\nTechnical guidance and instruction on the best practices from Data leaders at UDig\nDelivering executive and technical presentations\nCoaching to prepare you to for success on your first client project at UDig\n\nA typical day might entail:\n\nProject standups and check-ins\nDesign and development of data pipelines, BI reports, and more\nPlanning and participating in project demos\nContributing to internal UDig projects\nCareer and job growth through UDig-provided training\nCreating long-lasting friendships with teammates through fun activities as well as your projects (including social events, winning a prize for guessing the weight of pumpkin, or sharing experience on cool tech side projects)\n\nUDig believes it is important for Associate Consultants going through our Breaking Ground program to reside in one of our major markets (Richmond, VA or Nashville, TN metro area) for at least the first year of employment.\n\nWant to learn more? Hear directly from our teammates by checking out our 'A Day in the Life' blog series: https://www.udig.com/digging-in/breaking-ground-2023/\n\nSound Interesting? Here's the Technical Piece\n\nBelow are a few boxes you should be able to check if\u202fyou're interested: Don't check every box? Go ahead and apply!\n\n1+ year of experience with relational database management systems such as Microsoft SQL Server, MySQL, or Oracle\nUnderstanding of the full lifecycle of design, development, and implementation of data solutions including architecture design, development, testing\nSome experience in ETL development or reporting is a plus\nAbility to present technical ideas and high-level concepts and solutions to internal and external team members with varying degrees of technical knowledge\nBachelor's Degree in in computer science, information technology, or any science or business discipline\nAbility to take your work seriously but also seriously enjoy a good time with your UDig colleagues\n\nWhat's in It For You? \n\nAt UDig, you will build your career alongside talented and experienced developers while gaining experience and having fun! We provide continuous challenges and professional development opportunities. Additionally, we offer: \n\nCompetitive salary, merit reviews, and career advancement paths\nFlexible, hybrid environment\nIndividual $1500 Training Budget\nRegular team building and social activities (virtual and/or in-person)\nTransparent culture with strong communication and access to leadership\nGreat benefits like\nGenerous Paid Time Off, company holidays, and parental leave\nMultiple Single and Family Health Insurance plans to choose from\nDental & Vision coverage\nShort-Term and Long-Term disability\nOptional accident and critical illness coverage\nMatching 401(k)\nand more!"}, {"title": "Expression of Interest: Data Engineer", "company_name": "Fingerprint for Success (F4S)", "location": "New York, NY (Remote)", "link": "/jobs/view/3787769880/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=l%2FEDpQqUNmCttGdIyTk7Cg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787769880", "description": "About the job\nWe are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.\n\nThe F4S Talent Pool is a pilot project designed to:\n\nHelp job seekers get discovered by our partners based on their anticipated hiring needs.\nProvide optional support and resources for job seekers in their career endeavors.\nHelp individuals understand, and bring out the best in themselves and each other.\n\nThe F4S Talent Pool process:\n\nOnce you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.\n\nAbout Fingerprint For Success (F4S)\n\nBacked by 20+ years of research, F4S\u2019s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high performing teams.\n\nKeep in mind, joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your F4S work style assessment to match the hiring needs of our partners.\n\nYour feedback is a gift! Write to us via:\n\nPowered by JazzHR\n\noWuPJ9lwq9"}, {"title": "Data Engineer Seattle, WA (remote)", "company_name": "Conch Technologies, Inc", "location": "United States (Remote)", "link": "/jobs/view/3765052211/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=SBETI78UMutCHuRp5FnP%2FA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3765052211", "description": "About the job\nHi,\n\nGreetings from Conch Technologies Inc\n\nData Engineering Lead \u2013 With Databricks and Unity Catalog\n\nLocation \u2013 Seattle, WA (remote)\n\nDuration \u2013 12 months\n\nNeed: Design and implement data architecture and strategy, leveraging technologies such as Databricks, Unity Catalog, Privacera, and Collibra.\n\nAs a Data Engineering Lead, you will play a pivotal role in defining and executing technical deliverables while assessing complexities and levels of effort. You'll engage with both vendor and internal teams to resolve technical challenges and ensure seamless collaboration. Your leadership will extend to planning and prioritization with the SCRUM master, facilitating effective coordination between onsite and offshore teams, and driving feature prioritization through collaboration with product managers.\n\nKey Responsibilities\n\n Lead the definition and breakdown of technical deliverables, assessing complexities and levels of effort.\n Collaborate with vendor and internal teams to address technical issues.\n Drive planning and prioritization in conjunction with the SCRUM master.\n Coordinate with onsite and offshore teams for timely project delivery.\n Collaborate with product managers to prioritize features, communicate with stakeholders, and clarify requirements.\n Collect and manage user-reported issues, ensuring prompt resolution and communication.\n Define comprehensive test plans, oversee their execution, and document results.\n\n--\n\nWith Regards,\n\nNagesh G\n\nMobile: 408-381-5645\n\nDesk: 901-313-3066\n\nEmail: nagesh@conchtech.com \n\nWeb: www.conchtech.com"}, {"title": "Remote Work - Need Azure Fabric Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3759325599/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=C3iVjmcR9wjUip1nNjUiCg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3759325599", "description": "About the job\nThe client is placing heavy emphasis on Microsoft and Azure Fabric experience\n\nInformation Management Engineer \u2013 Take the identified information management sources and assist migration to the Client Data Lake, Azure Fabric. The engineer will transform information management (de-dup, clean and enrich).\n\nTake the identified data sources and bring them into our Data Lake, Azure Fabric. Once the data is in, the engineer will need to be able to transform data (de-dup, clean and enrich the data). We are focused on using a medallion data model for data architecture. The tools we are working with are: Delta Parquet files, python, spark, notebooks, T-SQL, Fabric lakehouse/warehouse, Dataflows Gen2, Data Pipelines, Direct Lake, shortcuts, ADF."}, {"title": "Data Engineer", "company_name": "Catalist", "location": "Washington, DC (Remote)", "link": "/jobs/view/3744951279/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ui1Fk6ICmnPNqAlZDZAcwA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3744951279", "description": "About the job\nFor over 17 years, Catalist has been a leader in civic data and data science innovation. Our mission is to provide progressive organizations with the data, software, and services needed to better identify, understand, and communicate with the people they need to engage and mobilize. Our clients include the largest, most influential organizations in the U.S. active in civic engagement, advocacy, and political campaigns.\n\nCatalist is home to a dedicated, creative team of technologists, data scientists, and campaign experts committed to using our talents and technology to nurture a vibrant and growing progressive community.\n\nAs a Data Engineer at Catalist, you will work closely with the Analytics and Technology departments to design, build, support, and maintain various data pipelines and processes with an end goal of providing data and intelligence to the progressive community. This role primarily involves translating data architecture designs into functional processes, code, and systems.\n\nThe ideal candidate will be a highly motivated individual with excellent technical skills, a strong desire to learn new skills, and an interest in progressive politics. Catalist values creativity and problem-solving. Our work is on the cutting edge of data-driven politics, and your support will help Democratic candidates and progressive organizations conduct successful advocacy and electoral campaigns.\n\nThis position reports to the Deputy Chief Data Officer. The Data Engineer is a part of a growing Data team that supports all underlying work at Catalist.\n\nThis position is included in our CWA bargaining unit.\n\nPrinciple Duties & Responsibilities\n\nDevelop scalable, production processes, code, and systems for data ingest, transformation, modeling, and reporting across a variety of platforms\nTranslate mock-ups and designs into functional processes, code, and systems\nCreate architecture designs when needed\nProvide quality assurance and testing on processes, code, systems, and products\nBecome an internal subject matter expert on various datasets and support other Catalist departments/teams on usage of those datasets\nExecute ad-hoc data and database maintenance tasks as requiredProject manage cross-departmental efforts, with direct responsibility for stakeholder engagement, management, and execution of technical elements\n\n\nRequirements\n\nBS or BA in a technical field, or relevant experience\n1-2 years of experience working with SQL databases\nExperience with data cleaning or ETL processes\nExperience with distributed computing systems and/or distributed datastores (particularly the Hadoop ecosystem)\nExperience managing projects\nFamiliarity with Catalist data, progressive politics, voter files, and/or commercial data\nBackground check required\n\n\nPreferred Skills & Abilities\n\nWillingness to be a problem solver and produce results in a fast paced environment\nAbility to be creative and personable, and articulate ideas clearly\nExperience working with SQL databases\nProficiency with Python or another object-oriented programming language (R, Java, Scala, etc\u2026)\nExperience working in cloud environments (AWS, GCP, etc.)\nExperience working in command line environments such as Bash\nExperience with a version control tool such as git or github\n\n\nBenefits\n\nMedical, Dental, Vision, Prescription Drug\n\nCatalist offers Medical, Dental, Vision, and Prescription Drug coverage for eligible staff and their eligible dependents. Catalist\u2019s Medical plan is a comprehensive PPO program including Prescription Drug coverage with 85% of the premium paid by Catalist. Dental and Vision coverage is provided at no cost to employees.\n\nGroup Term Life Insurance and Long-Term & Short-Term Disability Coverage\n\nGroup Term Life Insurance and Long-Term and Short-Term Disability coverage is available for eligible staff. These benefits are provided at no cost to Catalist employees.\n\n401(k) Safe Harbor Plan\n\nA 401(k) Safe Harbor Plan is available to eligible staff with a 3% contribution from Catalist from the date of hire. Employees may contribute pre-tax or post-tax from their salary up to the legal limits set forth by the IRS.\n\nMedical and Dependent Care Flexible Spending Accounts (FSAs)\n\nCatalist offers an FSA Program that gives eligible staff the ability to pay out-of-pocket medical/dental/vision/child care expenses from pre-tax earnings.\n\nTransit Benefits\n\nCatalist also makes available a Transit benefit FSA program to eligible employees using pre-tax contributions with a company match.\n\nProfessional Development and Remote Work Expenses\n\nEligible employees may be reimbursed up to $750 each year for professional development / education and remote work expenses.\n\nStudent Loan PayDown or SaveUp\n\nCatalist offers a Student Loan PayDown and College SaveUp benefit for eligible staff.\n\nVacation, Personal Leave, Sick Leave Benefits\n\nCatalist offers generous vacation benefits to all eligible staff. Eligible employees also receive:\n\n 14 Paid Holidays\n Personal Days\n Sick Leave\n Parental Leave\n\n\nHybrid Office/Remote Work\n\nCertain positions at Catalist are eligible for Office/Remote Hybrid or full Remote status."}, {"title": "Remote Entry Level Data Analyst/Engineer", "company_name": "SynergisticIT", "location": "White Plains, NY (Remote)", "link": "/jobs/view/3774171423/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=q54sk1%2FRww0z%2BC%2BE8e0jQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774171423", "description": "About the job\nAt SynergisticIT, we're all about making connections. Whatever IT goals you have, our software programmers can help achieve those. Our Software development teams can take up turnkey projects and execute them in an effective and efficient manner. If you are looking to source talent our recruiters will find the ideal IT talent for your company. What's the secret to our success? Well, it all starts with taking quality time to listen to each client's specific needs. After we have a thorough grasp of your IT goals, we can better customize our Developments as per your specific needs. We can also tailor make recruiting programs to exceed your expectations. Since our founding in 2010, SynergisticIT's strategies have earned the company an enviable position in the software development, IT staffing and IT skill enhancement fields. SynergisticIT continues to work with hundreds of satisfied American clients with our software programmers working on our projects and after gaining hands on experience on cutting edge technologies moving to contribute their skills to great clients like Apple, Google, Client, Ebay, Paypal, Kroger, the Walt Disney Company and hundreds more. If you are tired of working with inefficient programmers who take a lot of time to ramp up we want you to try us. Our software programmers can hit the ground running and get you the maximum return on your investment. You have already tried the rest its time you tried the best. SynergisticIT - Home of the Best Data Scientists and Software Programmers in the Bay Area.\n\n Why Us ? \n\nSynergisticIT has a proven track record of successfully skill enhancement and staffing IT employees for some of the world's most iconic brands. Our team takes the time to fully understand every client's needs so we could best meet your IT staffing requirements. The knowledgeable staff at SynergisticIT is always more than happy to work with clients to ensure they reach their software development goals. Besides staffing, SynergisticIT is also committed to helping young IT professionals advance their career with a robust upskill program . Everyone who goes through SynergisticIT's program learns all the skills necessary to succeed in many IT fields ranging from Java to Machine Learning. Additionally, everyone trained at SynergisticIT has been through extensive mock and technical interview screenings to bolster their career prospects. Last, but certainly not least, SynergisticIT takes great care to respect the privacy considerations for every client. All companies who work with SynergisticIT can rest assured their confidential data is protected using the most up-to-date encryption technologies. SynergisticIT also complies with all the latest NDA agreements.\n\n REQUIRED SKILLS For Java /Software Programmers \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Time series analysis \n\n We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2022 and at Gartner Data Analytics Summit (Florida)-2023 \n\nOracle CloudWorld Event (OCW) Las Vegas 2022 | SynergisticIT - YouTube\n\n https://www.youtube.com/watch?v=OAFOhcGy9Z8 \n\n https://www.youtube.com/watch?v=EmO7NrWHkLM \n\n https://www.youtube.com/watch?v=NVBU9RYZ6UI \n\n https://www.youtube.com/watch?v=Yy74yvjatVg \n\nSynergisticIT at Gartner Data and Analytics Summit 2023 - YouTube\n\nFor preparing for interviews please visit  https://www.synergisticit.com/interview-questions/ \n\n We are looking for the right matching candidates for our clients \n\n Please apply via the job posting \n\n REQUIRED SKILLS For Java /Full Stack/Software Programmer \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning Positions \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Tensorflow \n\n If you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team. \n\n No phone calls please.  Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates"}, {"title": "Data Engineer", "company_name": "Anika Systems", "location": "Leesburg, VA (Remote)", "link": "/jobs/view/3787766893/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=mApL9JnN%2B%2Fr2%2BYUPlxRYOA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787766893", "description": "About the job\nAnika Systems is a fast growing, woman-owned small business that specializes in providing innovative IT solutions for federal government agencies. Our expertise lies in accelerating delivery in Data and Analytics, Intelligent Automation, Application Development, and IT Modernization. We are currently expanding our Federal team and are seeking a passionate and talented Data Engineer. This opportunity is 100% remote.\n\nMust be a U.S. Citizen with the ability to obtain and maintain a government suitability clearance\n\nResponsibilities\n\nDesigns, develops, builds, analyzes, evaluates, and installs database management systems to include database modeling and design, relational database architecture, metadata, and repository creation and configuration management.\nDefines and oversees database organizations, standards, controls, procedures, and documentation. Provides technical consulting in the definition, design, and creation of a database environment.\nAdvises applications development staff and users on data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nDesigns and implements databases with respect to access methods, access time, batch processes, device allocation, validation checks, organization, protection and security, documentation, and statistical methods.\nUses data mapping, data mining, and data transformational analysis tools to design and develop databases.\nDetermines data storage and optimum storage requirements.\nPrepares system requirements, source analysis, and process analyses and designs throughout the database implementation.\n\nRequired Skills And Experience\n\nBA/BS and 1 year of relevant experience\nExperience with DataBricks, SQL, Python\nApplications development experience with data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nExcellent communication skills (both oral and written) with process-oriented organizational skills to ensure project success\n\nDesired Skills And Experience\n\nDemonstrated successful development experience with the full life cycle of government database management systems including database modeling and design, relational database architecture, metadata and repository creation, and configuration management.\n\nPowered by JazzHR\n\nTlKWIorucJ"}, {"title": "Remote Work - Need Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3774712978/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=r3Bp%2F0qcOD2OpC5jpbUOcA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774712978", "description": "About the job\nTitle : Data Engineer\n\nLocation : Remote\n\nDuration : 6+ months to start (long term contract (work order are extended 2x year))\n\nPlease Fill out the skill matrix: Technology Some Awareness (1) Novice (2) Intermediate (3) Advanced (4) Expert (5) SQL Tableau Desktop /Creator Tableau API Integration AWS - S3 Glue AWS - Athena Redshift AWS - Lambda AWS - SNS/SQS msExcel\n\nProfile: Data Analyst will work closely with Architects and leads in the Interflow team, key business stakeholders including program staff to support the organizations effort in the areas of data Analysis and information management. Role should have strong experience in writing SQL queries to extract data for reports. Candidate should have experience working with large set of data. As these reports are very critical for the company and we are looking for an expert who is capable of handling, consuming and extracting large sets of data for analysis and reports.\n\nMust Haves\n\nAWS - Athena\nAWS - S3\nGlue\nhands-on development of reporting applications\nMS Excel\nRedShift\nSQL\nTableau Desktop /Creator\n\nNice\n\nAWS - Lambda\nAWS - SNS/SQS\nTableau API Integration\n\nSkills\n\nBA/BS required (major in an analytical field desired) \nA minimum 10+ years work-related experience. \nExperience in hands-on development of reporting applications for the Web in a professional environment \nThe Ethos of continuous improvement and interest in learning new things.\nStrong analytical thinking and structured problem-solving ability\nAbility to handle multiple projects and assignments simultaneously and effectively in a cross-functional team environment. \nVersed on the agile methodology and best practices. \nExcellent interpersonal and collaboration skill with the ability to work with a diverse set of colleagues, across functions, from different organizations, disciplines, etc. \nSelf-starter, ability to set priorities, work independently and attain goals"}, {"title": "Data Engineer", "company_name": "Team Remotely Inc", "location": "Houston, TX (Remote)", "link": "/jobs/view/3786249829/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=78lY5Jo%2FfyRPC6TQYCdvww%3D%3D&trk=flagship3_search_srp_jobs", "id": "3786249829", "description": "About the job\nThis is a remote position.\n\n Data Engineer (US/Canada Residents Only, 1 year experience, remote)\n\nTeam Remotely Inc. is a staffing and recruitment agency that offers a comprehensive solution for talent acquisition, including sourcing, vetting, pay rolling, and managing talent. Whether you need contract staffing, direct hire, direct sourcing, talent pools, or diversity initiatives, our model can support your hiring strategy.\n\n Hiring Type: Full-Time\n\n Base Salary: $55K-$70K Per Annum.\n\nResponsibilities\n\nDesigning user interface changes for web-based DB applications. \nReviewing application requirements and interface designs. \nDeveloping and implementing highly responsive user interface components using react concepts. \nWriting application interface codes using JavaScript following react.js workflows. \nTroubleshooting interface software and debugging application codes. \nDeveloping and implementing front-end architecture to support user interface concepts. \nMonitoring and improving front-end performance. \n\nQualifications And Experience\n\nBachelor\u2019s degree in computer science, information technology, or a similar field. \nPrevious experience working as a react.js developer. \nIn-depth knowledge of JavaScript, CSS, HTML, jQuery, and front-end languages. \nKnowledge of REACT tools including React.js, Webpack, Enzyme, Redux, and Flux. \nExperience with user interface design. \nKnowledge of performance testing frameworks, including Mocha and Jest. \nExperience with browser-based debugging and performance testing software. \nTroubleshooting skills. \nProject management skills. \nProblem-solving skills. \nVerbal communication skills. \n\n Why work with Team Remotely?\n\nTeam Remotely Inc. is a staffing platform offering a seamless experience for employers and candidates. Employers can post job openings and specify their requirements, while candidates can create profiles and upload resumes.\n\nThe team of Team Remotely continuously learns and adapts based on previous successful placements, constantly improving its matching capabilities. This ensures that the recommendations provided by Team Remotely are tailored and accurate, increasing the likelihood of a successful match between employers and candidates. By providing intelligent and data-driven solutions, they strive to enhance the efficiency and effectiveness of the hiring process, ultimately helping companies find the best talent and individuals find their dream jobs."}, {"title": "Data Engineer", "company_name": "ZAG Technical Services", "location": "United States (Remote)", "link": "/jobs/view/3771403464/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=Hf0VefrKXBFIL5MtHmY0Pw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3771403464", "description": "About the job\nWe are looking for a tech savvy Data Engineer to contribute to the building, operating, and scaling of next generation data platforms and tools that will power data-driven analytics capabilities throughout the ZAG products and services portfolio, spanning areas such as business intelligence, reporting, and data analytics.\n\nThe ideal team member is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. Must be comfortable supporting the data needs of multiple clients and teams, systems, and products within the modernization of ecosystems. The right-fit person will be excited by the prospect of optimizing or even re-designing data processes to support next generation of advanced analytics capabilities, which in-turn translates to implementing strategic enhancements to maintain competitive advantage in the industry.\n\nThe successful team member will leverage their proficiency to\u2026\n\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\nBuild the analytics processes required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Data Management and cloud based \u2018big data\u2019 technologies.\nBuild analytics tools that utilize technologies to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.\nCollaborate with client teams to identify and solution regional and local requirements within the context of the standard global business model for enterprise analytics.\nParticipates in meetings, discussions, strategy sessions where changes, improvements, and enhancements are proposed, evaluated, and approved. \n\n\nThe exceptional team member will have or demonstrate progressive experiences in\u2026 \n\nBachelor\u2019s degree or higher in Computer Science or related field\nExperience working with BI and data warehousing tools\nExperience working with enterprise data, where security is paramount and data governance is critical\nAbility to work independently and in a team environment, and effectively engage all levels of the organization\nUnderstanding of distributed systems driving large-scale data processing and analytics with a successful history of manipulating, processing, and extracting value from large, disconnected datasets.\nAbility to communicate effectively (listening, presenting, and questioning)\nStrong organizational, written, and communication skills\n\n\nAbout ZAG\n\nZAG Technical Services empowers our clients\u2019 success through intelligent business solutions that increase employee productivity, network security and team collaboration. We aim to ensure that our client\u2019s IT investments drive ROI, provide a competitive advantage and help businesses grow. We do this by living our core set of values: Integrity Foremost, Client-Centered, Accountable Always, Teamwork Throughout, and Exceptional is the Goal.\n\nWhat ZAG Has To Offer\n\nCompetitive compensation package \nBenefits including medical, dental, vision, 401K and life insurance \n\n\nZAG Technical Services is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all of our employees.\n\nZAG Technical Services is an E-Verify participant and will provide the Social Security Administration (SSA) and, if necessary, the Department of Homeland Security (DHS), with information from each new employee\u2019s Form I-9 to confirm work authorization."}, {"title": "Junior Data Engineer", "company_name": "HireMeFast LLC - Career Accelerator - Land A Job", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3786249797/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D&trk=flagship3_search_srp_jobs&lipi=urn%3Ali%3Apage%3Ad_flagship3_search_srp_jobs%3BQExogrHBQoSF9zZm%2B9kufw%3D%3D&lici=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D", "id": "3786249797", "description": "About the job\nThis is a remote position.\n\n Job Title: Junior Data Engineer\n\n Employment Type: Full-Time\n\n Salary: $64,000 - $76,000 per annum\n\n Experience Required:  Minimum 1 year of project experience\n\nAbout us: HireMeFast is a leading staffing and recruitment agency specializing in connecting businesses with top-tier talent across various industries. Our mission is to bridge the gap between exceptional candidates and organizations needing their skills, expertise, and unique qualities. Our team of experienced and dedicated recruitment specialists utilizes innovative sourcing strategies, and a vast network to identify and attract top talent. We conduct comprehensive procedures to ensure that only the most qualified candidates are presented to our clients.\n\nPosition Summary\n\nJoin the fast-paced, innovative, and collaborative environment focused on providing an AIOps platform that enhances the intelligence of the CVS Health infrastructure. Work closely with subject matter experts and colleagues to build and scale out machine learning and AI solutions that will detect, predict, and recommend solutions to correct issues before system impact and enhance efficiency, reliability, and performance of CVS Health\u2019s IT operations.\n\nKey Responsibilities include:\n\nData pipeline development: Designed, implemented, and managed data pipelines for extracting, transforming, and loading data from various sources into data lakes for processing, analytics, and correlation. \nData modeling: Create and maintain data models ensuring data quality, scalability, and efficiency \nDevelop and automate processes to clean, transform, and prepare data for analytics, ensuring data accuracy and consistency \nData Integration: Integrate data from disparate sources, both structured and unstructured to provide a unified view of key infrastructure platform and application data \nUtilize big data technologies such as Kafka to process and analyze large volumes of data efficiently \nImplement data security measures to protect sensitive information and ensure compliance with data and privacy regulation \nCreate/maintain documentation for data processes, data flows, and system configurations \nPerformance Optimization- Monitor and optimize data pipelines and systems for performance, scalability and cost-effectiveness \n\nCharacteristics of this role:\n\nTeam Player: Willing to teach, share knowledge, and work with others to make the team successful. \nCommunication: Exceptional verbal, written, organizational, presentation, and communication skills. \nCreativity: Ability to take written and verbal requirements and come up with other innovative ideas. \nAttention to detail: Systematically and accurately research future solutions and current problems. \nStrong work ethic: The innate drive to do work extremely well. \nPassion: A drive to deliver better products and services than expected to customers. \n\nRequired Qualifications\n\n2+ years of programming experience in languages such as Python, Java, SQL \n2+ years of experience with ETL tools and database management (relational, non-relational) \n2+ years of experience in data modeling techniques and tools to design efficient scalable data structures \nSkills in data quality assessment, data cleansing, and data validation \n\nQualifications\n\nKnowledge of big data technologies and cloud platforms \nExperience with technologies like PySpark, Databricks, and Azure Synapse. \n\nEducation\n\nBachelor\u2019s degree in Computer Science, Information Technology, or related field, or equivalent working experience\n\nWhy HireMeFast LLC?\n\nAt HireMeFast, we understand that finding the right individuals to join your team is crucial for success and growth of your organization. We are committed to streamlining the hiring process for our clients, ensuring they have access to a diverse pool of highly qualified candidates who are a perfect fit for their specific needs."}, {"title": "Data Engineer  - Need candidates from Texas ( REMOTE )", "company_name": "Pulivarthi Group (PG)", "location": "United States (Remote)", "link": "/jobs/view/3770685212/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=9gdMwuif%2BpB%2BCQYvQi3imQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3770685212", "description": "About the job\nFollow us on Linkedin: https://www.linkedin.com/company/pulivarthigroup/\n\nPulivarthi Group LLC is a Global Staffing & IT Technology Solutions company, with our prime focus of providing world class solutions to our customers with the right talent. We combine the expertise of our team and the culture of your company to help you with the solution that is affordable and innovative using high quality standards and technologies.\n\nWe\u2019ve served some of the largest healthcare, financial services, and government entities in the U.S.\n\nData Engineer - FT only - Azure SQL with data lakes experience\n\nData Engineer -\n\nthe resource requirement for this Data Engineer role needed more clarification.\n\nOur customer is requesting a resource who knows Azure CI/CD (Continuous Integration / Continuous Deployment) to build and integrate their DevOps pipeline.\n\nOnce the DevOps pipelines are mapped, they need the Data Engineer to connect to all the Data Sources, primarily their Data Lake.\n\nThe Customer (based in Houston) is interested in a resource in a time zone not too far removed from the Central Time Zone to allow real time collaboration."}, {"title": "Data Engineer", "company_name": "Otter Products", "location": "Fort Collins, CO (Remote)", "link": "/jobs/view/3766728302/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=77U3U7CAi1PxgDmpI3%2BiNQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3766728302", "description": "About the job\nOverview \n\nOtter Products is currently recruiting for a Data Engineer to join our Data team! You can be based in our Fort Collins, CO office with flexibility to work a portion of your time remotely, or 100% Remote in the U.S.\n\nThe Data Engineer applies professional experience, concepts, and company objectives toward building and operationalizing the minimally inclusive data necessary for the enterprise data initiatives following industry standard practices and tools. The bulk of the Data Engineer's work would be in designing, managing and optimizing data pipelines, then moving these data pipelines effectively into production for key data and analytics consumers like business/data analysts, data scientists or other stakeholder that needs curated data for data and analytics use cases across the enterprise. The Data Engineer also ensures compliance with data governance and data security requirements while creating, improving and operationalizing these integrated and reusable data pipelines to enable faster data access, integrated data reuse and vastly improved time-to-solution for data and analytics initiatives. Additionally, the Data Engineer will be expected to collaborate with other data team members and data consumers working on the models and algorithms developed by them toward optimization for data quality, security, and governance to put them into production leading to potentially large productivity gains.\n\n About Otter Products \n\nOtter Products, we grow to give. From our founder's garage in 1998 to the global technology leader we are today, Otter Products continues to drive growth through innovation.\n\nThrough our industry-leading brands - OtterBox, Liviri and OtterCares - we provide our partners the number-one selling and most trusted products in our categories. Our philanthropic spirit is the foundation on which we foster our partner relationships, allowing us to grow and to give - together.\n\nBy way of our charitable arm, the OtterCares Foundation, we support our communities and invest in our future through education that inspires kids to change the world.\n\nAnd even as our global community of Otters continues to grow, our founder's core values are still at the heart of everything we do. We measure our success by our ability to give back to our communities and strengthen opportunities for all.\n\nFor more information visit otterproducts.com\n\n Responsibilities \n\nComplete analysis, design and development of BI solutions using Azure SQL Server\nFamiliarity with Data Warehouse concepts\nExperience coding the extraction, transformation, and loading (ETL) processes\nDatabase development primarily in SSIS, Data Factory and SQL\nPartner with the business to determine end user database/reporting needs and requirements\nGenerate ad hoc reports using Power BI or SSRS\nCollaborate with other developers to create and implement best approach solution(s)\nTroubleshoot Azure tools, systems, and software\nReview queries for performance issues, making changes as needed\nCollaborate with team to performance-tune Azure application as necessary\nAssist with the analysis and extraction of relevant information from large amounts of historical business data to feed data science initiatives\nParticipate and support the design and documentation of processes for large scale data analyses, model development, model validation and model implementation\nSupport and maintain a positive safety culture by following all safety policies and procedures and actively contributing to a safe working environment\nOther duties as required\n\n Qualifications \n\nBachelor's degree required. Degree in Computer Science or Mathematics preferred\nMinimum of four years of experience in an IT or analytical role required. Experience in database development, report writing and/or statistics preferred.\n\n EEO \n\nOtter Products, LLC is an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, age, marital status, pregnancy, sex, sexual orientation, gender, gender identity or expression, national origin, disability, veteran status, or any other characteristic or status protected by law.\n\n For US Based Roles Only - Compensation Range Minimum \n\nUSD $90,000.00/Yr.\n\n For US Based Roles Only - Compensation Range Maximum \n\nUSD $120,000.00/Yr.\n\n Additional Total Rewards \n\nProfit Sharing Program Eligible, Benefits Eligible - Full Time- check out otterproducts.com/careers/why for more info"}, {"title": "Software Engineer, Distributed Systems", "company_name": "Hightouch", "location": "United States (Remote)", "link": "/jobs/view/3628572539/?eBP=CwEAAAGMd4QO8APODyE_-mMOPhYyQGg5ZjVbPwq_3V7DK0B2bLAf10pcedM9S2T71X5i4TiJuEwOzWBbSiTKbLuk-VD-_Y-dN5uyf8YpYkHYqEYFxySMS9zr2hRjNrxX9gXbxssh35TFK9YTNV2WeH2Ncb675JgFbJTgNWSZySFC6rrdJGmdSDTr2q5d3VoxORz3GjsDbzjeyZQCvQ07RqYw7oujWSgslycs2LMCY_-N0NEj6e0PvbaYKiZ_5IW34bGVSYLXpHwqa8oOcNttYZ90UA3E_7sDSFzABdIVlfrp5zzERrIYy5sh2MIInxHdDoKgQw8VmOn5uM4S0tXUl-i7LnN0_W0NfPBmLiVDNuveIOm4MfDNPxfe5QKPk6FUAVpvgbcoJn9yRyE&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=rph%2FT6Q6Vkp6YoggJf9cJQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3628572539", "description": "About the job\nAbout Hightouch\n\nHightouch\u2019s mission is to empower everyone to take action on their data. Through our Reverse ETL platform, business and data users can seamlessly sync data from where it resides, such as warehouses and databases, to where it is needed, including operational systems and SaaS tools. Traditionally, acting on data has required engineering time and bandwidth, and left most business users stuck with charts and reports that are unable to take automated action on their data. With Hightouch, every business user, without writing any code, can activate data to streamline critical processes, improve marketing performance, and scale operations.\n\nOur team operates with a focus on making a meaningful impact for our customers. We believe in approaching challenges with a first principles thinking mindset, moving quickly and embracing our value of efficient execution, and treating each other with compassion and kindness. We look for team members that are strong communicators, have a growth mindset, and are motivated and persistent in achieving our goals.\n\nHundreds of companies use Hightouch, including Spotify, Ramp, Retool, NBA, Plaid, and Betterment. We\u2019re based in San Francisco, are remote-friendly, and backed by leading investors such as Amplify Partners, ICONIQ Growth, Bain Capital Ventures, Y-Combinator, and Afore Capital.\n\nAbout The Role\n\nWe are looking for a distributed systems engineer to work on the systems that power the syncing engine our customers and other engineering teams rely on. This presents an exciting challenge where you can apply your expertise in distributed systems, performance optimization, and troubleshooting to push the boundaries of what is possible and meaningful improve our ability to move massive amounts of customer data. This role also provides a unique opportunity to work on a multi-cloud and multi-region infrastructure that supports a global customer base.\n\nWe believe in enabling our engineers to do their best work for our customers by giving them extremely high levels of ownership and autonomy. This comes in different forms: you will own and deliver projects from start to finish, you will work directly with customers to solve their hardest scaling problems, and you will have a lot of influence over what we work on as a team and company.\n\nSome of the problems we\u2019ll be working on include:\n\nSync Speed: Customers want to sync a lot of data to important destinations like Facebook and Snapchat, which requires us to analyze every part of our syncing process and find where we can optimize to sync data more quickly\nStreaming Syncing: We currently sync data from sources in batch only, but envision a world where we also do syncs in real-time and support streaming sources like webhooks and queues\nScalability and Reliability: As part of our rapid growth, we\u2019re always evaluating current future bottlenecks in scaling and reliability, and architecting for the next order of magnitude of growth\nPersonalization API: Our Personalization API product provides a low latency caching layer on top of data warehouses that enable customers to use their data warehouse for real-time personalization use cases with \nMulti-Region and Multi-Cloud: Supporting our multi-region and multi-cloud backend, including extending it to launch Hightouch on in new regions to support data residency requirements of our global customer base\n\nWe are looking for talented, intellectually curious, and motivated individuals who are interested in tackling the problems above. This is a senior role, but we focus on impact and potential for growth more than years of experience. The salary range for this position is $170,000 - $240,000 USD per year, which is location independent in accordance with our remote-first policy.\n\nAbout You\n\nYou are an engineer with a passion for solving hard technical problems that generate real value for customers. You\u2019re motivated by high ownership and are comfortable in a fast-paced, startup environment.\n\nYou have experience and comfort with distributed systems and high-scale systems or the ability and desire to build this experience quickly. This experience can take different forms:\n\nYou\u2019ve built syncing engines that process and move terabytes of data per day in a scalable way\nYou\u2019ve created event collection or data streaming services that power company critical systems\nYou\u2019ve owned systems that have undergone orders of magnitude of growth and helped them scale accordingly\nYou have relevant skills and are a fast learner who is excited to take on the challenges we face\n\nInterview Process\n\nOur goal with the interview process is to balance speed with giving both parties opportunities to assess whether there is a strong mutual fit. We will ask you questions, but we want you to ask us questions! Our technical interviews focus on how you design systems because we believe this is the best way for us to see how you work and for you to see how we collaborate. We don\u2019t ask you to write code to solve technical brainteasers that don\u2019t appear in your day to day job.\n\nRecruiter screen [30m]: Introductory call with our recruiting team to get to know each other and see if the role could be a good mutual fit.\nSystems design screen [45m]: Designing a data processing feature end-to-end.\nHiring manager interview [30m]: Chat with hiring manager about past experiences and future operating preferences to assess fit on company values and operating principles.\nSystems design interview [90m]: Work with the interviewer to architect a system at a conceptual level. The problem will be at a pretty high level - and have both product and customer requirements as well as technical."}, {"title": "Entry-Level Data Scientist Engineer", "company_name": "Phoenix Recruitment LLC", "location": "Texas City, TX (Remote)", "link": "/jobs/view/3788727625/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=8iwtytHsCu4wMwJ4fj3pxw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3788727625", "description": "About the job\nThis is a remote position.\n\nTitle - Entry-Level Data Scientist Engineer, 1 year of project experience\n\nEmployment Type: Full-time\n\nBase Salary: $60K-$70K\n\nPhoenix Recruitment offers a variety of recruiting services to assist both employers and employees. They specialize in marketing open positions, recruiting, and helping employers to find qualified candidates across various industries. Phoenix Recruitment has expertise in streamlining the hiring process. They can help ensure that the process is efficient, well organized, and compliant with relevant regulations.\n\nSkills and Abilities:\n\nStrong knowledge of R or Python for data analysis and modeling. \nProficiency in statistical programs such as R, SAS, MATLAB, or Python. \nFamiliarity with spreadsheets (VBA) and database applications (Access, Oracle, SQL, or equivalent technology). \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \nAbility to learn new methods quickly and work under deadlines. \nExcellent teamwork and communication skills. \nStrong analytical and problem-solving abilities. \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \n\nPreferred:\n\nKnowledge of actuarial concepts and life, health, and/or annuity products. \nExperience with statistical modeling techniques such as GLM, Decision Trees, Time Series, Regression, etc. \nFamiliarity with Microsoft DeployR. \nExposure to insurance risk analysis. \nBasic experience in computational finance, econometrics, statistics, and math. \nKnowledge of SQL and VBA. \nFamiliarity with R or Python for predictive modeling \n\nWhy Phoenix Recruitment LLC?\n\nPhoenix Recruitment often has an extensive network of employers and candidates. This network allows them to tap into a pool of qualified candidates and connect them with suitable job opportunities. They can also leverage their connections to help employers find the right talent efficiently. Outsourcing the recruitment process to a specialized agency can save your time and resources, avoid delays, reduce administrative burdens, and increase the chances of finding the right fit for your organization."}, {"title": "Data Engineer - Remote Work", "company_name": "BairesDev", "location": "United States (Remote)", "link": "/jobs/view/3783162297/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=OtoCrAAHZFomVup04fy35g%3D%3D&trk=flagship3_search_srp_jobs", "id": "3783162297", "description": "About the job\nWho We are\n\nBairesDev is proud to be the fastest-growing company in America. With people in five continents and world-class clients, we are only as strong as the multicultural teams at the heart of our business. To consistently deliver the highest quality solutions to our clients, we only hire the Top 1% of the best talents and nurture their professional growth on exciting projects.\n\nWe are looking for a Data Engineer to join our Development team and participate in different projects made up of multicultural teams distributed throughout the world.\n\nWhat You Will Do\n\n Interview other Data Scientists and Data engineers to vet their technical abilities.\n Input data from those interviews into the product and help develop a predictive, fair, and enjoyable solution.\n Work together with developers, tech leads, and solution architects to build applications.\n Improve existing structures by adding new functionalities or proposing technological updates so that as a result, the impact of your contribution is significant in the core of each business.\n Improve the user experience on scalable and high availability platforms, contributing to the key differential of each business.\n\nHere\u2019s What We Are Looking For\n\n 5+ years of experience working as a developer.\n 3+ years of experience working as a Data Scientist / Data Engineer.\n Proficient with analysis, troubleshooting, and problem-solving.\n Hands-on experience with managing data loads and data quality.\n Advanced English level.\n\nHow we do make your work (and your life) easier:\n\n 100% remote work.\n Hardware setup for you to work from home.\n Flexible hours - make your schedule.\n Paid parental leave, vacation & holidays.\n Diverse and multicultural work environment.\n An innovative environment with the structure and resources of a leading multinational.\n Excellent compensation \u2014 well above the market average.\n Here you can grow at the speed of your learning curve.\n\nOur people work remotely but with a consistent and robust culture that promotes diversity and teamwork. To continue being the leading software development company in Latin America, we want to ensure that every BairesDev member gets the best growth and professional development opportunities in a diverse, welcoming, and innovative environment.\n\nEvery BairesDev team member brings something unique to our company.\n\nWe want to hear your story. Apply now!"}, {"title": "Data Engineer (Remote)", "company_name": "Cascade Debt", "location": "United States (Remote)", "link": "/jobs/view/3755336571/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=pmUtn1j2zyPr25CNPh2RNw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3755336571", "description": "About the job\nDo you want to work in a small team where you can make a real difference in a company? Would you embrace the challenge of building a high impact platform that is used globally? Then we want to speak with you!\n\nAbout Cascade\n\nCascade is a fintech startup backed by Canadian and US investors that empowers companies to grow by democratizing access to institutional debt. We are building tools that modernize how companies raise and manage debt, lowering the barriers to entry to the $7 trillion specialty finance and alternative credit market for companies around the world. Founded just last year, we are gearing up for our next phase of technical development and are seeking a talented Data Engineer to lead the way.\n\nAbout The Role\n\nAs a Data Engineer you\u2019ll be an early member of a growing team building a pioneering platform in the debt infrastructure space. You will design, build, and launch efficient, scalable, and reliable data pipelines to move and transform data .\n\nWhat You\u2019ll Do\n\n Building databases, data lakes and data ingestion pipelines to integrate customer databases and datasets to our systems \n Ingesting financial datasets from external customers, then u pdating and maintaining accurate and complete data mappings to ensure that our product s are displaying high quality data \n M onitoring and alerting across our data pipelines in order to make sure that our data ingests are reliable and correct \n Translat ing business goals and stakeholder requirements into new data flows and deliver analytical insights ensuring that the data flows create real business value continuously \n\nSkills And Experience\n\n 2 + years experience in a data engineering or data analyst role \n E xperience with relational SQL and NoSQL databases, including PostgreSQL , MySQL, MariaDB, MongoDB, Bigtable, etc. \n E xperience working with ETL technologies , such as Databricks, Fivetran , or dbt \n Proficiency in writing SQL queries and knowledge of analytical data warehouses such as RedShift, BigQuery , and Snowflake \n Some experience with CI/CD automation such as GitHub Actions \n Developing APIs and integrating with 3rd party APIs \n Bonus: experience in fintech / SaaS / credit analysis \n\nPeople Who Thrive At Cascade Are\n\n Self-starters, who take the initiative to tackle challenges in a remote work environment \n Good communicators, who can operate without ego to discuss, learn, grow, and help others do the same \n Passionate about creating great digital experiences for users \n Problem solvers who are not satisfied with the status quo \n\nBenefits\n\nRemote: we are a remote-first company and are very flexible on hours as long as things get done \nHome-office: there\u2019s a $1000 USD home office allowance to set yourself up \nEquity: we expect you to have an owner-mentality, and have the equity plan to match \nBenefits: health, dental, vision, and more \nPerks: we offer free lunches weekly and off-site trips \nJob satisfaction: we offer autonomy, ample opportunities for mastery, and an opportunity to make a difference for companies around the world"}, {"title": "Data Engineer", "company_name": "ISC (Integrated Specialty Coverages, LLC)", "location": "Carlsbad, CA (Remote)", "link": "/jobs/view/3778293528/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ew7%2FfEuXSujs5IGabdqUyw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3778293528", "description": "About the job\nAbout Integrated Specialty Coverages\n\nIntegrated Specialty Coverages LLC (ISC) is a growth stage technology and data-driven insurance company leading innovation in the market.\n\nBacked by one of the leading private equity firms, KKR, and led by a forward-thinking management team, ISC is combining the worlds of insurance and technology to create an Insurtech powerhouse. As a leading online distributor of insurance products for a range of industries and \"Main Street USA\", we are looking for the right people to help us in our mission of achieving exponential growth. We strive to be the number one place to go for brokers and agents to source insurance. To accomplish this, we're building a digitally focused team that deeply understands the intersection between user experience, data, and AI/ML to optimize the way we engage with our customers and partners.\n\nJob Summary\n\nThe Data Engineer role will be the technical liaison between multiple groups including a data science team, the engineering team, product management, and business stakeholders. You do not need any insurance knowledge prior, however, you must quickly dive deep into the insurance world and ask questions to become a subject matter expert. You will be responsible for building a data platform to facilitate the data science team. You must be a self-starter that can build out features such as a data pipeline from scratch. There will be support from both engineering and data science for any buildout.\n\nJob Responsibilities\n\nCreate and maintain optimal data pipeline architecture. \nAssemble large, complex data sets that meet functional / non-functional business requirements. \nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. \nBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using AWS technologies, SQL, and Python. \nWork with stakeholders including the Executive, Product, Data Science, and Engineering teams to assist with data-related technical issues and support their data infrastructure needs. \nWork with data science and analytics teams to strive for greater functionality in our data systems. \n\nMinimum Qualifications:\n\nAdvanced working SQL knowledge and experience working with relational databases, strong query authoring (SQL) as well as working familiarity with a variety of databases (Snowflake, Redshift, MySQL, MSSQL, etc.)\nExperience building and optimizing data pipelines, architecture and data sets. \nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. \nBuild processes supporting data transformation, data structures, metadata, dependency, and workload management. \nA successful history of transforming, processing and extracting value from large disconnected, datasets from a variety of data sources (Flat files, Excel, databases, etc.)\nStrong analytic skills related to working with unstructured datasets. \nStrong project management and organizational skills. \nExperience supporting and working with cross-functional teams in a dynamic environment. \nAbility to mentor/guide/collaborate with other team members. \n\nWe are looking for a candidate with 3-5 years of experience in a Data Engineer role. They should also have experience using the following software/tools:\n\nExperience with relational and MPP databases, including Snowflake and MySQL. \nExperience developing software in an agile environment from the requirements stage to production. \nExperience with version control: git\nExperience with container technologies: Docker\nExperience with data pipeline and workflow management tools: Airflow, Jenkins, AWS Glue, Azkaban, Luigi, etc. \nExperience with AWS cloud services: EC2, ECS, Batch, S3, EMR, RDS, Redshift\nExperience with other cloud services: Snowflake, Airflow\nExperience with object-oriented/object function scripting languages: Python, Java, C++, etc. \nExperience with data modeling and data warehouse design\nExperience with data visualization tools (PowerBI, QuickSight)\n\nThe starting annual pay scale for this position is listed below. Actual starting pay will be based on factors such as skills, qualifications, training, and experience. In addition, the company offers comprehensive benefits including medical, dental and vision insurance, 401(k) plan with match, paid time off, and other benefits.\n\nISC's salary ranges are determined by role and level. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations and could be higher or lower based on a multitude of factors, including job-related skills, experience, and relevant education or training.\n\nNational Pay Range\n\n$100,000\u2014$110,000 USD\n\nBenefits of Working at ISC\n\nCompetitive vacation and flexible working arrangements\nComprehensive and inclusive health benefits\nA variety of professional development and mentorship opportunities\nChoice of technology whether at home or in the office\n\nISC's Ownership Behaviors\n\nDo the Right Thing *Be Relentless. Pursue Excellence. *Personal Responsibility & Own Outcomes\nTry Fast. Learn Fast. Fail Fast. Think Big. *Build Extraordinary Owners & Join Forces *Grit & Determination\n\nApplicants with disabilities may contact ISC HR department via e-mail, and other means to request and arrange accommodation. If you need assistance to accommodate a disability, you may request accommodation at any time. Please contact ISC at HR@ISCMGA.com.\n\nISC believes in creating long-term relationships by being responsive and relevant and by consistently delivering value to our community of customers. Specifically, with our employees, we focus on attracting, developing, and retaining the best talent for our business, challenging our people, demonstrating a \"can-do\" attitude, and fostering a collaborative and mutually supportive environment. ISC is an equal opportunity employer and a participant in the US federal E-Verify program (US).\n\nDiversity creates a healthier atmosphere: All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, gender, gender identity, sexual orientation, marital status, medical condition, genetic information, mental or physical disability, military or veteran status, or any other characteristic protected by local, state, or Federal law."}, {"title": "Data Engineer -Remote", "company_name": "RIT Solutions, Inc.", "location": "Denver, CO (Remote)", "link": "/jobs/view/3768007496/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=1hQH8EDaII1eJ7nQ9BACQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3768007496", "description": "About the job\nDuration: 6+ months CTH\n\nMust have a valid LinkedIn profile\n\nRequired Skills\n\n 3-6 years of experience (no more than 6 years)\n Start up environment experience\n Python, SQL, and DBT"}, {"title": "Big Data Engineer - W2", "company_name": "Megan Soft Inc", "location": "United States (Remote)", "link": "/jobs/view/3674845652/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fCAxh4N%2F4YZJ5aEIASaeeA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3674845652", "description": "About the job\nTitle: Big Data Engineer - W2\n\nLocation: Dearborn, MI (Remote)\n\nDuration: 12+ Months\n\nDirect Client: Ford Motors\n\nMust have skills: Should be able to handle GCP requirements\n\n\"Translates business requirements to build repeatable, sustainable, efficient, coded processes that can be productionized by Software Engineers and readily modified by other Data Engineer Creates POC processes in Dev/QA and works with Software Engineers to productionize and align best coding and process practices that maximize efficiency, speed, stability, system resources and capabilities Awareness of and compliance with data privacy, security, legal and contractual guidelines Incorporates data quality and privacy checks/alerts to minimize bad data being consumed by end users, models and dashboards, and to protect customer data; revises checks for new data issues Validates data products and pipelines are functioning as expected following system or application upgrades, source changes, etc. Works closely with Architects if required to align systems, tools and applications being utilized with business use case and performance requirements Provides data product support and maintenance Establishes, tracks and monitors KPIs related to specific data products and deliverable Participate in iteration cadence activities (Iteration Planning, task estimation, daily scrum, demos, etc.). Will be a part of on-call team to resolve critical production support tickets during business hours and off hours.\n\nSkills Required\n\nExperience with Big Data Technologies (Hadoop , ETL Tool, Spark. Hive etc..) Someone who understands Cloud and has experience working with Cloud Platforms (preferably GCP) Should be able to handle GCP requirements - ingesting into GCS/ Big Query, working with GCP components Hands-on in Data Flow, Cloud Storage (GCS), Big Query and GitHub. Proficient in Python and SQL.\n\nSkills Preferred\n\nManages ETL: uses programming and tools for data ingestion, configures pipelines, applies transformations and decoding, integrates and fuses data, moves and securely deliver Experience NoSQL databases\n\nAdditional Information\n\nIf the candidate is remote only, please indicate \"\"100% Remote\"\" under candidate's name on resume. Other similar schedule notations can be \"\"Local Candidate\"\" or \"\"Hybrid Candidate.\"\" ***HRA Test Required***\""}, {"title": "Remote Opportunity: Quantexa Data Engineer", "company_name": "SPAR Information Systems LLC", "location": "United States (Remote)", "link": "/jobs/view/3680073191/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ioykVCZSOQ7BK5%2FwRQHOWg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3680073191", "description": "About the job\nHello All, Hope you are doing great!! Please go through the job descriptions and let me know your interest. Role: Quantexa Data engineer  Location: Remote Duration: Long Term Contract Job Description: 1. End to End usage of Quantexa including guiding our client team on how to use from both front end and data ingestion Reason they are asking for Certification 2. Resolve Entity issues with data 3. Knowledge of Scala and Hive which requires coding experience using parquet 4. Advanced SQL knowledge requiring writing SQL queries 5. Experience having worked with GCP data lake 6. Quantexa certification required\n\nThanks & Regards,\n\nSatnam Singh\n\nDirect: 201 623 3660\n\nEmail : Satnam.singh@sparinfosys.com"}, {"title": "Remote Work - Need Staff Data Engineer - USC only", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3746283980/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=CvIuLBNO%2BPMGi4IXVRkP0A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3746283980", "description": "About the job\n100% Remote, client is in NY\n\nLook for strong job history, Python, Machine Learning, SQL, 10/10 comms, etc. \n\nMUST be an Engineer \n\nSummary\n\nWe're looking for an experienced Data Engineer to join our client's growing team of top notched IT professionals. You will work closely across multiple groups including Editorial and Audio, Marketing, Advertising, etc. to achieve their goals and objectives. The Data Engineering team builds tools throughout the stack while sharing knowledge across these departments.\n\nAs the company grows, they're looking for Data Engineer who will help solidify and expand our pipelines and maintain their data warehouse. Their business model is based on performing complex and very detailed analyses of how our products perform with their subscribers. You'll be responsible for working alongside analysts, product, marketing, and other teams to define new data collection components and measurement schemes that support each new product and feature.\n\nHow Can Add Value To Their Mission\n\nCreate and maintain data pipelines to provide insights and drive business decisions\nEstablish data warehousing strategy (ex. Kimball, Data Vault, etc.)\nMaintain data infrastructure on our AWS accounts\nCollaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.\nWrite unit/integration tests, contributes to engineering wiki, and documents work\nImplement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it\n\nWhat You'll Need To Succeed\n\n7+ years of industry experience measuring product performance and user behavior\nExperience working with a variety of data management technologies, including RedShift, Kinesis/Kafka, Glue, Spark, Postgres, Airflow, dbt, and others.\nExperience implementing BI reporting tools such as Looker\nExperience interfacing with engineers, product managers and analysts to understand data needs\nKnowledge of a variety of measurement beacons, SDKs, APIs including Google Analytics, Amplitude, Braze, Stripe, email service providers\nA commitment to building secure, resilient, fault-tolerant architectures, with clear documentation and procedures in place for support\nA focus on accuracy and detail as you build, ensuring data pipelines produce clear, predictable results\nUnderstanding of the typical metrics a subscription and advertising-supported business needs to measure success\nExperience with Client techniques as applied to behavioral segmentation or anomaly identification is a definite plus\nFamiliarity using developer tools that increase productivity and facilitate the development of resilient code (eg. Docker, CircleCI, Serverless) is a plus"}, {"title": "ENTRY LEVEL DATA ENGINEER (REMOTE)", "company_name": "SynergisticIT", "location": "Jacksonville, FL (Remote)", "link": "/jobs/view/3767587909/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=om7L3CnowiC4PLIcpZD70A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3767587909", "description": "About the job\nSYNERGISTICIT wants every candidate to know that the Job Market is Challenging and to stand out, you need to have exceptional skills and technologies and that's where we come in to make sure you get the attention which you need\n\nSynergisticIT understands the complex nature of the job market and how difficult it can be to secure a position, especially for fresh graduates. Therefore, we assist and help tech-savvies to convert their passions into professions. We go above and beyond to keep you working in your niche.\n\nAs we focus on long-term success, we provide complete career development solutions. From job search to upskilling portfolio and interview preparation, we can guide you at every step of your career.\n\nSynergisticIT spares no efforts to connect you with a large network of tech giants, including Google, Apple, PayPal, Dell, Cisco, Client, etc. Presently, we are actively looking for ENTRY LEVEL DATA ENGINEER a driven mindset. Get the right opportunity and gain experience in building web-centric solutions on Java.\n\nWho Should Apply : Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or anyone looking to make their career in IT Industry\n\nWe also assist in filing for STEM extension and H1b and Green card filing.\n\nCandidates who are serious about their future in the IT Industry and have set big goals for themselves.\n\nCandidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. We also offer Skill Enhancement Programs if the candidates are missing skills or experience which our clients need with great outcomes\n\nCandidates can benefit from skill enhancement if they fall into the below categories. If they are qualified with enough skills then no need for skill enhancement\n\nCandidates Who Lack Experience\n\nHave had a break in careers\n\nLack Technical Competency\n\ncandidates who want to get employed and make a career in the Tech Industry \n\nPlease Also Check The Below Links\n\nhttps://www.synergisticit.com/candidate-outcomes/\n\nhttps://www.synergisticit.com/java-track/\n\nhttps://www.synergisticit.com/data-science-track/\n\nhttps://www.synergisticit.com/which-is-the-best-option-for-tech-job-seekers-staffing-companies-consulting-companies-bootcamps-or-synergisticit/\n\nhttps://www.synergisticit.com/contact-us/\n\nIf the skills are not a match candidates can opt for Skill enhancement. Or their resume can be sent out to clients to see if responses are achievable\n\nREQUIRED SKILLS For Java/Software Programmers\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Core Java , javascript , C++ or software programming\nSpring boot, Microservices and REST API's experience\nExcellent written and verbal communication skills\n\nFor data Science/Machine learning\n\nRequired Skills\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Statistics, Python, data visualization tools\nExcellent written and verbal communication skills\n\nPreferred skills: NLP, Text mining, Tableau, Time series analysis\n\nTechnical skills are required by clients for selection even if its Junior or entry level position each additional Technical skill helps a candidate's resume to be picked by clients over other job seekers.\n\nClients hire candidates with the right technical skills which they need and reject candidates who lack the required technical skills.\n\nNo third party candidates or c2c candidates\n\nPlease apply to the posting\n\nNo phone calls please. Shortlisted candidates would be reached out"}, {"title": "Data Engineer - Remote | WFH", "company_name": "Get It Recruit - Information Technology", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3782241984/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=BgBf1kSAAX%2BSuw6inJsIVw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3782241984", "description": "About the job\nJoin an innovative team at a leading company in the engineering and manufacturing industry, where we are committed to leveraging data for continuous improvement and operational excellence. This hybrid position allows you to enjoy the flexibility of remote work on Mondays and Fridays.\n\nWhy Join Us\n\nAt our company, we prioritize your well-being and professional growth. We offer a comprehensive benefits package, including excellent health benefits, a 6% 401(k) match, generous paid vacation and sick time, 12 paid holidays, and more.\n\nRole Overview\n\nAs a Data Engineer, you will play a pivotal role in transforming raw data into actionable insights, driving improvements in engineering and manufacturing processes, enhancing product quality, and optimizing operational efficiency. Collaborate with dynamic cross-functional teams to extract, analyze, and visualize data, influencing strategic decisions through your valuable insights.\n\nKey Responsibilities\n\nData Collection and Integration:\n\nGather, clean, and integrate data from diverse sources, including production databases, sensor data, and external datasets.\n\nData Analysis\n\nConduct exploratory data analysis to identify patterns, trends, and anomalies.\n\nPerform statistical analysis and hypothesis testing to derive meaningful insights.\n\nData Visualization\n\nCreate informative and visually appealing dashboards and reports using tools like Tableau, Power BI, or Python libraries.\n\nPredictive Modeling\n\nDevelop predictive models to forecast manufacturing performance, quality, and operational outcomes.\n\nRoot Cause Analysis\n\nInvestigate production issues and quality concerns by analyzing data to identify root causes and recommend corrective actions.\n\nPerformance Monitoring\n\nEstablish and maintain key performance indicators (KPIs) to track manufacturing and engineering metrics.\n\nGenerate regular reports to communicate performance trends to stakeholders.\n\nCross-functional Collaboration\n\nCollaborate with engineering, manufacturing, and quality assurance teams to understand their data needs and provide analytical support.\n\nContinuous Improvement\n\nSuggest process improvements and optimizations based on data analysis.\n\nData Security And Compliance\n\nEnsure data security and compliance with relevant regulations, such as GDPR or industry-specific standards.\n\nInfrastructure And Development\n\nBuild and maintain the infrastructure for collecting, storing, and processing data.\n\nDesign, develop, and manage data pipelines, ETL processes, and data warehouses.\n\nRequired Experience\n\nProven experience as a Sr. Data Analyst and/or Data Engineer in an engineering or manufacturing environment.\n\nProficiency in data analysis tools and programming languages, such as Python, R, SQL, and Excel.\n\nStrong data visualization skills with tools like Tableau or Power BI.\n\nExcellent knowledge of statistical analysis and machine learning techniques.\n\nAbility to communicate complex findings and insights effectively to both technical and non-technical stakeholders.\n\nStrong problem-solving skills and attention to detail.\n\nFamiliarity with manufacturing processes and quality control is a plus.\n\nExperience with enterprise ERPs (SAP S4HANA, Oracle EBS), CRMs (Salesforce, Hubspot), and IoT.\n\nProficient in working with cloud computing platforms including AWS, GCP, and Azure.\n\nExpertise in configuring and optimizing cloud-based data solutions for scalability, reliability, and performance.\n\nBachelor's Degree in Data Science, Statistics, Engineering, or Computer Science Required.\n\nApply Now and be part of a team shaping the future of data-driven innovation in engineering and manufacturing!\n\nEmployment Type: Full-Time"}, {"title": "Entry Level Software Engineer - Data Backend Engineer (Remote - Canada)", "company_name": "Yelp", "location": "United States (Remote)", "link": "/jobs/view/3752881757/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fankeoxifvhpTzOYvowHYQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3752881757", "description": "About the job\nYelp engineering culture is driven by our values: we\u2019re a cooperative team that values individual authenticity and encourages creative solutions to problems. All new engineers deploy working code their first week, and we strive to broaden individual impact with support from managers, mentors, and teams. At the end of the day, we\u2019re all about helping our users, growing as engineers, and having fun in a collaborative environment.\n Interested in exploring data and finding innovative ways to collect it, curate it, and make it easily accessible for others to use? How about doing that at an enormous scale? Yelp\u2019s datasets contain billions of interactions between users and local businesses around the globe. If working with our vast amount of data sounds exciting, come and join us!\n\nThe Marketplace Data Observability team is responsible for managing, governing, and monitoring all of Yelp\u2019s Ads data to protect our revenue stream and also support offline analysis that aids in making long term company decisions. We build tools that help us to better understand our data quality, and we strive to uncover data issues before they impact our consumers.\n\nAs a Data Backend Engineer on this team, you will be responsible for building elegant and scalable data products that serve critical, up-to-date, structured information to support different types of analytics within Yelp. As a core contributor to our growing data modeling and data warehousing engineering efforts, you will help design and own mission-critical data flow pipelines and datastores to enable decisions including effective A/B testing and company investments.\n This opportunity is fully remote and does not require you to be located in any particular area in Canada. We welcome applicants from throughout Canada. We\u2019d love to have you apply, even if you don\u2019t feel you meet every single requirement in this posting. At Yelp, we\u2019re looking for great people, not just those who simply check off all the boxes.\n  Build systems that can effectively store and crunch terabytes of data.\nDesign and develop data models for efficient data storage, retrieval, and reporting.\nCreate and maintain conceptual, logical, and physical data models using industry-standard modeling tools.\nCollaborate with cross-functional teams, including engineers, data analysts, business analysts, and data scientists, to understand data requirements and translate them into effective data models.\nParticipate in data integration efforts, including ETL processes and data migration.\nStay up-to-date with industry best practices, emerging technologies, and trends related to data warehousing.\nSupport on-call rotations as needed to operate the team.\n  Understanding of high performing and scalable data systems.\nExperience in building and orchestrating ETL pipelines.\nExperience with Data Lake or Data Warehouse landscape.\nA hunger for tracking down root causes and fixing them in systematic ways.\nAbility to communicate effectively to technical and non-technical cohorts alike. \nExposure to some of the following technologies: Python, AWS Redshift, AWS Athena / Apache Presto, Big Data technologies (e.g S3, Hadoop, Hive, Spark, Flink, Kafka etc), NoSQL systems like Cassandra, DBT."}, {"title": "Data Engineer (Associate Consultant), June 2024", "company_name": "UDig", "location": "Nashville, TN (Remote)", "link": "/jobs/view/3716039960/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=XUjo40kU55HnDCqJ%2F3FdPg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3716039960", "description": "About the job\nCan UDig It? \n\nUDig designs, builds, and implements technology solutions that deliver on business objectives. People join our team because they are excited to have an impact on the future of our organization, passionate about delivering meaningful results, and energized by solving complex problems. Oh, and they like working with other driven, smart, and authentic people!\n\nUDig is proud to be recognized as a \"Best Places to Work\" in Virginia and Tennessee. We're committed to fostering a transparent environment where leadership is accessible, professional development opportunities are abundant, and you are empowered to achieve your career goals. You'll collaborate, develop deep relationships with teammates, and accelerate your success while having fun!\n\nIf you're graduating in May 2024 and excited by the idea of impacting organizations by building data solutions to achieve business goals, you should consider joining our growing technology team.\n\nHow Things Go Down\n\nAs a new, entry-level teammate, you'll take part in our \"Breaking Ground\" training program designed to help you quickly get up-to-speed on UDig's delivery approach within a project team. The program covers:\n\nUDig delivery process and methodologies such as gathering requirements and delivering projects using agile\nTechnical guidance and instruction on the best practices from Data leaders at UDig\nDelivering executive and technical presentations\nCoaching to prepare you to for success on your first client project at UDig\n\nA typical day might entail:\n\nProject standups and check-ins\nDesign and development of data pipelines, BI reports, and more\nPlanning and participating in project demos\nContributing to internal UDig projects\nCareer and job growth through UDig-provided training\nCreating long-lasting friendships with teammates through fun activities as well as your projects (including social events, winning a prize for guessing the weight of pumpkin, or sharing experience on cool tech side projects)\n\nUDig believes it is important for Associate Consultants going through our Breaking Ground program to reside in one of our major markets (Richmond, VA or Nashville, TN metro area) for at least the first year of employment.\n\nWant to learn more? Hear directly from our teammates by checking out our 'A Day in the Life' blog series: https://www.udig.com/digging-in/breaking-ground-2023/\n\nSound Interesting? Here's the Technical Piece\n\nBelow are a few boxes you should be able to check if\u202fyou're interested: Don't check every box? Go ahead and apply!\n\n1+ year of experience with relational database management systems such as Microsoft SQL Server, MySQL, or Oracle\nUnderstanding of the full lifecycle of design, development, and implementation of data solutions including architecture design, development, testing\nSome experience in ETL development or reporting is a plus\nAbility to present technical ideas and high-level concepts and solutions to internal and external team members with varying degrees of technical knowledge\nBachelor's Degree in in computer science, information technology, or any science or business discipline\nAbility to take your work seriously but also seriously enjoy a good time with your UDig colleagues\n\nWhat's in It For You? \n\nAt UDig, you will build your career alongside talented and experienced developers while gaining experience and having fun! We provide continuous challenges and professional development opportunities. Additionally, we offer: \n\nCompetitive salary, merit reviews, and career advancement paths\nFlexible, hybrid environment\nIndividual $1500 Training Budget\nRegular team building and social activities (virtual and/or in-person)\nTransparent culture with strong communication and access to leadership\nGreat benefits like\nGenerous Paid Time Off, company holidays, and parental leave\nMultiple Single and Family Health Insurance plans to choose from\nDental & Vision coverage\nShort-Term and Long-Term disability\nOptional accident and critical illness coverage\nMatching 401(k)\nand more!"}, {"title": "Expression of Interest: Data Engineer", "company_name": "Fingerprint for Success (F4S)", "location": "New York, NY (Remote)", "link": "/jobs/view/3787769880/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=l%2FEDpQqUNmCttGdIyTk7Cg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787769880", "description": "About the job\nWe are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.\n\nThe F4S Talent Pool is a pilot project designed to:\n\nHelp job seekers get discovered by our partners based on their anticipated hiring needs.\nProvide optional support and resources for job seekers in their career endeavors.\nHelp individuals understand, and bring out the best in themselves and each other.\n\nThe F4S Talent Pool process:\n\nOnce you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.\n\nAbout Fingerprint For Success (F4S)\n\nBacked by 20+ years of research, F4S\u2019s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high performing teams.\n\nKeep in mind, joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your F4S work style assessment to match the hiring needs of our partners.\n\nYour feedback is a gift! Write to us via:\n\nPowered by JazzHR\n\noWuPJ9lwq9"}, {"title": "Data Engineer Seattle, WA (remote)", "company_name": "Conch Technologies, Inc", "location": "United States (Remote)", "link": "/jobs/view/3765052211/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=SBETI78UMutCHuRp5FnP%2FA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3765052211", "description": "About the job\nHi,\n\nGreetings from Conch Technologies Inc\n\nData Engineering Lead \u2013 With Databricks and Unity Catalog\n\nLocation \u2013 Seattle, WA (remote)\n\nDuration \u2013 12 months\n\nNeed: Design and implement data architecture and strategy, leveraging technologies such as Databricks, Unity Catalog, Privacera, and Collibra.\n\nAs a Data Engineering Lead, you will play a pivotal role in defining and executing technical deliverables while assessing complexities and levels of effort. You'll engage with both vendor and internal teams to resolve technical challenges and ensure seamless collaboration. Your leadership will extend to planning and prioritization with the SCRUM master, facilitating effective coordination between onsite and offshore teams, and driving feature prioritization through collaboration with product managers.\n\nKey Responsibilities\n\n Lead the definition and breakdown of technical deliverables, assessing complexities and levels of effort.\n Collaborate with vendor and internal teams to address technical issues.\n Drive planning and prioritization in conjunction with the SCRUM master.\n Coordinate with onsite and offshore teams for timely project delivery.\n Collaborate with product managers to prioritize features, communicate with stakeholders, and clarify requirements.\n Collect and manage user-reported issues, ensuring prompt resolution and communication.\n Define comprehensive test plans, oversee their execution, and document results.\n\n--\n\nWith Regards,\n\nNagesh G\n\nMobile: 408-381-5645\n\nDesk: 901-313-3066\n\nEmail: nagesh@conchtech.com \n\nWeb: www.conchtech.com"}, {"title": "Remote Work - Need Azure Fabric Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3759325599/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=C3iVjmcR9wjUip1nNjUiCg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3759325599", "description": "About the job\nThe client is placing heavy emphasis on Microsoft and Azure Fabric experience\n\nInformation Management Engineer \u2013 Take the identified information management sources and assist migration to the Client Data Lake, Azure Fabric. The engineer will transform information management (de-dup, clean and enrich).\n\nTake the identified data sources and bring them into our Data Lake, Azure Fabric. Once the data is in, the engineer will need to be able to transform data (de-dup, clean and enrich the data). We are focused on using a medallion data model for data architecture. The tools we are working with are: Delta Parquet files, python, spark, notebooks, T-SQL, Fabric lakehouse/warehouse, Dataflows Gen2, Data Pipelines, Direct Lake, shortcuts, ADF."}, {"title": "Data Engineer", "company_name": "Catalist", "location": "Washington, DC (Remote)", "link": "/jobs/view/3744951279/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ui1Fk6ICmnPNqAlZDZAcwA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3744951279", "description": "About the job\nFor over 17 years, Catalist has been a leader in civic data and data science innovation. Our mission is to provide progressive organizations with the data, software, and services needed to better identify, understand, and communicate with the people they need to engage and mobilize. Our clients include the largest, most influential organizations in the U.S. active in civic engagement, advocacy, and political campaigns.\n\nCatalist is home to a dedicated, creative team of technologists, data scientists, and campaign experts committed to using our talents and technology to nurture a vibrant and growing progressive community.\n\nAs a Data Engineer at Catalist, you will work closely with the Analytics and Technology departments to design, build, support, and maintain various data pipelines and processes with an end goal of providing data and intelligence to the progressive community. This role primarily involves translating data architecture designs into functional processes, code, and systems.\n\nThe ideal candidate will be a highly motivated individual with excellent technical skills, a strong desire to learn new skills, and an interest in progressive politics. Catalist values creativity and problem-solving. Our work is on the cutting edge of data-driven politics, and your support will help Democratic candidates and progressive organizations conduct successful advocacy and electoral campaigns.\n\nThis position reports to the Deputy Chief Data Officer. The Data Engineer is a part of a growing Data team that supports all underlying work at Catalist.\n\nThis position is included in our CWA bargaining unit.\n\nPrinciple Duties & Responsibilities\n\nDevelop scalable, production processes, code, and systems for data ingest, transformation, modeling, and reporting across a variety of platforms\nTranslate mock-ups and designs into functional processes, code, and systems\nCreate architecture designs when needed\nProvide quality assurance and testing on processes, code, systems, and products\nBecome an internal subject matter expert on various datasets and support other Catalist departments/teams on usage of those datasets\nExecute ad-hoc data and database maintenance tasks as requiredProject manage cross-departmental efforts, with direct responsibility for stakeholder engagement, management, and execution of technical elements\n\n\nRequirements\n\nBS or BA in a technical field, or relevant experience\n1-2 years of experience working with SQL databases\nExperience with data cleaning or ETL processes\nExperience with distributed computing systems and/or distributed datastores (particularly the Hadoop ecosystem)\nExperience managing projects\nFamiliarity with Catalist data, progressive politics, voter files, and/or commercial data\nBackground check required\n\n\nPreferred Skills & Abilities\n\nWillingness to be a problem solver and produce results in a fast paced environment\nAbility to be creative and personable, and articulate ideas clearly\nExperience working with SQL databases\nProficiency with Python or another object-oriented programming language (R, Java, Scala, etc\u2026)\nExperience working in cloud environments (AWS, GCP, etc.)\nExperience working in command line environments such as Bash\nExperience with a version control tool such as git or github\n\n\nBenefits\n\nMedical, Dental, Vision, Prescription Drug\n\nCatalist offers Medical, Dental, Vision, and Prescription Drug coverage for eligible staff and their eligible dependents. Catalist\u2019s Medical plan is a comprehensive PPO program including Prescription Drug coverage with 85% of the premium paid by Catalist. Dental and Vision coverage is provided at no cost to employees.\n\nGroup Term Life Insurance and Long-Term & Short-Term Disability Coverage\n\nGroup Term Life Insurance and Long-Term and Short-Term Disability coverage is available for eligible staff. These benefits are provided at no cost to Catalist employees.\n\n401(k) Safe Harbor Plan\n\nA 401(k) Safe Harbor Plan is available to eligible staff with a 3% contribution from Catalist from the date of hire. Employees may contribute pre-tax or post-tax from their salary up to the legal limits set forth by the IRS.\n\nMedical and Dependent Care Flexible Spending Accounts (FSAs)\n\nCatalist offers an FSA Program that gives eligible staff the ability to pay out-of-pocket medical/dental/vision/child care expenses from pre-tax earnings.\n\nTransit Benefits\n\nCatalist also makes available a Transit benefit FSA program to eligible employees using pre-tax contributions with a company match.\n\nProfessional Development and Remote Work Expenses\n\nEligible employees may be reimbursed up to $750 each year for professional development / education and remote work expenses.\n\nStudent Loan PayDown or SaveUp\n\nCatalist offers a Student Loan PayDown and College SaveUp benefit for eligible staff.\n\nVacation, Personal Leave, Sick Leave Benefits\n\nCatalist offers generous vacation benefits to all eligible staff. Eligible employees also receive:\n\n 14 Paid Holidays\n Personal Days\n Sick Leave\n Parental Leave\n\n\nHybrid Office/Remote Work\n\nCertain positions at Catalist are eligible for Office/Remote Hybrid or full Remote status."}, {"title": "Remote Entry Level Data Analyst/Engineer", "company_name": "SynergisticIT", "location": "White Plains, NY (Remote)", "link": "/jobs/view/3774171423/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=q54sk1%2FRww0z%2BC%2BE8e0jQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774171423", "description": "About the job\nAt SynergisticIT, we're all about making connections. Whatever IT goals you have, our software programmers can help achieve those. Our Software development teams can take up turnkey projects and execute them in an effective and efficient manner. If you are looking to source talent our recruiters will find the ideal IT talent for your company. What's the secret to our success? Well, it all starts with taking quality time to listen to each client's specific needs. After we have a thorough grasp of your IT goals, we can better customize our Developments as per your specific needs. We can also tailor make recruiting programs to exceed your expectations. Since our founding in 2010, SynergisticIT's strategies have earned the company an enviable position in the software development, IT staffing and IT skill enhancement fields. SynergisticIT continues to work with hundreds of satisfied American clients with our software programmers working on our projects and after gaining hands on experience on cutting edge technologies moving to contribute their skills to great clients like Apple, Google, Client, Ebay, Paypal, Kroger, the Walt Disney Company and hundreds more. If you are tired of working with inefficient programmers who take a lot of time to ramp up we want you to try us. Our software programmers can hit the ground running and get you the maximum return on your investment. You have already tried the rest its time you tried the best. SynergisticIT - Home of the Best Data Scientists and Software Programmers in the Bay Area.\n\n Why Us ? \n\nSynergisticIT has a proven track record of successfully skill enhancement and staffing IT employees for some of the world's most iconic brands. Our team takes the time to fully understand every client's needs so we could best meet your IT staffing requirements. The knowledgeable staff at SynergisticIT is always more than happy to work with clients to ensure they reach their software development goals. Besides staffing, SynergisticIT is also committed to helping young IT professionals advance their career with a robust upskill program . Everyone who goes through SynergisticIT's program learns all the skills necessary to succeed in many IT fields ranging from Java to Machine Learning. Additionally, everyone trained at SynergisticIT has been through extensive mock and technical interview screenings to bolster their career prospects. Last, but certainly not least, SynergisticIT takes great care to respect the privacy considerations for every client. All companies who work with SynergisticIT can rest assured their confidential data is protected using the most up-to-date encryption technologies. SynergisticIT also complies with all the latest NDA agreements.\n\n REQUIRED SKILLS For Java /Software Programmers \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Time series analysis \n\n We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2022 and at Gartner Data Analytics Summit (Florida)-2023 \n\nOracle CloudWorld Event (OCW) Las Vegas 2022 | SynergisticIT - YouTube\n\n https://www.youtube.com/watch?v=OAFOhcGy9Z8 \n\n https://www.youtube.com/watch?v=EmO7NrWHkLM \n\n https://www.youtube.com/watch?v=NVBU9RYZ6UI \n\n https://www.youtube.com/watch?v=Yy74yvjatVg \n\nSynergisticIT at Gartner Data and Analytics Summit 2023 - YouTube\n\nFor preparing for interviews please visit  https://www.synergisticit.com/interview-questions/ \n\n We are looking for the right matching candidates for our clients \n\n Please apply via the job posting \n\n REQUIRED SKILLS For Java /Full Stack/Software Programmer \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning Positions \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Tensorflow \n\n If you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team. \n\n No phone calls please.  Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates"}, {"title": "Data Engineer", "company_name": "Anika Systems", "location": "Leesburg, VA (Remote)", "link": "/jobs/view/3787766893/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=mApL9JnN%2B%2Fr2%2BYUPlxRYOA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787766893", "description": "About the job\nAnika Systems is a fast growing, woman-owned small business that specializes in providing innovative IT solutions for federal government agencies. Our expertise lies in accelerating delivery in Data and Analytics, Intelligent Automation, Application Development, and IT Modernization. We are currently expanding our Federal team and are seeking a passionate and talented Data Engineer. This opportunity is 100% remote.\n\nMust be a U.S. Citizen with the ability to obtain and maintain a government suitability clearance\n\nResponsibilities\n\nDesigns, develops, builds, analyzes, evaluates, and installs database management systems to include database modeling and design, relational database architecture, metadata, and repository creation and configuration management.\nDefines and oversees database organizations, standards, controls, procedures, and documentation. Provides technical consulting in the definition, design, and creation of a database environment.\nAdvises applications development staff and users on data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nDesigns and implements databases with respect to access methods, access time, batch processes, device allocation, validation checks, organization, protection and security, documentation, and statistical methods.\nUses data mapping, data mining, and data transformational analysis tools to design and develop databases.\nDetermines data storage and optimum storage requirements.\nPrepares system requirements, source analysis, and process analyses and designs throughout the database implementation.\n\nRequired Skills And Experience\n\nBA/BS and 1 year of relevant experience\nExperience with DataBricks, SQL, Python\nApplications development experience with data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nExcellent communication skills (both oral and written) with process-oriented organizational skills to ensure project success\n\nDesired Skills And Experience\n\nDemonstrated successful development experience with the full life cycle of government database management systems including database modeling and design, relational database architecture, metadata and repository creation, and configuration management.\n\nPowered by JazzHR\n\nTlKWIorucJ"}, {"title": "Remote Work - Need Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3774712978/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=r3Bp%2F0qcOD2OpC5jpbUOcA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774712978", "description": "About the job\nTitle : Data Engineer\n\nLocation : Remote\n\nDuration : 6+ months to start (long term contract (work order are extended 2x year))\n\nPlease Fill out the skill matrix: Technology Some Awareness (1) Novice (2) Intermediate (3) Advanced (4) Expert (5) SQL Tableau Desktop /Creator Tableau API Integration AWS - S3 Glue AWS - Athena Redshift AWS - Lambda AWS - SNS/SQS msExcel\n\nProfile: Data Analyst will work closely with Architects and leads in the Interflow team, key business stakeholders including program staff to support the organizations effort in the areas of data Analysis and information management. Role should have strong experience in writing SQL queries to extract data for reports. Candidate should have experience working with large set of data. As these reports are very critical for the company and we are looking for an expert who is capable of handling, consuming and extracting large sets of data for analysis and reports.\n\nMust Haves\n\nAWS - Athena\nAWS - S3\nGlue\nhands-on development of reporting applications\nMS Excel\nRedShift\nSQL\nTableau Desktop /Creator\n\nNice\n\nAWS - Lambda\nAWS - SNS/SQS\nTableau API Integration\n\nSkills\n\nBA/BS required (major in an analytical field desired) \nA minimum 10+ years work-related experience. \nExperience in hands-on development of reporting applications for the Web in a professional environment \nThe Ethos of continuous improvement and interest in learning new things.\nStrong analytical thinking and structured problem-solving ability\nAbility to handle multiple projects and assignments simultaneously and effectively in a cross-functional team environment. \nVersed on the agile methodology and best practices. \nExcellent interpersonal and collaboration skill with the ability to work with a diverse set of colleagues, across functions, from different organizations, disciplines, etc. \nSelf-starter, ability to set priorities, work independently and attain goals"}, {"title": "Data Engineer", "company_name": "Team Remotely Inc", "location": "Houston, TX (Remote)", "link": "/jobs/view/3786249829/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=78lY5Jo%2FfyRPC6TQYCdvww%3D%3D&trk=flagship3_search_srp_jobs", "id": "3786249829", "description": "About the job\nThis is a remote position.\n\n Data Engineer (US/Canada Residents Only, 1 year experience, remote)\n\nTeam Remotely Inc. is a staffing and recruitment agency that offers a comprehensive solution for talent acquisition, including sourcing, vetting, pay rolling, and managing talent. Whether you need contract staffing, direct hire, direct sourcing, talent pools, or diversity initiatives, our model can support your hiring strategy.\n\n Hiring Type: Full-Time\n\n Base Salary: $55K-$70K Per Annum.\n\nResponsibilities\n\nDesigning user interface changes for web-based DB applications. \nReviewing application requirements and interface designs. \nDeveloping and implementing highly responsive user interface components using react concepts. \nWriting application interface codes using JavaScript following react.js workflows. \nTroubleshooting interface software and debugging application codes. \nDeveloping and implementing front-end architecture to support user interface concepts. \nMonitoring and improving front-end performance. \n\nQualifications And Experience\n\nBachelor\u2019s degree in computer science, information technology, or a similar field. \nPrevious experience working as a react.js developer. \nIn-depth knowledge of JavaScript, CSS, HTML, jQuery, and front-end languages. \nKnowledge of REACT tools including React.js, Webpack, Enzyme, Redux, and Flux. \nExperience with user interface design. \nKnowledge of performance testing frameworks, including Mocha and Jest. \nExperience with browser-based debugging and performance testing software. \nTroubleshooting skills. \nProject management skills. \nProblem-solving skills. \nVerbal communication skills. \n\n Why work with Team Remotely?\n\nTeam Remotely Inc. is a staffing platform offering a seamless experience for employers and candidates. Employers can post job openings and specify their requirements, while candidates can create profiles and upload resumes.\n\nThe team of Team Remotely continuously learns and adapts based on previous successful placements, constantly improving its matching capabilities. This ensures that the recommendations provided by Team Remotely are tailored and accurate, increasing the likelihood of a successful match between employers and candidates. By providing intelligent and data-driven solutions, they strive to enhance the efficiency and effectiveness of the hiring process, ultimately helping companies find the best talent and individuals find their dream jobs."}, {"title": "Data Engineer", "company_name": "ZAG Technical Services", "location": "United States (Remote)", "link": "/jobs/view/3771403464/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=Hf0VefrKXBFIL5MtHmY0Pw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3771403464", "description": "About the job\nWe are looking for a tech savvy Data Engineer to contribute to the building, operating, and scaling of next generation data platforms and tools that will power data-driven analytics capabilities throughout the ZAG products and services portfolio, spanning areas such as business intelligence, reporting, and data analytics.\n\nThe ideal team member is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. Must be comfortable supporting the data needs of multiple clients and teams, systems, and products within the modernization of ecosystems. The right-fit person will be excited by the prospect of optimizing or even re-designing data processes to support next generation of advanced analytics capabilities, which in-turn translates to implementing strategic enhancements to maintain competitive advantage in the industry.\n\nThe successful team member will leverage their proficiency to\u2026\n\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\nBuild the analytics processes required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Data Management and cloud based \u2018big data\u2019 technologies.\nBuild analytics tools that utilize technologies to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.\nCollaborate with client teams to identify and solution regional and local requirements within the context of the standard global business model for enterprise analytics.\nParticipates in meetings, discussions, strategy sessions where changes, improvements, and enhancements are proposed, evaluated, and approved. \n\n\nThe exceptional team member will have or demonstrate progressive experiences in\u2026 \n\nBachelor\u2019s degree or higher in Computer Science or related field\nExperience working with BI and data warehousing tools\nExperience working with enterprise data, where security is paramount and data governance is critical\nAbility to work independently and in a team environment, and effectively engage all levels of the organization\nUnderstanding of distributed systems driving large-scale data processing and analytics with a successful history of manipulating, processing, and extracting value from large, disconnected datasets.\nAbility to communicate effectively (listening, presenting, and questioning)\nStrong organizational, written, and communication skills\n\n\nAbout ZAG\n\nZAG Technical Services empowers our clients\u2019 success through intelligent business solutions that increase employee productivity, network security and team collaboration. We aim to ensure that our client\u2019s IT investments drive ROI, provide a competitive advantage and help businesses grow. We do this by living our core set of values: Integrity Foremost, Client-Centered, Accountable Always, Teamwork Throughout, and Exceptional is the Goal.\n\nWhat ZAG Has To Offer\n\nCompetitive compensation package \nBenefits including medical, dental, vision, 401K and life insurance \n\n\nZAG Technical Services is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all of our employees.\n\nZAG Technical Services is an E-Verify participant and will provide the Social Security Administration (SSA) and, if necessary, the Department of Homeland Security (DHS), with information from each new employee\u2019s Form I-9 to confirm work authorization."}, {"title": "Junior Data Engineer", "company_name": "HireMeFast LLC - Career Accelerator - Land A Job", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3786249797/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D&trk=flagship3_search_srp_jobs&lipi=urn%3Ali%3Apage%3Ad_flagship3_search_srp_jobs%3BQExogrHBQoSF9zZm%2B9kufw%3D%3D&lici=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D", "id": "3786249797", "description": "About the job\nThis is a remote position.\n\n Job Title: Junior Data Engineer\n\n Employment Type: Full-Time\n\n Salary: $64,000 - $76,000 per annum\n\n Experience Required:  Minimum 1 year of project experience\n\nAbout us: HireMeFast is a leading staffing and recruitment agency specializing in connecting businesses with top-tier talent across various industries. Our mission is to bridge the gap between exceptional candidates and organizations needing their skills, expertise, and unique qualities. Our team of experienced and dedicated recruitment specialists utilizes innovative sourcing strategies, and a vast network to identify and attract top talent. We conduct comprehensive procedures to ensure that only the most qualified candidates are presented to our clients.\n\nPosition Summary\n\nJoin the fast-paced, innovative, and collaborative environment focused on providing an AIOps platform that enhances the intelligence of the CVS Health infrastructure. Work closely with subject matter experts and colleagues to build and scale out machine learning and AI solutions that will detect, predict, and recommend solutions to correct issues before system impact and enhance efficiency, reliability, and performance of CVS Health\u2019s IT operations.\n\nKey Responsibilities include:\n\nData pipeline development: Designed, implemented, and managed data pipelines for extracting, transforming, and loading data from various sources into data lakes for processing, analytics, and correlation. \nData modeling: Create and maintain data models ensuring data quality, scalability, and efficiency \nDevelop and automate processes to clean, transform, and prepare data for analytics, ensuring data accuracy and consistency \nData Integration: Integrate data from disparate sources, both structured and unstructured to provide a unified view of key infrastructure platform and application data \nUtilize big data technologies such as Kafka to process and analyze large volumes of data efficiently \nImplement data security measures to protect sensitive information and ensure compliance with data and privacy regulation \nCreate/maintain documentation for data processes, data flows, and system configurations \nPerformance Optimization- Monitor and optimize data pipelines and systems for performance, scalability and cost-effectiveness \n\nCharacteristics of this role:\n\nTeam Player: Willing to teach, share knowledge, and work with others to make the team successful. \nCommunication: Exceptional verbal, written, organizational, presentation, and communication skills. \nCreativity: Ability to take written and verbal requirements and come up with other innovative ideas. \nAttention to detail: Systematically and accurately research future solutions and current problems. \nStrong work ethic: The innate drive to do work extremely well. \nPassion: A drive to deliver better products and services than expected to customers. \n\nRequired Qualifications\n\n2+ years of programming experience in languages such as Python, Java, SQL \n2+ years of experience with ETL tools and database management (relational, non-relational) \n2+ years of experience in data modeling techniques and tools to design efficient scalable data structures \nSkills in data quality assessment, data cleansing, and data validation \n\nQualifications\n\nKnowledge of big data technologies and cloud platforms \nExperience with technologies like PySpark, Databricks, and Azure Synapse. \n\nEducation\n\nBachelor\u2019s degree in Computer Science, Information Technology, or related field, or equivalent working experience\n\nWhy HireMeFast LLC?\n\nAt HireMeFast, we understand that finding the right individuals to join your team is crucial for success and growth of your organization. We are committed to streamlining the hiring process for our clients, ensuring they have access to a diverse pool of highly qualified candidates who are a perfect fit for their specific needs."}, {"title": "Data Engineer  - Need candidates from Texas ( REMOTE )", "company_name": "Pulivarthi Group (PG)", "location": "United States (Remote)", "link": "/jobs/view/3770685212/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=9gdMwuif%2BpB%2BCQYvQi3imQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3770685212", "description": "About the job\nFollow us on Linkedin: https://www.linkedin.com/company/pulivarthigroup/\n\nPulivarthi Group LLC is a Global Staffing & IT Technology Solutions company, with our prime focus of providing world class solutions to our customers with the right talent. We combine the expertise of our team and the culture of your company to help you with the solution that is affordable and innovative using high quality standards and technologies.\n\nWe\u2019ve served some of the largest healthcare, financial services, and government entities in the U.S.\n\nData Engineer - FT only - Azure SQL with data lakes experience\n\nData Engineer -\n\nthe resource requirement for this Data Engineer role needed more clarification.\n\nOur customer is requesting a resource who knows Azure CI/CD (Continuous Integration / Continuous Deployment) to build and integrate their DevOps pipeline.\n\nOnce the DevOps pipelines are mapped, they need the Data Engineer to connect to all the Data Sources, primarily their Data Lake.\n\nThe Customer (based in Houston) is interested in a resource in a time zone not too far removed from the Central Time Zone to allow real time collaboration."}, {"title": "Data Engineer", "company_name": "Otter Products", "location": "Fort Collins, CO (Remote)", "link": "/jobs/view/3766728302/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=77U3U7CAi1PxgDmpI3%2BiNQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3766728302", "description": "About the job\nOverview \n\nOtter Products is currently recruiting for a Data Engineer to join our Data team! You can be based in our Fort Collins, CO office with flexibility to work a portion of your time remotely, or 100% Remote in the U.S.\n\nThe Data Engineer applies professional experience, concepts, and company objectives toward building and operationalizing the minimally inclusive data necessary for the enterprise data initiatives following industry standard practices and tools. The bulk of the Data Engineer's work would be in designing, managing and optimizing data pipelines, then moving these data pipelines effectively into production for key data and analytics consumers like business/data analysts, data scientists or other stakeholder that needs curated data for data and analytics use cases across the enterprise. The Data Engineer also ensures compliance with data governance and data security requirements while creating, improving and operationalizing these integrated and reusable data pipelines to enable faster data access, integrated data reuse and vastly improved time-to-solution for data and analytics initiatives. Additionally, the Data Engineer will be expected to collaborate with other data team members and data consumers working on the models and algorithms developed by them toward optimization for data quality, security, and governance to put them into production leading to potentially large productivity gains.\n\n About Otter Products \n\nOtter Products, we grow to give. From our founder's garage in 1998 to the global technology leader we are today, Otter Products continues to drive growth through innovation.\n\nThrough our industry-leading brands - OtterBox, Liviri and OtterCares - we provide our partners the number-one selling and most trusted products in our categories. Our philanthropic spirit is the foundation on which we foster our partner relationships, allowing us to grow and to give - together.\n\nBy way of our charitable arm, the OtterCares Foundation, we support our communities and invest in our future through education that inspires kids to change the world.\n\nAnd even as our global community of Otters continues to grow, our founder's core values are still at the heart of everything we do. We measure our success by our ability to give back to our communities and strengthen opportunities for all.\n\nFor more information visit otterproducts.com\n\n Responsibilities \n\nComplete analysis, design and development of BI solutions using Azure SQL Server\nFamiliarity with Data Warehouse concepts\nExperience coding the extraction, transformation, and loading (ETL) processes\nDatabase development primarily in SSIS, Data Factory and SQL\nPartner with the business to determine end user database/reporting needs and requirements\nGenerate ad hoc reports using Power BI or SSRS\nCollaborate with other developers to create and implement best approach solution(s)\nTroubleshoot Azure tools, systems, and software\nReview queries for performance issues, making changes as needed\nCollaborate with team to performance-tune Azure application as necessary\nAssist with the analysis and extraction of relevant information from large amounts of historical business data to feed data science initiatives\nParticipate and support the design and documentation of processes for large scale data analyses, model development, model validation and model implementation\nSupport and maintain a positive safety culture by following all safety policies and procedures and actively contributing to a safe working environment\nOther duties as required\n\n Qualifications \n\nBachelor's degree required. Degree in Computer Science or Mathematics preferred\nMinimum of four years of experience in an IT or analytical role required. Experience in database development, report writing and/or statistics preferred.\n\n EEO \n\nOtter Products, LLC is an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, age, marital status, pregnancy, sex, sexual orientation, gender, gender identity or expression, national origin, disability, veteran status, or any other characteristic or status protected by law.\n\n For US Based Roles Only - Compensation Range Minimum \n\nUSD $90,000.00/Yr.\n\n For US Based Roles Only - Compensation Range Maximum \n\nUSD $120,000.00/Yr.\n\n Additional Total Rewards \n\nProfit Sharing Program Eligible, Benefits Eligible - Full Time- check out otterproducts.com/careers/why for more info"}, {"title": "Software Engineer, Distributed Systems", "company_name": "Hightouch", "location": "United States (Remote)", "link": "/jobs/view/3628572539/?eBP=CwEAAAGMd4QO8APODyE_-mMOPhYyQGg5ZjVbPwq_3V7DK0B2bLAf10pcedM9S2T71X5i4TiJuEwOzWBbSiTKbLuk-VD-_Y-dN5uyf8YpYkHYqEYFxySMS9zr2hRjNrxX9gXbxssh35TFK9YTNV2WeH2Ncb675JgFbJTgNWSZySFC6rrdJGmdSDTr2q5d3VoxORz3GjsDbzjeyZQCvQ07RqYw7oujWSgslycs2LMCY_-N0NEj6e0PvbaYKiZ_5IW34bGVSYLXpHwqa8oOcNttYZ90UA3E_7sDSFzABdIVlfrp5zzERrIYy5sh2MIInxHdDoKgQw8VmOn5uM4S0tXUl-i7LnN0_W0NfPBmLiVDNuveIOm4MfDNPxfe5QKPk6FUAVpvgbcoJn9yRyE&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=rph%2FT6Q6Vkp6YoggJf9cJQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3628572539", "description": "About the job\nAbout Hightouch\n\nHightouch\u2019s mission is to empower everyone to take action on their data. Through our Reverse ETL platform, business and data users can seamlessly sync data from where it resides, such as warehouses and databases, to where it is needed, including operational systems and SaaS tools. Traditionally, acting on data has required engineering time and bandwidth, and left most business users stuck with charts and reports that are unable to take automated action on their data. With Hightouch, every business user, without writing any code, can activate data to streamline critical processes, improve marketing performance, and scale operations.\n\nOur team operates with a focus on making a meaningful impact for our customers. We believe in approaching challenges with a first principles thinking mindset, moving quickly and embracing our value of efficient execution, and treating each other with compassion and kindness. We look for team members that are strong communicators, have a growth mindset, and are motivated and persistent in achieving our goals.\n\nHundreds of companies use Hightouch, including Spotify, Ramp, Retool, NBA, Plaid, and Betterment. We\u2019re based in San Francisco, are remote-friendly, and backed by leading investors such as Amplify Partners, ICONIQ Growth, Bain Capital Ventures, Y-Combinator, and Afore Capital.\n\nAbout The Role\n\nWe are looking for a distributed systems engineer to work on the systems that power the syncing engine our customers and other engineering teams rely on. This presents an exciting challenge where you can apply your expertise in distributed systems, performance optimization, and troubleshooting to push the boundaries of what is possible and meaningful improve our ability to move massive amounts of customer data. This role also provides a unique opportunity to work on a multi-cloud and multi-region infrastructure that supports a global customer base.\n\nWe believe in enabling our engineers to do their best work for our customers by giving them extremely high levels of ownership and autonomy. This comes in different forms: you will own and deliver projects from start to finish, you will work directly with customers to solve their hardest scaling problems, and you will have a lot of influence over what we work on as a team and company.\n\nSome of the problems we\u2019ll be working on include:\n\nSync Speed: Customers want to sync a lot of data to important destinations like Facebook and Snapchat, which requires us to analyze every part of our syncing process and find where we can optimize to sync data more quickly\nStreaming Syncing: We currently sync data from sources in batch only, but envision a world where we also do syncs in real-time and support streaming sources like webhooks and queues\nScalability and Reliability: As part of our rapid growth, we\u2019re always evaluating current future bottlenecks in scaling and reliability, and architecting for the next order of magnitude of growth\nPersonalization API: Our Personalization API product provides a low latency caching layer on top of data warehouses that enable customers to use their data warehouse for real-time personalization use cases with \nMulti-Region and Multi-Cloud: Supporting our multi-region and multi-cloud backend, including extending it to launch Hightouch on in new regions to support data residency requirements of our global customer base\n\nWe are looking for talented, intellectually curious, and motivated individuals who are interested in tackling the problems above. This is a senior role, but we focus on impact and potential for growth more than years of experience. The salary range for this position is $170,000 - $240,000 USD per year, which is location independent in accordance with our remote-first policy.\n\nAbout You\n\nYou are an engineer with a passion for solving hard technical problems that generate real value for customers. You\u2019re motivated by high ownership and are comfortable in a fast-paced, startup environment.\n\nYou have experience and comfort with distributed systems and high-scale systems or the ability and desire to build this experience quickly. This experience can take different forms:\n\nYou\u2019ve built syncing engines that process and move terabytes of data per day in a scalable way\nYou\u2019ve created event collection or data streaming services that power company critical systems\nYou\u2019ve owned systems that have undergone orders of magnitude of growth and helped them scale accordingly\nYou have relevant skills and are a fast learner who is excited to take on the challenges we face\n\nInterview Process\n\nOur goal with the interview process is to balance speed with giving both parties opportunities to assess whether there is a strong mutual fit. We will ask you questions, but we want you to ask us questions! Our technical interviews focus on how you design systems because we believe this is the best way for us to see how you work and for you to see how we collaborate. We don\u2019t ask you to write code to solve technical brainteasers that don\u2019t appear in your day to day job.\n\nRecruiter screen [30m]: Introductory call with our recruiting team to get to know each other and see if the role could be a good mutual fit.\nSystems design screen [45m]: Designing a data processing feature end-to-end.\nHiring manager interview [30m]: Chat with hiring manager about past experiences and future operating preferences to assess fit on company values and operating principles.\nSystems design interview [90m]: Work with the interviewer to architect a system at a conceptual level. The problem will be at a pretty high level - and have both product and customer requirements as well as technical."}, {"title": "Entry-Level Data Scientist Engineer", "company_name": "Phoenix Recruitment LLC", "location": "Texas City, TX (Remote)", "link": "/jobs/view/3788727625/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=8iwtytHsCu4wMwJ4fj3pxw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3788727625", "description": "About the job\nThis is a remote position.\n\nTitle - Entry-Level Data Scientist Engineer, 1 year of project experience\n\nEmployment Type: Full-time\n\nBase Salary: $60K-$70K\n\nPhoenix Recruitment offers a variety of recruiting services to assist both employers and employees. They specialize in marketing open positions, recruiting, and helping employers to find qualified candidates across various industries. Phoenix Recruitment has expertise in streamlining the hiring process. They can help ensure that the process is efficient, well organized, and compliant with relevant regulations.\n\nSkills and Abilities:\n\nStrong knowledge of R or Python for data analysis and modeling. \nProficiency in statistical programs such as R, SAS, MATLAB, or Python. \nFamiliarity with spreadsheets (VBA) and database applications (Access, Oracle, SQL, or equivalent technology). \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \nAbility to learn new methods quickly and work under deadlines. \nExcellent teamwork and communication skills. \nStrong analytical and problem-solving abilities. \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \n\nPreferred:\n\nKnowledge of actuarial concepts and life, health, and/or annuity products. \nExperience with statistical modeling techniques such as GLM, Decision Trees, Time Series, Regression, etc. \nFamiliarity with Microsoft DeployR. \nExposure to insurance risk analysis. \nBasic experience in computational finance, econometrics, statistics, and math. \nKnowledge of SQL and VBA. \nFamiliarity with R or Python for predictive modeling \n\nWhy Phoenix Recruitment LLC?\n\nPhoenix Recruitment often has an extensive network of employers and candidates. This network allows them to tap into a pool of qualified candidates and connect them with suitable job opportunities. They can also leverage their connections to help employers find the right talent efficiently. Outsourcing the recruitment process to a specialized agency can save your time and resources, avoid delays, reduce administrative burdens, and increase the chances of finding the right fit for your organization."}, {"title": "Data Engineer - Remote Work", "company_name": "BairesDev", "location": "United States (Remote)", "link": "/jobs/view/3783162297/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=OtoCrAAHZFomVup04fy35g%3D%3D&trk=flagship3_search_srp_jobs", "id": "3783162297", "description": "About the job\nWho We are\n\nBairesDev is proud to be the fastest-growing company in America. With people in five continents and world-class clients, we are only as strong as the multicultural teams at the heart of our business. To consistently deliver the highest quality solutions to our clients, we only hire the Top 1% of the best talents and nurture their professional growth on exciting projects.\n\nWe are looking for a Data Engineer to join our Development team and participate in different projects made up of multicultural teams distributed throughout the world.\n\nWhat You Will Do\n\n Interview other Data Scientists and Data engineers to vet their technical abilities.\n Input data from those interviews into the product and help develop a predictive, fair, and enjoyable solution.\n Work together with developers, tech leads, and solution architects to build applications.\n Improve existing structures by adding new functionalities or proposing technological updates so that as a result, the impact of your contribution is significant in the core of each business.\n Improve the user experience on scalable and high availability platforms, contributing to the key differential of each business.\n\nHere\u2019s What We Are Looking For\n\n 5+ years of experience working as a developer.\n 3+ years of experience working as a Data Scientist / Data Engineer.\n Proficient with analysis, troubleshooting, and problem-solving.\n Hands-on experience with managing data loads and data quality.\n Advanced English level.\n\nHow we do make your work (and your life) easier:\n\n 100% remote work.\n Hardware setup for you to work from home.\n Flexible hours - make your schedule.\n Paid parental leave, vacation & holidays.\n Diverse and multicultural work environment.\n An innovative environment with the structure and resources of a leading multinational.\n Excellent compensation \u2014 well above the market average.\n Here you can grow at the speed of your learning curve.\n\nOur people work remotely but with a consistent and robust culture that promotes diversity and teamwork. To continue being the leading software development company in Latin America, we want to ensure that every BairesDev member gets the best growth and professional development opportunities in a diverse, welcoming, and innovative environment.\n\nEvery BairesDev team member brings something unique to our company.\n\nWe want to hear your story. Apply now!"}, {"title": "Data Engineer (Remote)", "company_name": "Cascade Debt", "location": "United States (Remote)", "link": "/jobs/view/3755336571/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=pmUtn1j2zyPr25CNPh2RNw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3755336571", "description": "About the job\nDo you want to work in a small team where you can make a real difference in a company? Would you embrace the challenge of building a high impact platform that is used globally? Then we want to speak with you!\n\nAbout Cascade\n\nCascade is a fintech startup backed by Canadian and US investors that empowers companies to grow by democratizing access to institutional debt. We are building tools that modernize how companies raise and manage debt, lowering the barriers to entry to the $7 trillion specialty finance and alternative credit market for companies around the world. Founded just last year, we are gearing up for our next phase of technical development and are seeking a talented Data Engineer to lead the way.\n\nAbout The Role\n\nAs a Data Engineer you\u2019ll be an early member of a growing team building a pioneering platform in the debt infrastructure space. You will design, build, and launch efficient, scalable, and reliable data pipelines to move and transform data .\n\nWhat You\u2019ll Do\n\n Building databases, data lakes and data ingestion pipelines to integrate customer databases and datasets to our systems \n Ingesting financial datasets from external customers, then u pdating and maintaining accurate and complete data mappings to ensure that our product s are displaying high quality data \n M onitoring and alerting across our data pipelines in order to make sure that our data ingests are reliable and correct \n Translat ing business goals and stakeholder requirements into new data flows and deliver analytical insights ensuring that the data flows create real business value continuously \n\nSkills And Experience\n\n 2 + years experience in a data engineering or data analyst role \n E xperience with relational SQL and NoSQL databases, including PostgreSQL , MySQL, MariaDB, MongoDB, Bigtable, etc. \n E xperience working with ETL technologies , such as Databricks, Fivetran , or dbt \n Proficiency in writing SQL queries and knowledge of analytical data warehouses such as RedShift, BigQuery , and Snowflake \n Some experience with CI/CD automation such as GitHub Actions \n Developing APIs and integrating with 3rd party APIs \n Bonus: experience in fintech / SaaS / credit analysis \n\nPeople Who Thrive At Cascade Are\n\n Self-starters, who take the initiative to tackle challenges in a remote work environment \n Good communicators, who can operate without ego to discuss, learn, grow, and help others do the same \n Passionate about creating great digital experiences for users \n Problem solvers who are not satisfied with the status quo \n\nBenefits\n\nRemote: we are a remote-first company and are very flexible on hours as long as things get done \nHome-office: there\u2019s a $1000 USD home office allowance to set yourself up \nEquity: we expect you to have an owner-mentality, and have the equity plan to match \nBenefits: health, dental, vision, and more \nPerks: we offer free lunches weekly and off-site trips \nJob satisfaction: we offer autonomy, ample opportunities for mastery, and an opportunity to make a difference for companies around the world"}, {"title": "Data Engineer", "company_name": "ISC (Integrated Specialty Coverages, LLC)", "location": "Carlsbad, CA (Remote)", "link": "/jobs/view/3778293528/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ew7%2FfEuXSujs5IGabdqUyw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3778293528", "description": "About the job\nAbout Integrated Specialty Coverages\n\nIntegrated Specialty Coverages LLC (ISC) is a growth stage technology and data-driven insurance company leading innovation in the market.\n\nBacked by one of the leading private equity firms, KKR, and led by a forward-thinking management team, ISC is combining the worlds of insurance and technology to create an Insurtech powerhouse. As a leading online distributor of insurance products for a range of industries and \"Main Street USA\", we are looking for the right people to help us in our mission of achieving exponential growth. We strive to be the number one place to go for brokers and agents to source insurance. To accomplish this, we're building a digitally focused team that deeply understands the intersection between user experience, data, and AI/ML to optimize the way we engage with our customers and partners.\n\nJob Summary\n\nThe Data Engineer role will be the technical liaison between multiple groups including a data science team, the engineering team, product management, and business stakeholders. You do not need any insurance knowledge prior, however, you must quickly dive deep into the insurance world and ask questions to become a subject matter expert. You will be responsible for building a data platform to facilitate the data science team. You must be a self-starter that can build out features such as a data pipeline from scratch. There will be support from both engineering and data science for any buildout.\n\nJob Responsibilities\n\nCreate and maintain optimal data pipeline architecture. \nAssemble large, complex data sets that meet functional / non-functional business requirements. \nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. \nBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using AWS technologies, SQL, and Python. \nWork with stakeholders including the Executive, Product, Data Science, and Engineering teams to assist with data-related technical issues and support their data infrastructure needs. \nWork with data science and analytics teams to strive for greater functionality in our data systems. \n\nMinimum Qualifications:\n\nAdvanced working SQL knowledge and experience working with relational databases, strong query authoring (SQL) as well as working familiarity with a variety of databases (Snowflake, Redshift, MySQL, MSSQL, etc.)\nExperience building and optimizing data pipelines, architecture and data sets. \nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. \nBuild processes supporting data transformation, data structures, metadata, dependency, and workload management. \nA successful history of transforming, processing and extracting value from large disconnected, datasets from a variety of data sources (Flat files, Excel, databases, etc.)\nStrong analytic skills related to working with unstructured datasets. \nStrong project management and organizational skills. \nExperience supporting and working with cross-functional teams in a dynamic environment. \nAbility to mentor/guide/collaborate with other team members. \n\nWe are looking for a candidate with 3-5 years of experience in a Data Engineer role. They should also have experience using the following software/tools:\n\nExperience with relational and MPP databases, including Snowflake and MySQL. \nExperience developing software in an agile environment from the requirements stage to production. \nExperience with version control: git\nExperience with container technologies: Docker\nExperience with data pipeline and workflow management tools: Airflow, Jenkins, AWS Glue, Azkaban, Luigi, etc. \nExperience with AWS cloud services: EC2, ECS, Batch, S3, EMR, RDS, Redshift\nExperience with other cloud services: Snowflake, Airflow\nExperience with object-oriented/object function scripting languages: Python, Java, C++, etc. \nExperience with data modeling and data warehouse design\nExperience with data visualization tools (PowerBI, QuickSight)\n\nThe starting annual pay scale for this position is listed below. Actual starting pay will be based on factors such as skills, qualifications, training, and experience. In addition, the company offers comprehensive benefits including medical, dental and vision insurance, 401(k) plan with match, paid time off, and other benefits.\n\nISC's salary ranges are determined by role and level. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations and could be higher or lower based on a multitude of factors, including job-related skills, experience, and relevant education or training.\n\nNational Pay Range\n\n$100,000\u2014$110,000 USD\n\nBenefits of Working at ISC\n\nCompetitive vacation and flexible working arrangements\nComprehensive and inclusive health benefits\nA variety of professional development and mentorship opportunities\nChoice of technology whether at home or in the office\n\nISC's Ownership Behaviors\n\nDo the Right Thing *Be Relentless. Pursue Excellence. *Personal Responsibility & Own Outcomes\nTry Fast. Learn Fast. Fail Fast. Think Big. *Build Extraordinary Owners & Join Forces *Grit & Determination\n\nApplicants with disabilities may contact ISC HR department via e-mail, and other means to request and arrange accommodation. If you need assistance to accommodate a disability, you may request accommodation at any time. Please contact ISC at HR@ISCMGA.com.\n\nISC believes in creating long-term relationships by being responsive and relevant and by consistently delivering value to our community of customers. Specifically, with our employees, we focus on attracting, developing, and retaining the best talent for our business, challenging our people, demonstrating a \"can-do\" attitude, and fostering a collaborative and mutually supportive environment. ISC is an equal opportunity employer and a participant in the US federal E-Verify program (US).\n\nDiversity creates a healthier atmosphere: All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, gender, gender identity, sexual orientation, marital status, medical condition, genetic information, mental or physical disability, military or veteran status, or any other characteristic protected by local, state, or Federal law."}, {"title": "Data Engineer -Remote", "company_name": "RIT Solutions, Inc.", "location": "Denver, CO (Remote)", "link": "/jobs/view/3768007496/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=1hQH8EDaII1eJ7nQ9BACQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3768007496", "description": "About the job\nDuration: 6+ months CTH\n\nMust have a valid LinkedIn profile\n\nRequired Skills\n\n 3-6 years of experience (no more than 6 years)\n Start up environment experience\n Python, SQL, and DBT"}, {"title": "Big Data Engineer - W2", "company_name": "Megan Soft Inc", "location": "United States (Remote)", "link": "/jobs/view/3674845652/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fCAxh4N%2F4YZJ5aEIASaeeA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3674845652", "description": "About the job\nTitle: Big Data Engineer - W2\n\nLocation: Dearborn, MI (Remote)\n\nDuration: 12+ Months\n\nDirect Client: Ford Motors\n\nMust have skills: Should be able to handle GCP requirements\n\n\"Translates business requirements to build repeatable, sustainable, efficient, coded processes that can be productionized by Software Engineers and readily modified by other Data Engineer Creates POC processes in Dev/QA and works with Software Engineers to productionize and align best coding and process practices that maximize efficiency, speed, stability, system resources and capabilities Awareness of and compliance with data privacy, security, legal and contractual guidelines Incorporates data quality and privacy checks/alerts to minimize bad data being consumed by end users, models and dashboards, and to protect customer data; revises checks for new data issues Validates data products and pipelines are functioning as expected following system or application upgrades, source changes, etc. Works closely with Architects if required to align systems, tools and applications being utilized with business use case and performance requirements Provides data product support and maintenance Establishes, tracks and monitors KPIs related to specific data products and deliverable Participate in iteration cadence activities (Iteration Planning, task estimation, daily scrum, demos, etc.). Will be a part of on-call team to resolve critical production support tickets during business hours and off hours.\n\nSkills Required\n\nExperience with Big Data Technologies (Hadoop , ETL Tool, Spark. Hive etc..) Someone who understands Cloud and has experience working with Cloud Platforms (preferably GCP) Should be able to handle GCP requirements - ingesting into GCS/ Big Query, working with GCP components Hands-on in Data Flow, Cloud Storage (GCS), Big Query and GitHub. Proficient in Python and SQL.\n\nSkills Preferred\n\nManages ETL: uses programming and tools for data ingestion, configures pipelines, applies transformations and decoding, integrates and fuses data, moves and securely deliver Experience NoSQL databases\n\nAdditional Information\n\nIf the candidate is remote only, please indicate \"\"100% Remote\"\" under candidate's name on resume. Other similar schedule notations can be \"\"Local Candidate\"\" or \"\"Hybrid Candidate.\"\" ***HRA Test Required***\""}, {"title": "Remote Opportunity: Quantexa Data Engineer", "company_name": "SPAR Information Systems LLC", "location": "United States (Remote)", "link": "/jobs/view/3680073191/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ioykVCZSOQ7BK5%2FwRQHOWg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3680073191", "description": "About the job\nHello All, Hope you are doing great!! Please go through the job descriptions and let me know your interest. Role: Quantexa Data engineer  Location: Remote Duration: Long Term Contract Job Description: 1. End to End usage of Quantexa including guiding our client team on how to use from both front end and data ingestion Reason they are asking for Certification 2. Resolve Entity issues with data 3. Knowledge of Scala and Hive which requires coding experience using parquet 4. Advanced SQL knowledge requiring writing SQL queries 5. Experience having worked with GCP data lake 6. Quantexa certification required\n\nThanks & Regards,\n\nSatnam Singh\n\nDirect: 201 623 3660\n\nEmail : Satnam.singh@sparinfosys.com"}, {"title": "Remote Work - Need Staff Data Engineer - USC only", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3746283980/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=CvIuLBNO%2BPMGi4IXVRkP0A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3746283980", "description": "About the job\n100% Remote, client is in NY\n\nLook for strong job history, Python, Machine Learning, SQL, 10/10 comms, etc. \n\nMUST be an Engineer \n\nSummary\n\nWe're looking for an experienced Data Engineer to join our client's growing team of top notched IT professionals. You will work closely across multiple groups including Editorial and Audio, Marketing, Advertising, etc. to achieve their goals and objectives. The Data Engineering team builds tools throughout the stack while sharing knowledge across these departments.\n\nAs the company grows, they're looking for Data Engineer who will help solidify and expand our pipelines and maintain their data warehouse. Their business model is based on performing complex and very detailed analyses of how our products perform with their subscribers. You'll be responsible for working alongside analysts, product, marketing, and other teams to define new data collection components and measurement schemes that support each new product and feature.\n\nHow Can Add Value To Their Mission\n\nCreate and maintain data pipelines to provide insights and drive business decisions\nEstablish data warehousing strategy (ex. Kimball, Data Vault, etc.)\nMaintain data infrastructure on our AWS accounts\nCollaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.\nWrite unit/integration tests, contributes to engineering wiki, and documents work\nImplement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it\n\nWhat You'll Need To Succeed\n\n7+ years of industry experience measuring product performance and user behavior\nExperience working with a variety of data management technologies, including RedShift, Kinesis/Kafka, Glue, Spark, Postgres, Airflow, dbt, and others.\nExperience implementing BI reporting tools such as Looker\nExperience interfacing with engineers, product managers and analysts to understand data needs\nKnowledge of a variety of measurement beacons, SDKs, APIs including Google Analytics, Amplitude, Braze, Stripe, email service providers\nA commitment to building secure, resilient, fault-tolerant architectures, with clear documentation and procedures in place for support\nA focus on accuracy and detail as you build, ensuring data pipelines produce clear, predictable results\nUnderstanding of the typical metrics a subscription and advertising-supported business needs to measure success\nExperience with Client techniques as applied to behavioral segmentation or anomaly identification is a definite plus\nFamiliarity using developer tools that increase productivity and facilitate the development of resilient code (eg. Docker, CircleCI, Serverless) is a plus"}, {"title": "ENTRY LEVEL DATA ENGINEER (REMOTE)", "company_name": "SynergisticIT", "location": "Jacksonville, FL (Remote)", "link": "/jobs/view/3767587909/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=om7L3CnowiC4PLIcpZD70A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3767587909", "description": "About the job\nSYNERGISTICIT wants every candidate to know that the Job Market is Challenging and to stand out, you need to have exceptional skills and technologies and that's where we come in to make sure you get the attention which you need\n\nSynergisticIT understands the complex nature of the job market and how difficult it can be to secure a position, especially for fresh graduates. Therefore, we assist and help tech-savvies to convert their passions into professions. We go above and beyond to keep you working in your niche.\n\nAs we focus on long-term success, we provide complete career development solutions. From job search to upskilling portfolio and interview preparation, we can guide you at every step of your career.\n\nSynergisticIT spares no efforts to connect you with a large network of tech giants, including Google, Apple, PayPal, Dell, Cisco, Client, etc. Presently, we are actively looking for ENTRY LEVEL DATA ENGINEER a driven mindset. Get the right opportunity and gain experience in building web-centric solutions on Java.\n\nWho Should Apply : Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or anyone looking to make their career in IT Industry\n\nWe also assist in filing for STEM extension and H1b and Green card filing.\n\nCandidates who are serious about their future in the IT Industry and have set big goals for themselves.\n\nCandidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. We also offer Skill Enhancement Programs if the candidates are missing skills or experience which our clients need with great outcomes\n\nCandidates can benefit from skill enhancement if they fall into the below categories. If they are qualified with enough skills then no need for skill enhancement\n\nCandidates Who Lack Experience\n\nHave had a break in careers\n\nLack Technical Competency\n\ncandidates who want to get employed and make a career in the Tech Industry \n\nPlease Also Check The Below Links\n\nhttps://www.synergisticit.com/candidate-outcomes/\n\nhttps://www.synergisticit.com/java-track/\n\nhttps://www.synergisticit.com/data-science-track/\n\nhttps://www.synergisticit.com/which-is-the-best-option-for-tech-job-seekers-staffing-companies-consulting-companies-bootcamps-or-synergisticit/\n\nhttps://www.synergisticit.com/contact-us/\n\nIf the skills are not a match candidates can opt for Skill enhancement. Or their resume can be sent out to clients to see if responses are achievable\n\nREQUIRED SKILLS For Java/Software Programmers\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Core Java , javascript , C++ or software programming\nSpring boot, Microservices and REST API's experience\nExcellent written and verbal communication skills\n\nFor data Science/Machine learning\n\nRequired Skills\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Statistics, Python, data visualization tools\nExcellent written and verbal communication skills\n\nPreferred skills: NLP, Text mining, Tableau, Time series analysis\n\nTechnical skills are required by clients for selection even if its Junior or entry level position each additional Technical skill helps a candidate's resume to be picked by clients over other job seekers.\n\nClients hire candidates with the right technical skills which they need and reject candidates who lack the required technical skills.\n\nNo third party candidates or c2c candidates\n\nPlease apply to the posting\n\nNo phone calls please. Shortlisted candidates would be reached out"}, {"title": "Data Engineer - Remote | WFH", "company_name": "Get It Recruit - Information Technology", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3782241984/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=BgBf1kSAAX%2BSuw6inJsIVw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3782241984", "description": "About the job\nJoin an innovative team at a leading company in the engineering and manufacturing industry, where we are committed to leveraging data for continuous improvement and operational excellence. This hybrid position allows you to enjoy the flexibility of remote work on Mondays and Fridays.\n\nWhy Join Us\n\nAt our company, we prioritize your well-being and professional growth. We offer a comprehensive benefits package, including excellent health benefits, a 6% 401(k) match, generous paid vacation and sick time, 12 paid holidays, and more.\n\nRole Overview\n\nAs a Data Engineer, you will play a pivotal role in transforming raw data into actionable insights, driving improvements in engineering and manufacturing processes, enhancing product quality, and optimizing operational efficiency. Collaborate with dynamic cross-functional teams to extract, analyze, and visualize data, influencing strategic decisions through your valuable insights.\n\nKey Responsibilities\n\nData Collection and Integration:\n\nGather, clean, and integrate data from diverse sources, including production databases, sensor data, and external datasets.\n\nData Analysis\n\nConduct exploratory data analysis to identify patterns, trends, and anomalies.\n\nPerform statistical analysis and hypothesis testing to derive meaningful insights.\n\nData Visualization\n\nCreate informative and visually appealing dashboards and reports using tools like Tableau, Power BI, or Python libraries.\n\nPredictive Modeling\n\nDevelop predictive models to forecast manufacturing performance, quality, and operational outcomes.\n\nRoot Cause Analysis\n\nInvestigate production issues and quality concerns by analyzing data to identify root causes and recommend corrective actions.\n\nPerformance Monitoring\n\nEstablish and maintain key performance indicators (KPIs) to track manufacturing and engineering metrics.\n\nGenerate regular reports to communicate performance trends to stakeholders.\n\nCross-functional Collaboration\n\nCollaborate with engineering, manufacturing, and quality assurance teams to understand their data needs and provide analytical support.\n\nContinuous Improvement\n\nSuggest process improvements and optimizations based on data analysis.\n\nData Security And Compliance\n\nEnsure data security and compliance with relevant regulations, such as GDPR or industry-specific standards.\n\nInfrastructure And Development\n\nBuild and maintain the infrastructure for collecting, storing, and processing data.\n\nDesign, develop, and manage data pipelines, ETL processes, and data warehouses.\n\nRequired Experience\n\nProven experience as a Sr. Data Analyst and/or Data Engineer in an engineering or manufacturing environment.\n\nProficiency in data analysis tools and programming languages, such as Python, R, SQL, and Excel.\n\nStrong data visualization skills with tools like Tableau or Power BI.\n\nExcellent knowledge of statistical analysis and machine learning techniques.\n\nAbility to communicate complex findings and insights effectively to both technical and non-technical stakeholders.\n\nStrong problem-solving skills and attention to detail.\n\nFamiliarity with manufacturing processes and quality control is a plus.\n\nExperience with enterprise ERPs (SAP S4HANA, Oracle EBS), CRMs (Salesforce, Hubspot), and IoT.\n\nProficient in working with cloud computing platforms including AWS, GCP, and Azure.\n\nExpertise in configuring and optimizing cloud-based data solutions for scalability, reliability, and performance.\n\nBachelor's Degree in Data Science, Statistics, Engineering, or Computer Science Required.\n\nApply Now and be part of a team shaping the future of data-driven innovation in engineering and manufacturing!\n\nEmployment Type: Full-Time"}, {"title": "Entry Level Software Engineer - Data Backend Engineer (Remote - Canada)", "company_name": "Yelp", "location": "United States (Remote)", "link": "/jobs/view/3752881757/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fankeoxifvhpTzOYvowHYQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3752881757", "description": "About the job\nYelp engineering culture is driven by our values: we\u2019re a cooperative team that values individual authenticity and encourages creative solutions to problems. All new engineers deploy working code their first week, and we strive to broaden individual impact with support from managers, mentors, and teams. At the end of the day, we\u2019re all about helping our users, growing as engineers, and having fun in a collaborative environment.\n Interested in exploring data and finding innovative ways to collect it, curate it, and make it easily accessible for others to use? How about doing that at an enormous scale? Yelp\u2019s datasets contain billions of interactions between users and local businesses around the globe. If working with our vast amount of data sounds exciting, come and join us!\n\nThe Marketplace Data Observability team is responsible for managing, governing, and monitoring all of Yelp\u2019s Ads data to protect our revenue stream and also support offline analysis that aids in making long term company decisions. We build tools that help us to better understand our data quality, and we strive to uncover data issues before they impact our consumers.\n\nAs a Data Backend Engineer on this team, you will be responsible for building elegant and scalable data products that serve critical, up-to-date, structured information to support different types of analytics within Yelp. As a core contributor to our growing data modeling and data warehousing engineering efforts, you will help design and own mission-critical data flow pipelines and datastores to enable decisions including effective A/B testing and company investments.\n This opportunity is fully remote and does not require you to be located in any particular area in Canada. We welcome applicants from throughout Canada. We\u2019d love to have you apply, even if you don\u2019t feel you meet every single requirement in this posting. At Yelp, we\u2019re looking for great people, not just those who simply check off all the boxes.\n  Build systems that can effectively store and crunch terabytes of data.\nDesign and develop data models for efficient data storage, retrieval, and reporting.\nCreate and maintain conceptual, logical, and physical data models using industry-standard modeling tools.\nCollaborate with cross-functional teams, including engineers, data analysts, business analysts, and data scientists, to understand data requirements and translate them into effective data models.\nParticipate in data integration efforts, including ETL processes and data migration.\nStay up-to-date with industry best practices, emerging technologies, and trends related to data warehousing.\nSupport on-call rotations as needed to operate the team.\n  Understanding of high performing and scalable data systems.\nExperience in building and orchestrating ETL pipelines.\nExperience with Data Lake or Data Warehouse landscape.\nA hunger for tracking down root causes and fixing them in systematic ways.\nAbility to communicate effectively to technical and non-technical cohorts alike. \nExposure to some of the following technologies: Python, AWS Redshift, AWS Athena / Apache Presto, Big Data technologies (e.g S3, Hadoop, Hive, Spark, Flink, Kafka etc), NoSQL systems like Cassandra, DBT."}, {"title": "Data Engineer (Associate Consultant), June 2024", "company_name": "UDig", "location": "Nashville, TN (Remote)", "link": "/jobs/view/3716039960/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=XUjo40kU55HnDCqJ%2F3FdPg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3716039960", "description": "About the job\nCan UDig It? \n\nUDig designs, builds, and implements technology solutions that deliver on business objectives. People join our team because they are excited to have an impact on the future of our organization, passionate about delivering meaningful results, and energized by solving complex problems. Oh, and they like working with other driven, smart, and authentic people!\n\nUDig is proud to be recognized as a \"Best Places to Work\" in Virginia and Tennessee. We're committed to fostering a transparent environment where leadership is accessible, professional development opportunities are abundant, and you are empowered to achieve your career goals. You'll collaborate, develop deep relationships with teammates, and accelerate your success while having fun!\n\nIf you're graduating in May 2024 and excited by the idea of impacting organizations by building data solutions to achieve business goals, you should consider joining our growing technology team.\n\nHow Things Go Down\n\nAs a new, entry-level teammate, you'll take part in our \"Breaking Ground\" training program designed to help you quickly get up-to-speed on UDig's delivery approach within a project team. The program covers:\n\nUDig delivery process and methodologies such as gathering requirements and delivering projects using agile\nTechnical guidance and instruction on the best practices from Data leaders at UDig\nDelivering executive and technical presentations\nCoaching to prepare you to for success on your first client project at UDig\n\nA typical day might entail:\n\nProject standups and check-ins\nDesign and development of data pipelines, BI reports, and more\nPlanning and participating in project demos\nContributing to internal UDig projects\nCareer and job growth through UDig-provided training\nCreating long-lasting friendships with teammates through fun activities as well as your projects (including social events, winning a prize for guessing the weight of pumpkin, or sharing experience on cool tech side projects)\n\nUDig believes it is important for Associate Consultants going through our Breaking Ground program to reside in one of our major markets (Richmond, VA or Nashville, TN metro area) for at least the first year of employment.\n\nWant to learn more? Hear directly from our teammates by checking out our 'A Day in the Life' blog series: https://www.udig.com/digging-in/breaking-ground-2023/\n\nSound Interesting? Here's the Technical Piece\n\nBelow are a few boxes you should be able to check if\u202fyou're interested: Don't check every box? Go ahead and apply!\n\n1+ year of experience with relational database management systems such as Microsoft SQL Server, MySQL, or Oracle\nUnderstanding of the full lifecycle of design, development, and implementation of data solutions including architecture design, development, testing\nSome experience in ETL development or reporting is a plus\nAbility to present technical ideas and high-level concepts and solutions to internal and external team members with varying degrees of technical knowledge\nBachelor's Degree in in computer science, information technology, or any science or business discipline\nAbility to take your work seriously but also seriously enjoy a good time with your UDig colleagues\n\nWhat's in It For You? \n\nAt UDig, you will build your career alongside talented and experienced developers while gaining experience and having fun! We provide continuous challenges and professional development opportunities. Additionally, we offer: \n\nCompetitive salary, merit reviews, and career advancement paths\nFlexible, hybrid environment\nIndividual $1500 Training Budget\nRegular team building and social activities (virtual and/or in-person)\nTransparent culture with strong communication and access to leadership\nGreat benefits like\nGenerous Paid Time Off, company holidays, and parental leave\nMultiple Single and Family Health Insurance plans to choose from\nDental & Vision coverage\nShort-Term and Long-Term disability\nOptional accident and critical illness coverage\nMatching 401(k)\nand more!"}, {"title": "Expression of Interest: Data Engineer", "company_name": "Fingerprint for Success (F4S)", "location": "New York, NY (Remote)", "link": "/jobs/view/3787769880/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=l%2FEDpQqUNmCttGdIyTk7Cg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787769880", "description": "About the job\nWe are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.\n\nThe F4S Talent Pool is a pilot project designed to:\n\nHelp job seekers get discovered by our partners based on their anticipated hiring needs.\nProvide optional support and resources for job seekers in their career endeavors.\nHelp individuals understand, and bring out the best in themselves and each other.\n\nThe F4S Talent Pool process:\n\nOnce you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.\n\nAbout Fingerprint For Success (F4S)\n\nBacked by 20+ years of research, F4S\u2019s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high performing teams.\n\nKeep in mind, joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your F4S work style assessment to match the hiring needs of our partners.\n\nYour feedback is a gift! Write to us via:\n\nPowered by JazzHR\n\noWuPJ9lwq9"}, {"title": "Data Engineer Seattle, WA (remote)", "company_name": "Conch Technologies, Inc", "location": "United States (Remote)", "link": "/jobs/view/3765052211/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=SBETI78UMutCHuRp5FnP%2FA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3765052211", "description": "About the job\nHi,\n\nGreetings from Conch Technologies Inc\n\nData Engineering Lead \u2013 With Databricks and Unity Catalog\n\nLocation \u2013 Seattle, WA (remote)\n\nDuration \u2013 12 months\n\nNeed: Design and implement data architecture and strategy, leveraging technologies such as Databricks, Unity Catalog, Privacera, and Collibra.\n\nAs a Data Engineering Lead, you will play a pivotal role in defining and executing technical deliverables while assessing complexities and levels of effort. You'll engage with both vendor and internal teams to resolve technical challenges and ensure seamless collaboration. Your leadership will extend to planning and prioritization with the SCRUM master, facilitating effective coordination between onsite and offshore teams, and driving feature prioritization through collaboration with product managers.\n\nKey Responsibilities\n\n Lead the definition and breakdown of technical deliverables, assessing complexities and levels of effort.\n Collaborate with vendor and internal teams to address technical issues.\n Drive planning and prioritization in conjunction with the SCRUM master.\n Coordinate with onsite and offshore teams for timely project delivery.\n Collaborate with product managers to prioritize features, communicate with stakeholders, and clarify requirements.\n Collect and manage user-reported issues, ensuring prompt resolution and communication.\n Define comprehensive test plans, oversee their execution, and document results.\n\n--\n\nWith Regards,\n\nNagesh G\n\nMobile: 408-381-5645\n\nDesk: 901-313-3066\n\nEmail: nagesh@conchtech.com \n\nWeb: www.conchtech.com"}, {"title": "Remote Work - Need Azure Fabric Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3759325599/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=C3iVjmcR9wjUip1nNjUiCg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3759325599", "description": "About the job\nThe client is placing heavy emphasis on Microsoft and Azure Fabric experience\n\nInformation Management Engineer \u2013 Take the identified information management sources and assist migration to the Client Data Lake, Azure Fabric. The engineer will transform information management (de-dup, clean and enrich).\n\nTake the identified data sources and bring them into our Data Lake, Azure Fabric. Once the data is in, the engineer will need to be able to transform data (de-dup, clean and enrich the data). We are focused on using a medallion data model for data architecture. The tools we are working with are: Delta Parquet files, python, spark, notebooks, T-SQL, Fabric lakehouse/warehouse, Dataflows Gen2, Data Pipelines, Direct Lake, shortcuts, ADF."}, {"title": "Data Engineer", "company_name": "Catalist", "location": "Washington, DC (Remote)", "link": "/jobs/view/3744951279/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ui1Fk6ICmnPNqAlZDZAcwA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3744951279", "description": "About the job\nFor over 17 years, Catalist has been a leader in civic data and data science innovation. Our mission is to provide progressive organizations with the data, software, and services needed to better identify, understand, and communicate with the people they need to engage and mobilize. Our clients include the largest, most influential organizations in the U.S. active in civic engagement, advocacy, and political campaigns.\n\nCatalist is home to a dedicated, creative team of technologists, data scientists, and campaign experts committed to using our talents and technology to nurture a vibrant and growing progressive community.\n\nAs a Data Engineer at Catalist, you will work closely with the Analytics and Technology departments to design, build, support, and maintain various data pipelines and processes with an end goal of providing data and intelligence to the progressive community. This role primarily involves translating data architecture designs into functional processes, code, and systems.\n\nThe ideal candidate will be a highly motivated individual with excellent technical skills, a strong desire to learn new skills, and an interest in progressive politics. Catalist values creativity and problem-solving. Our work is on the cutting edge of data-driven politics, and your support will help Democratic candidates and progressive organizations conduct successful advocacy and electoral campaigns.\n\nThis position reports to the Deputy Chief Data Officer. The Data Engineer is a part of a growing Data team that supports all underlying work at Catalist.\n\nThis position is included in our CWA bargaining unit.\n\nPrinciple Duties & Responsibilities\n\nDevelop scalable, production processes, code, and systems for data ingest, transformation, modeling, and reporting across a variety of platforms\nTranslate mock-ups and designs into functional processes, code, and systems\nCreate architecture designs when needed\nProvide quality assurance and testing on processes, code, systems, and products\nBecome an internal subject matter expert on various datasets and support other Catalist departments/teams on usage of those datasets\nExecute ad-hoc data and database maintenance tasks as requiredProject manage cross-departmental efforts, with direct responsibility for stakeholder engagement, management, and execution of technical elements\n\n\nRequirements\n\nBS or BA in a technical field, or relevant experience\n1-2 years of experience working with SQL databases\nExperience with data cleaning or ETL processes\nExperience with distributed computing systems and/or distributed datastores (particularly the Hadoop ecosystem)\nExperience managing projects\nFamiliarity with Catalist data, progressive politics, voter files, and/or commercial data\nBackground check required\n\n\nPreferred Skills & Abilities\n\nWillingness to be a problem solver and produce results in a fast paced environment\nAbility to be creative and personable, and articulate ideas clearly\nExperience working with SQL databases\nProficiency with Python or another object-oriented programming language (R, Java, Scala, etc\u2026)\nExperience working in cloud environments (AWS, GCP, etc.)\nExperience working in command line environments such as Bash\nExperience with a version control tool such as git or github\n\n\nBenefits\n\nMedical, Dental, Vision, Prescription Drug\n\nCatalist offers Medical, Dental, Vision, and Prescription Drug coverage for eligible staff and their eligible dependents. Catalist\u2019s Medical plan is a comprehensive PPO program including Prescription Drug coverage with 85% of the premium paid by Catalist. Dental and Vision coverage is provided at no cost to employees.\n\nGroup Term Life Insurance and Long-Term & Short-Term Disability Coverage\n\nGroup Term Life Insurance and Long-Term and Short-Term Disability coverage is available for eligible staff. These benefits are provided at no cost to Catalist employees.\n\n401(k) Safe Harbor Plan\n\nA 401(k) Safe Harbor Plan is available to eligible staff with a 3% contribution from Catalist from the date of hire. Employees may contribute pre-tax or post-tax from their salary up to the legal limits set forth by the IRS.\n\nMedical and Dependent Care Flexible Spending Accounts (FSAs)\n\nCatalist offers an FSA Program that gives eligible staff the ability to pay out-of-pocket medical/dental/vision/child care expenses from pre-tax earnings.\n\nTransit Benefits\n\nCatalist also makes available a Transit benefit FSA program to eligible employees using pre-tax contributions with a company match.\n\nProfessional Development and Remote Work Expenses\n\nEligible employees may be reimbursed up to $750 each year for professional development / education and remote work expenses.\n\nStudent Loan PayDown or SaveUp\n\nCatalist offers a Student Loan PayDown and College SaveUp benefit for eligible staff.\n\nVacation, Personal Leave, Sick Leave Benefits\n\nCatalist offers generous vacation benefits to all eligible staff. Eligible employees also receive:\n\n 14 Paid Holidays\n Personal Days\n Sick Leave\n Parental Leave\n\n\nHybrid Office/Remote Work\n\nCertain positions at Catalist are eligible for Office/Remote Hybrid or full Remote status."}, {"title": "Remote Entry Level Data Analyst/Engineer", "company_name": "SynergisticIT", "location": "White Plains, NY (Remote)", "link": "/jobs/view/3774171423/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=q54sk1%2FRww0z%2BC%2BE8e0jQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774171423", "description": "About the job\nAt SynergisticIT, we're all about making connections. Whatever IT goals you have, our software programmers can help achieve those. Our Software development teams can take up turnkey projects and execute them in an effective and efficient manner. If you are looking to source talent our recruiters will find the ideal IT talent for your company. What's the secret to our success? Well, it all starts with taking quality time to listen to each client's specific needs. After we have a thorough grasp of your IT goals, we can better customize our Developments as per your specific needs. We can also tailor make recruiting programs to exceed your expectations. Since our founding in 2010, SynergisticIT's strategies have earned the company an enviable position in the software development, IT staffing and IT skill enhancement fields. SynergisticIT continues to work with hundreds of satisfied American clients with our software programmers working on our projects and after gaining hands on experience on cutting edge technologies moving to contribute their skills to great clients like Apple, Google, Client, Ebay, Paypal, Kroger, the Walt Disney Company and hundreds more. If you are tired of working with inefficient programmers who take a lot of time to ramp up we want you to try us. Our software programmers can hit the ground running and get you the maximum return on your investment. You have already tried the rest its time you tried the best. SynergisticIT - Home of the Best Data Scientists and Software Programmers in the Bay Area.\n\n Why Us ? \n\nSynergisticIT has a proven track record of successfully skill enhancement and staffing IT employees for some of the world's most iconic brands. Our team takes the time to fully understand every client's needs so we could best meet your IT staffing requirements. The knowledgeable staff at SynergisticIT is always more than happy to work with clients to ensure they reach their software development goals. Besides staffing, SynergisticIT is also committed to helping young IT professionals advance their career with a robust upskill program . Everyone who goes through SynergisticIT's program learns all the skills necessary to succeed in many IT fields ranging from Java to Machine Learning. Additionally, everyone trained at SynergisticIT has been through extensive mock and technical interview screenings to bolster their career prospects. Last, but certainly not least, SynergisticIT takes great care to respect the privacy considerations for every client. All companies who work with SynergisticIT can rest assured their confidential data is protected using the most up-to-date encryption technologies. SynergisticIT also complies with all the latest NDA agreements.\n\n REQUIRED SKILLS For Java /Software Programmers \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Time series analysis \n\n We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2022 and at Gartner Data Analytics Summit (Florida)-2023 \n\nOracle CloudWorld Event (OCW) Las Vegas 2022 | SynergisticIT - YouTube\n\n https://www.youtube.com/watch?v=OAFOhcGy9Z8 \n\n https://www.youtube.com/watch?v=EmO7NrWHkLM \n\n https://www.youtube.com/watch?v=NVBU9RYZ6UI \n\n https://www.youtube.com/watch?v=Yy74yvjatVg \n\nSynergisticIT at Gartner Data and Analytics Summit 2023 - YouTube\n\nFor preparing for interviews please visit  https://www.synergisticit.com/interview-questions/ \n\n We are looking for the right matching candidates for our clients \n\n Please apply via the job posting \n\n REQUIRED SKILLS For Java /Full Stack/Software Programmer \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning Positions \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Tensorflow \n\n If you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team. \n\n No phone calls please.  Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates"}, {"title": "Data Engineer", "company_name": "Anika Systems", "location": "Leesburg, VA (Remote)", "link": "/jobs/view/3787766893/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=mApL9JnN%2B%2Fr2%2BYUPlxRYOA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787766893", "description": "About the job\nAnika Systems is a fast growing, woman-owned small business that specializes in providing innovative IT solutions for federal government agencies. Our expertise lies in accelerating delivery in Data and Analytics, Intelligent Automation, Application Development, and IT Modernization. We are currently expanding our Federal team and are seeking a passionate and talented Data Engineer. This opportunity is 100% remote.\n\nMust be a U.S. Citizen with the ability to obtain and maintain a government suitability clearance\n\nResponsibilities\n\nDesigns, develops, builds, analyzes, evaluates, and installs database management systems to include database modeling and design, relational database architecture, metadata, and repository creation and configuration management.\nDefines and oversees database organizations, standards, controls, procedures, and documentation. Provides technical consulting in the definition, design, and creation of a database environment.\nAdvises applications development staff and users on data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nDesigns and implements databases with respect to access methods, access time, batch processes, device allocation, validation checks, organization, protection and security, documentation, and statistical methods.\nUses data mapping, data mining, and data transformational analysis tools to design and develop databases.\nDetermines data storage and optimum storage requirements.\nPrepares system requirements, source analysis, and process analyses and designs throughout the database implementation.\n\nRequired Skills And Experience\n\nBA/BS and 1 year of relevant experience\nExperience with DataBricks, SQL, Python\nApplications development experience with data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nExcellent communication skills (both oral and written) with process-oriented organizational skills to ensure project success\n\nDesired Skills And Experience\n\nDemonstrated successful development experience with the full life cycle of government database management systems including database modeling and design, relational database architecture, metadata and repository creation, and configuration management.\n\nPowered by JazzHR\n\nTlKWIorucJ"}, {"title": "Remote Work - Need Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3774712978/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=r3Bp%2F0qcOD2OpC5jpbUOcA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774712978", "description": "About the job\nTitle : Data Engineer\n\nLocation : Remote\n\nDuration : 6+ months to start (long term contract (work order are extended 2x year))\n\nPlease Fill out the skill matrix: Technology Some Awareness (1) Novice (2) Intermediate (3) Advanced (4) Expert (5) SQL Tableau Desktop /Creator Tableau API Integration AWS - S3 Glue AWS - Athena Redshift AWS - Lambda AWS - SNS/SQS msExcel\n\nProfile: Data Analyst will work closely with Architects and leads in the Interflow team, key business stakeholders including program staff to support the organizations effort in the areas of data Analysis and information management. Role should have strong experience in writing SQL queries to extract data for reports. Candidate should have experience working with large set of data. As these reports are very critical for the company and we are looking for an expert who is capable of handling, consuming and extracting large sets of data for analysis and reports.\n\nMust Haves\n\nAWS - Athena\nAWS - S3\nGlue\nhands-on development of reporting applications\nMS Excel\nRedShift\nSQL\nTableau Desktop /Creator\n\nNice\n\nAWS - Lambda\nAWS - SNS/SQS\nTableau API Integration\n\nSkills\n\nBA/BS required (major in an analytical field desired) \nA minimum 10+ years work-related experience. \nExperience in hands-on development of reporting applications for the Web in a professional environment \nThe Ethos of continuous improvement and interest in learning new things.\nStrong analytical thinking and structured problem-solving ability\nAbility to handle multiple projects and assignments simultaneously and effectively in a cross-functional team environment. \nVersed on the agile methodology and best practices. \nExcellent interpersonal and collaboration skill with the ability to work with a diverse set of colleagues, across functions, from different organizations, disciplines, etc. \nSelf-starter, ability to set priorities, work independently and attain goals"}, {"title": "Data Engineer", "company_name": "Team Remotely Inc", "location": "Houston, TX (Remote)", "link": "/jobs/view/3786249829/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=78lY5Jo%2FfyRPC6TQYCdvww%3D%3D&trk=flagship3_search_srp_jobs", "id": "3786249829", "description": "About the job\nThis is a remote position.\n\n Data Engineer (US/Canada Residents Only, 1 year experience, remote)\n\nTeam Remotely Inc. is a staffing and recruitment agency that offers a comprehensive solution for talent acquisition, including sourcing, vetting, pay rolling, and managing talent. Whether you need contract staffing, direct hire, direct sourcing, talent pools, or diversity initiatives, our model can support your hiring strategy.\n\n Hiring Type: Full-Time\n\n Base Salary: $55K-$70K Per Annum.\n\nResponsibilities\n\nDesigning user interface changes for web-based DB applications. \nReviewing application requirements and interface designs. \nDeveloping and implementing highly responsive user interface components using react concepts. \nWriting application interface codes using JavaScript following react.js workflows. \nTroubleshooting interface software and debugging application codes. \nDeveloping and implementing front-end architecture to support user interface concepts. \nMonitoring and improving front-end performance. \n\nQualifications And Experience\n\nBachelor\u2019s degree in computer science, information technology, or a similar field. \nPrevious experience working as a react.js developer. \nIn-depth knowledge of JavaScript, CSS, HTML, jQuery, and front-end languages. \nKnowledge of REACT tools including React.js, Webpack, Enzyme, Redux, and Flux. \nExperience with user interface design. \nKnowledge of performance testing frameworks, including Mocha and Jest. \nExperience with browser-based debugging and performance testing software. \nTroubleshooting skills. \nProject management skills. \nProblem-solving skills. \nVerbal communication skills. \n\n Why work with Team Remotely?\n\nTeam Remotely Inc. is a staffing platform offering a seamless experience for employers and candidates. Employers can post job openings and specify their requirements, while candidates can create profiles and upload resumes.\n\nThe team of Team Remotely continuously learns and adapts based on previous successful placements, constantly improving its matching capabilities. This ensures that the recommendations provided by Team Remotely are tailored and accurate, increasing the likelihood of a successful match between employers and candidates. By providing intelligent and data-driven solutions, they strive to enhance the efficiency and effectiveness of the hiring process, ultimately helping companies find the best talent and individuals find their dream jobs."}, {"title": "Data Engineer", "company_name": "ZAG Technical Services", "location": "United States (Remote)", "link": "/jobs/view/3771403464/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=Hf0VefrKXBFIL5MtHmY0Pw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3771403464", "description": "About the job\nWe are looking for a tech savvy Data Engineer to contribute to the building, operating, and scaling of next generation data platforms and tools that will power data-driven analytics capabilities throughout the ZAG products and services portfolio, spanning areas such as business intelligence, reporting, and data analytics.\n\nThe ideal team member is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. Must be comfortable supporting the data needs of multiple clients and teams, systems, and products within the modernization of ecosystems. The right-fit person will be excited by the prospect of optimizing or even re-designing data processes to support next generation of advanced analytics capabilities, which in-turn translates to implementing strategic enhancements to maintain competitive advantage in the industry.\n\nThe successful team member will leverage their proficiency to\u2026\n\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\nBuild the analytics processes required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Data Management and cloud based \u2018big data\u2019 technologies.\nBuild analytics tools that utilize technologies to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.\nCollaborate with client teams to identify and solution regional and local requirements within the context of the standard global business model for enterprise analytics.\nParticipates in meetings, discussions, strategy sessions where changes, improvements, and enhancements are proposed, evaluated, and approved. \n\n\nThe exceptional team member will have or demonstrate progressive experiences in\u2026 \n\nBachelor\u2019s degree or higher in Computer Science or related field\nExperience working with BI and data warehousing tools\nExperience working with enterprise data, where security is paramount and data governance is critical\nAbility to work independently and in a team environment, and effectively engage all levels of the organization\nUnderstanding of distributed systems driving large-scale data processing and analytics with a successful history of manipulating, processing, and extracting value from large, disconnected datasets.\nAbility to communicate effectively (listening, presenting, and questioning)\nStrong organizational, written, and communication skills\n\n\nAbout ZAG\n\nZAG Technical Services empowers our clients\u2019 success through intelligent business solutions that increase employee productivity, network security and team collaboration. We aim to ensure that our client\u2019s IT investments drive ROI, provide a competitive advantage and help businesses grow. We do this by living our core set of values: Integrity Foremost, Client-Centered, Accountable Always, Teamwork Throughout, and Exceptional is the Goal.\n\nWhat ZAG Has To Offer\n\nCompetitive compensation package \nBenefits including medical, dental, vision, 401K and life insurance \n\n\nZAG Technical Services is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all of our employees.\n\nZAG Technical Services is an E-Verify participant and will provide the Social Security Administration (SSA) and, if necessary, the Department of Homeland Security (DHS), with information from each new employee\u2019s Form I-9 to confirm work authorization."}, {"title": "Junior Data Engineer", "company_name": "HireMeFast LLC - Career Accelerator - Land A Job", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3786249797/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D&trk=flagship3_search_srp_jobs&lipi=urn%3Ali%3Apage%3Ad_flagship3_search_srp_jobs%3BQExogrHBQoSF9zZm%2B9kufw%3D%3D&lici=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D", "id": "3786249797", "description": "About the job\nThis is a remote position.\n\n Job Title: Junior Data Engineer\n\n Employment Type: Full-Time\n\n Salary: $64,000 - $76,000 per annum\n\n Experience Required:  Minimum 1 year of project experience\n\nAbout us: HireMeFast is a leading staffing and recruitment agency specializing in connecting businesses with top-tier talent across various industries. Our mission is to bridge the gap between exceptional candidates and organizations needing their skills, expertise, and unique qualities. Our team of experienced and dedicated recruitment specialists utilizes innovative sourcing strategies, and a vast network to identify and attract top talent. We conduct comprehensive procedures to ensure that only the most qualified candidates are presented to our clients.\n\nPosition Summary\n\nJoin the fast-paced, innovative, and collaborative environment focused on providing an AIOps platform that enhances the intelligence of the CVS Health infrastructure. Work closely with subject matter experts and colleagues to build and scale out machine learning and AI solutions that will detect, predict, and recommend solutions to correct issues before system impact and enhance efficiency, reliability, and performance of CVS Health\u2019s IT operations.\n\nKey Responsibilities include:\n\nData pipeline development: Designed, implemented, and managed data pipelines for extracting, transforming, and loading data from various sources into data lakes for processing, analytics, and correlation. \nData modeling: Create and maintain data models ensuring data quality, scalability, and efficiency \nDevelop and automate processes to clean, transform, and prepare data for analytics, ensuring data accuracy and consistency \nData Integration: Integrate data from disparate sources, both structured and unstructured to provide a unified view of key infrastructure platform and application data \nUtilize big data technologies such as Kafka to process and analyze large volumes of data efficiently \nImplement data security measures to protect sensitive information and ensure compliance with data and privacy regulation \nCreate/maintain documentation for data processes, data flows, and system configurations \nPerformance Optimization- Monitor and optimize data pipelines and systems for performance, scalability and cost-effectiveness \n\nCharacteristics of this role:\n\nTeam Player: Willing to teach, share knowledge, and work with others to make the team successful. \nCommunication: Exceptional verbal, written, organizational, presentation, and communication skills. \nCreativity: Ability to take written and verbal requirements and come up with other innovative ideas. \nAttention to detail: Systematically and accurately research future solutions and current problems. \nStrong work ethic: The innate drive to do work extremely well. \nPassion: A drive to deliver better products and services than expected to customers. \n\nRequired Qualifications\n\n2+ years of programming experience in languages such as Python, Java, SQL \n2+ years of experience with ETL tools and database management (relational, non-relational) \n2+ years of experience in data modeling techniques and tools to design efficient scalable data structures \nSkills in data quality assessment, data cleansing, and data validation \n\nQualifications\n\nKnowledge of big data technologies and cloud platforms \nExperience with technologies like PySpark, Databricks, and Azure Synapse. \n\nEducation\n\nBachelor\u2019s degree in Computer Science, Information Technology, or related field, or equivalent working experience\n\nWhy HireMeFast LLC?\n\nAt HireMeFast, we understand that finding the right individuals to join your team is crucial for success and growth of your organization. We are committed to streamlining the hiring process for our clients, ensuring they have access to a diverse pool of highly qualified candidates who are a perfect fit for their specific needs."}, {"title": "Data Engineer  - Need candidates from Texas ( REMOTE )", "company_name": "Pulivarthi Group (PG)", "location": "United States (Remote)", "link": "/jobs/view/3770685212/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=9gdMwuif%2BpB%2BCQYvQi3imQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3770685212", "description": "About the job\nFollow us on Linkedin: https://www.linkedin.com/company/pulivarthigroup/\n\nPulivarthi Group LLC is a Global Staffing & IT Technology Solutions company, with our prime focus of providing world class solutions to our customers with the right talent. We combine the expertise of our team and the culture of your company to help you with the solution that is affordable and innovative using high quality standards and technologies.\n\nWe\u2019ve served some of the largest healthcare, financial services, and government entities in the U.S.\n\nData Engineer - FT only - Azure SQL with data lakes experience\n\nData Engineer -\n\nthe resource requirement for this Data Engineer role needed more clarification.\n\nOur customer is requesting a resource who knows Azure CI/CD (Continuous Integration / Continuous Deployment) to build and integrate their DevOps pipeline.\n\nOnce the DevOps pipelines are mapped, they need the Data Engineer to connect to all the Data Sources, primarily their Data Lake.\n\nThe Customer (based in Houston) is interested in a resource in a time zone not too far removed from the Central Time Zone to allow real time collaboration."}, {"title": "Data Engineer", "company_name": "Otter Products", "location": "Fort Collins, CO (Remote)", "link": "/jobs/view/3766728302/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=77U3U7CAi1PxgDmpI3%2BiNQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3766728302", "description": "About the job\nOverview \n\nOtter Products is currently recruiting for a Data Engineer to join our Data team! You can be based in our Fort Collins, CO office with flexibility to work a portion of your time remotely, or 100% Remote in the U.S.\n\nThe Data Engineer applies professional experience, concepts, and company objectives toward building and operationalizing the minimally inclusive data necessary for the enterprise data initiatives following industry standard practices and tools. The bulk of the Data Engineer's work would be in designing, managing and optimizing data pipelines, then moving these data pipelines effectively into production for key data and analytics consumers like business/data analysts, data scientists or other stakeholder that needs curated data for data and analytics use cases across the enterprise. The Data Engineer also ensures compliance with data governance and data security requirements while creating, improving and operationalizing these integrated and reusable data pipelines to enable faster data access, integrated data reuse and vastly improved time-to-solution for data and analytics initiatives. Additionally, the Data Engineer will be expected to collaborate with other data team members and data consumers working on the models and algorithms developed by them toward optimization for data quality, security, and governance to put them into production leading to potentially large productivity gains.\n\n About Otter Products \n\nOtter Products, we grow to give. From our founder's garage in 1998 to the global technology leader we are today, Otter Products continues to drive growth through innovation.\n\nThrough our industry-leading brands - OtterBox, Liviri and OtterCares - we provide our partners the number-one selling and most trusted products in our categories. Our philanthropic spirit is the foundation on which we foster our partner relationships, allowing us to grow and to give - together.\n\nBy way of our charitable arm, the OtterCares Foundation, we support our communities and invest in our future through education that inspires kids to change the world.\n\nAnd even as our global community of Otters continues to grow, our founder's core values are still at the heart of everything we do. We measure our success by our ability to give back to our communities and strengthen opportunities for all.\n\nFor more information visit otterproducts.com\n\n Responsibilities \n\nComplete analysis, design and development of BI solutions using Azure SQL Server\nFamiliarity with Data Warehouse concepts\nExperience coding the extraction, transformation, and loading (ETL) processes\nDatabase development primarily in SSIS, Data Factory and SQL\nPartner with the business to determine end user database/reporting needs and requirements\nGenerate ad hoc reports using Power BI or SSRS\nCollaborate with other developers to create and implement best approach solution(s)\nTroubleshoot Azure tools, systems, and software\nReview queries for performance issues, making changes as needed\nCollaborate with team to performance-tune Azure application as necessary\nAssist with the analysis and extraction of relevant information from large amounts of historical business data to feed data science initiatives\nParticipate and support the design and documentation of processes for large scale data analyses, model development, model validation and model implementation\nSupport and maintain a positive safety culture by following all safety policies and procedures and actively contributing to a safe working environment\nOther duties as required\n\n Qualifications \n\nBachelor's degree required. Degree in Computer Science or Mathematics preferred\nMinimum of four years of experience in an IT or analytical role required. Experience in database development, report writing and/or statistics preferred.\n\n EEO \n\nOtter Products, LLC is an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, age, marital status, pregnancy, sex, sexual orientation, gender, gender identity or expression, national origin, disability, veteran status, or any other characteristic or status protected by law.\n\n For US Based Roles Only - Compensation Range Minimum \n\nUSD $90,000.00/Yr.\n\n For US Based Roles Only - Compensation Range Maximum \n\nUSD $120,000.00/Yr.\n\n Additional Total Rewards \n\nProfit Sharing Program Eligible, Benefits Eligible - Full Time- check out otterproducts.com/careers/why for more info"}, {"title": "Software Engineer, Distributed Systems", "company_name": "Hightouch", "location": "United States (Remote)", "link": "/jobs/view/3628572539/?eBP=CwEAAAGMd4QO8APODyE_-mMOPhYyQGg5ZjVbPwq_3V7DK0B2bLAf10pcedM9S2T71X5i4TiJuEwOzWBbSiTKbLuk-VD-_Y-dN5uyf8YpYkHYqEYFxySMS9zr2hRjNrxX9gXbxssh35TFK9YTNV2WeH2Ncb675JgFbJTgNWSZySFC6rrdJGmdSDTr2q5d3VoxORz3GjsDbzjeyZQCvQ07RqYw7oujWSgslycs2LMCY_-N0NEj6e0PvbaYKiZ_5IW34bGVSYLXpHwqa8oOcNttYZ90UA3E_7sDSFzABdIVlfrp5zzERrIYy5sh2MIInxHdDoKgQw8VmOn5uM4S0tXUl-i7LnN0_W0NfPBmLiVDNuveIOm4MfDNPxfe5QKPk6FUAVpvgbcoJn9yRyE&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=rph%2FT6Q6Vkp6YoggJf9cJQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3628572539", "description": "About the job\nAbout Hightouch\n\nHightouch\u2019s mission is to empower everyone to take action on their data. Through our Reverse ETL platform, business and data users can seamlessly sync data from where it resides, such as warehouses and databases, to where it is needed, including operational systems and SaaS tools. Traditionally, acting on data has required engineering time and bandwidth, and left most business users stuck with charts and reports that are unable to take automated action on their data. With Hightouch, every business user, without writing any code, can activate data to streamline critical processes, improve marketing performance, and scale operations.\n\nOur team operates with a focus on making a meaningful impact for our customers. We believe in approaching challenges with a first principles thinking mindset, moving quickly and embracing our value of efficient execution, and treating each other with compassion and kindness. We look for team members that are strong communicators, have a growth mindset, and are motivated and persistent in achieving our goals.\n\nHundreds of companies use Hightouch, including Spotify, Ramp, Retool, NBA, Plaid, and Betterment. We\u2019re based in San Francisco, are remote-friendly, and backed by leading investors such as Amplify Partners, ICONIQ Growth, Bain Capital Ventures, Y-Combinator, and Afore Capital.\n\nAbout The Role\n\nWe are looking for a distributed systems engineer to work on the systems that power the syncing engine our customers and other engineering teams rely on. This presents an exciting challenge where you can apply your expertise in distributed systems, performance optimization, and troubleshooting to push the boundaries of what is possible and meaningful improve our ability to move massive amounts of customer data. This role also provides a unique opportunity to work on a multi-cloud and multi-region infrastructure that supports a global customer base.\n\nWe believe in enabling our engineers to do their best work for our customers by giving them extremely high levels of ownership and autonomy. This comes in different forms: you will own and deliver projects from start to finish, you will work directly with customers to solve their hardest scaling problems, and you will have a lot of influence over what we work on as a team and company.\n\nSome of the problems we\u2019ll be working on include:\n\nSync Speed: Customers want to sync a lot of data to important destinations like Facebook and Snapchat, which requires us to analyze every part of our syncing process and find where we can optimize to sync data more quickly\nStreaming Syncing: We currently sync data from sources in batch only, but envision a world where we also do syncs in real-time and support streaming sources like webhooks and queues\nScalability and Reliability: As part of our rapid growth, we\u2019re always evaluating current future bottlenecks in scaling and reliability, and architecting for the next order of magnitude of growth\nPersonalization API: Our Personalization API product provides a low latency caching layer on top of data warehouses that enable customers to use their data warehouse for real-time personalization use cases with \nMulti-Region and Multi-Cloud: Supporting our multi-region and multi-cloud backend, including extending it to launch Hightouch on in new regions to support data residency requirements of our global customer base\n\nWe are looking for talented, intellectually curious, and motivated individuals who are interested in tackling the problems above. This is a senior role, but we focus on impact and potential for growth more than years of experience. The salary range for this position is $170,000 - $240,000 USD per year, which is location independent in accordance with our remote-first policy.\n\nAbout You\n\nYou are an engineer with a passion for solving hard technical problems that generate real value for customers. You\u2019re motivated by high ownership and are comfortable in a fast-paced, startup environment.\n\nYou have experience and comfort with distributed systems and high-scale systems or the ability and desire to build this experience quickly. This experience can take different forms:\n\nYou\u2019ve built syncing engines that process and move terabytes of data per day in a scalable way\nYou\u2019ve created event collection or data streaming services that power company critical systems\nYou\u2019ve owned systems that have undergone orders of magnitude of growth and helped them scale accordingly\nYou have relevant skills and are a fast learner who is excited to take on the challenges we face\n\nInterview Process\n\nOur goal with the interview process is to balance speed with giving both parties opportunities to assess whether there is a strong mutual fit. We will ask you questions, but we want you to ask us questions! Our technical interviews focus on how you design systems because we believe this is the best way for us to see how you work and for you to see how we collaborate. We don\u2019t ask you to write code to solve technical brainteasers that don\u2019t appear in your day to day job.\n\nRecruiter screen [30m]: Introductory call with our recruiting team to get to know each other and see if the role could be a good mutual fit.\nSystems design screen [45m]: Designing a data processing feature end-to-end.\nHiring manager interview [30m]: Chat with hiring manager about past experiences and future operating preferences to assess fit on company values and operating principles.\nSystems design interview [90m]: Work with the interviewer to architect a system at a conceptual level. The problem will be at a pretty high level - and have both product and customer requirements as well as technical."}, {"title": "Entry-Level Data Scientist Engineer", "company_name": "Phoenix Recruitment LLC", "location": "Texas City, TX (Remote)", "link": "/jobs/view/3788727625/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=8iwtytHsCu4wMwJ4fj3pxw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3788727625", "description": "About the job\nThis is a remote position.\n\nTitle - Entry-Level Data Scientist Engineer, 1 year of project experience\n\nEmployment Type: Full-time\n\nBase Salary: $60K-$70K\n\nPhoenix Recruitment offers a variety of recruiting services to assist both employers and employees. They specialize in marketing open positions, recruiting, and helping employers to find qualified candidates across various industries. Phoenix Recruitment has expertise in streamlining the hiring process. They can help ensure that the process is efficient, well organized, and compliant with relevant regulations.\n\nSkills and Abilities:\n\nStrong knowledge of R or Python for data analysis and modeling. \nProficiency in statistical programs such as R, SAS, MATLAB, or Python. \nFamiliarity with spreadsheets (VBA) and database applications (Access, Oracle, SQL, or equivalent technology). \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \nAbility to learn new methods quickly and work under deadlines. \nExcellent teamwork and communication skills. \nStrong analytical and problem-solving abilities. \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \n\nPreferred:\n\nKnowledge of actuarial concepts and life, health, and/or annuity products. \nExperience with statistical modeling techniques such as GLM, Decision Trees, Time Series, Regression, etc. \nFamiliarity with Microsoft DeployR. \nExposure to insurance risk analysis. \nBasic experience in computational finance, econometrics, statistics, and math. \nKnowledge of SQL and VBA. \nFamiliarity with R or Python for predictive modeling \n\nWhy Phoenix Recruitment LLC?\n\nPhoenix Recruitment often has an extensive network of employers and candidates. This network allows them to tap into a pool of qualified candidates and connect them with suitable job opportunities. They can also leverage their connections to help employers find the right talent efficiently. Outsourcing the recruitment process to a specialized agency can save your time and resources, avoid delays, reduce administrative burdens, and increase the chances of finding the right fit for your organization."}, {"title": "Data Engineer - Remote Work", "company_name": "BairesDev", "location": "United States (Remote)", "link": "/jobs/view/3783162297/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=OtoCrAAHZFomVup04fy35g%3D%3D&trk=flagship3_search_srp_jobs", "id": "3783162297", "description": "About the job\nWho We are\n\nBairesDev is proud to be the fastest-growing company in America. With people in five continents and world-class clients, we are only as strong as the multicultural teams at the heart of our business. To consistently deliver the highest quality solutions to our clients, we only hire the Top 1% of the best talents and nurture their professional growth on exciting projects.\n\nWe are looking for a Data Engineer to join our Development team and participate in different projects made up of multicultural teams distributed throughout the world.\n\nWhat You Will Do\n\n Interview other Data Scientists and Data engineers to vet their technical abilities.\n Input data from those interviews into the product and help develop a predictive, fair, and enjoyable solution.\n Work together with developers, tech leads, and solution architects to build applications.\n Improve existing structures by adding new functionalities or proposing technological updates so that as a result, the impact of your contribution is significant in the core of each business.\n Improve the user experience on scalable and high availability platforms, contributing to the key differential of each business.\n\nHere\u2019s What We Are Looking For\n\n 5+ years of experience working as a developer.\n 3+ years of experience working as a Data Scientist / Data Engineer.\n Proficient with analysis, troubleshooting, and problem-solving.\n Hands-on experience with managing data loads and data quality.\n Advanced English level.\n\nHow we do make your work (and your life) easier:\n\n 100% remote work.\n Hardware setup for you to work from home.\n Flexible hours - make your schedule.\n Paid parental leave, vacation & holidays.\n Diverse and multicultural work environment.\n An innovative environment with the structure and resources of a leading multinational.\n Excellent compensation \u2014 well above the market average.\n Here you can grow at the speed of your learning curve.\n\nOur people work remotely but with a consistent and robust culture that promotes diversity and teamwork. To continue being the leading software development company in Latin America, we want to ensure that every BairesDev member gets the best growth and professional development opportunities in a diverse, welcoming, and innovative environment.\n\nEvery BairesDev team member brings something unique to our company.\n\nWe want to hear your story. Apply now!"}, {"title": "Data Engineer (Remote)", "company_name": "Cascade Debt", "location": "United States (Remote)", "link": "/jobs/view/3755336571/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=pmUtn1j2zyPr25CNPh2RNw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3755336571", "description": "About the job\nDo you want to work in a small team where you can make a real difference in a company? Would you embrace the challenge of building a high impact platform that is used globally? Then we want to speak with you!\n\nAbout Cascade\n\nCascade is a fintech startup backed by Canadian and US investors that empowers companies to grow by democratizing access to institutional debt. We are building tools that modernize how companies raise and manage debt, lowering the barriers to entry to the $7 trillion specialty finance and alternative credit market for companies around the world. Founded just last year, we are gearing up for our next phase of technical development and are seeking a talented Data Engineer to lead the way.\n\nAbout The Role\n\nAs a Data Engineer you\u2019ll be an early member of a growing team building a pioneering platform in the debt infrastructure space. You will design, build, and launch efficient, scalable, and reliable data pipelines to move and transform data .\n\nWhat You\u2019ll Do\n\n Building databases, data lakes and data ingestion pipelines to integrate customer databases and datasets to our systems \n Ingesting financial datasets from external customers, then u pdating and maintaining accurate and complete data mappings to ensure that our product s are displaying high quality data \n M onitoring and alerting across our data pipelines in order to make sure that our data ingests are reliable and correct \n Translat ing business goals and stakeholder requirements into new data flows and deliver analytical insights ensuring that the data flows create real business value continuously \n\nSkills And Experience\n\n 2 + years experience in a data engineering or data analyst role \n E xperience with relational SQL and NoSQL databases, including PostgreSQL , MySQL, MariaDB, MongoDB, Bigtable, etc. \n E xperience working with ETL technologies , such as Databricks, Fivetran , or dbt \n Proficiency in writing SQL queries and knowledge of analytical data warehouses such as RedShift, BigQuery , and Snowflake \n Some experience with CI/CD automation such as GitHub Actions \n Developing APIs and integrating with 3rd party APIs \n Bonus: experience in fintech / SaaS / credit analysis \n\nPeople Who Thrive At Cascade Are\n\n Self-starters, who take the initiative to tackle challenges in a remote work environment \n Good communicators, who can operate without ego to discuss, learn, grow, and help others do the same \n Passionate about creating great digital experiences for users \n Problem solvers who are not satisfied with the status quo \n\nBenefits\n\nRemote: we are a remote-first company and are very flexible on hours as long as things get done \nHome-office: there\u2019s a $1000 USD home office allowance to set yourself up \nEquity: we expect you to have an owner-mentality, and have the equity plan to match \nBenefits: health, dental, vision, and more \nPerks: we offer free lunches weekly and off-site trips \nJob satisfaction: we offer autonomy, ample opportunities for mastery, and an opportunity to make a difference for companies around the world"}, {"title": "Data Engineer", "company_name": "ISC (Integrated Specialty Coverages, LLC)", "location": "Carlsbad, CA (Remote)", "link": "/jobs/view/3778293528/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ew7%2FfEuXSujs5IGabdqUyw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3778293528", "description": "About the job\nAbout Integrated Specialty Coverages\n\nIntegrated Specialty Coverages LLC (ISC) is a growth stage technology and data-driven insurance company leading innovation in the market.\n\nBacked by one of the leading private equity firms, KKR, and led by a forward-thinking management team, ISC is combining the worlds of insurance and technology to create an Insurtech powerhouse. As a leading online distributor of insurance products for a range of industries and \"Main Street USA\", we are looking for the right people to help us in our mission of achieving exponential growth. We strive to be the number one place to go for brokers and agents to source insurance. To accomplish this, we're building a digitally focused team that deeply understands the intersection between user experience, data, and AI/ML to optimize the way we engage with our customers and partners.\n\nJob Summary\n\nThe Data Engineer role will be the technical liaison between multiple groups including a data science team, the engineering team, product management, and business stakeholders. You do not need any insurance knowledge prior, however, you must quickly dive deep into the insurance world and ask questions to become a subject matter expert. You will be responsible for building a data platform to facilitate the data science team. You must be a self-starter that can build out features such as a data pipeline from scratch. There will be support from both engineering and data science for any buildout.\n\nJob Responsibilities\n\nCreate and maintain optimal data pipeline architecture. \nAssemble large, complex data sets that meet functional / non-functional business requirements. \nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. \nBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using AWS technologies, SQL, and Python. \nWork with stakeholders including the Executive, Product, Data Science, and Engineering teams to assist with data-related technical issues and support their data infrastructure needs. \nWork with data science and analytics teams to strive for greater functionality in our data systems. \n\nMinimum Qualifications:\n\nAdvanced working SQL knowledge and experience working with relational databases, strong query authoring (SQL) as well as working familiarity with a variety of databases (Snowflake, Redshift, MySQL, MSSQL, etc.)\nExperience building and optimizing data pipelines, architecture and data sets. \nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. \nBuild processes supporting data transformation, data structures, metadata, dependency, and workload management. \nA successful history of transforming, processing and extracting value from large disconnected, datasets from a variety of data sources (Flat files, Excel, databases, etc.)\nStrong analytic skills related to working with unstructured datasets. \nStrong project management and organizational skills. \nExperience supporting and working with cross-functional teams in a dynamic environment. \nAbility to mentor/guide/collaborate with other team members. \n\nWe are looking for a candidate with 3-5 years of experience in a Data Engineer role. They should also have experience using the following software/tools:\n\nExperience with relational and MPP databases, including Snowflake and MySQL. \nExperience developing software in an agile environment from the requirements stage to production. \nExperience with version control: git\nExperience with container technologies: Docker\nExperience with data pipeline and workflow management tools: Airflow, Jenkins, AWS Glue, Azkaban, Luigi, etc. \nExperience with AWS cloud services: EC2, ECS, Batch, S3, EMR, RDS, Redshift\nExperience with other cloud services: Snowflake, Airflow\nExperience with object-oriented/object function scripting languages: Python, Java, C++, etc. \nExperience with data modeling and data warehouse design\nExperience with data visualization tools (PowerBI, QuickSight)\n\nThe starting annual pay scale for this position is listed below. Actual starting pay will be based on factors such as skills, qualifications, training, and experience. In addition, the company offers comprehensive benefits including medical, dental and vision insurance, 401(k) plan with match, paid time off, and other benefits.\n\nISC's salary ranges are determined by role and level. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations and could be higher or lower based on a multitude of factors, including job-related skills, experience, and relevant education or training.\n\nNational Pay Range\n\n$100,000\u2014$110,000 USD\n\nBenefits of Working at ISC\n\nCompetitive vacation and flexible working arrangements\nComprehensive and inclusive health benefits\nA variety of professional development and mentorship opportunities\nChoice of technology whether at home or in the office\n\nISC's Ownership Behaviors\n\nDo the Right Thing *Be Relentless. Pursue Excellence. *Personal Responsibility & Own Outcomes\nTry Fast. Learn Fast. Fail Fast. Think Big. *Build Extraordinary Owners & Join Forces *Grit & Determination\n\nApplicants with disabilities may contact ISC HR department via e-mail, and other means to request and arrange accommodation. If you need assistance to accommodate a disability, you may request accommodation at any time. Please contact ISC at HR@ISCMGA.com.\n\nISC believes in creating long-term relationships by being responsive and relevant and by consistently delivering value to our community of customers. Specifically, with our employees, we focus on attracting, developing, and retaining the best talent for our business, challenging our people, demonstrating a \"can-do\" attitude, and fostering a collaborative and mutually supportive environment. ISC is an equal opportunity employer and a participant in the US federal E-Verify program (US).\n\nDiversity creates a healthier atmosphere: All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, gender, gender identity, sexual orientation, marital status, medical condition, genetic information, mental or physical disability, military or veteran status, or any other characteristic protected by local, state, or Federal law."}, {"title": "Data Engineer -Remote", "company_name": "RIT Solutions, Inc.", "location": "Denver, CO (Remote)", "link": "/jobs/view/3768007496/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=1hQH8EDaII1eJ7nQ9BACQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3768007496", "description": "About the job\nDuration: 6+ months CTH\n\nMust have a valid LinkedIn profile\n\nRequired Skills\n\n 3-6 years of experience (no more than 6 years)\n Start up environment experience\n Python, SQL, and DBT"}, {"title": "Big Data Engineer - W2", "company_name": "Megan Soft Inc", "location": "United States (Remote)", "link": "/jobs/view/3674845652/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fCAxh4N%2F4YZJ5aEIASaeeA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3674845652", "description": "About the job\nTitle: Big Data Engineer - W2\n\nLocation: Dearborn, MI (Remote)\n\nDuration: 12+ Months\n\nDirect Client: Ford Motors\n\nMust have skills: Should be able to handle GCP requirements\n\n\"Translates business requirements to build repeatable, sustainable, efficient, coded processes that can be productionized by Software Engineers and readily modified by other Data Engineer Creates POC processes in Dev/QA and works with Software Engineers to productionize and align best coding and process practices that maximize efficiency, speed, stability, system resources and capabilities Awareness of and compliance with data privacy, security, legal and contractual guidelines Incorporates data quality and privacy checks/alerts to minimize bad data being consumed by end users, models and dashboards, and to protect customer data; revises checks for new data issues Validates data products and pipelines are functioning as expected following system or application upgrades, source changes, etc. Works closely with Architects if required to align systems, tools and applications being utilized with business use case and performance requirements Provides data product support and maintenance Establishes, tracks and monitors KPIs related to specific data products and deliverable Participate in iteration cadence activities (Iteration Planning, task estimation, daily scrum, demos, etc.). Will be a part of on-call team to resolve critical production support tickets during business hours and off hours.\n\nSkills Required\n\nExperience with Big Data Technologies (Hadoop , ETL Tool, Spark. Hive etc..) Someone who understands Cloud and has experience working with Cloud Platforms (preferably GCP) Should be able to handle GCP requirements - ingesting into GCS/ Big Query, working with GCP components Hands-on in Data Flow, Cloud Storage (GCS), Big Query and GitHub. Proficient in Python and SQL.\n\nSkills Preferred\n\nManages ETL: uses programming and tools for data ingestion, configures pipelines, applies transformations and decoding, integrates and fuses data, moves and securely deliver Experience NoSQL databases\n\nAdditional Information\n\nIf the candidate is remote only, please indicate \"\"100% Remote\"\" under candidate's name on resume. Other similar schedule notations can be \"\"Local Candidate\"\" or \"\"Hybrid Candidate.\"\" ***HRA Test Required***\""}, {"title": "Remote Opportunity: Quantexa Data Engineer", "company_name": "SPAR Information Systems LLC", "location": "United States (Remote)", "link": "/jobs/view/3680073191/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ioykVCZSOQ7BK5%2FwRQHOWg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3680073191", "description": "About the job\nHello All, Hope you are doing great!! Please go through the job descriptions and let me know your interest. Role: Quantexa Data engineer  Location: Remote Duration: Long Term Contract Job Description: 1. End to End usage of Quantexa including guiding our client team on how to use from both front end and data ingestion Reason they are asking for Certification 2. Resolve Entity issues with data 3. Knowledge of Scala and Hive which requires coding experience using parquet 4. Advanced SQL knowledge requiring writing SQL queries 5. Experience having worked with GCP data lake 6. Quantexa certification required\n\nThanks & Regards,\n\nSatnam Singh\n\nDirect: 201 623 3660\n\nEmail : Satnam.singh@sparinfosys.com"}, {"title": "Remote Work - Need Staff Data Engineer - USC only", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3746283980/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=CvIuLBNO%2BPMGi4IXVRkP0A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3746283980", "description": "About the job\n100% Remote, client is in NY\n\nLook for strong job history, Python, Machine Learning, SQL, 10/10 comms, etc. \n\nMUST be an Engineer \n\nSummary\n\nWe're looking for an experienced Data Engineer to join our client's growing team of top notched IT professionals. You will work closely across multiple groups including Editorial and Audio, Marketing, Advertising, etc. to achieve their goals and objectives. The Data Engineering team builds tools throughout the stack while sharing knowledge across these departments.\n\nAs the company grows, they're looking for Data Engineer who will help solidify and expand our pipelines and maintain their data warehouse. Their business model is based on performing complex and very detailed analyses of how our products perform with their subscribers. You'll be responsible for working alongside analysts, product, marketing, and other teams to define new data collection components and measurement schemes that support each new product and feature.\n\nHow Can Add Value To Their Mission\n\nCreate and maintain data pipelines to provide insights and drive business decisions\nEstablish data warehousing strategy (ex. Kimball, Data Vault, etc.)\nMaintain data infrastructure on our AWS accounts\nCollaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.\nWrite unit/integration tests, contributes to engineering wiki, and documents work\nImplement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it\n\nWhat You'll Need To Succeed\n\n7+ years of industry experience measuring product performance and user behavior\nExperience working with a variety of data management technologies, including RedShift, Kinesis/Kafka, Glue, Spark, Postgres, Airflow, dbt, and others.\nExperience implementing BI reporting tools such as Looker\nExperience interfacing with engineers, product managers and analysts to understand data needs\nKnowledge of a variety of measurement beacons, SDKs, APIs including Google Analytics, Amplitude, Braze, Stripe, email service providers\nA commitment to building secure, resilient, fault-tolerant architectures, with clear documentation and procedures in place for support\nA focus on accuracy and detail as you build, ensuring data pipelines produce clear, predictable results\nUnderstanding of the typical metrics a subscription and advertising-supported business needs to measure success\nExperience with Client techniques as applied to behavioral segmentation or anomaly identification is a definite plus\nFamiliarity using developer tools that increase productivity and facilitate the development of resilient code (eg. Docker, CircleCI, Serverless) is a plus"}, {"title": "ENTRY LEVEL DATA ENGINEER (REMOTE)", "company_name": "SynergisticIT", "location": "Jacksonville, FL (Remote)", "link": "/jobs/view/3767587909/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=om7L3CnowiC4PLIcpZD70A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3767587909", "description": "About the job\nSYNERGISTICIT wants every candidate to know that the Job Market is Challenging and to stand out, you need to have exceptional skills and technologies and that's where we come in to make sure you get the attention which you need\n\nSynergisticIT understands the complex nature of the job market and how difficult it can be to secure a position, especially for fresh graduates. Therefore, we assist and help tech-savvies to convert their passions into professions. We go above and beyond to keep you working in your niche.\n\nAs we focus on long-term success, we provide complete career development solutions. From job search to upskilling portfolio and interview preparation, we can guide you at every step of your career.\n\nSynergisticIT spares no efforts to connect you with a large network of tech giants, including Google, Apple, PayPal, Dell, Cisco, Client, etc. Presently, we are actively looking for ENTRY LEVEL DATA ENGINEER a driven mindset. Get the right opportunity and gain experience in building web-centric solutions on Java.\n\nWho Should Apply : Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or anyone looking to make their career in IT Industry\n\nWe also assist in filing for STEM extension and H1b and Green card filing.\n\nCandidates who are serious about their future in the IT Industry and have set big goals for themselves.\n\nCandidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. We also offer Skill Enhancement Programs if the candidates are missing skills or experience which our clients need with great outcomes\n\nCandidates can benefit from skill enhancement if they fall into the below categories. If they are qualified with enough skills then no need for skill enhancement\n\nCandidates Who Lack Experience\n\nHave had a break in careers\n\nLack Technical Competency\n\ncandidates who want to get employed and make a career in the Tech Industry \n\nPlease Also Check The Below Links\n\nhttps://www.synergisticit.com/candidate-outcomes/\n\nhttps://www.synergisticit.com/java-track/\n\nhttps://www.synergisticit.com/data-science-track/\n\nhttps://www.synergisticit.com/which-is-the-best-option-for-tech-job-seekers-staffing-companies-consulting-companies-bootcamps-or-synergisticit/\n\nhttps://www.synergisticit.com/contact-us/\n\nIf the skills are not a match candidates can opt for Skill enhancement. Or their resume can be sent out to clients to see if responses are achievable\n\nREQUIRED SKILLS For Java/Software Programmers\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Core Java , javascript , C++ or software programming\nSpring boot, Microservices and REST API's experience\nExcellent written and verbal communication skills\n\nFor data Science/Machine learning\n\nRequired Skills\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Statistics, Python, data visualization tools\nExcellent written and verbal communication skills\n\nPreferred skills: NLP, Text mining, Tableau, Time series analysis\n\nTechnical skills are required by clients for selection even if its Junior or entry level position each additional Technical skill helps a candidate's resume to be picked by clients over other job seekers.\n\nClients hire candidates with the right technical skills which they need and reject candidates who lack the required technical skills.\n\nNo third party candidates or c2c candidates\n\nPlease apply to the posting\n\nNo phone calls please. Shortlisted candidates would be reached out"}, {"title": "Data Engineer - Remote | WFH", "company_name": "Get It Recruit - Information Technology", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3782241984/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=BgBf1kSAAX%2BSuw6inJsIVw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3782241984", "description": "About the job\nJoin an innovative team at a leading company in the engineering and manufacturing industry, where we are committed to leveraging data for continuous improvement and operational excellence. This hybrid position allows you to enjoy the flexibility of remote work on Mondays and Fridays.\n\nWhy Join Us\n\nAt our company, we prioritize your well-being and professional growth. We offer a comprehensive benefits package, including excellent health benefits, a 6% 401(k) match, generous paid vacation and sick time, 12 paid holidays, and more.\n\nRole Overview\n\nAs a Data Engineer, you will play a pivotal role in transforming raw data into actionable insights, driving improvements in engineering and manufacturing processes, enhancing product quality, and optimizing operational efficiency. Collaborate with dynamic cross-functional teams to extract, analyze, and visualize data, influencing strategic decisions through your valuable insights.\n\nKey Responsibilities\n\nData Collection and Integration:\n\nGather, clean, and integrate data from diverse sources, including production databases, sensor data, and external datasets.\n\nData Analysis\n\nConduct exploratory data analysis to identify patterns, trends, and anomalies.\n\nPerform statistical analysis and hypothesis testing to derive meaningful insights.\n\nData Visualization\n\nCreate informative and visually appealing dashboards and reports using tools like Tableau, Power BI, or Python libraries.\n\nPredictive Modeling\n\nDevelop predictive models to forecast manufacturing performance, quality, and operational outcomes.\n\nRoot Cause Analysis\n\nInvestigate production issues and quality concerns by analyzing data to identify root causes and recommend corrective actions.\n\nPerformance Monitoring\n\nEstablish and maintain key performance indicators (KPIs) to track manufacturing and engineering metrics.\n\nGenerate regular reports to communicate performance trends to stakeholders.\n\nCross-functional Collaboration\n\nCollaborate with engineering, manufacturing, and quality assurance teams to understand their data needs and provide analytical support.\n\nContinuous Improvement\n\nSuggest process improvements and optimizations based on data analysis.\n\nData Security And Compliance\n\nEnsure data security and compliance with relevant regulations, such as GDPR or industry-specific standards.\n\nInfrastructure And Development\n\nBuild and maintain the infrastructure for collecting, storing, and processing data.\n\nDesign, develop, and manage data pipelines, ETL processes, and data warehouses.\n\nRequired Experience\n\nProven experience as a Sr. Data Analyst and/or Data Engineer in an engineering or manufacturing environment.\n\nProficiency in data analysis tools and programming languages, such as Python, R, SQL, and Excel.\n\nStrong data visualization skills with tools like Tableau or Power BI.\n\nExcellent knowledge of statistical analysis and machine learning techniques.\n\nAbility to communicate complex findings and insights effectively to both technical and non-technical stakeholders.\n\nStrong problem-solving skills and attention to detail.\n\nFamiliarity with manufacturing processes and quality control is a plus.\n\nExperience with enterprise ERPs (SAP S4HANA, Oracle EBS), CRMs (Salesforce, Hubspot), and IoT.\n\nProficient in working with cloud computing platforms including AWS, GCP, and Azure.\n\nExpertise in configuring and optimizing cloud-based data solutions for scalability, reliability, and performance.\n\nBachelor's Degree in Data Science, Statistics, Engineering, or Computer Science Required.\n\nApply Now and be part of a team shaping the future of data-driven innovation in engineering and manufacturing!\n\nEmployment Type: Full-Time"}, {"title": "Entry Level Software Engineer - Data Backend Engineer (Remote - Canada)", "company_name": "Yelp", "location": "United States (Remote)", "link": "/jobs/view/3752881757/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fankeoxifvhpTzOYvowHYQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3752881757", "description": "About the job\nYelp engineering culture is driven by our values: we\u2019re a cooperative team that values individual authenticity and encourages creative solutions to problems. All new engineers deploy working code their first week, and we strive to broaden individual impact with support from managers, mentors, and teams. At the end of the day, we\u2019re all about helping our users, growing as engineers, and having fun in a collaborative environment.\n Interested in exploring data and finding innovative ways to collect it, curate it, and make it easily accessible for others to use? How about doing that at an enormous scale? Yelp\u2019s datasets contain billions of interactions between users and local businesses around the globe. If working with our vast amount of data sounds exciting, come and join us!\n\nThe Marketplace Data Observability team is responsible for managing, governing, and monitoring all of Yelp\u2019s Ads data to protect our revenue stream and also support offline analysis that aids in making long term company decisions. We build tools that help us to better understand our data quality, and we strive to uncover data issues before they impact our consumers.\n\nAs a Data Backend Engineer on this team, you will be responsible for building elegant and scalable data products that serve critical, up-to-date, structured information to support different types of analytics within Yelp. As a core contributor to our growing data modeling and data warehousing engineering efforts, you will help design and own mission-critical data flow pipelines and datastores to enable decisions including effective A/B testing and company investments.\n This opportunity is fully remote and does not require you to be located in any particular area in Canada. We welcome applicants from throughout Canada. We\u2019d love to have you apply, even if you don\u2019t feel you meet every single requirement in this posting. At Yelp, we\u2019re looking for great people, not just those who simply check off all the boxes.\n  Build systems that can effectively store and crunch terabytes of data.\nDesign and develop data models for efficient data storage, retrieval, and reporting.\nCreate and maintain conceptual, logical, and physical data models using industry-standard modeling tools.\nCollaborate with cross-functional teams, including engineers, data analysts, business analysts, and data scientists, to understand data requirements and translate them into effective data models.\nParticipate in data integration efforts, including ETL processes and data migration.\nStay up-to-date with industry best practices, emerging technologies, and trends related to data warehousing.\nSupport on-call rotations as needed to operate the team.\n  Understanding of high performing and scalable data systems.\nExperience in building and orchestrating ETL pipelines.\nExperience with Data Lake or Data Warehouse landscape.\nA hunger for tracking down root causes and fixing them in systematic ways.\nAbility to communicate effectively to technical and non-technical cohorts alike. \nExposure to some of the following technologies: Python, AWS Redshift, AWS Athena / Apache Presto, Big Data technologies (e.g S3, Hadoop, Hive, Spark, Flink, Kafka etc), NoSQL systems like Cassandra, DBT."}, {"title": "Data Engineer (Associate Consultant), June 2024", "company_name": "UDig", "location": "Nashville, TN (Remote)", "link": "/jobs/view/3716039960/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=XUjo40kU55HnDCqJ%2F3FdPg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3716039960", "description": "About the job\nCan UDig It? \n\nUDig designs, builds, and implements technology solutions that deliver on business objectives. People join our team because they are excited to have an impact on the future of our organization, passionate about delivering meaningful results, and energized by solving complex problems. Oh, and they like working with other driven, smart, and authentic people!\n\nUDig is proud to be recognized as a \"Best Places to Work\" in Virginia and Tennessee. We're committed to fostering a transparent environment where leadership is accessible, professional development opportunities are abundant, and you are empowered to achieve your career goals. You'll collaborate, develop deep relationships with teammates, and accelerate your success while having fun!\n\nIf you're graduating in May 2024 and excited by the idea of impacting organizations by building data solutions to achieve business goals, you should consider joining our growing technology team.\n\nHow Things Go Down\n\nAs a new, entry-level teammate, you'll take part in our \"Breaking Ground\" training program designed to help you quickly get up-to-speed on UDig's delivery approach within a project team. The program covers:\n\nUDig delivery process and methodologies such as gathering requirements and delivering projects using agile\nTechnical guidance and instruction on the best practices from Data leaders at UDig\nDelivering executive and technical presentations\nCoaching to prepare you to for success on your first client project at UDig\n\nA typical day might entail:\n\nProject standups and check-ins\nDesign and development of data pipelines, BI reports, and more\nPlanning and participating in project demos\nContributing to internal UDig projects\nCareer and job growth through UDig-provided training\nCreating long-lasting friendships with teammates through fun activities as well as your projects (including social events, winning a prize for guessing the weight of pumpkin, or sharing experience on cool tech side projects)\n\nUDig believes it is important for Associate Consultants going through our Breaking Ground program to reside in one of our major markets (Richmond, VA or Nashville, TN metro area) for at least the first year of employment.\n\nWant to learn more? Hear directly from our teammates by checking out our 'A Day in the Life' blog series: https://www.udig.com/digging-in/breaking-ground-2023/\n\nSound Interesting? Here's the Technical Piece\n\nBelow are a few boxes you should be able to check if\u202fyou're interested: Don't check every box? Go ahead and apply!\n\n1+ year of experience with relational database management systems such as Microsoft SQL Server, MySQL, or Oracle\nUnderstanding of the full lifecycle of design, development, and implementation of data solutions including architecture design, development, testing\nSome experience in ETL development or reporting is a plus\nAbility to present technical ideas and high-level concepts and solutions to internal and external team members with varying degrees of technical knowledge\nBachelor's Degree in in computer science, information technology, or any science or business discipline\nAbility to take your work seriously but also seriously enjoy a good time with your UDig colleagues\n\nWhat's in It For You? \n\nAt UDig, you will build your career alongside talented and experienced developers while gaining experience and having fun! We provide continuous challenges and professional development opportunities. Additionally, we offer: \n\nCompetitive salary, merit reviews, and career advancement paths\nFlexible, hybrid environment\nIndividual $1500 Training Budget\nRegular team building and social activities (virtual and/or in-person)\nTransparent culture with strong communication and access to leadership\nGreat benefits like\nGenerous Paid Time Off, company holidays, and parental leave\nMultiple Single and Family Health Insurance plans to choose from\nDental & Vision coverage\nShort-Term and Long-Term disability\nOptional accident and critical illness coverage\nMatching 401(k)\nand more!"}, {"title": "Expression of Interest: Data Engineer", "company_name": "Fingerprint for Success (F4S)", "location": "New York, NY (Remote)", "link": "/jobs/view/3787769880/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=l%2FEDpQqUNmCttGdIyTk7Cg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787769880", "description": "About the job\nWe are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.\n\nThe F4S Talent Pool is a pilot project designed to:\n\nHelp job seekers get discovered by our partners based on their anticipated hiring needs.\nProvide optional support and resources for job seekers in their career endeavors.\nHelp individuals understand, and bring out the best in themselves and each other.\n\nThe F4S Talent Pool process:\n\nOnce you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.\n\nAbout Fingerprint For Success (F4S)\n\nBacked by 20+ years of research, F4S\u2019s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high performing teams.\n\nKeep in mind, joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your F4S work style assessment to match the hiring needs of our partners.\n\nYour feedback is a gift! Write to us via:\n\nPowered by JazzHR\n\noWuPJ9lwq9"}, {"title": "Data Engineer Seattle, WA (remote)", "company_name": "Conch Technologies, Inc", "location": "United States (Remote)", "link": "/jobs/view/3765052211/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=SBETI78UMutCHuRp5FnP%2FA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3765052211", "description": "About the job\nHi,\n\nGreetings from Conch Technologies Inc\n\nData Engineering Lead \u2013 With Databricks and Unity Catalog\n\nLocation \u2013 Seattle, WA (remote)\n\nDuration \u2013 12 months\n\nNeed: Design and implement data architecture and strategy, leveraging technologies such as Databricks, Unity Catalog, Privacera, and Collibra.\n\nAs a Data Engineering Lead, you will play a pivotal role in defining and executing technical deliverables while assessing complexities and levels of effort. You'll engage with both vendor and internal teams to resolve technical challenges and ensure seamless collaboration. Your leadership will extend to planning and prioritization with the SCRUM master, facilitating effective coordination between onsite and offshore teams, and driving feature prioritization through collaboration with product managers.\n\nKey Responsibilities\n\n Lead the definition and breakdown of technical deliverables, assessing complexities and levels of effort.\n Collaborate with vendor and internal teams to address technical issues.\n Drive planning and prioritization in conjunction with the SCRUM master.\n Coordinate with onsite and offshore teams for timely project delivery.\n Collaborate with product managers to prioritize features, communicate with stakeholders, and clarify requirements.\n Collect and manage user-reported issues, ensuring prompt resolution and communication.\n Define comprehensive test plans, oversee their execution, and document results.\n\n--\n\nWith Regards,\n\nNagesh G\n\nMobile: 408-381-5645\n\nDesk: 901-313-3066\n\nEmail: nagesh@conchtech.com \n\nWeb: www.conchtech.com"}, {"title": "Remote Work - Need Azure Fabric Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3759325599/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=C3iVjmcR9wjUip1nNjUiCg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3759325599", "description": "About the job\nThe client is placing heavy emphasis on Microsoft and Azure Fabric experience\n\nInformation Management Engineer \u2013 Take the identified information management sources and assist migration to the Client Data Lake, Azure Fabric. The engineer will transform information management (de-dup, clean and enrich).\n\nTake the identified data sources and bring them into our Data Lake, Azure Fabric. Once the data is in, the engineer will need to be able to transform data (de-dup, clean and enrich the data). We are focused on using a medallion data model for data architecture. The tools we are working with are: Delta Parquet files, python, spark, notebooks, T-SQL, Fabric lakehouse/warehouse, Dataflows Gen2, Data Pipelines, Direct Lake, shortcuts, ADF."}, {"title": "Data Engineer", "company_name": "Catalist", "location": "Washington, DC (Remote)", "link": "/jobs/view/3744951279/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ui1Fk6ICmnPNqAlZDZAcwA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3744951279", "description": "About the job\nFor over 17 years, Catalist has been a leader in civic data and data science innovation. Our mission is to provide progressive organizations with the data, software, and services needed to better identify, understand, and communicate with the people they need to engage and mobilize. Our clients include the largest, most influential organizations in the U.S. active in civic engagement, advocacy, and political campaigns.\n\nCatalist is home to a dedicated, creative team of technologists, data scientists, and campaign experts committed to using our talents and technology to nurture a vibrant and growing progressive community.\n\nAs a Data Engineer at Catalist, you will work closely with the Analytics and Technology departments to design, build, support, and maintain various data pipelines and processes with an end goal of providing data and intelligence to the progressive community. This role primarily involves translating data architecture designs into functional processes, code, and systems.\n\nThe ideal candidate will be a highly motivated individual with excellent technical skills, a strong desire to learn new skills, and an interest in progressive politics. Catalist values creativity and problem-solving. Our work is on the cutting edge of data-driven politics, and your support will help Democratic candidates and progressive organizations conduct successful advocacy and electoral campaigns.\n\nThis position reports to the Deputy Chief Data Officer. The Data Engineer is a part of a growing Data team that supports all underlying work at Catalist.\n\nThis position is included in our CWA bargaining unit.\n\nPrinciple Duties & Responsibilities\n\nDevelop scalable, production processes, code, and systems for data ingest, transformation, modeling, and reporting across a variety of platforms\nTranslate mock-ups and designs into functional processes, code, and systems\nCreate architecture designs when needed\nProvide quality assurance and testing on processes, code, systems, and products\nBecome an internal subject matter expert on various datasets and support other Catalist departments/teams on usage of those datasets\nExecute ad-hoc data and database maintenance tasks as requiredProject manage cross-departmental efforts, with direct responsibility for stakeholder engagement, management, and execution of technical elements\n\n\nRequirements\n\nBS or BA in a technical field, or relevant experience\n1-2 years of experience working with SQL databases\nExperience with data cleaning or ETL processes\nExperience with distributed computing systems and/or distributed datastores (particularly the Hadoop ecosystem)\nExperience managing projects\nFamiliarity with Catalist data, progressive politics, voter files, and/or commercial data\nBackground check required\n\n\nPreferred Skills & Abilities\n\nWillingness to be a problem solver and produce results in a fast paced environment\nAbility to be creative and personable, and articulate ideas clearly\nExperience working with SQL databases\nProficiency with Python or another object-oriented programming language (R, Java, Scala, etc\u2026)\nExperience working in cloud environments (AWS, GCP, etc.)\nExperience working in command line environments such as Bash\nExperience with a version control tool such as git or github\n\n\nBenefits\n\nMedical, Dental, Vision, Prescription Drug\n\nCatalist offers Medical, Dental, Vision, and Prescription Drug coverage for eligible staff and their eligible dependents. Catalist\u2019s Medical plan is a comprehensive PPO program including Prescription Drug coverage with 85% of the premium paid by Catalist. Dental and Vision coverage is provided at no cost to employees.\n\nGroup Term Life Insurance and Long-Term & Short-Term Disability Coverage\n\nGroup Term Life Insurance and Long-Term and Short-Term Disability coverage is available for eligible staff. These benefits are provided at no cost to Catalist employees.\n\n401(k) Safe Harbor Plan\n\nA 401(k) Safe Harbor Plan is available to eligible staff with a 3% contribution from Catalist from the date of hire. Employees may contribute pre-tax or post-tax from their salary up to the legal limits set forth by the IRS.\n\nMedical and Dependent Care Flexible Spending Accounts (FSAs)\n\nCatalist offers an FSA Program that gives eligible staff the ability to pay out-of-pocket medical/dental/vision/child care expenses from pre-tax earnings.\n\nTransit Benefits\n\nCatalist also makes available a Transit benefit FSA program to eligible employees using pre-tax contributions with a company match.\n\nProfessional Development and Remote Work Expenses\n\nEligible employees may be reimbursed up to $750 each year for professional development / education and remote work expenses.\n\nStudent Loan PayDown or SaveUp\n\nCatalist offers a Student Loan PayDown and College SaveUp benefit for eligible staff.\n\nVacation, Personal Leave, Sick Leave Benefits\n\nCatalist offers generous vacation benefits to all eligible staff. Eligible employees also receive:\n\n 14 Paid Holidays\n Personal Days\n Sick Leave\n Parental Leave\n\n\nHybrid Office/Remote Work\n\nCertain positions at Catalist are eligible for Office/Remote Hybrid or full Remote status."}, {"title": "Remote Entry Level Data Analyst/Engineer", "company_name": "SynergisticIT", "location": "White Plains, NY (Remote)", "link": "/jobs/view/3774171423/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=q54sk1%2FRww0z%2BC%2BE8e0jQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774171423", "description": "About the job\nAt SynergisticIT, we're all about making connections. Whatever IT goals you have, our software programmers can help achieve those. Our Software development teams can take up turnkey projects and execute them in an effective and efficient manner. If you are looking to source talent our recruiters will find the ideal IT talent for your company. What's the secret to our success? Well, it all starts with taking quality time to listen to each client's specific needs. After we have a thorough grasp of your IT goals, we can better customize our Developments as per your specific needs. We can also tailor make recruiting programs to exceed your expectations. Since our founding in 2010, SynergisticIT's strategies have earned the company an enviable position in the software development, IT staffing and IT skill enhancement fields. SynergisticIT continues to work with hundreds of satisfied American clients with our software programmers working on our projects and after gaining hands on experience on cutting edge technologies moving to contribute their skills to great clients like Apple, Google, Client, Ebay, Paypal, Kroger, the Walt Disney Company and hundreds more. If you are tired of working with inefficient programmers who take a lot of time to ramp up we want you to try us. Our software programmers can hit the ground running and get you the maximum return on your investment. You have already tried the rest its time you tried the best. SynergisticIT - Home of the Best Data Scientists and Software Programmers in the Bay Area.\n\n Why Us ? \n\nSynergisticIT has a proven track record of successfully skill enhancement and staffing IT employees for some of the world's most iconic brands. Our team takes the time to fully understand every client's needs so we could best meet your IT staffing requirements. The knowledgeable staff at SynergisticIT is always more than happy to work with clients to ensure they reach their software development goals. Besides staffing, SynergisticIT is also committed to helping young IT professionals advance their career with a robust upskill program . Everyone who goes through SynergisticIT's program learns all the skills necessary to succeed in many IT fields ranging from Java to Machine Learning. Additionally, everyone trained at SynergisticIT has been through extensive mock and technical interview screenings to bolster their career prospects. Last, but certainly not least, SynergisticIT takes great care to respect the privacy considerations for every client. All companies who work with SynergisticIT can rest assured their confidential data is protected using the most up-to-date encryption technologies. SynergisticIT also complies with all the latest NDA agreements.\n\n REQUIRED SKILLS For Java /Software Programmers \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Time series analysis \n\n We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2022 and at Gartner Data Analytics Summit (Florida)-2023 \n\nOracle CloudWorld Event (OCW) Las Vegas 2022 | SynergisticIT - YouTube\n\n https://www.youtube.com/watch?v=OAFOhcGy9Z8 \n\n https://www.youtube.com/watch?v=EmO7NrWHkLM \n\n https://www.youtube.com/watch?v=NVBU9RYZ6UI \n\n https://www.youtube.com/watch?v=Yy74yvjatVg \n\nSynergisticIT at Gartner Data and Analytics Summit 2023 - YouTube\n\nFor preparing for interviews please visit  https://www.synergisticit.com/interview-questions/ \n\n We are looking for the right matching candidates for our clients \n\n Please apply via the job posting \n\n REQUIRED SKILLS For Java /Full Stack/Software Programmer \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning Positions \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Tensorflow \n\n If you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team. \n\n No phone calls please.  Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates"}, {"title": "Data Engineer", "company_name": "Anika Systems", "location": "Leesburg, VA (Remote)", "link": "/jobs/view/3787766893/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=mApL9JnN%2B%2Fr2%2BYUPlxRYOA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787766893", "description": "About the job\nAnika Systems is a fast growing, woman-owned small business that specializes in providing innovative IT solutions for federal government agencies. Our expertise lies in accelerating delivery in Data and Analytics, Intelligent Automation, Application Development, and IT Modernization. We are currently expanding our Federal team and are seeking a passionate and talented Data Engineer. This opportunity is 100% remote.\n\nMust be a U.S. Citizen with the ability to obtain and maintain a government suitability clearance\n\nResponsibilities\n\nDesigns, develops, builds, analyzes, evaluates, and installs database management systems to include database modeling and design, relational database architecture, metadata, and repository creation and configuration management.\nDefines and oversees database organizations, standards, controls, procedures, and documentation. Provides technical consulting in the definition, design, and creation of a database environment.\nAdvises applications development staff and users on data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nDesigns and implements databases with respect to access methods, access time, batch processes, device allocation, validation checks, organization, protection and security, documentation, and statistical methods.\nUses data mapping, data mining, and data transformational analysis tools to design and develop databases.\nDetermines data storage and optimum storage requirements.\nPrepares system requirements, source analysis, and process analyses and designs throughout the database implementation.\n\nRequired Skills And Experience\n\nBA/BS and 1 year of relevant experience\nExperience with DataBricks, SQL, Python\nApplications development experience with data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nExcellent communication skills (both oral and written) with process-oriented organizational skills to ensure project success\n\nDesired Skills And Experience\n\nDemonstrated successful development experience with the full life cycle of government database management systems including database modeling and design, relational database architecture, metadata and repository creation, and configuration management.\n\nPowered by JazzHR\n\nTlKWIorucJ"}, {"title": "Remote Work - Need Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3774712978/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=r3Bp%2F0qcOD2OpC5jpbUOcA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774712978", "description": "About the job\nTitle : Data Engineer\n\nLocation : Remote\n\nDuration : 6+ months to start (long term contract (work order are extended 2x year))\n\nPlease Fill out the skill matrix: Technology Some Awareness (1) Novice (2) Intermediate (3) Advanced (4) Expert (5) SQL Tableau Desktop /Creator Tableau API Integration AWS - S3 Glue AWS - Athena Redshift AWS - Lambda AWS - SNS/SQS msExcel\n\nProfile: Data Analyst will work closely with Architects and leads in the Interflow team, key business stakeholders including program staff to support the organizations effort in the areas of data Analysis and information management. Role should have strong experience in writing SQL queries to extract data for reports. Candidate should have experience working with large set of data. As these reports are very critical for the company and we are looking for an expert who is capable of handling, consuming and extracting large sets of data for analysis and reports.\n\nMust Haves\n\nAWS - Athena\nAWS - S3\nGlue\nhands-on development of reporting applications\nMS Excel\nRedShift\nSQL\nTableau Desktop /Creator\n\nNice\n\nAWS - Lambda\nAWS - SNS/SQS\nTableau API Integration\n\nSkills\n\nBA/BS required (major in an analytical field desired) \nA minimum 10+ years work-related experience. \nExperience in hands-on development of reporting applications for the Web in a professional environment \nThe Ethos of continuous improvement and interest in learning new things.\nStrong analytical thinking and structured problem-solving ability\nAbility to handle multiple projects and assignments simultaneously and effectively in a cross-functional team environment. \nVersed on the agile methodology and best practices. \nExcellent interpersonal and collaboration skill with the ability to work with a diverse set of colleagues, across functions, from different organizations, disciplines, etc. \nSelf-starter, ability to set priorities, work independently and attain goals"}, {"title": "Data Engineer", "company_name": "Team Remotely Inc", "location": "Houston, TX (Remote)", "link": "/jobs/view/3786249829/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=78lY5Jo%2FfyRPC6TQYCdvww%3D%3D&trk=flagship3_search_srp_jobs", "id": "3786249829", "description": "About the job\nThis is a remote position.\n\n Data Engineer (US/Canada Residents Only, 1 year experience, remote)\n\nTeam Remotely Inc. is a staffing and recruitment agency that offers a comprehensive solution for talent acquisition, including sourcing, vetting, pay rolling, and managing talent. Whether you need contract staffing, direct hire, direct sourcing, talent pools, or diversity initiatives, our model can support your hiring strategy.\n\n Hiring Type: Full-Time\n\n Base Salary: $55K-$70K Per Annum.\n\nResponsibilities\n\nDesigning user interface changes for web-based DB applications. \nReviewing application requirements and interface designs. \nDeveloping and implementing highly responsive user interface components using react concepts. \nWriting application interface codes using JavaScript following react.js workflows. \nTroubleshooting interface software and debugging application codes. \nDeveloping and implementing front-end architecture to support user interface concepts. \nMonitoring and improving front-end performance. \n\nQualifications And Experience\n\nBachelor\u2019s degree in computer science, information technology, or a similar field. \nPrevious experience working as a react.js developer. \nIn-depth knowledge of JavaScript, CSS, HTML, jQuery, and front-end languages. \nKnowledge of REACT tools including React.js, Webpack, Enzyme, Redux, and Flux. \nExperience with user interface design. \nKnowledge of performance testing frameworks, including Mocha and Jest. \nExperience with browser-based debugging and performance testing software. \nTroubleshooting skills. \nProject management skills. \nProblem-solving skills. \nVerbal communication skills. \n\n Why work with Team Remotely?\n\nTeam Remotely Inc. is a staffing platform offering a seamless experience for employers and candidates. Employers can post job openings and specify their requirements, while candidates can create profiles and upload resumes.\n\nThe team of Team Remotely continuously learns and adapts based on previous successful placements, constantly improving its matching capabilities. This ensures that the recommendations provided by Team Remotely are tailored and accurate, increasing the likelihood of a successful match between employers and candidates. By providing intelligent and data-driven solutions, they strive to enhance the efficiency and effectiveness of the hiring process, ultimately helping companies find the best talent and individuals find their dream jobs."}, {"title": "Data Engineer", "company_name": "ZAG Technical Services", "location": "United States (Remote)", "link": "/jobs/view/3771403464/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=Hf0VefrKXBFIL5MtHmY0Pw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3771403464", "description": "About the job\nWe are looking for a tech savvy Data Engineer to contribute to the building, operating, and scaling of next generation data platforms and tools that will power data-driven analytics capabilities throughout the ZAG products and services portfolio, spanning areas such as business intelligence, reporting, and data analytics.\n\nThe ideal team member is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. Must be comfortable supporting the data needs of multiple clients and teams, systems, and products within the modernization of ecosystems. The right-fit person will be excited by the prospect of optimizing or even re-designing data processes to support next generation of advanced analytics capabilities, which in-turn translates to implementing strategic enhancements to maintain competitive advantage in the industry.\n\nThe successful team member will leverage their proficiency to\u2026\n\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\nBuild the analytics processes required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Data Management and cloud based \u2018big data\u2019 technologies.\nBuild analytics tools that utilize technologies to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.\nCollaborate with client teams to identify and solution regional and local requirements within the context of the standard global business model for enterprise analytics.\nParticipates in meetings, discussions, strategy sessions where changes, improvements, and enhancements are proposed, evaluated, and approved. \n\n\nThe exceptional team member will have or demonstrate progressive experiences in\u2026 \n\nBachelor\u2019s degree or higher in Computer Science or related field\nExperience working with BI and data warehousing tools\nExperience working with enterprise data, where security is paramount and data governance is critical\nAbility to work independently and in a team environment, and effectively engage all levels of the organization\nUnderstanding of distributed systems driving large-scale data processing and analytics with a successful history of manipulating, processing, and extracting value from large, disconnected datasets.\nAbility to communicate effectively (listening, presenting, and questioning)\nStrong organizational, written, and communication skills\n\n\nAbout ZAG\n\nZAG Technical Services empowers our clients\u2019 success through intelligent business solutions that increase employee productivity, network security and team collaboration. We aim to ensure that our client\u2019s IT investments drive ROI, provide a competitive advantage and help businesses grow. We do this by living our core set of values: Integrity Foremost, Client-Centered, Accountable Always, Teamwork Throughout, and Exceptional is the Goal.\n\nWhat ZAG Has To Offer\n\nCompetitive compensation package \nBenefits including medical, dental, vision, 401K and life insurance \n\n\nZAG Technical Services is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all of our employees.\n\nZAG Technical Services is an E-Verify participant and will provide the Social Security Administration (SSA) and, if necessary, the Department of Homeland Security (DHS), with information from each new employee\u2019s Form I-9 to confirm work authorization."}, {"title": "Junior Data Engineer", "company_name": "HireMeFast LLC - Career Accelerator - Land A Job", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3786249797/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D&trk=flagship3_search_srp_jobs&lipi=urn%3Ali%3Apage%3Ad_flagship3_search_srp_jobs%3BQExogrHBQoSF9zZm%2B9kufw%3D%3D&lici=WZSsp%2Bl028W7hbWJh7zvCQ%3D%3D", "id": "3786249797", "description": "About the job\nThis is a remote position.\n\n Job Title: Junior Data Engineer\n\n Employment Type: Full-Time\n\n Salary: $64,000 - $76,000 per annum\n\n Experience Required:  Minimum 1 year of project experience\n\nAbout us: HireMeFast is a leading staffing and recruitment agency specializing in connecting businesses with top-tier talent across various industries. Our mission is to bridge the gap between exceptional candidates and organizations needing their skills, expertise, and unique qualities. Our team of experienced and dedicated recruitment specialists utilizes innovative sourcing strategies, and a vast network to identify and attract top talent. We conduct comprehensive procedures to ensure that only the most qualified candidates are presented to our clients.\n\nPosition Summary\n\nJoin the fast-paced, innovative, and collaborative environment focused on providing an AIOps platform that enhances the intelligence of the CVS Health infrastructure. Work closely with subject matter experts and colleagues to build and scale out machine learning and AI solutions that will detect, predict, and recommend solutions to correct issues before system impact and enhance efficiency, reliability, and performance of CVS Health\u2019s IT operations.\n\nKey Responsibilities include:\n\nData pipeline development: Designed, implemented, and managed data pipelines for extracting, transforming, and loading data from various sources into data lakes for processing, analytics, and correlation. \nData modeling: Create and maintain data models ensuring data quality, scalability, and efficiency \nDevelop and automate processes to clean, transform, and prepare data for analytics, ensuring data accuracy and consistency \nData Integration: Integrate data from disparate sources, both structured and unstructured to provide a unified view of key infrastructure platform and application data \nUtilize big data technologies such as Kafka to process and analyze large volumes of data efficiently \nImplement data security measures to protect sensitive information and ensure compliance with data and privacy regulation \nCreate/maintain documentation for data processes, data flows, and system configurations \nPerformance Optimization- Monitor and optimize data pipelines and systems for performance, scalability and cost-effectiveness \n\nCharacteristics of this role:\n\nTeam Player: Willing to teach, share knowledge, and work with others to make the team successful. \nCommunication: Exceptional verbal, written, organizational, presentation, and communication skills. \nCreativity: Ability to take written and verbal requirements and come up with other innovative ideas. \nAttention to detail: Systematically and accurately research future solutions and current problems. \nStrong work ethic: The innate drive to do work extremely well. \nPassion: A drive to deliver better products and services than expected to customers. \n\nRequired Qualifications\n\n2+ years of programming experience in languages such as Python, Java, SQL \n2+ years of experience with ETL tools and database management (relational, non-relational) \n2+ years of experience in data modeling techniques and tools to design efficient scalable data structures \nSkills in data quality assessment, data cleansing, and data validation \n\nQualifications\n\nKnowledge of big data technologies and cloud platforms \nExperience with technologies like PySpark, Databricks, and Azure Synapse. \n\nEducation\n\nBachelor\u2019s degree in Computer Science, Information Technology, or related field, or equivalent working experience\n\nWhy HireMeFast LLC?\n\nAt HireMeFast, we understand that finding the right individuals to join your team is crucial for success and growth of your organization. We are committed to streamlining the hiring process for our clients, ensuring they have access to a diverse pool of highly qualified candidates who are a perfect fit for their specific needs."}, {"title": "Data Engineer  - Need candidates from Texas ( REMOTE )", "company_name": "Pulivarthi Group (PG)", "location": "United States (Remote)", "link": "/jobs/view/3770685212/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=9gdMwuif%2BpB%2BCQYvQi3imQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3770685212", "description": "About the job\nFollow us on Linkedin: https://www.linkedin.com/company/pulivarthigroup/\n\nPulivarthi Group LLC is a Global Staffing & IT Technology Solutions company, with our prime focus of providing world class solutions to our customers with the right talent. We combine the expertise of our team and the culture of your company to help you with the solution that is affordable and innovative using high quality standards and technologies.\n\nWe\u2019ve served some of the largest healthcare, financial services, and government entities in the U.S.\n\nData Engineer - FT only - Azure SQL with data lakes experience\n\nData Engineer -\n\nthe resource requirement for this Data Engineer role needed more clarification.\n\nOur customer is requesting a resource who knows Azure CI/CD (Continuous Integration / Continuous Deployment) to build and integrate their DevOps pipeline.\n\nOnce the DevOps pipelines are mapped, they need the Data Engineer to connect to all the Data Sources, primarily their Data Lake.\n\nThe Customer (based in Houston) is interested in a resource in a time zone not too far removed from the Central Time Zone to allow real time collaboration."}, {"title": "Data Engineer", "company_name": "Otter Products", "location": "Fort Collins, CO (Remote)", "link": "/jobs/view/3766728302/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=77U3U7CAi1PxgDmpI3%2BiNQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3766728302", "description": "About the job\nOverview \n\nOtter Products is currently recruiting for a Data Engineer to join our Data team! You can be based in our Fort Collins, CO office with flexibility to work a portion of your time remotely, or 100% Remote in the U.S.\n\nThe Data Engineer applies professional experience, concepts, and company objectives toward building and operationalizing the minimally inclusive data necessary for the enterprise data initiatives following industry standard practices and tools. The bulk of the Data Engineer's work would be in designing, managing and optimizing data pipelines, then moving these data pipelines effectively into production for key data and analytics consumers like business/data analysts, data scientists or other stakeholder that needs curated data for data and analytics use cases across the enterprise. The Data Engineer also ensures compliance with data governance and data security requirements while creating, improving and operationalizing these integrated and reusable data pipelines to enable faster data access, integrated data reuse and vastly improved time-to-solution for data and analytics initiatives. Additionally, the Data Engineer will be expected to collaborate with other data team members and data consumers working on the models and algorithms developed by them toward optimization for data quality, security, and governance to put them into production leading to potentially large productivity gains.\n\n About Otter Products \n\nOtter Products, we grow to give. From our founder's garage in 1998 to the global technology leader we are today, Otter Products continues to drive growth through innovation.\n\nThrough our industry-leading brands - OtterBox, Liviri and OtterCares - we provide our partners the number-one selling and most trusted products in our categories. Our philanthropic spirit is the foundation on which we foster our partner relationships, allowing us to grow and to give - together.\n\nBy way of our charitable arm, the OtterCares Foundation, we support our communities and invest in our future through education that inspires kids to change the world.\n\nAnd even as our global community of Otters continues to grow, our founder's core values are still at the heart of everything we do. We measure our success by our ability to give back to our communities and strengthen opportunities for all.\n\nFor more information visit otterproducts.com\n\n Responsibilities \n\nComplete analysis, design and development of BI solutions using Azure SQL Server\nFamiliarity with Data Warehouse concepts\nExperience coding the extraction, transformation, and loading (ETL) processes\nDatabase development primarily in SSIS, Data Factory and SQL\nPartner with the business to determine end user database/reporting needs and requirements\nGenerate ad hoc reports using Power BI or SSRS\nCollaborate with other developers to create and implement best approach solution(s)\nTroubleshoot Azure tools, systems, and software\nReview queries for performance issues, making changes as needed\nCollaborate with team to performance-tune Azure application as necessary\nAssist with the analysis and extraction of relevant information from large amounts of historical business data to feed data science initiatives\nParticipate and support the design and documentation of processes for large scale data analyses, model development, model validation and model implementation\nSupport and maintain a positive safety culture by following all safety policies and procedures and actively contributing to a safe working environment\nOther duties as required\n\n Qualifications \n\nBachelor's degree required. Degree in Computer Science or Mathematics preferred\nMinimum of four years of experience in an IT or analytical role required. Experience in database development, report writing and/or statistics preferred.\n\n EEO \n\nOtter Products, LLC is an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, age, marital status, pregnancy, sex, sexual orientation, gender, gender identity or expression, national origin, disability, veteran status, or any other characteristic or status protected by law.\n\n For US Based Roles Only - Compensation Range Minimum \n\nUSD $90,000.00/Yr.\n\n For US Based Roles Only - Compensation Range Maximum \n\nUSD $120,000.00/Yr.\n\n Additional Total Rewards \n\nProfit Sharing Program Eligible, Benefits Eligible - Full Time- check out otterproducts.com/careers/why for more info"}, {"title": "Software Engineer, Distributed Systems", "company_name": "Hightouch", "location": "United States (Remote)", "link": "/jobs/view/3628572539/?eBP=CwEAAAGMd4QO8APODyE_-mMOPhYyQGg5ZjVbPwq_3V7DK0B2bLAf10pcedM9S2T71X5i4TiJuEwOzWBbSiTKbLuk-VD-_Y-dN5uyf8YpYkHYqEYFxySMS9zr2hRjNrxX9gXbxssh35TFK9YTNV2WeH2Ncb675JgFbJTgNWSZySFC6rrdJGmdSDTr2q5d3VoxORz3GjsDbzjeyZQCvQ07RqYw7oujWSgslycs2LMCY_-N0NEj6e0PvbaYKiZ_5IW34bGVSYLXpHwqa8oOcNttYZ90UA3E_7sDSFzABdIVlfrp5zzERrIYy5sh2MIInxHdDoKgQw8VmOn5uM4S0tXUl-i7LnN0_W0NfPBmLiVDNuveIOm4MfDNPxfe5QKPk6FUAVpvgbcoJn9yRyE&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=rph%2FT6Q6Vkp6YoggJf9cJQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3628572539", "description": "About the job\nAbout Hightouch\n\nHightouch\u2019s mission is to empower everyone to take action on their data. Through our Reverse ETL platform, business and data users can seamlessly sync data from where it resides, such as warehouses and databases, to where it is needed, including operational systems and SaaS tools. Traditionally, acting on data has required engineering time and bandwidth, and left most business users stuck with charts and reports that are unable to take automated action on their data. With Hightouch, every business user, without writing any code, can activate data to streamline critical processes, improve marketing performance, and scale operations.\n\nOur team operates with a focus on making a meaningful impact for our customers. We believe in approaching challenges with a first principles thinking mindset, moving quickly and embracing our value of efficient execution, and treating each other with compassion and kindness. We look for team members that are strong communicators, have a growth mindset, and are motivated and persistent in achieving our goals.\n\nHundreds of companies use Hightouch, including Spotify, Ramp, Retool, NBA, Plaid, and Betterment. We\u2019re based in San Francisco, are remote-friendly, and backed by leading investors such as Amplify Partners, ICONIQ Growth, Bain Capital Ventures, Y-Combinator, and Afore Capital.\n\nAbout The Role\n\nWe are looking for a distributed systems engineer to work on the systems that power the syncing engine our customers and other engineering teams rely on. This presents an exciting challenge where you can apply your expertise in distributed systems, performance optimization, and troubleshooting to push the boundaries of what is possible and meaningful improve our ability to move massive amounts of customer data. This role also provides a unique opportunity to work on a multi-cloud and multi-region infrastructure that supports a global customer base.\n\nWe believe in enabling our engineers to do their best work for our customers by giving them extremely high levels of ownership and autonomy. This comes in different forms: you will own and deliver projects from start to finish, you will work directly with customers to solve their hardest scaling problems, and you will have a lot of influence over what we work on as a team and company.\n\nSome of the problems we\u2019ll be working on include:\n\nSync Speed: Customers want to sync a lot of data to important destinations like Facebook and Snapchat, which requires us to analyze every part of our syncing process and find where we can optimize to sync data more quickly\nStreaming Syncing: We currently sync data from sources in batch only, but envision a world where we also do syncs in real-time and support streaming sources like webhooks and queues\nScalability and Reliability: As part of our rapid growth, we\u2019re always evaluating current future bottlenecks in scaling and reliability, and architecting for the next order of magnitude of growth\nPersonalization API: Our Personalization API product provides a low latency caching layer on top of data warehouses that enable customers to use their data warehouse for real-time personalization use cases with \nMulti-Region and Multi-Cloud: Supporting our multi-region and multi-cloud backend, including extending it to launch Hightouch on in new regions to support data residency requirements of our global customer base\n\nWe are looking for talented, intellectually curious, and motivated individuals who are interested in tackling the problems above. This is a senior role, but we focus on impact and potential for growth more than years of experience. The salary range for this position is $170,000 - $240,000 USD per year, which is location independent in accordance with our remote-first policy.\n\nAbout You\n\nYou are an engineer with a passion for solving hard technical problems that generate real value for customers. You\u2019re motivated by high ownership and are comfortable in a fast-paced, startup environment.\n\nYou have experience and comfort with distributed systems and high-scale systems or the ability and desire to build this experience quickly. This experience can take different forms:\n\nYou\u2019ve built syncing engines that process and move terabytes of data per day in a scalable way\nYou\u2019ve created event collection or data streaming services that power company critical systems\nYou\u2019ve owned systems that have undergone orders of magnitude of growth and helped them scale accordingly\nYou have relevant skills and are a fast learner who is excited to take on the challenges we face\n\nInterview Process\n\nOur goal with the interview process is to balance speed with giving both parties opportunities to assess whether there is a strong mutual fit. We will ask you questions, but we want you to ask us questions! Our technical interviews focus on how you design systems because we believe this is the best way for us to see how you work and for you to see how we collaborate. We don\u2019t ask you to write code to solve technical brainteasers that don\u2019t appear in your day to day job.\n\nRecruiter screen [30m]: Introductory call with our recruiting team to get to know each other and see if the role could be a good mutual fit.\nSystems design screen [45m]: Designing a data processing feature end-to-end.\nHiring manager interview [30m]: Chat with hiring manager about past experiences and future operating preferences to assess fit on company values and operating principles.\nSystems design interview [90m]: Work with the interviewer to architect a system at a conceptual level. The problem will be at a pretty high level - and have both product and customer requirements as well as technical."}, {"title": "Entry-Level Data Scientist Engineer", "company_name": "Phoenix Recruitment LLC", "location": "Texas City, TX (Remote)", "link": "/jobs/view/3788727625/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=8iwtytHsCu4wMwJ4fj3pxw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3788727625", "description": "About the job\nThis is a remote position.\n\nTitle - Entry-Level Data Scientist Engineer, 1 year of project experience\n\nEmployment Type: Full-time\n\nBase Salary: $60K-$70K\n\nPhoenix Recruitment offers a variety of recruiting services to assist both employers and employees. They specialize in marketing open positions, recruiting, and helping employers to find qualified candidates across various industries. Phoenix Recruitment has expertise in streamlining the hiring process. They can help ensure that the process is efficient, well organized, and compliant with relevant regulations.\n\nSkills and Abilities:\n\nStrong knowledge of R or Python for data analysis and modeling. \nProficiency in statistical programs such as R, SAS, MATLAB, or Python. \nFamiliarity with spreadsheets (VBA) and database applications (Access, Oracle, SQL, or equivalent technology). \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \nAbility to learn new methods quickly and work under deadlines. \nExcellent teamwork and communication skills. \nStrong analytical and problem-solving abilities. \nBasic understanding of SQL, Javascript, XML, JSON, and HTML. \n\nPreferred:\n\nKnowledge of actuarial concepts and life, health, and/or annuity products. \nExperience with statistical modeling techniques such as GLM, Decision Trees, Time Series, Regression, etc. \nFamiliarity with Microsoft DeployR. \nExposure to insurance risk analysis. \nBasic experience in computational finance, econometrics, statistics, and math. \nKnowledge of SQL and VBA. \nFamiliarity with R or Python for predictive modeling \n\nWhy Phoenix Recruitment LLC?\n\nPhoenix Recruitment often has an extensive network of employers and candidates. This network allows them to tap into a pool of qualified candidates and connect them with suitable job opportunities. They can also leverage their connections to help employers find the right talent efficiently. Outsourcing the recruitment process to a specialized agency can save your time and resources, avoid delays, reduce administrative burdens, and increase the chances of finding the right fit for your organization."}, {"title": "Data Engineer - Remote Work", "company_name": "BairesDev", "location": "United States (Remote)", "link": "/jobs/view/3783162297/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=OtoCrAAHZFomVup04fy35g%3D%3D&trk=flagship3_search_srp_jobs", "id": "3783162297", "description": "About the job\nWho We are\n\nBairesDev is proud to be the fastest-growing company in America. With people in five continents and world-class clients, we are only as strong as the multicultural teams at the heart of our business. To consistently deliver the highest quality solutions to our clients, we only hire the Top 1% of the best talents and nurture their professional growth on exciting projects.\n\nWe are looking for a Data Engineer to join our Development team and participate in different projects made up of multicultural teams distributed throughout the world.\n\nWhat You Will Do\n\n Interview other Data Scientists and Data engineers to vet their technical abilities.\n Input data from those interviews into the product and help develop a predictive, fair, and enjoyable solution.\n Work together with developers, tech leads, and solution architects to build applications.\n Improve existing structures by adding new functionalities or proposing technological updates so that as a result, the impact of your contribution is significant in the core of each business.\n Improve the user experience on scalable and high availability platforms, contributing to the key differential of each business.\n\nHere\u2019s What We Are Looking For\n\n 5+ years of experience working as a developer.\n 3+ years of experience working as a Data Scientist / Data Engineer.\n Proficient with analysis, troubleshooting, and problem-solving.\n Hands-on experience with managing data loads and data quality.\n Advanced English level.\n\nHow we do make your work (and your life) easier:\n\n 100% remote work.\n Hardware setup for you to work from home.\n Flexible hours - make your schedule.\n Paid parental leave, vacation & holidays.\n Diverse and multicultural work environment.\n An innovative environment with the structure and resources of a leading multinational.\n Excellent compensation \u2014 well above the market average.\n Here you can grow at the speed of your learning curve.\n\nOur people work remotely but with a consistent and robust culture that promotes diversity and teamwork. To continue being the leading software development company in Latin America, we want to ensure that every BairesDev member gets the best growth and professional development opportunities in a diverse, welcoming, and innovative environment.\n\nEvery BairesDev team member brings something unique to our company.\n\nWe want to hear your story. Apply now!"}, {"title": "Data Engineer (Remote)", "company_name": "Cascade Debt", "location": "United States (Remote)", "link": "/jobs/view/3755336571/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=pmUtn1j2zyPr25CNPh2RNw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3755336571", "description": "About the job\nDo you want to work in a small team where you can make a real difference in a company? Would you embrace the challenge of building a high impact platform that is used globally? Then we want to speak with you!\n\nAbout Cascade\n\nCascade is a fintech startup backed by Canadian and US investors that empowers companies to grow by democratizing access to institutional debt. We are building tools that modernize how companies raise and manage debt, lowering the barriers to entry to the $7 trillion specialty finance and alternative credit market for companies around the world. Founded just last year, we are gearing up for our next phase of technical development and are seeking a talented Data Engineer to lead the way.\n\nAbout The Role\n\nAs a Data Engineer you\u2019ll be an early member of a growing team building a pioneering platform in the debt infrastructure space. You will design, build, and launch efficient, scalable, and reliable data pipelines to move and transform data .\n\nWhat You\u2019ll Do\n\n Building databases, data lakes and data ingestion pipelines to integrate customer databases and datasets to our systems \n Ingesting financial datasets from external customers, then u pdating and maintaining accurate and complete data mappings to ensure that our product s are displaying high quality data \n M onitoring and alerting across our data pipelines in order to make sure that our data ingests are reliable and correct \n Translat ing business goals and stakeholder requirements into new data flows and deliver analytical insights ensuring that the data flows create real business value continuously \n\nSkills And Experience\n\n 2 + years experience in a data engineering or data analyst role \n E xperience with relational SQL and NoSQL databases, including PostgreSQL , MySQL, MariaDB, MongoDB, Bigtable, etc. \n E xperience working with ETL technologies , such as Databricks, Fivetran , or dbt \n Proficiency in writing SQL queries and knowledge of analytical data warehouses such as RedShift, BigQuery , and Snowflake \n Some experience with CI/CD automation such as GitHub Actions \n Developing APIs and integrating with 3rd party APIs \n Bonus: experience in fintech / SaaS / credit analysis \n\nPeople Who Thrive At Cascade Are\n\n Self-starters, who take the initiative to tackle challenges in a remote work environment \n Good communicators, who can operate without ego to discuss, learn, grow, and help others do the same \n Passionate about creating great digital experiences for users \n Problem solvers who are not satisfied with the status quo \n\nBenefits\n\nRemote: we are a remote-first company and are very flexible on hours as long as things get done \nHome-office: there\u2019s a $1000 USD home office allowance to set yourself up \nEquity: we expect you to have an owner-mentality, and have the equity plan to match \nBenefits: health, dental, vision, and more \nPerks: we offer free lunches weekly and off-site trips \nJob satisfaction: we offer autonomy, ample opportunities for mastery, and an opportunity to make a difference for companies around the world"}, {"title": "Data Engineer", "company_name": "ISC (Integrated Specialty Coverages, LLC)", "location": "Carlsbad, CA (Remote)", "link": "/jobs/view/3778293528/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ew7%2FfEuXSujs5IGabdqUyw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3778293528", "description": "About the job\nAbout Integrated Specialty Coverages\n\nIntegrated Specialty Coverages LLC (ISC) is a growth stage technology and data-driven insurance company leading innovation in the market.\n\nBacked by one of the leading private equity firms, KKR, and led by a forward-thinking management team, ISC is combining the worlds of insurance and technology to create an Insurtech powerhouse. As a leading online distributor of insurance products for a range of industries and \"Main Street USA\", we are looking for the right people to help us in our mission of achieving exponential growth. We strive to be the number one place to go for brokers and agents to source insurance. To accomplish this, we're building a digitally focused team that deeply understands the intersection between user experience, data, and AI/ML to optimize the way we engage with our customers and partners.\n\nJob Summary\n\nThe Data Engineer role will be the technical liaison between multiple groups including a data science team, the engineering team, product management, and business stakeholders. You do not need any insurance knowledge prior, however, you must quickly dive deep into the insurance world and ask questions to become a subject matter expert. You will be responsible for building a data platform to facilitate the data science team. You must be a self-starter that can build out features such as a data pipeline from scratch. There will be support from both engineering and data science for any buildout.\n\nJob Responsibilities\n\nCreate and maintain optimal data pipeline architecture. \nAssemble large, complex data sets that meet functional / non-functional business requirements. \nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. \nBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using AWS technologies, SQL, and Python. \nWork with stakeholders including the Executive, Product, Data Science, and Engineering teams to assist with data-related technical issues and support their data infrastructure needs. \nWork with data science and analytics teams to strive for greater functionality in our data systems. \n\nMinimum Qualifications:\n\nAdvanced working SQL knowledge and experience working with relational databases, strong query authoring (SQL) as well as working familiarity with a variety of databases (Snowflake, Redshift, MySQL, MSSQL, etc.)\nExperience building and optimizing data pipelines, architecture and data sets. \nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. \nBuild processes supporting data transformation, data structures, metadata, dependency, and workload management. \nA successful history of transforming, processing and extracting value from large disconnected, datasets from a variety of data sources (Flat files, Excel, databases, etc.)\nStrong analytic skills related to working with unstructured datasets. \nStrong project management and organizational skills. \nExperience supporting and working with cross-functional teams in a dynamic environment. \nAbility to mentor/guide/collaborate with other team members. \n\nWe are looking for a candidate with 3-5 years of experience in a Data Engineer role. They should also have experience using the following software/tools:\n\nExperience with relational and MPP databases, including Snowflake and MySQL. \nExperience developing software in an agile environment from the requirements stage to production. \nExperience with version control: git\nExperience with container technologies: Docker\nExperience with data pipeline and workflow management tools: Airflow, Jenkins, AWS Glue, Azkaban, Luigi, etc. \nExperience with AWS cloud services: EC2, ECS, Batch, S3, EMR, RDS, Redshift\nExperience with other cloud services: Snowflake, Airflow\nExperience with object-oriented/object function scripting languages: Python, Java, C++, etc. \nExperience with data modeling and data warehouse design\nExperience with data visualization tools (PowerBI, QuickSight)\n\nThe starting annual pay scale for this position is listed below. Actual starting pay will be based on factors such as skills, qualifications, training, and experience. In addition, the company offers comprehensive benefits including medical, dental and vision insurance, 401(k) plan with match, paid time off, and other benefits.\n\nISC's salary ranges are determined by role and level. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations and could be higher or lower based on a multitude of factors, including job-related skills, experience, and relevant education or training.\n\nNational Pay Range\n\n$100,000\u2014$110,000 USD\n\nBenefits of Working at ISC\n\nCompetitive vacation and flexible working arrangements\nComprehensive and inclusive health benefits\nA variety of professional development and mentorship opportunities\nChoice of technology whether at home or in the office\n\nISC's Ownership Behaviors\n\nDo the Right Thing *Be Relentless. Pursue Excellence. *Personal Responsibility & Own Outcomes\nTry Fast. Learn Fast. Fail Fast. Think Big. *Build Extraordinary Owners & Join Forces *Grit & Determination\n\nApplicants with disabilities may contact ISC HR department via e-mail, and other means to request and arrange accommodation. If you need assistance to accommodate a disability, you may request accommodation at any time. Please contact ISC at HR@ISCMGA.com.\n\nISC believes in creating long-term relationships by being responsive and relevant and by consistently delivering value to our community of customers. Specifically, with our employees, we focus on attracting, developing, and retaining the best talent for our business, challenging our people, demonstrating a \"can-do\" attitude, and fostering a collaborative and mutually supportive environment. ISC is an equal opportunity employer and a participant in the US federal E-Verify program (US).\n\nDiversity creates a healthier atmosphere: All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, gender, gender identity, sexual orientation, marital status, medical condition, genetic information, mental or physical disability, military or veteran status, or any other characteristic protected by local, state, or Federal law."}, {"title": "Data Engineer -Remote", "company_name": "RIT Solutions, Inc.", "location": "Denver, CO (Remote)", "link": "/jobs/view/3768007496/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=1hQH8EDaII1eJ7nQ9BACQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3768007496", "description": "About the job\nDuration: 6+ months CTH\n\nMust have a valid LinkedIn profile\n\nRequired Skills\n\n 3-6 years of experience (no more than 6 years)\n Start up environment experience\n Python, SQL, and DBT"}, {"title": "Big Data Engineer - W2", "company_name": "Megan Soft Inc", "location": "United States (Remote)", "link": "/jobs/view/3674845652/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fCAxh4N%2F4YZJ5aEIASaeeA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3674845652", "description": "About the job\nTitle: Big Data Engineer - W2\n\nLocation: Dearborn, MI (Remote)\n\nDuration: 12+ Months\n\nDirect Client: Ford Motors\n\nMust have skills: Should be able to handle GCP requirements\n\n\"Translates business requirements to build repeatable, sustainable, efficient, coded processes that can be productionized by Software Engineers and readily modified by other Data Engineer Creates POC processes in Dev/QA and works with Software Engineers to productionize and align best coding and process practices that maximize efficiency, speed, stability, system resources and capabilities Awareness of and compliance with data privacy, security, legal and contractual guidelines Incorporates data quality and privacy checks/alerts to minimize bad data being consumed by end users, models and dashboards, and to protect customer data; revises checks for new data issues Validates data products and pipelines are functioning as expected following system or application upgrades, source changes, etc. Works closely with Architects if required to align systems, tools and applications being utilized with business use case and performance requirements Provides data product support and maintenance Establishes, tracks and monitors KPIs related to specific data products and deliverable Participate in iteration cadence activities (Iteration Planning, task estimation, daily scrum, demos, etc.). Will be a part of on-call team to resolve critical production support tickets during business hours and off hours.\n\nSkills Required\n\nExperience with Big Data Technologies (Hadoop , ETL Tool, Spark. Hive etc..) Someone who understands Cloud and has experience working with Cloud Platforms (preferably GCP) Should be able to handle GCP requirements - ingesting into GCS/ Big Query, working with GCP components Hands-on in Data Flow, Cloud Storage (GCS), Big Query and GitHub. Proficient in Python and SQL.\n\nSkills Preferred\n\nManages ETL: uses programming and tools for data ingestion, configures pipelines, applies transformations and decoding, integrates and fuses data, moves and securely deliver Experience NoSQL databases\n\nAdditional Information\n\nIf the candidate is remote only, please indicate \"\"100% Remote\"\" under candidate's name on resume. Other similar schedule notations can be \"\"Local Candidate\"\" or \"\"Hybrid Candidate.\"\" ***HRA Test Required***\""}, {"title": "Remote Opportunity: Quantexa Data Engineer", "company_name": "SPAR Information Systems LLC", "location": "United States (Remote)", "link": "/jobs/view/3680073191/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ioykVCZSOQ7BK5%2FwRQHOWg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3680073191", "description": "About the job\nHello All, Hope you are doing great!! Please go through the job descriptions and let me know your interest. Role: Quantexa Data engineer  Location: Remote Duration: Long Term Contract Job Description: 1. End to End usage of Quantexa including guiding our client team on how to use from both front end and data ingestion Reason they are asking for Certification 2. Resolve Entity issues with data 3. Knowledge of Scala and Hive which requires coding experience using parquet 4. Advanced SQL knowledge requiring writing SQL queries 5. Experience having worked with GCP data lake 6. Quantexa certification required\n\nThanks & Regards,\n\nSatnam Singh\n\nDirect: 201 623 3660\n\nEmail : Satnam.singh@sparinfosys.com"}, {"title": "Remote Work - Need Staff Data Engineer - USC only", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3746283980/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=CvIuLBNO%2BPMGi4IXVRkP0A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3746283980", "description": "About the job\n100% Remote, client is in NY\n\nLook for strong job history, Python, Machine Learning, SQL, 10/10 comms, etc. \n\nMUST be an Engineer \n\nSummary\n\nWe're looking for an experienced Data Engineer to join our client's growing team of top notched IT professionals. You will work closely across multiple groups including Editorial and Audio, Marketing, Advertising, etc. to achieve their goals and objectives. The Data Engineering team builds tools throughout the stack while sharing knowledge across these departments.\n\nAs the company grows, they're looking for Data Engineer who will help solidify and expand our pipelines and maintain their data warehouse. Their business model is based on performing complex and very detailed analyses of how our products perform with their subscribers. You'll be responsible for working alongside analysts, product, marketing, and other teams to define new data collection components and measurement schemes that support each new product and feature.\n\nHow Can Add Value To Their Mission\n\nCreate and maintain data pipelines to provide insights and drive business decisions\nEstablish data warehousing strategy (ex. Kimball, Data Vault, etc.)\nMaintain data infrastructure on our AWS accounts\nCollaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.\nWrite unit/integration tests, contributes to engineering wiki, and documents work\nImplement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it\n\nWhat You'll Need To Succeed\n\n7+ years of industry experience measuring product performance and user behavior\nExperience working with a variety of data management technologies, including RedShift, Kinesis/Kafka, Glue, Spark, Postgres, Airflow, dbt, and others.\nExperience implementing BI reporting tools such as Looker\nExperience interfacing with engineers, product managers and analysts to understand data needs\nKnowledge of a variety of measurement beacons, SDKs, APIs including Google Analytics, Amplitude, Braze, Stripe, email service providers\nA commitment to building secure, resilient, fault-tolerant architectures, with clear documentation and procedures in place for support\nA focus on accuracy and detail as you build, ensuring data pipelines produce clear, predictable results\nUnderstanding of the typical metrics a subscription and advertising-supported business needs to measure success\nExperience with Client techniques as applied to behavioral segmentation or anomaly identification is a definite plus\nFamiliarity using developer tools that increase productivity and facilitate the development of resilient code (eg. Docker, CircleCI, Serverless) is a plus"}, {"title": "ENTRY LEVEL DATA ENGINEER (REMOTE)", "company_name": "SynergisticIT", "location": "Jacksonville, FL (Remote)", "link": "/jobs/view/3767587909/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=om7L3CnowiC4PLIcpZD70A%3D%3D&trk=flagship3_search_srp_jobs", "id": "3767587909", "description": "About the job\nSYNERGISTICIT wants every candidate to know that the Job Market is Challenging and to stand out, you need to have exceptional skills and technologies and that's where we come in to make sure you get the attention which you need\n\nSynergisticIT understands the complex nature of the job market and how difficult it can be to secure a position, especially for fresh graduates. Therefore, we assist and help tech-savvies to convert their passions into professions. We go above and beyond to keep you working in your niche.\n\nAs we focus on long-term success, we provide complete career development solutions. From job search to upskilling portfolio and interview preparation, we can guide you at every step of your career.\n\nSynergisticIT spares no efforts to connect you with a large network of tech giants, including Google, Apple, PayPal, Dell, Cisco, Client, etc. Presently, we are actively looking for ENTRY LEVEL DATA ENGINEER a driven mindset. Get the right opportunity and gain experience in building web-centric solutions on Java.\n\nWho Should Apply : Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or anyone looking to make their career in IT Industry\n\nWe also assist in filing for STEM extension and H1b and Green card filing.\n\nCandidates who are serious about their future in the IT Industry and have set big goals for themselves.\n\nCandidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. We also offer Skill Enhancement Programs if the candidates are missing skills or experience which our clients need with great outcomes\n\nCandidates can benefit from skill enhancement if they fall into the below categories. If they are qualified with enough skills then no need for skill enhancement\n\nCandidates Who Lack Experience\n\nHave had a break in careers\n\nLack Technical Competency\n\ncandidates who want to get employed and make a career in the Tech Industry \n\nPlease Also Check The Below Links\n\nhttps://www.synergisticit.com/candidate-outcomes/\n\nhttps://www.synergisticit.com/java-track/\n\nhttps://www.synergisticit.com/data-science-track/\n\nhttps://www.synergisticit.com/which-is-the-best-option-for-tech-job-seekers-staffing-companies-consulting-companies-bootcamps-or-synergisticit/\n\nhttps://www.synergisticit.com/contact-us/\n\nIf the skills are not a match candidates can opt for Skill enhancement. Or their resume can be sent out to clients to see if responses are achievable\n\nREQUIRED SKILLS For Java/Software Programmers\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Core Java , javascript , C++ or software programming\nSpring boot, Microservices and REST API's experience\nExcellent written and verbal communication skills\n\nFor data Science/Machine learning\n\nRequired Skills\n\nBachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT\nHighly motivated, self-learner, and technically inquisitive\nExperience in programming language Java and understanding of the software development life cycle\nKnowledge of Statistics, Python, data visualization tools\nExcellent written and verbal communication skills\n\nPreferred skills: NLP, Text mining, Tableau, Time series analysis\n\nTechnical skills are required by clients for selection even if its Junior or entry level position each additional Technical skill helps a candidate's resume to be picked by clients over other job seekers.\n\nClients hire candidates with the right technical skills which they need and reject candidates who lack the required technical skills.\n\nNo third party candidates or c2c candidates\n\nPlease apply to the posting\n\nNo phone calls please. Shortlisted candidates would be reached out"}, {"title": "Data Engineer - Remote | WFH", "company_name": "Get It Recruit - Information Technology", "location": "Cleveland, OH (Remote)", "link": "/jobs/view/3782241984/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=BgBf1kSAAX%2BSuw6inJsIVw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3782241984", "description": "About the job\nJoin an innovative team at a leading company in the engineering and manufacturing industry, where we are committed to leveraging data for continuous improvement and operational excellence. This hybrid position allows you to enjoy the flexibility of remote work on Mondays and Fridays.\n\nWhy Join Us\n\nAt our company, we prioritize your well-being and professional growth. We offer a comprehensive benefits package, including excellent health benefits, a 6% 401(k) match, generous paid vacation and sick time, 12 paid holidays, and more.\n\nRole Overview\n\nAs a Data Engineer, you will play a pivotal role in transforming raw data into actionable insights, driving improvements in engineering and manufacturing processes, enhancing product quality, and optimizing operational efficiency. Collaborate with dynamic cross-functional teams to extract, analyze, and visualize data, influencing strategic decisions through your valuable insights.\n\nKey Responsibilities\n\nData Collection and Integration:\n\nGather, clean, and integrate data from diverse sources, including production databases, sensor data, and external datasets.\n\nData Analysis\n\nConduct exploratory data analysis to identify patterns, trends, and anomalies.\n\nPerform statistical analysis and hypothesis testing to derive meaningful insights.\n\nData Visualization\n\nCreate informative and visually appealing dashboards and reports using tools like Tableau, Power BI, or Python libraries.\n\nPredictive Modeling\n\nDevelop predictive models to forecast manufacturing performance, quality, and operational outcomes.\n\nRoot Cause Analysis\n\nInvestigate production issues and quality concerns by analyzing data to identify root causes and recommend corrective actions.\n\nPerformance Monitoring\n\nEstablish and maintain key performance indicators (KPIs) to track manufacturing and engineering metrics.\n\nGenerate regular reports to communicate performance trends to stakeholders.\n\nCross-functional Collaboration\n\nCollaborate with engineering, manufacturing, and quality assurance teams to understand their data needs and provide analytical support.\n\nContinuous Improvement\n\nSuggest process improvements and optimizations based on data analysis.\n\nData Security And Compliance\n\nEnsure data security and compliance with relevant regulations, such as GDPR or industry-specific standards.\n\nInfrastructure And Development\n\nBuild and maintain the infrastructure for collecting, storing, and processing data.\n\nDesign, develop, and manage data pipelines, ETL processes, and data warehouses.\n\nRequired Experience\n\nProven experience as a Sr. Data Analyst and/or Data Engineer in an engineering or manufacturing environment.\n\nProficiency in data analysis tools and programming languages, such as Python, R, SQL, and Excel.\n\nStrong data visualization skills with tools like Tableau or Power BI.\n\nExcellent knowledge of statistical analysis and machine learning techniques.\n\nAbility to communicate complex findings and insights effectively to both technical and non-technical stakeholders.\n\nStrong problem-solving skills and attention to detail.\n\nFamiliarity with manufacturing processes and quality control is a plus.\n\nExperience with enterprise ERPs (SAP S4HANA, Oracle EBS), CRMs (Salesforce, Hubspot), and IoT.\n\nProficient in working with cloud computing platforms including AWS, GCP, and Azure.\n\nExpertise in configuring and optimizing cloud-based data solutions for scalability, reliability, and performance.\n\nBachelor's Degree in Data Science, Statistics, Engineering, or Computer Science Required.\n\nApply Now and be part of a team shaping the future of data-driven innovation in engineering and manufacturing!\n\nEmployment Type: Full-Time"}, {"title": "Entry Level Software Engineer - Data Backend Engineer (Remote - Canada)", "company_name": "Yelp", "location": "United States (Remote)", "link": "/jobs/view/3752881757/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=fankeoxifvhpTzOYvowHYQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3752881757", "description": "About the job\nYelp engineering culture is driven by our values: we\u2019re a cooperative team that values individual authenticity and encourages creative solutions to problems. All new engineers deploy working code their first week, and we strive to broaden individual impact with support from managers, mentors, and teams. At the end of the day, we\u2019re all about helping our users, growing as engineers, and having fun in a collaborative environment.\n Interested in exploring data and finding innovative ways to collect it, curate it, and make it easily accessible for others to use? How about doing that at an enormous scale? Yelp\u2019s datasets contain billions of interactions between users and local businesses around the globe. If working with our vast amount of data sounds exciting, come and join us!\n\nThe Marketplace Data Observability team is responsible for managing, governing, and monitoring all of Yelp\u2019s Ads data to protect our revenue stream and also support offline analysis that aids in making long term company decisions. We build tools that help us to better understand our data quality, and we strive to uncover data issues before they impact our consumers.\n\nAs a Data Backend Engineer on this team, you will be responsible for building elegant and scalable data products that serve critical, up-to-date, structured information to support different types of analytics within Yelp. As a core contributor to our growing data modeling and data warehousing engineering efforts, you will help design and own mission-critical data flow pipelines and datastores to enable decisions including effective A/B testing and company investments.\n This opportunity is fully remote and does not require you to be located in any particular area in Canada. We welcome applicants from throughout Canada. We\u2019d love to have you apply, even if you don\u2019t feel you meet every single requirement in this posting. At Yelp, we\u2019re looking for great people, not just those who simply check off all the boxes.\n  Build systems that can effectively store and crunch terabytes of data.\nDesign and develop data models for efficient data storage, retrieval, and reporting.\nCreate and maintain conceptual, logical, and physical data models using industry-standard modeling tools.\nCollaborate with cross-functional teams, including engineers, data analysts, business analysts, and data scientists, to understand data requirements and translate them into effective data models.\nParticipate in data integration efforts, including ETL processes and data migration.\nStay up-to-date with industry best practices, emerging technologies, and trends related to data warehousing.\nSupport on-call rotations as needed to operate the team.\n  Understanding of high performing and scalable data systems.\nExperience in building and orchestrating ETL pipelines.\nExperience with Data Lake or Data Warehouse landscape.\nA hunger for tracking down root causes and fixing them in systematic ways.\nAbility to communicate effectively to technical and non-technical cohorts alike. \nExposure to some of the following technologies: Python, AWS Redshift, AWS Athena / Apache Presto, Big Data technologies (e.g S3, Hadoop, Hive, Spark, Flink, Kafka etc), NoSQL systems like Cassandra, DBT."}, {"title": "Data Engineer (Associate Consultant), June 2024", "company_name": "UDig", "location": "Nashville, TN (Remote)", "link": "/jobs/view/3716039960/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=XUjo40kU55HnDCqJ%2F3FdPg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3716039960", "description": "About the job\nCan UDig It? \n\nUDig designs, builds, and implements technology solutions that deliver on business objectives. People join our team because they are excited to have an impact on the future of our organization, passionate about delivering meaningful results, and energized by solving complex problems. Oh, and they like working with other driven, smart, and authentic people!\n\nUDig is proud to be recognized as a \"Best Places to Work\" in Virginia and Tennessee. We're committed to fostering a transparent environment where leadership is accessible, professional development opportunities are abundant, and you are empowered to achieve your career goals. You'll collaborate, develop deep relationships with teammates, and accelerate your success while having fun!\n\nIf you're graduating in May 2024 and excited by the idea of impacting organizations by building data solutions to achieve business goals, you should consider joining our growing technology team.\n\nHow Things Go Down\n\nAs a new, entry-level teammate, you'll take part in our \"Breaking Ground\" training program designed to help you quickly get up-to-speed on UDig's delivery approach within a project team. The program covers:\n\nUDig delivery process and methodologies such as gathering requirements and delivering projects using agile\nTechnical guidance and instruction on the best practices from Data leaders at UDig\nDelivering executive and technical presentations\nCoaching to prepare you to for success on your first client project at UDig\n\nA typical day might entail:\n\nProject standups and check-ins\nDesign and development of data pipelines, BI reports, and more\nPlanning and participating in project demos\nContributing to internal UDig projects\nCareer and job growth through UDig-provided training\nCreating long-lasting friendships with teammates through fun activities as well as your projects (including social events, winning a prize for guessing the weight of pumpkin, or sharing experience on cool tech side projects)\n\nUDig believes it is important for Associate Consultants going through our Breaking Ground program to reside in one of our major markets (Richmond, VA or Nashville, TN metro area) for at least the first year of employment.\n\nWant to learn more? Hear directly from our teammates by checking out our 'A Day in the Life' blog series: https://www.udig.com/digging-in/breaking-ground-2023/\n\nSound Interesting? Here's the Technical Piece\n\nBelow are a few boxes you should be able to check if\u202fyou're interested: Don't check every box? Go ahead and apply!\n\n1+ year of experience with relational database management systems such as Microsoft SQL Server, MySQL, or Oracle\nUnderstanding of the full lifecycle of design, development, and implementation of data solutions including architecture design, development, testing\nSome experience in ETL development or reporting is a plus\nAbility to present technical ideas and high-level concepts and solutions to internal and external team members with varying degrees of technical knowledge\nBachelor's Degree in in computer science, information technology, or any science or business discipline\nAbility to take your work seriously but also seriously enjoy a good time with your UDig colleagues\n\nWhat's in It For You? \n\nAt UDig, you will build your career alongside talented and experienced developers while gaining experience and having fun! We provide continuous challenges and professional development opportunities. Additionally, we offer: \n\nCompetitive salary, merit reviews, and career advancement paths\nFlexible, hybrid environment\nIndividual $1500 Training Budget\nRegular team building and social activities (virtual and/or in-person)\nTransparent culture with strong communication and access to leadership\nGreat benefits like\nGenerous Paid Time Off, company holidays, and parental leave\nMultiple Single and Family Health Insurance plans to choose from\nDental & Vision coverage\nShort-Term and Long-Term disability\nOptional accident and critical illness coverage\nMatching 401(k)\nand more!"}, {"title": "Expression of Interest: Data Engineer", "company_name": "Fingerprint for Success (F4S)", "location": "New York, NY (Remote)", "link": "/jobs/view/3787769880/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=l%2FEDpQqUNmCttGdIyTk7Cg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787769880", "description": "About the job\nWe are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.\n\nThe F4S Talent Pool is a pilot project designed to:\n\nHelp job seekers get discovered by our partners based on their anticipated hiring needs.\nProvide optional support and resources for job seekers in their career endeavors.\nHelp individuals understand, and bring out the best in themselves and each other.\n\nThe F4S Talent Pool process:\n\nOnce you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.\n\nAbout Fingerprint For Success (F4S)\n\nBacked by 20+ years of research, F4S\u2019s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high performing teams.\n\nKeep in mind, joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your F4S work style assessment to match the hiring needs of our partners.\n\nYour feedback is a gift! Write to us via:\n\nPowered by JazzHR\n\noWuPJ9lwq9"}, {"title": "Data Engineer Seattle, WA (remote)", "company_name": "Conch Technologies, Inc", "location": "United States (Remote)", "link": "/jobs/view/3765052211/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=SBETI78UMutCHuRp5FnP%2FA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3765052211", "description": "About the job\nHi,\n\nGreetings from Conch Technologies Inc\n\nData Engineering Lead \u2013 With Databricks and Unity Catalog\n\nLocation \u2013 Seattle, WA (remote)\n\nDuration \u2013 12 months\n\nNeed: Design and implement data architecture and strategy, leveraging technologies such as Databricks, Unity Catalog, Privacera, and Collibra.\n\nAs a Data Engineering Lead, you will play a pivotal role in defining and executing technical deliverables while assessing complexities and levels of effort. You'll engage with both vendor and internal teams to resolve technical challenges and ensure seamless collaboration. Your leadership will extend to planning and prioritization with the SCRUM master, facilitating effective coordination between onsite and offshore teams, and driving feature prioritization through collaboration with product managers.\n\nKey Responsibilities\n\n Lead the definition and breakdown of technical deliverables, assessing complexities and levels of effort.\n Collaborate with vendor and internal teams to address technical issues.\n Drive planning and prioritization in conjunction with the SCRUM master.\n Coordinate with onsite and offshore teams for timely project delivery.\n Collaborate with product managers to prioritize features, communicate with stakeholders, and clarify requirements.\n Collect and manage user-reported issues, ensuring prompt resolution and communication.\n Define comprehensive test plans, oversee their execution, and document results.\n\n--\n\nWith Regards,\n\nNagesh G\n\nMobile: 408-381-5645\n\nDesk: 901-313-3066\n\nEmail: nagesh@conchtech.com \n\nWeb: www.conchtech.com"}, {"title": "Remote Work - Need Azure Fabric Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3759325599/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=C3iVjmcR9wjUip1nNjUiCg%3D%3D&trk=flagship3_search_srp_jobs", "id": "3759325599", "description": "About the job\nThe client is placing heavy emphasis on Microsoft and Azure Fabric experience\n\nInformation Management Engineer \u2013 Take the identified information management sources and assist migration to the Client Data Lake, Azure Fabric. The engineer will transform information management (de-dup, clean and enrich).\n\nTake the identified data sources and bring them into our Data Lake, Azure Fabric. Once the data is in, the engineer will need to be able to transform data (de-dup, clean and enrich the data). We are focused on using a medallion data model for data architecture. The tools we are working with are: Delta Parquet files, python, spark, notebooks, T-SQL, Fabric lakehouse/warehouse, Dataflows Gen2, Data Pipelines, Direct Lake, shortcuts, ADF."}, {"title": "Data Engineer", "company_name": "Catalist", "location": "Washington, DC (Remote)", "link": "/jobs/view/3744951279/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=ui1Fk6ICmnPNqAlZDZAcwA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3744951279", "description": "About the job\nFor over 17 years, Catalist has been a leader in civic data and data science innovation. Our mission is to provide progressive organizations with the data, software, and services needed to better identify, understand, and communicate with the people they need to engage and mobilize. Our clients include the largest, most influential organizations in the U.S. active in civic engagement, advocacy, and political campaigns.\n\nCatalist is home to a dedicated, creative team of technologists, data scientists, and campaign experts committed to using our talents and technology to nurture a vibrant and growing progressive community.\n\nAs a Data Engineer at Catalist, you will work closely with the Analytics and Technology departments to design, build, support, and maintain various data pipelines and processes with an end goal of providing data and intelligence to the progressive community. This role primarily involves translating data architecture designs into functional processes, code, and systems.\n\nThe ideal candidate will be a highly motivated individual with excellent technical skills, a strong desire to learn new skills, and an interest in progressive politics. Catalist values creativity and problem-solving. Our work is on the cutting edge of data-driven politics, and your support will help Democratic candidates and progressive organizations conduct successful advocacy and electoral campaigns.\n\nThis position reports to the Deputy Chief Data Officer. The Data Engineer is a part of a growing Data team that supports all underlying work at Catalist.\n\nThis position is included in our CWA bargaining unit.\n\nPrinciple Duties & Responsibilities\n\nDevelop scalable, production processes, code, and systems for data ingest, transformation, modeling, and reporting across a variety of platforms\nTranslate mock-ups and designs into functional processes, code, and systems\nCreate architecture designs when needed\nProvide quality assurance and testing on processes, code, systems, and products\nBecome an internal subject matter expert on various datasets and support other Catalist departments/teams on usage of those datasets\nExecute ad-hoc data and database maintenance tasks as requiredProject manage cross-departmental efforts, with direct responsibility for stakeholder engagement, management, and execution of technical elements\n\n\nRequirements\n\nBS or BA in a technical field, or relevant experience\n1-2 years of experience working with SQL databases\nExperience with data cleaning or ETL processes\nExperience with distributed computing systems and/or distributed datastores (particularly the Hadoop ecosystem)\nExperience managing projects\nFamiliarity with Catalist data, progressive politics, voter files, and/or commercial data\nBackground check required\n\n\nPreferred Skills & Abilities\n\nWillingness to be a problem solver and produce results in a fast paced environment\nAbility to be creative and personable, and articulate ideas clearly\nExperience working with SQL databases\nProficiency with Python or another object-oriented programming language (R, Java, Scala, etc\u2026)\nExperience working in cloud environments (AWS, GCP, etc.)\nExperience working in command line environments such as Bash\nExperience with a version control tool such as git or github\n\n\nBenefits\n\nMedical, Dental, Vision, Prescription Drug\n\nCatalist offers Medical, Dental, Vision, and Prescription Drug coverage for eligible staff and their eligible dependents. Catalist\u2019s Medical plan is a comprehensive PPO program including Prescription Drug coverage with 85% of the premium paid by Catalist. Dental and Vision coverage is provided at no cost to employees.\n\nGroup Term Life Insurance and Long-Term & Short-Term Disability Coverage\n\nGroup Term Life Insurance and Long-Term and Short-Term Disability coverage is available for eligible staff. These benefits are provided at no cost to Catalist employees.\n\n401(k) Safe Harbor Plan\n\nA 401(k) Safe Harbor Plan is available to eligible staff with a 3% contribution from Catalist from the date of hire. Employees may contribute pre-tax or post-tax from their salary up to the legal limits set forth by the IRS.\n\nMedical and Dependent Care Flexible Spending Accounts (FSAs)\n\nCatalist offers an FSA Program that gives eligible staff the ability to pay out-of-pocket medical/dental/vision/child care expenses from pre-tax earnings.\n\nTransit Benefits\n\nCatalist also makes available a Transit benefit FSA program to eligible employees using pre-tax contributions with a company match.\n\nProfessional Development and Remote Work Expenses\n\nEligible employees may be reimbursed up to $750 each year for professional development / education and remote work expenses.\n\nStudent Loan PayDown or SaveUp\n\nCatalist offers a Student Loan PayDown and College SaveUp benefit for eligible staff.\n\nVacation, Personal Leave, Sick Leave Benefits\n\nCatalist offers generous vacation benefits to all eligible staff. Eligible employees also receive:\n\n 14 Paid Holidays\n Personal Days\n Sick Leave\n Parental Leave\n\n\nHybrid Office/Remote Work\n\nCertain positions at Catalist are eligible for Office/Remote Hybrid or full Remote status."}, {"title": "Remote Entry Level Data Analyst/Engineer", "company_name": "SynergisticIT", "location": "White Plains, NY (Remote)", "link": "/jobs/view/3774171423/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=q54sk1%2FRww0z%2BC%2BE8e0jQQ%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774171423", "description": "About the job\nAt SynergisticIT, we're all about making connections. Whatever IT goals you have, our software programmers can help achieve those. Our Software development teams can take up turnkey projects and execute them in an effective and efficient manner. If you are looking to source talent our recruiters will find the ideal IT talent for your company. What's the secret to our success? Well, it all starts with taking quality time to listen to each client's specific needs. After we have a thorough grasp of your IT goals, we can better customize our Developments as per your specific needs. We can also tailor make recruiting programs to exceed your expectations. Since our founding in 2010, SynergisticIT's strategies have earned the company an enviable position in the software development, IT staffing and IT skill enhancement fields. SynergisticIT continues to work with hundreds of satisfied American clients with our software programmers working on our projects and after gaining hands on experience on cutting edge technologies moving to contribute their skills to great clients like Apple, Google, Client, Ebay, Paypal, Kroger, the Walt Disney Company and hundreds more. If you are tired of working with inefficient programmers who take a lot of time to ramp up we want you to try us. Our software programmers can hit the ground running and get you the maximum return on your investment. You have already tried the rest its time you tried the best. SynergisticIT - Home of the Best Data Scientists and Software Programmers in the Bay Area.\n\n Why Us ? \n\nSynergisticIT has a proven track record of successfully skill enhancement and staffing IT employees for some of the world's most iconic brands. Our team takes the time to fully understand every client's needs so we could best meet your IT staffing requirements. The knowledgeable staff at SynergisticIT is always more than happy to work with clients to ensure they reach their software development goals. Besides staffing, SynergisticIT is also committed to helping young IT professionals advance their career with a robust upskill program . Everyone who goes through SynergisticIT's program learns all the skills necessary to succeed in many IT fields ranging from Java to Machine Learning. Additionally, everyone trained at SynergisticIT has been through extensive mock and technical interview screenings to bolster their career prospects. Last, but certainly not least, SynergisticIT takes great care to respect the privacy considerations for every client. All companies who work with SynergisticIT can rest assured their confidential data is protected using the most up-to-date encryption technologies. SynergisticIT also complies with all the latest NDA agreements.\n\n REQUIRED SKILLS For Java /Software Programmers \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Time series analysis \n\n We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2022 and at Gartner Data Analytics Summit (Florida)-2023 \n\nOracle CloudWorld Event (OCW) Las Vegas 2022 | SynergisticIT - YouTube\n\n https://www.youtube.com/watch?v=OAFOhcGy9Z8 \n\n https://www.youtube.com/watch?v=EmO7NrWHkLM \n\n https://www.youtube.com/watch?v=NVBU9RYZ6UI \n\n https://www.youtube.com/watch?v=Yy74yvjatVg \n\nSynergisticIT at Gartner Data and Analytics Summit 2023 - YouTube\n\nFor preparing for interviews please visit  https://www.synergisticit.com/interview-questions/ \n\n We are looking for the right matching candidates for our clients \n\n Please apply via the job posting \n\n REQUIRED SKILLS For Java /Full Stack/Software Programmer \n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Project work on the skills \n Knowledge of Core Java , javascript , C++ or software programming \n Spring boot, Microservices, Docker, Jenkins and REST API's experience \n Excellent written and verbal communication skills \n\n For data Science/Machine learning Positions \n\nRequired Skills\n\n Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT \n Project work on the technologies needed \n Highly motivated, self-learner, and technically inquisitive \n Experience in programming language Java and understanding of the software development life cycle \n Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools \n Excellent written and verbal communication skills \n\n Preferred skills: NLP, Text mining, Tableau, PowerBI, Tensorflow \n\n If you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team. \n\n No phone calls please.  Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates"}, {"title": "Data Engineer", "company_name": "Anika Systems", "location": "Leesburg, VA (Remote)", "link": "/jobs/view/3787766893/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=mApL9JnN%2B%2Fr2%2BYUPlxRYOA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3787766893", "description": "About the job\nAnika Systems is a fast growing, woman-owned small business that specializes in providing innovative IT solutions for federal government agencies. Our expertise lies in accelerating delivery in Data and Analytics, Intelligent Automation, Application Development, and IT Modernization. We are currently expanding our Federal team and are seeking a passionate and talented Data Engineer. This opportunity is 100% remote.\n\nMust be a U.S. Citizen with the ability to obtain and maintain a government suitability clearance\n\nResponsibilities\n\nDesigns, develops, builds, analyzes, evaluates, and installs database management systems to include database modeling and design, relational database architecture, metadata, and repository creation and configuration management.\nDefines and oversees database organizations, standards, controls, procedures, and documentation. Provides technical consulting in the definition, design, and creation of a database environment.\nAdvises applications development staff and users on data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nDesigns and implements databases with respect to access methods, access time, batch processes, device allocation, validation checks, organization, protection and security, documentation, and statistical methods.\nUses data mapping, data mining, and data transformational analysis tools to design and develop databases.\nDetermines data storage and optimum storage requirements.\nPrepares system requirements, source analysis, and process analyses and designs throughout the database implementation.\n\nRequired Skills And Experience\n\nBA/BS and 1 year of relevant experience\nExperience with DataBricks, SQL, Python\nApplications development experience with data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.\nExcellent communication skills (both oral and written) with process-oriented organizational skills to ensure project success\n\nDesired Skills And Experience\n\nDemonstrated successful development experience with the full life cycle of government database management systems including database modeling and design, relational database architecture, metadata and repository creation, and configuration management.\n\nPowered by JazzHR\n\nTlKWIorucJ"}, {"title": "Remote Work - Need Data Engineer", "company_name": "Steneral Consulting", "location": "United States (Remote)", "link": "/jobs/view/3774712978/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=r3Bp%2F0qcOD2OpC5jpbUOcA%3D%3D&trk=flagship3_search_srp_jobs", "id": "3774712978", "description": "About the job\nTitle : Data Engineer\n\nLocation : Remote\n\nDuration : 6+ months to start (long term contract (work order are extended 2x year))\n\nPlease Fill out the skill matrix: Technology Some Awareness (1) Novice (2) Intermediate (3) Advanced (4) Expert (5) SQL Tableau Desktop /Creator Tableau API Integration AWS - S3 Glue AWS - Athena Redshift AWS - Lambda AWS - SNS/SQS msExcel\n\nProfile: Data Analyst will work closely with Architects and leads in the Interflow team, key business stakeholders including program staff to support the organizations effort in the areas of data Analysis and information management. Role should have strong experience in writing SQL queries to extract data for reports. Candidate should have experience working with large set of data. As these reports are very critical for the company and we are looking for an expert who is capable of handling, consuming and extracting large sets of data for analysis and reports.\n\nMust Haves\n\nAWS - Athena\nAWS - S3\nGlue\nhands-on development of reporting applications\nMS Excel\nRedShift\nSQL\nTableau Desktop /Creator\n\nNice\n\nAWS - Lambda\nAWS - SNS/SQS\nTableau API Integration\n\nSkills\n\nBA/BS required (major in an analytical field desired) \nA minimum 10+ years work-related experience. \nExperience in hands-on development of reporting applications for the Web in a professional environment \nThe Ethos of continuous improvement and interest in learning new things.\nStrong analytical thinking and structured problem-solving ability\nAbility to handle multiple projects and assignments simultaneously and effectively in a cross-functional team environment. \nVersed on the agile methodology and best practices. \nExcellent interpersonal and collaboration skill with the ability to work with a diverse set of colleagues, across functions, from different organizations, disciplines, etc. \nSelf-starter, ability to set priorities, work independently and attain goals"}, {"title": "Data Engineer", "company_name": "Team Remotely Inc", "location": "Houston, TX (Remote)", "link": "/jobs/view/3786249829/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=78lY5Jo%2FfyRPC6TQYCdvww%3D%3D&trk=flagship3_search_srp_jobs", "id": "3786249829", "description": "About the job\nThis is a remote position.\n\n Data Engineer (US/Canada Residents Only, 1 year experience, remote)\n\nTeam Remotely Inc. is a staffing and recruitment agency that offers a comprehensive solution for talent acquisition, including sourcing, vetting, pay rolling, and managing talent. Whether you need contract staffing, direct hire, direct sourcing, talent pools, or diversity initiatives, our model can support your hiring strategy.\n\n Hiring Type: Full-Time\n\n Base Salary: $55K-$70K Per Annum.\n\nResponsibilities\n\nDesigning user interface changes for web-based DB applications. \nReviewing application requirements and interface designs. \nDeveloping and implementing highly responsive user interface components using react concepts. \nWriting application interface codes using JavaScript following react.js workflows. \nTroubleshooting interface software and debugging application codes. \nDeveloping and implementing front-end architecture to support user interface concepts. \nMonitoring and improving front-end performance. \n\nQualifications And Experience\n\nBachelor\u2019s degree in computer science, information technology, or a similar field. \nPrevious experience working as a react.js developer. \nIn-depth knowledge of JavaScript, CSS, HTML, jQuery, and front-end languages. \nKnowledge of REACT tools including React.js, Webpack, Enzyme, Redux, and Flux. \nExperience with user interface design. \nKnowledge of performance testing frameworks, including Mocha and Jest. \nExperience with browser-based debugging and performance testing software. \nTroubleshooting skills. \nProject management skills. \nProblem-solving skills. \nVerbal communication skills. \n\n Why work with Team Remotely?\n\nTeam Remotely Inc. is a staffing platform offering a seamless experience for employers and candidates. Employers can post job openings and specify their requirements, while candidates can create profiles and upload resumes.\n\nThe team of Team Remotely continuously learns and adapts based on previous successful placements, constantly improving its matching capabilities. This ensures that the recommendations provided by Team Remotely are tailored and accurate, increasing the likelihood of a successful match between employers and candidates. By providing intelligent and data-driven solutions, they strive to enhance the efficiency and effectiveness of the hiring process, ultimately helping companies find the best talent and individuals find their dream jobs."}, {"title": "Data Engineer", "company_name": "ZAG Technical Services", "location": "United States (Remote)", "link": "/jobs/view/3771403464/?eBP=JOB_SEARCH_ORGANIC&refId=jQPtGFs5iXyKE8nnWf47Qg%3D%3D&trackingId=Hf0VefrKXBFIL5MtHmY0Pw%3D%3D&trk=flagship3_search_srp_jobs", "id": "3771403464", "description": "About the job\nWe are looking for a tech savvy Data Engineer to contribute to the building, operating, and scaling of next generation data platforms and tools that will power data-driven analytics capabilities throughout the ZAG products and services portfolio, spanning areas such as business intelligence, reporting, and data analytics.\n\nThe ideal team member is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. Must be comfortable supporting the data needs of multiple clients and teams, systems, and products within the modernization of ecosystems. The right-fit person will be excited by the prospect of optimizing or even re-designing data processes to support next generation of advanced analytics capabilities, which in-turn translates to implementing strategic enhancements to maintain competitive advantage in the industry.\n\nThe successful team member will leverage their proficiency to\u2026\n\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\nBuild the analytics processes required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Data Management and cloud based \u2018big data\u2019 technologies.\nBuild analytics tools that utilize technologies to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.\nCollaborate with client teams to identify and solution regional and local requirements within the context of the standard global business model for enterprise analytics.\nParticipates in meetings, discussions, strategy sessions where changes, improvements, and enhancements are proposed, evaluated, and approved. \n\n\nThe exceptional team member will have or demonstrate progressive experiences in\u2026 \n\nBachelor\u2019s degree or higher in Computer Science or related field\nExperience working with BI and data warehousing tools\nExperience working with enterprise data, where security is paramount and data governance is critical\nAbility to work independently and in a team environment, and effectively engage all levels of the organization\nUnderstanding of distributed systems driving large-scale data processing and analytics with a successful history of manipulating, processing, and extracting value from large, disconnected datasets.\nAbility to communicate effectively (listening, presenting, and questioning)\nStrong organizational, written, and communication skills\n\n\nAbout ZAG\n\nZAG Technical Services empowers our clients\u2019 success through intelligent business solutions that increase employee productivity, network security and team collaboration. We aim to ensure that our client\u2019s IT investments drive ROI, provide a competitive advantage and help businesses grow. We do this by living our core set of values: Integrity Foremost, Client-Centered, Accountable Always, Teamwork Throughout, and Exceptional is the Goal.\n\nWhat ZAG Has To Offer\n\nCompetitive compensation package \nBenefits including medical, dental, vision, 401K and life insurance \n\n\nZAG Technical Services is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all of our employees.\n\nZAG Technical Services is an E-Verify participant and will provide the Social Security Administration (SSA) and, if necessary, the Department of Homeland Security (DHS), with information from each new employee\u2019s Form I-9 to confirm work authorization."}]