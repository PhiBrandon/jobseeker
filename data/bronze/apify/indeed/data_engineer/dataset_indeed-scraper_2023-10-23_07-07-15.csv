"company","description","descriptionHTML","externalApplyLink","id","jobType","jobType/0","jobType/1","location","positionName","postedAt","postingDateParsed","rating","reviewsCount","salary","scrapedAt","searchInput/country","searchInput/location","searchInput/position","url"
"Microagility","We are seeking an experienced ETL (Extract, Transform, and Load) Data Engineer with expertise in Google Cloud Platform (GCP) to join our client data engineering team.
Key Responsibilities:
· Design, develop, and maintain ETL pipelines on Google Cloud Platform (GCP) to ensure efficient data extraction, transformation, and loading processes.
· Extract data from various sources, including databases, APIs, and cloud storage, and ensure data quality and consistency.
· Collaborate with data scientists, data analysts, and other stakeholders to understand their data requirements and ensure data pipelines meet their needs.
· Implement data transformations, including cleaning, aggregation, and enrichment, to prepare data for analysis and reporting.
· Design, build, and maintain ETL pipelines using GCP services like Dataflow, Dataprep, or other relevant tools.
· Performance Optimization: Continuously optimize ETL processes for speed, efficiency, and scalability within the GCP environment.
Key Requirements:
· Proven experience in designing and developing ETL pipelines on Google Cloud Platform (GCP).
· Experience with GCP data tools and services, such as BigQuery, Dataflow, Dataprep, and Data Studio.
· Knowledge of ETL tools, methodologies, and best practices.
· Experience with data warehousing concepts.
· Proficiency in data scripting languages, such as Python or SQL.
· Strong problem-solving skills and attention to detail.
· Excellent communication and teamwork skills.
Education:

 Bachelor’s Degree in Computer Science or related field

Why Work with Us
Competitive compensation.
Opportunity to work on innovative projects for high-profile clients.
A collaborative, inclusive, and supportive work environment.
Opportunities for professional growth and development.
Work with a team of passionate, talented, and creative individuals.
Job Type: Contract
Schedule:

 8 hour shift

Experience:

 ETL: 7 years (Preferred)
 Google Cloud Platform: 5 years (Preferred)

Work Location: Remote","<p>We are seeking an experienced ETL (Extract, Transform, and Load) Data Engineer with expertise in Google Cloud Platform (GCP) to join our client data engineering team.</p>
<p><b>Key Responsibilities:</b></p>
<p>&#xb7; Design, develop, and maintain ETL pipelines on Google Cloud Platform (GCP) to ensure efficient data extraction, transformation, and loading processes.</p>
<p>&#xb7; Extract data from various sources, including databases, APIs, and cloud storage, and ensure data quality and consistency.</p>
<p>&#xb7; Collaborate with data scientists, data analysts, and other stakeholders to understand their data requirements and ensure data pipelines meet their needs.</p>
<p>&#xb7; Implement data transformations, including cleaning, aggregation, and enrichment, to prepare data for analysis and reporting.</p>
<p>&#xb7; Design, build, and maintain ETL pipelines using GCP services like Dataflow, Dataprep, or other relevant tools.</p>
<p>&#xb7; Performance Optimization: Continuously optimize ETL processes for speed, efficiency, and scalability within the GCP environment.</p>
<p><b>Key Requirements:</b></p>
<p>&#xb7; Proven experience in designing and developing ETL pipelines on Google Cloud Platform (GCP).</p>
<p>&#xb7; Experience with GCP data tools and services, such as BigQuery, Dataflow, Dataprep, and Data Studio.</p>
<p>&#xb7; Knowledge of ETL tools, methodologies, and best practices.</p>
<p>&#xb7; Experience with data warehousing concepts.</p>
<p>&#xb7; Proficiency in data scripting languages, such as Python or SQL.</p>
<p>&#xb7; Strong problem-solving skills and attention to detail.</p>
<p>&#xb7; Excellent communication and teamwork skills.</p>
<p><b>Education:</b></p>
<ul>
 <li>Bachelor&#x2019;s Degree in Computer Science or related field</li>
</ul>
<p><b>Why Work with Us</b></p>
<p>Competitive compensation.</p>
<p>Opportunity to work on innovative projects for high-profile clients.</p>
<p>A collaborative, inclusive, and supportive work environment.</p>
<p>Opportunities for professional growth and development.</p>
<p>Work with a team of passionate, talented, and creative individuals.</p>
<p>Job Type: Contract</p>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
</ul>
<p>Experience:</p>
<ul>
 <li>ETL: 7 years (Preferred)</li>
 <li>Google Cloud Platform: 5 years (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,"e981504596ce7e00",,"Contract",,"Remote","Data Engineer GCP ETL","2 days ago","2023-10-21T13:05:25.133Z",,,,"2023-10-23T13:05:25.825Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=e981504596ce7e00&from=jasx&tk=1hdea7hq42do5000&vjs=3"
"InfoMagnus","Data Governance Engineer
This is a contract remote role must reside in US.
We are people-oriented technologists, analysts and designers helping companies solve complex business problems with technology.
InfoMagnus was built to provide an environment where passion is celebrated, personal time is respected and hard work is rewarded.
Our Beliefs:
Integrity, dignity and respect in every interaction. Commitment to our employees. Trust and responsibility in all relationships. Giving back to our community.
Job Description:
We are looking for a skilled Data Governance Engineer who is well-versed in data governance principles and practices, with expertise in utilizing data quality platforms like Anomalo or data catalog platforms like Alation. In this role, you will play a crucial part in ensuring the integrity, security, and quality of our data assets.
Key Responsibilities:
· Collaborate with cross-functional teams to establish and maintain data governance policies, standards, and procedures.
· Implement and manage data quality checks and data lineage using Anomalo or Alation.
· Design and execute data quality audits and assessments to identify and resolve data issues.
· Develop and maintain data cataloging and metadata management processes.
Qualifications:
· Bachelor’s degree in Computer Science, Information Systems, or a related field. Master's degree preferred.
· Proven experience in data governance and data quality management.
· Proficiency in utilizing data quality platforms like Anomalo or data catalog platform like Alation.
· Strong understanding of data governance frameworks, policies and procedures.
· Excellent problem-solving skills and attention to detail.
· Strong clear communication skills and interpersonal skills to collaborate with various teams.
Equal Opportunity Employer
Job Type: Contract
Pay: $60.00 - $70.00 per hour
Experience level:

 3 years

Schedule:

 Monday to Friday

Application Question(s):

 Do you have an employer that sponsors

your Visa?

 Do you have experience using the data catalog platform Alation?
 Do you have experience using the data quality platform Anomalo?

Work Location: Remote","<p><b>Data Governance Engineer</b></p>
<p><i><b>This is a contract remote role must reside in US</b></i><i>.</i></p>
<p>We are people-oriented technologists, analysts and designers helping companies solve complex business problems with technology.</p>
<p>InfoMagnus was built to provide an environment where passion is celebrated, personal time is respected and hard work is rewarded.</p>
<p>Our Beliefs:</p>
<p>Integrity, dignity and respect in every interaction. Commitment to our employees. Trust and responsibility in all relationships. Giving back to our community.</p>
<p><b>Job Description:</b></p>
<p>We are looking for a skilled Data Governance Engineer who is well-versed in data governance principles and practices, with expertise in utilizing data quality platforms like <b>Anomalo</b> or data catalog platforms like <b>Alation</b>. In this role, you will play a crucial part in ensuring the integrity, security, and quality of our data assets.</p>
<p><b>Key Responsibilities:</b></p>
<p>&#xb7; Collaborate with cross-functional teams to establish and maintain data governance policies, standards, and procedures.</p>
<p>&#xb7; Implement and manage data quality checks and data lineage using <b>Anomalo</b> or <b>Alation</b>.</p>
<p>&#xb7; Design and execute data quality audits and assessments to identify and resolve data issues.</p>
<p>&#xb7; Develop and maintain data cataloging and metadata management processes.</p>
<p><b>Qualifications:</b></p>
<p>&#xb7; Bachelor&#x2019;s degree in Computer Science, Information Systems, or a related field. Master&apos;s degree preferred.</p>
<p>&#xb7; Proven experience in data governance and data quality management.</p>
<p>&#xb7; Proficiency in utilizing data quality platforms like <b>Anomalo</b> or data catalog platform like <b>Alation.</b></p>
<p>&#xb7; Strong understanding of data governance frameworks, policies and procedures.</p>
<p>&#xb7; Excellent problem-solving skills and attention to detail.</p>
<p>&#xb7; Strong clear communication skills and interpersonal skills to collaborate with various teams.</p>
<p>Equal Opportunity Employer</p>
<p>Job Type: Contract</p>
<p>Pay: &#x24;60.00 - &#x24;70.00 per hour</p>
<p>Experience level:</p>
<ul>
 <li>3 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>Monday to Friday</li>
</ul>
<p>Application Question(s):</p>
<ul>
 <li>Do you have an employer that sponsors</li>
</ul>
<p>your Visa?</p>
<ul>
 <li>Do you have experience using the data catalog platform Alation?</li>
 <li>Do you have experience using the data quality platform Anomalo?</li>
</ul>
<p>Work Location: Remote</p>",,"19d5fafc9fb74494",,"Contract",,"Remote","Data Governance Engineer","1 day ago","2023-10-22T13:05:28.320Z",,,"$60 - $70 an hour","2023-10-23T13:05:28.324Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=19d5fafc9fb74494&from=jasx&tk=1hdea7hq42do5000&vjs=3"
"Mayo Clinic","Why Mayo Clinic 
  
 
   Mayo Clinic is top-ranked in more specialties than any other care provider according to U.S. News & World Report. As we work together to put the needs of the patient first, we are also dedicated to our employees, investing in competitive compensation and comprehensive benefit plans – to take care of you and your family, now and in the future. And with continuing education and advancement opportunities at every turn, you can build a long, successful career with Mayo Clinic. You’ll thrive in an environment that supports innovation, is committed to ending racism and supporting diversity, equity and inclusion, and provides the resources you need to succeed.
 
  
  
 Responsibilities
  
  As a member of the Data and Analytics organization, you will be responsible for building and delivering best-in-class clinical data initiatives aimed at driving best-in-class solutions. You will collaborate with analytic partners and business partners from product strategy, program management, IT, data strategy, and predictive analytics teams to develop effective solutions for our partners. 
  Lead data design, prototype, and development of data pipeline architecture pipelines. Lead implementation of internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability. Lead cause analysis on external and internal processes and data to identify opportunities for improvement and answer questions. Excellent analytic skills associated with working on unstructured datasets. Understand the architecture, be a team player, lead technical discussions and communicate the technical discussion. Be a senior Individual contributor of the Data or Software Engineering teams. Be part of Technical Review Board along with Manager and Principal Engineer. Be a technical liaison between Manager, Software Engineers and Principal Engineers. Collaborate with software engineers to analyze, develop and test functional requirements. Write clean, maintainable code 30% of the time and performing peer code-reviews. Mentor and Coach Engineers. Work with team members to investigate design approaches, prototype new technology and evaluate technical feasibility. Work in an Agile/Safe/Scrum environment to deliver high quality software. Establish architectural principles, select design patterns, and then mentor team members on their appropriate application. Facilitate and drive communication between front-end, back-end, data and platform engineers. Play a formal Engineering lead role in the area of expertise. Keep up-to-date with industry trends and developments. 
  Job Responsibilities: 
  
  Act as Product Owner for Data platform’s and Lead the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. 
  Evaluate the full technology stack of services required including PaaS, IaaS, SaaS, DataOps, operations, availability, and automation. 
  Research, design, and develop Public & Private Data Solutions, including impacts to enterprise architecture 
  Build high-performing clinical data processing frameworks leveraging Google Cloud Platform, GCP Shared Services like Google Healthcare API, Big Query, and HL7 FHIR store. 
  Participate in evaluation of supporting technologies and industry best practices with our cloud partners and peer teams. 
  Lead Modern Data Warehouse Solutions and Sizing efforts to create defined plans and work estimates for customer proposals and Statements of work. 
  Conduct full technical discovery, identifying pain points, business, and technical requirements, “as is” and “to be” scenarios! 
  Design and Develop clinical data pipelines integrating ingestion, harmonization, and consumption frameworks for onboarding clinical data from various data sources formatted in various industry standards (FHIR, C-CDA, HL7 V2, JSON, XML, etc.). 
  Build state-of-the-art data pipelines supporting both batch and real-time streams to enable Clinical data collection, storage, processing, transformation, aggregation, and dissemination through heterogeneous channels. 
  Build design specifications for health care data objects and surrounding data processing logic. 
  Lead innovation and research building proof of concepts for complex transformations, notification engines, analytical engines, and self-service analytics 
  Bring a DevOps mindset to enable big data and batch/real-time analytical solutions that leverage emerging technologies.
 
  
  
 Qualifications
  
  Bachelor’s Degree in Computer Science/Engineering or related field with 6 years of experience OR an Associate’s degree in Computer Science/Engineering or related field with 8 years of experience.
  Knowledge of professional software engineering practices and best practices for the full software development life cycle (SDLC), including coding standards, code reviews, source control management, build processes, testing, and operations. Have in-depth knowledge of data engineering and building data pipelines with a minimum of 5 years of experience in data engineering, data science or analytical modeling and basic knowledge of related disciplines. Worked and lead Data Engineering teams in Continuous Integration / Continuous Delivery model. Build/Lead Data products highly resilient in nature. Build/Lead Test Automation suites, Unit Testing coverage, Data Quality, Monitoring & Observability. A minimum experience of 5 years using relational databases and NoSQL Databases. Experience with cloud platforms such as GCP, Azure, AWS.
  Continuous Integration using Jenkins, Git Hub Actions or Azure Pipelines. Experience with cloud technologies, development and deployment. Experience with tools like Jira, GitHub, SharePoint, Azure Boards. Experience using advanced data processing solutions/capabilities such as Apache Spark, Hive, Airflow and Kafka, GCP Dataflow. Experience using big data, statistics and knowledge of data related aspects of machine learning. Experience with Google BigQuery, FHIR APIs, and Vertex AI. Knowledge of how workflow scheduling solutions such as Apache Airflow and Google Composer related to data systems. Knowledge of using Infrastructure as code (Kubernetes, Docker) in a cloud environment.
 
   Hands-on experience in architecture, design, and development of enterprise data applications and analytics solutions within the health care domain
   Experience in Google Cloud Platform/Shared Services such as Cloud Dataflow, Cloud Storage, Pub/sub, Cloud Composer, Big Query, and Health care API (FHIR store)
   They should be able to deliver an ingestion framework for relational data sources, understand layers and rules of a data lake and carry out all the tasks to operationalize data pipelines.
   Experience in Python, Java, Spark, Airflow, and Kafka development
   Hands-on experience working with “Big Data” technologies and experience with traditional RDBMS, Python, Unix Shell scripting, JSON, and XML
   Experience working with tools to automate CI/CD pipelines (e.g., Jenkins, GIT)
   Must have great articulation and communication skills.
   Working in a fluid environment, defining, and owning priorities that adapt to our larger goals. You can bring clarity to ambiguity while remaining open-minded to new information that might change your mind.
   Should have a strong understanding of healthcare data, including clinical data in proprietary and industry-standard formats.
   Participate in architectural discussions, perform system analysis which involves a review of the existing systems and operating methodologies. Participate in the analysis of newest technologies and suggest the optimal solutions which will be best suited for satisfying the current requirements and will simplify the future modifications
   Design appropriate data models for the use in transactional and big data environments as an input into Machine Learning processing.
   Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability
   Design and Build the necessary infrastructure for optimal ETL from a variety of data sources to be used on GCP services
   Collaborate with multiple stakeholders including Product Teams, Data Domain Owners, Infrastructure, Security and Global IT
   Identify, Implement, and continuously enhance the data automation process
   Develop proper Data Governance and Data Security
   Demonstrate strategic thinking and strong planning skills to establish long term roadmap and business plan
   Work with stakeholders to establish and meet data quality requirements, SLAs and SLOs for data ingestion
   Experience in Self-service Analytics/Visualization tools like PowerBI, Looker, Tableau
   Proven knowledge in implementing security & IAM requirements
   Experience building and maintaining a Data-Lake with DeltaLake
   Experience with ETL/ELT/DataMesh frameworks
   Experience with GCP Dataplex (Data Catalog, Clean Rooms)
 
  Authorization to work and remain in the United States, without necessity for Mayo Clinic sponsorship now, or in the future (for example, be a U.S. Citizen, national, or permanent resident, refugee, or asylee). Also, Mayo Clinic does not participate in the F-1 STEM OPT extension program.
  Exemption Status
  
  Exempt
  
  
 Compensation Detail
  
  $138,236.80 - $200,408.00 / year
  
  
 Benefits Eligible
  
  Yes
  
  
 Schedule
  
  Full Time
  
  
 Hours/Pay Period
  
  80
  
  
 Schedule Details
  
  Monday - Friday, 8:00 am - 5:00 pm
  
  
 Weekend Schedule
  
  As needed
  
  
 International Assignment
  
  No
  
  
 Site Description
  
 
  Just as our reputation has spread beyond our Minnesota roots, so have our locations. Today, our employees are located at our three major campuses in Phoenix/Scottsdale, Arizona, Jacksonville, Florida, Rochester, Minnesota, and at Mayo Clinic Health System campuses throughout Midwestern communities, and at our international locations. Each Mayo Clinic location is a special place where our employees thrive in both their work and personal lives. Learn more about what each unique Mayo Clinic campus has to offer, and where your best fit is.
   
 
 
   
  
   Affirmative Action and Equal Opportunity Employer 
  
  
    As an Affirmative Action and Equal Opportunity Employer Mayo Clinic is committed to creating an inclusive environment that values the diversity of its employees and does not discriminate against any employee or candidate. Women, minorities, veterans, people from the LGBTQ communities and people with disabilities are strongly encouraged to apply to join our teams. Reasonable accommodations to access job openings or to apply for a job are available.
    
  
 
  
  
 Recruiter
  
  Miranda Grabner","<div>
 <div>
  <b>Why Mayo Clinic</b> 
 </div> 
 <div>
  <br> Mayo Clinic is top-ranked in more specialties than any other care provider according to U.S. News &amp; World Report. As we work together to put the needs of the patient first, we are also dedicated to our employees, investing in competitive compensation and comprehensive benefit plans &#x2013; to take care of you and your family, now and in the future. And with continuing education and advancement opportunities at every turn, you can build a long, successful career with Mayo Clinic. You&#x2019;ll thrive in an environment that supports innovation, is committed to ending racism and supporting diversity, equity and inclusion, and provides the resources you need to succeed.
 </div>
 <br> 
 <br> 
 <b>Responsibilities</b>
 <br> 
 <p> As a member of the Data and Analytics organization, you will be responsible for building and delivering best-in-class clinical data initiatives aimed at driving best-in-class solutions. You will collaborate with analytic partners and business partners from product strategy, program management, IT, data strategy, and predictive analytics teams to develop effective solutions for our partners.</p> 
 <p> Lead data design, prototype, and development of data pipeline architecture pipelines. Lead implementation of internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability. Lead cause analysis on external and internal processes and data to identify opportunities for improvement and answer questions. Excellent analytic skills associated with working on unstructured datasets. Understand the architecture, be a team player, lead technical discussions and communicate the technical discussion. Be a senior Individual contributor of the Data or Software Engineering teams. Be part of Technical Review Board along with Manager and Principal Engineer. Be a technical liaison between Manager, Software Engineers and Principal Engineers. Collaborate with software engineers to analyze, develop and test functional requirements. Write clean, maintainable code 30% of the time and performing peer code-reviews. Mentor and Coach Engineers. Work with team members to investigate design approaches, prototype new technology and evaluate technical feasibility. Work in an Agile/Safe/Scrum environment to deliver high quality software. Establish architectural principles, select design patterns, and then mentor team members on their appropriate application. Facilitate and drive communication between front-end, back-end, data and platform engineers. Play a formal Engineering lead role in the area of expertise. Keep up-to-date with industry trends and developments.</p> 
 <p> Job Responsibilities:</p> 
 <ul> 
  <li>Act as Product Owner for Data platform&#x2019;s and Lead the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.</li> 
  <li>Evaluate the full technology stack of services required including PaaS, IaaS, SaaS, DataOps, operations, availability, and automation.</li> 
  <li>Research, design, and develop Public &amp; Private Data Solutions, including impacts to enterprise architecture</li> 
  <li>Build high-performing clinical data processing frameworks leveraging Google Cloud Platform, GCP Shared Services like Google Healthcare API, Big Query, and HL7 FHIR store.</li> 
  <li>Participate in evaluation of supporting technologies and industry best practices with our cloud partners and peer teams.</li> 
  <li>Lead Modern Data Warehouse Solutions and Sizing efforts to create defined plans and work estimates for customer proposals and Statements of work.</li> 
  <li>Conduct full technical discovery, identifying pain points, business, and technical requirements, &#x201c;as is&#x201d; and &#x201c;to be&#x201d; scenarios!</li> 
  <li>Design and Develop clinical data pipelines integrating ingestion, harmonization, and consumption frameworks for onboarding clinical data from various data sources formatted in various industry standards (FHIR, C-CDA, HL7 V2, JSON, XML, etc.).</li> 
  <li>Build state-of-the-art data pipelines supporting both batch and real-time streams to enable Clinical data collection, storage, processing, transformation, aggregation, and dissemination through heterogeneous channels.</li> 
  <li>Build design specifications for health care data objects and surrounding data processing logic.</li> 
  <li>Lead innovation and research building proof of concepts for complex transformations, notification engines, analytical engines, and self-service analytics</li> 
  <li>Bring a DevOps mindset to enable big data and batch/real-time analytical solutions that leverage emerging technologies.</li>
 </ul>
 <br> 
 <br> 
 <b>Qualifications</b>
 <br> 
 <p> Bachelor&#x2019;s Degree in Computer Science/Engineering or related field with 6 years of experience OR an Associate&#x2019;s degree in Computer Science/Engineering or related field with 8 years of experience.</p>
 <p> Knowledge of professional software engineering practices and best practices for the full software development life cycle (SDLC), including coding standards, code reviews, source control management, build processes, testing, and operations. Have in-depth knowledge of data engineering and building data pipelines with a minimum of 5 years of experience in data engineering, data science or analytical modeling and basic knowledge of related disciplines. Worked and lead Data Engineering teams in Continuous Integration / Continuous Delivery model. Build/Lead Data products highly resilient in nature. Build/Lead Test Automation suites, Unit Testing coverage, Data Quality, Monitoring &amp; Observability. A minimum experience of 5 years using relational databases and NoSQL Databases. Experience with cloud platforms such as GCP, Azure, AWS.</p>
 <p> Continuous Integration using Jenkins, Git Hub Actions or Azure Pipelines. Experience with cloud technologies, development and deployment. Experience with tools like Jira, GitHub, SharePoint, Azure Boards. Experience using advanced data processing solutions/capabilities such as Apache Spark, Hive, Airflow and Kafka, GCP Dataflow. Experience using big data, statistics and knowledge of data related aspects of machine learning. Experience with Google BigQuery, FHIR APIs, and Vertex AI. Knowledge of how workflow scheduling solutions such as Apache Airflow and Google Composer related to data systems. Knowledge of using Infrastructure as code (Kubernetes, Docker) in a cloud environment.</p>
 <ul>
  <li> Hands-on experience in architecture, design, and development of enterprise data applications and analytics solutions within the health care domain</li>
  <li> Experience in Google Cloud Platform/Shared Services such as Cloud Dataflow, Cloud Storage, Pub/sub, Cloud Composer, Big Query, and Health care API (FHIR store)</li>
  <li> They should be able to deliver an ingestion framework for relational data sources, understand layers and rules of a data lake and carry out all the tasks to operationalize data pipelines.</li>
  <li> Experience in Python, Java, Spark, Airflow, and Kafka development</li>
  <li> Hands-on experience working with &#x201c;Big Data&#x201d; technologies and experience with traditional RDBMS, Python, Unix Shell scripting, JSON, and XML</li>
  <li> Experience working with tools to automate CI/CD pipelines (e.g., Jenkins, GIT)</li>
  <li> Must have great articulation and communication skills.</li>
  <li> Working in a fluid environment, defining, and owning priorities that adapt to our larger goals. You can bring clarity to ambiguity while remaining open-minded to new information that might change your mind.</li>
  <li> Should have a strong understanding of healthcare data, including clinical data in proprietary and industry-standard formats.</li>
  <li> Participate in architectural discussions, perform system analysis which involves a review of the existing systems and operating methodologies. Participate in the analysis of newest technologies and suggest the optimal solutions which will be best suited for satisfying the current requirements and will simplify the future modifications</li>
  <li> Design appropriate data models for the use in transactional and big data environments as an input into Machine Learning processing.</li>
  <li> Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability</li>
  <li> Design and Build the necessary infrastructure for optimal ETL from a variety of data sources to be used on GCP services</li>
  <li> Collaborate with multiple stakeholders including Product Teams, Data Domain Owners, Infrastructure, Security and Global IT</li>
  <li> Identify, Implement, and continuously enhance the data automation process</li>
  <li> Develop proper Data Governance and Data Security</li>
  <li> Demonstrate strategic thinking and strong planning skills to establish long term roadmap and business plan</li>
  <li> Work with stakeholders to establish and meet data quality requirements, SLAs and SLOs for data ingestion</li>
  <li> Experience in Self-service Analytics/Visualization tools like PowerBI, Looker, Tableau</li>
  <li> Proven knowledge in implementing security &amp; IAM requirements</li>
  <li> Experience building and maintaining a Data-Lake with DeltaLake</li>
  <li> Experience with ETL/ELT/DataMesh frameworks</li>
  <li> Experience with GCP Dataplex (Data Catalog, Clean Rooms)</li>
 </ul>
 <p> Authorization to work and remain in the United States, without necessity for Mayo Clinic sponsorship now, or in the future (for example, be a U.S. Citizen, national, or permanent resident, refugee, or asylee). Also, Mayo Clinic does not participate in the F-1 STEM OPT extension program.</p>
 <b><br> Exemption Status</b>
 <br> 
 <br> Exempt
 <br> 
 <br> 
 <b>Compensation Detail</b>
 <br> 
 <br> &#x24;138,236.80 - &#x24;200,408.00 / year
 <br> 
 <br> 
 <b>Benefits Eligible</b>
 <br> 
 <br> Yes
 <br> 
 <br> 
 <b>Schedule</b>
 <br> 
 <br> Full Time
 <br> 
 <br> 
 <b>Hours/Pay Period</b>
 <br> 
 <br> 80
 <br> 
 <br> 
 <b>Schedule Details</b>
 <br> 
 <br> Monday - Friday, 8:00 am - 5:00 pm
 <br> 
 <br> 
 <b>Weekend Schedule</b>
 <br> 
 <br> As needed
 <br> 
 <br> 
 <b>International Assignment</b>
 <br> 
 <br> No
 <br> 
 <br> 
 <b>Site Description</b>
 <br> 
 <div>
  Just as our reputation has spread beyond our Minnesota roots, so have our locations. Today, our employees are located at our three major campuses in Phoenix/Scottsdale, Arizona, Jacksonville, Florida, Rochester, Minnesota, and at Mayo Clinic Health System campuses throughout Midwestern communities, and at our international locations. Each Mayo Clinic location is a special place where our employees thrive in both their work and personal lives. Learn more about what each unique Mayo Clinic campus has to offer, and where your best fit is.
  <br> 
 </div>
 <div>
  <br> 
  <div>
   <b>Affirmative Action and Equal Opportunity Employer</b> 
  </div>
  <div>
   <br> As an Affirmative Action and Equal Opportunity Employer Mayo Clinic is committed to creating an inclusive environment that values the diversity of its employees and does not discriminate against any employee or candidate. Women, minorities, veterans, people from the LGBTQ communities and people with disabilities are strongly encouraged to apply to join our teams. Reasonable accommodations to access job openings or to apply for a job are available.
   <br> 
  </div>
 </div>
 <br> 
 <br> 
 <b>Recruiter</b>
 <br> 
 <br> Miranda Grabner
</div>","https://www.indeed.com/rc/clk?jk=90ca40677b46e6ea&atk=&xpse=SoDJ67I3JucQ54QSDZ0LbzkdCdPP","90ca40677b46e6ea",,"Full-time",,"3636 Technology Dr NW, Rochester, MN 55901","IT Lead Data Engineer - Remote","1 day ago","2023-10-22T13:05:24.334Z","3.9","3032","$138,237 - $200,408 a year","2023-10-23T13:05:25.038Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=90ca40677b46e6ea&from=jasx&tk=1hdea7hq42do5000&vjs=3"
"General Dynamics Information Technology","Clearance Level None Category Data Science Location Remote, Working from District of Columbia 


 Public Trust: BI Full 6C (T4) 
 Requisition Type: Pipeline 
 Your Impact 
 Own your opportunity to manage the network that makes mission success possible. Make an impact by using your skills to deliver “One GDIT Network” for our clients.
  Job Description
 
   Deliver simple solutions to complex problems as a Data Engineer at GDIT. Here, you’ll tailor cutting-edge solutions to the unique requirements of our clients. With a career in application development, you’ll make the end user’s experience your priority and we’ll make your career growth ours.
  
   At GDIT, people are our differentiator. As a Data Engineer you will help ensure today is safe and tomorrow is smarter. Our work depends on a Data Engineer joining our team to help improve Aviation safety in collaboration with our stakeholders. Our customers deal with real-world safety problems and this candidate can help develop the required software designs and code to scale our system and improve user insights into safety issues. Applies fundamental concepts, processes, practices, and procedures on technical assignments. Performs work that requires practical experience and training. Work is performed under supervision.
   HOW A SOFTWARE ENGINEER WILL MAKE AN IMPACT:
  
    Codes, tests, debugs, implements, and documents low to highly complex programs.
    Creates appropriate documentation in work assignments such as program code, and technical documentation.
    Designs systems and programs to meet complex business needs.
    Prepares detailed specifications from which programs are developed and coded.
    Ensures programs meet standards and technical specifications; performs technical analysis and component delivery.
    Gathers information from existing systems, analyzes program and time requirements.
    Assists project manager in preparing time estimates and justification for assigned tasks.
    Designs programs for projects or enhancements to existing programs. Writes specifications for programs of low to advanced complexity.
    Assists support and/or project personnel in resolving varying levels of complex program problems.
    Works with client and management to resolve issues and validate programming requirements within their areas of responsibility.
    Provides technical advice on complex programming.
    Develops test plans to verify logic of new or modified programs.
    Conducts quality assurance activities such as peer reviews.
    Creates appropriate documentation in work assignments such as program code, and technical documentation.
    Remains abreast of industry technical trends and new development to maintain current skills and remain current with industry standards.
  
  
  
   Education and Required Experience: Bachelor’s Degree 2+ years experience
   Required Technical Skills: AWS Cloud Engineer
   Security Clearance Level: Public Trust
   Required Skills and Abilities: AWS Glue, AWS Lambda, Python, JIRA
   Location: Remote
   US Citizenship Required
  
  
   Preferred Skills 
  
   Preferred Skills: AWS Athena, AWS CI/CD pipeline
    Secret Clearance or ability to obtain a clearance
    Home location of DMV
  
  
    GDIT IS YOUR PLACE:
    
   
    Full-flex work week to own your priorities at work and at home
    401K with company match
    Comprehensive health and wellness packages
    Internal mobility team dedicated to helping you own your career
    Professional growth opportunities including paid education and certifications
    Cutting-edge technology you can learn from
    Rest and recharge with paid vacation and holidays
   
  
  
    GDIT IS YOUR PLACE:
    
   
    Full-flex work week to own your priorities at work and at home
    401K with company match
    Comprehensive health and wellness packages
    Internal mobility team dedicated to helping you own your career
    Professional growth opportunities including paid education and certifications
    Cutting-edge technology you can learn from
    Rest and recharge with paid vacation and holidays","Clearance Level None Category Data Science Location Remote, Working from District of Columbia 
<br>
<div>
 <h5 class=""jobSectionHeader""><b>Public Trust: </b><b>BI Full 6C (T4)</b><b> </b></h5>
 <h5 class=""jobSectionHeader""><b>Requisition Type: </b><b>Pipeline</b><b> </b></h5>
 <h5 class=""jobSectionHeader""><b>Your Impact </b></h5>
 <p>Own your opportunity to manage the network that makes mission success possible. Make an impact by using your skills to deliver &#x201c;One GDIT Network&#x201d; for our clients.</p>
 <h5 class=""jobSectionHeader""><b> Job Description</b></h5>
 <div>
  <p> Deliver simple solutions to complex problems as a Data Engineer at GDIT. Here, you&#x2019;ll tailor cutting-edge solutions to the unique requirements of our clients. With a career in application development, you&#x2019;ll make the end user&#x2019;s experience your priority and we&#x2019;ll make your career growth ours.</p>
  <p></p>
  <p> At GDIT, people are our differentiator. As a Data Engineer you will help ensure today is safe and tomorrow is smarter. Our work depends on a Data Engineer joining our team to help improve Aviation safety in collaboration with our stakeholders. Our customers deal with real-world safety problems and this candidate can help develop the required software designs and code to scale our system and improve user insights into safety issues.<br> Applies fundamental concepts, processes, practices, and procedures on technical assignments. Performs work that requires practical experience and training. Work is performed under supervision.</p>
  <p><b><br> HOW A SOFTWARE ENGINEER WILL MAKE AN IMPACT:</b></p>
  <ul>
   <li> Codes, tests, debugs, implements, and documents low to highly complex programs.</li>
   <li> Creates appropriate documentation in work assignments such as program code, and technical documentation.</li>
   <li> Designs systems and programs to meet complex business needs.</li>
   <li> Prepares detailed specifications from which programs are developed and coded.</li>
   <li> Ensures programs meet standards and technical specifications; performs technical analysis and component delivery.</li>
   <li> Gathers information from existing systems, analyzes program and time requirements.</li>
   <li> Assists project manager in preparing time estimates and justification for assigned tasks.</li>
   <li> Designs programs for projects or enhancements to existing programs. Writes specifications for programs of low to advanced complexity.</li>
   <li> Assists support and/or project personnel in resolving varying levels of complex program problems.</li>
   <li> Works with client and management to resolve issues and validate programming requirements within their areas of responsibility.</li>
   <li> Provides technical advice on complex programming.</li>
   <li> Develops test plans to verify logic of new or modified programs.</li>
   <li> Conducts quality assurance activities such as peer reviews.</li>
   <li> Creates appropriate documentation in work assignments such as program code, and technical documentation.</li>
   <li> Remains abreast of industry technical trends and new development to maintain current skills and remain current with industry standards.</li>
  </ul>
  <p></p>
  <ul>
   <li>Education and Required Experience: Bachelor&#x2019;s Degree 2+ years experience</li>
   <li>Required Technical Skills: AWS Cloud Engineer</li>
   <li>Security Clearance Level: Public Trust</li>
   <li>Required Skills and Abilities<b>: AWS Glue, AWS Lambda, Python, JIRA</b></li>
   <li>Location: Remote</li>
   <li><b>US Citizenship Required</b></li>
  </ul>
  <p></p>
  <p><b> Preferred Skills</b><b> </b></p>
  <ul>
   <li>Preferred Skills: AWS Athena, AWS CI/CD pipeline</li>
   <li> Secret Clearance or ability to obtain a clearance</li>
   <li> Home location of DMV</li>
  </ul>
  <div>
   <b><br> GDIT IS YOUR PLACE:</b>
   <br> 
   <ul>
    <li>Full-flex work week to own your priorities at work and at home</li>
    <li>401K with company match</li>
    <li>Comprehensive health and wellness packages</li>
    <li>Internal mobility team dedicated to helping you own your career</li>
    <li>Professional growth opportunities including paid education and certifications</li>
    <li>Cutting-edge technology you can learn from</li>
    <li>Rest and recharge with paid vacation and holidays</li>
   </ul>
  </div>
  <div>
   <b><br> GDIT IS YOUR PLACE:</b>
   <br> 
   <ul>
    <li>Full-flex work week to own your priorities at work and at home</li>
    <li>401K with company match</li>
    <li>Comprehensive health and wellness packages</li>
    <li>Internal mobility team dedicated to helping you own your career</li>
    <li>Professional growth opportunities including paid education and certifications</li>
    <li>Cutting-edge technology you can learn from</li>
    <li>Rest and recharge with paid vacation and holidays</li>
   </ul>
  </div>
 </div>
</div>","https://www.indeed.com/rc/clk?jk=b313953a37e916a3&atk=&xpse=SoDZ67I3JucQ4Dw9cp0LbzkdCdPP","b313953a37e916a3",,,,"Washington, DC","TSS Data Engineer","1 day ago","2023-10-22T13:05:26.016Z","3.7","5652",,"2023-10-23T13:05:26.017Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=b313953a37e916a3&from=jasx&tk=1hdea7hq42do5000&vjs=3"
"Vision Government Solutions Inc","About Vision
  Vision Government Solutions is a leading government technology firm providing cutting-edge software to the public sector. We are at an incredible inflection point of growth and are looking for exceptional individuals to join our Software Implementation team to help us successfully welcome new communities to our Vision software.
  Our software implementation philosophy emphasizes the importance of customer delight, speed, and long-term partnership. To that end, we are searching for ambitious, motivated, detail-oriented individuals looking to further a career in Implementation Engineering. The right candidate will be driven by customer happiness, be obsessed with continuous improvement, and have strong data engineering and ETL toolkit to master the vast landscape of unique implementation projects we encounter.
  Summary of Role & Responsibilities
  The Data Engineer (Software Implementation) will primarily be responsible for writing and deploying custom data conversions, working in concert with technical project managers and architects. Sample responsibilities include:
  Data Engineering
 
   Deliver custom database conversions during new customer implementation projects
   Migrate data from competitor solutions to our in-house software
   Upgrade existing customers from our previous Oracle/VB6 software version to our current SQL Server/C#/.Net product
   Assist with the development of ad hoc T-SQL scripts for occasional data engineering needs
   Collaborate with the Continuous Improvement team to streamline future processes, automate repeatable tasks, improve quality assurance, and minimize rework
   Develop custom routines to convert graphical building drawings from various formats, including (but not limited to) Traverse, SVG, AutoCAD DXF, and binary, to our proprietary Sketch XML format
   Create and alter client analytics dashboards using PowerBI
   Embrace agile methodologies to achieve industry-leading project delivery schedules while maintaining ongoing customer engagement and excitement
   Partner with project teams to provide a seamless customer transition experience
 
 
  Role Evolution
  This role will begin with a primary focus on municipal assessment software implementation projects (~90% of the time), with the remaining 10% focused on continuous improvement initiatives and team growth. In addition, role evolution may include calibration and modeling efforts, branching into our SaaS product world, advanced data engineering opportunities, and exciting continuous improvement projects.
  Who We Are Looking For
  The ideal person for this role will have demonstrated the following abilities and traits:
  Skills and Ambitions:
 
   Ability to thrive in a fast-paced, ever-changing landscape
   5 + years of experience working in a SQL-focused conversion role
   Solid knowledge of T-SQL and SQL Server Management Studio
   Familiarity with Oracle SQL*Plus is desired but not required
   Experience with ETL and Data Migration
   Understanding of relational database concepts
   Impressive oral and written communication skills
   Exceptional time management capabilities; deadlines are not flexible
   Ability to communicate and translate ideas between technical and non-technical parties
   SQL Server 2012+/SSRS/SSIS/Jira/GitHub/Confluence
   Skills using Apache NiFi, R, or Python are a plus but not required
   BS/BA in Computer Science or related field preferred
 
  Total Compensation Package: To be discussed
  Benefits Package: Vision offers health, dental, and vision plans, as well as a 401(k)-matching program.
  Work Location: Remote
  Equal Employment Opportunity 
 Vision Government Solutions is an Equal Opportunity Employer and committed to a diverse and inclusive workplace. All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law. 
 We're proud to be an equal opportunity employer and celebrate our employees' differences, including race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability and Veteran status. 
 Vision Government Solutions maintains a drug-free workplace.
  
 R8YtQTsBA6","<div>
 <p><b>About Vision</b></p>
 <p> Vision Government Solutions is a leading government technology firm providing cutting-edge software to the public sector. We are at an incredible inflection point of growth and are looking for exceptional individuals to join our Software Implementation team to help us successfully welcome new communities to our Vision software.</p>
 <p> Our software implementation philosophy emphasizes the importance of customer delight, speed, and long-term partnership. To that end, we are searching for ambitious, motivated, detail-oriented individuals looking to further a career in Implementation Engineering. The right candidate will be driven by customer happiness, be obsessed with continuous improvement, and have strong data engineering and ETL toolkit to master the vast landscape of unique implementation projects we encounter.</p>
 <p><b> Summary of Role &amp; Responsibilities</b></p>
 <p> The Data Engineer (Software Implementation) will primarily be responsible for writing and deploying custom data conversions, working in concert with technical project managers and architects. Sample responsibilities include:</p>
 <p><b> Data Engineering</b></p>
 <ul>
  <li> Deliver custom database conversions during new customer implementation projects</li>
  <li> Migrate data from competitor solutions to our in-house software</li>
  <li> Upgrade existing customers from our previous Oracle/VB6 software version to our current SQL Server/C#/.Net product</li>
  <li> Assist with the development of ad hoc T-SQL scripts for occasional data engineering needs</li>
  <li> Collaborate with the Continuous Improvement team to streamline future processes, automate repeatable tasks, improve quality assurance, and minimize rework</li>
  <li> Develop custom routines to convert graphical building drawings from various formats, including (but not limited to) Traverse, SVG, AutoCAD DXF, and binary, to our proprietary Sketch XML format</li>
  <li> Create and alter client analytics dashboards using PowerBI</li>
  <li> Embrace agile methodologies to achieve industry-leading project delivery schedules while maintaining ongoing customer engagement and excitement</li>
  <li> Partner with project teams to provide a seamless customer transition experience</li>
 </ul>
 <p></p>
 <p><b><br> Role Evolution</b></p>
 <p> This role will begin with a primary focus on municipal assessment software implementation projects (~90% of the time), with the remaining 10% focused on continuous improvement initiatives and team growth. In addition, role evolution may include calibration and modeling efforts, branching into our SaaS product world, advanced data engineering opportunities, and exciting continuous improvement projects.</p>
 <p><b> Who We Are Looking For</b></p>
 <p> The ideal person for this role will have demonstrated the following abilities and traits:</p>
 <p><b> Skills and Ambitions:</b></p>
 <ul>
  <li> Ability to thrive in a fast-paced, ever-changing landscape</li>
  <li> 5 + years of experience working in a SQL-focused conversion role</li>
  <li> Solid knowledge of T-SQL and SQL Server Management Studio</li>
  <li> Familiarity with Oracle SQL*Plus is desired but not required</li>
  <li> Experience with ETL and Data Migration</li>
  <li> Understanding of relational database concepts</li>
  <li> Impressive oral and written communication skills</li>
  <li> Exceptional time management capabilities; deadlines are not flexible</li>
  <li> Ability to communicate and translate ideas between technical and non-technical parties</li>
  <li> SQL Server 2012+/SSRS/SSIS/Jira/GitHub/Confluence</li>
  <li> Skills using Apache NiFi, R, or Python are a plus but not required</li>
  <li> BS/BA in Computer Science or related field preferred</li>
 </ul>
 <p><b> Total Compensation Package:</b> To be discussed</p>
 <p><b> Benefits Package:</b> Vision offers health, dental, and vision plans, as well as a 401(k)-matching program.</p>
 <p><b> Work Location:</b> Remote</p>
 <p><b><i> Equal Employment Opportunity</i></b></p> 
 <p><i>Vision Government Solutions is an Equal Opportunity Employer and committed to a diverse and inclusive workplace. All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.</i></p> 
 <p><i>We&apos;re proud to be an equal opportunity employer and celebrate our employees&apos; differences, including race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability and Veteran status.</i></p> 
 <p><i>Vision Government Solutions maintains a drug-free workplace.</i></p>
 <p> </p>
 <p>R8YtQTsBA6</p>
</div>","https://www.indeed.com/rc/clk?jk=8a288fca6ecd6019&atk=&xpse=SoBy67I3JucQO6AB1j0LbzkdCdPP","8a288fca6ecd6019",,"Full-time",,"Remote","Data Engineer","2 days ago","2023-10-21T13:05:31.115Z","3.4","13",,"2023-10-23T13:05:31.117Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=8a288fca6ecd6019&from=jasx&tk=1hdea7hq42do5000&vjs=3"
"ASCENDING","Location: 100% Remote (Driving distance to Ashburn, VA within 5 hours preferred) Contract Duration: 5 years. Security Clearance Requirement: Must be a U.S. Citizen to obtain clearance
  Job Description:
  We are seeking a highly skilled and experienced Senior Azure Data Engineer to join our team on a long-term contract. The successful candidate will play a critical role in designing, implementing, and optimizing data solutions using Microsoft Azure technologies. This position is 100% remote, but proximity to Ashburn, VA within a 5-hour driving distance is preferred.
  Responsibilities:
 
   Collaborate with cross-functional teams to design, develop, and maintain data solutions on the Azure cloud platform.
   Build and optimize data pipelines using Azure Data Factory, Azure Databricks, and related services.
   Develop, test, and deploy ETL processes for data transformation and integration.
   Work with programming languages such as Python and SQL to manipulate and analyze data.
   Ensure data security and compliance with Homeland Security requirements.
 
  Qualifications:
  The ideal candidate will possess the following qualifications:
 
   Bachelor's or Master's degree in Computer Science, Engineering, or a related field.
   A minimum of 7 years of experience in the field of Information Technology.
   Proven experience of at least 3 years working as a Data Engineer with a primary focus on Microsoft Azure technologies, with recent hands-on experience being highly preferred.
   Strong expertise in designing, building, and optimizing data pipelines using Azure Data Factory, Azure Databricks, and related services.
   Proficiency in programming languages such as Python and SQL, with a solid understanding of ETL frameworks.
   Possession of relevant Azure certifications, such as Microsoft Certified: Azure Data Engineer, is a plus.
 
  We are looking for a self-motivated and detail-oriented individual who can work independently and collaboratively to meet the unique data engineering needs of Homeland Security. If you are a U.S. Citizen with a passion for cutting-edge technology and a desire to contribute to national security, we encourage you to apply.
  To apply for this position, please submit your resume and a cover letter outlining your relevant experience and qualifications.
  
 ebrOM6Ntph","<div>
 <p>Location: <b>100% Remote</b> (Driving distance to Ashburn, VA within 5 hours preferred)<br> Contract Duration: 5 years.<br> Security Clearance Requirement: Must be a<b> U.S. Citizen to obtain clearance</b></p>
 <p><b> Job Description</b>:</p>
 <p> We are seeking a highly skilled and experienced Senior Azure Data Engineer to join our team on a long-term contract. The successful candidate will play a critical role in designing, implementing, and optimizing data solutions using Microsoft Azure technologies. This position is 100% remote, but proximity to Ashburn, VA within a 5-hour driving distance is preferred.</p>
 <p><b> Responsibilities</b>:</p>
 <ul>
  <li> Collaborate with cross-functional teams to design, develop, and maintain data solutions on the Azure cloud platform.</li>
  <li> Build and optimize data pipelines using <b>Azure</b> Data Factory, Azure Databricks, and related services.</li>
  <li> Develop, test, and deploy <b>ETL</b> processes for data transformation and integration.</li>
  <li> Work with programming languages such as <b>Python</b> and <b>SQL</b> to manipulate and analyze data.</li>
  <li> Ensure data security and compliance with Homeland Security requirements.</li>
 </ul>
 <p><b> Qualifications</b>:</p>
 <p> The ideal candidate will possess the following qualifications:</p>
 <ul>
  <li><b> Bachelor&apos;s</b> or <b>Master&apos;s</b> degree in Computer Science, Engineering, or a related field.</li>
  <li> A minimum of <b>7 years of experience</b> in the field of Information Technology.</li>
  <li> Proven experience of at least <b>3 years</b> working as a <b>Data Engineer</b> with a primary focus on Microsoft Azure technologies, with recent hands-on experience being highly preferred.</li>
  <li> Strong expertise in designing, building, and optimizing data pipelines using <b>Azure Data Factory, Azure Databricks</b>, and related services.</li>
  <li> Proficiency in programming languages such as <b>Python</b> and <b>SQL</b>, with a solid understanding of <b>ETL</b> frameworks.</li>
  <li> Possession of relevant <b>Azure</b> certifications, such as <b>Microsoft Certified: Azure Data Engineer, is a </b><b><i>plus</i></b>.</li>
 </ul>
 <p> We are looking for a self-motivated and detail-oriented individual who can work independently and collaboratively to meet the unique data engineering needs of Homeland Security. If you are a U.S. Citizen with a passion for cutting-edge technology and a desire to contribute to national security, we encourage you to apply.</p>
 <p> To apply for this position, please submit your resume and a cover letter outlining your relevant experience and qualifications.</p>
 <p> </p>
 <p>ebrOM6Ntph</p>
</div>","https://www.indeed.com/rc/clk?jk=282a7c727e0fad73&atk=&xpse=SoD467I3JucQJ4zOqJ0LbzkdCdPP","282a7c727e0fad73",,"Full-time",,"Ashburn, VA","Azure Data Engineer","2 days ago","2023-10-21T13:05:32.085Z",,,,"2023-10-23T13:05:32.086Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=282a7c727e0fad73&from=jasx&tk=1hdea7hq42do5000&vjs=3"
"KinderCare Education","Futures start here. Where first steps, new friendships, and confident learners are born. At KinderCare Learning Companies, the first and only early childhood education provider recognized with the Gallup Exceptional Workplace Award, we offer a variety of early education and child care options for families. We pave the way for their lifelong learning journey ahead. And we want you to join us in shaping a future we can all be proud to share—in neighborhoods, at work, and in schools nationwide. 
 At KinderCare Learning Companies, you’ll use your skills and expertise to support the work (and fun) that happens in our sites and centers every day. From marketers and strategists to financial analysts and data engineers, and so much more, we’re all passionate about crafting a world where children, families, and organizations can thrive. 
 As the Data Engineer, you will be responsible for the design, development, and implementation of data pipelines, data lake solutions, and data models using an Agile methodology. This role will also partner with team members to define and operationalize data governance standards, data quality and reliability processes, and data lake architecture design sessions, with a goal of optimizing the effectiveness and efficiency of product development and delivery.
   RESPONSIBILITIES: 
  
  Collaborate with team to craft, develop, and implement data pipelines and ETL systems for the data lake 
  Develop data models that support Business Intelligence products 
  Perform data validation to ensure data quality and reliability 
  Work with BI architect and Data Engineering Lead to create technical requirements and specifications for Business Intelligence products 
  Identify, tackle, and fix data pipeline Production issues 
  Build clear documentation of procedures and protocols 
  Implement data governance, data security and privacy policies and collaborate multi-functionally with technical and non-technical team members 
 
 Qualifications: 
 
  Bachelor’s degree in Computer Science, Information Systems, Engineering, Statistics, related field; or equivalent experience 
  5-7 years’ experience as a Data Engineer designing, developing, and implementing data pipelines and ETL systems and 3 years’ experience with data lake technologies and architectures (S3, Glue, Redshift, RDS, DynamoDB, Data Pipeline, EMR) 
  Ambitious knowledge in data warehousing solutions (Panoply, Redshift, etc.) with hands on experience using Cloud based database systems (Azure SQL, Bigtable, etc.) 
  Hands on development experience using Python, Perl, Scala, Java, SAS, C++ 
  Fluent in SQL, ability to write new, complex queries with no supervision; strong written and verbal communication skills 
  Experience with Agile development methodologies, CI/CD automation, Test Driven Development 
  Knowledge of standard processes in database engineering and data security with high level cloud migration & tools knowledge 
  Familiarity with machine learning methodologies and strong problem-solving skills 
  
 Our benefits meet you where you are. We’re here to help our employees navigate the integration of work and life: 
  
  Know your whole family is supported with discounted child care benefits. 
  Breathe easy with medical, dental, and vision benefits for your family (and pets, too!). 
  Feel supported in your mental health and personal growth with employee assistance programs. 
  Feel great and thrive with access to health and wellness programs, unlimited paid time off and discounts for work necessities, such as cell phones. 
  … and much more. 
  
 We operate research-backed, accredited, and customizable programs in more than 2,000 sites and centers across 40 states and the District of Columbia. As we expand, we’re matching the needs of more and more families, dynamic work environments, and diverse communities from coast to coast. Because we believe every family deserves access to high-quality child care, no matter who they are or where they live. Every day, you’ll help bring this mission to life by building community and delivering exceptional experiences. And if you’re anything like us, you’ll come for the work, and stay for the people. 
  KinderCare Learning Companies is an Equal Opportunity employer. All qualified applicants will receive consideration for employment without regard to race, national origin, age, sex, religion, disability, sexual orientation, marital status, military or veteran status, gender identity or expression, or any other basis protected by local, state, or federal law.
  Primary Location : Portland, Oregon, United States
  Job : Corporate","<div>
 <p>Futures start here. Where first steps, new friendships, and confident learners are born. At KinderCare Learning Companies, the first and only early childhood education provider recognized with the <b>Gallup Exceptional Workplace Award</b>, we offer a variety of early education and child care options for families. We pave the way for their lifelong learning journey ahead. And we want you to join us in shaping a future we can all be proud to share&#x2014;in neighborhoods, at work, and in schools nationwide. </p>
 <p>At KinderCare Learning Companies, you&#x2019;ll use your skills and expertise to support the work (and fun) that happens in our sites and centers every day. From marketers and strategists to financial analysts and data engineers, and so much more, we&#x2019;re all passionate about crafting a world where children, families, and organizations can thrive. </p>
 <p>As the Data Engineer, you will be responsible for the design, development, and implementation of data pipelines, data lake solutions, and data models using an Agile methodology. This role will also partner with team members to define and operationalize data governance standards, data quality and reliability processes, and data lake architecture design sessions, with a goal of optimizing the effectiveness and efficiency of product development and delivery.</p>
 <p><br> <b> RESPONSIBILITIES:</b></p> 
 <ul> 
  <li>Collaborate with team to craft, develop, and implement data pipelines and ETL systems for the data lake</li> 
  <li>Develop data models that support Business Intelligence products</li> 
  <li>Perform data validation to ensure data quality and reliability</li> 
  <li>Work with BI architect and Data Engineering Lead to create technical requirements and specifications for Business Intelligence products</li> 
  <li>Identify, tackle, and fix data pipeline Production issues</li> 
  <li>Build clear documentation of procedures and protocols</li> 
  <li>Implement data governance, data security and privacy policies and collaborate multi-functionally with technical and non-technical team members</li> 
 </ul>
 <h4 class=""jobSectionHeader""><b>Qualifications: </b></h4>
 <ul>
  <li>Bachelor&#x2019;s degree in Computer Science, Information Systems, Engineering, Statistics, related field; or equivalent experience</li> 
  <li>5-7 years&#x2019; experience as a Data Engineer designing, developing, and implementing data pipelines and ETL systems and 3 years&#x2019; experience with data lake technologies and architectures (S3, Glue, Redshift, RDS, DynamoDB, Data Pipeline, EMR)</li> 
  <li>Ambitious knowledge in data warehousing solutions (Panoply, Redshift, etc.) with hands on experience using Cloud based database systems (Azure SQL, Bigtable, etc.)</li> 
  <li>Hands on development experience using Python, Perl, Scala, Java, SAS, C++</li> 
  <li>Fluent in SQL, ability to write new, complex queries with no supervision; strong written and verbal communication skills</li> 
  <li>Experience with Agile development methodologies, CI/CD automation, Test Driven Development</li> 
  <li>Knowledge of standard processes in database engineering and data security with high level cloud migration &amp; tools knowledge</li> 
  <li>Familiarity with machine learning methodologies and strong problem-solving skills</li> 
 </ul> 
 <p><b>Our benefits meet you where you are.</b> We&#x2019;re here to help our employees navigate the integration of work and life:</p> 
 <ul> 
  <li>Know your whole family is supported with <b>discounted child care benefits.</b></li> 
  <li>Breathe easy with <b>medical, dental, and vision benefits</b> for your family (and pets, too!).</li> 
  <li>Feel supported in your mental health and personal growth with <b>employee assistance programs.</b></li> 
  <li>Feel great and thrive with access to <b>health and wellness programs, unlimited paid time off </b>and <b>discounts</b> for work necessities, such as cell phones.</li> 
  <li>&#x2026; and much more.</li> 
 </ul> 
 <p>We operate research-backed, accredited, and customizable programs in more than 2,000 sites and centers across 40 states and the District of Columbia. As we expand, we&#x2019;re matching the needs of more and more families, dynamic work environments, and diverse communities from coast to coast. Because we believe every family deserves access to high-quality child care, no matter who they are or where they live. Every day, you&#x2019;ll help bring this mission to life by building community and delivering exceptional experiences. And if you&#x2019;re anything like us, you&#x2019;ll come for the work, and stay for the people.</p> 
 <p> KinderCare Learning Companies is an Equal Opportunity employer. All qualified applicants will receive consideration for employment without regard to race, national origin, age, sex, religion, disability, sexual orientation, marital status, military or veteran status, gender identity or expression, or any other basis protected by local, state, or federal law.</p>
 <p> Primary Location : Portland, Oregon, United States</p>
 <p> Job : Corporate</p>
</div>","https://www.indeed.com/rc/clk?jk=707d3fdd870a23bd&atk=&xpse=SoA667I3JucvxPxcBx0LbzkdCdPP","707d3fdd870a23bd",,"Full-time",,"Portland, OR 97204","Data Engineer - Remote Opportunity","2 days ago","2023-10-21T13:05:33.118Z","2.7","768",,"2023-10-23T13:05:33.120Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=707d3fdd870a23bd&from=jasx&tk=1hdea7hq42do5000&vjs=3"
"Nuna","At Nuna, our mission is to make high-quality healthcare affordable and accessible for everyone. We are dedicated to tackling one of our nation's biggest problems with ingenuity, creativity, and a keen moral compass.  Nuna is committed to simple principles: a rigorous understanding of data, modern technology, and most importantly, compassion and care for our fellow human. We want to know what really works, what doesn't—and why. 
   Nuna partners with healthcare payers, including government agencies and health plans, to turn data into learnings and information into meaning.
 
  YOUR TEAM 
  We build technology to enable users (from data scientists to analysts to policy-makers) to understand healthcare data while ensuring its integrity, security and privacy. Our work runs the gamut from joining streams of messy real-world data to building queryable data warehouses to constructing visualizations and dashboards that provide actionable insight. We build systems that are auditable, automated, an accurate representation of the underlying data, and, most importantly, responsive to our end users' needs. We strive for a creative, collaborative engineering environment that implements best practices of peer review, readability, maintainability, and security of the code base and infrastructure. 
  The Nuna Integration team is responsible for developing the automation platform that ingests customer and third-party data into the Nuna value-based care cloud. This role primarily focuses on backend software development. We work closely with health data experts to combine engineering excellence with healthcare expertise, addressing the diverse data needs of our customers. Our tasks include integrating large datasets and orchestrating various processes. We measure the success of our products by their ability to improve healthcare and reduce costs. 
  YOUR OPPORTUNITIES 
  In this role, you will play a pivotal role in shaping and executing our overall architecture and strategy. You will have the opportunity to work with cutting-edge technologies such as generative AI and collaborate with a highly skilled team of engineers and health data experts. Your work will directly impact our ability to acquire, transform, and deliver high-quality data to drive critical business decisions. 
  Your responsibilities will include: 
  
  Architect and Develop Data Ingestion Solutions: Design, develop, and maintain robust and scalable data ingestion pipelines that efficiently collect data from various sources, ensuring data quality and reliability. 
  Data Curation and Transformation: Transform and curate raw data into structured and usable formats. Implement data validation, cleaning, and enrichment processes to maintain data integrity 
  Performance Optimization: Continuously optimize data ingestion and curation processes for speed, efficiency, and scalability 
  Collaboration: Collaborate with cross-functional teams including data scientists, data analysts, and domain experts to understand data requirements and deliver actionable insights 
  Quality Assurance: Implement best practices for data quality monitoring, validation, and error handling to ensure data accuracy and reliability 
  Documentation: Maintain comprehensive documentation for data ingestion and curation processes, making it easy for team members to understand and use the pipelines 
  Mentorship: Provide technical leadership and mentorship to junior engineers, fostering their growth and development 
  Stay Current: Keep abreast of emerging technologies and industry best practices in data engineering and data management 
 
 QUALIFICATIONS 
 Required Qualifications 
 
  Bachelor's or Master's degree in Computer Science, Engineering, or related field 
  8 years of experience in software engineering with a focus on backend software development 
  Proficiency in programming languages such as Python, Javascript, Typescript, Go or Java 
  Strong experience with data processing frameworks and tools, e.g. Apache Spark or similar 
  Deep understanding of data storage and retrieval technologies, both relational and NoSQL databases 
  Experience with data modeling, ETL processes, and data warehousing 
  Excellent problem-solving skills and a passion for delivering high-quality data solutions 
  Ability to diagram, articulate, and document data science and engineering concepts 
  Strong communication and collaboration skills 
 
 Preferred Qualifications 
 
  Experience with cloud technologies in AWS or GCP, as well as container systems, such as Docker or Kubernetes 
  Familiarity with orchestration tools such as Airflow or Prefect 
  Experience with Great Expectations library 
  
 We take into account an individual's qualifications, skillset, and experience in determining final salary. This role is eligible for health insurance, life insurance, retirement benefits, participation in the company's equity program, paid time off, including vacation and sick leave. The expected salary range for this position is $165,000 to $230,000. The actual offer will be at the company's sole discretion and determined by relevant business considerations, including the final candidate's qualifications, years of experience, and skillset.
 
   Nuna is an Equal Employment Opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, disability, genetics and/or veteran status.","<div>
 <div>
  <p><i>At Nuna, our mission is to make high-quality healthcare affordable and accessible for everyone. We are dedicated to tackling one of our nation&apos;s biggest problems with ingenuity, creativity, and a keen moral compass.</i><br> <br> <i>Nuna is committed to simple principles: a rigorous understanding of data, modern technology, and most importantly, compassion and care for our fellow human. We want to know what really works, what doesn&apos;t&#x2014;and why.</i></p> 
  <p><i> Nuna partners with healthcare payers, including government agencies and health plans, to turn data into learnings and information into meaning.</i></p>
 </div>
 <h2 class=""jobSectionHeader""><b> YOUR TEAM</b></h2> 
 <p> We build technology to enable users (from data scientists to analysts to policy-makers) to understand healthcare data while ensuring its integrity, security and privacy. Our work runs the gamut from joining streams of messy real-world data to building queryable data warehouses to constructing visualizations and dashboards that provide actionable insight. We build systems that are auditable, automated, an accurate representation of the underlying data, and, most importantly, responsive to our end users&apos; needs. We strive for a creative, collaborative engineering environment that implements best practices of peer review, readability, maintainability, and security of the code base and infrastructure.</p> 
 <p> The Nuna Integration team is responsible for developing the automation platform that ingests customer and third-party data into the Nuna value-based care cloud. This role primarily focuses on backend software development. We work closely with health data experts to combine engineering excellence with healthcare expertise, addressing the diverse data needs of our customers. Our tasks include integrating large datasets and orchestrating various processes. We measure the success of our products by their ability to improve healthcare and reduce costs.</p> 
 <h2 class=""jobSectionHeader""><b> YOUR OPPORTUNITIES</b></h2> 
 <p> In this role, you will play a pivotal role in shaping and executing our overall architecture and strategy. You will have the opportunity to work with cutting-edge technologies such as generative AI and collaborate with a highly skilled team of engineers and health data experts. Your work will directly impact our ability to acquire, transform, and deliver high-quality data to drive critical business decisions.</p> 
 <p> Your responsibilities will include:</p> 
 <ul> 
  <li>Architect and Develop Data Ingestion Solutions: Design, develop, and maintain robust and scalable data ingestion pipelines that efficiently collect data from various sources, ensuring data quality and reliability.</li> 
  <li>Data Curation and Transformation: Transform and curate raw data into structured and usable formats. Implement data validation, cleaning, and enrichment processes to maintain data integrity</li> 
  <li>Performance Optimization: Continuously optimize data ingestion and curation processes for speed, efficiency, and scalability</li> 
  <li>Collaboration: Collaborate with cross-functional teams including data scientists, data analysts, and domain experts to understand data requirements and deliver actionable insights</li> 
  <li>Quality Assurance: Implement best practices for data quality monitoring, validation, and error handling to ensure data accuracy and reliability</li> 
  <li>Documentation: Maintain comprehensive documentation for data ingestion and curation processes, making it easy for team members to understand and use the pipelines</li> 
  <li>Mentorship: Provide technical leadership and mentorship to junior engineers, fostering their growth and development</li> 
  <li>Stay Current: Keep abreast of emerging technologies and industry best practices in data engineering and data management</li> 
 </ul>
 <h2 class=""jobSectionHeader""><b>QUALIFICATIONS</b></h2> 
 <h4 class=""jobSectionHeader""><b>Required Qualifications</b></h4> 
 <ul>
  <li>Bachelor&apos;s or Master&apos;s degree in Computer Science, Engineering, or related field</li> 
  <li>8 years of experience in software engineering with a focus on backend software development</li> 
  <li>Proficiency in programming languages such as Python, Javascript, Typescript, Go or Java</li> 
  <li>Strong experience with data processing frameworks and tools, e.g. Apache Spark or similar</li> 
  <li>Deep understanding of data storage and retrieval technologies, both relational and NoSQL databases</li> 
  <li>Experience with data modeling, ETL processes, and data warehousing</li> 
  <li>Excellent problem-solving skills and a passion for delivering high-quality data solutions</li> 
  <li>Ability to diagram, articulate, and document data science and engineering concepts</li> 
  <li>Strong communication and collaboration skills</li> 
 </ul>
 <h4 class=""jobSectionHeader""><b>Preferred Qualifications</b></h4> 
 <ul>
  <li>Experience with cloud technologies in AWS or GCP, as well as container systems, such as Docker or Kubernetes</li> 
  <li>Familiarity with orchestration tools such as Airflow or Prefect</li> 
  <li>Experience with Great Expectations library</li> 
 </ul> 
 <p>We take into account an individual&apos;s qualifications, skillset, and experience in determining final salary. This role is eligible for health insurance, life insurance, retirement benefits, participation in the company&apos;s equity program, paid time off, including vacation and sick leave. The expected salary range for this position is &#x24;165,000 to &#x24;230,000. The actual offer will be at the company&apos;s sole discretion and determined by relevant business considerations, including the final candidate&apos;s qualifications, years of experience, and skillset.</p>
 <div>
  <p><i> Nuna is an Equal Employment Opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, disability, genetics and/or veteran status.</i></p>
 </div>
</div>","https://www.indeed.com/rc/clk?jk=7430eb68479730fd&atk=&xpse=SoB367I3JucvktTOqJ0LbzkdCdPP","7430eb68479730fd",,,,"San Francisco, CA","Staff Software Engineer, Data Integration","2 days ago","2023-10-21T13:05:34.419Z",,,"$165,000 - $230,000 a year","2023-10-23T13:05:34.421Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=7430eb68479730fd&from=jasx&tk=1hdea7hq42do5000&vjs=3"
"Intellipro Group Inc","Duties
The Strategy & Operations (SS&O) team is a key strategic partner for advertising sales leaders across sales strategy and analytics, operations and process and tools. Team members are experts in business strategy and operations and analytical and strategic thinking. We’re looking for an experienced Data Engineer who can support the SS&O team by streamlining and optimizing our data pipelines. You’ll know the ins and outs of Spark, providing support to help us fine tune and re-architect data pipelines as needed.
What You’ll Do:

 Build robust data pipelines that collect, process, and compute business metrics from sales data using Spark.
 Develop and iterate on existing workflows that will execute jobs consistently and at scale.
 Understand our database infrastructure to its core and optimize Spark SQL performance to improve landing times of our core tables.
 Work closely with the Sales Ops Business Intelligence team and other data engineering teams as necessary.
 Publish clear documentation and training materials to inform and educate SS&O team members, gather feedback to identify opportunities for improvement.

Skills

 Strong data analytics skills and experience working with and maintaining large data sets.
 Passion for building new solutions and enabling analysis at scale to drive decision-making.
 Proven ability to be successful in a complex, fast-paced environment.
 Organized, detail-oriented and strategically focused.
 Strong written and verbal communication skills.
 Understanding of sales and digital advertising platforms and working knowledge of sales systems and tools is a plus.

We are an Equal Opportunity Employer and take pride in a diverse environment. We do not discriminate in recruitment, hiring, training, promotion or other employment practices for reasons of race, color, religion, gender, sexual orientation, national origin, age, marital or veteran status, medical condition or disability.
Job Types: Full-time, Contract
Pay: $70.00 per hour
Schedule:

 8 hour shift

Application Question(s):

 Are you comfortable working initially for 6+ months contract position with possibility of extension?
 Do you have experience with Spark SQL?

Work Location: Remote","<p><b>Duties</b></p>
<p>The Strategy &amp; Operations (SS&amp;O) team is a key strategic partner for advertising sales leaders across sales strategy and analytics, operations and process and tools. Team members are experts in business strategy and operations and analytical and strategic thinking. We&#x2019;re looking for an experienced Data Engineer who can support the SS&amp;O team by streamlining and optimizing our data pipelines. You&#x2019;ll know the ins and outs of Spark, providing support to help us fine tune and re-architect data pipelines as needed.</p>
<p><b>What You&#x2019;ll Do:</b></p>
<ul>
 <li>Build robust data pipelines that collect, process, and compute business metrics from sales data using Spark.</li>
 <li>Develop and iterate on existing workflows that will execute jobs consistently and at scale.</li>
 <li>Understand our database infrastructure to its core and optimize Spark SQL performance to improve landing times of our core tables.</li>
 <li>Work closely with the Sales Ops Business Intelligence team and other data engineering teams as necessary.</li>
 <li>Publish clear documentation and training materials to inform and educate SS&amp;O team members, gather feedback to identify opportunities for improvement.</li>
</ul>
<p><b>Skills</b></p>
<ul>
 <li>Strong data analytics skills and experience working with and maintaining large data sets.</li>
 <li>Passion for building new solutions and enabling analysis at scale to drive decision-making.</li>
 <li>Proven ability to be successful in a complex, fast-paced environment.</li>
 <li>Organized, detail-oriented and strategically focused.</li>
 <li>Strong written and verbal communication skills.</li>
 <li>Understanding of sales and digital advertising platforms and working knowledge of sales systems and tools is a plus.</li>
</ul>
<p><b>We are an Equal Opportunity Employer and take pride in a diverse environment. We do not discriminate</b> <b>in recruitment, hiring, training, promotion or other employment practices for reasons of race, color, religion, gender, sexual orientation, national origin, age, marital or veteran status, medical condition or disability.</b></p>
<p>Job Types: Full-time, Contract</p>
<p>Pay: &#x24;70.00 per hour</p>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
</ul>
<p>Application Question(s):</p>
<ul>
 <li>Are you comfortable working initially for 6+ months contract position with possibility of extension?</li>
 <li>Do you have experience with Spark SQL?</li>
</ul>
<p>Work Location: Remote</p>",,"095f064bfba48308",,"Full-time","Contract","Remote","Data Engineer","2 days ago","2023-10-21T13:05:41.032Z",,,"$70 an hour","2023-10-23T13:05:41.034Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=095f064bfba48308&from=jasx&tk=1hdea7hq42do5000&vjs=3"
"Concentrix","Job Title: Big Data Engineer Job Ref #: 981496
 
  Job Description Concentrix CVG Customer Management Group Inc., Cincinnati OH, has multiple openings for the position of Big Data Engineer. Work will be performed in various unanticipated locations throughout the U.S. Travel and/or relocation is required. Telecommuting may be permitted.
  The Big Data Engineer will write, update, and maintain software applications; perform production maintenance of code; gather solutions requirements. Own technical commitments to clients and work with the team to successful delivery of solutions. Analyze, design, and code for complex requirements as well as write programs of complexity. Responsible for defining problems, collecting data, establishing facts, drawing valid conclusions, and preparing appropriate reports.
 
  The position requires a Master’s degree in Computer Science, Engineering (any), or any technical/analytical field that is closely related to the specialty, plus knowledge of: HTML5, CSS3, MySQL, and jQuery.
  To apply, send resume to ctlyst_postings@concentrix.com with Job Ref# 981496 in the subject line of the email.
 
  Location: USA, OH, Work-at-Home
 
  Language Requirements:
 
  Time Type:
  If you are a California resident, by submitting your information, you acknowledge that you have read and have access to the Job Applicant Privacy Notice for California Residents
  Concentrix is an Equal Opportunity/Affirmative Action Employer including Disabled/Vets.
 
  For more information regarding your EEO rights as an applicant, please visit the following websites: 
 
  English
  Spanish
 
 
  To request a reasonable accommodation please click here.
 
  If you wish to review the Affirmative Action Plan, please click here.","<p></p>
<div>
 <p>Job Title:</p> Big Data Engineer Job Ref #: 981496
 <p></p>
 <p> Job Description</p> Concentrix CVG Customer Management Group Inc., Cincinnati OH, has multiple openings for the position of Big Data Engineer. Work will be performed in various unanticipated locations throughout the U.S. Travel and/or relocation is required. Telecommuting may be permitted.
 <p> The Big Data Engineer will write, update, and maintain software applications; perform production maintenance of code; gather solutions requirements. Own technical commitments to clients and work with the team to successful delivery of solutions. Analyze, design, and code for complex requirements as well as write programs of complexity. Responsible for defining problems, collecting data, establishing facts, drawing valid conclusions, and preparing appropriate reports.</p>
 <p></p>
 <p> The position requires a Master&#x2019;s degree in Computer Science, Engineering (any), or any technical/analytical field that is closely related to the specialty, plus knowledge of: HTML5, CSS3, MySQL, and jQuery.</p>
 <p> To apply, send resume to ctlyst_postings@concentrix.com with Job Ref# 981496 in the subject line of the email.</p>
 <p></p>
 <p> Location:</p> USA, OH, Work-at-Home
 <p></p>
 <p> Language Requirements:</p>
 <p></p>
 <p> Time Type:</p>
 <p><b> If you are a California resident, by submitting your information, you acknowledge that you have read and have access to the </b><b>Job Applicant Privacy Notice for California Residents</b></p>
 <p><br> Concentrix is an Equal Opportunity/Affirmative Action Employer including Disabled/Vets.</p>
 <p></p>
 <p> For more information regarding your EEO rights as an applicant, please visit the following websites: </p>
 <ul>
  <li>English</li>
  <li>Spanish</li>
 </ul>
 <p></p>
 <p> To request a reasonable accommodation please click here.</p>
 <p></p>
 <p> If you wish to review the Affirmative Action Plan, please click here.</p>
</div>
<p></p>","https://www.indeed.com/rc/clk?jk=2ba6006821dec6ec&atk=&xpse=SoB467I3JucuyygRzr0LbzkdCdPP","2ba6006821dec6ec",,,,"Remote","Big Data Engineer Job Ref #: 981496","2 days ago","2023-10-21T13:05:40.775Z","3.4","31685",,"2023-10-23T13:05:40.776Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=2ba6006821dec6ec&from=jasx&tk=1hdea7hq42do5000&vjs=3"
"OneStudyTeam","At OneStudyTeam (a Reify Health company), we specialize in speeding up clinical trials and increasing the chance of new therapies being approved with the ultimate goal of improving patient outcomes. Our cloud-based platform, StudyTeam, brings research site workflows online and enables sites, sponsors, and other key stakeholders to work together more effectively. StudyTeam is trusted by the largest global biopharmaceutical companies, used in over 6,000 research sites, and is available in over 100 countries. Join us in our mission to advance clinical research and improve patient care. 
   One mission. One team. That's OneStudyTeam.
 
  Our unique, rapidly growing data streams are enabling novel opportunities to manage clinical trials more efficiently and predictably. The Data Engineering team is looking for talented Senior Data Engineers to build, expand, and support a cutting-edge data architecture which is the analytical backbone of our company. If you are empathetic, business-driven, and want to use your data engineering and data architecture skills to make a tangible impact in the clinical research community then this may be the role for you. 
  We're looking for people who can effectively balance rapid execution and delivery with sustainable and scalable architectural initiatives to serve the business most effectively. You have strong opinions, weakly held, and while well-versed technically know when to choose the right tool, for the right job, at the right level of complexity. You will work closely with the rest of our Data Engineering team and our Data Science teams to help collect, stream, transform, and effectively manage data for integration into critical reporting, data visualizations, and ModelOps systems for emerging data science products. 
  What You'll Be Working On 
 
  Build reliable and robust data integrations with external partners 
  Supporting the development and expansion of modern, privacy-aware, data warehouse and data mesh architectures 
  Helping to build, manage, orchestrate, and integrate streaming data sources, data lakes, ELT processes, columnar storage systems, and distributed query execution solutions 
  Establishing proactive data quality/freshness dashboards, monitoring, alerting, and anomaly remediation systems 
  Building practical data onboarding tooling and process automation solutions 
  Learning to effectively understand and deftly navigate the global compliance ecosystem (HIPAA, GDPR, etc.) to ensure your work respects the rights, regulations, and consent preferences of all stakeholders, including historical underserved or underrepresented populations 
  Developing a deep understanding of the clinical ecosystem, our products, and our business and how they all uniquely interact to help people 
 
 What You'll Bring to OneStudyTeam 
 
  4+ years of experience successfully developing and deploying data pipelines and distributed architectures, ideally in a space similar to ours (startup, healthcare, regulated data) 
  Hands-on experience implementing ETL/ELT best practices at scale and demonstrated practical experience or familiarity with a good portion of our stack, including: AWS services (Redshift, MSK, Lambda, ECS, ECR, EC2, Glue, Quicksight, Spectrum, S3, etc.), Postgres, dbt, Kafka, Prefect, Docker, Terraform 
  Excellent programming skills in Python and deep comfort with SQL. Clojure experience is also highly appreciated 
  Experience or interest in developing and managing enterprise-scale data, distributed data architectures 
  Able to independently ship medium-to-large features and start to support or participate in architectural design 
  Excellent written and verbal communication skills 
  Strong attention to detail is key, especially when considering correctness, security, and compliance 
  Solid software testing, documentation, and debugging practices in the context of distributed systems 
 
 
   Learn more about our global benefits offerings on our careers site: https://careers.onestudyteam.com/us-benefits 
   We value diversity and believe the unique contributions each of us brings drives our success. We do not discriminate on the basis of race, sex, religion, color, national origin, gender identity, age, marital status, veteran status, or disability status. 
   Note: OneStudyTeam is unable to sponsor work visas at this time. If you are a non-U.S. resident applicant, please note that OST works with a Professional Employer Organization. 
   As a condition of employment, you will abide by all organizational security and privacy policies. 
   For a detailed overview of OneStudyTeam's candidate privacy policy, please visit https://careers.onestudyteam.com/candidate-privacy-policy. This organization participates in E-Verify (E-Verify's Right to Work guidance can be found here).","<div>
 <div>
  <p>At OneStudyTeam (a Reify Health company), we specialize in speeding up clinical trials and increasing the chance of new therapies being approved with the ultimate goal of improving patient outcomes. Our cloud-based platform, StudyTeam, brings research site workflows online and enables sites, sponsors, and other key stakeholders to work together more effectively. StudyTeam is trusted by the largest global biopharmaceutical companies, used in over 6,000 research sites, and is available in over 100 countries. Join us in our mission to advance clinical research and improve patient care.</p> 
  <p><b> One mission. One team. That&apos;s OneStudyTeam.</b></p>
 </div>
 <p> Our unique, rapidly growing data streams are enabling novel opportunities to manage clinical trials more efficiently and predictably. The Data Engineering team is looking for talented <b>Senior Data Engineers</b> to build, expand, and support a cutting-edge data architecture which is the analytical backbone of our company. If you are empathetic, business-driven, and want to use your data engineering and data architecture skills to make a tangible impact in the clinical research community then this may be the role for you.</p> 
 <p> We&apos;re looking for people who can effectively balance rapid execution and delivery with sustainable and scalable architectural initiatives to serve the business most effectively. You have strong opinions, weakly held, and while well-versed technically know when to choose the right tool, for the right job, at the right level of complexity. You will work closely with the rest of our Data Engineering team and our Data Science teams to help collect, stream, transform, and effectively manage data for integration into critical reporting, data visualizations, and ModelOps systems for emerging data science products.</p> 
 <h3 class=""jobSectionHeader""><b> What You&apos;ll Be Working On</b></h3> 
 <ul>
  <li>Build reliable and robust data integrations with external partners</li> 
  <li>Supporting the development and expansion of modern, privacy-aware, data warehouse and data mesh architectures</li> 
  <li>Helping to build, manage, orchestrate, and integrate streaming data sources, data lakes, ELT processes, columnar storage systems, and distributed query execution solutions</li> 
  <li>Establishing proactive data quality/freshness dashboards, monitoring, alerting, and anomaly remediation systems</li> 
  <li>Building practical data onboarding tooling and process automation solutions</li> 
  <li>Learning to effectively understand and deftly navigate the global compliance ecosystem (HIPAA, GDPR, etc.) to ensure your work respects the rights, regulations, and consent preferences of all stakeholders, including historical underserved or underrepresented populations</li> 
  <li>Developing a deep understanding of the clinical ecosystem, our products, and our business and how they all uniquely interact to help people</li> 
 </ul>
 <h3 class=""jobSectionHeader""><b>What You&apos;ll Bring to OneStudyTeam</b></h3> 
 <ul>
  <li>4+ years of experience successfully developing and deploying data pipelines and distributed architectures, ideally in a space similar to ours (startup, healthcare, regulated data)</li> 
  <li>Hands-on experience implementing ETL/ELT best practices at scale and demonstrated practical experience or familiarity with a good portion of our stack, including: AWS services (Redshift, MSK, Lambda, ECS, ECR, EC2, Glue, Quicksight, Spectrum, S3, etc.), Postgres, dbt, Kafka, Prefect, Docker, Terraform</li> 
  <li>Excellent programming skills in Python and deep comfort with SQL. Clojure experience is also highly appreciated</li> 
  <li>Experience or interest in developing and managing enterprise-scale data, distributed data architectures</li> 
  <li>Able to independently ship medium-to-large features and start to support or participate in architectural design</li> 
  <li>Excellent written and verbal communication skills</li> 
  <li>Strong attention to detail is key, especially when considering correctness, security, and compliance</li> 
  <li>Solid software testing, documentation, and debugging practices in the context of distributed systems</li> 
 </ul>
 <div>
  <p> Learn more about our global benefits offerings on our careers site: https://careers.onestudyteam.com/us-benefits</p> 
  <p><i> We value diversity and believe the unique contributions each of us brings drives our success. We do not discriminate on the basis of race, sex, religion, color, national origin, gender identity, age, marital status, veteran status, or disability status.</i></p> 
  <p><b><i> Note</i></b><i>: OneStudyTeam</i><i> is unable to sponsor work visas at this time. If you are a non-U.S. resident applicant, please note that OST works with a Professional Employer Organization.</i></p> 
  <p><i> As a condition of employment, you will abide by all organizational security and privacy policies.</i></p> 
  <p><i> For a detailed overview of OneStudyTeam&apos;s candidate privacy policy, please visit</i><i> </i><i>https://careers.onestudyteam.com/candidate-privacy-policy</i><i>. This organization participates in </i><i>E-Verify</i><i> (E-Verify&apos;s Right to Work guidance can be found </i><i>here</i><i>).</i></p>
 </div>
</div>","https://careers.onestudyteam.com/jobs/?gh_jid=5003597004&gh_src=620145ed4us","6e452e2e53e76bbd",,,,"Remote","Senior Data Engineer","2 days ago","2023-10-21T13:05:35.224Z",,,,"2023-10-23T13:05:35.226Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=6e452e2e53e76bbd&from=jasx&tk=1hdea7hq42do5000&vjs=3"
"Paramount+","OVERVIEW & RESPONSIBILITIES 
We are looking for someone who is thrilled to build Data Science and ML products that formulate our business strategy, optimize our content, and guide Marketing investment decisions. To build these products, you will leverage data collected from tens of millions of Paramount+ content users, including video consumption data from Adobe Analytics™ ClickStream, purchase / subscription data, and Ad revenue data from Doubleclick™. Efficiently building Data Science and ML products at Paramount+ requires experience in conceiving and deploying statistical and/or machine learning models, deep Python and SQL experience, perspicacity writing code in a collaborative setting, and excellent communication skills. 

 RESPONSIBILITIES 
Translate complex business problems into coherent, actionable quantitative solutions 
Implement, automate, and maintain reliable, performant, and end-to-end ML systems using software engineering and MLOps standard methodologies 
Deliver clear and actionable insights to collaborators 
Collaborate with partners across the org to implement data science solutions that inform the business strategy, optimize our content, and guide marketing investment decisions 
Build models to guide our content strategy such as content affinity and valuation models 
Build MLOps products used to monitor, persist, and expose ML models 
Build models to guide our customer lifecycle strategy, such as subscriber churn models 
Build models to guide our marketing strategy, such as audience segmentation models 

 BASIC QUALIFICATIONS 
STEM undergraduate degree in Statistics, Engineering, Informatics, Computer Science or similar 
4+ years experience in Data Science or ML Engineering 
Broad experience with both supervised and unsupervised machine learning techniques 
Ability to break complex problems into simple, coherent solutions 
Deep knowledge of the range and breadth of Data Science tools best suited for a given business problem 
Deep experience writing robust Python (Pandas, Airflow) and complex SQL code in a collaborative team setting, using Git™ distributed version control system 
Full stack Data Science experience, ranging from data pipeline to model deployment and maintenance 
Strong detail orientation with a penchant for data accuracy 
An ability to communicate concisely and persuasively with engineers, product managers, partners and collaborators 

 ADDITIONAL QUALIFICATIONS 
STEM graduate or post-graduate degree in Statistics, Engineering, Informatics, Computer Science or similar 
Experience with media or subscription businesses 
Experience using Google Cloud Platform (BigQuery, ML Engine, and APIs) 
Experience using project management tools (JIRA, Confluence) 
#LI-FV 37877 
#LI-REMOTE 

 Paramount+, a direct-to-consumer digital subscription video on-demand and live streaming service from Paramount Global, combines live sports, breaking news, and a mountain of entertainment. The premium streaming service features an expansive library of original series, hit shows and popular movies across every genre from world-renowned brands and production studios, including BET, CBS, Comedy Central, MTV, Nickelodeon, Paramount Pictures and the Smithsonian Channel. The service is also the streaming home to unmatched sports programming, including every CBS Sports event, from golf to football to basketball and more, plus exclusive streaming rights for major sports properties, including some of the world’s biggest and most popular soccer leagues. Paramount+ also enables subscribers to stream local CBS stations live across the U.S. in addition to the ability to stream Paramount Streaming’s other live channels: CBSN for 24/7 news, CBS Sports HQ for sports news and analysis, and ET Live for entertainment coverage. 

 
Paramount Global (NASDAQ: PARA, PARAA) is a leading global media and entertainment company that creates premium content and experiences for audiences worldwide. Driven by iconic studios, networks and streaming services, Paramount's portfolio of consumer brands includes CBS, Showtime Networks, Paramount Pictures, Nickelodeon, MTV, Comedy Central, BET, Paramount+, Pluto TV and Simon & Schuster, among others. Paramount delivers the largest share of the U.S. television audience and boasts one of the industry's most important and extensive libraries of TV and film titles. In addition to offering innovative streaming services and digital video products, the company provides powerful capabilities in production, distribution and advertising solutions. 

 ADDITIONAL INFORMATION 

 
Hiring Salary Range: $124,000.00 - 165,000.00. 

 The hiring salary range for this position applies to New York City, California, Colorado, Washington state, and most other geographies. Starting pay for the successful applicant depends on a variety of job-related factors, including but not limited to geographic location, market demands, experience, training, and education. The benefits available for this position include medical, dental, vision, 401(k) plan, life insurance coverage, disability benefits, tuition assistance program and PTO or, if applicable, as otherwise dictated by the appropriate Collective Bargaining Agreement. This position is bonus eligible. 

 https://www.paramount.com/careers/benefits 

 Paramount is an equal opportunity employer (EOE) including disability/vet. 

 At Paramount, the spirit of inclusion feeds into everything that we do, on-screen and off. From the programming and movies we create to employee benefits/programs and social impact outreach initiatives, we believe that opportunity, access, resources and rewards should be available to and for the benefit of all. Paramount is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ethnicity, ancestry, religion, creed, sex, national origin, sexual orientation, age, citizenship status, marital status, disability, gender identity, gender expression, and Veteran status. 

 If you are a qualified individual with a disability or a disabled veteran, you may request a reasonable accommodation if you are unable or limited in your ability to use or access. https://www.paramount.com/careers as a result of your disability. You can request reasonable accommodations by calling 212.846.5500 or by sending an email to paramountaccommodations@paramount.com. Only messages left for this purpose will be returned.","OVERVIEW &amp; RESPONSIBILITIES 
<br>We are looking for someone who is thrilled to build Data Science and ML products that formulate our business strategy, optimize our content, and guide Marketing investment decisions. To build these products, you will leverage data collected from tens of millions of Paramount+ content users, including video consumption data from Adobe Analytics&#x2122; ClickStream, purchase / subscription data, and Ad revenue data from Doubleclick&#x2122;. Efficiently building Data Science and ML products at Paramount+ requires experience in conceiving and deploying statistical and/or machine learning models, deep Python and SQL experience, perspicacity writing code in a collaborative setting, and excellent communication skills. 
<br>
<br> RESPONSIBILITIES 
<br>Translate complex business problems into coherent, actionable quantitative solutions 
<br>Implement, automate, and maintain reliable, performant, and end-to-end ML systems using software engineering and MLOps standard methodologies 
<br>Deliver clear and actionable insights to collaborators 
<br>Collaborate with partners across the org to implement data science solutions that inform the business strategy, optimize our content, and guide marketing investment decisions 
<br>Build models to guide our content strategy such as content affinity and valuation models 
<br>Build MLOps products used to monitor, persist, and expose ML models 
<br>Build models to guide our customer lifecycle strategy, such as subscriber churn models 
<br>Build models to guide our marketing strategy, such as audience segmentation models 
<br>
<br> BASIC QUALIFICATIONS 
<br>STEM undergraduate degree in Statistics, Engineering, Informatics, Computer Science or similar 
<br>4+ years experience in Data Science or ML Engineering 
<br>Broad experience with both supervised and unsupervised machine learning techniques 
<br>Ability to break complex problems into simple, coherent solutions 
<br>Deep knowledge of the range and breadth of Data Science tools best suited for a given business problem 
<br>Deep experience writing robust Python (Pandas, Airflow) and complex SQL code in a collaborative team setting, using Git&#x2122; distributed version control system 
<br>Full stack Data Science experience, ranging from data pipeline to model deployment and maintenance 
<br>Strong detail orientation with a penchant for data accuracy 
<br>An ability to communicate concisely and persuasively with engineers, product managers, partners and collaborators 
<br>
<br> ADDITIONAL QUALIFICATIONS 
<br>STEM graduate or post-graduate degree in Statistics, Engineering, Informatics, Computer Science or similar 
<br>Experience with media or subscription businesses 
<br>Experience using Google Cloud Platform (BigQuery, ML Engine, and APIs) 
<br>Experience using project management tools (JIRA, Confluence) 
<br>#LI-FV 37877 
<br>#LI-REMOTE 
<br>
<br> Paramount+, a direct-to-consumer digital subscription video on-demand and live streaming service from Paramount Global, combines live sports, breaking news, and a mountain of entertainment. The premium streaming service features an expansive library of original series, hit shows and popular movies across every genre from world-renowned brands and production studios, including BET, CBS, Comedy Central, MTV, Nickelodeon, Paramount Pictures and the Smithsonian Channel. The service is also the streaming home to unmatched sports programming, including every CBS Sports event, from golf to football to basketball and more, plus exclusive streaming rights for major sports properties, including some of the world&#x2019;s biggest and most popular soccer leagues. Paramount+ also enables subscribers to stream local CBS stations live across the U.S. in addition to the ability to stream Paramount Streaming&#x2019;s other live channels: CBSN for 24/7 news, CBS Sports HQ for sports news and analysis, and ET Live for entertainment coverage. 
<br>
<br> 
<b>Paramount Global (NASDAQ:</b> PARA, PARAA) is a leading global media and entertainment company that creates premium content and experiences for audiences worldwide. Driven by iconic studios, networks and streaming services, Paramount&apos;s portfolio of consumer brands includes CBS, Showtime Networks, Paramount Pictures, Nickelodeon, MTV, Comedy Central, BET, Paramount+, Pluto TV and Simon &amp; Schuster, among others. Paramount delivers the largest share of the U.S. television audience and boasts one of the industry&apos;s most important and extensive libraries of TV and film titles. In addition to offering innovative streaming services and digital video products, the company provides powerful capabilities in production, distribution and advertising solutions. 
<br>
<br> ADDITIONAL INFORMATION 
<br>
<br> 
<b>Hiring Salary Range:</b> &#x24;124,000.00 - 165,000.00. 
<br>
<br> The hiring salary range for this position applies to New York City, California, Colorado, Washington state, and most other geographies. Starting pay for the successful applicant depends on a variety of job-related factors, including but not limited to geographic location, market demands, experience, training, and education. The benefits available for this position include medical, dental, vision, 401(k) plan, life insurance coverage, disability benefits, tuition assistance program and PTO or, if applicable, as otherwise dictated by the appropriate Collective Bargaining Agreement. This position is bonus eligible. 
<br>
<br> https://www.paramount.com/careers/benefits 
<br>
<br> Paramount is an equal opportunity employer (EOE) including disability/vet. 
<br>
<br> At Paramount, the spirit of inclusion feeds into everything that we do, on-screen and off. From the programming and movies we create to employee benefits/programs and social impact outreach initiatives, we believe that opportunity, access, resources and rewards should be available to and for the benefit of all. Paramount is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ethnicity, ancestry, religion, creed, sex, national origin, sexual orientation, age, citizenship status, marital status, disability, gender identity, gender expression, and Veteran status. 
<br>
<br> If you are a qualified individual with a disability or a disabled veteran, you may request a reasonable accommodation if you are unable or limited in your ability to use or access. https://www.paramount.com/careers as a result of your disability. You can request reasonable accommodations by calling 212.846.5500 or by sending an email to paramountaccommodations@paramount.com. Only messages left for this purpose will be returned.","https://www.indeed.com/rc/clk?jk=241a80b3883d2e12&atk=&xpse=SoD867I3JucuS2XvQR0KbzkdCdPP","241a80b3883d2e12",,"Full-time",,"New York, NY 10036","Senior Data Scientist / Machine Learning Engineer","Today","2023-10-23T13:05:44.697Z","3.9","2002","$124,000 - $165,000 a year","2023-10-23T13:05:44.698Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=241a80b3883d2e12&from=jasx&tk=1hdea7hq42do5000&vjs=3"
"Conversant Group","Conversant Group is an IT infrastructure and security consulting company founded in 2009 and based in Chattanooga, TN. We are the 
  world’s first Civilian Cybersecurity Force, dedicated to defeating cyber terrorists. To do so, Conversant Group is organized into three battalions: 
  Athena7
   which provides infrastructure assessment, strategy and remediation, 
  Grypho5
   which offers ongoing managed protection, and 
  Fenix24
   which provides rapid restoration in case of a cyberattack.
 
 
 
   The Rapid Betterment Engineer IV role is a Senior / Technical Lead, principally responsible for the rapid implementation of Conversant Group’s Managed Security Solutions. This role reports to the Director of Rapid Betterment and is 100% Remote with a potential requirement to travel to client locations when necessary and if applicable (subject to availability). An ideal candidate will be able to; [1] lead technical design and architectural workshops, [2] deploy and configure complex technical solutions using our internal specifications, documentation, and best practices, with minimal supervision, [3] resolve implementation issues with a combination of skills, experience, vendor support, and internal consultations, [4] maintain an exhaustive understanding of our products and solutions, [5] create and maintain implementation documentation leveraging Conversant Group’s security standards, [6] maintain a high degree of proficiency in the products and software to be implemented, [7] act as a subject matter expert for Conversant Group’s product and service solutions, [8] provide mentoring and be a technical escalation for junior / mid-level engineers.
  
 Preferred Skills
 
   Lead client facing technical architecture and design workshops.
   Implement complex technical security solutions with minimal supervision.
   Resolve implementation-related technical issues.
   Work closely with Project Managers on defining task details, timelines, status updates, and working within project scope.
   Understand project scope definition, validation, and rapid deployment of services.
   Maintain an exhaustive understanding of the guiding principles for Conversant Group product & service offerings.
   Act as a SME for Conversant Group’s Managed Security Solutions portfolio.
   Provide mentoring and be a technical escalation for team members.
   Maintain select vendor technical certifications.
   Provide client education for deployed solutions.
   Collaborate with clients, partners, and vendors to aid in successful deployment of security solutions.
 
  Qualifications
 
   7+ years of Managed Services Support Engineering and/or Architectural Design & Implementation Services Engineering.
   7+ years of Security Product experience Implementing and Supporting On-premises, Hybrid, Cloud based Security Solutions such as Cohesity, Microsoft Azure, AWS and VMWare.
   Service Delivery in a Managed Services / Professional Services organization highly desired.
   Working Knowledge and Understanding of Microsoft and VMWare platforms highly desired.
   Industry Certifications in Cohesity, Microsoft, and VMware highly desired.
   Bachelor’s Degree in Information Technology, related field, or equivalent work history.
   Experience working for and within IT for the Legal Industry a plus.
 
 
   Why work for us?
 
 
 
   We offer a dynamic, innovative work environment with rewarding work - help save our clients from disaster!
 
 
   We truly value our employees and provide an extraordinary package to prove it, including:
   
 
 
  
 
  Internal and external learning & development opportunities, including career advancement.
 
 
   Competitive compensation & benefits.
 
 
   Scheduled & flexible PTO programs.
 
 
   Fully remote work options.
 
 
   Family friendly programs
 
 
   Care packages 
 
 
  Regular team building events.
 
 
 
   Join the world's first Civilian Cybersecurity Force and take your career to the next level!","<div>
 <div>
  Conversant Group is an IT infrastructure and security consulting company founded in 2009 and based in Chattanooga, TN. We are the 
  <b>world&#x2019;s first Civilian Cybersecurity Force</b>, dedicated to defeating cyber terrorists. To do so, Conversant Group is organized into three battalions: 
  <b><i>Athena7</i></b>
  <i> </i>which provides infrastructure assessment, strategy and remediation, 
  <b><i>Grypho5</i></b>
  <i> </i>which offers ongoing managed protection, and 
  <b><i>Fenix24</i></b>
  <i> </i>which provides rapid restoration in case of a cyberattack.
 </div>
 <div></div>
 <div>
  <br> The Rapid Betterment Engineer IV role is a Senior / Technical Lead, principally responsible for the rapid implementation of Conversant Group&#x2019;s Managed Security Solutions. This role reports to the Director of Rapid Betterment and is 100% Remote with a potential requirement to travel to client locations when necessary and if applicable (subject to availability). An ideal candidate will be able to; [1] lead technical design and architectural workshops, [2] deploy and configure complex technical solutions using our internal specifications, documentation, and best practices, with minimal supervision, [3] resolve implementation issues with a combination of skills, experience, vendor support, and internal consultations, [4] maintain an exhaustive understanding of our products and solutions, [5] create and maintain implementation documentation leveraging Conversant Group&#x2019;s security standards, [6] maintain a high degree of proficiency in the products and software to be implemented, [7] act as a subject matter expert for Conversant Group&#x2019;s product and service solutions, [8] provide mentoring and be a technical escalation for junior / mid-level engineers.
 </div> 
 <h3 class=""jobSectionHeader""><b>Preferred Skills</b></h3>
 <ul>
  <li> Lead client facing technical architecture and design workshops.</li>
  <li> Implement complex technical security solutions with minimal supervision.</li>
  <li> Resolve implementation-related technical issues.</li>
  <li> Work closely with Project Managers on defining task details, timelines, status updates, and working within project scope.</li>
  <li> Understand project scope definition, validation, and rapid deployment of services.</li>
  <li> Maintain an exhaustive understanding of the guiding principles for Conversant Group product &amp; service offerings.</li>
  <li> Act as a SME for Conversant Group&#x2019;s Managed Security Solutions portfolio.</li>
  <li> Provide mentoring and be a technical escalation for team members.</li>
  <li> Maintain select vendor technical certifications.</li>
  <li> Provide client education for deployed solutions.</li>
  <li> Collaborate with clients, partners, and vendors to aid in successful deployment of security solutions.</li>
 </ul>
 <h3 class=""jobSectionHeader""><b> Qualifications</b></h3>
 <ul>
  <li> 7+ years of Managed Services Support Engineering and/or Architectural Design &amp; Implementation Services Engineering.</li>
  <li> 7+ years of Security Product experience Implementing and Supporting On-premises, Hybrid, Cloud based Security Solutions such as Cohesity, Microsoft Azure, AWS and VMWare.</li>
  <li> Service Delivery in a Managed Services / Professional Services organization highly desired.</li>
  <li> Working Knowledge and Understanding of Microsoft and VMWare platforms highly desired.</li>
  <li> Industry Certifications in Cohesity, Microsoft, and VMware highly desired.</li>
  <li> Bachelor&#x2019;s Degree in Information Technology, related field, or equivalent work history.</li>
  <li> Experience working for and within IT for the Legal Industry a plus.</li>
 </ul>
 <div>
  <b> Why work for us?</b>
 </div>
 <div></div>
 <div>
  <br> We offer a dynamic, innovative work environment with rewarding work - help save our clients from disaster!
 </div>
 <div>
   We truly value our employees and provide an extraordinary package to prove it, including:
  <br> 
 </div>
 <div></div>
 <br> 
 <div>
  Internal and external learning &amp; development opportunities, including career advancement.
 </div>
 <div>
   Competitive compensation &amp; benefits.
 </div>
 <div>
   Scheduled &amp; flexible PTO programs.
 </div>
 <div>
   Fully remote work options.
 </div>
 <div>
   Family friendly programs
 </div>
 <div>
   Care packages 
 </div>
 <div>
  Regular team building events.
 </div>
 <div></div>
 <div>
  <br> Join the world&apos;s first Civilian Cybersecurity Force and take your career to the next level!
 </div>
</div>
<div></div>","https://jobs.lever.co/conversantgroup/997f5e26-2f70-4e11-a84f-6e7f72b06d23?lever-source=Indeed","326aecf5562629b4",,"Full-time",,"Remote","Data Protection & Cloud Engineer (Cohesity)","2 days ago","2023-10-21T13:05:40.301Z","3.9","7",,"2023-10-23T13:05:40.303Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=326aecf5562629b4&from=jasx&tk=1hdea7hq42do5000&vjs=3"
"Right Talents","Estimated Best in Market 
   
  
 
 
  
   Long Term
  
  
   
     Posted on: 12/22/2022 
   
  
 
 
  Job Description:
  
   Looking for candidates on W2
   Need to attend Initial Screening Test
 
 
 
   Responsibilities:
  
   
   Working closely with business stakeholders and product owners on requirement gathering & assessment. 
   Working with QA & Business Analysts on Integration & User Acceptance Testing. 
   Working using Agile Delivery Methodology, to deliver in Quick Delivery Cycles. 
   Developing Power BI datasets, dataflows and dashboard creation from various data sources. 
   Developing visual reports, dashboards and KPI scorecards using BI tools like Power BI, Qlik Sense and Tableau. 
   Developing paginated reports using Power Bi Build. 
   Connecting to data sources, importing data and transforming data for Business Intelligence using Qlik Sense and Power BI. Create and Maintain Design specifications and Support documentation. 
   Involve in Design Reviews & Post-Production Support Activities. 
   Troubleshoot systems production/performance issues. 
   Must be passionate about contributing to an organization focused on continuously improving consumer experiences. 
   Must be open to new data technologies and self-learning for new business needs. 
   
 
 
 
  Requirements:
  
   
   5+ years of overall IT Experience working with Business Intelligence and Data understanding. 
   3 + years designing and developing reports and dashboards in Power BI. 
   3 + Strong SQL skills (complex queries on relational databases, stored procedures, automation) required. 
   2+ years designing and developing reports and dashboards using BI tools like Power bi , DAX functions, Qlik Sense and Tableau. 
   Experience with Designing Data Visualization for Business Needs. 
   Excellent communication skills and understanding Business requirements. 
   Hands-On experience with GitHub, JIRA, and CI/CD tools. 
   Experience using statistical computer languages (Python, SQL, R, etc.) to manipulate data and draw insights from large data sets 
   Experience working in an Agile Scrum environment or good familiarity with the Agile methodologies. 
   Good understanding of Data Warehousing & Data Modeling fundamentals. 
   Strong ability to communicate clearly and professionally with end customers. 
   Any Cloud Application, Database, Data Integration, Business Intelligence Application Certifications are preferable. 
   Exposure to Data Streaming, and Data API Design & Development experience would be preferable. 
   Familiarity or working experience with one of tools like SSIS, ER Studio.","<div></div>
<div>
 <div>
  <div>
   <div>
    Estimated Best in Market 
   </div>
  </div>
 </div>
 <div>
  <div>
   Long Term
  </div>
  <div>
   <div>
     Posted on: 12/22/2022 
   </div>
  </div>
 </div>
 <div>
  <h6 class=""jobSectionHeader""><b>Job Description:</b></h6>
  <p></p>
  <p> Looking for candidates on W2</p>
  <p> Need to attend Initial Screening Test</p>
 </div>
 <p></p>
 <div>
  <h6 class=""jobSectionHeader""><b> Responsibilities:</b></h6>
  <p></p>
  <ul> 
   <li>Working closely with business stakeholders and product owners on requirement gathering &amp; assessment.</li> 
   <li>Working with QA &amp; Business Analysts on Integration &amp; User Acceptance Testing.</li> 
   <li>Working using Agile Delivery Methodology, to deliver in Quick Delivery Cycles.</li> 
   <li>Developing Power BI datasets, dataflows and dashboard creation from various data sources.</li> 
   <li>Developing visual reports, dashboards and KPI scorecards using BI tools like Power BI, Qlik Sense and Tableau.</li> 
   <li>Developing paginated reports using Power Bi Build.</li> 
   <li>Connecting to data sources, importing data and transforming data for Business Intelligence using Qlik Sense and Power BI.<br> Create and Maintain Design specifications and Support documentation.</li> 
   <li>Involve in Design Reviews &amp; Post-Production Support Activities.</li> 
   <li>Troubleshoot systems production/performance issues.</li> 
   <li>Must be passionate about contributing to an organization focused on continuously improving consumer experiences.</li> 
   <li>Must be open to new data technologies and self-learning for new business needs.</li> 
  </ul> 
 </div>
 <p></p>
 <div>
  <h6 class=""jobSectionHeader""><b>Requirements:</b></h6>
  <p></p>
  <ul> 
   <li>5+ years of overall IT Experience working with Business Intelligence and Data understanding.</li> 
   <li>3 + years designing and developing reports and dashboards in Power BI.</li> 
   <li>3 + Strong SQL skills (complex queries on relational databases, stored procedures, automation) required.</li> 
   <li>2+ years designing and developing reports and dashboards using BI tools like Power bi , DAX functions, Qlik Sense and Tableau.</li> 
   <li>Experience with Designing Data Visualization for Business Needs.</li> 
   <li>Excellent communication skills and understanding Business requirements.</li> 
   <li>Hands-On experience with GitHub, JIRA, and CI/CD tools.</li> 
   <li>Experience using statistical computer languages (Python, SQL, R, etc.) to manipulate data and draw insights from large data sets</li> 
   <li>Experience working in an Agile Scrum environment or good familiarity with the Agile methodologies.</li> 
   <li>Good understanding of Data Warehousing &amp; Data Modeling fundamentals.</li> 
   <li>Strong ability to communicate clearly and professionally with end customers.</li> 
   <li>Any Cloud Application, Database, Data Integration, Business Intelligence Application Certifications are preferable.</li> 
   <li>Exposure to Data Streaming, and Data API Design &amp; Development experience would be preferable.</li> 
   <li>Familiarity or working experience with one of tools like SSIS, ER Studio.</li>
  </ul>
 </div>
</div>
<p></p>","https://righttalents.net/openpositions/jobdetails/Data%20Analyst%2FEngineer%2FScientist_CBJOBID0314","639c90e33f632674",,,,"Remote","Data Analyst/Engineer/Scientist","Today","2023-10-23T13:05:46.376Z",,,,"2023-10-23T13:05:46.377Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=639c90e33f632674&from=jasx&tk=1hdea7hq42do5000&vjs=3"
"Cyberjin","Hybrid/Remote role
  Looking for a talented Data Engineer to support the acquisition of mission critical and mission support data sets. The preferred candidate will have a background in supporting cyber and/or network related missions within the military spaces, as either a developer, analyst or engineer. Work is mostly on customer site in San Antonio, TX with some hybrid support.
 
  Essential Job Responsibilities
  The ideal candidate will have worked with big data systems, complex structured and unstructured data sets, and have supported government data acquisition, analysis, and/or sharing efforts in the past. 
 To excel in the position, the candidate shall have a strong attention to detail, be able to understand technical complexities, and have the willingness to learn and adapt to the situation. 
 The candidate will work both independently and as part of a large team to accomplish client objectives. 
 Minimum Qualifications
  Security Clearance - Must have a current TS/SCI level security clearance and therefore all candidates must be a U.S. Citizen. 
 5 years experience as a developer, analyst, or engineer with a Bachelors in related field; OR 3 years relevant experience with Masters in related field; OR High School Diploma or equivalent and 9 years relevant experience.
  Experience with programming languages such as Python and Java.
  Proficiency with acquisition and understanding of network data and the associated metadata.
  Fluency with data extraction, translation, and loading including data prep and labeling to enable data analytics.
  Experience with Kibana and Elasticsearch.
  Familiarity with various log formats such as JSON, XML, and others.
  Experience with data flow, management, and storage solutions (i.e. Kafka, NiFi, and AWS S3 and SQS solutions).
  Ability to decompose technical problems and troubleshoot system and dataflow issues.
  Must be able to work on customer site most of the time.
  Preferred Requirements
  Experience with NOSQL databases such as Accumulo desired
  Prior Experience supporting cyber and/or network security operations within a large enterprise, as either an analyst, engineer, architect, or developer.
  
 c9sKXV1qkh","<div>
 <p><b>Hybrid/Remote role</b></p>
 <p> Looking for a talented Data Engineer to support the acquisition of mission critical and mission support data sets. The preferred candidate will have a background in supporting cyber and/or network related missions within the military spaces, as either a developer, analyst or engineer. Work is mostly on customer site in San Antonio, TX with some hybrid support.</p>
 <p></p>
 <p><br> Essential Job Responsibilities</p>
 <p> The ideal candidate will have worked with big data systems, complex structured and unstructured data sets, and have supported government data acquisition, analysis, and/or sharing efforts in the past. </p>
 <p>To excel in the position, the candidate shall have a strong attention to detail, be able to understand technical complexities, and have the willingness to learn and adapt to the situation. </p>
 <p>The candidate will work both independently and as part of a large team to accomplish client objectives. </p>
 <p>Minimum Qualifications</p>
 <p> Security Clearance - Must have a current TS/SCI level security clearance and therefore all candidates must be a U.S. Citizen. </p>
 <p>5 years experience as a developer, analyst, or engineer with a Bachelors in related field; OR 3 years relevant experience with Masters in related field; OR High School Diploma or equivalent and 9 years relevant experience.</p>
 <p> Experience with programming languages such as Python and Java.</p>
 <p> Proficiency with acquisition and understanding of network data and the associated metadata.</p>
 <p> Fluency with data extraction, translation, and loading including data prep and labeling to enable data analytics.</p>
 <p> Experience with Kibana and Elasticsearch.</p>
 <p> Familiarity with various log formats such as JSON, XML, and others.</p>
 <p> Experience with data flow, management, and storage solutions (i.e. Kafka, NiFi, and AWS S3 and SQS solutions).</p>
 <p> Ability to decompose technical problems and troubleshoot system and dataflow issues.</p>
 <p> Must be able to work on customer site most of the time.</p>
 <p> Preferred Requirements</p>
 <p> Experience with NOSQL databases such as Accumulo desired</p>
 <p> Prior Experience supporting cyber and/or network security operations within a large enterprise, as either an analyst, engineer, architect, or developer.</p>
 <p> </p>
 <p>c9sKXV1qkh</p>
</div>","https://www.indeed.com/rc/clk?jk=699df5b9ae819cbf&atk=&xpse=SoDE67I3Juct7oydcp0LbzkdCdPP","699df5b9ae819cbf",,"Full-time",,"Remote","Data Engineer TS/SCI","Just posted","2023-10-23T13:05:50.052Z",,,,"2023-10-23T13:05:50.053Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=699df5b9ae819cbf&from=jasx&tk=1hdea7hq42do5000&vjs=3"
"TEKletics","BUSINESS INTELLIGENCE (ETL)
Job Responsibilities:The successful candidate will work with high level BI-ETL developers to process data from various source systems into an Enterprise Data Warehouse.
Primary duties include, but are not limited to:

 Building/interpreting design documents
 Building/interpreting mapping documents
 Building/completing test documents
 Participate in technical design reviews and code reviews
 Understand and adhere to ETL best practices
 Develop and troubleshoot ETL code
 Participate in peer review sessions

Required Skills: Comfortable working in a team environment  Strong work ethic*

 Excellent time management
 Excellent oral/written communication
 Prior experience using MS Office (word, excel, outlook)

Preferred Skills: Fundamental understanding of database concepts * Experience with UNIX scripting

 Experience with Object-Oriented Programming
 ETL experience
 Hadoop Experience
 Leadership Experience

Job Type: Full-time
Pay: $15.00 - $30.00 per hour
Benefits:

 Paid time off

Schedule:

 8 hour shift

COVID-19 considerations:All employees are currently working remotely
Experience:

 SQL: 1 year (Preferred)
 Python: 1 year (Preferred)
 Azure: 1 year (Preferred)

Work Location: Remote","<p><b>BUSINESS INTELLIGENCE (ETL)</b></p>
<p><b>Job Responsibilities:</b>The successful candidate will work with high level BI-ETL developers to process data from various source systems into an Enterprise Data Warehouse.</p>
<p><b>Primary duties include, but are not limited to:</b></p>
<ul>
 <li>Building/interpreting design documents</li>
 <li>Building/interpreting mapping documents</li>
 <li>Building/completing test documents</li>
 <li>Participate in technical design reviews and code reviews</li>
 <li>Understand and adhere to ETL best practices</li>
 <li>Develop and troubleshoot ETL code</li>
 <li>Participate in peer review sessions</li>
</ul>
<p><b>Required Skills:</b> <b>Comfortable working in a team environment </b> Strong work ethic<br>*</p>
<ul>
 <li>Excellent time management</li>
 <li>Excellent oral/written communication</li>
 <li>Prior experience using MS Office (word, excel, outlook)</li>
</ul>
<p>Preferred Skills: Fundamental understanding of database concepts * Experience with UNIX scripting</p>
<ul>
 <li>Experience with Object-Oriented Programming</li>
 <li>ETL experience</li>
 <li>Hadoop Experience</li>
 <li>Leadership Experience</li>
</ul>
<p>Job Type: Full-time</p>
<p>Pay: &#x24;15.00 - &#x24;30.00 per hour</p>
<p>Benefits:</p>
<ul>
 <li>Paid time off</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
</ul>
<p>COVID-19 considerations:<br>All employees are currently working remotely</p>
<p>Experience:</p>
<ul>
 <li>SQL: 1 year (Preferred)</li>
 <li>Python: 1 year (Preferred)</li>
 <li>Azure: 1 year (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,"5de691f5f49787f9",,"Full-time",,"Remote","Junior Data Engineer","3 days ago","2023-10-20T13:05:58.248Z","2.5","2","$15 - $30 an hour","2023-10-23T13:05:58.250Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=5de691f5f49787f9&from=jasx&tk=1hdea8gffk2nb800&vjs=3"
"OneSignal","OneSignal is a leading omnichannel customer engagement solution, powering personalized customer journeys across mobile and web push notifications, in-app messaging, SMS, and email. On a mission to democratize engagement, we enable over a million businesses to keep their users - including readers, fans, players and shoppers - engaged and up to date by delivering 12 billion messages daily. 
   1 in 5 new apps launches using OneSignal! We support companies in 140 countries, including Zynga, USA Today, Bitcoin.com, Eventbrite, Tribune, and many more - from startups and small businesses just getting off the ground to established companies communicating with millions of customers. 
   We’re venture-backed by SignalFire, Rakuten Ventures, Y Combinator, HubSpot, and BAM Elevate (read more about our recent Series C!). We offer remote work as the default option in the United States in California, New York, Pennsylvania, Texas, and Utah as well as in the UK and Singapore - with plans to expand the locations we support in the future. We also have offices in San Mateo, CA, New York City, and London, UK. Hiring in Singapore is done in partnership with a local PEO. 
   OneSignal has a lot of the great tech startup qualities you'd expect, but we don't stop there. Our massive scale and small team, emphasis on healthy life balance and kindness in all our interactions, and focus on ownership and personal growth make OneSignal a uniquely great place to work.
 
  Our blog contains more information about the OneSignal Engineering career ladder, and our diverse team. 
  About The Team: 
  Our User Data team empowers OneSignal customers with a Customer Data Platform that serves as a real-time system of record for user and audience data and provides timely and useful insights to our customers so that they can optimally understand and engage their users. 
  As a Senior Software Engineer, you'll have the autonomy to take ownership of significant projects and directly impact our platform's performance and features. Your expertise will shape the way businesses engage with their users. Working remotely, you'll have the flexibility to create a schedule that works best for you, allowing you to excel in both your professional and personal life. 
  What You'll Do: 
 
  Collaborate closely with Product Managers, Designers, and fellow engineers to design and implement new full-stack features and functionalities for our Customer Data Platform, using languages such as React/TypeScript, Ruby, Golang and Rust 
  Actively participate in peer code reviews and Technical Design Spec reviews, providing valuable technical insights to continuously improve our code base 
  Work together with the team to efficiently resolve production issues and ensure the system scales smoothly to meet the growing demands of our customers. 
  Conduct data analysis and performance monitoring to identify areas for optimization and enhancement 
  Stay up-to-date with the latest industry trends and technologies, incorporating new ideas into our engineering processes 
  Ability to work independently in uncertainty and drive multiple experiments to derive at a solution to unblock business and customer operations. 
  Work on customer driven product development 
 
 What You'll Bring: 
 
  6+ years of professional software development experience 
  Experience building backend frameworks at scale 
  Experience with Rust and/or Golang, or a strong willingness to learn these two languages quickly 
  Experience with distributed system event streaming framework such as Apache Kafka 
  Experience with Docker and Kubernetes 
  Experience designing RESTful API 
 
 We value a variety of experiences, and these are not required. It would be an added bonus if you have experience in any of the following: 
 
  Experience with ScyllaDB 
  Experience with Ruby/Rails 
  Experience building a robust React Web application 
  Experience with continuous build in an Agile Environment 
  Have a good understanding of clean software design principles 
  
 The New York and California base salary for this full time position is between $160,000 to $180,000. Your exact starting salary is determined by a number of factors such as your experience, skills, and qualifications. In addition to base salary, we also offer a competitive equity program and comprehensive and inclusive benefits.
 
   Qualities we look for: 
  
   Friendliness & Empathy 
   Accountability & Collaboration 
   Proactiveness & Urgency 
   Growth Mindset & Love of Learning 
   
  In keeping with our beliefs and goals, no employee or applicant will face discrimination/harassment based on: race, color, ancestry, national origin, religion, age, gender, marital domestic partner status, sexual orientation, gender identity, disability status, or veteran status. Above and beyond discrimination/harassment based on 'protected categories,' we also strive to prevent other, subtler forms of inappropriate behavior (e.g., stereotyping) from ever gaining a foothold in our office. Whether blatant or hidden, barriers to success have no place in our workplace. 
   Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and certain state or local laws. A reasonable accommodation is a change in the way things are normally done which will ensure an equal employment opportunity without imposing undue hardship on OneSignal. Please inform us if you need assistance completing any forms or to otherwise participate in the application and/or interview process. 
   OneSignal collects and processes personal data submitted by job applicants in accordance with our Privacy Policy - including GDPR and CCPA compliance.","<div>
 <div>
  <p>OneSignal is a leading omnichannel customer engagement solution, powering personalized customer journeys across mobile and web push notifications, in-app messaging, SMS, and email. On a mission to democratize engagement, we enable over a million businesses to keep their users - including readers, fans, players and shoppers - engaged and up to date by delivering 12 billion messages <i>daily.</i></p> 
  <p> 1 in 5 new apps launches using OneSignal! We support companies in 140 countries, including Zynga, USA Today, Bitcoin.com, Eventbrite, Tribune, and many more - from startups and small businesses just getting off the ground to established companies communicating with millions of customers.</p> 
  <p> We&#x2019;re venture-backed by SignalFire, Rakuten Ventures, Y Combinator, HubSpot, and BAM Elevate (read more about our recent Series C!). We offer remote work as the default option in the United States in California, New York, Pennsylvania, Texas, and Utah as well as in the UK and Singapore - with plans to expand the locations we support in the future. We also have offices in San Mateo, CA, New York City, and London, UK. Hiring in Singapore is done in partnership with a local PEO.</p> 
  <p> OneSignal has a lot of the great tech startup qualities you&apos;d expect, but we don&apos;t stop there. Our massive scale and small team, emphasis on healthy life balance and kindness in all our interactions, and focus on ownership and personal growth make OneSignal a uniquely great place to work.</p>
 </div>
 <p> Our blog contains more information about the OneSignal Engineering career ladder, and our diverse team.</p> 
 <h3 class=""jobSectionHeader""><b> About The Team:</b></h3> 
 <p> Our User Data team empowers OneSignal customers with a Customer Data Platform that serves as a real-time system of record for user and audience data and provides timely and useful insights to our customers so that they can optimally understand and engage their users.</p> 
 <p> As a Senior Software Engineer, you&apos;ll have the autonomy to take ownership of significant projects and directly impact our platform&apos;s performance and features. Your expertise will shape the way businesses engage with their users. Working remotely, you&apos;ll have the flexibility to create a schedule that works best for you, allowing you to excel in both your professional and personal life.</p> 
 <h3 class=""jobSectionHeader""><b> What You&apos;ll Do:</b></h3> 
 <ul>
  <li>Collaborate closely with Product Managers, Designers, and fellow engineers to design and implement new full-stack features and functionalities for our Customer Data Platform, using languages such as React/TypeScript, Ruby, Golang and Rust</li> 
  <li>Actively participate in peer code reviews and Technical Design Spec reviews, providing valuable technical insights to continuously improve our code base</li> 
  <li>Work together with the team to efficiently resolve production issues and ensure the system scales smoothly to meet the growing demands of our customers.</li> 
  <li>Conduct data analysis and performance monitoring to identify areas for optimization and enhancement</li> 
  <li>Stay up-to-date with the latest industry trends and technologies, incorporating new ideas into our engineering processes</li> 
  <li>Ability to work independently in uncertainty and drive multiple experiments to derive at a solution to unblock business and customer operations.</li> 
  <li>Work on customer driven product development</li> 
 </ul>
 <h3 class=""jobSectionHeader""><b>What You&apos;ll Bring:</b></h3> 
 <ul>
  <li>6+ years of professional software development experience</li> 
  <li>Experience building backend frameworks at scale</li> 
  <li>Experience with Rust and/or Golang, or a strong willingness to learn these two languages quickly</li> 
  <li>Experience with distributed system event streaming framework such as Apache Kafka</li> 
  <li>Experience with Docker and Kubernetes</li> 
  <li>Experience designing RESTful API</li> 
 </ul>
 <h3 class=""jobSectionHeader""><b>We value a variety of experiences, and these are not required. It would be an added bonus if you have experience in any of the following:</b></h3> 
 <ul>
  <li>Experience with ScyllaDB</li> 
  <li>Experience with Ruby/Rails</li> 
  <li>Experience building a robust React Web application</li> 
  <li>Experience with continuous build in an Agile Environment</li> 
  <li>Have a good understanding of clean software design principles</li> 
 </ul> 
 <p><i>The New York and California base salary for this full time position is between &#x24;160,000 to &#x24;180,000. Your exact starting salary is determined by a number of factors such as your experience, skills, and qualifications. In addition to base salary, we also offer a competitive equity program and comprehensive and inclusive benefits.</i></p>
 <div>
  <h3 class=""jobSectionHeader""><b> Qualities we look for:</b></h3> 
  <ul>
   <li>Friendliness &amp; Empathy</li> 
   <li>Accountability &amp; Collaboration</li> 
   <li>Proactiveness &amp; Urgency</li> 
   <li>Growth Mindset &amp; Love of Learning</li> 
  </ul> 
  <p><i>In keeping with our beliefs and goals, no employee or applicant will face discrimination/harassment based on: race, color, ancestry, national origin, religion, age, gender, marital domestic partner status, sexual orientation, gender identity, disability status, or veteran status. Above and beyond discrimination/harassment based on &apos;protected categories,&apos; we also strive to prevent other, subtler forms of inappropriate behavior (e.g., stereotyping) from ever gaining a foothold in our office. Whether blatant or hidden, barriers to success have no place in our workplace.</i></p> 
  <p><i> Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and certain state or local laws. A reasonable accommodation is a change in the way things are normally done which will ensure an equal employment opportunity without imposing undue hardship on OneSignal. Please inform us if you need assistance completing any forms or to otherwise participate in the application and/or interview process.</i></p> 
  <p><i> OneSignal collects and processes personal data submitted by job applicants in accordance with our</i><i> Privacy Policy</i><i> - including GDPR and CCPA compliance.</i></p>
 </div>
</div>","https://onesignal.com/careers/4294005006","011c8a21605190d4",,"Full-time",,"Remote","Senior Software Engineer, User Data Team","3 days ago","2023-10-20T13:05:57.234Z",,,,"2023-10-23T13:05:57.318Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=011c8a21605190d4&from=jasx&tk=1hdea8gffk2nb800&vjs=3"
"MetroSys","Responsibilities:
 
   Collaborate with stakeholders to understand data migration requirements, including scope, timelines, and specific data sets.
   Design and develop a comprehensive data migration plan, considering factors such as data volume, complexity, and business continuity.
   Utilize Komprise data management software to perform assessments, identify target data, and orchestrate the migration process.
   Configure and optimize Komprise settings to align with migration goals, ensuring efficient and secure data transfer.
   Conduct pre-migration validation tests to ensure data integrity and accuracy before the actual migration process.
   Monitor the data migration process in real-time, addressing any issues or discrepancies as they arise.
   Implement data validation and reconciliation procedures to verify the successful completion of the migration.
   Collaborate with cross-functional teams, including storage administrators and system engineers, to ensure seamless integration with the NetApp environment.
   Document the entire migration process, including configurations, settings, and any custom scripts or workflows used.
   Provide knowledge transfer and training to internal teams for ongoing management and maintenance of the migrated data.
 
  Requirements:
 
   Bachelor's degree in Information Technology, Computer Science, or a related field (preferred) or equivalent work experience.
   Proven work experience as a Data Migration Engineer with specific expertise in migrating data from Isilon to NetApp using Komprise.
   Strong proficiency in Komprise data management software and related tools.
   In-depth knowledge of Isilon and NetApp storage platforms, including file systems, protocols, and administration.
   Experience with scripting languages (e.g., Python, PowerShell) for automation and customization of migration processes.
   Excellent problem-solving and analytical skills, with the ability to diagnose and resolve complex data migration issues.
   Strong communication and interpersonal skills, with the ability to collaborate effectively with technical and non-technical stakeholders.
 
  
 T7ZuUtppMf","<div>
 <p>Responsibilities:</p>
 <ul>
  <li> Collaborate with stakeholders to understand data migration requirements, including scope, timelines, and specific data sets.</li>
  <li> Design and develop a comprehensive data migration plan, considering factors such as data volume, complexity, and business continuity.</li>
  <li> Utilize Komprise data management software to perform assessments, identify target data, and orchestrate the migration process.</li>
  <li> Configure and optimize Komprise settings to align with migration goals, ensuring efficient and secure data transfer.</li>
  <li> Conduct pre-migration validation tests to ensure data integrity and accuracy before the actual migration process.</li>
  <li> Monitor the data migration process in real-time, addressing any issues or discrepancies as they arise.</li>
  <li> Implement data validation and reconciliation procedures to verify the successful completion of the migration.</li>
  <li> Collaborate with cross-functional teams, including storage administrators and system engineers, to ensure seamless integration with the NetApp environment.</li>
  <li> Document the entire migration process, including configurations, settings, and any custom scripts or workflows used.</li>
  <li> Provide knowledge transfer and training to internal teams for ongoing management and maintenance of the migrated data.</li>
 </ul>
 <p> Requirements:</p>
 <ul>
  <li> Bachelor&apos;s degree in Information Technology, Computer Science, or a related field (preferred) or equivalent work experience.</li>
  <li> Proven work experience as a Data Migration Engineer with specific expertise in migrating data from Isilon to NetApp using Komprise.</li>
  <li> Strong proficiency in Komprise data management software and related tools.</li>
  <li> In-depth knowledge of Isilon and NetApp storage platforms, including file systems, protocols, and administration.</li>
  <li> Experience with scripting languages (e.g., Python, PowerShell) for automation and customization of migration processes.</li>
  <li> Excellent problem-solving and analytical skills, with the ability to diagnose and resolve complex data migration issues.</li>
  <li> Strong communication and interpersonal skills, with the ability to collaborate effectively with technical and non-technical stakeholders.</li>
 </ul>
 <p> </p>
 <p>T7ZuUtppMf</p>
</div>","https://www.indeed.com/rc/clk?jk=7f7a185825ef7e9f&atk=&xpse=SoDK67I3JucsiLSKKZ0LbzkdCdPP","7f7a185825ef7e9f",,,,"Remote","Data Migration Engineer","3 days ago","2023-10-20T13:05:59.579Z",,,"$60 - $80 an hour","2023-10-23T13:05:59.580Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=7f7a185825ef7e9f&from=jasx&tk=1hdea8gffk2nb800&vjs=3"
"Headspace","About the Staff Data Engineer at Headspace: 
  Headspace is seeking an experienced Staff Data Engineer to join our Data Products team (part of our Data Engineering org). In this role, you will be responsible for architecting and implementing a set of core data sets in our data lake. Your customers are our data consumers, including analysts, machine learning engineers and data scientists. 
  What you will do: 
  
  Design and implement mission critical data pipelines for our company 
  Help create a set of high-quality, composable data products for our data consumers 
  Write well designed, testable, performant, and efficient code that runs on Apache Spark and Delta Lake 
  Lead the development of a world-class data lake that meets the strict security, privacy, and compliance requirements of the healthcare industry 
  Collaborate with the data science and machine learning team to build data sets used for model training and development 
  Mentor and coach other engineers to build a data-first culture at the company 
  Write well designed, testable, performant, and efficient code 
  Contribute in all phases of the development lifecycle 
  
 What you will bring: 
  Required Skills: 
  
  6+ years professional software development 
  You’ve built high quality data pipelines before with comprehensive unit tests suites, data quality checks etc. 
  Has a solid grasp of building new frameworks, tools or systems. Able to bring creative technical solutions to the table and design solutions at scale. 
  Experience with Apache Spark and Delta Lake are a plus, but not required 
  Solid understanding of system topologies from machine architecture to network architecture. Ability to solve unique complex problems. 
  Ability to work independently with minimal supervision on system level projects. Identifies and corrects errors on their own. Assumes greater responsibilities and anticipates some team needs. A wide degree of creativity and latitude is expected. 
  Proposes new solutions, ideas, tools and techniques for moderately complex problems. 
  Begins to assume a lead role in team projects. Mentors and provides guidance. 
  Considers multiple approaches and recommends best technical direction including logic and reasoning to areas outside of the immediate team. 
  Proven hands-on Software Development experience, especially API and microservices architecture 
  
 Preferred Skills: 
  
  Having experience apache spark would be useful 
  Experience with data modeling 
  Python experience is a plus 
  
 Pay & Benefits: 
  The base salary range for this role is determined by a number of factors, including but not limited to skills and scope required, relevant licensure and certifications, and unique relevant experience and job-related skills. The base salary range for this role is $131,414 - $190,100. 
  At Headspace, cash salary is but one component of our Total Rewards package. We’re proud of our robust package inclusive of: base salary, stock awards, comprehensive healthcare coverage, monthly wellness stipend, retirement savings match, lifetime Headspace membership, unlimited, free mental health coaching, generous parental leave, and much more. Paid performance incentives are also included for those in eligible roles. Additional details about our Total Rewards package will be provided during the recruitment process. 
  How we feel about Diversity, Equity, Inclusion and Belonging: 
  Headspace is committed to bringing together humans from different backgrounds and perspectives, providing employees with a safe and welcoming work environment free of discrimination and harassment. We strive to create a diverse & inclusive environment where everyone can thrive, feel a sense of belonging, and do impactful work together. 
  As an equal opportunity employer, we prohibit any unlawful discrimination against a job applicant on the basis of their race, color, religion, gender, gender identity, gender expression, sexual orientation, national origin, family or parental status, disability*, age, veteran status, or any other status protected by the laws or regulations in the locations where we operate. We respect the laws enforced by the EEOC and are dedicated to going above and beyond in fostering diversity across our workplace. 
 
  Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and certain state or local laws. A reasonable accommodation is a change in the way things are normally done which will ensure an equal employment opportunity without imposing undue hardship on Headspace Health. Please inform our Talent team if you need any assistance completing any forms or to otherwise participate in the application or interview process.
  
  
 Headspace participates in the E-Verify Program. 
  Privacy Statement 
  All member records are protected according to our Privacy Policy. Further, while employees of Headspace (formerly Ginger) cannot access Headspace products/services, they will be offered benefits according to the company's benefit plan. To ensure we are adhering to best practice and ethical guidelines in the field of mental health, we take care to avoid dual relationships. A dual relationship occurs when a mental health care provider has a second, significantly different relationship with their client in addition to the traditional client-therapist relationship—including, for example, a managerial relationship. 
  As such, Headspace requests that individuals who have received coaching or clinical services at Headspace wait until their care with Headspace is complete before applying for a position. If someone with a Headspace account is hired for a position, please note their account will be deactivated and they will not be able to use Headspace services for the duration of their employment. 
  Further, if Headspace cannot find a role that fails to resolve an ethical issue associated with a dual relationship, Headspace may need to take steps to ensure ethical obligations are being adhered to, including a delayed start date or a potential leave of absence. Such steps would be taken to protect both the former member, as well as any relevant individuals from their care team, from impairment, risk of exploitation, or harm. 
  For how how we will use the personal information you provide as part of the application process, please see: https://organizations.headspace.com/page/applicant-notice.","<div>
 <p><b>About the Staff Data Engineer</b><b> at Headspace:</b></p> 
 <p><i> Headspace is seeking an experienced Staff Data Engineer to join our Data Products team (part of our Data Engineering org). In this role, you will be responsible for architecting and implementing a set of core data sets in our data lake. Your customers are our data consumers, including analysts, machine learning engineers and data scientists.</i></p> 
 <p><b> What you will do:</b></p> 
 <ul> 
  <li>Design and implement mission critical data pipelines for our company</li> 
  <li>Help create a set of high-quality, composable data products for our data consumers</li> 
  <li>Write well designed, testable, performant, and efficient code that runs on Apache Spark and Delta Lake</li> 
  <li>Lead the development of a world-class data lake that meets the strict security, privacy, and compliance requirements of the healthcare industry</li> 
  <li>Collaborate with the data science and machine learning team to build data sets used for model training and development</li> 
  <li>Mentor and coach other engineers to build a data-first culture at the company</li> 
  <li>Write well designed, testable, performant, and efficient code</li> 
  <li>Contribute in all phases of the development lifecycle</li> 
 </ul> 
 <p><b>What you will bring</b>:</p> 
 <p><b> Required Skills:</b></p> 
 <ul> 
  <li>6+ years professional software development</li> 
  <li>You&#x2019;ve built high quality data pipelines before with comprehensive unit tests suites, data quality checks etc.</li> 
  <li>Has a solid grasp of building new frameworks, tools or systems. Able to bring creative technical solutions to the table and design solutions at scale.</li> 
  <li>Experience with Apache Spark and Delta Lake are a plus, but not required</li> 
  <li>Solid understanding of system topologies from machine architecture to network architecture. Ability to solve unique complex problems.</li> 
  <li>Ability to work independently with minimal supervision on system level projects. Identifies and corrects errors on their own. Assumes greater responsibilities and anticipates some team needs. A wide degree of creativity and latitude is expected.</li> 
  <li>Proposes new solutions, ideas, tools and techniques for moderately complex problems.</li> 
  <li>Begins to assume a lead role in team projects. Mentors and provides guidance.</li> 
  <li>Considers multiple approaches and recommends best technical direction including logic and reasoning to areas outside of the immediate team.</li> 
  <li>Proven hands-on Software Development experience, especially API and microservices architecture</li> 
 </ul> 
 <p><b>Preferred Skills:</b></p> 
 <ul> 
  <li>Having experience apache spark would be useful</li> 
  <li>Experience with data modeling</li> 
  <li>Python experience is a plus</li> 
 </ul> 
 <p><b>Pay &amp; Benefits</b>:</p> 
 <p> The base salary range for this role is determined by a number of factors, including but not limited to skills and scope required, relevant licensure and certifications, and unique relevant experience and job-related skills. The base salary range for this role is <b>&#x24;131,414 - &#x24;190,100</b>.</p> 
 <p> At Headspace, cash salary is but one component of our Total Rewards package. We&#x2019;re proud of our robust package inclusive of: base salary, stock awards, comprehensive healthcare coverage, monthly wellness stipend, retirement savings match, lifetime Headspace membership, unlimited, free mental health coaching, generous parental leave, and much more. Paid performance incentives are also included for those in eligible roles. Additional details about our Total Rewards package will be provided during the recruitment process.</p> 
 <p><b> How we feel about Diversity, Equity, Inclusion and Belonging:</b></p> 
 <p> Headspace is committed to bringing together humans from different backgrounds and perspectives, providing employees with a safe and welcoming work environment free of discrimination and harassment. We strive to create a diverse &amp; inclusive environment where everyone can thrive, feel a sense of belonging, and do impactful work together.</p> 
 <p> As an equal opportunity employer, we prohibit any unlawful discrimination against a job applicant on the basis of their race, color, religion, gender, gender identity, gender expression, sexual orientation, national origin, family or parental status, disability*, age, veteran status, or any other status protected by the laws or regulations in the locations where we operate. We respect the laws enforced by the EEOC and are dedicated to going above and beyond in fostering diversity across our workplace.</p> 
 <ul>
  <li><i>Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and certain state or local laws. A reasonable accommodation is a change in the way things are normally done which will ensure an equal employment opportunity without imposing undue hardship on Headspace Health. </i><b><i>Please inform our Talent team</i></b><b><i> if you need any assistance completing any forms or to otherwise participate in the application or interview process.</i></b></li>
 </ul> 
 <p></p> 
 <p><i>Headspace participates in the </i><i>E-Verify Program</i><i>.</i></p> 
 <p><b><i> Privacy Statement</i></b></p> 
 <p><i> All member records are protected according to our</i><i> </i><i>Privacy Policy</i><i>. Further, while employees of Headspace (formerly Ginger) cannot access Headspace products/services, they will be offered benefits according to the company&apos;s benefit plan. To ensure we are adhering to best practice and ethical guidelines in the field of mental health, we take care to avoid dual relationships. A dual relationship occurs when a mental health care provider has a second, significantly different relationship with their client in addition to the traditional client-therapist relationship&#x2014;including, for example, a managerial relationship.</i></p> 
 <p><i> As such, Headspace requests that individuals who have received coaching or clinical services at Headspace wait until their care with Headspace is complete before applying for a position. If someone with a Headspace account is hired for a position, please note their account will be deactivated and they will not be able to use Headspace services for the duration of their employment.</i></p> 
 <p><i> Further, if Headspace cannot find a role that fails to resolve an ethical issue associated with a dual relationship, Headspace may need to take steps to ensure ethical obligations are being adhered to, including a delayed start date or a potential leave of absence. Such steps would be taken to protect both the former member, as well as any relevant individuals from their care team, from impairment, risk of exploitation, or harm.</i></p> 
 <p> For how how we will use the personal information you provide as part of the application process, please see: https://organizations.headspace.com/page/applicant-notice.</p>
</div>","https://www.indeed.com/rc/clk?jk=f1db132bcf22f966&atk=&xpse=SoA367I3JucsSmSRZp0LbzkdCdPP","f1db132bcf22f966",,,,"Remote","Staff Data Engineer","3 days ago","2023-10-20T13:06:01.244Z",,,"$131,414 - $190,100 a year","2023-10-23T13:06:01.246Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=f1db132bcf22f966&from=jasx&tk=1hdea8gffk2nb800&vjs=3"
"Sporty Group","Sporty's sites are some of the most popular on the internet, consistently staying in Alexa's list of top websites for the countries they operate in
 
 
 
   As a Data Engineer at Sporty, you will play a critical role in ensuring the smooth processing and handling of data for our machine learning and data science initiatives. Your primary responsibilities will include designing, building, testing, optimising, and maintaining data pipelines and architectures for various aspects of our rapidly growing business.
 
 
 
   Who We Are
 
 
 
   Sporty Group is a consumer internet and technology business with an unrivalled sports media, gaming, social and fintech platform which serves millions of daily active users across the globe via technology and operations hubs across more than 10 countries and 3 continents.
 
 
   The recipe for our success is to discover intelligent and energetic people, who are passionate about our products and serving our users, and attract and retain them with a dynamic and flexible work life which empowers them to create value and rewards them generously based upon their contribution.
 
 
   We have already built a capable and proven team of 300+ high achievers from a diverse set of backgrounds and we are looking for more talented individuals to drive further growth and contribute to the innovation, creativity and hard work that currently serves our users further via their grit and innovation.
 
 
 
   Responsibilities
 
 
 
   Design, develop and maintain scalable batch ETL and near-real-time data pipelines and architectures for various parts of our business, on fast and versatile data sources with millions of changes per day
 
 
   Ensure all data provided is of the highest quality, accuracy, and consistency
 
 
   Identify, design, and implement internal process improvements for optimising data delivery and re-designing infrastructure for greater scalability
 
 
   Builds out new API integrations to support continuing increases in data volume and complexity
 
 
   Communicate with data scientist, MLOps engineers, product owners and BI analysts in order to understand business processes and system architecture for specific product features
 
 
 
   Requirements
 
 
 
   Bachelor’s degree, or equivalent experience, in Computer Science, Engineering, Mathematics, or a related technical field
 
 
   3+ years of experience in data engineering, data platforms, BI or related domain
 
 
   Experience in successfully implementing data-centric applications, such as data warehouses, operational data stores, and data integration projects
 
 
   Experience with large-scale production relational and NoSQL databases
 
 
   Experience with data modelling
 
 
   General understanding of data architectures and event-driven architectures
 
 
   Proficient in SQL
 
 
   Familiarity with one scripting language, preferably Python
 
 
   Experience with Apache Airflow
 
 
   Solid understanding of cloud data services: AWS services such as S3, Athena, EC2, RedShift, EMR (Elastic MapReduce), EKS, RDS (Relational Database Services) and Lambda
 
 
 
   Nice to have:
 
 
 
   Apache Spark
 
 
   Understanding of containerisation and orchestration technologies like Docker/Kubernetes
 
 
   Relevant knowledge or experience in the gaming industry
  
 
  Benefits
 
 
 
   Quarterly and flash bonuses
 
 
   We have core hours of 10am-3pm in a local timezone, but flexible hours outside of this
 
 
   Education allowance
 
 
   Referral bonuses
 
 
   28 days paid annual leave
 
 
   2 x annual company retreats (Lisbon + Dubai in 2022 / Phuket in Q2 2023 + 1 more TBC!)
 
 
   Highly talented, dependable co-workers in a global, multicultural organisation
 
 
   Payment via world class online wallet system DEEL
 
 
   Top of the line equipment supplied by market leader Hofy
 
 
   We score 100% on The Joel Test 
 
 
  Our teams are small enough for you to be impactful
 
 
   Our business is globally established and successful, offering stability and security to our Team Members
 
 
 
   Our Mission
 
 
 
   Our mission is to be an everyday entertainment platform for everyone
 
 
 
   Our Operating Principles
 
 
 
   1. Create Value for Users
 
 
   2. Act in the Long-Term Interests of Sporty
 
 
   3. Focus on Product Improvements & Innovation
 
 
   4. Be Responsible
 
 
   5. Preserve Integrity & Honesty
 
 
   6. Respect Confidentiality & Privacy
 
 
   7. Ensure Stability, Security & Scalability
 
 
   8. Work Hard with Passion & Pride
 
 
 
   Interview Process
 
 
 
   HackerRank Test
 
 
   Remote video screening with our Talent Acquisition Team + live ID check 
 
 
  Remote 90 min video interview loop with 3 x Team Members (30 mins each) 
 
 
  Pre offer call with Talent Acquisition Team
 
 
   ID check via Zinc 
 
 
  24-72 hour feedback loops throughout process
   
 
 
  
 
  Working at Sporty
 
 
 
   The top-down mentality at Sporty is high performance based, meaning we trust you to do your job with an emphasis on support to help you achieve, grow and de-block any issues when they're in your way.
 
 
   Generally employees can choose their own hours, as long as they are collaborating and doing stand-ups etc. The emphasis is really on results.
 
 
 
   As we are a highly structured and established company we are able to offer the security and support of a global business with the allure of a startup environment. Sporty is independently managed and financed, meaning we don’t have arbitrary shareholder or VC targets to cater to.
 
 
 
   We literally build, spend and make decisions based on the ethos of building THE best platform of its kind. We are truly a tech company to the core and take excellent care of our Team Members.","<div>
 <div>
  Sporty&apos;s sites are some of the most popular on the internet, consistently staying in Alexa&apos;s list of top websites for the countries they operate in
 </div>
 <div></div>
 <div>
  <br> As a Data Engineer at Sporty, you will play a critical role in ensuring the smooth processing and handling of data for our machine learning and data science initiatives. Your primary responsibilities will include designing, building, testing, optimising, and maintaining data pipelines and architectures for various aspects of our rapidly growing business.
 </div>
 <div></div>
 <div>
  <b><br> Who We Are</b>
 </div>
 <div></div>
 <div>
  <br> Sporty Group is a consumer internet and technology business with an unrivalled sports media, gaming, social and fintech platform which serves millions of daily active users across the globe via technology and operations hubs across more than 10 countries and 3 continents.
 </div>
 <div>
   The recipe for our success is to discover intelligent and energetic people, who are passionate about our products and serving our users, and attract and retain them with a dynamic and flexible work life which empowers them to create value and rewards them generously based upon their contribution.
 </div>
 <div>
   We have already built a capable and proven team of 300+ high achievers from a diverse set of backgrounds and we are looking for more talented individuals to drive further growth and contribute to the innovation, creativity and hard work that currently serves our users further via their grit and innovation.
 </div>
 <div></div>
 <div>
  <b><br> Responsibilities</b>
 </div>
 <div></div>
 <div>
  <br> Design, develop and maintain scalable batch ETL and near-real-time data pipelines and architectures for various parts of our business, on fast and versatile data sources with millions of changes per day
 </div>
 <div>
   Ensure all data provided is of the highest quality, accuracy, and consistency
 </div>
 <div>
   Identify, design, and implement internal process improvements for optimising data delivery and re-designing infrastructure for greater scalability
 </div>
 <div>
   Builds out new API integrations to support continuing increases in data volume and complexity
 </div>
 <div>
   Communicate with data scientist, MLOps engineers, product owners and BI analysts in order to understand business processes and system architecture for specific product features
 </div>
 <div></div>
 <div>
  <b><br> Requirements</b>
 </div>
 <div></div>
 <div>
  <br> Bachelor&#x2019;s degree, or equivalent experience, in Computer Science, Engineering, Mathematics, or a related technical field
 </div>
 <div>
   3+ years of experience in data engineering, data platforms, BI or related domain
 </div>
 <div>
   Experience in successfully implementing data-centric applications, such as data warehouses, operational data stores, and data integration projects
 </div>
 <div>
   Experience with large-scale production relational and NoSQL databases
 </div>
 <div>
   Experience with data modelling
 </div>
 <div>
   General understanding of data architectures and event-driven architectures
 </div>
 <div>
   Proficient in SQL
 </div>
 <div>
   Familiarity with one scripting language, preferably Python
 </div>
 <div>
   Experience with Apache Airflow
 </div>
 <div>
   Solid understanding of cloud data services: AWS services such as S3, Athena, EC2, RedShift, EMR (Elastic MapReduce), EKS, RDS (Relational Database Services) and Lambda
 </div>
 <div></div>
 <div>
  <b><br> Nice to have:</b>
 </div>
 <div></div>
 <div>
  <br> Apache Spark
 </div>
 <div>
   Understanding of containerisation and orchestration technologies like Docker/Kubernetes
 </div>
 <div>
   Relevant knowledge or experience in the gaming industry
 </div> 
 <div>
  <b>Benefits</b>
 </div>
 <div></div>
 <div>
  <br> Quarterly and flash bonuses
 </div>
 <div>
   We have core hours of 10am-3pm in a local timezone, but flexible hours outside of this
 </div>
 <div>
   Education allowance
 </div>
 <div>
   Referral bonuses
 </div>
 <div>
   28 days paid annual leave
 </div>
 <div>
   2 x annual company retreats (Lisbon + Dubai in 2022 / Phuket in Q2 2023 + 1 more TBC!)
 </div>
 <div>
   Highly talented, dependable co-workers in a global, multicultural organisation
 </div>
 <div>
   Payment via world class online wallet system DEEL
 </div>
 <div>
   Top of the line equipment supplied by market leader Hofy
 </div>
 <div>
   We score 100% on The Joel Test 
 </div>
 <div>
  Our teams are small enough for you to be impactful
 </div>
 <div>
   Our business is globally established and successful, offering stability and security to our Team Members
 </div>
 <div></div>
 <div>
  <b><br> Our Mission</b>
 </div>
 <div></div>
 <div>
  <br> Our mission is to be an everyday entertainment platform for everyone
 </div>
 <div></div>
 <div>
  <b><br> Our Operating Principles</b>
 </div>
 <div></div>
 <div>
  <br> 1. Create Value for Users
 </div>
 <div>
   2. Act in the Long-Term Interests of Sporty
 </div>
 <div>
   3. Focus on Product Improvements &amp; Innovation
 </div>
 <div>
   4. Be Responsible
 </div>
 <div>
   5. Preserve Integrity &amp; Honesty
 </div>
 <div>
   6. Respect Confidentiality &amp; Privacy
 </div>
 <div>
   7. Ensure Stability, Security &amp; Scalability
 </div>
 <div>
   8. Work Hard with Passion &amp; Pride
 </div>
 <div></div>
 <div>
  <b><br> Interview Process</b>
 </div>
 <div></div>
 <div>
  <br> HackerRank Test
 </div>
 <div>
   Remote video screening with our Talent Acquisition Team + live ID check 
 </div>
 <div>
  Remote 90 min video interview loop with 3 x Team Members (30 mins each) 
 </div>
 <div>
  Pre offer call with Talent Acquisition Team
 </div>
 <div>
   ID check via Zinc 
 </div>
 <div>
  24-72 hour feedback loops throughout process
  <br> 
 </div>
 <div></div>
 <br> 
 <div>
  <b>Working at Sporty</b>
 </div>
 <div></div>
 <div>
  <br> The top-down mentality at Sporty is high performance based, meaning we trust you to do your job with an emphasis on support to help you achieve, grow and de-block any issues when they&apos;re in your way.
 </div>
 <div>
   Generally employees can choose their own hours, as long as they are collaborating and doing stand-ups etc. The emphasis is really on results.
 </div>
 <div></div>
 <div>
  <br> As we are a highly structured and established company we are able to offer the security and support of a global business with the allure of a startup environment. Sporty is independently managed and financed, meaning we don&#x2019;t have arbitrary shareholder or VC targets to cater to.
 </div>
 <div></div>
 <div>
  <br> We literally build, spend and make decisions based on the ethos of building THE best platform of its kind. We are truly a tech company to the core and take excellent care of our Team Members.
 </div>
</div>","https://www.indeed.com/rc/clk?jk=2d1de91e6e9422b6&atk=&xpse=SoAL67I3JucsaOwjlR0LbzkdCdPP","2d1de91e6e9422b6",,"Full-time",,"Remote","Data Engineer","3 days ago","2023-10-20T13:06:02.255Z",,,,"2023-10-23T13:06:02.256Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=2d1de91e6e9422b6&from=jasx&tk=1hdea8ee5ipb2801&vjs=3"
"Aflac, Incorporated","Salary Range: $55,000 - $140,000
 
  We’ve Got You Under Our Wing
  We are the duck. We develop and empower our people, cultivate relationships, give back to our community, and celebrate every success along the way. We do it all…The Aflac Way.
 
  Aflac, a Fortune 500 company, is an industry leader in voluntary insurance products that pay cash directly to policyholders and one of America's best-known brands. Aflac has been recognized as Fortune’s 50 Best Workplaces for Diversity and as one of World’s Most Ethical Companies by Ethisphere.com.
  
 Our business is about being there for people in need. So, ask yourself, are you the duck? If so, there’s a home, and a flourishing career for you at Aflac.
 
  Work Designation. Depending on your location within the continental US, this role may be hybrid or remote. 
 
  If you live within 50 miles of the Aflac offices located in Columbus, GA or Columbia, SC, this role will be hybrid. This means you will be expected to work in the office for at least 60% of the work week. You will work from your home (within the continental US) for the remaining portion of the work week. Details of this schedule will be discussed with your leadership.
   If you live more than 50 miles from the Aflac offices located in Columbus, GA or Columbia, SC, this role will be remote. This means you will be expected to work from your home, within the continental US. If the role is remote, there may be occasions that you are requested to come to the office based on business need. Any requests to come to the office would be communicated with you in advance.
 
 
  What does it take to be successful at Aflac?
 
   Acting with Integrity
   Communicating Effectively
   Pursuing Self-Development
   Serving Customers
   Supporting Change
   Supporting Organizational Goals
   Working with Diverse Populations
 
 
  What does it take to be successful in this role?
 
  AWS Data Platform - Cloud infrastructure, Datalake/Cloud Formation, Automation, CI/CD 
  Amazon Cloud Data Storage – S3, RedShift, DynamoDB, NoSQL 
  ETL Tools – AWS Glue, Informatica Suite, SSIS, Infoworks 
  SQL & Relational Databases – SQL Server, Teradata, MS Access, HIVE, HBase 
  XML 
  XSLT 
  .NET Framework 
  C# 
  Java 
  JavaScript 
  jQuery 
  LINQ 
  MVC Framework 
  ASPX 
  Angular.js 
  Bootstrap 
  Knockout 
  Business Intelligence 
  ETL Techniques 
  Data Modeling 
  Data Warehousing/Business Intelligence 
  Meta Data Repository 
  MS SQL Server
 
 
  Education & Experience Required
 
   Bachelor's Degree In Programming/systems or computer science, or related field
   Four or more years of programming experience
   Experience and understanding of multiple programming languages and applicable applications including SQL and ETL
   Experienced in Cloud data storage and consumption models such as S3 Buckets, Lake Formation, RedShift, Dynamo DB
   Experienced in working with compute engines such as Spark, EMR, Data bricks, Snowflake etc.
 
  Or an equivalent combination of education and experience
 
  Principal Duties & Responsibilities
 
  Works under minimum supervisor to exercise independent decision making; Creates processes which initiate the ETL or Batch cycle; develops streaming processes for extracted data loading to destination database, including on-the-fly processing where extract and transformation phase to no go to persistent storage; Performs data profiling of source data in order to identify data quality issues and anomalies, business knowledge embedded in data; natural keys, and meta data information
 
  
 
  Build repeatable, automated and sustainable Extract, Transform and Load (ETL) processes leveraging platforms such as AWS cloud native – AWS Glue, DMS, Informatica, Infoworks, Hadoop, Spark processing Engines
 
  
 
  Creates data validation rule on source data to confirm the data has correct and/or expected values; Writes alternate workflow steps or reports back to the source for further analysis and correction of incorrect record(s) when validation rules are not passed
 
  
 
  Develops processes to be applied to extracted source data to move to target state; Writes data cleansing functions to get data to proper prunes data set to include only fields needed; translates source code values to target value; Standardizes free form values to codes; Derives new values through calculations on existing fields; Merges data from multiple in order to generate on consolidated source for the target
 
  
 
  Sorts and Aggregates records into rollup where multiple records are represented; Creates surrogate-key values to use in place of multiple natural keys; Turns multiple columns into multiple rows or vice–versa (Transposing or Pivoting); Splits multi-valued column data into multiple columns; Disaggregates repeating columns into separate detail table(s); Creates lookup tables; Looks up and validates reference information as part of data validation
 
  
 
  Creates and applies data validation step process in order to perform partial, full or no record’s rejection; Writes processes which handle exceptions and/or move records exceptions to alternate Transform step(s)
 
  
 
  Develops processes which load the transformed data into end target systems (database, file, application, etc.); may apply different techniques based on business needs including inserting new data into target; Over write existing data with cumulative information; Updates existing data at some frequency; Creates data validation steps in this layer to ensure loaded data
 
  
 
  Creates process cleanup after complex ETL processes which release resources used to run ETL; Creates processes to archive data
 
  
 
  Participates in project collaboration meeting with clients, business analysts, and team members in order to analyze and clarify business requirements; Translates business requirements into detailed technical specifications
 
  
 
  Works with project teams to define and design scope for each project; Creates unit test cases to ensure the application meets the needs of the business
 
  
 
  Ensures proper configuration management and change controls are implemented; Provides technical assistance and cross training to other team members
 
  
 
  Designs and implements technology best practices, guidelines and repeatable processes; Prepares and presents status updates on various projects
 
  
 
  Performs other duties as required
 
 
  Total Rewards
  This compensation range is specific to the job level and takes into account the wide range of factors that are considered in making compensation decisions including, but not limited to: education, experience, licensure, certifications, geographic location, and internal equity. The range has been created in good faith based on information known to Aflac at the time of the posting. Compensation decisions are dependent on the circumstances of each case. This salary range does not include any potential incentive pay or benefits, however, such information will be provided separately when appropriate. The salary range for this position is $55,000 to $140,000.
 
  In addition to the base salary, we offer an array of benefits to meet your needs including medical, dental, and vision coverage, prescription drug coverage, health care flexible spending, dependent care flexible spending, Aflac supplemental policies (Accident, Cancer, Critical Illness and Hospital Indemnity offered at no costs to employee), 401(k) plans, annual bonuses, and an opportunity to purchase company stock. On an annual basis, you’ll also be offered 11 paid holidays, up to 20 days PTO to be used for any reason, and, if eligible, state mandated sick leave (Washington employees accrue 1 hour sick leave for every 40 hours worked) and other leaves of absence, if eligible, when needed to support your physical, financial, and emotional well-being. Aflac complies with all applicable leave laws, including, but not limited to sick and safe leave, and adoption and parental leave, in all states and localities.","<div>
 <p><b>Salary Range: </b>&#x24;55,000 - &#x24;140,000</p>
 <p></p>
 <p><b><br> We&#x2019;ve Got You Under Our Wing</b></p>
 <p> We are the duck. We develop and empower our people, cultivate relationships, give back to our community, and celebrate every success along the way. We do it all&#x2026;<i>The Aflac Way</i>.</p>
 <p></p>
 <p><br> Aflac, a Fortune 500 company, is an industry leader in voluntary insurance products that pay cash directly to policyholders and one of America&apos;s best-known brands. Aflac has been recognized as Fortune&#x2019;s 50 Best Workplaces for Diversity and as one of World&#x2019;s Most Ethical Companies by Ethisphere.com.</p>
 <p><br> </p>
 <p>Our business is about being there for people in need. So, ask yourself, are you the duck? If so, there&#x2019;s a home, and a flourishing career for you at Aflac.</p>
 <p></p>
 <p><b><br> Work Designation.</b> Depending on your location within the continental US, this role may be <b>hybrid</b> or <b>remote. </b></p>
 <ul>
  <li>If you live <i>within 50 miles</i> of the Aflac offices located in Columbus, GA or Columbia, SC, this role will be <b>hybrid.</b><b> </b>This means you will be expected to work in the office for at least 60% of the work week. You will work from your home (within the continental US) for the remaining portion of the work week. Details of this schedule will be discussed with your leadership.</li>
  <li> If you live <i>more than 50 miles</i> from the Aflac offices located in Columbus, GA or Columbia, SC, this role will be <b>remote.</b> This means you will be expected to work from your home, within the continental US. If the role is remote, there may be occasions that you are requested to come to the office based on business need. Any requests to come to the office would be communicated with you in advance.</li>
 </ul>
 <p></p>
 <p><b><br> What does it take to be successful at Aflac?</b></p>
 <ul>
  <li> Acting with Integrity</li>
  <li> Communicating Effectively</li>
  <li> Pursuing Self-Development</li>
  <li> Serving Customers</li>
  <li> Supporting Change</li>
  <li> Supporting Organizational Goals</li>
  <li> Working with Diverse Populations</li>
 </ul>
 <p></p>
 <p><b><br> What does it take to be successful in this role?</b></p>
 <ul>
  <li>AWS Data Platform - Cloud infrastructure, Datalake/Cloud Formation, Automation, CI/CD </li>
  <li>Amazon Cloud Data Storage &#x2013; S3, RedShift, DynamoDB, NoSQL </li>
  <li>ETL Tools &#x2013; AWS Glue, Informatica Suite, SSIS, Infoworks </li>
  <li>SQL &amp; Relational Databases &#x2013; SQL Server, Teradata, MS Access, HIVE, HBase </li>
  <li>XML </li>
  <li>XSLT </li>
  <li>.NET Framework </li>
  <li>C# </li>
  <li>Java </li>
  <li>JavaScript </li>
  <li>jQuery </li>
  <li>LINQ </li>
  <li>MVC Framework </li>
  <li>ASPX </li>
  <li>Angular.js </li>
  <li>Bootstrap </li>
  <li>Knockout </li>
  <li>Business Intelligence </li>
  <li>ETL Techniques </li>
  <li>Data Modeling </li>
  <li>Data Warehousing/Business Intelligence </li>
  <li>Meta Data Repository </li>
  <li>MS SQL Server</li>
 </ul>
 <p></p>
 <p><b><br> Education &amp; Experience Required</b></p>
 <ul>
  <li> Bachelor&apos;s Degree In Programming/systems or computer science, or related field</li>
  <li> Four or more years of programming experience</li>
  <li> Experience and understanding of multiple programming languages and applicable applications including SQL and ETL</li>
  <li> Experienced in Cloud data storage and consumption models such as S3 Buckets, Lake Formation, RedShift, Dynamo DB</li>
  <li> Experienced in working with compute engines such as Spark, EMR, Data bricks, Snowflake etc.</li>
 </ul>
 <p><i> Or an equivalent combination of education and experience</i></p>
 <p></p>
 <p><b><br> Principal Duties &amp; Responsibilities</b></p>
 <ul>
  <li>Works under minimum supervisor to exercise independent decision making; Creates processes which initiate the ETL or Batch cycle; develops streaming processes for extracted data loading to destination database, including on-the-fly processing where extract and transformation phase to no go to persistent storage; Performs data profiling of source data in order to identify data quality issues and anomalies, business knowledge embedded in data; natural keys, and meta data information</li>
 </ul>
 <p><br> </p>
 <ul>
  <li>Build repeatable, automated and sustainable Extract, Transform and Load (ETL) processes leveraging platforms such as AWS cloud native &#x2013; AWS Glue, DMS, Informatica, Infoworks, Hadoop, Spark processing Engines</li>
 </ul>
 <p><br> </p>
 <ul>
  <li>Creates data validation rule on source data to confirm the data has correct and/or expected values; Writes alternate workflow steps or reports back to the source for further analysis and correction of incorrect record(s) when validation rules are not passed</li>
 </ul>
 <p><br> </p>
 <ul>
  <li>Develops processes to be applied to extracted source data to move to target state; Writes data cleansing functions to get data to proper prunes data set to include only fields needed; translates source code values to target value; Standardizes free form values to codes; Derives new values through calculations on existing fields; Merges data from multiple in order to generate on consolidated source for the target</li>
 </ul>
 <p><br> </p>
 <ul>
  <li>Sorts and Aggregates records into rollup where multiple records are represented; Creates surrogate-key values to use in place of multiple natural keys; Turns multiple columns into multiple rows or vice&#x2013;versa (Transposing or Pivoting); Splits multi-valued column data into multiple columns; Disaggregates repeating columns into separate detail table(s); Creates lookup tables; Looks up and validates reference information as part of data validation</li>
 </ul>
 <p><br> </p>
 <ul>
  <li>Creates and applies data validation step process in order to perform partial, full or no record&#x2019;s rejection; Writes processes which handle exceptions and/or move records exceptions to alternate Transform step(s)</li>
 </ul>
 <p><br> </p>
 <ul>
  <li>Develops processes which load the transformed data into end target systems (database, file, application, etc.); may apply different techniques based on business needs including inserting new data into target; Over write existing data with cumulative information; Updates existing data at some frequency; Creates data validation steps in this layer to ensure loaded data</li>
 </ul>
 <p><br> </p>
 <ul>
  <li>Creates process cleanup after complex ETL processes which release resources used to run ETL; Creates processes to archive data</li>
 </ul>
 <p><br> </p>
 <ul>
  <li>Participates in project collaboration meeting with clients, business analysts, and team members in order to analyze and clarify business requirements; Translates business requirements into detailed technical specifications</li>
 </ul>
 <p><br> </p>
 <ul>
  <li>Works with project teams to define and design scope for each project; Creates unit test cases to ensure the application meets the needs of the business</li>
 </ul>
 <p><br> </p>
 <ul>
  <li>Ensures proper configuration management and change controls are implemented; Provides technical assistance and cross training to other team members</li>
 </ul>
 <p><br> </p>
 <ul>
  <li>Designs and implements technology best practices, guidelines and repeatable processes; Prepares and presents status updates on various projects</li>
 </ul>
 <p><br> </p>
 <ul>
  <li>Performs other duties as required</li>
 </ul>
 <p></p>
 <p><b><br> Total Rewards</b></p>
 <p><i> This compensation range is specific to the job level and takes into account the wide range of factors that are considered in making compensation decisions including, but not limited to: education, experience, licensure, certifications, geographic location, and internal equity. The range has been created in good faith based on information known to Aflac at the time of the posting. Compensation decisions are dependent on the circumstances of each case. This salary range does not include any potential incentive pay or benefits, however, such information will be provided separately when appropriate. The salary range for this position is &#x24;55,000 to &#x24;140,000.</i></p>
 <p></p>
 <p><i><br> In addition to the base salary, we offer an array of benefits to meet your needs including medical, dental, and vision coverage, prescription drug coverage, health care flexible spending, dependent care flexible spending, Aflac supplemental policies (Accident, Cancer, Critical Illness and Hospital Indemnity offered at no costs to employee), 401(k) plans, annual bonuses, and an opportunity to purchase company stock. On an annual basis, you&#x2019;ll also be offered 11 paid holidays, up to 20 days PTO to be used for any reason, and, if eligible, state mandated sick leave (Washington employees accrue 1 hour sick leave for every 40 hours worked) and other leaves of absence, if eligible, when needed to support your physical, financial, and emotional well-being. Aflac complies with all applicable leave laws, including, but not limited to sick and safe leave, and adoption and parental leave, in all states and localities.</i></p>
</div>
<p></p>","https://careers.aflac.com/job/Remote-Data-Engineer-%28AWSETL%29-OR-31999/1089155300/?feedId=342200&utm_source=Indeed&utm_campaign=Aflac_Indeed","6b0246ca2a37effe",,,,"Remote","Data Engineer (AWS/ETL)","3 days ago","2023-10-20T13:06:03.170Z","3.5","4234","$55,000 - $140,000 a year","2023-10-23T13:06:03.172Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=6b0246ca2a37effe&from=jasx&tk=1hdea8ee5ipb2801&vjs=3"
"Liberty Mutual","Pay Philosophy
  The typical starting salary range for this role is determined by a number of factors including skills, experience, education, certifications and location. The full salary range for this role reflects the competitive labor market value for all employees in these positions across the national market and provides an opportunity to progress as employees grow and develop within the role. Some roles at Liberty Mutual have a corresponding compensation plan which may include commission and/or bonus earnings at rates that vary based on multiple factors set forth in the compensation plan for the role.
  Description 
 
  Under direct supervision, responsible for the analysis, development, and execution of data solutions of low to moderate complexity that assists with the information lifecycle needs of an organization
   Assists with collecting, integrating, and analyzing organizational data with the purpose of drawing conclusions from that information
   Develops, constructs, tests, and maintains data architectures for data platform, database, analytical, reporting, or data science systems
   Recognizes opportunities to improve data reliability, quality, and efficiency and may make recommendations where appropriate 
  Designs and develops low complexity programs and tools to support ingestion, curation and provisioning of enterprise data to achieve analytics or reporting
   Builds and designs data models and data architecture that improve accessibility, efficiency, governance and quality of data
   Recognizes opportunities to improve data quality
   Assists with aspects of deployment of data solutions
   Helps identify possible process improvements that address technology gaps within a single business process of low to moderate complexity
   Analyzes and prepare low to moderately complex technology enabled recommendations to address gaps within a single business process
   Performs other projects and duties as assigned
   Telecommuting permitted up to 100%
 
  Qualifications
  The position requires a Bachelor’s degree, or foreign equivalent, in Electrical Engineering, or a related technical or business field plus two (2) years of experience in the job offered or a Associate Data Engineer-related occupation. Position also requires demonstrable experience with each of the following:
 
   New and emerging technologies including AWS SDK, and Docker/Kubernetes
   IT concepts, strategies and methodologies
   IT architectures and technical standards
   Business function and business operations
   Design and development tools
   Layered systems architectures and shared data engineering concepts
   Agile data engineering concepts and processes
   Applying customer requirements, including drawing out unforeseen implications and making recommendations for design, the ability to define design reasoning, understanding potential impacts of design requirements
   Telecommuting permitted up to 100%
 
  To apply, please visit https://jobs.libertymutualgroup.com/, select “Search Jobs,” enter job requisition #2023-61263 in the “Job ID or Keywords” field, and submit resume. Alternatively, you may apply by submitting a resume via e-mail to RecruitLM@LibertyMutual.com. Reference requisition number in subject of e-mail.
  About Us
  **This position may have in-office requirements depending on candidate location.**
 
  At Liberty Mutual, our purpose is to help people embrace today and confidently pursue tomorrow. That’s why we provide an environment focused on openness, inclusion, trust and respect. Here, you’ll discover our expansive range of roles, and a workplace where we aim to help turn your passion into a rewarding profession.
 
  Liberty Mutual has proudly been recognized as a “Great Place to Work” by Great Place to Work® US for the past several years. We were also selected as one of the “100 Best Places to Work in IT” on IDG’s Insider Pro and Computerworld’s 2020 list. For many years running, we have been named by Forbes as one of America’s Best Employers for Women and one of America’s Best Employers for New Graduates—as well as one of America’s Best Employers for Diversity. To learn more about our commitment to diversity and inclusion please visit: https://jobs.libertymutualgroup.com/diversity-equity-inclusion/
 
  We value your hard work, integrity and commitment to make things better, and we put people first by offering you benefits that support your life and well-being. To learn more about our benefit offerings please visit: https://LMI.co/Benefits
 
  Liberty Mutual is an equal opportunity employer. We will not tolerate discrimination on the basis of race, color, national origin, sex, sexual orientation, gender identity, religion, age, disability, veteran’s status, pregnancy, genetic information or on any basis prohibited by federal, state or local law.","<div>
 <b>Pay Philosophy</b>
 <p> The typical starting salary range for this role is determined by a number of factors including skills, experience, education, certifications and location. The full salary range for this role reflects the competitive labor market value for all employees in these positions across the national market and provides an opportunity to progress as employees grow and develop within the role. Some roles at Liberty Mutual have a corresponding compensation plan which may include commission and/or bonus earnings at rates that vary based on multiple factors set forth in the compensation plan for the role.</p>
 <b><br> Description </b>
 <ul>
  <li>Under direct supervision, responsible for the analysis, development, and execution of data solutions of low to moderate complexity that assists with the information lifecycle needs of an organization</li>
  <li> Assists with collecting, integrating, and analyzing organizational data with the purpose of drawing conclusions from that information</li>
  <li> Develops, constructs, tests, and maintains data architectures for data platform, database, analytical, reporting, or data science systems</li>
  <li> Recognizes opportunities to improve data reliability, quality, and efficiency and may make recommendations where appropriate </li>
  <li>Designs and develops low complexity programs and tools to support ingestion, curation and provisioning of enterprise data to achieve analytics or reporting</li>
  <li> Builds and designs data models and data architecture that improve accessibility, efficiency, governance and quality of data</li>
  <li> Recognizes opportunities to improve data quality</li>
  <li> Assists with aspects of deployment of data solutions</li>
  <li> Helps identify possible process improvements that address technology gaps within a single business process of low to moderate complexity</li>
  <li> Analyzes and prepare low to moderately complex technology enabled recommendations to address gaps within a single business process</li>
  <li> Performs other projects and duties as assigned</li>
  <li> Telecommuting permitted up to 100%</li>
 </ul>
 <b> Qualifications</b>
 <p> The position requires a Bachelor&#x2019;s degree, or foreign equivalent, in Electrical Engineering, or a related technical or business field plus two (2) years of experience in the job offered or a Associate Data Engineer-related occupation. Position also requires demonstrable experience with each of the following:</p>
 <ul>
  <li> New and emerging technologies including AWS SDK, and Docker/Kubernetes</li>
  <li> IT concepts, strategies and methodologies</li>
  <li> IT architectures and technical standards</li>
  <li> Business function and business operations</li>
  <li> Design and development tools</li>
  <li> Layered systems architectures and shared data engineering concepts</li>
  <li> Agile data engineering concepts and processes</li>
  <li> Applying customer requirements, including drawing out unforeseen implications and making recommendations for design, the ability to define design reasoning, understanding potential impacts of design requirements</li>
  <li> Telecommuting permitted up to 100%</li>
 </ul>
 <p> To apply, please visit https://jobs.libertymutualgroup.com/, select &#x201c;Search Jobs,&#x201d; enter job requisition #2023-61263 in the &#x201c;Job ID or Keywords&#x201d; field, and submit resume. Alternatively, you may apply by submitting a resume via e-mail to RecruitLM@LibertyMutual.com. Reference requisition number in subject of e-mail.</p>
 <b> About Us</b>
 <p> **This position may have in-office requirements depending on candidate location.**</p>
 <p></p>
 <p><br> At Liberty Mutual, our purpose is to help people embrace today and confidently pursue tomorrow. That&#x2019;s why we provide an environment focused on openness, inclusion, trust and respect. Here, you&#x2019;ll discover our expansive range of roles, and a workplace where we aim to help turn your passion into a rewarding profession.</p>
 <p></p>
 <p><br> Liberty Mutual has proudly been recognized as a &#x201c;Great Place to Work&#x201d; by Great Place to Work&#xae; US for the past several years. We were also selected as one of the &#x201c;100 Best Places to Work in IT&#x201d; on IDG&#x2019;s Insider Pro and Computerworld&#x2019;s 2020 list. For many years running, we have been named by Forbes as one of America&#x2019;s Best Employers for Women and one of America&#x2019;s Best Employers for New Graduates&#x2014;as well as one of America&#x2019;s Best Employers for Diversity. To learn more about our commitment to diversity and inclusion please visit: https://jobs.libertymutualgroup.com/diversity-equity-inclusion/</p>
 <p></p>
 <p><br> We value your hard work, integrity and commitment to make things better, and we put people first by offering you benefits that support your life and well-being. To learn more about our benefit offerings please visit: https://LMI.co/Benefits</p>
 <p></p>
 <p><br> Liberty Mutual is an equal opportunity employer. We will not tolerate discrimination on the basis of race, color, national origin, sex, sexual orientation, gender identity, religion, age, disability, veteran&#x2019;s status, pregnancy, genetic information or on any basis prohibited by federal, state or local law.</p>
</div>","https://www.indeed.com/rc/clk?jk=ae83164544567281&atk=&xpse=SoB267I3Jucr-byRQB0LbzkdCdPP","ae83164544567281",,"Full-time",,"Columbus, OH","Associate Data Engineer","3 days ago","2023-10-20T13:06:06.148Z","3.5","5354","$79,602 - $114,700 a year","2023-10-23T13:06:06.151Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=ae83164544567281&from=jasx&tk=1hdea8ee5ipb2801&vjs=3"
"Olsson","Company Description
  We are Olsson, a team-based, purpose-driven engineering and design firm. Our solutions improve communities and our people make it possible.
  Our most meaningful asset is our people, and we are dedicated to providing an environment where they can continue to learn, grow, and thrive. Our entrepreneurial spirit is what has allowed us — and will continue to allow us — to grow. The result? Inspired people, amazing designs, and projects with purpose.
 
 

 Job Description
  Olsson provides multidisciplinary design services for some of the largest and most forward-thinking and desirable companies in the world to work for. The large hyperscale data center campuses we design throughout the U.S. will give you the opportunity to work on some of the largest and most complex engineering-driven projects being built today. Our clients are relationship based and truly value the work we do for them, affording us the opportunity to contribute to society’s technological and connected community through the design of the critical infrastructure that is the foundation of these projects.
  As a Licensed Civil Engineer on our Data Center Civil Team, you will be a part of the firm’s largest and most complex projects. You will serve as a project manager on some projects and lead design engineer on others. Prepare planning and design documents, process design calculations, and develop and maintain team and client standards. You may lead quality assurance/quality control and act as an advisor on complex projects. You will also coordinate with other Olsson teams, professional staff, technical staff, clients, and other consultants.
  You may travel to job sites for observation and attend client meetings.
 
  Olsson currently has one opportunity for a Licensed Civil Engineer. This role offers flexible work options, including remote and hybrid opportunities, to accommodate diverse working preferences and promote work-life balance. Candidates can live in Lincoln, Omaha, Overland Park, Denver or Dallas-Fort Worth area and work remotely or work out of any Olsson office location in these regions/areas.
 
 
 

 Qualifications
  You are passionate about:
 
   Working collaboratively with others
   Having ownership in the work you do
   Using your talents to positively affect communities
   Solving problems
   Providing excellence in client service
 
  You bring to the team:
 
   Strong communication skills
   Ability to contribute and work well on a team
   Bachelor's Degree in civil engineering
   At least 6 years of related civil engineering experience
   Proficient in Civil 3D software
   Must be a registered professional engineer
 
  Additional Information
  Olsson is a nationally recognized, employee-owned firm specializing in planning and design, engineering, field services, environmental, and technology. Founded in 1956 on the very mindset that drives us today, we’re here to improve communities by making them more sustainable, better connected, and more efficient. Simply put, we work to leave the world better than we found it.
  As an Olsson employee, you’ll receive our traditional benefits package (health care, vision, dental, paid time off, etc.), plus you’ll:
 
   Become an owner in the company after your first year through our Employee Stock Ownership Plan (ESOP)
   Engage in work that has a positive impact in communities
   Receive an excellent 401(k) match
   Participate in a wellness program promoting balanced lifestyles
   Benefit from a bonus system that rewards performance
   Have the possibility for flexible work arrangements
 
  Olsson is an EEO employer. We encourage qualified minority, female, veteran and disabled candidates to apply and be considered for open positions. We do not discriminate against any applicant for employment, or any employee because of race, color, religion, national origin, age, sex, sexual orientation, gender identity, gender, disability, age, or military status.
  #LI-LA1
  #LI-Remote","<div>
 Company Description
 <p><br> We are Olsson, a team-based, purpose-driven engineering and design firm. Our solutions improve communities and our people make it possible.</p>
 <p> Our most meaningful asset is our people, and we are dedicated to providing an environment where they can continue to learn, grow, and thrive. Our entrepreneurial spirit is what has allowed us &#x2014; and will continue to allow us &#x2014; to grow. The result? Inspired people, amazing designs, and projects with purpose.</p>
</div> 
<br> 
<div>
 Job Description
 <p><br> Olsson provides multidisciplinary design services for some of the largest and most forward-thinking and desirable companies in the world to work for. The large hyperscale data center campuses we design throughout the U.S. will give you the opportunity to work on some of the largest and most complex engineering-driven projects being built today. Our clients are relationship based and truly value the work we do for them, affording us the opportunity to contribute to society&#x2019;s technological and connected community through the design of the critical infrastructure that is the foundation of these projects.</p>
 <p> As a Licensed Civil Engineer on our Data Center Civil Team, you will be a part of the firm&#x2019;s largest and most complex projects. You will serve as a project manager on some projects and lead design engineer on others. Prepare planning and design documents, process design calculations, and develop and maintain team and client standards. You may lead quality assurance/quality control and act as an advisor on complex projects. You will also coordinate with other Olsson teams, professional staff, technical staff, clients, and other consultants.</p>
 <p><i> You may travel to job sites for observation and attend client meetings.</i></p>
 <ul>
  <li><i>Olsson currently has one opportunity for a Licensed Civil Engineer. This role offers flexible work options, including remote and hybrid opportunities, to accommodate diverse working preferences and promote work-life balance. Candidates can live in Lincoln, Omaha, Overland Park, Denver or Dallas-Fort Worth area and work remotely or work out of any Olsson office location in these regions/areas.</i></li>
 </ul>
</div> 
<br> 
<div>
 Qualifications
 <p><b><br> You are passionate about:</b></p>
 <ul>
  <li> Working collaboratively with others</li>
  <li> Having ownership in the work you do</li>
  <li> Using your talents to positively affect communities</li>
  <li> Solving problems</li>
  <li> Providing excellence in client service</li>
 </ul>
 <p><b> You bring to the team:</b></p>
 <ul>
  <li> Strong communication skills</li>
  <li> Ability to contribute and work well on a team</li>
  <li> Bachelor&apos;s Degree in civil engineering</li>
  <li> At least 6 years of related civil engineering experience</li>
  <li> Proficient in Civil 3D software</li>
  <li> Must be a registered professional engineer</li>
 </ul>
 <br> Additional Information
 <p><br> Olsson is a nationally recognized, employee-owned firm specializing in planning and design, engineering, field services, environmental, and technology. Founded in 1956 on the very mindset that drives us today, we&#x2019;re here to improve communities by making them more sustainable, better connected, and more efficient. Simply put, we work to leave the world better than we found it.</p>
 <p> As an Olsson employee, you&#x2019;ll receive our traditional benefits package (health care, vision, dental, paid time off, etc.), plus you&#x2019;ll:</p>
 <ul>
  <li> Become an owner in the company after your first year through our Employee Stock Ownership Plan (ESOP)</li>
  <li> Engage in work that has a positive impact in communities</li>
  <li> Receive an excellent 401(k) match</li>
  <li> Participate in a wellness program promoting balanced lifestyles</li>
  <li> Benefit from a bonus system that rewards performance</li>
  <li> Have the possibility for flexible work arrangements</li>
 </ul>
 <p> Olsson is an EEO employer. We encourage qualified minority, female, veteran and disabled candidates to apply and be considered for open positions. We do not discriminate against any applicant for employment, or any employee because of race, color, religion, national origin, age, sex, sexual orientation, gender identity, gender, disability, age, or military status.</p>
 <p> #LI-LA1</p>
 <p> #LI-Remote</p>
</div>","https://www.indeed.com/applystart?jk=6bbc2980914165f8&from=vj&pos=top&mvj=0&spon=0&sjdu=YmZE5d5THV8u75cuc0H6Y26AwfY51UOGmh3Z9h4OvXhPiBPHeOuJfiMv5TBTSK4cl8EZNoFLzyHkpvcd7-Y9bg&vjfrom=serp&astse=2b605a2fe7b82dcb&assa=8495","6bbc2980914165f8",,"Full-time",,"601 P Street, Lincoln, NE 68508","Licensed Civil Engineer - Data Center (Remote)","2 days ago","2023-10-21T13:06:09.063Z","3.2","23",,"2023-10-23T13:06:09.064Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=6bbc2980914165f8&from=jasx&tk=1hdea8ee5ipb2801&vjs=3"
"OM Group Inc.","Data Engineer
  
 
 
  OM Group, Inc. is a fast-growing Microsoft Azure and AWS Partner delivering full-stack cloud services for Federal & DoD clients. We are currently expanding support for the Enterprise Data Warehouse to continue and evolve on-prem/cloud/hybrid data migration and enterprise reporting platforms.
  
 
 
  We are hiring a 
  Data Engineer with experience creating and maintaining complex shell and SQL scripts used to expand and populate data into the enterprise data warehouse. The successful candidate will be responsible for managing and maintaining the Extract, Transform and Load (ETL) processes to populate the Enterprise Data Warehouse/ Enterprise Virtual Viewer (EDW/EVV). The EDW/EVV is powered by a multi-tier environment encapsulation of Windows/Unix and Linux Environments. The primary data repository, as well as host to the virtualization/governance and catalog layers live on the IBM Cloud Pak for Data System (CP4D) and Netezza Performance Server (NPS) PostgreSQL based database. The CP4D system is Linux-based on a combined OpenShift and Redhat 7 platform. The EDW/EVV also utilizes SAP Business Objects (BOBJ) to do reporting based on a Windows Virtual Machine (VM) architecture. The Oracle 19c database is on Oracle Solaris VMs and acts as repository databases for SAP BOBJ.
  
 
 
  This position is remote on Eastern Time zone schedule.
  
 
 
  Responsibilities
  
 
  Work with stakeholders to evaluate business needs and develop tasks to meet requirements and objectives 
  Provide quality assurance and identify bugs or required fixes and communicate to respective teams. Ensure data being migrated to the EDW/EVV production environment is documented 
  Enforce EDW/EVV standards / policies and provide quality assurance for universe, star schema/build, report, and dashboard migrations from the EDW/EVV development environment into the EDW/EVV production environment (see Appendix A for migration metrics) 
  Maintain existing Unix Shell and Structured Query Language (SQL) script based ETL processes. Design, Develop, document, and maintain new and/or expanding script based ETL processes e.g., Unix Shell and SQL scripts 
  Coordinate, test, implement, document, and manage required upgrades and capability enhancements for the EDW/EVV. Perform tasks for data repository expansion as more data content is transitioned to the warehouse 
  Work with infrastructure team to ensure that all the required monitoring, exception handling and fault tolerance is in place for a production-quality data platform 
  Team up with analysts, product managers, and other stakeholders to understand evolving business needs and translate reporting capabilities accordingly 
  Assist with process improvement with a customer-focused, progressive mindset 
  Understand data classification and adhere to the information protection and privacy restrictions 
  Troubleshoot and provide technical support for staff and back-end system users 
  Document and support code migrations and provide quality assurance / control of EDW/EVV star schemas/builds, universes, local data, and reports
 
  
 
 
  Requirements
  
 
  Active DoD Secret Security clearance 
  Hold DoD IAT-III, IAM-II and IASAE-II certifications and Data related industry certifications 
  5+ years experience with Linux Shell Script development, testing, debugging and deployment 
  5+ years experience with SQL, relational database and data warehouse technologies 
  3+ years experience developing and maintaining Python or similar scripting language 
  3+ years experience in multiple subversion technologies such as Subversion, GitHub or Tortoise 
  Knowledge of AWS technologies such as S3, EC2, Redshift, Glue, Athena and Step Functions preferred 
  Experience in data mining and development of ETL processes, distributed data architectures and big data processing technologies 
  Knowledge of production / environment control 
  Knowledge of Agile software development lifecycle 
  Excellent problem-solving skills and attention to detail 
  Strong documentation and training skills 
  
 
 
  OM Group, Inc. recently voted one of the top workplaces in the Washington, DC area, is a fast-growing Microsoft Azure and AWS Partner delivering full-stack cloud services for Federal & DoD clients. We are a growing company that values your skills, training, and ideas and strives to foster a welcoming, diverse, and inclusive environment. OM Group provides competitive compensation and benefits including health insurance coverage, 401(k), paid time off, as well as support for continuous education and training.
  
 
 
  OM Group, Inc. is an equal opportunity employer (EEO) and does not discriminate on the basis of race, color, religion, sex, national origin, age, disability, veteran status, or any other characteristic protected by law. We are committed to creating a diverse and inclusive workplace where all employees are treated with respect and dignity","<div>
 <div>
  <b>Data Engineer</b>
 </div> 
 <div></div>
 <div>
  <b>OM Group, Inc.</b> is a fast-growing Microsoft Azure and AWS Partner delivering full-stack cloud services for Federal &amp; DoD clients. We are currently expanding support for the Enterprise Data Warehouse to continue and evolve on-prem/cloud/hybrid data migration and enterprise reporting platforms.
 </div> 
 <div></div>
 <div>
  We are hiring a 
  <b>Data Engineer </b>with experience creating and maintaining complex shell and SQL scripts used to expand and populate data into the enterprise data warehouse. The successful candidate will be responsible for managing and maintaining the Extract, Transform and Load (ETL) processes to populate the Enterprise Data Warehouse/ Enterprise Virtual Viewer (EDW/EVV). The EDW/EVV is powered by a multi-tier environment encapsulation of Windows/Unix and Linux Environments. The primary data repository, as well as host to the virtualization/governance and catalog layers live on the IBM Cloud Pak for Data System (CP4D) and Netezza Performance Server (NPS) PostgreSQL based database. The CP4D system is Linux-based on a combined OpenShift and Redhat 7 platform. The EDW/EVV also utilizes SAP Business Objects (BOBJ) to do reporting based on a Windows Virtual Machine (VM) architecture. The Oracle 19c database is on Oracle Solaris VMs and acts as repository databases for SAP BOBJ.
 </div> 
 <div></div>
 <div>
  This position is remote on Eastern Time zone schedule.
 </div> 
 <div></div>
 <div>
  <b>Responsibilities</b>
 </div> 
 <ul>
  <li>Work with stakeholders to evaluate business needs and develop tasks to meet requirements and objectives</li> 
  <li>Provide quality assurance and identify bugs or required fixes and communicate to respective teams. Ensure data being migrated to the EDW/EVV production environment is documented</li> 
  <li>Enforce EDW/EVV standards / policies and provide quality assurance for universe, star schema/build, report, and dashboard migrations from the EDW/EVV development environment into the EDW/EVV production environment (see Appendix A for migration metrics)</li> 
  <li>Maintain existing Unix Shell and Structured Query Language (SQL) script based ETL processes. Design, Develop, document, and maintain new and/or expanding script based ETL processes e.g., Unix Shell and SQL scripts</li> 
  <li>Coordinate, test, implement, document, and manage required upgrades and capability enhancements for the EDW/EVV. Perform tasks for data repository expansion as more data content is transitioned to the warehouse</li> 
  <li>Work with infrastructure team to ensure that all the required monitoring, exception handling and fault tolerance is in place for a production-quality data platform</li> 
  <li>Team up with analysts, product managers, and other stakeholders to understand evolving business needs and translate reporting capabilities accordingly</li> 
  <li>Assist with process improvement with a customer-focused, progressive mindset</li> 
  <li>Understand data classification and adhere to the information protection and privacy restrictions</li> 
  <li>Troubleshoot and provide technical support for staff and back-end system users</li> 
  <li>Document and support code migrations and provide quality assurance / control of EDW/EVV star schemas/builds, universes, local data, and reports</li>
 </ul>
 <br> 
 <div></div>
 <div>
  <b>Requirements</b>
 </div> 
 <ul>
  <li>Active DoD Secret Security clearance</li> 
  <li>Hold DoD IAT-III, IAM-II and IASAE-II certifications and Data related industry certifications</li> 
  <li>5+ years experience with Linux Shell Script development, testing, debugging and deployment</li> 
  <li>5+ years experience with SQL, relational database and data warehouse technologies</li> 
  <li>3+ years experience developing and maintaining Python or similar scripting language</li> 
  <li>3+ years experience in multiple subversion technologies such as Subversion, GitHub or Tortoise</li> 
  <li>Knowledge of AWS technologies such as S3, EC2, Redshift, Glue, Athena and Step Functions preferred</li> 
  <li>Experience in data mining and development of ETL processes, distributed data architectures and big data processing technologies</li> 
  <li>Knowledge of production / environment control</li> 
  <li>Knowledge of Agile software development lifecycle</li> 
  <li>Excellent problem-solving skills and attention to detail</li> 
  <li>Strong documentation and training skills</li> 
 </ul> 
 <div></div>
 <div>
  <b>OM Group, Inc.</b> recently voted one of the top workplaces in the Washington, DC area, is a fast-growing Microsoft Azure and AWS Partner delivering full-stack cloud services for Federal &amp; DoD clients. We are a growing company that values your skills, training, and ideas and strives to foster a welcoming, diverse, and inclusive environment. OM Group provides competitive compensation and benefits including health insurance coverage, 401(k), paid time off, as well as support for continuous education and training.
 </div> 
 <div></div>
 <div>
  <b>OM Group, Inc. is an equal opportunity employer (EEO)</b> and does not discriminate on the basis of race, color, religion, sex, national origin, age, disability, veteran status, or any other characteristic protected by law. We are committed to creating a diverse and inclusive workplace where all employees are treated with respect and dignity
 </div>
</div>
<div></div>","https://www.indeed.com/rc/clk?jk=e5a378ed11848b80&atk=&xpse=SoCJ67I3JucrU4RcBx0LbzkdCdPP","e5a378ed11848b80",,,,"Remote","Senior Data Engineer","2 days ago","2023-10-21T13:06:09.926Z","3.2","6",,"2023-10-23T13:06:09.927Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=e5a378ed11848b80&from=jasx&tk=1hdea8ee5ipb2801&vjs=3"
"Team Velocity","Fast-paced, high-growth, technology driven marketing company serving the digital marketing automotive industry seeks hard working Senior Data Engineer. 
 Strong MSSQL experience, a must! 
 Ideal candidate has very strong knowledge of the Microsoft SQL Server engine with a focus on the ability to troubleshoot, optimize and support existing pipelines (SSIS) and processes, and a desire to learn (or already possess and grow) popular cloud technologies like Snowflake, BigQuery, Matillion, and more. 
 The ideal candidate is analytical, results-oriented, self-driven, confident, and able to meaningfully contribute to solution design among a team of other skilled data engineers. 
 This is a full-time, salaried, remote position headquartered in Herndon, VA. Applicants must be located within the continental U.S., eastern or central time zones highly preferred. 
 RESPONSIBILITIES: 
 
  Maintain and support existing MSSQL processes, pipelines (SSIS), and schemas 
  Migrate existing pipelines and processes to the cloud (Snowflake, GCP) 
  Analyze and organize raw data sets to meet both functional and non-functional requirements 
  Develop and test new pipelines and processes, both in MSSQL and cloud environments 
  With the data engineering team, design and implement solutions that scale to meet business needs 
  Design and implement data models to support analytics and reporting initiatives 
  With the software team, develop products that deliver value to our clients 
  Participate in peer-reviews of solution designs and related artifacts 
  Package and support deployment of releases 
  Analyze and resolve technical and application problems 
  Adhere to high-quality development principles while delivering solutions on-time and on-budget 
  Provide third-level support to business users 
  Participate in business operations as necessary to meet business goals 
  
 REQUIREMENTS: 
 
  Bachelor’s degree in Information Systems, Computer Science, or Information Technology (a 4-year bachelor’s degree is acceptable) - OR - expertise in above subject matters matched by experience 
  7+ years’ experience in MSSQL data engineering and development 
  Excellent understanding of T-SQL, stored procedures, indexes, stats, functions, and views 
  Expert understanding of relational and warehousing database design and querying concepts 
  Experience with SSIS, data integration and ETL/ELT procedures 
  Exposure to agile development methodology 
  Understanding of version control concepts 
  Strong desire to learn 
  Strong desire to be world-class data and SQL troubleshooter 
  
 BENEFICIAL: 
 
  Experience with Snowflake cloud data warehouse 
  Experience with Matillion (Snowflake/BigQuery) 
  Experience with Google Cloud Platform technology stack: BigQuery, Data Flow, Data Fusion 
  Experience with BI tools like Sigma, Tableau or others 
  Experience with NoSQL databases 
  
 COMPENSATION  This is a full-time, salaried, remote position headquartered in Herndon, VA. Eastern or central time zones preferred. Competitive compensation commensurate with experience. Participation in company benefit offerings include paid time off, medical, dental, vision, 401(k)/matching, wellness, and more. 
 NEXT STEPS  If you meet the requirements, and are interested in applying for this role, please complete the online application, be sure to include a current resume, contact information, and salary requirements. NO PHONE CALLS PLEASE. 
 
 ABOUT TEAM VELOCITY  Team Velocity is a SaaS technology provider serving the automotive industry. We provide an omni-channel marketing automation platform and retailing solutions to OEMs and dealerships nationwide. We are revolutionizing the automotive industry with cutting-edge technology to help dealers sell and service more cars. Made by dealers for dealers, Team Velocity’s proprietary technology platform Apollo® analyzes consumer behavior to predict who will buy, what they will buy, and when they are ready to service. Apollo automates the entire communication process by delivering hyper-personalized campaigns across every touchpoint, maximizing ROI, and lifetime revenue.   Our vision is to serve our clients with a single technology platform that empowers them to execute intelligent marketing across every online and offline channel. We aim to deliver a frictionless consumer experience, from the initial engagement to final transaction.   Our team members are hard-working and driven to achieve success for our clients and our unique culture promotes creativity, camaraderie, and success.","<div>
 <p>Fast-paced, high-growth, technology driven marketing company serving the digital marketing automotive industry seeks hard working <b>Senior Data Engineer</b>. </p>
 <p><b><i>Strong MSSQL experience, a must! </i></b></p>
 <p>Ideal candidate has very strong knowledge of the Microsoft SQL Server engine with a focus on the ability to troubleshoot, optimize and support existing pipelines (SSIS) and processes, and a desire to learn (or already possess and grow) popular cloud technologies like Snowflake, BigQuery, Matillion, and more. </p>
 <p>The ideal candidate is analytical, results-oriented, self-driven, confident, and able to meaningfully contribute to solution design among a team of other skilled data engineers.</p> 
 <p><b><i>This is a full-time, salaried, remote position headquartered in Herndon, VA. Applicants must be located within the continental U.S., eastern or central time zones highly preferred. </i></b></p>
 <p><b>RESPONSIBILITIES:</b></p> 
 <ul>
  <li>Maintain and support existing MSSQL processes, pipelines (SSIS), and schemas</li> 
  <li>Migrate existing pipelines and processes to the cloud (Snowflake, GCP)</li> 
  <li>Analyze and organize raw data sets to meet both functional and non-functional requirements</li> 
  <li>Develop and test new pipelines and processes, both in MSSQL and cloud environments</li> 
  <li>With the data engineering team, design and implement solutions that scale to meet business needs</li> 
  <li>Design and implement data models to support analytics and reporting initiatives</li> 
  <li>With the software team, develop products that deliver value to our clients</li> 
  <li>Participate in peer-reviews of solution designs and related artifacts</li> 
  <li>Package and support deployment of releases</li> 
  <li>Analyze and resolve technical and application problems</li> 
  <li>Adhere to high-quality development principles while delivering solutions on-time and on-budget</li> 
  <li>Provide third-level support to business users</li> 
  <li>Participate in business operations as necessary to meet business goals</li> 
 </ul> 
 <p><b>REQUIREMENTS:</b></p> 
 <ul>
  <li>Bachelor&#x2019;s degree in Information Systems, Computer Science, or Information Technology (a 4-year bachelor&#x2019;s degree is acceptable) <b>- OR -</b> expertise in above subject matters matched by experience</li> 
  <li>7+ years&#x2019; experience in MSSQL data engineering and development</li> 
  <li>Excellent understanding of T-SQL, stored procedures, indexes, stats, functions, and views</li> 
  <li>Expert understanding of relational and warehousing database design and querying concepts</li> 
  <li>Experience with SSIS, data integration and ETL/ELT procedures</li> 
  <li>Exposure to agile development methodology</li> 
  <li>Understanding of version control concepts</li> 
  <li>Strong desire to learn</li> 
  <li>Strong desire to be world-class data and SQL troubleshooter</li> 
 </ul> 
 <p><b>BENEFICIAL:</b></p> 
 <ul>
  <li>Experience with Snowflake cloud data warehouse</li> 
  <li>Experience with Matillion (Snowflake/BigQuery)</li> 
  <li>Experience with Google Cloud Platform technology stack: BigQuery, Data Flow, Data Fusion</li> 
  <li>Experience with BI tools like Sigma, Tableau or others</li> 
  <li>Experience with NoSQL databases</li> 
 </ul> 
 <p><b>COMPENSATION </b><br> This is a full-time, salaried, remote position headquartered in Herndon, VA. Eastern or central time zones preferred. Competitive compensation commensurate with experience. Participation in company benefit offerings include paid time off, medical, dental, vision, 401(k)/matching, wellness, and more.</p> 
 <p><b>NEXT STEPS </b><br> If you meet the requirements, and are interested in applying for this role, <b>please complete the online application</b>, be sure to include a current resume, contact information, and salary requirements. NO PHONE CALLS PLEASE.<br> </p>
 <p></p>
 <p><b>ABOUT TEAM VELOCITY </b><br> Team Velocity is a SaaS technology provider serving the automotive industry. We provide an omni-channel marketing automation platform and retailing solutions to OEMs and dealerships nationwide. We are revolutionizing the automotive industry with cutting-edge technology to help dealers sell and service more cars. Made by dealers for dealers, Team Velocity&#x2019;s proprietary technology platform Apollo&#xae; analyzes consumer behavior to predict who will buy, what they will buy, and when they are ready to service. Apollo automates the entire communication process by delivering hyper-personalized campaigns across every touchpoint, maximizing ROI, and lifetime revenue. <br> <br> Our vision is to serve our clients with a single technology platform that empowers them to execute intelligent marketing across every online and offline channel. We aim to deliver a frictionless consumer experience, from the initial engagement to final transaction. <br> <br> Our team members are hard-working and driven to achieve success for our clients and our unique culture promotes creativity, camaraderie, and success.</p>
</div>","https://teamvelocitymarketing.hrmdirect.com/employment/view.php?req=2800220&jbsrc=1014","a777e3c8aa805996",,"Full-time",,"Herndon, VA 20171","Senior Data Engineer","2 days ago","2023-10-21T13:06:14.485Z","3.1","12",,"2023-10-23T13:06:14.487Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=a777e3c8aa805996&from=jasx&tk=1hdea8ee5ipb2801&vjs=3"
"FreeWheel","Comcast brings together the best in media and technology. We drive innovation to create the world's best entertainment and online experiences. As a Fortune 50 leader, we set the pace in a variety of innovative and fascinating businesses and create career opportunities across a wide range of locations and disciplines. We are at the forefront of change and move at an amazing pace, thanks to our remarkable people, who bring cutting-edge products and services to life for millions of customers every day. If you share in our passion for teamwork, our vision to revolutionize industries and our goal to lead the future in media and technology, we want you to fast-forward your career at Comcast.
  Job Summary
  Job Description
  DUTIES: Provide technical leadership to a team responsible for developing software components for FreeWheel’s advertising platform products; build data platforms, Video Integration products, Linear Integration products, and new web frontend frameworks; develop software using Java, Python, Scala, and Go programming languages, which are run on big data platforms including Apache Hadoop, Spark, and Snowflake on AWS cloud platform; design, develop, test, and maintain software that extracts, transforms, and loads large volumes of data; create dashboards and monitors on Datadog to ensure 24x7 availability of critical software deployments; manage data held in relational database management systems (RDBMS) using SQL; develop and deploy complex SQL queries to validate impressions from set top boxes on a massive scale; write scripts for CI/CD to enable software artifacts to be built and deployed on AWS and Databricks, using Jenkins or similar tools; debug functional and performance issues on software modules running on Databricks and Spark; analyze product specifications, write technical specs, create monitoring dashboards, develop test suites, design workflows, and setup database schemas and tables; interface with global engineering, operations, services, and business operations teams to execute proof of concepts and incorporate new requirements; improve system performance and ensure availability and scalability of services; and guide and mentor junior-level engineers. Position is eligible for 100% remote work.
 
  REQUIREMENTS: Bachelor’s degree, or foreign equivalent, in Computer Science, Engineering, or related technical field, and five (5) years of experience developing software using Python; managing data held in relational database management systems (RDBMS) using SQL; and developing data architecture; of which one (1) year includes developing software using Spark; and working with cloud technologies, including AWS.
 
  Disclaimer:
 
 
   This information has been designed to indicate the general nature and level of work performed by employees in this role. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications.
 
 
  Comcast is proud to be an equal opportunity workplace. We will consider all qualified applicants for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran status, genetic information, or any other basis protected by applicable law.
  Base pay is one part of the Total Rewards that Comcast provides to compensate and recognize employees for their work. Most sales positions are eligible for a Commission under the terms of an applicable plan, while most non-sales positions are eligible for a Bonus. Additionally, Comcast provides best-in-class Benefits. We believe that benefits should connect you to the support you need when it matters most, and should help you care for those who matter most. That’s why we provide an array of options, expert guidance and always-on tools, that are personalized to meet the needs of your reality – to help support you physically, financially and emotionally through the big milestones and in your everyday life. Please visit the compensation and benefits summary on our careers site for more details.","<div>
 Comcast brings together the best in media and technology. We drive innovation to create the world&apos;s best entertainment and online experiences. As a Fortune 50 leader, we set the pace in a variety of innovative and fascinating businesses and create career opportunities across a wide range of locations and disciplines. We are at the forefront of change and move at an amazing pace, thanks to our remarkable people, who bring cutting-edge products and services to life for millions of customers every day. If you share in our passion for teamwork, our vision to revolutionize industries and our goal to lead the future in media and technology, we want you to fast-forward your career at Comcast.
 <p><b><br> Job Summary</b></p>
 <p><b> Job Description</b></p>
 <p> DUTIES: Provide technical leadership to a team responsible for developing software components for FreeWheel&#x2019;s advertising platform products; build data platforms, Video Integration products, Linear Integration products, and new web frontend frameworks; develop software using Java, Python, Scala, and Go programming languages, which are run on big data platforms including Apache Hadoop, Spark, and Snowflake on AWS cloud platform; design, develop, test, and maintain software that extracts, transforms, and loads large volumes of data; create dashboards and monitors on Datadog to ensure 24x7 availability of critical software deployments; manage data held in relational database management systems (RDBMS) using SQL; develop and deploy complex SQL queries to validate impressions from set top boxes on a massive scale; write scripts for CI/CD to enable software artifacts to be built and deployed on AWS and Databricks, using Jenkins or similar tools; debug functional and performance issues on software modules running on Databricks and Spark; analyze product specifications, write technical specs, create monitoring dashboards, develop test suites, design workflows, and setup database schemas and tables; interface with global engineering, operations, services, and business operations teams to execute proof of concepts and incorporate new requirements; improve system performance and ensure availability and scalability of services; and guide and mentor junior-level engineers. Position is eligible for 100% remote work.</p>
 <p></p>
 <p> REQUIREMENTS: Bachelor&#x2019;s degree, or foreign equivalent, in Computer Science, Engineering, or related technical field, and five (5) years of experience developing software using Python; managing data held in relational database management systems (RDBMS) using SQL; and developing data architecture; of which one (1) year includes developing software using Spark; and working with cloud technologies, including AWS.</p>
 <p></p>
 <p><b> Disclaimer:</b></p>
 <p></p>
 <ul>
  <li> This information has been designed to indicate the general nature and level of work performed by employees in this role. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications.</li>
 </ul>
 <p></p>
 <p> Comcast is proud to be an equal opportunity workplace. We will consider all qualified applicants for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran status, genetic information, or any other basis protected by applicable law.</p>
 <p><br> Base pay is one part of the Total Rewards that Comcast provides to compensate and recognize employees for their work. Most sales positions are eligible for a Commission under the terms of an applicable plan, while most non-sales positions are eligible for a Bonus. Additionally, Comcast provides best-in-class Benefits. We believe that benefits should connect you to the support you need when it matters most, and should help you care for those who matter most. That&#x2019;s why we provide an array of options, expert guidance and always-on tools, that are personalized to meet the needs of your reality &#x2013; to help support you physically, financially and emotionally through the big milestones and in your everyday life. Please visit the compensation and benefits summary on our careers site for more details.</p>
</div>","https://jobs.comcast.com/jobs/description?external_or_internal=External&job_id=R374527&source=ind_orga_at&jobPipeline=Indeed","8d363231c84d5af6",,,,"Philadelphia, PA 19103","Sr. Software Engineer (Data) [Multiple Openings]-9199","2 days ago","2023-10-21T13:06:15.029Z","3.8","4",,"2023-10-23T13:06:15.030Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=8d363231c84d5af6&from=jasx&tk=1hdea8ee5ipb2801&vjs=3"
"Gridiron IT","Seeking a Junior Data Engineer on a remote basis. Secret clearance is required. 
Overview: We are looking to immediately fill a Junior Data Engineer on our team. The ADE is one of the major pillars of MyNavy HR Transformation and serves as an enterprise-wide centralized repository that provides seamless and secure data access. This pilot’s objective is to support defining a comprehensive future-state ADE data model by informing the total number of unique data elements and to assess the ability to accelerate the data integration process using new data tools available following the Authority to Operate (ATO).
Minimum Qualifications:

 2+ years of experience with scalable ETL workflows/development, extract, cleanse, and process disparate data sources
 Secret Clearance is required.
 HS Diploma required (Bachelor's preferred)
 Experience with cleaning and transforming data utilizing Python and/or SQL, specifically complex SQL queries
 Familiarity with acquiring data from disparate data sources using APIs and SQL
 Ability to develop scripts and programs for converting various types of data into usable formats and support project team to scale, monitor, and operate data platforms

Job Type: Full-time
Pay: $48.00 - $52.00 per hour
Benefits:

 401(k)
 401(k) matching
 Dental insurance
 Health insurance
 Vision insurance

Schedule:

 8 hour shift

Work Location: Remote","<p><b>Seeking a Junior Data Engineer on a remote basis. </b><br><b>Secret clearance is required. </b></p>
<p><b>Overview:</b> We are looking to immediately fill a Junior Data Engineer on our team. The ADE is one of the major pillars of MyNavy HR Transformation and serves as an enterprise-wide centralized repository that provides seamless and secure data access. This pilot&#x2019;s objective is to support defining a comprehensive future-state ADE data model by informing the total number of unique data elements and to assess the ability to accelerate the data integration process using new data tools available following the Authority to Operate (ATO).</p>
<p><b>Minimum Qualifications:</b></p>
<ul>
 <li>2+ years of experience with scalable ETL workflows/development, extract, cleanse, and process disparate data sources</li>
 <li>Secret Clearance is required.</li>
 <li>HS Diploma required (Bachelor&apos;s preferred)</li>
 <li>Experience with cleaning and transforming data utilizing Python and/or SQL, specifically complex SQL queries</li>
 <li>Familiarity with acquiring data from disparate data sources using APIs and SQL</li>
 <li>Ability to develop scripts and programs for converting various types of data into usable formats and support project team to scale, monitor, and operate data platforms</li>
</ul>
<p>Job Type: Full-time</p>
<p>Pay: &#x24;48.00 - &#x24;52.00 per hour</p>
<p>Benefits:</p>
<ul>
 <li>401(k)</li>
 <li>401(k) matching</li>
 <li>Dental insurance</li>
 <li>Health insurance</li>
 <li>Vision insurance</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
</ul>
<p>Work Location: Remote</p>",,"45a231ffe43c577a",,"Full-time",,"Remote","Jr Data Engineer - Secret Cleared","3 days ago","2023-10-20T13:06:25.094Z","4.2","17","$48 - $52 an hour","2023-10-23T13:06:25.117Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=45a231ffe43c577a&from=jasx&tk=1hdea9902imai801&vjs=3"
"Cognizant Technology Solutions","We are Cognizant Artificial Intelligence 
  Digital technologies, including analytics and AI, give companies a once-in-a-generation opportunity to perform orders of magnitude better than ever before. But clients need new business models built from analyzing customers and business operations at every angle to really understand them. 
  With the power to apply artificial intelligence and data science to business decisions via enterprise data management solutions, we help leading companies prototype, refine, validate and scale the most desirable products and delivery models to enterprise scale within weeks
  
  
  Role and Responsibilities: 
  
  5+ years of industry experience in software development data engineering or a related field with a solid track record of building services for manipulating processing datasets 
  Hands-on experience and advanced knowledge of AWS DataOps (i.e. IAM Lambda Step Functions EMR/Glue and DynamoDB) 
  Hands-on experience and advanced knowledge of SQL/Non-relational Data Modeling 
  Experience working with data streaming technologies (Kafka Spark Streaming etc.) 
 
 
  Designing and implementing complex ingestion and processing pipelines through orchestration 
  Design and implement API interfaces for engineering teams to interact with ingestion/processing pipelines 
  Design implement and support scalable multi-tenant service and data infrastructure solutions to integrate with multi heterogeneous data sources aggregate and retrieve data in a fast and secure mode curate data that can be used in reporting analysis machine learning models and ad-hoc data requests 
  Interface with other engineering and ML teams to extract transform and load data from a wide variety of data sources 
  Work with business product owners to understand gather and analyze their processing and extraction needs to solve problems
 
  
  
  Salary and Other Compensation 
  The annual salary for this position is between USD ($110kp/a – $120kp/a) depending on experience and other qualifications of the successful candidate. 
  This position is also eligible for Cognizant’s discretionary annual incentive program, based on performance and subject to the terms of Cognizant’s applicable plans. 
  Benefits: Cognizant offers the following benefits for this position, subject to applicable eligibility requirements: 
  
  Medical/Dental/Vision/Life Insurance 
  Paid holidays plus Paid Time Off 
  401(k) plan and contributions 
  Long-term/Short-term Disability 
  Paid Parental Leave 
  Employee Stock Purchase Plan 
  
 Disclaimer: The salary, other compensation, and benefits information is accurate as of the date of this posting. Cognizant reserves the right to modify this information at any time, subject to applicable law.
  
  
  #LI-JL1 
  #CB 
  #IND123
 
  Employee Status : Full Time Employee
  Shift : Day Job
  Travel : No
  Job Posting : Oct 17 2023
 
 
   About Cognizant
  Cognizant (Nasdaq-100: CTSH) is one of the world's leading professional services companies, transforming clients' business, operating and technology models for the digital era. Our unique industry-based, consultative approach helps clients envision, build and run more innovative and efficient businesses. Headquartered in the U.S., Cognizant is ranked 185 on the Fortune 500 and is consistently listed among the most admired companies in the world. Learn how Cognizant helps clients lead with digital at www.cognizant.com or follow us @USJobsCognizant.
 
  Applicants may be required to attend interviews in person or by video conference. In addition, candidates may be required to present their current state or government issued ID during each interview.
 
  Cognizant is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected Veteran status, age, or any other characteristic protected by law.
  If you have a disability that requires a reasonable accommodation to search for a job opening or submit an application, please email CareersNA2@cognizant.com with your request and contact information.","<div>
 <p><b>We are Cognizant Artificial Intelligence</b></p> 
 <p> Digital technologies, including analytics and AI, give companies a once-in-a-generation opportunity to perform orders of magnitude better than ever before. But clients need new business models built from analyzing customers and business operations at every angle to really understand them.</p> 
 <p> With the power to apply artificial intelligence and data science to business decisions via enterprise data management solutions, we help leading companies prototype, refine, validate and scale the most desirable products and delivery models to enterprise scale within weeks</p>
 <br> 
 <p></p> 
 <p><b> Role and Responsibilities:</b></p> 
 <ul> 
  <li>5+ years of industry experience in software development data engineering or a related field with a solid track record of building services for manipulating processing datasets</li> 
  <li>Hands-on experience and advanced knowledge of AWS DataOps (i.e. IAM Lambda Step Functions EMR/Glue and DynamoDB)</li> 
  <li>Hands-on experience and advanced knowledge of SQL/Non-relational Data Modeling</li> 
  <li>Experience working with data streaming technologies (Kafka Spark Streaming etc.)</li> 
 </ul>
 <ul>
  <li>Designing and implementing complex ingestion and processing pipelines through orchestration</li> 
  <li>Design and implement API interfaces for engineering teams to interact with ingestion/processing pipelines</li> 
  <li>Design implement and support scalable multi-tenant service and data infrastructure solutions to integrate with multi heterogeneous data sources aggregate and retrieve data in a fast and secure mode curate data that can be used in reporting analysis machine learning models and ad-hoc data requests</li> 
  <li>Interface with other engineering and ML teams to extract transform and load data from a wide variety of data sources</li> 
  <li>Work with business product owners to understand gather and analyze their processing and extraction needs to solve problems</li>
 </ul>
 <br> 
 <p></p> 
 <p><b> Salary and Other Compensation</b></p> 
 <p> The annual salary for this position is between USD (&#x24;110kp/a &#x2013; &#x24;120kp/a) depending on experience and other qualifications of the successful candidate.</p> 
 <p> This position is also eligible for Cognizant&#x2019;s discretionary annual incentive program, based on performance and subject to the terms of Cognizant&#x2019;s applicable plans.</p> 
 <p> Benefits: Cognizant offers the following benefits for this position, subject to applicable eligibility requirements:</p> 
 <ul> 
  <li>Medical/Dental/Vision/Life Insurance</li> 
  <li>Paid holidays plus Paid Time Off</li> 
  <li>401(k) plan and contributions</li> 
  <li>Long-term/Short-term Disability</li> 
  <li>Paid Parental Leave</li> 
  <li>Employee Stock Purchase Plan</li> 
 </ul> 
 <p>Disclaimer: The salary, other compensation, and benefits information is accurate as of the date of this posting. Cognizant reserves the right to modify this information at any time, subject to applicable law.</p>
 <br> 
 <p></p> 
 <p> #LI-JL1</p> 
 <p> #CB</p> 
 <p> #IND123</p>
 <p></p>
 <p><b><br> Employee Status : </b>Full Time Employee</p>
 <p><b> Shift : </b>Day Job</p>
 <p><b> Travel : </b>No</p>
 <p><b> Job Posting : </b>Oct 17 2023</p>
 <p></p>
 <div>
  <b> About Cognizant</b>
 </div> Cognizant (Nasdaq-100: CTSH) is one of the world&apos;s leading professional services companies, transforming clients&apos; business, operating and technology models for the digital era. Our unique industry-based, consultative approach helps clients envision, build and run more innovative and efficient businesses. Headquartered in the U.S., Cognizant is ranked 185 on the Fortune 500 and is consistently listed among the most admired companies in the world. Learn how Cognizant helps clients lead with digital at www.cognizant.com or follow us @USJobsCognizant.
 <p></p>
 <p> Applicants may be required to attend interviews in person or by video conference. In addition, candidates may be required to present their current state or government issued ID during each interview.</p>
 <p></p>
 <p> Cognizant is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected Veteran status, age, or any other characteristic protected by law.</p>
 <p> If you have a disability that requires a reasonable accommodation to search for a job opening or submit an application, please email CareersNA2@cognizant.com with your request and contact information.</p>
</div>
<p></p>","https://click.appcast.io/track/hra6v3l-org?cs=hqw&jg=6mnw&ittk=T9XYNDK2D2","8c2d8bd464c9e1f8",,"Full-time",,"Chicago, IL 60290","Sr. AWS Data Engineer (Remote)","5 days ago","2023-10-18T13:06:21.522Z","3.9","15978","$110,000 - $120,000 a year","2023-10-23T13:06:21.524Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=8c2d8bd464c9e1f8&from=jasx&tk=1hdea988rk6dn800&vjs=3"
"OpenEarth Foundation","Lead Data Engineer:
Building Climate Solutions for Cities
Link to application: https://noteforms.com/forms/openearth-job-application-ctwllg
Have you created a successful career in tech and are ready to do something good with your skills and experience? If yes, then join us in our Earthshot mission to build open source digital systems and solutions to battle environmental threats.
Open Earth Foundation is a California-based research and deployment non-profit developing open innovative technology to increase planetary resilience and avoid a catastrophic climate crisis.
We are building open infrastructure in support of the UNFCCC Paris Agreement, solutions in climate finance, biodiversity tracking, community empowerment and other critical problems and solutions.
We are a diverse international team, working remotely with a network of collaborators and partners globally, from the United Nations to climate tech startups.
We have funding and a team of experts focused on Earth systems and digital innovation.
Your mission, should you choose to accept it:
As a lead data engineer, you will leverage your data and engineering expertise and analytical skills to build open digital infrastructure, with a particular focus on our newly launched CityCatalyst project.
As a Lead Data Engineer you will provide leadership and hands-on data management in our climate accounting technology programs. Your work will cover climate data pipelines, infrastructure design and implementation, working on greenhouse gas data inventories from satellite imagery and diverse data sources, being part of a software production team and even interfacing with new AI and LLM tools for data harmonization.
You will be part of the technical team and work directly with the Director of Open Technology. You will also work with a variety of team members in the organization from the Product Team and Community Manager.
The position is remote and international candidates are welcome, but prefer those willing to work with time zones compatible with PST. We're working on the planet's problems and we need the planet's best people to fix them.
The following requirements describe our ideal candidate. If you don't meet some of the requirements, you're encouraged to apply anyway. Let us know if there is something missing and how we can work together to make it up.
Essential Functions and Specific Duties:

 Design, architect, build and maintain data pipeline systems
 Write code for importing and updating large datasets to relational database and search indexes
 Define and maintain database schemas and data file formats
 Collaborate with web developers on optimizing database schemas for APIs and Web applications
 Collaborate with a team of software engineering peers
 Mentor and guide more junior data engineering staff
 Define and maintain data management processes for the organization
 Work with product managers to develop schedules, estimate tasks, and define success criteria
 Collaborate with team members from other disciplines such as web development, design, product management, and devops
 Coordinate with Open Source contributors
 Coordinate with open standards community to define interoperability standards
 Actively participate in team building and culture development activities at Open Earth Foundation
 Other duties as assigned

Required skills:

 Python programming focused on big data management
 PostgreSQL or other relational database
 Docker
 Kubernetes
 Git

Optional skills that will make a candidate stand out:

 Generative AI and large language model (LLM) APIs and data applications
 GIS tools such as ESRI
 Amazon Web Services
 ElasticSearch
 Data pipeline tools, e.g. Pachyderm
 Experience with 100Gb or larger data sets
 Climate action data such as emissions, targets, and action plans
 Physical (lat, lon, alt) and political (city, state, country) geographical data
 Remote-sensing and satellite data
 RESTful Web APIs
 Engineering leadership
 Open Source project maintainership
 Machine learning, especially for anomaly detection, pattern recognition, clustering, time series analysis

Qualifications:

 Bachelor’s degree in computer science, electrical engineering, or equivalent technical or scientific degree, or equivalent on-the-job experience
 5 years of experience in software development for data systems
 3 shipped projects

Interpersonal skills:

 Clear communicator with good verbal and written skills in English (additional languages a plus)
 Creative, flexible and efficient with a focus on details
 Capacity to work with team members and partners at all levels and across time zones in a highly collaborative and often remote environment.
 Ability to embrace new challenges, take ownership and initiative as a key team player.

Compensation and benefits

 This position is full-time with compensation of $60,000-$105,000 /year, dependent on experience and location
 Open Earth offers unlimited paid time off, paid holidays and paid sick leave
 You will work remotely within a dynamic and international environment
 We celebrate our achievements during our annual team retreat

OEF is an Equal Employment Opportunity Employer. We support diversity, equity and inclusion in teams, and believe people should align their work with their purpose. Join us if you love Earth.
Please apply by submitting your resume AND a cover letter, it is your opportunity to highlight why you feel you would be a great addition to the team.
We look forward to hearing from you!
Open Earth Foundation seeks diverse applicants from underrepresented communities. If you would like to pursue this job opportunity but don’t believe you meet all the requirements, please apply and note what’s missing in your cover letter.Lead Data Engineer:
Building Climate Solutions for Cities
Link to application: https://noteforms.com/forms/openearth-job-application-ctwllg
Have you created a successful career in tech and are ready to do something good with your skills and experience? If yes, then join us in our Earthshot mission to build open source digital systems and solutions to battle environmental threats.
Open Earth Foundation is a California-based research and deployment non-profit developing open innovative technology to increase planetary resilience and avoid a catastrophic climate crisis.
We are building open infrastructure in support of the UNFCCC Paris Agreement, solutions in climate finance, biodiversity tracking, community empowerment and other critical problems and solutions.
We are a diverse international team, working remotely with a network of collaborators and partners globally, from the United Nations to climate tech startups.
We have funding and a team of experts focused on Earth systems and digital innovation.
Your mission, should you choose to accept it:
As a lead data engineer, you will leverage your data and engineering expertise and analytical skills to build open digital infrastructure, with a particular focus on our newly launched CityCatalyst project.
As a Lead Data Engineer you will provide leadership and hands-on data management in our climate accounting technology programs. Your work will cover climate data pipelines, infrastructure design and implementation, working on greenhouse gas data inventories from satellite imagery and diverse data sources, being part of a software production team and even interfacing with new AI and LLM tools for data harmonization.
You will be part of the technical team and work directly with the Director of Open Technology. You will also work with a variety of team members in the organization from the Product Team and Community Manager.
The position is remote and international candidates are welcome, but prefer those willing to work with time zones compatible with PST. We're working on the planet's problems and we need the planet's best people to fix them.
The following requirements describe our ideal candidate. If you don't meet some of the requirements, you're encouraged to apply anyway. Let us know if there is something missing and how we can work together to make it up.
Essential Functions and Specific Duties:

 Design, architect, build and maintain data pipeline systems
 Write code for importing and updating large datasets to relational database and search indexes
 Define and maintain database schemas and data file formats
 Collaborate with web developers on optimizing database schemas for APIs and Web applications
 Collaborate with a team of software engineering peers
 Mentor and guide more junior data engineering staff
 Define and maintain data management processes for the organization
 Work with product managers to develop schedules, estimate tasks, and define success criteria
 Collaborate with team members from other disciplines such as web development, design, product management, and devops
 Coordinate with Open Source contributors
 Coordinate with open standards community to define interoperability standards
 Actively participate in team building and culture development activities at Open Earth Foundation
 Other duties as assigned

Required skills:

 Python programming focused on big data management
 PostgreSQL or other relational database
 Docker
 Kubernetes
 Git

Optional skills that will make a candidate stand out:

 Generative AI and large language model (LLM) APIs and data applications
 GIS tools such as ESRI
 Amazon Web Services
 ElasticSearch
 Data pipeline tools, e.g. Pachyderm
 Experience with 100Gb or larger data sets
 Climate action data such as emissions, targets, and action plans
 Physical (lat, lon, alt) and political (city, state, country) geographical data
 Remote-sensing and satellite data
 RESTful Web APIs
 Engineering leadership
 Open Source project maintainership
 Machine learning, especially for anomaly detection, pattern recognition, clustering, time series analysis

Qualifications:

 Bachelor’s degree in computer science, electrical engineering, or equivalent technical or scientific degree, or equivalent on-the-job experience
 5 years of experience in software development for data systems
 3 shipped projects

Interpersonal skills:

 Clear communicator with good verbal and written skills in English (additional languages a plus)
 Creative, flexible and efficient with a focus on details
 Capacity to work with team members and partners at all levels and across time zones in a highly collaborative and often remote environment.
 Ability to embrace new challenges, take ownership and initiative as a key team player.

Compensation and benefits

 This position is full-time with compensation of $60,000-$105,000 /year, dependent on experience and location
 Open Earth offers unlimited paid time off, paid holidays and paid sick leave
 You will work remotely within a dynamic and international environment
 We celebrate our achievements during our annual team retreat

OEF is an Equal Employment Opportunity Employer. We support diversity, equity and inclusion in teams, and believe people should align their work with their purpose. Join us if you love Earth.
Please apply by submitting your resume AND a cover letter, it is your opportunity to highlight why you feel you would be a great addition to the team.
We look forward to hearing from you!
Open Earth Foundation seeks diverse applicants from underrepresented communities. If you would like to pursue this job opportunity but don’t believe you meet all the requirements, please apply and note what’s missing in your cover letter.
Job Type: Full-time
Pay: $60,000.00 - $105,000.00 per year
Benefits:

 401(k)
 Dental insurance
 Flexible schedule
 Health insurance
 Paid time off
 Vision insurance

Compensation package:

 Bonus opportunities
 Yearly pay

Experience level:

 3 years
 4 years
 5 years
 6 years
 7 years
 8 years

Schedule:

 8 hour shift
 Monday to Friday

Experience:

 Informatica: 1 year (Preferred)
 SQL: 1 year (Preferred)
 Data warehouse: 1 year (Preferred)

Work Location: Remote","<p><b>Lead Data Engineer:</b></p>
<p><b>Building Climate Solutions for Cities</b></p>
<p>Link to application: https://noteforms.com/forms/openearth-job-application-ctwllg</p>
<p><i><b>Have you created a successful career in tech and are ready to do something good with your skills and experience? If yes, then join us in our Earthshot mission to build open source digital systems and solutions to battle environmental threats.</b></i></p>
<p><i>Open Earth Foundation is a California-based research and deployment non-profit developing open innovative technology to increase planetary resilience and avoid a catastrophic climate crisis.</i></p>
<p><i>We are building open infrastructure in support of the UNFCCC Paris Agreement, solutions in climate finance, biodiversity tracking, community empowerment and other critical problems and solutions.</i></p>
<p><i>We are a diverse international team, working remotely with a network of collaborators and partners globally, from the United Nations to climate tech startups.</i></p>
<p><i>We have funding and a team of experts focused on Earth systems and digital innovation.</i></p>
<p><i><b>Your mission, should you choose to accept it:</b></i></p>
<p>As a <b>lead data engineer</b>, you will leverage your data and engineering expertise and analytical skills to build open digital infrastructure, with a particular focus on our newly launched CityCatalyst project.</p>
<p>As a Lead Data Engineer you will provide leadership and hands-on data management in our climate accounting technology programs. Your work will cover climate data pipelines, infrastructure design and implementation, working on greenhouse gas data inventories from satellite imagery and diverse data sources, being part of a software production team and even interfacing with new AI and LLM tools for data harmonization.</p>
<p>You will be part of the technical team and work directly with the Director of Open Technology. You will also work with a variety of team members in the organization from the Product Team and Community Manager.</p>
<p>The position is remote and international candidates are welcome, but prefer those willing to work with time zones compatible with PST. We&apos;re working on the planet&apos;s problems and we need the planet&apos;s best people to fix them.</p>
<p><i>The following requirements describe our ideal candidate. If you don&apos;t meet some of the requirements, you&apos;re encouraged to apply anyway. Let us know if there is something missing and how we can work together to make it up.</i></p>
<p><b>Essential Functions and Specific Duties:</b></p>
<ul>
 <li>Design, architect, build and maintain data pipeline systems</li>
 <li>Write code for importing and updating large datasets to relational database and search indexes</li>
 <li>Define and maintain database schemas and data file formats</li>
 <li>Collaborate with web developers on optimizing database schemas for APIs and Web applications</li>
 <li>Collaborate with a team of software engineering peers</li>
 <li>Mentor and guide more junior data engineering staff</li>
 <li>Define and maintain data management processes for the organization</li>
 <li>Work with product managers to develop schedules, estimate tasks, and define success criteria</li>
 <li>Collaborate with team members from other disciplines such as web development, design, product management, and devops</li>
 <li>Coordinate with Open Source contributors</li>
 <li>Coordinate with open standards community to define interoperability standards</li>
 <li>Actively participate in team building and culture development activities at Open Earth Foundation</li>
 <li>Other duties as assigned</li>
</ul>
<p><b>Required skills:</b></p>
<ul>
 <li>Python programming focused on big data management</li>
 <li>PostgreSQL or other relational database</li>
 <li>Docker</li>
 <li>Kubernetes</li>
 <li>Git</li>
</ul>
<p><b>Optional skills that will make a candidate stand out:</b></p>
<ul>
 <li>Generative AI and large language model (LLM) APIs and data applications</li>
 <li>GIS tools such as ESRI</li>
 <li>Amazon Web Services</li>
 <li>ElasticSearch</li>
 <li>Data pipeline tools, e.g. Pachyderm</li>
 <li>Experience with 100Gb or larger data sets</li>
 <li>Climate action data such as emissions, targets, and action plans</li>
 <li>Physical (lat, lon, alt) and political (city, state, country) geographical data</li>
 <li>Remote-sensing and satellite data</li>
 <li>RESTful Web APIs</li>
 <li>Engineering leadership</li>
 <li>Open Source project maintainership</li>
 <li>Machine learning, especially for anomaly detection, pattern recognition, clustering, time series analysis</li>
</ul>
<p><b>Qualifications:</b></p>
<ul>
 <li>Bachelor&#x2019;s degree in computer science, electrical engineering, or equivalent technical or scientific degree, or equivalent on-the-job experience</li>
 <li>5 years of experience in software development for data systems</li>
 <li>3 shipped projects</li>
</ul>
<p><b>Interpersonal skills:</b></p>
<ul>
 <li>Clear communicator with good verbal and written skills in English (additional languages a plus)</li>
 <li>Creative, flexible and efficient with a focus on details</li>
 <li>Capacity to work with team members and partners at all levels and across time zones in a highly collaborative and often remote environment.</li>
 <li>Ability to embrace new challenges, take ownership and initiative as a key team player.</li>
</ul>
<p><i><b>Compensation and benefits</b></i></p>
<ul>
 <li>This position is full-time with compensation of &#x24;60,000-&#x24;105,000 /year, dependent on experience and location</li>
 <li>Open Earth offers unlimited paid time off, paid holidays and paid sick leave</li>
 <li>You will work remotely within a dynamic and international environment</li>
 <li>We celebrate our achievements during our annual team retreat</li>
</ul>
<p><i>OEF is an Equal Employment Opportunity Employer. We support diversity, equity and inclusion in teams, and believe people should align their work with their purpose. Join us if you love Earth.</i></p>
<p><i>Please apply by submitting your resume AND a cover letter, it is your opportunity to highlight why you feel you would be a great addition to the team.</i></p>
<p><i>We look forward to hearing from you!</i></p>
<p><i>Open Earth Foundation seeks diverse applicants from underrepresented communities. If you would like to pursue this job opportunity but don&#x2019;t believe you meet all the requirements, please apply and note what&#x2019;s missing in your cover letter.</i><b>Lead Data Engineer:</b></p>
<p><b>Building Climate Solutions for Cities</b></p>
<p>Link to application: https://noteforms.com/forms/openearth-job-application-ctwllg</p>
<p><i><b>Have you created a successful career in tech and are ready to do something good with your skills and experience? If yes, then join us in our Earthshot mission to build open source digital systems and solutions to battle environmental threats.</b></i></p>
<p><i>Open Earth Foundation is a California-based research and deployment non-profit developing open innovative technology to increase planetary resilience and avoid a catastrophic climate crisis.</i></p>
<p><i>We are building open infrastructure in support of the UNFCCC Paris Agreement, solutions in climate finance, biodiversity tracking, community empowerment and other critical problems and solutions.</i></p>
<p><i>We are a diverse international team, working remotely with a network of collaborators and partners globally, from the United Nations to climate tech startups.</i></p>
<p><i>We have funding and a team of experts focused on Earth systems and digital innovation.</i></p>
<p><i><b>Your mission, should you choose to accept it:</b></i></p>
<p>As a <b>lead data engineer</b>, you will leverage your data and engineering expertise and analytical skills to build open digital infrastructure, with a particular focus on our newly launched CityCatalyst project.</p>
<p>As a Lead Data Engineer you will provide leadership and hands-on data management in our climate accounting technology programs. Your work will cover climate data pipelines, infrastructure design and implementation, working on greenhouse gas data inventories from satellite imagery and diverse data sources, being part of a software production team and even interfacing with new AI and LLM tools for data harmonization.</p>
<p>You will be part of the technical team and work directly with the Director of Open Technology. You will also work with a variety of team members in the organization from the Product Team and Community Manager.</p>
<p>The position is remote and international candidates are welcome, but prefer those willing to work with time zones compatible with PST. We&apos;re working on the planet&apos;s problems and we need the planet&apos;s best people to fix them.</p>
<p><i>The following requirements describe our ideal candidate. If you don&apos;t meet some of the requirements, you&apos;re encouraged to apply anyway. Let us know if there is something missing and how we can work together to make it up.</i></p>
<p><b>Essential Functions and Specific Duties:</b></p>
<ul>
 <li>Design, architect, build and maintain data pipeline systems</li>
 <li>Write code for importing and updating large datasets to relational database and search indexes</li>
 <li>Define and maintain database schemas and data file formats</li>
 <li>Collaborate with web developers on optimizing database schemas for APIs and Web applications</li>
 <li>Collaborate with a team of software engineering peers</li>
 <li>Mentor and guide more junior data engineering staff</li>
 <li>Define and maintain data management processes for the organization</li>
 <li>Work with product managers to develop schedules, estimate tasks, and define success criteria</li>
 <li>Collaborate with team members from other disciplines such as web development, design, product management, and devops</li>
 <li>Coordinate with Open Source contributors</li>
 <li>Coordinate with open standards community to define interoperability standards</li>
 <li>Actively participate in team building and culture development activities at Open Earth Foundation</li>
 <li>Other duties as assigned</li>
</ul>
<p><b>Required skills:</b></p>
<ul>
 <li>Python programming focused on big data management</li>
 <li>PostgreSQL or other relational database</li>
 <li>Docker</li>
 <li>Kubernetes</li>
 <li>Git</li>
</ul>
<p><b>Optional skills that will make a candidate stand out:</b></p>
<ul>
 <li>Generative AI and large language model (LLM) APIs and data applications</li>
 <li>GIS tools such as ESRI</li>
 <li>Amazon Web Services</li>
 <li>ElasticSearch</li>
 <li>Data pipeline tools, e.g. Pachyderm</li>
 <li>Experience with 100Gb or larger data sets</li>
 <li>Climate action data such as emissions, targets, and action plans</li>
 <li>Physical (lat, lon, alt) and political (city, state, country) geographical data</li>
 <li>Remote-sensing and satellite data</li>
 <li>RESTful Web APIs</li>
 <li>Engineering leadership</li>
 <li>Open Source project maintainership</li>
 <li>Machine learning, especially for anomaly detection, pattern recognition, clustering, time series analysis</li>
</ul>
<p><b>Qualifications:</b></p>
<ul>
 <li>Bachelor&#x2019;s degree in computer science, electrical engineering, or equivalent technical or scientific degree, or equivalent on-the-job experience</li>
 <li>5 years of experience in software development for data systems</li>
 <li>3 shipped projects</li>
</ul>
<p><b>Interpersonal skills:</b></p>
<ul>
 <li>Clear communicator with good verbal and written skills in English (additional languages a plus)</li>
 <li>Creative, flexible and efficient with a focus on details</li>
 <li>Capacity to work with team members and partners at all levels and across time zones in a highly collaborative and often remote environment.</li>
 <li>Ability to embrace new challenges, take ownership and initiative as a key team player.</li>
</ul>
<p><i><b>Compensation and benefits</b></i></p>
<ul>
 <li>This position is full-time with compensation of &#x24;60,000-&#x24;105,000 /year, dependent on experience and location</li>
 <li>Open Earth offers unlimited paid time off, paid holidays and paid sick leave</li>
 <li>You will work remotely within a dynamic and international environment</li>
 <li>We celebrate our achievements during our annual team retreat</li>
</ul>
<p><i>OEF is an Equal Employment Opportunity Employer. We support diversity, equity and inclusion in teams, and believe people should align their work with their purpose. Join us if you love Earth.</i></p>
<p><i>Please apply by submitting your resume AND a cover letter, it is your opportunity to highlight why you feel you would be a great addition to the team.</i></p>
<p><i>We look forward to hearing from you!</i></p>
<p><i>Open Earth Foundation seeks diverse applicants from underrepresented communities. If you would like to pursue this job opportunity but don&#x2019;t believe you meet all the requirements, please apply and note what&#x2019;s missing in your cover letter.</i></p>
<p>Job Type: Full-time</p>
<p>Pay: &#x24;60,000.00 - &#x24;105,000.00 per year</p>
<p>Benefits:</p>
<ul>
 <li>401(k)</li>
 <li>Dental insurance</li>
 <li>Flexible schedule</li>
 <li>Health insurance</li>
 <li>Paid time off</li>
 <li>Vision insurance</li>
</ul>
<p>Compensation package:</p>
<ul>
 <li>Bonus opportunities</li>
 <li>Yearly pay</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>3 years</li>
 <li>4 years</li>
 <li>5 years</li>
 <li>6 years</li>
 <li>7 years</li>
 <li>8 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
 <li>Monday to Friday</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Informatica: 1 year (Preferred)</li>
 <li>SQL: 1 year (Preferred)</li>
 <li>Data warehouse: 1 year (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,"37b5edb36e885430",,"Full-time",,"Remote","Lead Data Engineer","5 days ago","2023-10-18T13:06:28.447Z",,,"$60,000 - $105,000 a year","2023-10-23T13:06:28.450Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=37b5edb36e885430&from=jasx&tk=1hdea988rk6dn800&vjs=3"
"TEKAPPS","Mid to Sr Data Engineer
Backend Java development at least 3+ yrs
Python development at least 1+ yrs
Experience with microservice design and development using Spring at least 2+ years
Must have worked with automation testing frameworks like Karate, Cucumber
Must have worked with testing libraries JUNIT, TestNG, Spock
MongoDB development and NoSQL design at least 2+ years
MariaDB or MySQL relational design at least 2+ years
Good understanding of webservices, cloud computing with AWS, Kubernetes
Understanding of cyber security aspects including data protection, encryption, anonymization
Familiarity of ML tools and libraries in a Python environment such as Pandas, Scikit
Some exposure with streaming data warehouse engines like Databricks, Snowflake or equivalent
Some exposure to building big data tools experience Kafka, Spark
Nice to have knowledge of Flink, JSON schema, NewRelic, DataDog, gradle, GITHub
Job Types: Contract, Full-time
Pay: $50.00 - $60.00 per hour
Experience level:

 9 years

Schedule:

 8 hour shift
 Day shift
 Monday to Friday

Application Question(s):

 Work Authorization / Visa type

Experience:

 Java Backend development: 9 years (Preferred)
 Python: 3 years (Preferred)
 MongoDB and NoSQL: 3 years (Preferred)
 MySQL: 3 years (Preferred)

Work Location: Remote","<p>Mid to Sr Data Engineer</p>
<p>Backend Java development at least 3+ yrs</p>
<p>Python development at least 1+ yrs</p>
<p>Experience with microservice design and development using Spring at least 2+ years</p>
<p>Must have worked with automation testing frameworks like Karate, Cucumber</p>
<p>Must have worked with testing libraries JUNIT, TestNG, Spock</p>
<p>MongoDB development and NoSQL design at least 2+ years</p>
<p>MariaDB or MySQL relational design at least 2+ years</p>
<p>Good understanding of webservices, cloud computing with AWS, Kubernetes</p>
<p>Understanding of cyber security aspects including data protection, encryption, anonymization</p>
<p>Familiarity of ML tools and libraries in a Python environment such as Pandas, Scikit</p>
<p>Some exposure with streaming data warehouse engines like Databricks, Snowflake or equivalent</p>
<p>Some exposure to building big data tools experience Kafka, Spark</p>
<p>Nice to have knowledge of Flink, JSON schema, NewRelic, DataDog, gradle, GITHub</p>
<p>Job Types: Contract, Full-time</p>
<p>Pay: &#x24;50.00 - &#x24;60.00 per hour</p>
<p>Experience level:</p>
<ul>
 <li>9 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
 <li>Day shift</li>
 <li>Monday to Friday</li>
</ul>
<p>Application Question(s):</p>
<ul>
 <li>Work Authorization / Visa type</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Java Backend development: 9 years (Preferred)</li>
 <li>Python: 3 years (Preferred)</li>
 <li>MongoDB and NoSQL: 3 years (Preferred)</li>
 <li>MySQL: 3 years (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,"4e37d548afe793c2",,"Full-time","Contract","Remote","Data Engineer with Java, Python, Mongodb and NoSQL experience","2 days ago","2023-10-21T13:06:29.504Z",,,"$50 - $60 an hour","2023-10-23T13:06:29.506Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=4e37d548afe793c2&from=jasx&tk=1hdea8ee5ipb2801&vjs=3"
"Analytica","ANALYTICA is seeking a Senior Data Engineer to support a federal government client in the DC metro area (Note - your work location is REMOTE). In this assignment, you will be a team member serving the client in advancing the customers use data, metadata, as well as explore new technologies to better meet those needs.  This is a mission that takes some serious smarts, intense curiosity and a background in developing data solutions across the data lifecycle.  Analytica has been recognized by Inc. Magazine as the fastest-growing private US small business. We work with U.S. government customers in health, civilian, and national security missions. As a core member you’ll work with a diverse team of professionals to solution matters, architect nuisances, and come up with alternatives. We offer competitive compensation with opportunities for bonuses, employer paid health care, training and development funds, and 401k match.  Responsibilities include (But Are Not Necessarily Limited To):
 
   Research, design, build, optimize and maintain reliable, efficient, and accessible data models, systems and pipelines/APIs etc.
   Support, with guidance, the analytic and/or operational use of data.
   Align closely with Enterprise partners in data science, architecture, governance, infrastructure, and security to apply standards and optimize production environments and practices.
   Collaborate with business owners to optimize data collection, movement, storage, and usage to data process and data quality.
   Convert concepts & ideas into workable prototypes (custom or COTS products) for client reviews and acceptance.
   Translate business needs into:
   
     data architecture solutions development within supported data systems.
     data orchestration pipelines (source to target analysis & recommendations), data sourcing, cleansing, augmentation and quality control processes within supported data systems.
     Prototype, test and integrate new data tools (i.e. data features and functionality) as defined by the product owners and business teams
   
 
  Competency and skill set will determine level of placement within the posted job family.  Qualifications:
 
   Bachelor’s degree in computer science, information systems management or similarly related degree.
   7+ years of professional data solutions development and implementation experience with:
   
     AWS (Glue, Athena, API Gateway)
     SQL, NoSQL
     Data developments with modeling tools such as Neo4J, Erwin, Embarcadero, transforming logical, physical, conceptual, reverse engineering & forward engineering.
     Development with Alation and/or EASparx
     Data Movement tools such as Informatica & others…
     Unit testing
     RESTful API Development
     Desire and willingness to learn new data tools
   
   Has an Agile mindset and iterative development process background
   
     Help promote a culture of diversity and inclusion within the department and the larger organization
     Value different ideas and opinions
     Listen courageously and remain curious in all that you do
   
   CMS data experience a must
   CMS Public Trust clearance, EUA highly preferred
 
  Valuable Experience:
 
   AWS CDK and/or other AWS services (or comparable cloud data solutioning tools)
   Experience with Git and CICD pipelines
   Relational database design
   Microservices / Containers (Docker, Kubernetes)
   Informatica Intelligent Cloud Services (IICS)
   Prior experience with CMS, preferably within clinical quality or standards area
 
  About ANALYTICA: Analytica is a leading consulting and information technology solutions provider to public sector organizations supporting health, civilian, and national security missions. Founded in 2009 and headquartered in Bethesda, MD., the company is an established 8(a) small business that has been recognized by Inc. Magazine each of the past three years as one of the 250 fastest-growing companies in the U.S. Analytica specializes in providing software and systems engineering, information management, analytics & visualization, agile project management, and management consulting services. The company is appraised by the Software Engineering Institute (SEI) at CMMI® Maturity Level 3 and is an ISO 9001:2008 certified provider.  As a federal contractor, Analytica is required to verify that all employees are fully vaccinated against COVID-19. If you receive an offer and are unable to get vaccinated for religious or medical reasons, you may request a reasonable accommodation. 
  
 vR3cn1Uzfh","<div>
 <p>ANALYTICA is seeking a <b>Senior </b><b>Data Engineer </b>to support a federal government client in the DC metro area (Note - your work location is REMOTE). In this assignment, you will be a team member serving the client in advancing the customers use data, metadata, as well as explore new technologies to better meet those needs.<br> <br> This is a mission that takes some serious smarts, intense curiosity and a background in developing data solutions across the data lifecycle.<br> <br> Analytica has been recognized by Inc. Magazine as the fastest-growing private US small business. We work with U.S. government customers in health, civilian, and national security missions. As a core member you&#x2019;ll work with a diverse team of professionals to solution matters, architect nuisances, and come up with alternatives. We offer competitive compensation with opportunities for bonuses, employer paid health care, training and development funds, and 401k match.<br> <br> <b>Responsibilities include (But Are Not Necessarily Limited To):</b></p>
 <ul>
  <li> Research, design, build, optimize and maintain reliable, efficient, and accessible data models, systems and pipelines/APIs etc.</li>
  <li> Support, with guidance, the analytic and/or operational use of data.</li>
  <li> Align closely with Enterprise partners in data science, architecture, governance, infrastructure, and security to apply standards and optimize production environments and practices.</li>
  <li> Collaborate with business owners to optimize data collection, movement, storage, and usage to data process and data quality.</li>
  <li> Convert concepts &amp; ideas into workable prototypes (custom or COTS products) for client reviews and acceptance.</li>
  <li> Translate business needs into:
   <ul>
    <li> data architecture solutions development within supported data systems.</li>
    <li> data orchestration pipelines (source to target analysis &amp; recommendations), data sourcing, cleansing, augmentation and quality control processes within supported data systems.</li>
    <li> Prototype, test and integrate new data tools (i.e. data features and functionality) as defined by the product owners and business teams</li>
   </ul></li>
 </ul>
 <p> Competency and skill set will determine level of placement within the posted job family.<br> <br> <b>Qualifications:</b></p>
 <ul>
  <li> Bachelor&#x2019;s degree in computer science, information systems management or similarly related degree.</li>
  <li> 7+ years of professional data solutions development and implementation experience with:
   <ul>
    <li> AWS (Glue, Athena, API Gateway)</li>
    <li> SQL, NoSQL</li>
    <li> Data developments with modeling tools such as Neo4J, Erwin, Embarcadero, transforming logical, physical, conceptual, reverse engineering &amp; forward engineering.</li>
    <li> Development with Alation and/or EASparx</li>
    <li> Data Movement tools such as Informatica &amp; others&#x2026;</li>
    <li> Unit testing</li>
    <li> RESTful API Development</li>
    <li> Desire and willingness to learn new data tools</li>
   </ul></li>
  <li> Has an Agile mindset and iterative development process background
   <ul>
    <li> Help promote a culture of diversity and inclusion within the department and the larger organization</li>
    <li> Value different ideas and opinions</li>
    <li> Listen courageously and remain curious in all that you do</li>
   </ul></li>
  <li> CMS data experience a must</li>
  <li> CMS Public Trust clearance, EUA highly preferred</li>
 </ul>
 <p><b> Valuable Experience:</b></p>
 <ul>
  <li> AWS CDK and/or other AWS services (or comparable cloud data solutioning tools)</li>
  <li> Experience with Git and CICD pipelines</li>
  <li> Relational database design</li>
  <li> Microservices / Containers (Docker, Kubernetes)</li>
  <li> Informatica Intelligent Cloud Services (IICS)</li>
  <li> Prior experience with CMS, preferably within clinical quality or standards area</li>
 </ul>
 <p><b><br> About </b><b>ANALYTICA</b>: Analytica is a leading consulting and information technology solutions provider to public sector organizations supporting health, civilian, and national security missions. Founded in 2009 and headquartered in Bethesda, MD., the company is an established 8(a) small business that has been recognized by <i>Inc. Magazine</i> each of the past three years as one of the 250 fastest-growing companies in the U.S. Analytica specializes in providing software and systems engineering, information management, analytics &amp; visualization, agile project management, and management consulting services. The company is appraised by the Software Engineering Institute (SEI) at CMMI&#xae; Maturity Level 3 and is an ISO 9001:2008 certified provider.<br> <br> As a federal contractor, Analytica is required to verify that all employees are fully vaccinated against COVID-19. If you receive an offer and are unable to get vaccinated for religious or medical reasons, you may request a reasonable accommodation.<br> </p>
 <p> </p>
 <p>vR3cn1Uzfh</p>
</div>","https://www.indeed.com/rc/clk?jk=814edb5947a6ddd9&atk=&xpse=SoC367I3JucpCgQnDx0LbzkdCdPP","814edb5947a6ddd9",,"Full-time",,"Remote","Senior Data Engineer","5 days ago","2023-10-18T13:06:28.403Z","3.5","10",,"2023-10-23T13:06:28.404Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=814edb5947a6ddd9&from=jasx&tk=1hdea988rk6dn800&vjs=3"
"Leidos","Description 
 Leidos has an immediate opening for a Senior Systems Engineer supporting development of a data Lakehouse at HHS in Washington DC. This position is an excellent opportunity to interface directly with government personnel to help design and guide technical solutions and business processes for critical public health research and response mission spaces.
 
  Primary Responsibilities
 
   Work directly with the customer to capture and formalize ‘Baseline’ and ‘To Be’ business user stories, use cases, requirements, system architectures, and data schema.
   Track and communicate solution and system development, production and maintenance activities.
   Design and execute test plans for solution and system testing.
   Help support the identification and considerations for alternative application solutions to meet business customer needs.
   Work with the broader development, including SQL developers and Azure engineers.
   Consider security and infrastructure implications of requested changes.
 
 
  Basic Qualifications
 
   Bachelor’s Degree in Systems Engineering, Computer Science, Information Technology, Business Science, or related field required
   8+ years’ of combined experience in system engineering, software development or business analysis required.
   Candidate must demonstrate a willing initiative to solicit customer requirements and translate them into formalized structured technical products. Candidate must have strong verbal and written communication skills. Concise writing and ability to communicate technical content to broad audiences are critical candidate abilities.
   Candidate must be a US Citizen and be able to obtain and maintain a high-risk public trust clearance.
   Must have experience or familiarity with: executing Agile, Scrum, and Kanban methodologies. Soliciting and documenting use cases and designs for system requirements (User Stories, Use Cases, Requirements, Specifications, Data Schema, Business Process Workflows). Use of system management tools such as Azure DevOps, Jira, Redmine, or similar system. Developing and executing testing and acceptance plans. Tracking bugs, issues and resolutions. Performing data analysis and reporting. Knowledge of government system security policies (ATO process) Ability to flexibly pivot to varying needs of the project while maintaining situational awareness
 
 
  Preferred Qualifications
 
   The preferred candidate will possess broader Systems Engineering knowledge and can readily execute low-level engineering tasks as well as high-level technical project management tasks.
   The preferred candidate will have a MS in Systems Engineering or a related discipline.
   Preferred candidates will have experience or familiarity with:
   Knowledge of the Microsoft Azure ecosystem
   Experience with Data Lakes and preferably Data Lakehouses
   Experience working with federal IT systems
   Experience working with SQL developers (or knowledge of SQL)
   Prioritizing and communicating requirements and system development activities.
   Communicating impact of requirement changes on active development activities.
   Executing and presenting trade space of alternatives with cost benefit analysis.
   Supporting Authority to Operate (ATO) activities and other production system processes.
   Experience in developing advanced data visualizations.
 
 
  Pay Range: Pay Range $97,500.00 - $176,250.00
 
  The Leidos pay range for this job level is a general guideline only and not a guarantee of compensation or salary. Additional factors considered in extending an offer include (but are not limited to) responsibilities of the job, education, experience, knowledge, skills, and abilities, as well as internal equity, alignment with market data, applicable bargaining agreement (if any), or other law.
  #Remote","<div>
 <p><b>Description</b> </p>
 <p>Leidos has an immediate opening for a Senior Systems Engineer supporting development of a data Lakehouse at HHS in Washington DC. This position is an excellent opportunity to interface directly with government personnel to help design and guide technical solutions and business processes for critical public health research and response mission spaces.</p>
 <p></p>
 <p><b> Primary Responsibilities</b></p>
 <ul>
  <li> Work directly with the customer to capture and formalize &#x2018;Baseline&#x2019; and &#x2018;To Be&#x2019; business user stories, use cases, requirements, system architectures, and data schema.</li>
  <li> Track and communicate solution and system development, production and maintenance activities.</li>
  <li> Design and execute test plans for solution and system testing.</li>
  <li> Help support the identification and considerations for alternative application solutions to meet business customer needs.</li>
  <li> Work with the broader development, including SQL developers and Azure engineers.</li>
  <li> Consider security and infrastructure implications of requested changes.</li>
 </ul>
 <p></p>
 <p><b> Basic Qualifications</b></p>
 <ul>
  <li> Bachelor&#x2019;s Degree in Systems Engineering, Computer Science, Information Technology, Business Science, or related field required</li>
  <li> 8+ years&#x2019; of combined experience in system engineering, software development or business analysis required.</li>
  <li> Candidate must demonstrate a willing initiative to solicit customer requirements and translate them into formalized structured technical products. Candidate must have strong verbal and written communication skills. Concise writing and ability to communicate technical content to broad audiences are critical candidate abilities.</li>
  <li> Candidate must be a US Citizen and be able to obtain and maintain a high-risk public trust clearance.</li>
  <li> Must have experience or familiarity with: executing Agile, Scrum, and Kanban methodologies. Soliciting and documenting use cases and designs for system requirements (User Stories, Use Cases, Requirements, Specifications, Data Schema, Business Process Workflows). Use of system management tools such as Azure DevOps, Jira, Redmine, or similar system. Developing and executing testing and acceptance plans. Tracking bugs, issues and resolutions. Performing data analysis and reporting. Knowledge of government system security policies (ATO process) Ability to flexibly pivot to varying needs of the project while maintaining situational awareness</li>
 </ul>
 <p></p>
 <p><b> Preferred Qualifications</b></p>
 <ul>
  <li> The preferred candidate will possess broader Systems Engineering knowledge and can readily execute low-level engineering tasks as well as high-level technical project management tasks.</li>
  <li> The preferred candidate will have a MS in Systems Engineering or a related discipline.</li>
  <li> Preferred candidates will have experience or familiarity with:</li>
  <li> Knowledge of the Microsoft Azure ecosystem</li>
  <li> Experience with Data Lakes and preferably Data Lakehouses</li>
  <li> Experience working with federal IT systems</li>
  <li> Experience working with SQL developers (or knowledge of SQL)</li>
  <li> Prioritizing and communicating requirements and system development activities.</li>
  <li> Communicating impact of requirement changes on active development activities.</li>
  <li> Executing and presenting trade space of alternatives with cost benefit analysis.</li>
  <li> Supporting Authority to Operate (ATO) activities and other production system processes.</li>
  <li> Experience in developing advanced data visualizations.</li>
 </ul>
 <p></p>
 <h2 class=""jobSectionHeader""><b> Pay Range:</b></h2> Pay Range &#x24;97,500.00 - &#x24;176,250.00
 <p></p>
 <p> The Leidos pay range for this job level is a general guideline only and not a guarantee of compensation or salary. Additional factors considered in extending an offer include (but are not limited to) responsibilities of the job, education, experience, knowledge, skills, and abilities, as well as internal equity, alignment with market data, applicable bargaining agreement (if any), or other law.</p>
 <p> #Remote</p>
</div>","https://www.indeed.com/rc/clk?jk=e2317b3154109084&atk=&xpse=SoCG67I3JucowtwxhZ0LbzkdCdPP","e2317b3154109084",,"Full-time",,"Remote","Data Lakehouse Engineer","5 days ago","2023-10-18T13:06:30.564Z","3.7","1701","$97,500 - $176,250 a year","2023-10-23T13:06:30.566Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=e2317b3154109084&from=jasx&tk=1hdea988rk6dn800&vjs=3"
"Booz Allen Hamilton","Job Description 
  
 
 
  
   
    
     
      
       
        
         Location: 
        
        
         Springfield,VA,US 
        
       
       
        
         Remote Work: 
        
        
         Yes 
        
       
       
        
         Job Number: 
        
        
         R0182529
        
       
      
     
    
    
    
      
     
       
      
     
    
    
    
     
      
       
        
         Azure Data Engineer, Lead
          The Opportunity:
          The DHS Cube program is seeking an experienced Data Engineer for its data management, data integration, and business intelligence program. This high-visibility mission support data program is managed within Headquarters for the DHS and our numerous client stakeholders. Over the next year and beyond, this program will be looking to take on new challenges, including modernizing their business intelligence technologies through a move to cloud services, improving the data quality through the development of confidence scorecards, and development of a data governance or data standards prototype.
         
          As part of this role, our Azure ETL Engineer will be responsible for moving ETL pipelines from one framework to a new framework. The role will also perform ETL testing and troubleshooting, including testing the unit data models, system performance testing, uploading, downloading, or querying speed tests, and data flow validations. In this role, you will also be building repeatable documentation assets to enable implementation teams to take advantage of pre-built assets, identifying data storage requirements, determining the storage needs of the customer, and ongoing development of a repository of ETL knowledge and utilizing that to enhance the quality, speed, and productivity of the team. As a senior member of the team, you’ll also be supporting more mid-level engineers with the testing and QA of their code along with an expectation of enhanced code velocity, design thinking, and the ability to work independently and provide feedback to the Data Architect on required design updates throughout the process.
         
          Join us. The world can’t wait.
         
          You Have: 
         
          8+ years of experience in a role encompassing industry standard ETL development techniques
           8+ years of experience with data integration, and database technologies, including Oracle, Postgres, Cosmos, or SQL
           3+ years of experience with Azure Cloud SaaS solutions and managed services serverless technologies, including Azure Data Factory, Synapse Analytics, Logic Apps, or ADLS
           Experience with scripting and basic programming, including JavaScript, shell script, or Python
           Experience with data analysis and profiling of source data while developing or building robust ETL processes
           Knowledge of disparate data sources and targets
           Knowledge of data validation, cleansing, transformation, consolidation, de-duplication, aggregation, de-aggregation, and enrichment
           Knowledge of API development and testing
           DHS Suitability
           Bachelor’s degree
         
         
          Nice If You Have:
         
           Experience with Agile and Scrum methodologies
           Experience with Azure Platform CI/CD or DevOps
           Experience with reporting tools, including Tableau or Power BI
           Experience with industry standard ETL tools such as SQL, scripting languages, data modelling techniques, or relational and NoSQL database engineering and configuration, including document store, Azure Cosmos, or AWS DynamoDB
           Knowledge of data transactional requirements, business logic pertaining to commit and rollback cycles, and how to implement to preserve the integrity of related data elements in government, financial and similar systems
           Ability to build ETL processes that apply to data migration and data integration scenarios to know their key differences
           Ability to adapt to a rapidly changing product and respond strategically to client needs
           Ability to balance multiple efforts simultaneously and meet strict deadlines
           Ability to have a desire for learning and development, and a passion for exploratory analysis or exploratory learning
         
         
          Vetting: 
         Applicants selected will be subject to a government investigation and may need to meet eligibility requirements of the U.S. government client; DHS suitability is required.
         
          Create Your Career:
         
          Grow With Us
          Your growth matters to us—that’s why we offer a variety of ways for you to develop your career. With professional and leadership development opportunities like upskilling programs, tuition reimbursement, mentoring, and firm-sponsored networking, you can chart a unique and fulfilling career path on your own terms.
         
          A Place Where You Belong
          Diverse perspectives cultivate collective ingenuity. Booz Allen’s culture of respect, equity, and opportunity means that, here, you are free to bring your whole self to work. With an array of business resource groups and other opportunities for connection, you’ll develop your community in no time.
         
          Support Your Well-Being
          Our comprehensive benefits package includes wellness programs with HSA contributions, paid holidays, paid parental leave, a generous 401(k) match, and more. With these benefits, plus the option for flexible schedules and remote and hybrid locations, we’ll support you as you pursue a balanced, fulfilling life—at work and at home.
         
          Your Candidate Journey
          At Booz Allen, we know our people are what propel us forward, and we value relationships most of all. Here, we’ve compiled a list of resources so you’ll know what to expect as we forge a connection with you during your journey as a candidate with us.
         
          Compensation
          At Booz Allen, we celebrate your contributions, provide you with opportunities and choices, and support your total well-being. Our offerings include health, life, disability, financial, and retirement benefits, as well as paid leave, professional development, tuition assistance, work-life programs, and dependent care. Our recognition awards program acknowledges employees for exceptional performance and superior demonstration of our values. Full-time and part-time employees working at least 20 hours a week on a regular basis are eligible to participate in Booz Allen’s benefit programs. Individuals that do not meet the threshold are only eligible for select offerings, not inclusive of health benefits. We encourage you to learn more about our total benefits by visiting the Resource page on our Careers site and reviewing Our Employee Benefits page.
          Salary at Booz Allen is determined by various factors, including but not limited to location, the individual’s particular combination of education, knowledge, skills, competencies, and experience, as well as contract-specific affordability and organizational requirements. The projected compensation range for this position is $93,300.00 to $212,000.00 (annualized USD). The estimate displayed represents the typical salary range for this position and is just one component of Booz Allen’s total compensation package for employees.
         
          Work Model Our people-first culture prioritizes the benefits of flexibility and collaboration, whether that happens in person or remotely.
         
           If this position is listed as remote or hybrid, you’ll periodically work from a Booz Allen or client site facility.
           If this position is listed as onsite, you’ll work with colleagues and clients in person, as needed for the specific role.
         
         
          EEO Commitment
          We’re an equal employment opportunity/affirmative action employer that empowers our people to fearlessly drive change – no matter their race, color, ethnicity, religion, sex (including pregnancy, childbirth, lactation, or related medical conditions), national origin, ancestry, age, marital status, sexual orientation, gender identity and expression, disability, veteran status, military or uniformed service member status, genetic information, or any other status protected by applicable federal, state, local, or international law.","<div>
 <div>
  <div>
   <h2 class=""jobSectionHeader""><b>Job Description</b></h2> 
  </div>
 </div>
 <div>
  <div>
   <div>
    <div>
     <div>
      <div>
       <div>
        <div>
         Location: 
        </div>
        <div>
         Springfield,VA,US 
        </div>
       </div>
       <div>
        <div>
         Remote Work: 
        </div>
        <div>
         Yes 
        </div>
       </div>
       <div>
        <div>
         Job Number: 
        </div>
        <div>
         R0182529
        </div>
       </div>
      </div>
     </div>
    </div>
    <p></p>
    <div>
     <br> 
     <div>
      <div> 
      </div>
     </div>
    </div>
    <div></div>
    <div>
     <div>
      <div>
       <div>
        <div>
         Azure Data Engineer, Lead
         <p><b> The Opportunity:</b></p>
         <p> The DHS Cube program is seeking an experienced Data Engineer for its data management, data integration, and business intelligence program. This high-visibility mission support data program is managed within Headquarters for the DHS and our numerous client stakeholders. Over the next year and beyond, this program will be looking to take on new challenges, including modernizing their business intelligence technologies through a move to cloud services, improving the data quality through the development of confidence scorecards, and development of a data governance or data standards prototype.</p>
         <p></p>
         <p> As part of this role, our Azure ETL Engineer will be responsible for moving ETL pipelines from one framework to a new framework. The role will also perform ETL testing and troubleshooting, including testing the unit data models, system performance testing, uploading, downloading, or querying speed tests, and data flow validations. In this role, you will also be building repeatable documentation assets to enable implementation teams to take advantage of pre-built assets, identifying data storage requirements, determining the storage needs of the customer, and ongoing development of a repository of ETL knowledge and utilizing that to enhance the quality, speed, and productivity of the team. As a senior member of the team, you&#x2019;ll also be supporting more mid-level engineers with the testing and QA of their code along with an expectation of enhanced code velocity, design thinking, and the ability to work independently and provide feedback to the Data Architect on required design updates throughout the process.</p>
         <p></p>
         <p> Join us. The world can&#x2019;t wait.</p>
         <p></p>
         <p><b> You Have: </b></p>
         <ul>
          <li>8+ years of experience in a role encompassing industry standard ETL development techniques</li>
          <li> 8+ years of experience with data integration, and database technologies, including Oracle, Postgres, Cosmos, or SQL</li>
          <li> 3+ years of experience with Azure Cloud SaaS solutions and managed services serverless technologies, including Azure Data Factory, Synapse Analytics, Logic Apps, or ADLS</li>
          <li> Experience with scripting and basic programming, including JavaScript, shell script, or Python</li>
          <li> Experience with data analysis and profiling of source data while developing or building robust ETL processes</li>
          <li> Knowledge of disparate data sources and targets</li>
          <li> Knowledge of data validation, cleansing, transformation, consolidation, de-duplication, aggregation, de-aggregation, and enrichment</li>
          <li> Knowledge of API development and testing</li>
          <li> DHS Suitability</li>
          <li> Bachelor&#x2019;s degree</li>
         </ul>
         <p></p>
         <p><b> Nice If You Have:</b></p>
         <ul>
          <li> Experience with Agile and Scrum methodologies</li>
          <li> Experience with Azure Platform CI/CD or DevOps</li>
          <li> Experience with reporting tools, including Tableau or Power BI</li>
          <li> Experience with industry standard ETL tools such as SQL, scripting languages, data modelling techniques, or relational and NoSQL database engineering and configuration, including document store, Azure Cosmos, or AWS DynamoDB</li>
          <li> Knowledge of data transactional requirements, business logic pertaining to commit and rollback cycles, and how to implement to preserve the integrity of related data elements in government, financial and similar systems</li>
          <li> Ability to build ETL processes that apply to data migration and data integration scenarios to know their key differences</li>
          <li> Ability to adapt to a rapidly changing product and respond strategically to client needs</li>
          <li> Ability to balance multiple efforts simultaneously and meet strict deadlines</li>
          <li> Ability to have a desire for learning and development, and a passion for exploratory analysis or exploratory learning</li>
         </ul>
         <p></p>
         <p><b> Vetting: </b></p>
         <p>Applicants selected will be subject to a government investigation and may need to meet eligibility requirements of the U.S. government client; DHS suitability is required.</p>
         <p></p>
         <p><b> Create Your Career:</b></p>
         <p></p>
         <p><b> Grow With Us</b></p>
         <p> Your growth matters to us&#x2014;that&#x2019;s why we offer a variety of ways for you to develop your career. With professional and leadership development opportunities like upskilling programs, tuition reimbursement, mentoring, and firm-sponsored networking, you can chart a unique and fulfilling career path on your own terms.</p>
         <p></p>
         <p><b> A Place Where You Belong</b></p>
         <p> Diverse perspectives cultivate collective ingenuity. Booz Allen&#x2019;s culture of respect, equity, and opportunity means that, here, you are free to bring your whole self to work. With an array of business resource groups and other opportunities for connection, you&#x2019;ll develop your community in no time.</p>
         <p></p>
         <p><b> Support Your Well-Being</b></p>
         <p> Our comprehensive benefits package includes wellness programs with HSA contributions, paid holidays, paid parental leave, a generous 401(k) match, and more. With these benefits, plus the option for flexible schedules and remote and hybrid locations, we&#x2019;ll support you as you pursue a balanced, fulfilling life&#x2014;at work and at home.</p>
         <p></p>
         <p><b> Your Candidate Journey</b></p>
         <p> At Booz Allen, we know our people are what propel us forward, and we value relationships most of all. Here, we&#x2019;ve compiled a list of resources so you&#x2019;ll know what to expect as we forge a connection with you during your journey as a candidate with us.</p>
         <p></p>
         <p><b> Compensation</b></p>
         <p> At Booz Allen, we celebrate your contributions, provide you with opportunities and choices, and support your total well-being. Our offerings include health, life, disability, financial, and retirement benefits, as well as paid leave, professional development, tuition assistance, work-life programs, and dependent care. Our recognition awards program acknowledges employees for exceptional performance and superior demonstration of our values. Full-time and part-time employees working at least 20 hours a week on a regular basis are eligible to participate in Booz Allen&#x2019;s benefit programs. Individuals that do not meet the threshold are only eligible for select offerings, not inclusive of health benefits. We encourage you to learn more about our total benefits by visiting the Resource page on our Careers site and reviewing Our Employee Benefits page.</p>
         <p></p> Salary at Booz Allen is determined by various factors, including but not limited to location, the individual&#x2019;s particular combination of education, knowledge, skills, competencies, and experience, as well as contract-specific affordability and organizational requirements. The projected compensation range for this position is &#x24;93,300.00 to &#x24;212,000.00 (annualized USD). The estimate displayed represents the typical salary range for this position and is just one component of Booz Allen&#x2019;s total compensation package for employees.
         <p></p>
         <p><b> Work Model</b><br> Our people-first culture prioritizes the benefits of flexibility and collaboration, whether that happens in person or remotely.</p>
         <ul>
          <li> If this position is listed as remote or hybrid, you&#x2019;ll periodically work from a Booz Allen or client site facility.</li>
          <li> If this position is listed as onsite, you&#x2019;ll work with colleagues and clients in person, as needed for the specific role.</li>
         </ul>
         <p></p>
         <p><b> EEO Commitment</b></p>
         <p> We&#x2019;re an equal employment opportunity/affirmative action employer that empowers our people to fearlessly drive change &#x2013; no matter their race, color, ethnicity, religion, sex (including pregnancy, childbirth, lactation, or related medical conditions), national origin, ancestry, age, marital status, sexual orientation, gender identity and expression, disability, veteran status, military or uniformed service member status, genetic information, or any other status protected by applicable federal, state, local, or international law.</p>
        </div>
       </div>
      </div>
     </div>
    </div>
   </div>
  </div>
 </div>
</div>
<p></p>","https://www.indeed.com/rc/clk?jk=bcb3fa3a95822e7e&atk=&xpse=SoBf67I3JucoqTTdQ50LbzkdCdPP","bcb3fa3a95822e7e",,,,"Springfield, VA","Azure Data Engineer, Lead","4 days ago","2023-10-19T13:06:33.236Z","3.9","2514","$93,300 - $212,000 a year","2023-10-23T13:06:33.238Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=bcb3fa3a95822e7e&from=jasx&tk=1hdea988rk6dn800&vjs=3"
"Lucayan Technology Solutions","On-Site Job
Description:
12 Months Contract
MUST HAVE's:Database developmentWorking knowledge on AWSOracle, Snowflake, Python, CICD, Jenkins8 years' experience in the space
NICE TO HAVE's:Docker containers and EKS knowledgeReporting tools – Tableau or Power BICandidate that has worked in a complex financial environment
DescriptionIf you have a passion for working with data using multiple emerging technologies on the cloud, this might be the right opportunity for you! The Cloud Data Engineer will be working as part of a core team building solutions for the data analytics platform for Client’s Asset Management Division. This will involve designing and developing solutions for a variety of data lake needs using Snowflake as the data store for structured/semi-structured data and AWS s3 for unstructured data.We are seeking a highly motivated expert data engineer with a strong agile mentality, who’s looking for new challenges, we have an exciting opportunity for you to join our fast paced and highly collaborative group. This role will be involved in the full end-to-end process through planning, design, development, quality, and implementation of solutions.This position can be in Merrimack, Boston, Raleigh, and Westlake.
The Expertise and Skills You BringExpertise in SQL, identifying patterns and trends in data, recommend and define data requirements, mastery in implementing data quality checks to ensure accuracy and completeness.You enjoy learning new technologies, data analysis, identifying data patterns and trends and you can independently resolve technical challenges.Experience in processing and exposing data using AWS technologies like ec2, s3, Lambda, API Gateway, Load Balancers, Auto scaling etc.Expertise in building data ingestion tools using technologies like Python to extract data from Relational Databases/Web Scraping/External API’sExperience in Snowflake or any MPP and columnar database on the Cloud.Experience in CI/CD release automation and deployment (Jenkins, Concourse, CloudFormation etc.)Experience and good understating of databases (Oracle, Netezza) and ETL toolsExperience in scheduling tools like Autosys, Control-M, Airflow etc.Excellent programming skills in SQL, PL/SQL, Python, shell etc.Exposure to Big Data technologies (Hadoop, Spark, Hive, presto etc.)Exposure to streaming services like Kafka and containers will be an advantage.Good understanding of overall AWS security services like KMS, IAM, Security groups etc.At least 8 years of software development experience with at least 3 years working on Cloud/Big Data technologies.BS in Computer Science or related degree, or equivalent experienceAt Client, you can find it all here. We reward ambitious, passionate individuals with a work environment that furthers diversity, partnership and collaboration as well as encourages innovative ideas and fresh thinking. We recognize the value that employees’ individual differences can give to the forward-thinking and strong future of our company.
Education: Bachelors Degree
Job Type: Contract
Experience level:

 10 years

Work Location: On the road","<p>On-Site Job</p>
<p><b>Description:</b></p>
<p>12 Months Contract</p>
<p><b>MUST HAVE&apos;s:</b><br>Database development<br>Working knowledge on AWS<br>Oracle, Snowflake, Python, CICD, Jenkins<br>8 years&apos; experience in the space</p>
<p><b>NICE TO HAVE&apos;s:</b><br>Docker containers and EKS knowledge<br>Reporting tools &#x2013; Tableau or Power BI<br>Candidate that has worked in a complex financial environment</p>
<p><b>Description</b><br>If you have a passion for working with data using multiple emerging technologies on the cloud, this might be the right opportunity for you! The Cloud Data Engineer will be working as part of a core team building solutions for the data analytics platform for Client&#x2019;s Asset Management Division. This will involve designing and developing solutions for a variety of data lake needs using Snowflake as the data store for structured/semi-structured data and AWS s3 for unstructured data.<br>We are seeking a highly motivated expert data engineer with a strong agile mentality, who&#x2019;s looking for new challenges, we have an exciting opportunity for you to join our fast paced and highly collaborative group. This role will be involved in the full end-to-end process through planning, design, development, quality, and implementation of solutions.<br>This position can be in Merrimack, Boston, Raleigh, and Westlake.</p>
<p><b>The Expertise and Skills You Bring</b><br>Expertise in SQL, identifying patterns and trends in data, recommend and define data requirements, mastery in implementing data quality checks to ensure accuracy and completeness.<br>You enjoy learning new technologies, data analysis, identifying data patterns and trends and you can independently resolve technical challenges.<br>Experience in processing and exposing data using AWS technologies like ec2, s3, Lambda, API Gateway, Load Balancers, Auto scaling etc.<br>Expertise in building data ingestion tools using technologies like Python to extract data from Relational Databases/Web Scraping/External API&#x2019;s<br>Experience in Snowflake or any MPP and columnar database on the Cloud.<br>Experience in CI/CD release automation and deployment (Jenkins, Concourse, CloudFormation etc.)<br>Experience and good understating of databases (Oracle, Netezza) and ETL tools<br>Experience in scheduling tools like Autosys, Control-M, Airflow etc.<br>Excellent programming skills in SQL, PL/SQL, Python, shell etc.<br>Exposure to Big Data technologies (Hadoop, Spark, Hive, presto etc.)<br>Exposure to streaming services like Kafka and containers will be an advantage.<br>Good understanding of overall AWS security services like KMS, IAM, Security groups etc.<br>At least 8 years of software development experience with at least 3 years working on Cloud/Big Data technologies.<br>BS in Computer Science or related degree, or equivalent experience<br>At Client, you can find it all here. We reward ambitious, passionate individuals with a work environment that furthers diversity, partnership and collaboration as well as encourages innovative ideas and fresh thinking. We recognize the value that employees&#x2019; individual differences can give to the forward-thinking and strong future of our company.</p>
<p><b>Education:</b> Bachelors Degree</p>
<p>Job Type: Contract</p>
<p>Experience level:</p>
<ul>
 <li>10 years</li>
</ul>
<p>Work Location: On the road</p>",,"fb7783846e97f42c",,"Contract",,"West Lake Hills, TX","Data Engineer","3 days ago","2023-10-20T13:06:36.742Z",,,,"2023-10-23T13:06:36.744Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=fb7783846e97f42c&from=jasx&tk=1hdea9902imai801&vjs=3"
"CVS Health","Bring your heart to CVS Health. Every one of us at CVS Health shares a single, clear purpose: Bringing our heart to every moment of your health. This purpose guides our commitment to deliver enhanced human-centric health care for a rapidly changing world. Anchored in our brand — with heart at its center — our purpose sends a personal message that how we deliver our services is just as important as what we deliver.  Our Heart At Work Behaviors™ support this purpose. We want everyone who works at CVS Health to feel empowered by the role they play in transforming our culture and accelerating our ability to innovate and deliver solutions to make health care more personal, convenient and affordable.
 
  Position Summary
  Manages the creation and/or implementation of information security policies, programs, and procedures to cost-effectively and efficiently protect information and information systems assets from Intentional or inadvertent modification; disclosure or destruction; unauthorized access; reduced, interrupted or terminated, processing capability; malicious logic or virus activity; or loss, theft, damage or destruction of any IT resources.
 
  Designs, implements and deploys information security policies, procedures and guidelines. Responsible for developing, maintaining, publishing and/or enforcing information security standards and guidelines encompassing data, and intellectual security. Provides reports to management regarding the effectiveness of network and data security and making recommendations for the adoption of new procedures and technologies, as required. Monitors changes in legislation and accreditation standards that affect information security. Monitors internal control systems to ensure that appropriate information access levels and security clearances are maintained. Establishes meaningful metrics on key critical infrastructure components of information security and monitoring of these to ensure the confidentiality, integrity and availability of information and processes. Ensures awareness of organization's information security policies and procedures among employees, contractors, alliances and other third parties. Initiates, facilitates, and promotes activities to foster information security awareness within the organization. Provides direct information security training to all employees, contractors, alliances, and other third parties. Coordinates internal and external audits and follow up with implementation, based audit recommendations. Serves as an internal information security consultant to the organization. Monitors advancements in information security technologies. Communicates unresolved information security exposures, misuse, or non-compliance situations to senior management. Participates in the activities of the Information Security Committee, responsible for the organization's information security program.
  Extensive experience with data ingest process, Hadoop, Apache Nifi, Cloud platform (AWS, GCP, Azure)
 
 
  Support the design, development and implementation of Security Data Analytic (SDA) capabilities that leverage cybersecurity data and big data technologies to deliver efficiencies and new discovery capabilities
  Design and implement resilient data pipelines and architecture for ingesting unbounded data from multiple sources with different formats leveraging Hadoop, Spark, NiFi and Kafka, Cloud-based applications (AWS, GCP, Azure)
  Develop capabilities for ingesting and indexing large volumes of security-related data in Hadoop, AWS, GCP, Azure environments
  Stay apprised of the latest data science technologies and techniques in order to identify areas where new tools can be applied to support SDA use cases
  Design and implement batch pipelines using Hive, Spark, Pig and Python for global security analytics
  Design and implement discrete and behavioral analytic capabilities to support cybersecurity operations
  Develop visualization capabilities to data using Tableau
  Support the SDA infrastructure design and implementation and ensure integration with other GS systems and operational workflows
  Stay apprised of the latest industry security trends and threats and apply that knowledge to the design and implementation of SDA software solutions
 
  Required Qualifications
  4+ years experience designing and implementing capabilities in a Hadoop-based, Spark big data environment
  4+ years experience in design and implementing real-time data pipelines using NiFi, Kafka and Spark Streaming 
 4+ years experience processing large volumes of data with Hive, Pig, Spark, NiFi, Kafka and Python 
   Preferred Qualifications
 
   Familiarity with streaming big data applications like, Kafka, NiFi, Spark streaming/ Strom
   Strong background in Information System Security
   Excellent written and verbal communication skills
   Familiarity with statistical computing techniques, and statistics software (e.g. R and Tableau)
   Experience using source code management systems (e.g. Git)
   Interest and passion for learning about new technologies
   Experience in processing TB’s worth of data
 
  Education
  Bachelor in engineering, computer science, or related fields
 
  Pay Range
  The typical pay range for this role is:
  $94,500.00 - $196,000.00
 
  This pay range represents the base hourly rate or base annual full-time salary for all positions in the job grade within which this position falls. The actual base salary offer will depend on a variety of factors including experience, education, geography and other relevant factors. This position is eligible for a CVS Health bonus, commission or short-term incentive program in addition to the base pay range listed above.  In addition to your compensation, enjoy the rewards of an organization that puts our heart into caring for our colleagues and our communities. The Company offers a full range of medical, dental, and vision benefits. Eligible employees may enroll in the Company’s 401(k) retirement savings plan, and an Employee Stock Purchase Plan is also available for eligible employees. The Company provides a fully-paid term life insurance plan to eligible employees, and short-term and long term disability benefits. CVS Health also offers numerous well-being programs, education assistance, free development courses, a CVS store discount, and discount programs with participating partners. As for time off, Company employees enjoy Paid Time Off (“PTO”) or vacation pay, as well as paid holidays throughout the calendar year. Number of paid holidays, sick time and other time off are provided consistent with relevant state law and Company policies.  For more detailed information on available benefits, please visit jobs.CVSHealth.com/benefits
 
  CVS Health requires certain colleagues to be fully vaccinated against COVID-19 (including any booster shots if required), where allowable under the law, unless they are approved for a reasonable accommodation based on disability, medical condition, religious belief, or other legally recognized reasons that prevents them from being vaccinated.
 
  You are required to have received at least one COVID-19 shot prior to your first day of employment and to provide proof of your vaccination status or apply for a reasonable accommodation within the first 10 days of your employment. Please note that in some states and roles, you may be required to provide proof of full vaccination or an approved reasonable accommodation before you can begin to actively work.
 
  CVS Health is committed to recruiting, hiring, developing, advancing, and retaining individuals with disabilities. As such, we strive to provide equal access to the benefits and privileges of employment, including the provision of a reasonable accommodation to perform essential job functions. CVS Health can provide a request for a reasonable accommodation, including a qualified interpreter, written information in other formats, translation or other services through ColleagueRelations@CVSHealth.com If you have a speech or hearing disability, please call 7-1-1 to utilize Telecommunications Relay Services (TRS). We will make every effort to respond to your request within 48 business hours and do everything we can to work towards a solution.","<div>
 <p>Bring your heart to CVS Health. Every one of us at CVS Health shares a single, clear purpose: Bringing our heart to every moment of your health. This purpose guides our commitment to deliver enhanced human-centric health care for a rapidly changing world. Anchored in our brand &#x2014; with heart at its center &#x2014; our purpose sends a personal message that how we deliver our services is just as important as what we deliver.<br> <br> Our Heart At Work Behaviors&#x2122; support this purpose. We want everyone who works at CVS Health to feel empowered by the role they play in transforming our culture and accelerating our ability to innovate and deliver solutions to make health care more personal, convenient and affordable.</p>
 <p></p>
 <p><b> Position Summary</b></p>
 <p> Manages the creation and/or implementation of information security policies, programs, and procedures to cost-effectively and efficiently protect information and information systems assets from Intentional or inadvertent modification; disclosure or destruction; unauthorized access; reduced, interrupted or terminated, processing capability; malicious logic or virus activity; or loss, theft, damage or destruction of any IT resources.</p>
 <p></p>
 <p> Designs, implements and deploys information security policies, procedures and guidelines. Responsible for developing, maintaining, publishing and/or enforcing information security standards and guidelines encompassing data, and intellectual security. Provides reports to management regarding the effectiveness of network and data security and making recommendations for the adoption of new procedures and technologies, as required. Monitors changes in legislation and accreditation standards that affect information security. Monitors internal control systems to ensure that appropriate information access levels and security clearances are maintained. Establishes meaningful metrics on key critical infrastructure components of information security and monitoring of these to ensure the confidentiality, integrity and availability of information and processes. Ensures awareness of organization&apos;s information security policies and procedures among employees, contractors, alliances and other third parties. Initiates, facilitates, and promotes activities to foster information security awareness within the organization. Provides direct information security training to all employees, contractors, alliances, and other third parties. Coordinates internal and external audits and follow up with implementation, based audit recommendations. Serves as an internal information security consultant to the organization. Monitors advancements in information security technologies. Communicates unresolved information security exposures, misuse, or non-compliance situations to senior management. Participates in the activities of the Information Security Committee, responsible for the organization&apos;s information security program.</p>
 <p> Extensive experience with data ingest process, Hadoop, Apache Nifi, Cloud platform (AWS, GCP, Azure)</p>
 <p></p>
 <ul>
  <li>Support the design, development and implementation of Security Data Analytic (SDA) capabilities that leverage cybersecurity data and big data technologies to deliver efficiencies and new discovery capabilities</li>
  <li>Design and implement resilient data pipelines and architecture for ingesting unbounded data from multiple sources with different formats leveraging Hadoop, Spark, NiFi and Kafka, Cloud-based applications (AWS, GCP, Azure)</li>
  <li>Develop capabilities for ingesting and indexing large volumes of security-related data in Hadoop, AWS, GCP, Azure environments</li>
  <li>Stay apprised of the latest data science technologies and techniques in order to identify areas where new tools can be applied to support SDA use cases</li>
  <li>Design and implement batch pipelines using Hive, Spark, Pig and Python for global security analytics</li>
  <li>Design and implement discrete and behavioral analytic capabilities to support cybersecurity operations</li>
  <li>Develop visualization capabilities to data using Tableau</li>
  <li>Support the SDA infrastructure design and implementation and ensure integration with other GS systems and operational workflows</li>
  <li>Stay apprised of the latest industry security trends and threats and apply that knowledge to the design and implementation of SDA software solutions</li>
 </ul>
 <p><b><br> Required Qualifications</b></p>
 <p> 4+ years experience designing and implementing capabilities in a Hadoop-based, Spark big data environment</p>
 <p> 4+ years experience in design and implementing real-time data pipelines using NiFi, Kafka and Spark Streaming </p>
 <p>4+ years experience processing large volumes of data with Hive, Pig, Spark, NiFi, Kafka and Python<br> </p>
 <p><br> <br> <b>Preferred Qualifications</b></p>
 <ul>
  <li> Familiarity with streaming big data applications like, Kafka, NiFi, Spark streaming/ Strom</li>
  <li> Strong background in Information System Security</li>
  <li> Excellent written and verbal communication skills</li>
  <li> Familiarity with statistical computing techniques, and statistics software (e.g. R and Tableau)</li>
  <li> Experience using source code management systems (e.g. Git)</li>
  <li> Interest and passion for learning about new technologies</li>
  <li> Experience in processing TB&#x2019;s worth of data</li>
 </ul>
 <p><b><br> Education</b></p>
 <p> Bachelor in engineering, computer science, or related fields</p>
 <p></p>
 <p><b> Pay Range</b></p>
 <p> The typical pay range for this role is:</p>
 <p></p> &#x24;94,500.00 - &#x24;196,000.00
 <p></p>
 <p> This pay range represents the base hourly rate or base annual full-time salary for all positions in the job grade within which this position falls. The actual base salary offer will depend on a variety of factors including experience, education, geography and other relevant factors. This position is eligible for a CVS Health bonus, commission or short-term incentive program in addition to the base pay range listed above.<br> <br> In addition to your compensation, enjoy the rewards of an organization that puts our heart into caring for our colleagues and our communities. The Company offers a full range of medical, dental, and vision benefits. Eligible employees may enroll in the Company&#x2019;s 401(k) retirement savings plan, and an Employee Stock Purchase Plan is also available for eligible employees. The Company provides a fully-paid term life insurance plan to eligible employees, and short-term and long term disability benefits. CVS Health also offers numerous well-being programs, education assistance, free development courses, a CVS store discount, and discount programs with participating partners. As for time off, Company employees enjoy Paid Time Off (&#x201c;PTO&#x201d;) or vacation pay, as well as paid holidays throughout the calendar year. Number of paid holidays, sick time and other time off are provided consistent with relevant state law and Company policies.<br> <br> For more detailed information on available benefits, please visit jobs.CVSHealth.com/benefits</p>
 <p></p>
 <p> CVS Health requires certain colleagues to be fully vaccinated against COVID-19 (including any booster shots if required), where allowable under the law, unless they are approved for a reasonable accommodation based on disability, medical condition, religious belief, or other legally recognized reasons that prevents them from being vaccinated.</p>
 <p></p>
 <p> You are required to have received at least one COVID-19 shot prior to your first day of employment and to provide proof of your vaccination status or apply for a reasonable accommodation within the first 10 days of your employment. Please note that in some states and roles, you may be required to provide proof of full vaccination or an approved reasonable accommodation before you can begin to actively work.</p>
 <p></p>
 <p> CVS Health is committed to recruiting, hiring, developing, advancing, and retaining individuals with disabilities. As such, we strive to provide equal access to the benefits and privileges of employment, including the provision of a reasonable accommodation to perform essential job functions. CVS Health can provide a request for a reasonable accommodation, including a qualified interpreter, written information in other formats, translation or other services through ColleagueRelations@CVSHealth.com If you have a speech or hearing disability, please call 7-1-1 to utilize Telecommunications Relay Services (TRS). We will make every effort to respond to your request within 48 business hours and do everything we can to work towards a solution.</p>
</div>","https://www.indeed.com/rc/clk?jk=b1473aeba6e7b340&atk=&xpse=SoAa67I3JucnW3TAMh0LbzkdCdPP","b1473aeba6e7b340",,"Full-time",,"1425 Union Meeting Rd, Blue Bell, PA","Mgr, Security Data Engineer","2 days ago","2023-10-21T13:06:41.902Z","3.2","44859","$90,000 - $196,000 a year","2023-10-23T13:06:41.904Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=b1473aeba6e7b340&from=jasx&tk=1hdea9902imai801&vjs=3"
"The Church of Jesus Christ of Latter-day Saints","As a Data Extraction/Machine Learning Engineer at FamilySearch.org, you will be at the forefront of developing and implementing advanced algorithms, models, and data-driven solutions that enhance the data extraction capabilities, accuracy, and user experience of our platform. Working in close collaboration with a team of talented engineers, data scientists, and domain experts, you will help create and refine the AI-driven features that enable millions of users worldwide to make remarkable discoveries about their ancestors and family history. 
 We are looking for a passionate and dedicated candidate who shares our vision of advancing the Lord's work. You will have the opportunity to use cutting-edge technologies, access high-performance computing resources, and collaborate with highly skilled peers. This is a rare and rewarding chance to grow your career and personal skills while making a positive impact. 
 Why Join FamilySearch.org? 
 
  Impactful Mission: Be part of a meaningful mission to help individuals discover their family history and create lasting connections. 
  Cutting-Edge Technology: Work with state-of-the-art AI and machine learning technologies to push the boundaries of genealogical research. 
  Collaborative Environment: Join a team of passionate engineers, data scientists, and domain experts who value collaboration, innovation, and knowledge sharing. 
  Continuous Growth: Engage in professional development opportunities, attend conferences, and stay at the forefront of AI advancements. 
  Work-Life Balance: FamilySearch.org values work-life balance and promotes a flexible and supportive work environment. 
 
 If you're a highly motivated and creative thinker who is excited to make a tangible impact in the world of artificial intelligence and family history, we would love to hear from you! Join us at FamilySearch.org and be part of a transformative journey that connects generations and unlocks the power of AI to bring the past to life.
 


 Responsibilities: 
 
  Design, develop and deploy algorithms to extract genealogical data from the web. 
  Design, develop, and deploy AI and machine learning models that extract genealogical data from various web sources. 
  Collaborate with cross-functional teams to understand business requirements and translate them into actionable AI solutions. 
  Build scalable and efficient data pipelines for processing and analyzing large-scale genealogical data. 
  Perform data exploration, feature engineering, and model evaluation to identify optimal solutions for complex problems. 
  Stay up-to-date with the latest advancements in AI and machine learning techniques and proactively explore their potential applications to enhance FamilySearch.org.
 
 


 Required: 
 Education: 
 
  Bachelor’s degree in Computer Science, Artificial Intelligence, Machine Learning, or a related field. 
 
 Work Experience: 
 
  8+ years of industry-recognized, progressive, and relevant professional experience 
   
    Significant experience designing, training, testing, and deploying deep learning models in Computer Vision and Natural Language Processing domains. Experience with Speech Recognition is a plus. 
   
 
 Demonstrated Skills & Abilities: 
 
  Solid understanding of deep learning concepts and practices. 
  Solid programming skills in Python on the Linux platform. 
  Solid Java programming skills. 
  Excellent communication skills including the ability to create, communicate, and direct work toward accomplishing an overall technical vision. 
  Demonstrated ability to mentor and train peers. 
  Keen interest in foreign languages, scripts, and historical documents. Beginner level of at least one foreign language desired. 
  Excellent data manipulation skills using tools on the Linux platform (grep, sed, awk, NumPy, etc.). 
  Solid command of TensorFlow and/or PyTorch deep learning frameworks. 
  Ability to think about human language structurally. 
  Familiarity with cloud compute environments such as AWS. 
  Track record of self-teaching significant new concepts. 
  Proven ability to work effectively with people of various educational levels and backgrounds. Comfortable conversing with scientists, executives, engineers, and end users alike. 
  Ability to self-direct and work independently for extended periods as required. 
 
 Preferred Qualifications: 
 
  Master’s or PhD in Computer Science or a related field desired. 
 
 #LI-KS1
 


 
  Church employees find joy and satisfaction in using their unique talents and abilities to further the Lord’s work. From the IT professional who develops an app that sends the gospel message worldwide, to the facilities manager who maintains our buildings— giving Church members places to worship, teach, learn, and receive sacred ordinances—our employees seek innovative ways to share the gospel of Jesus Christ with the world. They are literally working in His kingdom. 
 
 
  Only members of the Church who are worthy of a temple recommend qualify for employment. Apart from this, the Church is an equal opportunity employer and does not discriminate in its employment decisions on any basis that would violate U.S. or local law. 
 
 
  Qualified applicants will be considered for employment without regard to race, national origin, color, gender, pregnancy, marital status, age, disability, genetic information, veteran status, or other legally protected categories that apply to the Church. The Church will make reasonable accommodations for qualified individuals with known disabilities.","<div>
 <p>As a Data Extraction/Machine Learning Engineer at FamilySearch.org, you will be at the forefront of developing and implementing advanced algorithms, models, and data-driven solutions that enhance the data extraction capabilities, accuracy, and user experience of our platform. Working in close collaboration with a team of talented engineers, data scientists, and domain experts, you will help create and refine the AI-driven features that enable millions of users worldwide to make remarkable discoveries about their ancestors and family history. </p>
 <p>We are looking for a passionate and dedicated candidate who shares our vision of advancing the Lord&apos;s work. You will have the opportunity to use cutting-edge technologies, access high-performance computing resources, and collaborate with highly skilled peers. This is a rare and rewarding chance to grow your career and personal skills while making a positive impact. </p>
 <p>Why Join FamilySearch.org? </p>
 <ul>
  <li>Impactful Mission: Be part of a meaningful mission to help individuals discover their family history and create lasting connections. </li>
  <li>Cutting-Edge Technology: Work with state-of-the-art AI and machine learning technologies to push the boundaries of genealogical research. </li>
  <li>Collaborative Environment: Join a team of passionate engineers, data scientists, and domain experts who value collaboration, innovation, and knowledge sharing. </li>
  <li>Continuous Growth: Engage in professional development opportunities, attend conferences, and stay at the forefront of AI advancements. </li>
  <li>Work-Life Balance: FamilySearch.org values work-life balance and promotes a flexible and supportive work environment. </li>
 </ul>
 <p>If you&apos;re a highly motivated and creative thinker who is excited to make a tangible impact in the world of artificial intelligence and family history, we would love to hear from you! Join us at FamilySearch.org and be part of a transformative journey that connects generations and unlocks the power of AI to bring the past to life.</p>
</div> 
<br>
<div>
 <p>Responsibilities: </p>
 <ul>
  <li>Design, develop and deploy algorithms to extract genealogical data from the web. </li>
  <li>Design, develop, and deploy AI and machine learning models that extract genealogical data from various web sources. </li>
  <li>Collaborate with cross-functional teams to understand business requirements and translate them into actionable AI solutions. </li>
  <li>Build scalable and efficient data pipelines for processing and analyzing large-scale genealogical data. </li>
  <li>Perform data exploration, feature engineering, and model evaluation to identify optimal solutions for complex problems. </li>
  <li>Stay up-to-date with the latest advancements in AI and machine learning techniques and proactively explore their potential applications to enhance FamilySearch.org.</li>
 </ul>
</div> 
<br>
<div>
 <p>Required: </p>
 <p>Education: </p>
 <ul>
  <li>Bachelor&#x2019;s degree in Computer Science, Artificial Intelligence, Machine Learning, or a related field. </li>
 </ul>
 <p>Work Experience: </p>
 <ul>
  <li>8+ years of industry-recognized, progressive, and relevant professional experience 
   <ul>
    <li>Significant experience designing, training, testing, and deploying deep learning models in Computer Vision and Natural Language Processing domains. Experience with Speech Recognition is a plus. </li>
   </ul></li>
 </ul>
 <p>Demonstrated Skills &amp; Abilities: </p>
 <ul>
  <li>Solid understanding of deep learning concepts and practices. </li>
  <li>Solid programming skills in Python on the Linux platform. </li>
  <li>Solid Java programming skills. </li>
  <li>Excellent communication skills including the ability to create, communicate, and direct work toward accomplishing an overall technical vision. </li>
  <li>Demonstrated ability to mentor and train peers. </li>
  <li>Keen interest in foreign languages, scripts, and historical documents. Beginner level of at least one foreign language desired. </li>
  <li>Excellent data manipulation skills using tools on the Linux platform (grep, sed, awk, NumPy, etc.). </li>
  <li>Solid command of TensorFlow and/or PyTorch deep learning frameworks. </li>
  <li>Ability to think about human language structurally. </li>
  <li>Familiarity with cloud compute environments such as AWS. </li>
  <li>Track record of self-teaching significant new concepts. </li>
  <li>Proven ability to work effectively with people of various educational levels and backgrounds. Comfortable conversing with scientists, executives, engineers, and end users alike. </li>
  <li>Ability to self-direct and work independently for extended periods as required. </li>
 </ul>
 <p>Preferred Qualifications: </p>
 <ul>
  <li>Master&#x2019;s or PhD in Computer Science or a related field desired. </li>
 </ul>
 <p>#LI-KS1</p>
</div> 
<br>
<div>
 <div>
  Church employees find joy and satisfaction in using their unique talents and abilities to further the Lord&#x2019;s work. From the IT professional who develops an app that sends the gospel message worldwide, to the facilities manager who maintains our buildings&#x2014; giving Church members places to worship, teach, learn, and receive sacred ordinances&#x2014;our employees seek innovative ways to share the gospel of Jesus Christ with the world. They are literally working in His kingdom. 
 </div>
 <div>
  Only members of the Church who are worthy of a temple recommend qualify for employment. Apart from this, the Church is an equal opportunity employer and does not discriminate in its employment decisions on any basis that would violate U.S. or local law. 
 </div>
 <div>
  Qualified applicants will be considered for employment without regard to race, national origin, color, gender, pregnancy, marital status, age, disability, genetic information, veteran status, or other legally protected categories that apply to the Church. The Church will make reasonable accommodations for qualified individuals with known disabilities.
 </div>
</div>","https://epej.fa.us2.oraclecloud.com/hcmUI/CandidateExperience/en/sites/CX_1001/requisitions/preview/357347","da36bc301acf1248",,"Full-time",,"3201 N Garden Dr, Lehi, UT 84043","FamilySearch Data Extraction/Machine Learning Engineer (US-based, Remote Optional)","5 days ago","2023-10-18T13:06:33.083Z","4.7","2108",,"2023-10-23T13:06:33.084Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=da36bc301acf1248&from=jasx&tk=1hdea988rk6dn800&vjs=3"
"NTT DATA","Job Description
  
  Fully Remote Us/Texas/Plano
  
  
 Job Description: 
 Role Responsibilities: 
 
  Gather, organize, and document metadata and data-related information, ensuring efficient data discovery and governance processes 
  Configure and implement a Catalog on Microsoft purview. Perform Scan, Search, Update, and other features with system components, evolving it for required user experience and performance. 
  Design and implement APIs that are fast and flexible 
  Work with solution architecture and developer team to build a core product experience that enables users to create, collaborate and monitor highly curated data from underlying sources 
  Work with solution architecture team to establish alignment with existing models and standards on a cloud platform
  
 Basic Qualifications: 
 
  Experience in Microsoft Azure Purview 
  Manage Purview guiding team on integration end points 
  Possess knowledge on Purview APIs 
  Customize feature to produce functional requirements based on project /client needs 
  CS degree or equivalent 
  At least 2-6 years of experience working as a software engineer 
  1-2+ years of experience configuring and implementing Data Catalog products and services within cloud platforms 
  Experience with REST APIs is a must 
  Experience with relational Databases, storges, data lakes on cloud 
  Understand how to translate business requirements and ""user needs"" into Catalog configuration.
  
 Preferences: 
 
  Consulting experience highly preferred 
  Industry specific consulting experience in Manufacturing 
  Hands-on experience in purview 
  Experience in deployment using CI/CD framework 
  Experience in Big Data project, usage of data catalog for data engineering capabilities 
  Experience working with offshore development teams preferred 
  Experience working on integration projects in cloud platforms 
  Good understanding of other services in Azure (ADF, Synapse, etc) 
  Completed a design, configure and implementation of a data governance project
  #LI-IST
  
  About NTT DATA Services:
  
  NTT DATA Services is a recognized leader in IT and business services, including cloud, data and applications, headquartered in Texas. As part of NTT DATA, a $30 billion trusted global innovator with a combined global reach of over 80 countries, we help clients transform through business and technology consulting, industry and digital solutions, applications development and management, managed edge-to-cloud infrastructure services, BPO, systems integration and global data centers. We are committed to our clients' long-term success. Visit nttdata.com or LinkedIn to learn more.
  
  NTT DATA Services is an equal opportunity employer and considers all applicants without regarding to race, color, religion, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other characteristic protected by law. We are committed to creating a diverse and inclusive environment for all employees. If you need assistance or an accommodation due to a disability, please inform your recruiter so that we may connect you with the appropriate team.
  
  Where required by law, NTT DATA provides a reasonable range of compensation for specific roles. The starting hourly range for this remote role is $71
  .00 This range reflects the minimum and maximum target compensation for the position across all US locations. Actual compensation will depend on several factors, including the candidate's actual work location, relevant experience, technical skills, and other qualifications. This position may also be eligible for incentive compensation based on individual and/or company performance.
  
  This position is eligible for company benefits that will depend on the nature of the role offered. Company benefits may include medical, dental, and vision insurance, flexible spending or health savings account, life, and AD&D insurance, short-and long-term disability coverage, paid time off, employee assistance, participation in a 401k program with company match, and additional voluntary or legally required benefits .","<div>
 Job Description
 <br> 
 <br> Fully Remote Us/Texas/Plano
 <br> 
 <br> 
 <b>Job Description:</b> 
 <b>Role Responsibilities:</b> 
 <ul>
  <li>Gather, organize, and document metadata and data-related information, ensuring efficient data discovery and governance processes</li> 
  <li>Configure and implement a Catalog on Microsoft purview. Perform Scan, Search, Update, and other features with system components, evolving it for required user experience and performance.</li> 
  <li>Design and implement APIs that are fast and flexible</li> 
  <li>Work with solution architecture and developer team to build a core product experience that enables users to create, collaborate and monitor highly curated data from underlying sources</li> 
  <li>Work with solution architecture team to establish alignment with existing models and standards on a cloud platform</li>
 </ul> 
 <b>Basic Qualifications:</b> 
 <ul>
  <li>Experience in Microsoft Azure Purview</li> 
  <li>Manage Purview guiding team on integration end points</li> 
  <li>Possess knowledge on Purview APIs</li> 
  <li>Customize feature to produce functional requirements based on project /client needs</li> 
  <li>CS degree or equivalent</li> 
  <li>At least 2-6 years of experience working as a software engineer</li> 
  <li>1-2+ years of experience configuring and implementing Data Catalog products and services within cloud platforms</li> 
  <li>Experience with REST APIs is a must</li> 
  <li>Experience with relational Databases, storges, data lakes on cloud</li> 
  <li>Understand how to translate business requirements and &quot;user needs&quot; into Catalog configuration.</li>
 </ul> 
 <b>Preferences:</b> 
 <ul>
  <li>Consulting experience highly preferred</li> 
  <li>Industry specific consulting experience in Manufacturing</li> 
  <li>Hands-on experience in purview</li> 
  <li>Experience in deployment using CI/CD framework</li> 
  <li>Experience in Big Data project, usage of data catalog for data engineering capabilities</li> 
  <li>Experience working with offshore development teams preferred</li> 
  <li>Experience working on integration projects in cloud platforms</li> 
  <li>Good understanding of other services in Azure (ADF, Synapse, etc)</li> 
  <li>Completed a design, configure and implementation of a data governance project</li>
 </ul> #LI-IST
 <br> 
 <br> About NTT DATA Services:
 <br> 
 <br> NTT DATA Services is a recognized leader in IT and business services, including cloud, data and applications, headquartered in Texas. As part of NTT DATA, a &#x24;30 billion trusted global innovator with a combined global reach of over 80 countries, we help clients transform through business and technology consulting, industry and digital solutions, applications development and management, managed edge-to-cloud infrastructure services, BPO, systems integration and global data centers. We are committed to our clients&apos; long-term success. Visit nttdata.com or LinkedIn to learn more.
 <br> 
 <br> NTT DATA Services is an equal opportunity employer and considers all applicants without regarding to race, color, religion, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other characteristic protected by law. We are committed to creating a diverse and inclusive environment for all employees. If you need assistance or an accommodation due to a disability, please inform your recruiter so that we may connect you with the appropriate team.
 <br> 
 <br> Where required by law, NTT DATA provides a reasonable range of compensation for specific roles. The starting hourly range for this remote role is &#x24;71
 <b> .</b>00 This range reflects the minimum and maximum target compensation for the position across all US locations. Actual compensation will depend on several factors, including the candidate&apos;s actual work location, relevant experience, technical skills, and other qualifications. This position may also be eligible for incentive compensation based on individual and/or company performance.
 <br> 
 <br> This position is eligible for company benefits that will depend on the nature of the role offered. Company benefits may include medical, dental, and vision insurance, flexible spending or health savings account, life, and AD&amp;D insurance, short-and long-term disability coverage, paid time off, employee assistance, participation in a 401k program with company match, and additional voluntary or legally required benefits .
</div>","https://www.indeed.com/rc/clk?jk=a1a3aa525e8c5dc2&atk=&xpse=SoCr67I3JucneWw4rZ0LbzkdCdPP","a1a3aa525e8c5dc2",,,,"Dallas, TX 75215","Azure Data Engineer","2 days ago","2023-10-21T13:06:42.908Z","3.5","3974",,"2023-10-23T13:06:42.909Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=a1a3aa525e8c5dc2&from=jasx&tk=1hdea9902imai801&vjs=3"
"Crossover Health Management Services, Inc.","About Crossover Health
 
  Crossover Health is creating the future of health as it should be. A national, team-based medical group with a focus on wellbeing and prevention that extends beyond traditional sick care, the company delivers an entirely new model of healthcare—Primary Health—built on the foundation of trusted relationships, an interdisciplinary care team approach, and outcomes-based payment. Crossover’s Primary Health model integrates primary care, physical medicine, mental health, health coaching, care navigation and more, and delivers care in surround-sound—in-person, virtually and via asynchronous messaging. Together we are building a community of members that embraces healthcare as a proactive part of their lifestyle.
 
  Job Summary
  Crossover’s Data Engineer is responsible for managing and developing data sources for analytics at scale. This is a critical role for supporting Crossover’s growing analytics team, and serves as the connection between Crossover’s Product and Technology teams, Data Science team, and data infrastructure vendors. In this role, the successful candidate will build out new data sources within the enterprise data warehouse, guide data modeling efforts for new and existing projects, and manage data ingress and egress between Crossover teams, clients, partners, and vendors. The ideal candidate will have experience with both clinical healthcare data as well as healthcare claims data.
 
  Job Responsibilities
 
   Develop and maintain data sources within Crossover’s enterprise data warehouse (inclusive of our current vendor and/or future data infrastructure)
   Assist with recommendations for data architecture, data storage, data integration, data quality, and data models
   Contribute to design sessions based on technical requirements, and build data models to clean and transform datasets for use by Crossover’s Data Science and Analytics teams
   Assist with ETL, ELT, and reverse-ETL design and development initiatives including data analysis, source-target mapping, data profiling, change data capture, QA testing, and performance tuning to guarantee quality and repeatability of data model results
   Create and maintain data model standards, including MDM (Master Data Management) and codebase standardization
   Migrate Enterprise Workloads to Snowflake using industry standard methodologies
   Automate and deploy as well as build CI/CD pipelines to support cloud based workload
   Design, deliver cloud native, hybrid, and multi-cloud Workloads
   Invest in documentation, including all system design, architecture and ongoing changes
   Design and support production job schedules, including alerting, monitoring, break fixes, and performance tuning
   Build solutions that are automated, scalable, and sustainable while minimizing defects and technical debt
   Assist stakeholders including analytics, design, product, and executive teams with data-related technical issues
   Ability to work independently with little instruction or direct oversight
   Perform other duties as assigned
 
 
  Minimum Qualifications
 
   Bachelors in Computer Science or Data Engineering, related degree, or equivalent professional experience
   3+ years relevant work experience within a complex, dynamic environment, with preference for experience with clinical healthcare data
   3+ years architecting , implementing, and supporting data infrastructure and topologies
 
 
   Experience building and operating highly available, distributed systems of extraction, ingestion, and processing of large data sets across a variety of applications (OLTP, OLAP and DSS)
   3+ years Experience with Data warehousing, methodologies, modeling techniques, design patterns, and technologies.
   Experience with data migration tools and deploying cloudbase solutions
   Experience in writing advanced SQL (DML & DDL), including Stored Procedures, Indexes, user defined functions, windows functions, correlated subqueries and CTE's, and related data query and management technology
   Coding ability in R, Python, and Shell Scripting to build and deploy Pipelines
   Working knowledge of Git, or similar collaborative code management software
   Experience with data integration tools such as FiveTran, DBT, Informatica, Matillion, or similar ETL/ELT tools
   Experience with Snowflake’s data platform
 
 
  Preferred Qualifications
 
   Masters in Computer Science, Data Engineering, or related degree
   Healthcare data acquisition, ingestion, processing, and analytics knowledge highly preferred
   Previous experience with health informatics, taxonomies, terminologies, and code sets
   Knowledge and understanding of product features: IAAS, PAAS and SAAS solutions
   Experience with healthcare claims data, formats, and analytics
   Experience with Health Catalyst’s data and analytics platform
   Experience with Tableau Cloud administration
   Experience with Master Data Management
   Understand Cloud Ecosystem
 
  The base pay range for this position is $91,428.00 to $105,143.00 per year. Pay range may vary depending on work location, applicable knowledge, skills, and experience. This position will be eligible for an annual bonus opportunity and comprehensive benefits package that includes Medical Insurance, Dental Insurance, Vision Insurance, Short- and Long-Term Disability, Life Insurance, Paid Time Off and 401K.
 
  Crossover Health is committed to Equal Employment Opportunity regardless of race, color, national origin, gender, sexual orientation, age, religion, veteran status, disability, history of disability or perceived disability. If you need assistance or an accommodation due to a disability, you may email us at careers@crossoverhealth.com.
 
  To all recruitment agencies: We do not accept unsolicited agency resumes and are not responsible for any fees related to unsolicited resumes.
  #LI-Remote","<div>
 <p><b>About Crossover Health</b></p>
 <p></p>
 <p> Crossover Health is creating the future of health as it should be. A national, team-based medical group with a focus on wellbeing and prevention that extends beyond traditional sick care, the company delivers an entirely new model of healthcare&#x2014;Primary Health&#x2014;built on the foundation of trusted relationships, an interdisciplinary care team approach, and outcomes-based payment. Crossover&#x2019;s Primary Health model integrates primary care, physical medicine, mental health, health coaching, care navigation and more, and delivers care in surround-sound&#x2014;in-person, virtually and via asynchronous messaging. Together we are building a community of members that embraces healthcare as a proactive part of their lifestyle.</p>
 <p></p>
 <p><b> Job Summary</b></p>
 <br> Crossover&#x2019;s Data Engineer is responsible for managing and developing data sources for analytics at scale. This is a critical role for supporting Crossover&#x2019;s growing analytics team, and serves as the connection between Crossover&#x2019;s Product and Technology teams, Data Science team, and data infrastructure vendors. In this role, the successful candidate will build out new data sources within the enterprise data warehouse, guide data modeling efforts for new and existing projects, and manage data ingress and egress between Crossover teams, clients, partners, and vendors. The ideal candidate will have experience with both clinical healthcare data as well as healthcare claims data.
 <p></p>
 <p><b> Job Responsibilities</b></p>
 <ul>
  <li><p> Develop and maintain data sources within Crossover&#x2019;s enterprise data warehouse (inclusive of our current vendor and/or future data infrastructure)</p></li>
  <li><p> Assist with recommendations for data architecture, data storage, data integration, data quality, and data models</p></li>
  <li><p> Contribute to design sessions based on technical requirements, and build data models to clean and transform datasets for use by Crossover&#x2019;s Data Science and Analytics teams</p></li>
  <li><p> Assist with ETL, ELT, and reverse-ETL design and development initiatives including data analysis, source-target mapping, data profiling, change data capture, QA testing, and performance tuning to guarantee quality and repeatability of data model results</p></li>
  <li><p> Create and maintain data model standards, including MDM (Master Data Management) and codebase standardization</p></li>
  <li><p> Migrate Enterprise Workloads to Snowflake using industry standard methodologies</p></li>
  <li><p> Automate and deploy as well as build CI/CD pipelines to support cloud based workload</p></li>
  <li><p> Design, deliver cloud native, hybrid, and multi-cloud Workloads</p></li>
  <li><p> Invest in documentation, including all system design, architecture and ongoing changes</p></li>
  <li><p> Design and support production job schedules, including alerting, monitoring, break fixes, and performance tuning</p></li>
  <li><p> Build solutions that are automated, scalable, and sustainable while minimizing defects and technical debt</p></li>
  <li><p> Assist stakeholders including analytics, design, product, and executive teams with data-related technical issues</p></li>
  <li><p> Ability to work independently with little instruction or direct oversight</p></li>
  <li><p> Perform other duties as assigned</p></li>
 </ul>
 <p></p>
 <p><b> Minimum Qualifications</b></p>
 <ul>
  <li><p> Bachelors in Computer Science or Data Engineering, related degree, or equivalent professional experience</p></li>
  <li><p> 3+ years relevant work experience within a complex, dynamic environment, with preference for experience with clinical healthcare data</p></li>
  <li><p> 3+ years architecting , implementing, and supporting data infrastructure and topologies</p></li>
 </ul>
 <ul>
  <li><p> Experience building and operating highly available, distributed systems of extraction, ingestion, and processing of large data sets across a variety of applications (OLTP, OLAP and DSS)</p></li>
  <li><p> 3+ years Experience with Data warehousing, methodologies, modeling techniques, design patterns, and technologies.</p></li>
  <li><p> Experience with data migration tools and deploying cloudbase solutions</p></li>
  <li><p> Experience in writing advanced SQL (DML &amp; DDL), including Stored Procedures, Indexes, user defined functions, windows functions, correlated subqueries and CTE&apos;s, and related data query and management technology</p></li>
  <li><p> Coding ability in R, Python, and Shell Scripting to build and deploy Pipelines</p></li>
  <li><p> Working knowledge of Git, or similar collaborative code management software</p></li>
  <li><p> Experience with data integration tools such as FiveTran, DBT, Informatica, Matillion, or similar ETL/ELT tools</p></li>
  <li><p> Experience with Snowflake&#x2019;s data platform</p></li>
 </ul>
 <p></p>
 <p><b> Preferred Qualifications</b></p>
 <ul>
  <li><p> Masters in Computer Science, Data Engineering, or related degree</p></li>
  <li><p> Healthcare data acquisition, ingestion, processing, and analytics knowledge highly preferred</p></li>
  <li><p> Previous experience with health informatics, taxonomies, terminologies, and code sets</p></li>
  <li><p> Knowledge and understanding of product features: IAAS, PAAS and SAAS solutions</p></li>
  <li><p> Experience with healthcare claims data, formats, and analytics</p></li>
  <li><p> Experience with Health Catalyst&#x2019;s data and analytics platform</p></li>
  <li><p> Experience with Tableau Cloud administration</p></li>
  <li><p> Experience with Master Data Management</p></li>
  <li><p> Understand Cloud Ecosystem</p></li>
 </ul>
 <p></p> The base pay range for this position is &#x24;91,428.00 to &#x24;105,143.00 per year. Pay range may vary depending on work location, applicable knowledge, skills, and experience. This position will be eligible for an annual bonus opportunity and comprehensive benefits package that includes Medical Insurance, Dental Insurance, Vision Insurance, Short- and Long-Term Disability, Life Insurance, Paid Time Off and 401K.
 <p></p>
 <p> Crossover Health is committed to Equal Employment Opportunity regardless of race, color, national origin, gender, sexual orientation, age, religion, veteran status, disability, history of disability or perceived disability. If you need assistance or an accommodation due to a disability, you may email us at careers@crossoverhealth.com.</p>
 <p></p>
 <p><b> To all recruitment agencies</b>: We do not accept unsolicited agency resumes and are not responsible for any fees related to unsolicited resumes.</p>
 <p></p> #LI-Remote
</div>","https://crossoverhealth.wd1.myworkdayjobs.com/Careers/job/Remote-USA/Data-Engineer_R23_774","d7a6a168405310c5",,"Full-time",,"101 West Avenida Vista Hermosa, San Clemente, CA 92672","Data Engineer","3 days ago","2023-10-20T13:06:40.241Z","3.4","47","$91,428 - $105,143 a year","2023-10-23T13:06:40.243Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=d7a6a168405310c5&from=jasx&tk=1hdea9902imai801&vjs=3"
"Brillio","Lead data engineer - R01530706 
 
 
  About Brillio: 
 
 
  Brillio is the partner of choice for many Fortune 1000 companies seeking to turn disruption into a competitive advantage through innovative digital adoption. Backed by Bain Capital, Brillio is one of the fastest growing digital technology service providers. We help clients harness the transformative potential of the four superpowers of technology - cloud computing, internet of things (IoT), artificial intelligence (AI), and mobility. Born digital in 2014, we apply Customer Experience Solutions, Data Analytics and AI, Digital Infrastructure and Security, and Platform and Product Engineering expertise to help clients quickly innovate for growth, create digital products, build service platforms, and drive smarter, data-driven performance. With delivery locations across United States, Romania, Canada, Mexico, and India, our growing global workforce of over 6,000 Brillians blends the latest technology and design thinking with digital fluency to solve complex business problems and drive competitive differentiation for our clients. Brillio was awarded ‘Great Place to Work’ in 2021 and 2022
 
 
 
   Lead data engineer
 
  Primary Skills
 
   SNS, SQS, Athena, CloudWatch, Kinesis, Redshift 
 
 Specialization
 
   AWS Data EngineerIng Advanced: Associate Data Engineer 
 
 Job requirements
 
   Role: Lead Data Engineer
 
 
   Years of Experience: 10+ years
 
 
   Travel Required: Yes
 
 
   Location: Remote
 
  
 
 
   As a consultant within the DIE team, you will work with our clients to define their digital strategy and execution roadmap, and design and implement differentiated digital solutions to help deliver measurable value.
 
  
 
 
   Your responsibilities in this role will include:
 
 
   Primary focus is on Glue, S3, Redshift, Lambda, PySpark, Spark. 
  Then added skillset which could add value are AWS Step function, NoSQL DB like Dynamo DB and AWS Data Migration Service in that order of priority.
   Data engineer should have at least 2 years of relevant AWS experience with their services mentioned above. 
  Experience in data security or governance and performance improvement is an added benefit
   Only focus is on AWS services and tech stack.""
 
 
 
   Why should you apply for this role?
 
 
   As Brillio continues to gain momentum as a trusted partner for our clients in their digital transformation journey, we strive to set new benchmarks for speed and value creation. The DIE team at Brillio is at the forefront of leading this charge by reimagining and executing how we structure, sell and deliver our services to better serve our clients.
 
  
 
 
   DAE: https://www.brillio.com/services-data-analytics/
 
 
 
   Know what it’s like to work and grow at Brillio:https://www.brillio.com/join-us/
 
 
 
   Equal Employment Opportunity Declaration
 
 
   Brillio is an equal opportunity employer to all, regardless of age, ancestry, colour, disability (mental and physical), exercising the right to family care and medical leave, gender, gender expression, gender identity, genetic information, marital status, medical condition, military or veteran status, national origin, political affiliation, race, religious creed, sex (includes pregnancy, childbirth, breastfeeding, and related medical conditions), and sexual orientation.
 
  
 
 
   #LI-RJ1
 
  
 
 
   Know what it’s like to work and grow at Brillio: Click here","<div>
 <h1 class=""jobSectionHeader""><b>Lead data engineer - R01530706</b></h1> 
 <p></p>
 <div>
  <b>About Brillio: </b>
 </div>
 <div>
  Brillio is the partner of choice for many Fortune 1000 companies seeking to turn disruption into a competitive advantage through innovative digital adoption. Backed by Bain Capital, Brillio is one of the fastest growing digital technology service providers. We help clients harness the transformative potential of the four superpowers of technology - cloud computing, internet of things (IoT), artificial intelligence (AI), and mobility. Born digital in 2014, we apply Customer Experience Solutions, Data Analytics and AI, Digital Infrastructure and Security, and Platform and Product Engineering expertise to help clients quickly innovate for growth, create digital products, build service platforms, and drive smarter, data-driven performance. With delivery locations across United States, Romania, Canada, Mexico, and India, our growing global workforce of over 6,000 Brillians blends the latest technology and design thinking with digital fluency to solve complex business problems and drive competitive differentiation for our clients. Brillio was awarded &#x2018;Great Place to Work&#x2019; in 2021 and 2022
 </div>
 <div></div>
 <div>
  <b><br> Lead data engineer</b>
 </div>
 <h6 class=""jobSectionHeader""><b> Primary Skills</b></h6>
 <ul>
  <li> SNS, SQS, Athena, CloudWatch, Kinesis, Redshift </li>
 </ul>
 <h6 class=""jobSectionHeader""><b>Specialization</b></h6>
 <ul>
  <li> AWS Data EngineerIng Advanced: Associate Data Engineer </li>
 </ul>
 <h6 class=""jobSectionHeader""><b>Job requirements</b></h6>
 <div>
  <b><br> Role:</b> Lead Data Engineer
 </div>
 <div>
  <b> Years of Experience:</b> 10+ years
 </div>
 <div>
  <b> Travel Required:</b> Yes
 </div>
 <div>
  <b> Location:</b> Remote
 </div>
 <div> 
 </div>
 <div>
   As a consultant within the DIE team, you will work with our clients to define their digital strategy and execution roadmap, and design and implement differentiated digital solutions to help deliver measurable value.
 </div>
 <div> 
 </div>
 <div>
  <b> Your responsibilities in this role will include:</b>
 </div>
 <ul>
  <li> Primary focus is on Glue, S3, Redshift, Lambda, PySpark, Spark. </li>
  <li>Then added skillset which could add value are AWS Step function, NoSQL DB like Dynamo DB and AWS Data Migration Service in that order of priority.</li>
  <li> Data engineer should have at least 2 years of relevant AWS experience with their services mentioned above. </li>
  <li>Experience in data security or governance and performance improvement is an added benefit</li>
  <li> Only focus is on AWS services and tech stack.&quot;</li>
 </ul>
 <div></div>
 <div>
  <b><br> Why should you apply for this role?</b>
 </div>
 <div>
   As Brillio continues to gain momentum as a trusted partner for our clients in their digital transformation journey, we strive to set new benchmarks for speed and value creation. The DIE team at Brillio is at the forefront of leading this charge by reimagining and executing how we structure, sell and deliver our services to better serve our clients.
 </div>
 <div> 
 </div>
 <div>
   DAE: https://www.brillio.com/services-data-analytics/
 </div>
 <div></div>
 <div>
  <br> Know what it&#x2019;s like to work and grow at Brillio:https://www.brillio.com/join-us/
 </div>
 <div></div>
 <div>
  <b><br> Equal Employment Opportunity Declaration</b>
 </div>
 <div>
   Brillio is an equal opportunity employer to all, regardless of age, ancestry, colour, disability (mental and physical), exercising the right to family care and medical leave, gender, gender expression, gender identity, genetic information, marital status, medical condition, military or veteran status, national origin, political affiliation, race, religious creed, sex (includes pregnancy, childbirth, breastfeeding, and related medical conditions), and sexual orientation.
 </div>
 <div> 
 </div>
 <div>
   #LI-RJ1
 </div>
 <div> 
 </div>
 <div>
   Know what it&#x2019;s like to work and grow at Brillio: Click here
 </div>
</div>
<p></p>","https://www.indeed.com/rc/clk?jk=1135c1400eced1b6&atk=&xpse=SoBD67I3JucnEZw4gp0LbzkdCdPP","1135c1400eced1b6",,,,"San Ramon, CA","Lead data engineer – R01530706","4 days ago","2023-10-19T13:06:42.949Z","3.4","170",,"2023-10-23T13:06:42.950Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=1135c1400eced1b6&from=jasx&tk=1hdea988rk6dn800&vjs=3"
"AIR HAMBURG Luftverkehrsgesellschaft mbH","Requisition ID
       
      
       2023-3757 
      
     
     
      
       City 
      
      
       Fort Lauderdale 
      
     
     
      
       Position Type
       
      
       Permanent Full-Time 
      
     
     
      
       Work Base
       
      
       Remote 
      
     
     
      
       Category
       
      
       Engineering 
      
     
    
   
  
 
 Job Profile 
 
  
   
    
     About Team
    
    
      The Data Foundation Team is highly critical for the organization to provide timely, accurate and most up to date data so that the business can take decisions accordingly. The team works with several application teams, data analytics, data science team etc. We are looking for a highly skilled and experienced 
     Senior Data Engineer to design, implement, and maintain robust and scalable data pipelines.
    
    
    
      About Company
    
    
      Vista Tech plays a vital role in the Vista group operations by delivering and accelerating comprehensive technology solutions across all brands. Vista’s end-to-end and click-to-flight solutions offer the industry's only comprehensive flight booking platform, seamlessly integrating global operations, and leveraging AI and machine learning to optimize pricing and fleet movement. Comprised of the Product Management, Engineering, and IT teams, Vista Tech’s mission is to enhance transparency and accessibility in private aviation through the development of the world's largest digital private aviation marketplace. In achieving this, Vista Tech always ensures the utmost safety and efficiency for FLIGHT CREW, EMPLOYEES and Members, while fostering a culture of innovation and excellence.
    
    
    
      You will report to Engineering Manager and play a crucial role in driving the technical direction of our projects and guiding the team in adopting best practices and cutting-edge technologies. This position is a 100% remote role with regular shif timings (9 AM to 6 AM EST). You will collaborate with cross-functional teams, provide technical leadership, and contribute to the entire software development lifecycle.
     
   
  
 
 Your Responsibilities 
 
  
   
    
     Scalable Data Infrastructure: Lead the development and maintenance of highly scalable data pipelines, playing a crucial role in fortifying our data foundation.
      Technical Excellence: Demonstrate hands-on technical expertise in designing, building, and documenting complex data pipelines while adhering to data engineering best practices.
      Cross-Functional Collaboration: Collaborate closely with data engineering, analytics, and data science leadership to continuously enhance the functionality and capabilities of our data systems.
      Process Optimization: Identify opportunities for internal process improvements, spearheading automation of manual tasks, optimizing data delivery mechanisms, and redesigning infrastructure to ensure greater scalability and efficiency.
      Data Integration Mastery: Define and construct the infrastructure necessary to facilitate efficient extraction, transformation, and loading (ETL) of data from a diverse array of sources.
     
   
  
 
 Required Skills, Qualifications, and Experience 
 
  
   
    
     
      
       Strong Analytical Foundation: A robust background in mathematics, statistics, computer science, data science, or a related discipline, showcasing your analytical prowess.
      
     
      
        Programming Proficiency: Advanced expertise in programming languages, particularly Python and SQL, to tackle complex data challenges.
      
     
      
        Production Experience: Proven experience in the production environment with a range of essential tools and platforms, including Snowflake, DBT, Airflow, Amazon Web Services (AWS), Docker/Kubernetes, and PostgreSQL.
      
     
      
        Database Mastery: Proficiency in database technologies, including Snowflake, PostgreSQL, Redshift, and others, enabling efficient data management.
      
     
      
        Exceptional Organizational Skills: Strong organizational capabilities, allowing you to manage multiple projects and priorities concurrently while consistently meeting deadlines.
      
     
      
        Additional Assets: Familiarity and experience with additional tools and technologies, such as AWS certification, Kafka Streaming/Kafka Connect, MongoDB, and CI/CD tools like GitLab, Jira, and Confluence, are highly advantageous.","<div></div>
<div>
 <div>
  <div>
   <div>
    <div>
     <div>
      <div>
       Requisition ID
      </div> 
      <div>
       2023-3757 
      </div>
     </div>
     <div>
      <div>
       City 
      </div>
      <div>
       Fort Lauderdale 
      </div>
     </div>
     <div>
      <div>
       Position Type
      </div> 
      <div>
       Permanent Full-Time 
      </div>
     </div>
     <div>
      <div>
       Work Base
      </div> 
      <div>
       Remote 
      </div>
     </div>
     <div>
      <div>
       Category
      </div> 
      <div>
       Engineering 
      </div>
     </div>
    </div>
   </div>
  </div>
 </div>
 <h2 class=""jobSectionHeader""><b>Job Profile </b></h2>
 <div>
  <div>
   <div>
    <div>
     <b>About Team</b>
    </div>
    <div>
      The Data Foundation Team is highly critical for the organization to provide timely, accurate and most up to date data so that the business can take decisions accordingly. The team works with several application teams, data analytics, data science team etc. We are looking for a highly skilled and experienced 
     <b>Senior Data Engineer</b> to design, implement, and maintain robust and scalable data pipelines.
    </div>
    <div></div>
    <div>
     <b><br> About Company</b>
    </div>
    <div>
      Vista Tech plays a vital role in the Vista group operations by delivering and accelerating comprehensive technology solutions across all brands. Vista&#x2019;s end-to-end and click-to-flight solutions offer the industry&apos;s only comprehensive flight booking platform, seamlessly integrating global operations, and leveraging AI and machine learning to optimize pricing and fleet movement. Comprised of the Product Management, Engineering, and IT teams, Vista Tech&#x2019;s mission is to enhance transparency and accessibility in private aviation through the development of the world&apos;s largest digital private aviation marketplace. In achieving this, Vista Tech always ensures the utmost safety and efficiency for FLIGHT CREW, EMPLOYEES and Members, while fostering a culture of innovation and excellence.
    </div>
    <div></div>
    <div>
     <br> You will report to Engineering Manager and play a crucial role in driving the technical direction of our projects and guiding the team in adopting best practices and cutting-edge technologies. This position is a 100% remote role with regular shif timings (9 AM to 6 AM EST). You will collaborate with cross-functional teams, provide technical leadership, and contribute to the entire software development lifecycle.
    </div> 
   </div>
  </div>
 </div>
 <h2 class=""jobSectionHeader""><b>Your Responsibilities </b></h2>
 <div>
  <div>
   <div>
    <ul>
     <li><b>Scalable Data Infrastructure:</b> Lead the development and maintenance of highly scalable data pipelines, playing a crucial role in fortifying our data foundation.</li>
     <li><b> Technical Excellence:</b> Demonstrate hands-on technical expertise in designing, building, and documenting complex data pipelines while adhering to data engineering best practices.</li>
     <li><b> Cross-Functional Collaboration:</b> Collaborate closely with data engineering, analytics, and data science leadership to continuously enhance the functionality and capabilities of our data systems.</li>
     <li><b> Process Optimization:</b> Identify opportunities for internal process improvements, spearheading automation of manual tasks, optimizing data delivery mechanisms, and redesigning infrastructure to ensure greater scalability and efficiency.</li>
     <li><b> Data Integration Mastery:</b> Define and construct the infrastructure necessary to facilitate efficient extraction, transformation, and loading (ETL) of data from a diverse array of sources.</li>
    </ul> 
   </div>
  </div>
 </div>
 <h2 class=""jobSectionHeader""><b>Required Skills, Qualifications, and Experience </b></h2>
 <div>
  <div>
   <div>
    <ul>
     <li>
      <div>
       <b>Strong Analytical Foundation:</b> A robust background in mathematics, statistics, computer science, data science, or a related discipline, showcasing your analytical prowess.
      </div></li>
     <li>
      <div>
       <b> Programming Proficiency:</b> Advanced expertise in programming languages, particularly Python and SQL, to tackle complex data challenges.
      </div></li>
     <li>
      <div>
       <b> Production Experience:</b> Proven experience in the production environment with a range of essential tools and platforms, including Snowflake, DBT, Airflow, Amazon Web Services (AWS), Docker/Kubernetes, and PostgreSQL.
      </div></li>
     <li>
      <div>
       <b> Database Mastery:</b> Proficiency in database technologies, including Snowflake, PostgreSQL, Redshift, and others, enabling efficient data management.
      </div></li>
     <li>
      <div>
       <b> Exceptional Organizational Skills:</b> Strong organizational capabilities, allowing you to manage multiple projects and priorities concurrently while consistently meeting deadlines.
      </div></li>
     <li>
      <div>
       <b> Additional Assets:</b> Familiarity and experience with additional tools and technologies, such as AWS certification, Kafka Streaming/Kafka Connect, MongoDB, and CI/CD tools like GitLab, Jira, and Confluence, are highly advantageous.
      </div></li>
    </ul>
   </div>
  </div>
 </div>
</div>
<div></div>","https://www.indeed.com/rc/clk?jk=70e539c357b570ae&atk=&xpse=SoAE67I3JucmuEQX2J0LbzkdCdPP","70e539c357b570ae",,"Full-time",,"Fort Lauderdale, FL","SENIOR DATA ENGINEER","4 days ago","2023-10-19T13:06:49.585Z",,,,"2023-10-23T13:06:49.589Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=70e539c357b570ae&from=jasx&tk=1hdea988rk6dn800&vjs=3"
"Western Governors University","The salary range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.
 
 
 
   At WGU, it is not typical for an individual to be hired at or near the top of the range for their role, and compensation decisions are dependent on the facts and circumstances of each case. A reasonable estimate of the current range is:
 
  Pay Range: $127,700.00 - $191,500.00
 
   If you’re passionate about building a better future for individuals, communities, and our country—and you’re committed to working hard to play your part in building that future—consider WGU as the next step in your career.
 
 
 
   Driven by a mission to expand access to higher education through online, competency-based degree programs, WGU is also committed to being a great place to work for a diverse workforce of student-focused professionals. The university has pioneered a new way to learn in the 21st century, one that has received praise from academic, industry, government, and media leaders. Whatever your role, working for WGU gives you a part to play in helping students graduate, creating a better tomorrow for themselves and their families.
 
 
   Current WGU employees should submit an internal application before 10/27/2023 to be considered.
 
 
 
   The Staff Data Engineer should be agnostic to tools and should be able to supervise, design, architect and code using Apache Spark and other cloud technologies. The position will supervise and design how data will flow through hybrid data environments comprised of open-source big data platforms and traditional database systems. The core responsibility for this position includes supervision of data engineering technical aspects, design of data and system architecture for the Data Lake and data warehouse, supervision of the technical aspects of a data engineering team and projects encompassing dimensional and normalized data modeling. The Staff Data Engineer will improve technical standards in the environment ensuring optimal use of data warehouse and other data stores to solve business problems. They will serve as the lead engineer and go to person for all aspects of the data engineer team including solution architecture of data systems.
 
 
 
   Essential Functions and Responsibilities:
 
 
  
   
     Supervise work on cloud technologies and architect scalable and performant Data Lake systems.
   
  
   
     Establish design and methodology for database build processes.
   
  
   
     Supervise the architecture and design of complete data model solutions.
   
  
   
     Supervise necessary data protection and security processes.
   
  
   
     Create and design extract processes for data access layer.
   
  
   
     Translate business problems/information requirements accurately to logical/physical data models aligning with customers’ data architecture standards.
   
  
   
     Supervise and perform research and analysis to find solutions for complex business problems.
   
  
   
     Monitor job performance and fine tune Spark SQL queries as appropriate on a regular basis.
   
  
   
     Supervise the profiling of data, the publishing of data profiles and corrective actions if required to ensure data quality.
   
  
   
     Supervise and perform documentation / reverse engineering / analysis of data mapping using data integration code/tools.
   
  
   
     Work with APIs for data wrangling and integrations with other systems data in the EDW.
   
  
   
     Perform impact analysis using Data Integration/Data Virtualization tool repositories, DB data dictionary, UNIX scripts and frontend code on versioning systems.
   
  
   
     Analyze / research data on multiple platforms as wells as multiple heterogeneous databases including custom developed databases.
   
  
   
     Positively impact projects by completing tasks assigned on time.
   
  
   
     Communicate technical and domain knowledge as it relates to work, to both technical and non-technical audiences.
   
  
   
     Ingest and transform structured, semi-structured, and unstructured data from sources including relational databases, NoSQL, external APIs, JSON, XML, delimited files, and more.
   
  
   
     Support business and functional requirements and translate these requirements into robust, scalable, solutions.
   
  
   
     Collaborate with engineers to help adopt best practices in data system creation, data integrity, test design, analysis, validation, and documentation.
   
  
   
     Help continually improve ongoing reporting and analysis processes, automate, or simplify self-service modelling and production support for customers.
   
  
   
     Performs other related duties as assigned.
   
 
 
 
   Knowledge, Skill and Abilities:
 
 
  
   
     Expertise with analytical reporting tools, preferably Cognos and Tableau.
   
  
   
     Mastery in code based ETL/ ELT tools for importing and exporting data across disparate systems.
   
  
   
     Expertise in analytic skills related to working with unstructured datasets.
   
  
   
     Use of industry best practices for code development, testing, implementation and documentation.
   
  
   
     Ability to evaluate and prioritize work based on the organization’s needs.
   
  
   
     Ability to supervise cross team projects to accomplish data integrations and pipelines.
   
  
   
     Supervisory abilities for data engineering team with respect to technical design and architecture.
   
  
   
     Excellent verbal & written communication, along with technical documentation
   
  
   
     Ability to work and deliver in a team environment
   
  
   
     Ability to manage the use of tools like Jira, Confluence, GitHub
   
  
   
     Architect and Develop processes for audit of Data Integrity
   
  
   
     Ability to mentor Associate/Senior/Data Engineer in data pipeline architecture and coding standards
   
  
   
     Supervise Validation and testing to analyze and debug issues
   
  
   
     Mastery of AWS cloud technologies, REST API, and HTML5
   
  
   
     Mastery of relational SQL and NoSQL databases
   
  
   
     Mastery with object-oriented/object function scripting languages: Python, Java, Scala
   
  
   
     Mastery of big data tools: Hadoop, Spark, Kafka, Databricks, etc.
   
 
 
 
   Competencies:
 
 
   Organizational or Student Impact:
 
 
  
   
     Recommends and implements changes in technical/business processes; identifies areas for improvement.
   
  
   
     Helps lead/coordinate extremely complex technical projects and programs and leads development and implementation of innovative solutions for specialized technical issues.
   
  
   
     Works proactively; identifies and helps prevent/ solve problems that may cross disciplines.
   
  
   
     Fully understands and quantifies project risks with impact. Identifies, generates, and implements innovative solutions.
   
 
 
   Problem Solving & Decision Making:
 
 
  
   
     This individual accomplishes goals and objectives independently.
   
  
   
     Builds and leads teams, influencing decisions and results.
   
  
   
     Uses discretion to fully scope, design, and implement solutions to complex technical problems.
   
  
   
     The individual provides regular technical advice and direction to technical teams and management.
   
  
   
     Models and helps set high standards for effective interactions with internal and external individuals.
   
 
 
   Communication & Influence:
 
 
  
   
     Communicates with parties within and outside of their job function and typically has responsibilities for communicating with parties external to the organization.
   
  
   
     Works to influence others to accept and understand new concepts, practices, and approaches. Requires ability to communicate with executive leadership regarding matters of significant importance to the organization.
   
  
   
     This individual may conduct briefings with senior leaders within the technical function.
   
 
 
   Leadership:
 
 
  
   
     Frequently responsible for providing guidance, coaching, and training to other employees across the Company within the area of expertise.
   
  
   
     Responsible for managing large, complex project initiatives or strategically important solutions to the organization, involving large cross-functional teams.
   
  
   
     May have direct reports but generally fewer than three.
   
 
 
 
   Job Qualifications:
 
 
  
   
     M. S. in Business, Management Information Systems, Computer Science, or a related field, or an equivalent combination of experience and training.
   
  
   
     Seven or more years of experience as a Data Engineer, Data Integration, Big Data, or Business Intelligence Engineer with a background as a Software Engineer.
   
 
 
 
   Preferred Qualifications:
 
 
  
   
     Strong experience with distance education and distance learning students is preferred.
   
  
   
     Higher Education domain knowledge
   
  
   
     Experience as a Lead or Staff Data Engineer
   
 
 
 
   #LI-REMOTE
 
 
   #LI-ZARD
 
 
   As an equal opportunity employer, WGU recognizes that our strength lies in our people. We are committed to diversity.","<div>
 <div>
  The salary range for this role takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.
 </div>
 <div></div>
 <div>
   At WGU, it is not typical for an individual to be hired at or near the top of the range for their role, and compensation decisions are dependent on the facts and circumstances of each case. A reasonable estimate of the current range is:
 </div>
 <br> Pay Range: &#x24;127,700.00 - &#x24;191,500.00
 <div>
  <br> If you&#x2019;re passionate about building a better future for individuals, communities, and our country&#x2014;and you&#x2019;re committed to working hard to play your part in building that future&#x2014;consider WGU as the next step in your career.
 </div>
 <div></div>
 <div>
   Driven by a mission to expand access to higher education through online, competency-based degree programs, WGU is also committed to being a great place to work for a diverse workforce of student-focused professionals. The university has pioneered a new way to learn in the 21st century, one that has received praise from academic, industry, government, and media leaders. Whatever your role, working for WGU gives you a part to play in helping students graduate, creating a better tomorrow for themselves and their families.
 </div>
 <div>
   Current WGU employees should submit an internal application before 10/27/2023 to be considered.
 </div>
 <div></div>
 <div>
   The Staff Data Engineer should be agnostic to tools and should be able to supervise, design, architect and code using Apache Spark and other cloud technologies. The position will supervise and design how data will flow through hybrid data environments comprised of open-source big data platforms and traditional database systems. The core responsibility for this position includes supervision of data engineering technical aspects, design of data and system architecture for the Data Lake and data warehouse, supervision of the technical aspects of a data engineering team and projects encompassing dimensional and normalized data modeling. The Staff Data Engineer will improve technical standards in the environment ensuring optimal use of data warehouse and other data stores to solve business problems. They will serve as the lead engineer and go to person for all aspects of the data engineer team including solution architecture of data systems.
 </div>
 <div></div>
 <div>
   Essential Functions and Responsibilities:
 </div>
 <ul>
  <li>
   <div>
     Supervise work on cloud technologies and architect scalable and performant Data Lake systems.
   </div></li>
  <li>
   <div>
     Establish design and methodology for database build processes.
   </div></li>
  <li>
   <div>
     Supervise the architecture and design of complete data model solutions.
   </div></li>
  <li>
   <div>
     Supervise necessary data protection and security processes.
   </div></li>
  <li>
   <div>
     Create and design extract processes for data access layer.
   </div></li>
  <li>
   <div>
     Translate business problems/information requirements accurately to logical/physical data models aligning with customers&#x2019; data architecture standards.
   </div></li>
  <li>
   <div>
     Supervise and perform research and analysis to find solutions for complex business problems.
   </div></li>
  <li>
   <div>
     Monitor job performance and fine tune Spark SQL queries as appropriate on a regular basis.
   </div></li>
  <li>
   <div>
     Supervise the profiling of data, the publishing of data profiles and corrective actions if required to ensure data quality.
   </div></li>
  <li>
   <div>
     Supervise and perform documentation / reverse engineering / analysis of data mapping using data integration code/tools.
   </div></li>
  <li>
   <div>
     Work with APIs for data wrangling and integrations with other systems data in the EDW.
   </div></li>
  <li>
   <div>
     Perform impact analysis using Data Integration/Data Virtualization tool repositories, DB data dictionary, UNIX scripts and frontend code on versioning systems.
   </div></li>
  <li>
   <div>
     Analyze / research data on multiple platforms as wells as multiple heterogeneous databases including custom developed databases.
   </div></li>
  <li>
   <div>
     Positively impact projects by completing tasks assigned on time.
   </div></li>
  <li>
   <div>
     Communicate technical and domain knowledge as it relates to work, to both technical and non-technical audiences.
   </div></li>
  <li>
   <div>
     Ingest and transform structured, semi-structured, and unstructured data from sources including relational databases, NoSQL, external APIs, JSON, XML, delimited files, and more.
   </div></li>
  <li>
   <div>
     Support business and functional requirements and translate these requirements into robust, scalable, solutions.
   </div></li>
  <li>
   <div>
     Collaborate with engineers to help adopt best practices in data system creation, data integrity, test design, analysis, validation, and documentation.
   </div></li>
  <li>
   <div>
     Help continually improve ongoing reporting and analysis processes, automate, or simplify self-service modelling and production support for customers.
   </div></li>
  <li>
   <div>
     Performs other related duties as assigned.
   </div></li>
 </ul>
 <div></div>
 <div>
   Knowledge, Skill and Abilities:
 </div>
 <ul>
  <li>
   <div>
     Expertise with analytical reporting tools, preferably Cognos and Tableau.
   </div></li>
  <li>
   <div>
     Mastery in code based ETL/ ELT tools for importing and exporting data across disparate systems.
   </div></li>
  <li>
   <div>
     Expertise in analytic skills related to working with unstructured datasets.
   </div></li>
  <li>
   <div>
     Use of industry best practices for code development, testing, implementation and documentation.
   </div></li>
  <li>
   <div>
     Ability to evaluate and prioritize work based on the organization&#x2019;s needs.
   </div></li>
  <li>
   <div>
     Ability to supervise cross team projects to accomplish data integrations and pipelines.
   </div></li>
  <li>
   <div>
     Supervisory abilities for data engineering team with respect to technical design and architecture.
   </div></li>
  <li>
   <div>
     Excellent verbal &amp; written communication, along with technical documentation
   </div></li>
  <li>
   <div>
     Ability to work and deliver in a team environment
   </div></li>
  <li>
   <div>
     Ability to manage the use of tools like Jira, Confluence, GitHub
   </div></li>
  <li>
   <div>
     Architect and Develop processes for audit of Data Integrity
   </div></li>
  <li>
   <div>
     Ability to mentor Associate/Senior/Data Engineer in data pipeline architecture and coding standards
   </div></li>
  <li>
   <div>
     Supervise Validation and testing to analyze and debug issues
   </div></li>
  <li>
   <div>
     Mastery of AWS cloud technologies, REST API, and HTML5
   </div></li>
  <li>
   <div>
     Mastery of relational SQL and NoSQL databases
   </div></li>
  <li>
   <div>
     Mastery with object-oriented/object function scripting languages: Python, Java, Scala
   </div></li>
  <li>
   <div>
     Mastery of big data tools: Hadoop, Spark, Kafka, Databricks, etc.
   </div></li>
 </ul>
 <div></div>
 <div>
   Competencies:
 </div>
 <div>
   Organizational or Student Impact:
 </div>
 <ul>
  <li>
   <div>
     Recommends and implements changes in technical/business processes; identifies areas for improvement.
   </div></li>
  <li>
   <div>
     Helps lead/coordinate extremely complex technical projects and programs and leads development and implementation of innovative solutions for specialized technical issues.
   </div></li>
  <li>
   <div>
     Works proactively; identifies and helps prevent/ solve problems that may cross disciplines.
   </div></li>
  <li>
   <div>
     Fully understands and quantifies project risks with impact. Identifies, generates, and implements innovative solutions.
   </div></li>
 </ul>
 <div>
   Problem Solving &amp; Decision Making:
 </div>
 <ul>
  <li>
   <div>
     This individual accomplishes goals and objectives independently.
   </div></li>
  <li>
   <div>
     Builds and leads teams, influencing decisions and results.
   </div></li>
  <li>
   <div>
     Uses discretion to fully scope, design, and implement solutions to complex technical problems.
   </div></li>
  <li>
   <div>
     The individual provides regular technical advice and direction to technical teams and management.
   </div></li>
  <li>
   <div>
     Models and helps set high standards for effective interactions with internal and external individuals.
   </div></li>
 </ul>
 <div>
   Communication &amp; Influence:
 </div>
 <ul>
  <li>
   <div>
     Communicates with parties within and outside of their job function and typically has responsibilities for communicating with parties external to the organization.
   </div></li>
  <li>
   <div>
     Works to influence others to accept and understand new concepts, practices, and approaches. Requires ability to communicate with executive leadership regarding matters of significant importance to the organization.
   </div></li>
  <li>
   <div>
     This individual may conduct briefings with senior leaders within the technical function.
   </div></li>
 </ul>
 <div>
   Leadership:
 </div>
 <ul>
  <li>
   <div>
     Frequently responsible for providing guidance, coaching, and training to other employees across the Company within the area of expertise.
   </div></li>
  <li>
   <div>
     Responsible for managing large, complex project initiatives or strategically important solutions to the organization, involving large cross-functional teams.
   </div></li>
  <li>
   <div>
     May have direct reports but generally fewer than three.
   </div></li>
 </ul>
 <div></div>
 <div>
   Job Qualifications:
 </div>
 <ul>
  <li>
   <div>
     M. S. in Business, Management Information Systems, Computer Science, or a related field, or an equivalent combination of experience and training.
   </div></li>
  <li>
   <div>
     Seven or more years of experience as a Data Engineer, Data Integration, Big Data, or Business Intelligence Engineer with a background as a Software Engineer.
   </div></li>
 </ul>
 <div></div>
 <div>
   Preferred Qualifications:
 </div>
 <ul>
  <li>
   <div>
     Strong experience with distance education and distance learning students is preferred.
   </div></li>
  <li>
   <div>
     Higher Education domain knowledge
   </div></li>
  <li>
   <div>
     Experience as a Lead or Staff Data Engineer
   </div></li>
 </ul>
 <div></div>
 <div>
   #LI-REMOTE
 </div>
 <div>
   #LI-ZARD
 </div>
 <div>
  <br> As an equal opportunity employer, WGU recognizes that our strength lies in our people. We are committed to diversity.
 </div>
</div>","https://www.indeed.com/rc/clk?jk=6d6399c51c6d5573&atk=&xpse=SoAD67I3JucmYWQiGR0LbzkdCdPP","6d6399c51c6d5573",,"Full-time",,"Remote","Staff Data Engineer","4 days ago","2023-10-19T13:06:51.850Z","3.6","498","$127,700 - $191,500 a year","2023-10-23T13:06:51.852Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=6d6399c51c6d5573&from=jasx&tk=1hdea988rk6dn800&vjs=3"
"Gridiron IT","Seeking a Junior Data Engineer on a remote basis. Secret clearance is required. 
Overview: We are looking to immediately fill a Junior Data Engineer on our team. The ADE is one of the major pillars of MyNavy HR Transformation and serves as an enterprise-wide centralized repository that provides seamless and secure data access. This pilot’s objective is to support defining a comprehensive future-state ADE data model by informing the total number of unique data elements and to assess the ability to accelerate the data integration process using new data tools available following the Authority to Operate (ATO).
Minimum Qualifications:

 2+ years of experience with scalable ETL workflows/development, extract, cleanse, and process disparate data sources
 Secret Clearance is required.
 HS Diploma required (Bachelor's preferred)
 Experience with cleaning and transforming data utilizing Python and/or SQL, specifically complex SQL queries
 Familiarity with acquiring data from disparate data sources using APIs and SQL
 Ability to develop scripts and programs for converting various types of data into usable formats and support project team to scale, monitor, and operate data platforms

Job Type: Full-time
Pay: $48.00 - $52.00 per hour
Benefits:

 401(k)
 401(k) matching
 Dental insurance
 Health insurance
 Vision insurance

Schedule:

 8 hour shift

Work Location: Remote","<p><b>Seeking a Junior Data Engineer on a remote basis. </b><br><b>Secret clearance is required. </b></p>
<p><b>Overview:</b> We are looking to immediately fill a Junior Data Engineer on our team. The ADE is one of the major pillars of MyNavy HR Transformation and serves as an enterprise-wide centralized repository that provides seamless and secure data access. This pilot&#x2019;s objective is to support defining a comprehensive future-state ADE data model by informing the total number of unique data elements and to assess the ability to accelerate the data integration process using new data tools available following the Authority to Operate (ATO).</p>
<p><b>Minimum Qualifications:</b></p>
<ul>
 <li>2+ years of experience with scalable ETL workflows/development, extract, cleanse, and process disparate data sources</li>
 <li>Secret Clearance is required.</li>
 <li>HS Diploma required (Bachelor&apos;s preferred)</li>
 <li>Experience with cleaning and transforming data utilizing Python and/or SQL, specifically complex SQL queries</li>
 <li>Familiarity with acquiring data from disparate data sources using APIs and SQL</li>
 <li>Ability to develop scripts and programs for converting various types of data into usable formats and support project team to scale, monitor, and operate data platforms</li>
</ul>
<p>Job Type: Full-time</p>
<p>Pay: &#x24;48.00 - &#x24;52.00 per hour</p>
<p>Benefits:</p>
<ul>
 <li>401(k)</li>
 <li>401(k) matching</li>
 <li>Dental insurance</li>
 <li>Health insurance</li>
 <li>Vision insurance</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
</ul>
<p>Work Location: Remote</p>",,"45a231ffe43c577a",,"Full-time",,"Remote","Jr Data Engineer - Secret Cleared","3 days ago","2023-10-20T13:06:55.139Z","4.2","17","$48 - $52 an hour","2023-10-23T13:06:55.140Z","US","remote","data engineer","https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0CTHA6cd59lXtQJ-DuZtBHQsSjOn019HaVEc20FtZol16Ry6pW_HxgBH8El4_gqC7uB205Bc4zOhTy7DBwJrfD44a3c9HUREaGRFI5ejKinAPOL_vJfvFL9nNjZP6gLraBPcFDt0aZzYrykvYsV_cRLZ2eQszrPLV-htZd4Np3oZsEDg_gp_3mWkyPcjDEu8-28CAPYcseLdyLUCsdilU4lflT5HyIa79_Q0FfY6yyKdmZ5oDKVgX863YoRvUBSc0YZaXHQhSaXUz0nzd_0CESpz46Me0O0E3IID_axiDaGxASt9PCplqXE-OyFtR2Dr8CTx7aoi4bnDRFmGwynCUWiUBjGGsXYFJMQDmFn4RtjeP3prX4kK9ecusgpMpDepjEIifKktlvT74d7_x9kKJ7q_88nFi8Rwq0MgKylqawKkrV41TxQz3QWI4ObwnWisyb9t-YwDk0ghGAdKXlktWU52V9QHNuGj2LY10iWSljcLgS3EULWji-JPNa266oxTm02FcyKD31Xq47EUJZLhhpQFkvGXW75RzsoAGnS3Bd2gr7C5YJjK-Oo82KT2wDALfaNZ5zEcfsQUu3T32PUBNCnZD9DQgNNQXOirSb3QzrAiWycawMpj87BwVVcdLh7flg%3D&xkcb=SoBK-_M3Jucq0MwcyR0JbzkdCdPP&p=1&fvj=1&vjs=3&jsa=4945&tk=1hdea988rk6dn800&from=jasx&wvign=1"
"Verizon","When you join Verizon
  Verizon is one of the world’s leading providers of technology and communications services, transforming the way we connect around the world. We’re a human network that reaches across the globe and works behind the scenes. We anticipate, lead, and believe that listening is where learning begins. In crisis and in celebration, we come together—lifting up our communities and striving to make an impact to move the world forward. If you’re fueled by purpose, and powered by persistence, explore a career with us. Here, you’ll discover the rigor it takes to make a difference and the fulfillment that comes with living the #NetworkLife.
 
  What you’ll be doing...
 
  Are you ready for the challenge of driving transformation at the scale of a $130B+ Fortune 50 company through an industry leading technology stack?
 
  In this role, you will be responsible for Technical Product Management for Verizon’s Corporate end to end master data management capabilities, focused on building a robust human centered experience spanning across the Supply Chain, Finance, and HR data domains. Trusted data starts with complete, accurate and valid master data. Your work will focus on shaping human centered end to end processes from identification of the need for new master data, through to its request and eventual creation / maintenance including full accountability for product vision & strategy, roadmap and backlogs around key mission objectives, delivering features from concept to launch through agile delivery methodologies, ensuring value realization, run governance and support that delivers outstanding business results / user experience. Partnership is essential in this role, as we focus on shaping a federated data strategy that requires the definition of data standards in partnership with various GPOs within the Finance, Supply Chain and HR domains, collaboration in the establishment of data catalogs that enable end users to leverage plain business language to query their data, and connectivity with owners of commercial and other non-corporate data to which the corporate functions will subscribe.
 
  The ideal candidate for this role will be equal parts technical and functional leader to be successful, with the ability to set rigorous technical / architectural standards whilst speaking the language of our business partners to influence the strategic direction, master data and governance capabilities. Focusing on driving the innovation agenda, this leader will work with internal & external industry leaders to shape world-class capabilities and enable continuous improvement.
 
  This Principal Engineer will drive the MDM strategy with other MDM SME’s, Analysts and 3rd party partners, as well as influence Global Process Owners to deliver transformative solutions.
 
  Responsibilities 
 
  Collaborating with cross-functional teams to define and enforce data governance policies, standards, and best practices within Oracle EDMCS and SAP MDG.
   Utilizing your deep knowledge of Oracle EDMCS and SAP MDG to configure and maintain workflows, and validation rules.
   Being proficient in Google Cloud Platform (GCP) services and tools, including BigQuery, Dataflow, Dataproc, and Pub/Sub.
   Working closely with IT and Business teams to ensure seamless data integration between Oracle EDMCS, SAP MDG, and other enterprise systems.
   Developing and maintaining data mapping and transformation rules to ensure data consistency and compliance.
   Collaborating with data stewards and business users to resolve data-related issues and support data maintenance activities as well as own function design fo the end to end master data lifecycle mgmt
   Creating and maintaining comprehensive documentation for data management processes, configurations, and standard operating procedures.
 
 
  What we’re looking for...  You’ll need to have:
 
   Bachelor’s degree or four or more years of work experience.
   Six or more years of relevant work experience
   Experience with Oracle EDMCS/DRM and SAP MDG.
   Experience in managing or executing data cleansing, data mapping, and data governance areas, preferably in an SAP environment as well as integration across complex ERP landscapes.
   Experience of interfacing SAP with legacy and modern data ecosystems.
   Experience in data modeling, data mapping, and data transformation.
   Experience in data governance, data quality management, and MDM concepts.
 
 
  Even better if you have one or more of the following:
 
   SAP MDG or Oracle EDMCS certification is a plus.
   Working knowledge of SAP finance modules.
   Experience with Collibra workflows and master data modules.
   Experience in successful delivery of scaled agile.
   Scripting and programming skills in languages like Python, Java, or Scala.
   Experience as a Data Engineer or similar role with a focus on GCP. Strong knowledge of cloud-based data storage solutions, such as Cloud Storage, Cloud SQL, and Bigtable.
   Knowledge of data privacy regulations and compliance standards (e.g., GDPR, CCPA).
 
 
  If Verizon and this role sound like a fit for you, we encourage you to apply even if you don’t meet every “even better” qualification listed above.  This role may be considered as part of the Department of Defense SkillBridge Program.
 
 
  
   
    
     
      
       
         Where you’ll be working
       
      
     
    
   
  
  In this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager.
 
  Scheduled Weekly Hours 40
 
  Equal Employment Opportunity
  We’re proud to be an equal opportunity employer - and celebrate our employees’ differences, including race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, and Veteran status. At Verizon, we know that diversity makes us stronger. We are committed to a collaborative, inclusive environment that encourages authenticity and fosters a sense of belonging. We strive for everyone to feel valued, connected, and empowered to reach their potential and contribute their best. Check out our diversity and inclusion page to learn more.","<div>
 <h3 class=""jobSectionHeader""><b>When you join Verizon</b></h3>
 <p> Verizon is one of the world&#x2019;s leading providers of technology and communications services, transforming the way we connect around the world. We&#x2019;re a human network that reaches across the globe and works behind the scenes. We anticipate, lead, and believe that listening is where learning begins. In crisis and in celebration, we come together&#x2014;lifting up our communities and striving to make an impact to move the world forward. If you&#x2019;re fueled by purpose, and powered by persistence, explore a career with us. Here, you&#x2019;ll discover the rigor it takes to make a difference and the fulfillment that comes with living the #NetworkLife.</p>
 <p></p>
 <h3 class=""jobSectionHeader""><b> What you&#x2019;ll be doing...</b></h3>
 <p></p>
 <p> Are you ready for the challenge of driving transformation at the scale of a &#x24;130B+ Fortune 50 company through an industry leading technology stack?</p>
 <p></p>
 <p> In this role, you will be responsible for Technical Product Management for Verizon&#x2019;s Corporate end to end master data management capabilities, focused on building a robust human centered experience spanning across the Supply Chain, Finance, and HR data domains. Trusted data starts with complete, accurate and valid master data. Your work will focus on shaping human centered end to end processes from identification of the need for new master data, through to its request and eventual creation / maintenance including full accountability for product vision &amp; strategy, roadmap and backlogs around key mission objectives, delivering features from concept to launch through agile delivery methodologies, ensuring value realization, run governance and support that delivers outstanding business results / user experience. Partnership is essential in this role, as we focus on shaping a federated data strategy that requires the definition of data standards in partnership with various GPOs within the Finance, Supply Chain and HR domains, collaboration in the establishment of data catalogs that enable end users to leverage plain business language to query their data, and connectivity with owners of commercial and other non-corporate data to which the corporate functions will subscribe.</p>
 <p></p>
 <p> The ideal candidate for this role will be equal parts technical and functional leader to be successful, with the ability to set rigorous technical / architectural standards whilst speaking the language of our business partners to influence the strategic direction, master data and governance capabilities. Focusing on driving the innovation agenda, this leader will work with internal &amp; external industry leaders to shape world-class capabilities and enable continuous improvement.</p>
 <p></p>
 <p> This Principal Engineer will drive the MDM strategy with other MDM SME&#x2019;s, Analysts and 3rd party partners, as well as influence Global Process Owners to deliver transformative solutions.</p>
 <p></p>
 <p><b> Responsibilities</b><b> </b></p>
 <ul>
  <li><p>Collaborating with cross-functional teams to define and enforce data governance policies, standards, and best practices within Oracle EDMCS and SAP MDG.</p></li>
  <li><p> Utilizing your deep knowledge of Oracle EDMCS and SAP MDG to configure and maintain workflows, and validation rules.</p></li>
  <li><p> Being proficient in Google Cloud Platform (GCP) services and tools, including BigQuery, Dataflow, Dataproc, and Pub/Sub.</p></li>
  <li><p> Working closely with IT and Business teams to ensure seamless data integration between Oracle EDMCS, SAP MDG, and other enterprise systems.</p></li>
  <li><p> Developing and maintaining data mapping and transformation rules to ensure data consistency and compliance.</p></li>
  <li><p> Collaborating with data stewards and business users to resolve data-related issues and support data maintenance activities as well as own function design fo the end to end master data lifecycle mgmt</p></li>
  <li><p> Creating and maintaining comprehensive documentation for data management processes, configurations, and standard operating procedures.</p></li>
 </ul>
 <p></p>
 <p><b> What we&#x2019;re looking for...</b><br> <br> You&#x2019;ll need to have:</p>
 <ul>
  <li><p> Bachelor&#x2019;s degree or four or more years of work experience.</p></li>
  <li><p> Six or more years of relevant work experience</p></li>
  <li><p> Experience with Oracle EDMCS/DRM and SAP MDG.</p></li>
  <li><p> Experience in managing or executing data cleansing, data mapping, and data governance areas, preferably in an SAP environment as well as integration across complex ERP landscapes.</p></li>
  <li><p> Experience of interfacing SAP with legacy and modern data ecosystems.</p></li>
  <li><p> Experience in data modeling, data mapping, and data transformation.</p></li>
  <li><p> Experience in data governance, data quality management, and MDM concepts.</p></li>
 </ul>
 <p></p>
 <p> Even better if you have one or more of the following:</p>
 <ul>
  <li><p> SAP MDG or Oracle EDMCS certification is a plus.</p></li>
  <li><p> Working knowledge of SAP finance modules.</p></li>
  <li><p> Experience with Collibra workflows and master data modules.</p></li>
  <li><p> Experience in successful delivery of scaled agile.</p></li>
  <li><p> Scripting and programming skills in languages like Python, Java, or Scala.</p></li>
  <li><p> Experience as a Data Engineer or similar role with a focus on GCP. Strong knowledge of cloud-based data storage solutions, such as Cloud Storage, Cloud SQL, and Bigtable.</p></li>
  <li><p> Knowledge of data privacy regulations and compliance standards (e.g., GDPR, CCPA).</p></li>
 </ul>
 <p></p>
 <p> If Verizon and this role sound like a fit for you, we encourage you to apply even if you don&#x2019;t meet every &#x201c;even better&#x201d; qualification listed above.<br> <br> This role may be considered as part of the Department of Defense SkillBridge Program.</p>
 <p></p>
 <div>
  <div>
   <div>
    <div>
     <div>
      <div>
       <div>
        <h3 class=""jobSectionHeader""><b> Where you&#x2019;ll be working</b></h3>
       </div>
      </div>
     </div>
    </div>
   </div>
  </div>
 </div> In this hybrid role, you&apos;ll have a defined work location that includes work from home and assigned office days set by your manager.
 <p></p>
 <h3 class=""jobSectionHeader""><b> Scheduled Weekly Hours</b></h3> 40
 <p></p>
 <h3 class=""jobSectionHeader""><b> Equal Employment Opportunity</b></h3>
 <p> We&#x2019;re proud to be an equal opportunity employer - and celebrate our employees&#x2019; differences, including race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, and Veteran status. At Verizon, we know that diversity makes us stronger. We are committed to a collaborative, inclusive environment that encourages authenticity and fosters a sense of belonging. We strive for everyone to feel valued, connected, and empowered to reach their potential and contribute their best. Check out our diversity and inclusion page to learn more.</p>
</div>
<p></p>","https://www.indeed.com/rc/clk?jk=53f6056d9cfd3d9f&atk=&xpse=SoDb67I3JucmFzALBD0LbzkdCdPP","53f6056d9cfd3d9f",,"Full-time",,"Irving, TX","Principal Engineer - Master Data Management","4 days ago","2023-10-19T13:06:52.637Z","3.8","32089",,"2023-10-23T13:06:52.639Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=53f6056d9cfd3d9f&from=jasx&tk=1hdea988rk6dn800&vjs=3"
"Bright Vision Technologies","Bright Vision Technologies has an immediate opportunity for Lead Data Engineer in Santa Clara, CA.(Remote)
  Title: Lead Data Engineer Role: Remote Location: Santa Clara, CA.
  As a consultant within the DIE team, you will work with our clients to define their digital strategy and execution roadmap, and design and implement differentiated digital solutions to help deliver measurable value.
  Must have 12+years of experience and should have worked as Lead.
  This is W2 role.
  Your responsibilities in this role will include:
 
   Primary focus is on Glue, S3, Redshift, Lambda, PySpark, Spark.
   Then added skillset which could add value are AWS Step function, NoSQL DB like Dynamo DB and AWS Data Migration Service in that order of priority.
   Data engineer should have at least 2 years of relevant AWS experience with their services mentioned above.
   Experience in data security or governance and performance improvement is an added benefit
   Only focus is on AWS services and tech stack.
 
  Would you like to know more about our new opportunity? For immediate consideration, please send your resume directly to Venkata Raju at venkat.r@bvteck.com or Phone +1 (732) 298-7641 
 https://calendly.com/venkat-bvteck/15min 
 At BVTeck, we are committed to providing equal employment opportunities and fostering an inclusive work environment. We encourage applications from all qualified individuals regardless of race, ethnicity, religion, gender identity, sexual orientation, age, disability, or any other protected status. If you require accommodations during the recruitment process, please let us know.
  
 5S8nYMOotm","<div>
 <p>Bright Vision Technologies has an immediate opportunity for Lead Data Engineer in Santa Clara, CA.(Remote)</p>
 <p> Title: Lead Data Engineer<br> Role: Remote<br> Location: Santa Clara, CA.</p>
 <p> As a consultant within the DIE team, you will work with our clients to define their digital strategy and execution roadmap, and design and implement differentiated digital solutions to help deliver measurable value.</p>
 <p><b> Must have 12+years of experience and should have worked as Lead.</b></p>
 <p><b> This is W2 role.</b></p>
 <p> Your responsibilities in this role will include:</p>
 <ul>
  <li> Primary focus is on Glue, S3, Redshift, Lambda, PySpark, Spark.</li>
  <li> Then added skillset which could add value are AWS Step function, NoSQL DB like Dynamo DB and AWS Data Migration Service in that order of priority.</li>
  <li> Data engineer should have at least 2 years of relevant AWS experience with their services mentioned above.</li>
  <li> Experience in data security or governance and performance improvement is an added benefit</li>
  <li> Only focus is on AWS services and tech stack.</li>
 </ul>
 <p><i> Would you like to know more about our new opportunity? For immediate consideration, please send your resume directly to Venkata Raju at venkat.r@bvteck.com or Phone +1 (732) 298-7641</i></p> 
 <p><i>https://calendly.com/venkat-bvteck/15min</i></p> 
 <p><i>At BVTeck, we are committed to providing equal employment opportunities and fostering an inclusive work environment. We encourage applications from all qualified individuals regardless of race, ethnicity, religion, gender identity, sexual orientation, age, disability, or any other protected status. If you require accommodations during the recruitment process, please let us know.</i></p>
 <p> </p>
 <p>5S8nYMOotm</p>
</div>","https://www.indeed.com/rc/clk?jk=67ac721633aee65d&atk=&xpse=SoDJ67I3Jucl9HyDz50LbzkdCdPP","67ac721633aee65d",,,,"Remote","Lead Data Engineer","3 days ago","2023-10-20T13:06:55.835Z",,,"$70 an hour","2023-10-23T13:06:55.837Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=67ac721633aee65d&from=jasx&tk=1hdea988rk6dn800&vjs=3"
"Netflix","Remote, United States
     
    
    
    
     
      
       
         Data Platform
       
      
     
    
   
  
  
   
     At Netflix, we want to entertain the world and are constantly innovating on how entertainment is imagined, created and delivered to a global audience. We currently stream content in more than 30 languages in 190 countries, topping over 220 million paid subscribers and are expanding into new forms of entertainment such as gaming.
   
   
   
     The data infrastructure teams at Netflix enable us to leverage data to bring joy to our members in many different ways. We provide centralized data platforms and tools for various business functions at Netflix, so they can utilize our data to make critical data-driven decisions. We do all the heavy lifting to make it easy for our business partners to work with data efficiently, securely, and responsibly. We aspire to lead the industry standard in building a world-class data infrastructure, as Netflix leads the way to be the most popular and pervasive destination for global internet entertainment.
   
   
   
     We are looking for distributed systems engineers to help evolve and innovate our infrastructure as we work towards our ambitious goal of 500 million members worldwide. We are committed to building a diverse and inclusive team to bring new perspectives as we solve the next set of challenges. In addition, we are open to remote candidates. We value what you can do, from anywhere in the U.S.
   
   
   
     Spotlight on Data Infrastructure Teams:
   
   
   
     Database Access Platform |Learn More
   
   
     The Database Access Platform team builds and operates a flexible query gateway that facilitates data abstractions to operate at sub-millisecond latencies while allowing Netflix microservices to more easily store, consume, and manage their data. This team holds a substantial responsibility in enabling Netflix microservices to satisfy their ever-growing and evolving data needs.
   
   
     This team is passionate about distributed data systems technology. We are active in the open-source community and believe in operating what we own. We are a small team responsible for business-critical systems and are committed to a culture of feedback and engineering
     
   
   
    
    
   
    This would be your dream job if you enjoy:
    
      Solving real business needs at large scale by applying your software engineering and analytical problem solving skills.
      Architecting and building a robust, scalable, and highly available distributed infrastructure.
      Leading cross-functional initiatives and collaborating with engineers, product managers, and TPM across teams.
      Sharing our experiences with the open source communities and contributing to Netflix OSS. 
    
   
   
    About you:
    
      You have 5+ years of experience in building large-scale distributed systems or applications.
      You are proficient in design and development of RESTful web services.
      Experienced building and operating scalable, fault-tolerant, distributed systems
      You are an expert in Java or other object-oriented programming languages. Python or Scala expertise is a plus.
      Multi-threading is a challenge that you are comfortable tackling.
      You have a BS in Computer Science or related field.
    
   
   
     A few more things about us:
   
   
   
     As a team, we come from many different countries and our fields of education range from the humanities to engineering to computer science. Our team includes product managers, program managers, designers, full-stack developers, distributed systems engineers, and data scientists. Folks have the opportunity to wear different hats, should they choose to. We strongly believe this diversity has helped us build an inclusive and empathetic environment and look forward to adding your perspective to the mix!
   
   
   
     At Netflix, we carefully consider a wide range of compensation factors to determine your personal top of market. We rely on market indicators to determine compensation and consider your specific job family, background, skills, and experience to get it right. These considerations can cause your compensation to vary and will also be dependent on your location.
   
   
   
     The overall market range for roles in this area of Netflix is typically $100,000 - $700,000
   
   
   
     This market range is based on total compensation (vs. only base salary), which is in line with our compensation philosophy. Our culture is unique, and we tend to live by our values, so it’s worth learning more about Netflix here.","<div></div>
<div>
 <div>
  <div>
   <ul>
    <div>
     <div>
      Remote, United States
     </div>
    </div>
    <p></p>
    <div>
     <div>
      <div>
       <div>
         Data Platform
       </div>
      </div>
     </div>
    </div>
   </ul>
  </div>
  <div>
   <div>
     At Netflix, we want to entertain the world and are constantly innovating on how entertainment is imagined, created and delivered to a global audience. We currently stream content in more than 30 languages in 190 countries, topping over 220 million paid subscribers and are expanding into new forms of entertainment such as gaming.
   </div>
   <div></div>
   <div>
    <br> The data infrastructure teams at Netflix enable us to leverage data to bring joy to our members in many different ways. We provide centralized data platforms and tools for various business functions at Netflix, so they can utilize our data to make critical data-driven decisions. We do all the heavy lifting to make it easy for our business partners to work with data efficiently, securely, and responsibly. We aspire to lead the industry standard in building a world-class data infrastructure, as Netflix leads the way to be the most popular and pervasive destination for global internet entertainment.
   </div>
   <div></div>
   <div>
    <br> We are looking for distributed systems engineers to help evolve and innovate our infrastructure as we work towards our ambitious goal of 500 million members worldwide. We are committed to building a diverse and inclusive team to bring new perspectives as we solve the next set of challenges. In addition, we are open to remote candidates. We value what you can do, from anywhere in the U.S.
   </div>
   <div></div>
   <div>
    <br> Spotlight on Data Infrastructure Teams:
   </div>
   <div></div>
   <div>
    <br> Database Access Platform |Learn More
   </div>
   <div>
     The Database Access Platform team builds and operates a flexible query gateway that facilitates data abstractions to operate at sub-millisecond latencies while allowing Netflix microservices to more easily store, consume, and manage their data. This team holds a substantial responsibility in enabling Netflix microservices to satisfy their ever-growing and evolving data needs.
   </div>
   <div>
     This team is passionate about distributed data systems technology. We are active in the open-source community and believe in operating what we own. We are a small team responsible for business-critical systems and are committed to a culture of feedback and engineering
    <br> 
   </div>
   <div></div>
   <br> 
   <br> 
   <div>
    <h2 class=""jobSectionHeader""><b>This would be your dream job if you enjoy:</b></h2>
    <ul>
     <li> Solving real business needs at large scale by applying your software engineering and analytical problem solving skills.</li>
     <li> Architecting and building a robust, scalable, and highly available distributed infrastructure.</li>
     <li> Leading cross-functional initiatives and collaborating with engineers, product managers, and TPM across teams.</li>
     <li> Sharing our experiences with the open source communities and contributing to Netflix OSS. </li>
    </ul>
   </div>
   <div>
    <h2 class=""jobSectionHeader""><b>About you:</b></h2>
    <ul>
     <li> You have 5+ years of experience in building large-scale distributed systems or applications.</li>
     <li> You are proficient in design and development of RESTful web services.</li>
     <li> Experienced building and operating scalable, fault-tolerant, distributed systems</li>
     <li> You are an expert in Java or other object-oriented programming languages. Python or Scala expertise is a plus.</li>
     <li> Multi-threading is a challenge that you are comfortable tackling.</li>
     <li> You have a BS in Computer Science or related field.</li>
    </ul>
   </div>
   <div>
     A few more things about us:
   </div>
   <div></div>
   <div>
    <br> As a team, we come from many different countries and our fields of education range from the humanities to engineering to computer science. Our team includes product managers, program managers, designers, full-stack developers, distributed systems engineers, and data scientists. Folks have the opportunity to wear different hats, should they choose to. We strongly believe this diversity has helped us build an inclusive and empathetic environment and look forward to adding your perspective to the mix!
   </div>
   <div></div>
   <div>
    <br> At Netflix, we carefully consider a wide range of compensation factors to determine your personal top of market. We rely on market indicators to determine compensation and consider your specific job family, background, skills, and experience to get it right. These considerations can cause your compensation to vary and will also be dependent on your location.
   </div>
   <div></div>
   <div>
    <br> The overall market range for roles in this area of Netflix is typically &#x24;100,000 - &#x24;700,000
   </div>
   <div></div>
   <div>
    <br> This market range is based on total compensation (vs. only base salary), which is in line with our compensation philosophy. Our culture is unique, and we tend to live by our values, so it&#x2019;s worth learning more about Netflix here.
   </div>
  </div>
 </div>
</div>
<p></p>","https://jobs.netflix.com/jobs/299375021","5129c67eff35d9e9",,,,"Remote","Distributed Systems Engineer (L5) - Data Platform","4 days ago","2023-10-19T13:06:54.466Z","3.9","760",,"2023-10-23T13:06:54.467Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=5129c67eff35d9e9&from=jasx&tk=1hdea988rk6dn800&vjs=3"
"Liberty Mutual","Pay Philosophy
  The typical starting salary range for this role is determined by a number of factors including skills, experience, education, certifications and location. The full salary range for this role reflects the competitive labor market value for all employees in these positions across the national market and provides an opportunity to progress as employees grow and develop within the role. Some roles at Liberty Mutual have a corresponding compensation plan which may include commission and/or bonus earnings at rates that vary based on multiple factors set forth in the compensation plan for the role.
  Description 
 
  Under direct supervision, responsible for the analysis, development, and execution of data solutions of low to moderate complexity that assists with the information lifecycle needs of an organization
   Assists with collecting, integrating, and analyzing organizational data with the purpose of drawing conclusions from that information
   Develops, constructs, tests, and maintains data architectures for data platform, database, analytical, reporting, or data science systems
   Recognizes opportunities to improve data reliability, quality, and efficiency and may make recommendations where appropriate 
  Designs and develops low complexity programs and tools to support ingestion, curation and provisioning of enterprise data to achieve analytics or reporting
   Builds and designs data models and data architecture that improve accessibility, efficiency, governance and quality of data
   Recognizes opportunities to improve data quality
   Assists with aspects of deployment of data solutions
   Helps identify possible process improvements that address technology gaps within a single business process of low to moderate complexity
   Analyzes and prepare low to moderately complex technology enabled recommendations to address gaps within a single business process
   Performs other projects and duties as assigned
   Telecommuting permitted up to 100%
 
  Qualifications
  The position requires a Bachelor’s degree, or foreign equivalent, in Electrical Engineering, or a related technical or business field plus two (2) years of experience in the job offered or a Associate Data Engineer-related occupation. Position also requires demonstrable experience with each of the following:
 
   New and emerging technologies including AWS SDK, and Docker/Kubernetes
   IT concepts, strategies and methodologies
   IT architectures and technical standards
   Business function and business operations
   Design and development tools
   Layered systems architectures and shared data engineering concepts
   Agile data engineering concepts and processes
   Applying customer requirements, including drawing out unforeseen implications and making recommendations for design, the ability to define design reasoning, understanding potential impacts of design requirements
   Telecommuting permitted up to 100%
 
  To apply, please visit https://jobs.libertymutualgroup.com/, select “Search Jobs,” enter job requisition #2023-61263 in the “Job ID or Keywords” field, and submit resume. Alternatively, you may apply by submitting a resume via e-mail to RecruitLM@LibertyMutual.com. Reference requisition number in subject of e-mail.
  About Us
  **This position may have in-office requirements depending on candidate location.**
 
  At Liberty Mutual, our purpose is to help people embrace today and confidently pursue tomorrow. That’s why we provide an environment focused on openness, inclusion, trust and respect. Here, you’ll discover our expansive range of roles, and a workplace where we aim to help turn your passion into a rewarding profession.
 
  Liberty Mutual has proudly been recognized as a “Great Place to Work” by Great Place to Work® US for the past several years. We were also selected as one of the “100 Best Places to Work in IT” on IDG’s Insider Pro and Computerworld’s 2020 list. For many years running, we have been named by Forbes as one of America’s Best Employers for Women and one of America’s Best Employers for New Graduates—as well as one of America’s Best Employers for Diversity. To learn more about our commitment to diversity and inclusion please visit: https://jobs.libertymutualgroup.com/diversity-equity-inclusion/
 
  We value your hard work, integrity and commitment to make things better, and we put people first by offering you benefits that support your life and well-being. To learn more about our benefit offerings please visit: https://LMI.co/Benefits
 
  Liberty Mutual is an equal opportunity employer. We will not tolerate discrimination on the basis of race, color, national origin, sex, sexual orientation, gender identity, religion, age, disability, veteran’s status, pregnancy, genetic information or on any basis prohibited by federal, state or local law.","<div>
 <b>Pay Philosophy</b>
 <p> The typical starting salary range for this role is determined by a number of factors including skills, experience, education, certifications and location. The full salary range for this role reflects the competitive labor market value for all employees in these positions across the national market and provides an opportunity to progress as employees grow and develop within the role. Some roles at Liberty Mutual have a corresponding compensation plan which may include commission and/or bonus earnings at rates that vary based on multiple factors set forth in the compensation plan for the role.</p>
 <b><br> Description </b>
 <ul>
  <li>Under direct supervision, responsible for the analysis, development, and execution of data solutions of low to moderate complexity that assists with the information lifecycle needs of an organization</li>
  <li> Assists with collecting, integrating, and analyzing organizational data with the purpose of drawing conclusions from that information</li>
  <li> Develops, constructs, tests, and maintains data architectures for data platform, database, analytical, reporting, or data science systems</li>
  <li> Recognizes opportunities to improve data reliability, quality, and efficiency and may make recommendations where appropriate </li>
  <li>Designs and develops low complexity programs and tools to support ingestion, curation and provisioning of enterprise data to achieve analytics or reporting</li>
  <li> Builds and designs data models and data architecture that improve accessibility, efficiency, governance and quality of data</li>
  <li> Recognizes opportunities to improve data quality</li>
  <li> Assists with aspects of deployment of data solutions</li>
  <li> Helps identify possible process improvements that address technology gaps within a single business process of low to moderate complexity</li>
  <li> Analyzes and prepare low to moderately complex technology enabled recommendations to address gaps within a single business process</li>
  <li> Performs other projects and duties as assigned</li>
  <li> Telecommuting permitted up to 100%</li>
 </ul>
 <b> Qualifications</b>
 <p> The position requires a Bachelor&#x2019;s degree, or foreign equivalent, in Electrical Engineering, or a related technical or business field plus two (2) years of experience in the job offered or a Associate Data Engineer-related occupation. Position also requires demonstrable experience with each of the following:</p>
 <ul>
  <li> New and emerging technologies including AWS SDK, and Docker/Kubernetes</li>
  <li> IT concepts, strategies and methodologies</li>
  <li> IT architectures and technical standards</li>
  <li> Business function and business operations</li>
  <li> Design and development tools</li>
  <li> Layered systems architectures and shared data engineering concepts</li>
  <li> Agile data engineering concepts and processes</li>
  <li> Applying customer requirements, including drawing out unforeseen implications and making recommendations for design, the ability to define design reasoning, understanding potential impacts of design requirements</li>
  <li> Telecommuting permitted up to 100%</li>
 </ul>
 <p> To apply, please visit https://jobs.libertymutualgroup.com/, select &#x201c;Search Jobs,&#x201d; enter job requisition #2023-61263 in the &#x201c;Job ID or Keywords&#x201d; field, and submit resume. Alternatively, you may apply by submitting a resume via e-mail to RecruitLM@LibertyMutual.com. Reference requisition number in subject of e-mail.</p>
 <b> About Us</b>
 <p> **This position may have in-office requirements depending on candidate location.**</p>
 <p></p>
 <p><br> At Liberty Mutual, our purpose is to help people embrace today and confidently pursue tomorrow. That&#x2019;s why we provide an environment focused on openness, inclusion, trust and respect. Here, you&#x2019;ll discover our expansive range of roles, and a workplace where we aim to help turn your passion into a rewarding profession.</p>
 <p></p>
 <p><br> Liberty Mutual has proudly been recognized as a &#x201c;Great Place to Work&#x201d; by Great Place to Work&#xae; US for the past several years. We were also selected as one of the &#x201c;100 Best Places to Work in IT&#x201d; on IDG&#x2019;s Insider Pro and Computerworld&#x2019;s 2020 list. For many years running, we have been named by Forbes as one of America&#x2019;s Best Employers for Women and one of America&#x2019;s Best Employers for New Graduates&#x2014;as well as one of America&#x2019;s Best Employers for Diversity. To learn more about our commitment to diversity and inclusion please visit: https://jobs.libertymutualgroup.com/diversity-equity-inclusion/</p>
 <p></p>
 <p><br> We value your hard work, integrity and commitment to make things better, and we put people first by offering you benefits that support your life and well-being. To learn more about our benefit offerings please visit: https://LMI.co/Benefits</p>
 <p></p>
 <p><br> Liberty Mutual is an equal opportunity employer. We will not tolerate discrimination on the basis of race, color, national origin, sex, sexual orientation, gender identity, religion, age, disability, veteran&#x2019;s status, pregnancy, genetic information or on any basis prohibited by federal, state or local law.</p>
</div>","https://app.eightfold.ai/careers/job/618494050518?domain=libertymutual.com&utm_source=indeed&microsite=libertymutual.com&mode=job&iis=Job+Board&iisn=IndeedPPC&extcmp=Indd-paid-text-Tech","ae83164544567281",,"Full-time",,"Columbus, OH","Associate Data Engineer","3 days ago","2023-10-20T13:06:58.692Z","3.5","5354","$79,602 - $114,700 a year","2023-10-23T13:06:58.694Z","US","remote","data engineer","https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0DepOg7TDxGKZPUDK5aMJwXQn2YTYIL3PVVUU2ENsp1lfsN6FS_mRfu3T7TjuZ6itqv3t74u0efg6sZxXkbK7Q2I5jHJ4OPu7F_0RrH5zbQRMtMUwzKjmEjzjhB_X-zgutf712hFuqVioBgEyODixr6dMTlxIsTWUReqV-kPMS5CGjA4Mz7cZ76Hi1aHlEjYQSx-OMC1Eh3_Wj1h9Tepo5zZMVep32s4kearMWz2qlc9QGC2bTO5mekaQ_yzw28SLOanPpDRlTRD3RvmSWhK2Nqj-TnO3rUv4JUVWIp4zu0eLCa8Z4j5k-wnO9bOo_-K11HOc_Z1-I4v9lUG4csTMT7Ps3HMIfvo8c_MSt7oZIUg22_s366tZHugTrw0SHyOGrFOL17ktj4rMBbPsvVKqzzGLyQZWJQQZjOGfdDUQ9OzeypeY0w4MrzcpMeHizD2Vl-LK-0Y8B3hqUosWv8uYd-fgRgeZumRyI69UN48AzBnwzjhhcmiVKGTL1Iq0isvKP4c4kCe0qZXGJItsg1fMtBHrO2VJv3VULsZybqFxswQj0EwJyAtb9sUwyx47cxTBmON8-gMRKd52Pd2FAqENvvDnvfrN2uGaT7-5oOrcRBbjjpIfUqyOAkwnlagqY4e_5hYldpJu-qkkWb21uykCm7w3gIOoTRpzqqK2tqZ9E09pyoo7nmfQHjCGedBS4m-NAMkhMxMACVzXJLX5qJBUj2aPH63Rw6z_JXvIpu43yz2aAnnKHhCueACFLX6s7-grfA_UWtNpcDfRHC21AT0IqgN13dYu5W8BBF7A4sB_MHYknlJMzZFNGDdyj4BMN2Rf64n506iF2LLmsHrXXDMRuTksVMpaSveS8uIwFpRYWpkzxpA7RKa5vt4GRFd6wWhK9rKXrOwr5OvmzSIRNRksAc2GzN4I56QOk%3D&xkcb=SoDX-_M3Jucq0MwcyR0KbzkdCdPP&p=0&fvj=0&vjs=3&jsa=4945&tk=1hdea988rk6dn800&from=jasx&wvign=1"
"Excelgens","Job description
Key Responsibilities:

 Utilize Python 3 and SQL to perform dataset analysis and transformations.
 Work with AWS services for data storage, processing, and deployment.
 Version control and collaborate using GIT for effective code management.
 Implement ETL (Extract, Transform, Load) processes to prepare data for analysis.
 Develop and maintain a Data Lake architecture for efficient data storage and retrieval.
 Leverage PySpark to process large datasets and perform distributed computing tasks.

Qualifications:

 Strong proficiency in Python 3 and SQL for data analysis and manipulation.
 Experience working with AWS services, including S3, Redshift, and Glue.
 Proficiency with GIT for version control and collaborative coding.
 Hands-on experience with PySpark for data processing and analytics.
 Proven expertise in building ETL pipelines for data integration and transformation.
 Familiarity with data lake architecture and best practices.
 Excellent problem-solving skills and ability to work with complex datasets.

Assessment:
Candidates will be required to complete an assessment prior to the interview. The assessment will include questions on Python 3, SQL, dataset analysis, and GIT.
Job Type: Contract
Salary: $60.00 - $65.00 per hour
Experience level:

 7 years

Experience:

 Python: 6 years (Required)
 Pyspark: 6 years (Required)
 AWS: 6 years (Required)
 Dataset Analysis: 5 years (Preferred)
 Data lake: 5 years (Preferred)

Work Location: Remote","<p><b>Job description</b></p>
<p><b>Key Responsibilities:</b></p>
<ul>
 <li>Utilize Python 3 and SQL to perform dataset analysis and transformations.</li>
 <li>Work with AWS services for data storage, processing, and deployment.</li>
 <li>Version control and collaborate using GIT for effective code management.</li>
 <li>Implement ETL (Extract, Transform, Load) processes to prepare data for analysis.</li>
 <li>Develop and maintain a Data Lake architecture for efficient data storage and retrieval.</li>
 <li>Leverage PySpark to process large datasets and perform distributed computing tasks.</li>
</ul>
<p><b>Qualifications:</b></p>
<ul>
 <li>Strong proficiency in Python 3 and SQL for data analysis and manipulation.</li>
 <li>Experience working with AWS services, including S3, Redshift, and Glue.</li>
 <li>Proficiency with GIT for version control and collaborative coding.</li>
 <li>Hands-on experience with PySpark for data processing and analytics.</li>
 <li>Proven expertise in building ETL pipelines for data integration and transformation.</li>
 <li>Familiarity with data lake architecture and best practices.</li>
 <li>Excellent problem-solving skills and ability to work with complex datasets.</li>
</ul>
<p><b>Assessment:</b></p>
<p>Candidates will be required to complete an assessment prior to the interview. The assessment will include questions on Python 3, SQL, dataset analysis, and GIT.</p>
<p>Job Type: Contract</p>
<p>Salary: &#x24;60.00 - &#x24;65.00 per hour</p>
<p>Experience level:</p>
<ul>
 <li>7 years</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Python: 6 years (Required)</li>
 <li>Pyspark: 6 years (Required)</li>
 <li>AWS: 6 years (Required)</li>
 <li>Dataset Analysis: 5 years (Preferred)</li>
 <li>Data lake: 5 years (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,"11466c7b4a6cfcce",,"Contract",,"Remote","Senior Data Engineer","3 days ago","2023-10-20T13:07:03.437Z",,,"$60 - $65 an hour","2023-10-23T13:07:03.439Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=11466c7b4a6cfcce&from=jasx&tk=1hdeaahlgk6pf800&vjs=3"
"Black Box","Qualified candidate for the Data Center Network Engineer needs to understand networking. Why is just as important as how. Given the freedom to innovate, you seek new and better ways of doing things. You love to mentor others because you know a stronger team means everybody wins. If you are looking for an opportunity to make a difference and build a team, talk to us. Provide post-sales deployment engineering expertise and hands-on support inclusive of architectural, design, integration and support the initial implementation, installation, configuration, and ongoing change support for network solutions across various BBOX product and service offerings.Come be a part of a team working and supporting data centers and remote offices within one of the world's largest financial networks. You need to have a desire to learn why as much as how. Attention to detail sets you apart from others. You have not reached your peak but are on your way. You love to mentor others because you know a stronger team means everybody wins. If you are looking for an opportunity to make a difference and build a team, talk to us.
You will be responsible to provide post-sales deployment engineering expertise and hands-on support inclusive of architectural, design, integration and support the initial implementation, installation, configuration, and ongoing change support for network solutions across various BBOX product and service offerings.
Primary Roles & Responsibilities:

 Provide post-sales engineering implementation services for network solutions across various BBOX product and service offerings such as requirement definitions and implementation plans, end to end system designs, equipment staging, configuration creation and testing, and solution deployment of varying types of software and/or hardware network solution deployments spanning geographical separation.
 Proactively assess solution specifications in light of changing customer requirements, and recommend solution changes that optimize value for both the client and BBOX organization.
 Prepare documentation for internal and external clients detailing system designs and configuration of deployed solutions.
 Ability to coordinate remotely with Smart Hands and Project Managers to complete the activity with clear communication and coordination
 Develop and maintain professional and productive relationships with clients, infrastructure vendors such as ISPs and Carrier Services, software & hardware vendors and related key contributors to ensure stable and quality product & service delivery consistent with company objectives and client expectations.
 Provide a technical overview of product architecture, functionality, system data requirements, service delivery and integration with enterprise applications.
 Continuously expand, research and leverage knowledge of market and industry trends and benchmarking to identify, recommend and implement best practices, methodologies and relevant analytics.
 Continuously develop and enhance knowledge, skills and abilities through various learning channels to expand technical and non-technical capabilities. Ensure further expansion of skill-set in the operating systems, networking infrastructure and products & services that BBOX supports.
 Meet all financial performance objectives for area of responsibility and take corrective action as needed.
 Implement and make recommendations to improve methodologies, core competencies and processes for deployment engineering to ensure stable and quality product & service delivery consistent with company objectives and client expectations.
 Maintain and enhance a strong client service-oriented environment focused on problem prediction, detection and resolution. Proactively identify and remove barriers to meeting client expectations. Provide timely documentation of issues, action plan and outcome. Achieve all client satisfaction objectives and internal and external SLAs.
 Actively and consistently recommend and support all efforts to improve, simplify, automate and enhance day to day service delivery operations and the client experience.
 Foster and contribute toward collaborative working relationships within operations and across all levels and departments of the organization to execute deployment engineering functions and company priorities.
 Utilize escalation and exception paths, processes and systems to report current performance and make recommendations for improvement of performance.
 Achieve performance targets established by leadership for applicable Key Performance Indicators.
 Perform other duties as assigned by management.

Knowledge, Skills, Abilities
Accountability - Demonstrates an understanding of the link between one’s own job responsibilities and overall organizational goals and needs, and performs one’s job with the broader goals in mind. Looks beyond the requirements of one’s own job to offer suggestions for improvements of overall organization operations. Takes personal ownership in the organization’s success.
Customer Focus - Demonstrates concern for meeting internal and external customers’ needs in a manner that provides satisfaction for the customer. Considers the impact on the external customer when taking action, setting policies or carrying out one’s own job tasks. Looks for external trends that are likely to shape the wants and needs of customers in the near future. Looks for creative approaches to providing or improving services that may increase efficiency and decrease cost.
·Business Acumen - Interprets situations and events from a business standpoint in order to make decisions that are consistent and congruent with the organization's strategic direction and goals. Demonstrates the ability to use technology to enhance decision making, and provide cost-effective organizational and management tools. Aligns policies for a consistent and united business approach. Increases cooperation and communications between departments.
Decision Making - Makes good decisions using a combination of analysis, knowledge, experience, and judgment. Analyzes and distinguishes core problems by looking at the symptoms. Resolves key issues behind major problems in the short term while developing and executing long term solutions. Has a strong record for making decisions that are correct and accurate. Applies strategies to implement effective decision making during crises.
Results Focused - Demonstrates concern for achieving or surpassing results against an internal or external standard of excellence, shows a passion for improving the delivery of services with a commitment to continuous improvement. Sets and maintains high performance standards for self and others that support the organization’s strategic plan.
Education / Experience Requirements

 Bachelor’s Degree in Engineering, Management Information Systems, Information Technology, Computer Science or related field, or equivalent, relevant experience.
 Minimum of 5 years with engineering deployment responsibilities involving complex client requirements assessments, solutions designs and implementation within the technology services industry.
 Ability to work within Change Management process/procedures and the ability to understand the requirements to compile documents with existing standards. The ability to gather requirements from the end user. Merge with existing standards within specified timeframes.
 Previous post-sales engineering experience with network solutions including requirement definitions and implementation plans, end-to-end system designs, equipment staging, configuration creation and testing, and solution deployment of varying types of software and/or hardware network solution deployments spanning geographical separation.
 Cisco Certified Network Professional (CCNP), Cisco Certified Network Associate (CCNA), Arista ACE or related certifications preferred.
 Working within a production Data Center environment including experience directly managing routers, switches, firewalls, global and local load balancing, DMZ and a good understanding of DDI.
 A proficient networking skillset within industry-standard networking technologies and infrastructures including cabling, LAN and WAN. Extensive routing experience with routing protocols including but not limited to a thorough understanding of BGP, OSPF, EIGRP, route subnetting and route aggregation techniques. Demonstrable switching experience and knowledge with datacenter switching, including but not limited to Nexus product line, Spine and leaf architecture, Cisco ACI, Arista CLOS.
 End-to-end process thinker, with proven experience in business processes and workflow design.
 Excellent problem solving and systems analysis skills with demonstrated success in root cause analysis, effectiveness measurements and related documentation.
 Demonstrated experience of continuously expanding and leveraging knowledge of technology, market and industry trends with success in identifying, recommending and implementing best practices and methodologies.
 Ability to work effectively across all functional groups to optimize product & service offerings, fostering a seamless internal and external client experience and track record of timely and accurate issue resolution.
 Proficient in MS Office (Word, Excel, PowerPoint), Outlook, SharePoint, ERP, service delivery management tools and related cloud based technology systems.

Job Type: Full-time
Benefits:

 401(k)
 401(k) matching
 Dental insurance
 Health insurance
 Health savings account
 Paid time off
 Parental leave
 Referral program
 Vision insurance

Schedule:

 8 hour shift

Experience:

 Computer networking: 5 years (Preferred)
 LAN: 2 years (Preferred)

Security clearance:

 Confidential (Preferred)

Work Location: Remote","<p>Qualified candidate for the Data Center Network Engineer needs to understand networking. Why is just as important as how. Given the freedom to innovate, you seek new and better ways of doing things. You love to mentor others because you know a stronger team means everybody wins. If you are looking for an opportunity to make a difference and build a team, talk to us. Provide post-sales deployment engineering expertise and hands-on support inclusive of architectural, design, integration and support the initial implementation, installation, configuration, and ongoing change support for network solutions across various BBOX product and service offerings.<br>Come be a part of a team working and supporting data centers and remote offices within one of the world&apos;s largest financial networks. You need to have a desire to learn why as much as how. Attention to detail sets you apart from others. You have not reached your peak but are on your way. You love to mentor others because you know a stronger team means everybody wins. If you are looking for an opportunity to make a difference and build a team, talk to us.</p>
<p>You will be responsible to provide post-sales deployment engineering expertise and hands-on support inclusive of architectural, design, integration and support the initial implementation, installation, configuration, and ongoing change support for network solutions across various BBOX product and service offerings.</p>
<p><b>Primary Roles &amp; Responsibilities:</b></p>
<ul>
 <li>Provide post-sales engineering implementation services for network solutions across various BBOX product and service offerings such as requirement definitions and implementation plans, end to end system designs, equipment staging, configuration creation and testing, and solution deployment of varying types of software and/or hardware network solution deployments spanning geographical separation.</li>
 <li>Proactively assess solution specifications in light of changing customer requirements, and recommend solution changes that optimize value for both the client and BBOX organization.</li>
 <li>Prepare documentation for internal and external clients detailing system designs and configuration of deployed solutions.</li>
 <li>Ability to coordinate remotely with Smart Hands and Project Managers to complete the activity with clear communication and coordination</li>
 <li>Develop and maintain professional and productive relationships with clients, infrastructure vendors such as ISPs and Carrier Services, software &amp; hardware vendors and related key contributors to ensure stable and quality product &amp; service delivery consistent with company objectives and client expectations.</li>
 <li>Provide a technical overview of product architecture, functionality, system data requirements, service delivery and integration with enterprise applications.</li>
 <li>Continuously expand, research and leverage knowledge of market and industry trends and benchmarking to identify, recommend and implement best practices, methodologies and relevant analytics.</li>
 <li>Continuously develop and enhance knowledge, skills and abilities through various learning channels to expand technical and non-technical capabilities. Ensure further expansion of skill-set in the operating systems, networking infrastructure and products &amp; services that BBOX supports.</li>
 <li>Meet all financial performance objectives for area of responsibility and take corrective action as needed.</li>
 <li>Implement and make recommendations to improve methodologies, core competencies and processes for deployment engineering to ensure stable and quality product &amp; service delivery consistent with company objectives and client expectations.</li>
 <li>Maintain and enhance a strong client service-oriented environment focused on problem prediction, detection and resolution. Proactively identify and remove barriers to meeting client expectations. Provide timely documentation of issues, action plan and outcome. Achieve all client satisfaction objectives and internal and external SLAs.</li>
 <li>Actively and consistently recommend and support all efforts to improve, simplify, automate and enhance day to day service delivery operations and the client experience.</li>
 <li>Foster and contribute toward collaborative working relationships within operations and across all levels and departments of the organization to execute deployment engineering functions and company priorities.</li>
 <li>Utilize escalation and exception paths, processes and systems to report current performance and make recommendations for improvement of performance.</li>
 <li>Achieve performance targets established by leadership for applicable Key Performance Indicators.</li>
 <li>Perform other duties as assigned by management.</li>
</ul>
<p><b>Knowledge, Skills, Abilities</b></p>
<p>Accountability - Demonstrates an understanding of the link between one&#x2019;s own job responsibilities and overall organizational goals and needs, and performs one&#x2019;s job with the broader goals in mind. Looks beyond the requirements of one&#x2019;s own job to offer suggestions for improvements of overall organization operations. Takes personal ownership in the organization&#x2019;s success.</p>
<p>Customer Focus - Demonstrates concern for meeting internal and external customers&#x2019; needs in a manner that provides satisfaction for the customer. Considers the impact on the external customer when taking action, setting policies or carrying out one&#x2019;s own job tasks. Looks for external trends that are likely to shape the wants and needs of customers in the near future. Looks for creative approaches to providing or improving services that may increase efficiency and decrease cost.</p>
<p>&#xb7;Business Acumen - Interprets situations and events from a business standpoint in order to make decisions that are consistent and congruent with the organization&apos;s strategic direction and goals. Demonstrates the ability to use technology to enhance decision making, and provide cost-effective organizational and management tools. Aligns policies for a consistent and united business approach. Increases cooperation and communications between departments.</p>
<p>Decision Making - Makes good decisions using a combination of analysis, knowledge, experience, and judgment. Analyzes and distinguishes core problems by looking at the symptoms. Resolves key issues behind major problems in the short term while developing and executing long term solutions. Has a strong record for making decisions that are correct and accurate. Applies strategies to implement effective decision making during crises.</p>
<p>Results Focused - Demonstrates concern for achieving or surpassing results against an internal or external standard of excellence, shows a passion for improving the delivery of services with a commitment to continuous improvement. Sets and maintains high performance standards for self and others that support the organization&#x2019;s strategic plan.</p>
<p><b>Education / Experience Requirements</b></p>
<ul>
 <li>Bachelor&#x2019;s Degree in Engineering, Management Information Systems, Information Technology, Computer Science or related field, or equivalent, relevant experience.</li>
 <li>Minimum of 5 years with engineering deployment responsibilities involving complex client requirements assessments, solutions designs and implementation within the technology services industry.</li>
 <li>Ability to work within Change Management process/procedures and the ability to understand the requirements to compile documents with existing standards. The ability to gather requirements from the end user. Merge with existing standards within specified timeframes.</li>
 <li>Previous post-sales engineering experience with network solutions including requirement definitions and implementation plans, end-to-end system designs, equipment staging, configuration creation and testing, and solution deployment of varying types of software and/or hardware network solution deployments spanning geographical separation.</li>
 <li>Cisco Certified Network Professional (CCNP), Cisco Certified Network Associate (CCNA), Arista ACE or related certifications preferred.</li>
 <li>Working within a production Data Center environment including experience directly managing routers, switches, firewalls, global and local load balancing, DMZ and a good understanding of DDI.</li>
 <li>A proficient networking skillset within industry-standard networking technologies and infrastructures including cabling, LAN and WAN. Extensive routing experience with routing protocols including but not limited to a thorough understanding of BGP, OSPF, EIGRP, route subnetting and route aggregation techniques. Demonstrable switching experience and knowledge with datacenter switching, including but not limited to Nexus product line, Spine and leaf architecture, Cisco ACI, Arista CLOS.</li>
 <li>End-to-end process thinker, with proven experience in business processes and workflow design.</li>
 <li>Excellent problem solving and systems analysis skills with demonstrated success in root cause analysis, effectiveness measurements and related documentation.</li>
 <li>Demonstrated experience of continuously expanding and leveraging knowledge of technology, market and industry trends with success in identifying, recommending and implementing best practices and methodologies.</li>
 <li>Ability to work effectively across all functional groups to optimize product &amp; service offerings, fostering a seamless internal and external client experience and track record of timely and accurate issue resolution.</li>
 <li>Proficient in MS Office (Word, Excel, PowerPoint), Outlook, SharePoint, ERP, service delivery management tools and related cloud based technology systems.</li>
</ul>
<p>Job Type: Full-time</p>
<p>Benefits:</p>
<ul>
 <li>401(k)</li>
 <li>401(k) matching</li>
 <li>Dental insurance</li>
 <li>Health insurance</li>
 <li>Health savings account</li>
 <li>Paid time off</li>
 <li>Parental leave</li>
 <li>Referral program</li>
 <li>Vision insurance</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Computer networking: 5 years (Preferred)</li>
 <li>LAN: 2 years (Preferred)</li>
</ul>
<p>Security clearance:</p>
<ul>
 <li>Confidential (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,"e6c5d0d5fcc570e0",,"Full-time",,"Remote","Data Center Network Engineer","3 days ago","2023-10-20T13:07:06.678Z",,,,"2023-10-23T13:07:06.679Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=e6c5d0d5fcc570e0&from=jasx&tk=1hdeaahlgk6pf800&vjs=3"
"Care.com","About Care.com 
  Care.com is a consumer tech company with heart. We're on a mission to solve a human challenge we all face: finding great care for the ones we love. We're moms and dads and pet parents. We have parents and grandparents, so we understand that everyone, at some point in their lives, could use a helping hand. Our culture and our products reflect that. 
  Here, entrepreneurs, self-starters, team players, and big thinkers unite behind a common cause. Here, we're applying data analytics, AI, and the latest technologies to solve universal problems and connect people in new ways. If you like having autonomy, if you thrive on collaboration and building new things, and if you're all about using your talent for good, Care.com is the place for you. 
  Office Locations: (This is a hybrid position) 
 
  NY, NY 10011 
  Austin, TX 78746 
  Shelton, CT 06484 
  
 What Your Days Will be Like: 
  The Data Engineer will be focusing on building out data feeds and tooling from our application platform and enable rapid ingestion into our centralized Data Lake/Data Warehouse. The Data Engineer will work across business areas and application teams at Care.com to rationalize data and design, build and maintain reusable data feeds which and ultimately empower analytics consumption at Care.com. 
  The ideal candidate will have professional experience building data pipelines in a technical environment. S/he will have an understanding of application development, data warehousing, demonstrate strong business judgment, and be able to prioritize in a fast-paced environment. 
  What You'll Be Working On: 
  
  Collaborate and partner with application teams to understand data collection/generation and design and partner to build and implement data feeds from our product tech stack 
  Design, develop, and build code for rapid feeds and ingestion into the Data Warehouse 
  Identify data sources used for building out data architecture diagrams/models 
  Establish engineering practices and setup frameworks for ""Data as a Service"" 
  Collaborate with relevant delivery teams, including infrastructure, operations, site reliability engineering, product development, and others to perform evaluations, POCs, and ultimately implement and operationalize new technology. 
  Solve code level problems quickly and efficiently 
  Participate in demos and code reviews 
  Promote software best approach, standards, and processes 
  Shape development processes to promote a high-quality output while continuing to iterate quickly 
  Incorporate best practices for security, performance, and data privacy into data pipelines 
  
 What You'll Need to Succeed: 
  
  BS or MS in Computer Science or relevant engineering experience 
  5+ years work experience in Data Engineering/data pipelines 
  3+ years SQL experience is a must 
  1+ years Unix/batch scripting preferred 
  1+ years Python experience is a plus 
  1+ years Windows server admin experience is a plus 
  Experience interfacing with business teams and turning requirements and vision into a technical reality 
  MySQL & Vertica Experience a plus/preferred 
  AWS experience is a plus 
  Ability to drive efforts from start to finish as a self-motivator 
  Knowledge in Data Warehousing is a MUST 
  Proven ability to maintain performance level in a fast-paced agile environment 
  Pragmatic and realistic with solutions 
  
 For a list of our Perks + Benefits, click here! 
  Care.com supports diverse families and communities and seeks employees who are just as diverse. As an equal opportunity employer, Care.com recognizes the power of a diverse and inclusive workforce and encourages applications from individuals with varied experiences, perspectives, and backgrounds. Care.com is committed to providing reasonable accommodations for qualified individuals with disabilities. If you need assistance or accommodation, please reach out to talent@care.com. 
  Company Overview: 
  Available in more than 20 countries, Care.com is the world's leading platform for finding and managing high-quality family care. Care.com is designed to meet the evolving needs of today's families and caregivers, offering everything from household tax and payroll services and customized corporate benefits packages covering the care needs of working families, to innovating new ways for caregivers to be paid and obtain professional benefits. Since 2007, families have relied on Care.com's industry-leading products—from child and elder care to pet care and home care. Care.com is an IAC company (NASDAQ: IAC). 
  Salary Range: 110,000 to 145,000. The base salary range above represents the anticipated low and high end of the national salary range for this position. Actual salaries may vary and may be above or below the range based on various factors including but not limited to work location, experience, and performance. The range listed is just one component of Care.com's total compensation package for employees. Other rewards may include annual bonuses and short- and long-term incentives. In addition, Care.com provides a variety of benefits to employees, including health insurance coverage, life, and disability insurance, a generous 401K employer matching program, paid holidays, and paid time off (PTO).","<div>
 <p><b>About Care.com</b></p> 
 <p> Care.com is a consumer tech company with heart. We&apos;re on a mission to solve a human challenge we all face: finding great care for the ones we love. We&apos;re moms and dads and pet parents. We have parents and grandparents, so we understand that everyone, at some point in their lives, could use a helping hand. Our culture and our products reflect that.</p> 
 <p> Here, entrepreneurs, self-starters, team players, and big thinkers unite behind a common cause. Here, we&apos;re applying data analytics, AI, and the latest technologies to solve universal problems and connect people in new ways. If you like having autonomy, if you thrive on collaboration and building new things, and if you&apos;re all about using your talent for good, Care.com is the place for you.</p> 
 <h2 class=""jobSectionHeader""><b> Office Locations: (This is a hybrid position)</b></h2> 
 <ul>
  <li><b>NY, NY</b> 10011</li> 
  <li><b>Austin, TX</b> 78746</li> 
  <li><b>Shelton, CT</b> 06484</li> 
 </ul> 
 <p><b>What Your Days Will be Like:</b></p> 
 <p> The Data Engineer will be focusing on building out data feeds and tooling from our application platform and enable rapid ingestion into our centralized Data Lake/Data Warehouse. The Data Engineer will work across business areas and application teams at Care.com to rationalize data and design, build and maintain reusable data feeds which and ultimately empower analytics consumption at Care.com.</p> 
 <p> The ideal candidate will have professional experience building data pipelines in a technical environment. S/he will have an understanding of application development, data warehousing, demonstrate strong business judgment, and be able to prioritize in a fast-paced environment.</p> 
 <p><b> What You&apos;ll Be Working On:</b></p> 
 <ul> 
  <li>Collaborate and partner with application teams to understand data collection/generation and design and partner to build and implement data feeds from our product tech stack</li> 
  <li>Design, develop, and build code for rapid feeds and ingestion into the Data Warehouse</li> 
  <li>Identify data sources used for building out data architecture diagrams/models</li> 
  <li>Establish engineering practices and setup frameworks for &quot;Data as a Service&quot;</li> 
  <li>Collaborate with relevant delivery teams, including infrastructure, operations, site reliability engineering, product development, and others to perform evaluations, POCs, and ultimately implement and operationalize new technology.</li> 
  <li>Solve code level problems quickly and efficiently</li> 
  <li>Participate in demos and code reviews</li> 
  <li>Promote software best approach, standards, and processes</li> 
  <li>Shape development processes to promote a high-quality output while continuing to iterate quickly</li> 
  <li>Incorporate best practices for security, performance, and data privacy into data pipelines</li> 
 </ul> 
 <p><b>What You&apos;ll Need to Succeed:</b></p> 
 <ul> 
  <li>BS or MS in Computer Science or relevant engineering experience</li> 
  <li>5+ years work experience in Data Engineering/data pipelines</li> 
  <li>3+ years SQL experience is a must</li> 
  <li>1+ years Unix/batch scripting preferred</li> 
  <li>1+ years Python experience is a plus</li> 
  <li>1+ years Windows server admin experience is a plus</li> 
  <li>Experience interfacing with business teams and turning requirements and vision into a technical reality</li> 
  <li>MySQL &amp; Vertica Experience a plus/preferred</li> 
  <li>AWS experience is a plus</li> 
  <li>Ability to drive efforts from start to finish as a self-motivator</li> 
  <li>Knowledge in Data Warehousing is a MUST</li> 
  <li>Proven ability to maintain performance level in a fast-paced agile environment</li> 
  <li>Pragmatic and realistic with solutions</li> 
 </ul> 
 <p><b>For a list of our Perks + Benefits, click</b> <b>here!</b></p> 
 <p> Care.com supports diverse families and communities and seeks employees who are just as diverse. As an equal opportunity employer, Care.com recognizes the power of a diverse and inclusive workforce and encourages applications from individuals with varied experiences, perspectives, and backgrounds. Care.com is committed to providing reasonable accommodations for qualified individuals with disabilities. If you need assistance or accommodation, please reach out to talent@care.com.</p> 
 <p><b> Company Overview:</b></p> 
 <p> Available in more than 20 countries, Care.com is the world&apos;s leading platform for finding and managing high-quality family care. Care.com is designed to meet the evolving needs of today&apos;s families and caregivers, offering everything from household tax and payroll services and customized corporate benefits packages covering the care needs of working families, to innovating new ways for caregivers to be paid and obtain professional benefits. Since 2007, families have relied on Care.com&apos;s industry-leading products&#x2014;from child and elder care to pet care and home care. Care.com is an IAC company (NASDAQ: IAC).</p> 
 <p><i> Salary Range: 110,000 to 145,000. The base salary range above represents the anticipated low and high end of the national salary range for this position. </i><b><i>Actual salaries may vary and may be above or below the range based on various factors including but not limited to work location, experience, and performance.</i></b><i> The range listed is just one component of Care.com&apos;s total compensation package for employees. Other rewards may include annual bonuses and short- and long-term incentives. In addition, Care.com provides a variety of benefits to employees, including health insurance coverage, life, and disability insurance, a generous 401K employer matching program, paid holidays, and paid time off (PTO).</i></p>
</div>
<p></p>","https://www.indeed.com/rc/clk?jk=87c74988107a90d3&atk=&xpse=SoCE67I3Juck4LwY7Z0LbzkdCdPP","87c74988107a90d3",,,,"Hybrid remote","Data Engineer","3 days ago","2023-10-20T13:07:04.074Z","4.2","1860","$110,000 - $145,000 a year","2023-10-23T13:07:04.076Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=87c74988107a90d3&from=jasx&tk=1hdeaahlgk6pf800&vjs=3"
"Vista Global","Job Profile: 
 
   About Team
 
 
   The Data Foundation Team is highly critical for the organization to provide timely, accurate and most up to date data so that the business can take decisions accordingly. The team works with several application teams, data analytics, data science team etc. We are looking for a highly skilled and experienced 
  Senior Data Engineer to design, implement, and maintain robust and scalable data pipelines.
 
 
 
   About Company
 
 
   Vista Tech plays a vital role in the Vista group operations by delivering and accelerating comprehensive technology solutions across all brands. Vista’s end-to-end and click-to-flight solutions offer the industry's only comprehensive flight booking platform, seamlessly integrating global operations, and leveraging AI and machine learning to optimize pricing and fleet movement. Comprised of the Product Management, Engineering, and IT teams, Vista Tech’s mission is to enhance transparency and accessibility in private aviation through the development of the world's largest digital private aviation marketplace. In achieving this, Vista Tech always ensures the utmost safety and efficiency for FLIGHT CREW, EMPLOYEES and Members, while fostering a culture of innovation and excellence.
 
 
 
   You will report to Engineering Manager and play a crucial role in driving the technical direction of our projects and guiding the team in adopting best practices and cutting-edge technologies. This position is a 100% remote role with regular shif timings (9 AM to 6 AM EST). You will collaborate with cross-functional teams, provide technical leadership, and contribute to the entire software development lifecycle.
  Your Responsibilities: 
 
  Scalable Data Infrastructure: Lead the development and maintenance of highly scalable data pipelines, playing a crucial role in fortifying our data foundation.
   Technical Excellence: Demonstrate hands-on technical expertise in designing, building, and documenting complex data pipelines while adhering to data engineering best practices.
   Cross-Functional Collaboration: Collaborate closely with data engineering, analytics, and data science leadership to continuously enhance the functionality and capabilities of our data systems.
   Process Optimization: Identify opportunities for internal process improvements, spearheading automation of manual tasks, optimizing data delivery mechanisms, and redesigning infrastructure to ensure greater scalability and efficiency.
   Data Integration Mastery: Define and construct the infrastructure necessary to facilitate efficient extraction, transformation, and loading (ETL) of data from a diverse array of sources.
  Required Skills, Qualifications, and Experience: 
 
  
   
     Strong Analytical Foundation: A robust background in mathematics, statistics, computer science, data science, or a related discipline, showcasing your analytical prowess.
   
  
   
     Programming Proficiency: Advanced expertise in programming languages, particularly Python and SQL, to tackle complex data challenges.
   
  
   
     Production Experience: Proven experience in the production environment with a range of essential tools and platforms, including Snowflake, DBT, Airflow, Amazon Web Services (AWS), Docker/Kubernetes, and PostgreSQL.
   
  
   
     Database Mastery: Proficiency in database technologies, including Snowflake, PostgreSQL, Redshift, and others, enabling efficient data management.
   
  
   
     Exceptional Organizational Skills: Strong organizational capabilities, allowing you to manage multiple projects and priorities concurrently while consistently meeting deadlines.
   
  
   
     Additional Assets: Familiarity and experience with additional tools and technologies, such as AWS certification, Kafka Streaming/Kafka Connect, MongoDB, and CI/CD tools like GitLab, Jira, and Confluence, are highly advantageous.","<div>
 Job Profile: 
 <div>
  <b> About Team</b>
 </div>
 <div>
   The Data Foundation Team is highly critical for the organization to provide timely, accurate and most up to date data so that the business can take decisions accordingly. The team works with several application teams, data analytics, data science team etc. We are looking for a highly skilled and experienced 
  <b>Senior Data Engineer</b> to design, implement, and maintain robust and scalable data pipelines.
 </div>
 <div></div>
 <div>
  <b><br> About Company</b>
 </div>
 <div>
   Vista Tech plays a vital role in the Vista group operations by delivering and accelerating comprehensive technology solutions across all brands. Vista&#x2019;s end-to-end and click-to-flight solutions offer the industry&apos;s only comprehensive flight booking platform, seamlessly integrating global operations, and leveraging AI and machine learning to optimize pricing and fleet movement. Comprised of the Product Management, Engineering, and IT teams, Vista Tech&#x2019;s mission is to enhance transparency and accessibility in private aviation through the development of the world&apos;s largest digital private aviation marketplace. In achieving this, Vista Tech always ensures the utmost safety and efficiency for FLIGHT CREW, EMPLOYEES and Members, while fostering a culture of innovation and excellence.
 </div>
 <div></div>
 <div>
  <br> You will report to Engineering Manager and play a crucial role in driving the technical direction of our projects and guiding the team in adopting best practices and cutting-edge technologies. This position is a 100% remote role with regular shif timings (9 AM to 6 AM EST). You will collaborate with cross-functional teams, provide technical leadership, and contribute to the entire software development lifecycle.
 </div> Your Responsibilities: 
 <ul>
  <li><b>Scalable Data Infrastructure:</b> Lead the development and maintenance of highly scalable data pipelines, playing a crucial role in fortifying our data foundation.</li>
  <li><b> Technical Excellence:</b> Demonstrate hands-on technical expertise in designing, building, and documenting complex data pipelines while adhering to data engineering best practices.</li>
  <li><b> Cross-Functional Collaboration:</b> Collaborate closely with data engineering, analytics, and data science leadership to continuously enhance the functionality and capabilities of our data systems.</li>
  <li><b> Process Optimization:</b> Identify opportunities for internal process improvements, spearheading automation of manual tasks, optimizing data delivery mechanisms, and redesigning infrastructure to ensure greater scalability and efficiency.</li>
  <li><b> Data Integration Mastery:</b> Define and construct the infrastructure necessary to facilitate efficient extraction, transformation, and loading (ETL) of data from a diverse array of sources.</li>
 </ul> Required Skills, Qualifications, and Experience: 
 <ul>
  <li>
   <div>
    <b> Strong Analytical Foundation:</b> A robust background in mathematics, statistics, computer science, data science, or a related discipline, showcasing your analytical prowess.
   </div></li>
  <li>
   <div>
    <b> Programming Proficiency:</b> Advanced expertise in programming languages, particularly Python and SQL, to tackle complex data challenges.
   </div></li>
  <li>
   <div>
    <b> Production Experience:</b> Proven experience in the production environment with a range of essential tools and platforms, including Snowflake, DBT, Airflow, Amazon Web Services (AWS), Docker/Kubernetes, and PostgreSQL.
   </div></li>
  <li>
   <div>
    <b> Database Mastery:</b> Proficiency in database technologies, including Snowflake, PostgreSQL, Redshift, and others, enabling efficient data management.
   </div></li>
  <li>
   <div>
    <b> Exceptional Organizational Skills:</b> Strong organizational capabilities, allowing you to manage multiple projects and priorities concurrently while consistently meeting deadlines.
   </div></li>
  <li>
   <div>
    <b> Additional Assets:</b> Familiarity and experience with additional tools and technologies, such as AWS certification, Kafka Streaming/Kafka Connect, MongoDB, and CI/CD tools like GitLab, Jira, and Confluence, are highly advantageous.
   </div></li>
 </ul>
</div>","https://www.indeed.com/rc/clk?jk=85b86aa2f4ff1163&atk=&xpse=SoDr67I3JucktDRTlx0LbzkdCdPP","85b86aa2f4ff1163",,"Full-time",,"Fort Lauderdale, FL 33394","Senior Data Engineer","3 days ago","2023-10-20T13:07:06.454Z",,,,"2023-10-23T13:07:06.457Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=85b86aa2f4ff1163&from=jasx&tk=1hdeaahlgk6pf800&vjs=3"
