"company","description","descriptionHTML","externalApplyLink","id","jobType","jobType/0","jobType/1","location","positionName","postedAt","postingDateParsed","rating","reviewsCount","salary","scrapedAt","searchInput/country","searchInput/location","searchInput/position","url"
"Pomeroy","General Function:
The Data Engineer will play an important role in our growing Enterprise Data and Analytics
team. The person in this role will build out a new centralized Analytics Data Lakehouse,
help maintain our existing Operational Data Warehouse, and the infrastructure that
underlies both. We are looking for a candidate with experience creatively solving data
complexities of various sizes and levels of cleanliness - with the goal of enabling data
analysts and business users throughout Pomeroy to make decisions backed by data.
Job Type: Full-time
Pay: Up to $85,000.00 per year
Benefits:

 401(k)
 Dental insurance
 Employee assistance program
 Employee discount
 Flexible spending account
 Health insurance
 Health savings account
 Life insurance
 Paid time off
 Professional development assistance
 Referral program
 Retirement plan
 Vision insurance

Compensation package:

 Yearly pay

Schedule:

 8 hour shift
 Day shift
 Monday to Friday

Application Question(s):

 How many years of work experience do you have in a Data Engineer position?

Experience:

 Oracle Integrator: 5 years (Required)
 SQL: 5 years (Required)
 Data warehouse: 5 years (Required)

Work Location: Remote","<p>General Function:</p>
<p>The Data Engineer will play an important role in our growing Enterprise Data and Analytics</p>
<p>team. The person in this role will build out a new centralized Analytics Data Lakehouse,</p>
<p>help maintain our existing Operational Data Warehouse, and the infrastructure that</p>
<p>underlies both. We are looking for a candidate with experience creatively solving data</p>
<p>complexities of various sizes and levels of cleanliness - with the goal of enabling data</p>
<p>analysts and business users throughout Pomeroy to make decisions backed by data.</p>
<p>Job Type: Full-time</p>
<p>Pay: Up to &#x24;85,000.00 per year</p>
<p>Benefits:</p>
<ul>
 <li>401(k)</li>
 <li>Dental insurance</li>
 <li>Employee assistance program</li>
 <li>Employee discount</li>
 <li>Flexible spending account</li>
 <li>Health insurance</li>
 <li>Health savings account</li>
 <li>Life insurance</li>
 <li>Paid time off</li>
 <li>Professional development assistance</li>
 <li>Referral program</li>
 <li>Retirement plan</li>
 <li>Vision insurance</li>
</ul>
<p>Compensation package:</p>
<ul>
 <li>Yearly pay</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
 <li>Day shift</li>
 <li>Monday to Friday</li>
</ul>
<p>Application Question(s):</p>
<ul>
 <li>How many years of work experience do you have in a Data Engineer position?</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Oracle Integrator: 5 years (Required)</li>
 <li>SQL: 5 years (Required)</li>
 <li>Data warehouse: 5 years (Required)</li>
</ul>
<p>Work Location: Remote</p>",,"104d041d2954dd4c",,"Full-time",,"Remote","Data Engineer","Today","2023-10-25T11:46:11.728Z","3.2","636","Up to $85,000 a year","2023-10-25T11:46:11.730Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=104d041d2954dd4c&from=jasx&tk=1hdjafq3r2cc7000&vjs=3"
"Marriott Vacations Worldwide","**Will consider remote candidates in states where MVW has eligible business entities** 

 Job Summary 
The Senior Platform Ops Engineer - Content Management and Enriched Data Solutions plays a key role employing standards for MVW’s infrastructure and application environment. The role excels in a fast paced and dynamic environment with ability to multi-task and come up with solutions for complex applications & platforms. The Senior Platform Ops Engineer must have a strong Linux and Windows background with the ability to code/script and experience with automation and configuration management tools. Understanding and experience with a hybrid cloud environment and building and maintaining tools for deployment, monitoring, and operations to streamline the processes and provide self-service is key. Communication and collaboration across all teams and departments is a must. As a senior engineer the person will be looked on to lead projects and serve as a mentor and resource for more junior engineers on the team. 

 Expected Contributions 

 
 Supports, maintains, and own all aspects of the following technology components with the ability to automate, build, and customize the platforms to meet various needs of multiple applications. 
 Load Balancing (F5) 
 Service Bus and Orchestration Platforms (Mulesoft ESB, IBM Integration Bus) 
 Caching Services (Memcached, Terracotta) 
 Experience with distributed architecture, micro services, integration with legacy systems such as Rocket, iSeries, and BBJ. Provides guidance to legacy technologies as mentioned above in terms of threads, CPU, memory, and any other critical Linux/application KPIs. 
 Monitoring tools (Splunk, APM, AIOps, etc.); configures monitoring dashboards, monitors best practices, installs agents, performs custom configurations, installs, and upgrades platforms, and acts as a SME for monitoring practices as well as continual improvements. Engages with business leaders to promote and improve “business” monitoring to provide value to the Enterprise as a 
 whole. 

 
 Performance Tuning (Java/JVM, etc.); works hand in hand with QA teams to promote new business functions with 0 impact to production. This means tuning all platforms under performance/load testing conditions. Have a stake and ownership of performance problems before they reach production. 
 Strong problem solving and troubleshooting skills; handles on-call production impacting issues and work till resolution. Being middleware/platform operations means the buck stops here and troubleshooting issues until root cause and resolution is found even in a supporting role for issues outside of platforms owned by the team. This also spans troubleshooting across the full lifecycle 
 in over 27 environments from dev to prod. 

 
 Coding/Scripting (Perl, bash, PHP, Java, Python, SOAP, REST, PowerShell, etc.) expected to think in terms of “automation first”. Well versed in a few languages and expected to contribute daily to GIT repos. Experience in some automation/CICD/pipelines framework (Jenkins, Chef, Ansible, 
 Bamboo…) not just as a user but as a contributor to the platform. 

 
 Understanding of the full layer 7 http stack and can investigate problems using netcat and tcpdumps; provide guidance to network team regarding problems at this layer. Understand UDP/TCP connection types, interpret timeouts, connection hung situations, and typical faulty connection problems. 
 PCI and SOX knowledge to remediate, mitigate, and document all security postures within the above platforms and systems. 
 SSL know how. Must be able to explain and functionally apply practices around key exchanges, tokens, keystores, certs, keys, CSRs, CRLs, protocols, and cipher suites. Understanding of encryption and able to implement security requirements into all supported platforms. Sound off as an SME regarding wildcard certificates, implementation strategies for certs, and certificate renewal/storage/access practices. 
 Disaster Recovery; provide plans for recovery of all above platforms in case of a disaster. Hands on involvement yearly in the recovery activities during DR tests. Write and maintain scripts to recover systems quickly and efficiently. 
 IBM SPSS CnDS Inventory Forecast 
 Hybris Content Application Server 
 Jahia Content Application Server 
 AppDynamics APM 
 Enterprise Splunk Deployment (including ES and ITSI) 
 IBM ODM Rules Engine System 
 In depth knowledge of Platform Technologies (IIS, I.H.S., Apache, .NET, .NET Core, PHP, SharePoint, Tomcat, WebSphere Application Server, etc.) 
 Load Balancing (F5, TMG, etc.) – Knowledge of Application dependencies on the load balancing configuration and ramifications if incorrectly configured. 
 Performance Tuning of Platform Technologies 
 Candidate Profile 

 Education 

 
 Bachelor’s degree in Information Technology preferred and or equivalent experience. 
 Experience 

 
 At least seven years in a related computer field with at least 5 years in Web application platforms in an enterprise multi-platform environment. 
 Skills/Attributes 

 
 
 Ability to work independently, as well as part of a team with little oversight. 
 Demonstrate proficiency in methodical troubleshooting to provide resolution to incidents. 
 Desire to always be learning, as this role will require the candidate to always be up to date on the latest technologies and tools. 
 Strong oral and written communication skills, including presentation skills (MS Visio, MS PowerPoint). 

 
 Strong verbal and written communication skills, with the ability to communicate core concepts. 
 Completes tasks assigned and project work within scope, priority, timelines, and budget. 
 Ability to work in a team that utilizes principles, which enable and empower the employee to act on behalf of the team and within the boundaries of clearly defined policies and objectives. 
 High energy, clear goal orientation and strong work ethic; can do attitude; professional attitude.","**Will consider remote candidates in states where MVW has eligible business entities** 
<br>
<br> Job Summary 
<br>The Senior Platform Ops Engineer - Content Management and Enriched Data Solutions plays a key role employing standards for MVW&#x2019;s infrastructure and application environment. The role excels in a fast paced and dynamic environment with ability to multi-task and come up with solutions for complex applications &amp; platforms. The Senior Platform Ops Engineer must have a strong Linux and Windows background with the ability to code/script and experience with automation and configuration management tools. Understanding and experience with a hybrid cloud environment and building and maintaining tools for deployment, monitoring, and operations to streamline the processes and provide self-service is key. Communication and collaboration across all teams and departments is a must. As a senior engineer the person will be looked on to lead projects and serve as a mentor and resource for more junior engineers on the team. 
<br>
<br> Expected Contributions 
<br>
<ul> 
 <li>Supports, maintains, and own all aspects of the following technology components with the ability to automate, build, and customize the platforms to meet various needs of multiple applications.</li> 
 <li>Load Balancing (F5)</li> 
 <li>Service Bus and Orchestration Platforms (Mulesoft ESB, IBM Integration Bus)</li> 
 <li>Caching Services (Memcached, Terracotta)</li> 
 <li>Experience with distributed architecture, micro services, integration with legacy systems such as Rocket, iSeries, and BBJ. Provides guidance to legacy technologies as mentioned above in terms of threads, CPU, memory, and any other critical Linux/application KPIs.</li> 
 <li>Monitoring tools (Splunk, APM, AIOps, etc.); configures monitoring dashboards, monitors best practices, installs agents, performs custom configurations, installs, and upgrades platforms, and acts as a SME for monitoring practices as well as continual improvements. Engages with business leaders to promote and improve &#x201c;business&#x201d; monitoring to provide value to the Enterprise as a</li> 
</ul> whole. 
<br>
<ul> 
 <li>Performance Tuning (Java/JVM, etc.); works hand in hand with QA teams to promote new business functions with 0 impact to production. This means tuning all platforms under performance/load testing conditions. Have a stake and ownership of performance problems before they reach production.</li> 
 <li>Strong problem solving and troubleshooting skills; handles on-call production impacting issues and work till resolution. Being middleware/platform operations means the buck stops here and troubleshooting issues until root cause and resolution is found even in a supporting role for issues outside of platforms owned by the team. This also spans troubleshooting across the full lifecycle</li> 
</ul> in over 27 environments from dev to prod. 
<br>
<ul> 
 <li>Coding/Scripting (Perl, bash, PHP, Java, Python, SOAP, REST, PowerShell, etc.) expected to think in terms of &#x201c;automation first&#x201d;. Well versed in a few languages and expected to contribute daily to GIT repos. Experience in some automation/CICD/pipelines framework (Jenkins, Chef, Ansible,</li> 
</ul> Bamboo&#x2026;) not just as a user but as a contributor to the platform. 
<br>
<ul> 
 <li>Understanding of the full layer 7 http stack and can investigate problems using netcat and tcpdumps; provide guidance to network team regarding problems at this layer. Understand UDP/TCP connection types, interpret timeouts, connection hung situations, and typical faulty connection problems.</li> 
 <li>PCI and SOX knowledge to remediate, mitigate, and document all security postures within the above platforms and systems.</li> 
 <li>SSL know how. Must be able to explain and functionally apply practices around key exchanges, tokens, keystores, certs, keys, CSRs, CRLs, protocols, and cipher suites. Understanding of encryption and able to implement security requirements into all supported platforms. Sound off as an SME regarding wildcard certificates, implementation strategies for certs, and certificate renewal/storage/access practices.</li> 
 <li>Disaster Recovery; provide plans for recovery of all above platforms in case of a disaster. Hands on involvement yearly in the recovery activities during DR tests. Write and maintain scripts to recover systems quickly and efficiently.</li> 
 <li>IBM SPSS CnDS Inventory Forecast</li> 
 <li>Hybris Content Application Server</li> 
 <li>Jahia Content Application Server</li> 
 <li>AppDynamics APM</li> 
 <li>Enterprise Splunk Deployment (including ES and ITSI)</li> 
 <li>IBM ODM Rules Engine System</li> 
 <li>In depth knowledge of Platform Technologies (IIS, I.H.S., Apache, .NET, .NET Core, PHP, SharePoint, Tomcat, WebSphere Application Server, etc.)</li> 
 <li>Load Balancing (F5, TMG, etc.) &#x2013; Knowledge of Application dependencies on the load balancing configuration and ramifications if incorrectly configured.</li> 
 <li>Performance Tuning of Platform Technologies</li> 
</ul> Candidate Profile 
<br>
<br> Education 
<br>
<ul> 
 <li>Bachelor&#x2019;s degree in Information Technology preferred and or equivalent experience.</li> 
</ul> Experience 
<br>
<ul> 
 <li>At least seven years in a related computer field with at least 5 years in Web application platforms in an enterprise multi-platform environment.</li> 
</ul> Skills/Attributes 
<br>
<br> 
<ul> 
 <li>Ability to work independently, as well as part of a team with little oversight.</li> 
 <li>Demonstrate proficiency in methodical troubleshooting to provide resolution to incidents.</li> 
 <li>Desire to always be learning, as this role will require the candidate to always be up to date on the latest technologies and tools.</li> 
</ul> Strong oral and written communication skills, including presentation skills (MS Visio, MS PowerPoint). 
<br>
<ul> 
 <li>Strong verbal and written communication skills, with the ability to communicate core concepts.</li> 
 <li>Completes tasks assigned and project work within scope, priority, timelines, and budget.</li> 
 <li>Ability to work in a team that utilizes principles, which enable and empower the employee to act on behalf of the team and within the boundaries of clearly defined policies and objectives.</li> 
 <li>High energy, clear goal orientation and strong work ethic; can do attitude; professional attitude.</li> 
</ul>","https://www.indeed.com/rc/clk?jk=8e913ca13dbe7818&atk=&xpse=SoC_67I3JzdPs0Q30h0LbzkdCdPP","8e913ca13dbe7818",,"Full-time",,"8403 South Park Circle, Orlando, FL 32819","Senior Platform Operations Engineer - Content Management and Enriched Data Solutions (Remote)","Today","2023-10-25T11:46:07.760Z","4.1","4989",,"2023-10-25T11:46:07.923Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=8e913ca13dbe7818&from=jasx&tk=1hdjafq3r2cc7000&vjs=3"
"General Dynamics Information Technology","Clearance Level None Category Data Science Location Remote, Working from District of Columbia 


 Public Trust: BI Full 6C (T4) 
 Requisition Type: Pipeline 
 Your Impact 
 Own your opportunity to manage the network that makes mission success possible. Make an impact by using your skills to deliver “One GDIT Network” for our clients.
  Job Description
 
   Deliver simple solutions to complex problems as a Data Engineer at GDIT. Here, you’ll tailor cutting-edge solutions to the unique requirements of our clients. With a career in application development, you’ll make the end user’s experience your priority and we’ll make your career growth ours.
  
   At GDIT, people are our differentiator. As a Data Engineer you will help ensure today is safe and tomorrow is smarter. Our work depends on a Data Engineer joining our team to help improve Aviation safety in collaboration with our stakeholders. Our customers deal with real-world safety problems and this candidate can help develop the required software designs and code to scale our system and improve user insights into safety issues. Applies fundamental concepts, processes, practices, and procedures on technical assignments. Performs work that requires practical experience and training. Work is performed under supervision.
   HOW A SOFTWARE ENGINEER WILL MAKE AN IMPACT:
  
    Codes, tests, debugs, implements, and documents low to highly complex programs.
    Creates appropriate documentation in work assignments such as program code, and technical documentation.
    Designs systems and programs to meet complex business needs.
    Prepares detailed specifications from which programs are developed and coded.
    Ensures programs meet standards and technical specifications; performs technical analysis and component delivery.
    Gathers information from existing systems, analyzes program and time requirements.
    Assists project manager in preparing time estimates and justification for assigned tasks.
    Designs programs for projects or enhancements to existing programs. Writes specifications for programs of low to advanced complexity.
    Assists support and/or project personnel in resolving varying levels of complex program problems.
    Works with client and management to resolve issues and validate programming requirements within their areas of responsibility.
    Provides technical advice on complex programming.
    Develops test plans to verify logic of new or modified programs.
    Conducts quality assurance activities such as peer reviews.
    Creates appropriate documentation in work assignments such as program code, and technical documentation.
    Remains abreast of industry technical trends and new development to maintain current skills and remain current with industry standards.
  
  
  
   Education and Required Experience: Bachelor’s Degree 2+ years experience
   Required Technical Skills: AWS Cloud Engineer
   Security Clearance Level: Public Trust
   Required Skills and Abilities: AWS Glue, AWS Lambda, Python, JIRA
   Location: Remote
   US Citizenship Required
  
  
   Preferred Skills 
  
   Preferred Skills: AWS Athena, AWS CI/CD pipeline
    Secret Clearance or ability to obtain a clearance
    Home location of DMV
  
  
    GDIT IS YOUR PLACE:
    
   
    Full-flex work week to own your priorities at work and at home
    401K with company match
    Comprehensive health and wellness packages
    Internal mobility team dedicated to helping you own your career
    Professional growth opportunities including paid education and certifications
    Cutting-edge technology you can learn from
    Rest and recharge with paid vacation and holidays
   
  
  
    GDIT IS YOUR PLACE:
    
   
    Full-flex work week to own your priorities at work and at home
    401K with company match
    Comprehensive health and wellness packages
    Internal mobility team dedicated to helping you own your career
    Professional growth opportunities including paid education and certifications
    Cutting-edge technology you can learn from
    Rest and recharge with paid vacation and holidays","Clearance Level None Category Data Science Location Remote, Working from District of Columbia 
<br>
<div>
 <h5 class=""jobSectionHeader""><b>Public Trust: </b><b>BI Full 6C (T4)</b><b> </b></h5>
 <h5 class=""jobSectionHeader""><b>Requisition Type: </b><b>Pipeline</b><b> </b></h5>
 <h5 class=""jobSectionHeader""><b>Your Impact </b></h5>
 <p>Own your opportunity to manage the network that makes mission success possible. Make an impact by using your skills to deliver &#x201c;One GDIT Network&#x201d; for our clients.</p>
 <h5 class=""jobSectionHeader""><b> Job Description</b></h5>
 <div>
  <p> Deliver simple solutions to complex problems as a Data Engineer at GDIT. Here, you&#x2019;ll tailor cutting-edge solutions to the unique requirements of our clients. With a career in application development, you&#x2019;ll make the end user&#x2019;s experience your priority and we&#x2019;ll make your career growth ours.</p>
  <p></p>
  <p> At GDIT, people are our differentiator. As a Data Engineer you will help ensure today is safe and tomorrow is smarter. Our work depends on a Data Engineer joining our team to help improve Aviation safety in collaboration with our stakeholders. Our customers deal with real-world safety problems and this candidate can help develop the required software designs and code to scale our system and improve user insights into safety issues.<br> Applies fundamental concepts, processes, practices, and procedures on technical assignments. Performs work that requires practical experience and training. Work is performed under supervision.</p>
  <p><b><br> HOW A SOFTWARE ENGINEER WILL MAKE AN IMPACT:</b></p>
  <ul>
   <li> Codes, tests, debugs, implements, and documents low to highly complex programs.</li>
   <li> Creates appropriate documentation in work assignments such as program code, and technical documentation.</li>
   <li> Designs systems and programs to meet complex business needs.</li>
   <li> Prepares detailed specifications from which programs are developed and coded.</li>
   <li> Ensures programs meet standards and technical specifications; performs technical analysis and component delivery.</li>
   <li> Gathers information from existing systems, analyzes program and time requirements.</li>
   <li> Assists project manager in preparing time estimates and justification for assigned tasks.</li>
   <li> Designs programs for projects or enhancements to existing programs. Writes specifications for programs of low to advanced complexity.</li>
   <li> Assists support and/or project personnel in resolving varying levels of complex program problems.</li>
   <li> Works with client and management to resolve issues and validate programming requirements within their areas of responsibility.</li>
   <li> Provides technical advice on complex programming.</li>
   <li> Develops test plans to verify logic of new or modified programs.</li>
   <li> Conducts quality assurance activities such as peer reviews.</li>
   <li> Creates appropriate documentation in work assignments such as program code, and technical documentation.</li>
   <li> Remains abreast of industry technical trends and new development to maintain current skills and remain current with industry standards.</li>
  </ul>
  <p></p>
  <ul>
   <li>Education and Required Experience: Bachelor&#x2019;s Degree 2+ years experience</li>
   <li>Required Technical Skills: AWS Cloud Engineer</li>
   <li>Security Clearance Level: Public Trust</li>
   <li>Required Skills and Abilities<b>: AWS Glue, AWS Lambda, Python, JIRA</b></li>
   <li>Location: Remote</li>
   <li><b>US Citizenship Required</b></li>
  </ul>
  <p></p>
  <p><b> Preferred Skills</b><b> </b></p>
  <ul>
   <li>Preferred Skills: AWS Athena, AWS CI/CD pipeline</li>
   <li> Secret Clearance or ability to obtain a clearance</li>
   <li> Home location of DMV</li>
  </ul>
  <div>
   <b><br> GDIT IS YOUR PLACE:</b>
   <br> 
   <ul>
    <li>Full-flex work week to own your priorities at work and at home</li>
    <li>401K with company match</li>
    <li>Comprehensive health and wellness packages</li>
    <li>Internal mobility team dedicated to helping you own your career</li>
    <li>Professional growth opportunities including paid education and certifications</li>
    <li>Cutting-edge technology you can learn from</li>
    <li>Rest and recharge with paid vacation and holidays</li>
   </ul>
  </div>
  <div>
   <b><br> GDIT IS YOUR PLACE:</b>
   <br> 
   <ul>
    <li>Full-flex work week to own your priorities at work and at home</li>
    <li>401K with company match</li>
    <li>Comprehensive health and wellness packages</li>
    <li>Internal mobility team dedicated to helping you own your career</li>
    <li>Professional growth opportunities including paid education and certifications</li>
    <li>Cutting-edge technology you can learn from</li>
    <li>Rest and recharge with paid vacation and holidays</li>
   </ul>
  </div>
 </div>
</div>","https://www.indeed.com/rc/clk?jk=9f06bf28fc0c9641&atk=&xpse=SoBx67I3JzdPq2TOlx0LbzkdCdPP","9f06bf28fc0c9641",,,,"Washington, DC","TSS Data Engineer","Today","2023-10-25T11:46:10.034Z","3.7","5652",,"2023-10-25T11:46:10.036Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=9f06bf28fc0c9641&from=jasx&tk=1hdjafq3r2cc7000&vjs=3"
"Builders FirstSource","We are Builders FirstSource, America’s largest supplier of building materials, value-added components and building services to the professional market. You’ll feel proud of the work you do here every day to transform the future of home building and help make the dream of home ownership more achievable. At BFS, we believe building a successful career is not solely defined by a degree. Your experience, skills, and passion are just as important, if not more so. As such, we are committed to creating a diverse and inclusive workplace that welcomes candidates from all backgrounds and experience levels. 

 This Data Engineer & architect role is responsible for providing the deep, hands-on technical and architecture expertise with building the enterprise data platform deployed using Azure Cloud native services. This role shall have deep knowledge in the space of cloud data platform enabling enterprise capabilities for big data, data lake, data warehouse, business intelligence and future use cases of predictive & prescriptive analytics. 

 This role will be a key technical lead in transforming Builder First Source data and analytics platform and compliment the ERP transformation program and value realization roadmap. Project Elevate is the single most important project at BFS and has the potential to transform the entire company and providing single source of trusted operational/master data and enabling BFS to become data driven digital business. It will enable our ambition for technology to be our competitive weapon for the next 10+ years. This role will be responsible for ensuring our data strategy and platforms are designed, developed, deployed and operationalized with that vision in mind. 

 ESSENTIAL DUTIES AND RESPONSIBILITIES 

 
 Conceptualize the enterprise cloud data platform vision and lead the complex, multi-phased projects in implementing that vision. 
 Collaborate with BI analytics, infrastructure, cloud architects and security teams to identify data platform capabilities requirements, design, develop and deploy the technical solution. 
 Leveraging deep knowledge of Azure Cloud offerings, Data Engineering, and technology best practices in architecting, developing, and deploying the BFS’s modern data & analytics platform. 
 Ability to collaborate and co-create the technical solution with implementation partners while safeguarding the BFS’s best interest. 
 Drives analyses from technical design through completion; guides others on working in a dev ops structure/ environment. 
 Enable Azure data platform to enable performance optimized, scalable, and secure environment to support… 
 Complex data orchestrations, high volume dataset processing, multi-data types and ad-hoc queries as requested by internal and external customers. 
 Building optimized data pipelines to subscribe data across legacy application, SAP S4/HANA and SaaS application portfolios. 
 Enabling capabilities for Data lineage, Meta Data management, Data classification and discovery 
 Zero-Trust security framework, Cost optimized, best fit Azure subscription & resources tagging strategies. 
 Developing services and plugin to enable enterprise data visualization and orchestration platforms like Power BI, Tableau and Alteryx. 
 Collaborates with Data Scientists to ensure architectural and project specification needs are being met. 
 Familiarity with agile way of working, Dec-Ops, CI/CD and MLOps. 
 SUPERVISORY RESPONSIBILITIES 
This job has no supervisory responsibilities. 

 MINIMUM REQUIREMENTS 
To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The requirements listed below are representative of the knowledge, skill, and/or ability required. 

 
 3+ years of Azure data cloud solution/technical architecture 
 3+ years of Big Data/analytics/information analysis/database management using cloud native Azure services. 
 Extensive hands-on experience implementing data migration and data processing using Azure services: 
 Azure Storage 
 Azure SQL DB, ADLS & Synapse 
 Azure Data Factory 
 Databricks 
 Azure Data Catalog 
 Azure DevOps - CI/CD 
 Familiarity with the Technology stack available in the industry for data management, data ingestion, change data capture, processing, and curation. 
 BI development experience using MS technology stack. 
 Bachelor's degree from an accredited college/university. 
 7+ years relevant work experience. 
 Or an equivalent combination of education and experience. 
 COMPETENCIES 

 
 Evaluates Problems: Evaluates and analyzes different types of information objectively to identify appropriate solutions; writes fluently, establishing the key facts clearly and interprets numerical data effectively. 
 Technical Communication/ Presentation: Communicates with clarity and precision, presenting complex information in a concise format that is audience appropriate. 
 Adjusting and Driving Change: Takes a positive approach to tackling work and embraces change; invites feedback relating to performance and deals constructively with criticism. Identifies the need for and drives change when required to achieve objectives. 
 Focuses on Customers: Understands and anticipates customer needs and takes action to provide high-quality products and services to exceed expectations. 
 Demonstrates Business Acumen: Demonstrates working knowledge of market, economic, legal, and regulatory environments and how they impact the business. 
 Agile Best Practices: Understands how agility is leveraged in IT ways of working. Adopts agile best practices as appropriate throughout the assigned work lifecycle. Responds to feedback quickly based on comments of internal and external customers and needs of the market. 
 Bias for Action: Takes initiative and identifies what needs to be done and acts without waiting to be asked. Executes work in a timely manner. Suggests improvements to current ways of working. 
 BFS COMPETENCIES 

 
 Business and Financial Acumen 
 Demonstrates depth of understanding for the P&L and financial analysis 
 Teaches business and financial acumen to others. 
 Understands KPIs and how BFS makes money. 
 Knows the different business segments and how they relate to one another. 
 Understands customer sales and engagement. 
 Demonstrates functional and/or technical expertise. 
 Understands complex issues and demonstrates problem solving skills. 
 Understands how to maximize business results regardless of industry cycle. 
 Results Driven 
 Holds self and others accountable. 
 Communicates and sets clear goals with plans to deliver. 
 Manages competing priorities effectively. 
 Demonstrates appropriate urgency. 
 Drives to exceed expectations in alignment with our BFS SPICE values. 
 Embraces and follows best practices. 
 Demonstrates self-starter, can-do attitude. 
 Strategic Thinking and Decision Making 
 Leverages resources and teams around them to solve problems and create mutually beneficial outcomes. 
 Demonstrates willingness and courage to make tough decisions in a timely manner. 
 Balances short-and-long term priorities 
 Demonstrates proactive versus reactive thinking. 
 Asks questions to identify root cause and analyze situations more accurately. 
 Servant Leadership 
 Demonstrates humility by putting others first. 
 Builds trust-based relationships. 
 Leads by example with kindness and respect. 
 Collaborates well across all areas of the business. 
 Advocates for others 
 Actively listens to understand the meaning and intent of what the other person is communicating. 
 Demonstrates authenticity and encourages others to do the same. 
 Emotional Intelligence 
 Demonstrates situational awareness – knows when and how to adjust leadership style in different situations. 
 Demonstrates self-awareness – understands strengths and weaknesses. 
 Demonstrates empathy – puts themselves in other’s shoes. 
 Assumes positive intent. 
 Develops and Leads Others 
 Drives alignment through clear communication of vision, goals, and expectations. 
 Invests time on a regular basis in performance feedback and developmental conversations. 
 Fosters a respectful and inclusive environment. 
 Empowers, motivates, and inspires others. 
 Coaches and mentor others for their development. 
 Guides and persuades others to deliver positive outcomes. 
 Growth Mindset 
 Demonstrates a growth mindset; takes appropriate risks, fails fast and forward, learns from mistakes. 
 Perseveres and champions growth, even in the face of resistance, ambiguity, or possible failure. 
 Thinks like an owner with an entrepreneurial spirit. 
 Demonstrates and encourages intellectual curiosity. 
 Continuous learner; seeks opportunities and knowledge for personal and professional growth. 
 Sees possibilities over problems – actively seeks solutions. 
 Innovation 
 Encourages out-of-the box thinking to create new ways of doing things. 
 Continuously seeks to improve and simplify pain points in the business. 
 Anticipates, embraces, and leads change. 
 Develops and executes breakthrough strategies. 
 Integrity 
 Does the right thing even under challenging circumstances? 
 Communicates with honesty. 
 Consistently treats others fairly and equitably. 
 Demonstrates reliability and does what they say they will do. 
 Conducts tough conversations and delivers difficult messages with kindness and respect. 
 WORK ENVIRONMENT / PHYSICAL ACTIVITY 
The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. 

 
 Subject to both typical office environment and outside locations with temperature and weather variations. 
 Must be able to lift and carry up to 25 pounds. 
 Occasional travel may be required. 
 In addition to the base wage listed, this position is also eligible to earn an annual bonus subject to changes in plan design and documents and in accordance with applicable law. Eligibility and the amount of the bonus varies based on overall company success, thresholds met and other terms and conditions of the Company’s active bonus policy for the respective year. 

 Full-Time Team Members are eligible for company benefits, including • Three Medical plan options • Dental & Vision • Critical Care • Accident & Hospital insurance 

 
 Flexible Spending Accounts for Health & Dependent Care • Health Savings Account • 401(k) with company match • Vacation & Sick Time • Paid company holidays 
 Company Paid Life & AD&D • Supplemental Life • Short & Long Term Disability • Bereavement • Paid Parental Leave • Team Member Assistance Program 
 All benefits are subject to change pursuant to state and federal guidelines. 

 Builders FirstSource is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, protected veteran status or status as an individual with a disability. 

 In compliance with the ADA Amendments Act (ADAAA), if you have a disability and would like to request an accommodation in order to apply for a position with Builders FirstSource, please call (214) 765-3990 or email: ADA.Accommodation@bldr.com. Please do not send resumes to this email address - it is intended only to be used to request an accommodation in submitting an application for a job opening. 

 https://www.dol.gov/ofccp/regs/compliance/posters/ofccpost.htm 

 EEO THE LAW - English/Spanish 
EEO IS THE LAW - SUPPLEMENT - English/Spanish 
Pay Transparency Provision - English/Spanish","We are Builders FirstSource, America&#x2019;s largest supplier of building materials, value-added components and building services to the professional market. You&#x2019;ll feel proud of the work you do here every day to transform the future of home building and help make the dream of home ownership more achievable. At BFS, we believe building a successful career is not solely defined by a degree. Your experience, skills, and passion are just as important, if not more so. As such, we are committed to creating a diverse and inclusive workplace that welcomes candidates from all backgrounds and experience levels. 
<br>
<br> This Data Engineer &amp; architect role is responsible for providing the deep, hands-on technical and architecture expertise with building the enterprise data platform deployed using Azure Cloud native services. This role shall have deep knowledge in the space of cloud data platform enabling enterprise capabilities for big data, data lake, data warehouse, business intelligence and future use cases of predictive &amp; prescriptive analytics. 
<br>
<br> This role will be a key technical lead in transforming Builder First Source data and analytics platform and compliment the ERP transformation program and value realization roadmap. Project Elevate is the single most important project at BFS and has the potential to transform the entire company and providing single source of trusted operational/master data and enabling BFS to become data driven digital business. It will enable our ambition for technology to be our competitive weapon for the next 10+ years. This role will be responsible for ensuring our data strategy and platforms are designed, developed, deployed and operationalized with that vision in mind. 
<br>
<br> ESSENTIAL DUTIES AND RESPONSIBILITIES 
<br>
<ul> 
 <li>Conceptualize the enterprise cloud data platform vision and lead the complex, multi-phased projects in implementing that vision.</li> 
 <li>Collaborate with BI analytics, infrastructure, cloud architects and security teams to identify data platform capabilities requirements, design, develop and deploy the technical solution.</li> 
 <li>Leveraging deep knowledge of Azure Cloud offerings, Data Engineering, and technology best practices in architecting, developing, and deploying the BFS&#x2019;s modern data &amp; analytics platform.</li> 
 <li>Ability to collaborate and co-create the technical solution with implementation partners while safeguarding the BFS&#x2019;s best interest.</li> 
 <li>Drives analyses from technical design through completion; guides others on working in a dev ops structure/ environment.</li> 
 <li>Enable Azure data platform to enable performance optimized, scalable, and secure environment to support&#x2026;</li> 
 <li>Complex data orchestrations, high volume dataset processing, multi-data types and ad-hoc queries as requested by internal and external customers.</li> 
 <li>Building optimized data pipelines to subscribe data across legacy application, SAP S4/HANA and SaaS application portfolios.</li> 
 <li>Enabling capabilities for Data lineage, Meta Data management, Data classification and discovery</li> 
 <li>Zero-Trust security framework, Cost optimized, best fit Azure subscription &amp; resources tagging strategies.</li> 
 <li>Developing services and plugin to enable enterprise data visualization and orchestration platforms like Power BI, Tableau and Alteryx.</li> 
 <li>Collaborates with Data Scientists to ensure architectural and project specification needs are being met.</li> 
 <li>Familiarity with agile way of working, Dec-Ops, CI/CD and MLOps.</li> 
</ul> SUPERVISORY RESPONSIBILITIES 
<br>This job has no supervisory responsibilities. 
<br>
<br> MINIMUM REQUIREMENTS 
<br>To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The requirements listed below are representative of the knowledge, skill, and/or ability required. 
<br>
<ul> 
 <li>3+ years of Azure data cloud solution/technical architecture</li> 
 <li>3+ years of Big Data/analytics/information analysis/database management using cloud native Azure services.</li> 
 <li>Extensive hands-on experience implementing data migration and data processing using Azure services:</li> 
 <li>Azure Storage</li> 
 <li>Azure SQL DB, ADLS &amp; Synapse</li> 
 <li>Azure Data Factory</li> 
 <li>Databricks</li> 
 <li>Azure Data Catalog</li> 
 <li>Azure DevOps - CI/CD</li> 
 <li>Familiarity with the Technology stack available in the industry for data management, data ingestion, change data capture, processing, and curation.</li> 
 <li>BI development experience using MS technology stack.</li> 
 <li>Bachelor&apos;s degree from an accredited college/university.</li> 
 <li>7+ years relevant work experience.</li> 
 <li>Or an equivalent combination of education and experience.</li> 
</ul> COMPETENCIES 
<br>
<ul> 
 <li>Evaluates Problems: Evaluates and analyzes different types of information objectively to identify appropriate solutions; writes fluently, establishing the key facts clearly and interprets numerical data effectively.</li> 
 <li>Technical Communication/ Presentation: Communicates with clarity and precision, presenting complex information in a concise format that is audience appropriate.</li> 
 <li>Adjusting and Driving Change: Takes a positive approach to tackling work and embraces change; invites feedback relating to performance and deals constructively with criticism. Identifies the need for and drives change when required to achieve objectives.</li> 
 <li>Focuses on Customers: Understands and anticipates customer needs and takes action to provide high-quality products and services to exceed expectations.</li> 
 <li>Demonstrates Business Acumen: Demonstrates working knowledge of market, economic, legal, and regulatory environments and how they impact the business.</li> 
 <li>Agile Best Practices: Understands how agility is leveraged in IT ways of working. Adopts agile best practices as appropriate throughout the assigned work lifecycle. Responds to feedback quickly based on comments of internal and external customers and needs of the market.</li> 
 <li>Bias for Action: Takes initiative and identifies what needs to be done and acts without waiting to be asked. Executes work in a timely manner. Suggests improvements to current ways of working.</li> 
</ul> BFS COMPETENCIES 
<br>
<ul> 
 <li>Business and Financial Acumen</li> 
 <li>Demonstrates depth of understanding for the P&amp;L and financial analysis</li> 
 <li>Teaches business and financial acumen to others.</li> 
 <li>Understands KPIs and how BFS makes money.</li> 
 <li>Knows the different business segments and how they relate to one another.</li> 
 <li>Understands customer sales and engagement.</li> 
 <li>Demonstrates functional and/or technical expertise.</li> 
 <li>Understands complex issues and demonstrates problem solving skills.</li> 
 <li>Understands how to maximize business results regardless of industry cycle.</li> 
 <li>Results Driven</li> 
 <li>Holds self and others accountable.</li> 
 <li>Communicates and sets clear goals with plans to deliver.</li> 
 <li>Manages competing priorities effectively.</li> 
 <li>Demonstrates appropriate urgency.</li> 
 <li>Drives to exceed expectations in alignment with our BFS SPICE values.</li> 
 <li>Embraces and follows best practices.</li> 
 <li>Demonstrates self-starter, can-do attitude.</li> 
 <li>Strategic Thinking and Decision Making</li> 
 <li>Leverages resources and teams around them to solve problems and create mutually beneficial outcomes.</li> 
 <li>Demonstrates willingness and courage to make tough decisions in a timely manner.</li> 
 <li>Balances short-and-long term priorities</li> 
 <li>Demonstrates proactive versus reactive thinking.</li> 
 <li>Asks questions to identify root cause and analyze situations more accurately.</li> 
 <li>Servant Leadership</li> 
 <li>Demonstrates humility by putting others first.</li> 
 <li>Builds trust-based relationships.</li> 
 <li>Leads by example with kindness and respect.</li> 
 <li>Collaborates well across all areas of the business.</li> 
 <li>Advocates for others</li> 
 <li>Actively listens to understand the meaning and intent of what the other person is communicating.</li> 
 <li>Demonstrates authenticity and encourages others to do the same.</li> 
 <li>Emotional Intelligence</li> 
 <li>Demonstrates situational awareness &#x2013; knows when and how to adjust leadership style in different situations.</li> 
 <li>Demonstrates self-awareness &#x2013; understands strengths and weaknesses.</li> 
 <li>Demonstrates empathy &#x2013; puts themselves in other&#x2019;s shoes.</li> 
 <li>Assumes positive intent.</li> 
 <li>Develops and Leads Others</li> 
 <li>Drives alignment through clear communication of vision, goals, and expectations.</li> 
 <li>Invests time on a regular basis in performance feedback and developmental conversations.</li> 
 <li>Fosters a respectful and inclusive environment.</li> 
 <li>Empowers, motivates, and inspires others.</li> 
 <li>Coaches and mentor others for their development.</li> 
 <li>Guides and persuades others to deliver positive outcomes.</li> 
 <li>Growth Mindset</li> 
 <li>Demonstrates a growth mindset; takes appropriate risks, fails fast and forward, learns from mistakes.</li> 
 <li>Perseveres and champions growth, even in the face of resistance, ambiguity, or possible failure.</li> 
 <li>Thinks like an owner with an entrepreneurial spirit.</li> 
 <li>Demonstrates and encourages intellectual curiosity.</li> 
 <li>Continuous learner; seeks opportunities and knowledge for personal and professional growth.</li> 
 <li>Sees possibilities over problems &#x2013; actively seeks solutions.</li> 
 <li>Innovation</li> 
 <li>Encourages out-of-the box thinking to create new ways of doing things.</li> 
 <li>Continuously seeks to improve and simplify pain points in the business.</li> 
 <li>Anticipates, embraces, and leads change.</li> 
 <li>Develops and executes breakthrough strategies.</li> 
 <li>Integrity</li> 
 <li>Does the right thing even under challenging circumstances?</li> 
 <li>Communicates with honesty.</li> 
 <li>Consistently treats others fairly and equitably.</li> 
 <li>Demonstrates reliability and does what they say they will do.</li> 
 <li>Conducts tough conversations and delivers difficult messages with kindness and respect.</li> 
</ul> WORK ENVIRONMENT / PHYSICAL ACTIVITY 
<br>The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. 
<br>
<ul> 
 <li>Subject to both typical office environment and outside locations with temperature and weather variations.</li> 
 <li>Must be able to lift and carry up to 25 pounds.</li> 
 <li>Occasional travel may be required.</li> 
</ul> In addition to the base wage listed, this position is also eligible to earn an annual bonus subject to changes in plan design and documents and in accordance with applicable law. Eligibility and the amount of the bonus varies based on overall company success, thresholds met and other terms and conditions of the Company&#x2019;s active bonus policy for the respective year. 
<br>
<br> Full-Time Team Members are eligible for company benefits, including &#x2022; Three Medical plan options &#x2022; Dental &amp; Vision &#x2022; Critical Care &#x2022; Accident &amp; Hospital insurance 
<br>
<ul> 
 <li>Flexible Spending Accounts for Health &amp; Dependent Care &#x2022; Health Savings Account &#x2022; 401(k) with company match &#x2022; Vacation &amp; Sick Time &#x2022; Paid company holidays</li> 
 <li>Company Paid Life &amp; AD&amp;D &#x2022; Supplemental Life &#x2022; Short &amp; Long Term Disability &#x2022; Bereavement &#x2022; Paid Parental Leave &#x2022; Team Member Assistance Program</li> 
</ul> All benefits are subject to change pursuant to state and federal guidelines. 
<br>
<br> Builders FirstSource is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, protected veteran status or status as an individual with a disability. 
<br>
<br> In compliance with the ADA Amendments Act (ADAAA), if you have a disability and would like to request an accommodation in order to apply for a position with Builders FirstSource, please call (214) 765-3990 or email: ADA.Accommodation@bldr.com. Please do not send resumes to this email address - it is intended only to be used to request an accommodation in submitting an application for a job opening. 
<br>
<br> https://www.dol.gov/ofccp/regs/compliance/posters/ofccpost.htm 
<br>
<br> EEO THE LAW - English/Spanish 
<br>EEO IS THE LAW - SUPPLEMENT - English/Spanish 
<br>Pay Transparency Provision - English/Spanish","https://builders.dayforcehcm.com/CandidatePortal/en-US/builders/Posting/View/119436?source=Indeed&source=indeed","140b95dd17ad2243",,"Full-time",,"Remote","Data Engineer/Architect- Azure","Today","2023-10-25T11:46:08.290Z","3.6","1387","$110,025 - $183,375 a year","2023-10-25T11:46:09.644Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=140b95dd17ad2243&from=jasx&tk=1hdjafq3r2cc7000&vjs=3"
"EXL Services","Company Overview and Culture 

EXL (NASDAQ: EXLS) is a global analytics and digital solutions company that partners with clients to improve business outcomes and unlock growth. Bringing together deep domain expertise with robust data, powerful analytics, cloud, and AI, we create agile, scalable solutions and execute complex operations for the world’s leading corporations in industries including insurance, healthcare, banking and financial services, media, and retail, among others. Focused on creating value from data for driving faster decision-making and transforming operating models, EXL was founded on the core values of innovation, collaboration, excellence, integrity and respect. Headquartered in New York, our team is over 40,000 strong, with more than 50 offices spanning six continents. For information, visit www.exlservice.com. 

 For the past 20 years, EXL has worked as a strategic partner and won awards in its approach to helping its clients solve business challenges such as digital transformation, improving customer experience, streamlining business operations, taking products to market faster, improving corporate finance, building models to become compliant more quickly with new regulations, turning volumes of data into business opportunities, creating new channels for growth and better adapting to change. The business operates within four business units: Insurance, Health, Analytics, and Emerging businesses. 

 About EXL Health 
We leverage Human Ingenuity and domain expertise to help clients improve outcomes, optimize revenue and maximize profitability across the healthcare ecosystem. Technology, data and analytics are at the heart of our solutions. We collaborate closely with clients to transform how care is delivered, managed and paid. 

 EXL Health combines deep domain expertise with analytic insights and technology-enabled services to transform how care is delivered, managed, and paid. Leveraging Human Ingenuity, we collaborate with our clients to solve complex problems and enhance their performance with nimble, scalable solutions. With data on more than 260 million lives, we work with hundreds of organizations across the healthcare ecosystem. 

 We help payers improve member care quality and network performance, manage population risk, and optimize revenue while decreasing administrative waste and reducing health claim expenditures. We help Pharmacy Benefit Managers (PBMs) manage member drug benefits and reduce drug spending while maintaining quality. We help provider organizations proactively manage risk, improve outcomes, and optimize network performance. We provide Life Sciences companies with enriched data, insights through advanced analytics and data visualization tools to get the right treatment to the right patient at the right time. 

 Data Engineer GCP 

 
Description:
 
 
 4+ years of Experience in Data Extraction and creating data pipeline workflows on Bigdata(Hive, HQL/Pyspark) with knowledge of Data Engineering concepts. 
 Exposure in analyzing large data sets from multiple data sources, perform validation of data 
 Knowledge of Hadoop Eco-system components like HDFS, Spark, Hive, Sqoop. 
 Experience writing code in Python. 
 Knowledge of SQL/HQL functionalities to write the optimized queries 
 Experience in GCP Cloud services such as Big Query, Airflow DAG, Dataflow, Beam etc. 
 Ability to build a migration plan in collaboration with stakeholders 
 Analytical and problem-solving skills 
 EEO/Minorities/Females/Vets/Disabilities 

 
Base Salary Range Disclaimer: The base salary range represents the low and high end of the EXL base salary range for this position. Actual salaries will vary depending on factors including but not limited to: location and experience. The base salary range listed is just one component of EXL's total compensation package for employees. Other rewards may include bonuses, as well as a Paid Time Off policy, and many region specific benefits. 

 Please also note that the data shared through the job application will be stored and processed by EXL in accordance with the EXL Privacy Policy. 

 Application & Interview Impersonation Warning – Purposely impersonating another individual when applying and / or participating in an interview in order to obtain employment with EXL Service Holdings, Inc. (the “Company”) for yourself or for the other individual is a crime. We have implemented measures to deter and to uncover such unlawful conduct. If the Company identifies such fraudulent conduct, it will result in, as applicable, the application being rejected, an offer (if made) being rescinded, or termination of employment as well as possible legal action against the impersonator(s). 

 EXL may use artificial intelligence to create insights on how your candidate information matches the requirements of the job for which you applied. While AI may be used in the recruiting process, all final decisions in the recruiting and hiring process will be taken by the recruiting and hiring teams after considering a candidate’s full profile. As a candidate, you can choose to opt out of this artificial intelligence screening process. Your decision to opt out will not negatively impact your opportunity for employment with EXL.","Company Overview and Culture 
<br>
<b>EXL (NASDAQ:</b> EXLS) is a global analytics and digital solutions company that partners with clients to improve business outcomes and unlock growth. Bringing together deep domain expertise with robust data, powerful analytics, cloud, and AI, we create agile, scalable solutions and execute complex operations for the world&#x2019;s leading corporations in industries including insurance, healthcare, banking and financial services, media, and retail, among others. Focused on creating value from data for driving faster decision-making and transforming operating models, EXL was founded on the core values of innovation, collaboration, excellence, integrity and respect. Headquartered in New York, our team is over 40,000 strong, with more than 50 offices spanning six continents. For information, visit www.exlservice.com. 
<br>
<br> For the past 20 years, EXL has worked as a strategic partner and won awards in its approach to helping its clients solve business challenges such as digital transformation, improving customer experience, streamlining business operations, taking products to market faster, improving corporate finance, building models to become compliant more quickly with new regulations, turning volumes of data into business opportunities, creating new channels for growth and better adapting to change. The business operates within four business units: Insurance, Health, Analytics, and Emerging businesses. 
<br>
<br> About EXL Health 
<br>We leverage Human Ingenuity and domain expertise to help clients improve outcomes, optimize revenue and maximize profitability across the healthcare ecosystem. Technology, data and analytics are at the heart of our solutions. We collaborate closely with clients to transform how care is delivered, managed and paid. 
<br>
<br> EXL Health combines deep domain expertise with analytic insights and technology-enabled services to transform how care is delivered, managed, and paid. Leveraging Human Ingenuity, we collaborate with our clients to solve complex problems and enhance their performance with nimble, scalable solutions. With data on more than 260 million lives, we work with hundreds of organizations across the healthcare ecosystem. 
<br>
<br> We help payers improve member care quality and network performance, manage population risk, and optimize revenue while decreasing administrative waste and reducing health claim expenditures. We help Pharmacy Benefit Managers (PBMs) manage member drug benefits and reduce drug spending while maintaining quality. We help provider organizations proactively manage risk, improve outcomes, and optimize network performance. We provide Life Sciences companies with enriched data, insights through advanced analytics and data visualization tools to get the right treatment to the right patient at the right time. 
<br>
<br> Data Engineer GCP 
<br>
<br> 
<b>Description:</b>
<br> 
<ul> 
 <li>4+ years of Experience in Data Extraction and creating data pipeline workflows on Bigdata(Hive, HQL/Pyspark) with knowledge of Data Engineering concepts.</li> 
 <li>Exposure in analyzing large data sets from multiple data sources, perform validation of data</li> 
 <li>Knowledge of Hadoop Eco-system components like HDFS, Spark, Hive, Sqoop.</li> 
 <li>Experience writing code in Python.</li> 
 <li>Knowledge of SQL/HQL functionalities to write the optimized queries</li> 
 <li>Experience in GCP Cloud services such as Big Query, Airflow DAG, Dataflow, Beam etc.</li> 
 <li>Ability to build a migration plan in collaboration with stakeholders</li> 
 <li>Analytical and problem-solving skills</li> 
</ul> EEO/Minorities/Females/Vets/Disabilities 
<br>
<br> 
<b>Base Salary Range Disclaimer:</b> The base salary range represents the low and high end of the EXL base salary range for this position. Actual salaries will vary depending on factors including but not limited to: location and experience. The base salary range listed is just one component of EXL&apos;s total compensation package for employees. Other rewards may include bonuses, as well as a Paid Time Off policy, and many region specific benefits. 
<br>
<br> Please also note that the data shared through the job application will be stored and processed by EXL in accordance with the EXL Privacy Policy. 
<br>
<br> Application &amp; Interview Impersonation Warning &#x2013; Purposely impersonating another individual when applying and / or participating in an interview in order to obtain employment with EXL Service Holdings, Inc. (the &#x201c;Company&#x201d;) for yourself or for the other individual is a crime. We have implemented measures to deter and to uncover such unlawful conduct. If the Company identifies such fraudulent conduct, it will result in, as applicable, the application being rejected, an offer (if made) being rescinded, or termination of employment as well as possible legal action against the impersonator(s). 
<br>
<br> EXL may use artificial intelligence to create insights on how your candidate information matches the requirements of the job for which you applied. While AI may be used in the recruiting process, all final decisions in the recruiting and hiring process will be taken by the recruiting and hiring teams after considering a candidate&#x2019;s full profile. As a candidate, you can choose to opt out of this artificial intelligence screening process. Your decision to opt out will not negatively impact your opportunity for employment with EXL.","https://us231.dayforcehcm.com/CandidatePortal/en-US/exl/Posting/View/47569?source=Indeed","010e5918b2aaef6d",,"Full-time",,"Remote","Data Engineer GCP","Today","2023-10-25T11:46:17.236Z","3.7","1873","$100,000 - $115,000 a year","2023-10-25T11:46:17.241Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=010e5918b2aaef6d&from=jasx&tk=1hdjafq3r2cc7000&vjs=3"
"Sumitomo Mitsui Banking Corporation","Join us on our mission to create a completely new, 100% digital bank that truly serves customers' best interests. We are a close-knit and fun-loving team of seasoned financial services professionals who came together for the challenge of building a bank from scratch - and we are committed to doing it all the right way (from technology infrastructure to modern marketing to customer experience).
 
  The anticipated salary range for this role is between $75,000.00 and $150,000.00. The specific salary offered to an applicant will be based on their individual qualifications, experiences, and an analysis of the current compensation paid in their geography and the market for similar roles at the time of hire. The role may also be eligible for an annual discretionary incentive award. In addition to cash compensation, SMBC offers a competitive portfolio of benefits to its employees.
 
  We work with the flexibility and speed of a start-up. But we also have significant stability and capital from being part of the SMBC Group (Sumitomo Mitsui Banking Corporation). SMBC is the second largest bank in Japan and the 12th largest bank in the world with operations in over forty countries. And SMBC is committed to disrupting the US marketplace with ground-breaking products.
 
  It is the best of both worlds, and we are seeking proven marketing leaders to propel us towards a national launch. We have both the ambitious growth plans and the 'patient capital' necessary to execute a multi-year plan. Join us on the journey to deliver an exciting concept of evolved banking.
 
  
   
     Job Summary
   
   
     Jenius Bank is looking for a hands-on Sr. Software Engineer - Data proficient in Java, Scala, and Python languages. You'll be part of the team that is responsible for building the Data and Analytics Platform for Jenius Bank. As a Sr. Software Engineer - Data, you will get an opportunity to perform proof of concept on new cloud technologies and build a highly scalable data platform to support critical business functions, create rest APIs to expose data services for internal and external consumers.
   
  
  
   
     Principal Duties and Responsibilities
   
   
    
      A solid experience and understanding of considerations for large-scale solutioning and operationalization of data warehouses, data lakes and analytics platforms within Cloud environments.
      Monitors the Data Lake constantly and ensures that the appropriate support teams are engaged at the right times.
      Design, build and test scalable data ingestion pipelines, perform end to end automation of ETL process for various datasets that are being ingested.
      Determine the best way to extract application telemetry data, structure it, send to proper tool for reporting (Kafka, Splunk).
      Work with business and cross-functional teams to gather and document requirements to meet business needs.
      Provide support as required to ensure the availability and performance of ETL/ELT jobs.
      Provide technical assistance and cross training to business and internal team members.
      Collaborate with business partners for continuous improvement opportunities.
    
   
  
  
   
     Position Specifications
   
   
    
      Bachelor's degree in Computer Science, Computer Engineering, or Information Systems Technology
      6+ years of experience in Data Engineering with an emphasis on Data Warehousing and Data Analytics.
      4+ years of experience with one of the leading public clouds.
      4+ years of experience in design and build of salable data pipelines that deal with extraction, transformation, and loading.
      4+ years of experience with Java, Python or Scala.
      Ability to work independently, solve problems, update the stake holders.
      Analyze, design, develop and deploy solutions as per business requirements.
      Strong understanding of relational and dimensional data modeling.
      Experience in CI/CD related technologies. You will be responsible for creating CI/CD pipelines.
      Excellent written, verbal communication skills, including experience in technical documentation and ability to communicate with senior business managers and executives.
      Knowledge of GCP Cloud data implementation projects (Dataflow, DataProc, Cloud Composer, Big Query, Cloud Storage, GKE, Airflow, etc.) is preferred.
    
   
  
 
  EOE STATEMENT We are an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, national origin, disability status, protected veteran status or any other characteristic protected by law.
 
  CCPA DISCLOSURE Personal Information Collection Notice: This notice contains information under the California Consumer Privacy Act (CCPA) about the categories of personal information (PI) of California residents that Manufacturers Bank collects and the business or commercial purpose(s) for which the PI may be used. We do not sell PI. More information about our collection and use of PI may be found in our CCPA Privacy Policy at https://www.manufacturersbank.com/CCPA-Privacy. Persons with disabilities may contact our Customer Contact Center toll-free at (877) 560-9812 to request the information in this Notice in an alternative format.","<div>
 <p>Join us on our mission to create a completely new, 100% digital bank that truly serves customers&apos; best interests. We are a close-knit and fun-loving team of seasoned financial services professionals who came together for the challenge of building a bank from scratch - and we are committed to doing it all the right way (from technology infrastructure to modern marketing to customer experience).</p>
 <p></p>
 <p><br> The anticipated salary range for this role is between &#x24;75,000.00 and &#x24;150,000.00. The specific salary offered to an applicant will be based on their individual qualifications, experiences, and an analysis of the current compensation paid in their geography and the market for similar roles at the time of hire. The role may also be eligible for an annual discretionary incentive award. In addition to cash compensation, SMBC offers a competitive portfolio of benefits to its employees.</p>
 <p></p>
 <p><br> We work with the flexibility and speed of a start-up. But we also have significant stability and capital from being part of the SMBC Group (Sumitomo Mitsui Banking Corporation). SMBC is the second largest bank in Japan and the 12th largest bank in the world with operations in over forty countries. And SMBC is committed to disrupting the US marketplace with ground-breaking products.</p>
 <p></p>
 <p><br> It is the best of both worlds, and we are seeking proven marketing leaders to propel us towards a national launch. We have both the ambitious growth plans and the &apos;patient capital&apos; necessary to execute a multi-year plan. Join us on the journey to deliver an exciting concept of evolved banking.</p>
 <div>
  <div>
   <div>
    <h2 class=""jobSectionHeader""><b> Job Summary</b></h2>
   </div>
   <div>
    <p> Jenius Bank is looking for a hands-on Sr. Software Engineer - Data proficient in Java, Scala, and Python languages. You&apos;ll be part of the team that is responsible for building the Data and Analytics Platform for Jenius Bank. As a Sr. Software Engineer - Data, you will get an opportunity to perform proof of concept on new cloud technologies and build a highly scalable data platform to support critical business functions, create rest APIs to expose data services for internal and external consumers.</p>
   </div>
  </div>
  <div>
   <div>
    <h2 class=""jobSectionHeader""><b> Principal Duties and Responsibilities</b></h2>
   </div>
   <div>
    <ul>
     <li> A solid experience and understanding of considerations for large-scale solutioning and operationalization of data warehouses, data lakes and analytics platforms within Cloud environments.</li>
     <li> Monitors the Data Lake constantly and ensures that the appropriate support teams are engaged at the right times.</li>
     <li> Design, build and test scalable data ingestion pipelines, perform end to end automation of ETL process for various datasets that are being ingested.</li>
     <li> Determine the best way to extract application telemetry data, structure it, send to proper tool for reporting (Kafka, Splunk).</li>
     <li> Work with business and cross-functional teams to gather and document requirements to meet business needs.</li>
     <li> Provide support as required to ensure the availability and performance of ETL/ELT jobs.</li>
     <li> Provide technical assistance and cross training to business and internal team members.</li>
     <li> Collaborate with business partners for continuous improvement opportunities.</li>
    </ul>
   </div>
  </div>
  <div>
   <div>
    <h2 class=""jobSectionHeader""><b> Position Specifications</b></h2>
   </div>
   <div>
    <ul>
     <li> Bachelor&apos;s degree in Computer Science, Computer Engineering, or Information Systems Technology</li>
     <li> 6+ years of experience in Data Engineering with an emphasis on Data Warehousing and Data Analytics.</li>
     <li> 4+ years of experience with one of the leading public clouds.</li>
     <li> 4+ years of experience in design and build of salable data pipelines that deal with extraction, transformation, and loading.</li>
     <li> 4+ years of experience with Java, Python or Scala.</li>
     <li> Ability to work independently, solve problems, update the stake holders.</li>
     <li> Analyze, design, develop and deploy solutions as per business requirements.</li>
     <li> Strong understanding of relational and dimensional data modeling.</li>
     <li> Experience in CI/CD related technologies. You will be responsible for creating CI/CD pipelines.</li>
     <li> Excellent written, verbal communication skills, including experience in technical documentation and ability to communicate with senior business managers and executives.</li>
     <li> Knowledge of GCP Cloud data implementation projects (Dataflow, DataProc, Cloud Composer, Big Query, Cloud Storage, GKE, Airflow, etc.) is preferred.</li>
    </ul>
   </div>
  </div>
 </div>
 <p><b> EOE STATEMENT</b><br> We are an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, national origin, disability status, protected veteran status or any other characteristic protected by law.</p>
 <p></p>
 <p><b><br> CCPA DISCLOSURE</b><br> Personal Information Collection Notice: This notice contains information under the California Consumer Privacy Act (CCPA) about the categories of personal information (PI) of California residents that Manufacturers Bank collects and the business or commercial purpose(s) for which the PI may be used. We do not sell PI. More information about our collection and use of PI may be found in our CCPA Privacy Policy at https://www.manufacturersbank.com/CCPA-Privacy. Persons with disabilities may contact our Customer Contact Center toll-free at (877) 560-9812 to request the information in this Notice in an alternative format.</p>
</div>","https://careers.smbcgroup.com/job/Scottsdale-Senior-Software-Engineer-Data-%28Remote%29-AZ-85255/1068765900/?feedId=392100&utm_source=Indeed&utm_campaign=SMBC_Indeed","184b3221fe14409e",,"Full-time",,"Scottsdale, AZ 85255","Senior Software Engineer - Data (Remote)","Today","2023-10-25T11:46:13.700Z","3.4","159","$75,000 - $150,000 a year","2023-10-25T11:46:13.701Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=184b3221fe14409e&from=jasx&tk=1hdjafq3r2cc7000&vjs=3"
"UniGroup","Under general supervision, formulates and defines system scope and objectives through research and fact-finding, documentation, coding, and testing required to develop or modify moderately complex information systems. This position is tasked with building modern software to connect people with the transportation and moving industries through technology. 

 The work location for this role is flexible if approved by UniGroup except this position may not be performed remotely from Colorado and California. You may read over the UniGroup privacy policy by clicking HERE. 

 
Essential Duties and Responsibilities:
 
 
 Technical Skills: 
 Consistently writes production-ready code that is easily testable, easily understood by other developers, and accounts for edge cases and errors. Understands when it is appropriate to leave comments, but biases towards self-documenting code. 
 Understands the testing approach of several teams and uses quality metrics to identify gaps. Works with those teams to recommend solutions that are in accordance with accepted testing frameworks and the testing pyramid. Influences organization wide testing strategy. 
 Proficient at using systematic debugging to diagnose all issues within a set of related domains. 
 Fosters a culture of observability across several teams and helps them use operational data to improve stability and performance of their domains. 
 Has expertise in a set of related team's domains, including the breadth of services, how they interact, and data flows between systems. 
 Works across teams to foster a culture of architecture that allows for iterative, autonomous development and future scaling. Guides several teams in anticipation of future use cases and helps them make design decisions that minimize the cost of future changes. 
 Actively works with the security team, as well as across several teams, to apply the organization's security strategy. Fosters a security first mindset across those teams, leading by example. 
 Delivery: 
 Reviews cross-team work critically and ensures it’s appropriately broken down and prioritized, and well understood by all involved teams. 
 Ensures cross-team dependencies are noted and well understood by all teams involved and other relevant stakeholders. Works across teams to foster a culture of priority setting and urgency in alignment with organizational strategy. 
 Effectively handles risk, change, and uncertainty across several teams. Decides and acts responsibly in their work across teams without having the total picture during routine business, as well as when in high pressure situations. 
 Successfully manages cross-team commitments, their progress, and roadmap to delivery. Anticipates and communicates blockers, delays, and cost ballooning across teams, before they require escalation. Ensures expectations across teams and stakeholders are clarified between all parties involved. 
 When taking action, weighs cost and value in order to make the most economic action. Uses this thinking in their own work, and to foster a culture across several teams where people apply economic thinking to make timely decisions. 
 Feedback, Communication, Collaboration: 
 Fosters a culture of delivering praise and constructive feedback across several teams as well as their respective business stakeholders. Actively demonstrates these behaviors. 
 Works across several teams and with their business stakeholders to foster a culture of seeking out feedback and using it as a tool for growth. Actively demonstrates these behaviors. 
 Is able to communicate effectively with a diverse set of teams. Fosters a culture of clear, concise, effective, audience-oriented communication across several teams, ensuring teammates actively listen to others and are understood. Actively demonstrates these behaviors. Pays attention to nonverbal communication. 
 Fosters a culture of documentation and knowledge sharing across several teams and their respective business stakeholders; actively demonstrates these behaviors. 
 Consistently works across teams to help them resolve blockers, and complete work tasks. Ensures that credit is shared and given where due. 
 Works to build and improve strong relationships with engineers and managers across the organization as well as relevant business stakeholders for several teams. Leverages relationships to better plan for and position those teams. 
 Fosters a culture across several teams where people are encouraged to share their opinions and contribute to discussions in a respectful manner, approach disagreement non-defensively with inquisitiveness, and use contradictory opinions as a basis for constructive, productive conversations. Works through surface-level disagreements to expose the concerns of disagreeing voices and integrates these concerns into their perspective and plans. 
 Leadership: 
 Takes ownership of decisions made across teams by helping them make clear decisions in alignment with organizational goals, backing decisions made, and taking responsibility for their success. Raises awareness for how biases impact decisions and ensures accountability is practiced throughout those teams. Demonstrates these behaviors themselves. 
 Fosters a culture across several teams of having conversations based on organizational strategy and principles to create alignment. Strongly oriented towards goals and ensures several teams are continuously working towards their goals. 
 Thinks about practices and processes that affect several teams, discusses improvements with appropriate parties, and drives implementation. Usually collaborates with others to improve organizational practices and processes. 
 Facilitates discussions across teams, ensuring that everyone has an opportunity to share their opinion and be heard, and that discussion outcomes tie to stated goals. Ensures relevant parties are included in discussions. Guides discussions toward decisions, clarifies and gets buy-in. 
 Mentors across teams in an open, respectful, flexible, empathetic manner. Fosters a culture of mentoring across teams by seeking out mentoring opportunities for themselves and others and supports others in their growth as mentors. 
 Strategic Impact: 
 Usually involved in strategic organizational decisions and plans. Leads cross-team strategic efforts, influencing decisions to achieve cross-team alignment on major goals. 
 Recognizes product opportunities and differentiators in relation to the competition. Often helps refine roadmaps across teams based on technical strategy & constraints. Helps to define & create new product abilities by changing technical strategy or constraints. 
 Technical expertise with data modelling and data mining techniques: 
 Experience with programming languages (e.g. Python and JavaScript/Typescript) 
 Experience with relational SQL and NoSQL databases, including Postgres and MongoDB. 
 Experience with cloud environments (Microservices / AWS / Docker / Kubernetes) 
 Experience of using Git / GitLab /GitHub 
 Hands-on experience with SQL database design 
 Experience with big data tools such as Kafka, etc. 
 Experience with data pipeline and workflow management tools 
 Great numerical and analytical skills 
 
Education, License or Certification:
 
 
 Bachelor’s degree in Information Systems or equivalent experience. 
 
Experience:
 
 
 6-8 years of experience in IS Development 
 We foster diversity, in part, by imposing a strict policy of non-discrimination. Employment decisions are made without regard to race, color, ethnicity, national origin, sex, sexual orientation, gender identity, age, religion, disability, veteran or military status, genetic information or other status protected by the law. 

 We value the unique skills and experiences that veterans and separated service members bring to our workforce. While serving our country you have gained skills such as leadership, flexibility, and agility, which will help to make you successful here. We are dedicated to supporting military families and ensuring that we provide a welcoming environment for our country’s heroes. We hope you consider joining the UniGroup family. 

 UniGroup is committed to the full inclusion of all qualified individuals. As part of this commitment, UniGroup will ensure that persons with disabilities are provided reasonable accommodations. If reasonable accommodation is needed to participate in the job application or interview process, to perform essential job functions, and/or to receive other benefits and privileges of employment, please contact careers@unigroup.com","Under general supervision, formulates and defines system scope and objectives through research and fact-finding, documentation, coding, and testing required to develop or modify moderately complex information systems. This position is tasked with building modern software to connect people with the transportation and moving industries through technology. 
<br>
<br> The work location for this role is flexible if approved by UniGroup except this position may not be performed remotely from Colorado and California. You may read over the UniGroup privacy policy by clicking HERE. 
<br>
<br> 
<b>Essential Duties and Responsibilities:</b>
<br> 
<ul> 
 <li>Technical Skills:</li> 
 <li>Consistently writes production-ready code that is easily testable, easily understood by other developers, and accounts for edge cases and errors. Understands when it is appropriate to leave comments, but biases towards self-documenting code.</li> 
 <li>Understands the testing approach of several teams and uses quality metrics to identify gaps. Works with those teams to recommend solutions that are in accordance with accepted testing frameworks and the testing pyramid. Influences organization wide testing strategy.</li> 
 <li>Proficient at using systematic debugging to diagnose all issues within a set of related domains.</li> 
 <li>Fosters a culture of observability across several teams and helps them use operational data to improve stability and performance of their domains.</li> 
 <li>Has expertise in a set of related team&apos;s domains, including the breadth of services, how they interact, and data flows between systems.</li> 
 <li>Works across teams to foster a culture of architecture that allows for iterative, autonomous development and future scaling. Guides several teams in anticipation of future use cases and helps them make design decisions that minimize the cost of future changes.</li> 
 <li>Actively works with the security team, as well as across several teams, to apply the organization&apos;s security strategy. Fosters a security first mindset across those teams, leading by example.</li> 
 <li>Delivery:</li> 
 <li>Reviews cross-team work critically and ensures it&#x2019;s appropriately broken down and prioritized, and well understood by all involved teams.</li> 
 <li>Ensures cross-team dependencies are noted and well understood by all teams involved and other relevant stakeholders. Works across teams to foster a culture of priority setting and urgency in alignment with organizational strategy.</li> 
 <li>Effectively handles risk, change, and uncertainty across several teams. Decides and acts responsibly in their work across teams without having the total picture during routine business, as well as when in high pressure situations.</li> 
 <li>Successfully manages cross-team commitments, their progress, and roadmap to delivery. Anticipates and communicates blockers, delays, and cost ballooning across teams, before they require escalation. Ensures expectations across teams and stakeholders are clarified between all parties involved.</li> 
 <li>When taking action, weighs cost and value in order to make the most economic action. Uses this thinking in their own work, and to foster a culture across several teams where people apply economic thinking to make timely decisions.</li> 
 <li>Feedback, Communication, Collaboration:</li> 
 <li>Fosters a culture of delivering praise and constructive feedback across several teams as well as their respective business stakeholders. Actively demonstrates these behaviors.</li> 
 <li>Works across several teams and with their business stakeholders to foster a culture of seeking out feedback and using it as a tool for growth. Actively demonstrates these behaviors.</li> 
 <li>Is able to communicate effectively with a diverse set of teams. Fosters a culture of clear, concise, effective, audience-oriented communication across several teams, ensuring teammates actively listen to others and are understood. Actively demonstrates these behaviors. Pays attention to nonverbal communication.</li> 
 <li>Fosters a culture of documentation and knowledge sharing across several teams and their respective business stakeholders; actively demonstrates these behaviors.</li> 
 <li>Consistently works across teams to help them resolve blockers, and complete work tasks. Ensures that credit is shared and given where due.</li> 
 <li>Works to build and improve strong relationships with engineers and managers across the organization as well as relevant business stakeholders for several teams. Leverages relationships to better plan for and position those teams.</li> 
 <li>Fosters a culture across several teams where people are encouraged to share their opinions and contribute to discussions in a respectful manner, approach disagreement non-defensively with inquisitiveness, and use contradictory opinions as a basis for constructive, productive conversations. Works through surface-level disagreements to expose the concerns of disagreeing voices and integrates these concerns into their perspective and plans.</li> 
 <li>Leadership:</li> 
 <li>Takes ownership of decisions made across teams by helping them make clear decisions in alignment with organizational goals, backing decisions made, and taking responsibility for their success. Raises awareness for how biases impact decisions and ensures accountability is practiced throughout those teams. Demonstrates these behaviors themselves.</li> 
 <li>Fosters a culture across several teams of having conversations based on organizational strategy and principles to create alignment. Strongly oriented towards goals and ensures several teams are continuously working towards their goals.</li> 
 <li>Thinks about practices and processes that affect several teams, discusses improvements with appropriate parties, and drives implementation. Usually collaborates with others to improve organizational practices and processes.</li> 
 <li>Facilitates discussions across teams, ensuring that everyone has an opportunity to share their opinion and be heard, and that discussion outcomes tie to stated goals. Ensures relevant parties are included in discussions. Guides discussions toward decisions, clarifies and gets buy-in.</li> 
 <li>Mentors across teams in an open, respectful, flexible, empathetic manner. Fosters a culture of mentoring across teams by seeking out mentoring opportunities for themselves and others and supports others in their growth as mentors.</li> 
 <li>Strategic Impact:</li> 
 <li>Usually involved in strategic organizational decisions and plans. Leads cross-team strategic efforts, influencing decisions to achieve cross-team alignment on major goals.</li> 
 <li>Recognizes product opportunities and differentiators in relation to the competition. Often helps refine roadmaps across teams based on technical strategy &amp; constraints. Helps to define &amp; create new product abilities by changing technical strategy or constraints.</li> 
 <li>Technical expertise with data modelling and data mining techniques:</li> 
 <li>Experience with programming languages (e.g. Python and JavaScript/Typescript)</li> 
 <li>Experience with relational SQL and NoSQL databases, including Postgres and MongoDB.</li> 
 <li>Experience with cloud environments (Microservices / AWS / Docker / Kubernetes)</li> 
 <li>Experience of using Git / GitLab /GitHub</li> 
 <li>Hands-on experience with SQL database design</li> 
 <li>Experience with big data tools such as Kafka, etc.</li> 
 <li>Experience with data pipeline and workflow management tools</li> 
 <li>Great numerical and analytical skills</li> 
</ul> 
<b>Education, License or Certification:</b>
<br> 
<ul> 
 <li>Bachelor&#x2019;s degree in Information Systems or equivalent experience.</li> 
</ul> 
<b>Experience:</b>
<br> 
<ul> 
 <li>6-8 years of experience in IS Development</li> 
</ul> We foster diversity, in part, by imposing a strict policy of non-discrimination. Employment decisions are made without regard to race, color, ethnicity, national origin, sex, sexual orientation, gender identity, age, religion, disability, veteran or military status, genetic information or other status protected by the law. 
<br>
<br> We value the unique skills and experiences that veterans and separated service members bring to our workforce. While serving our country you have gained skills such as leadership, flexibility, and agility, which will help to make you successful here. We are dedicated to supporting military families and ensuring that we provide a welcoming environment for our country&#x2019;s heroes. We hope you consider joining the UniGroup family. 
<br>
<br> UniGroup is committed to the full inclusion of all qualified individuals. As part of this commitment, UniGroup will ensure that persons with disabilities are provided reasonable accommodations. If reasonable accommodation is needed to participate in the job application or interview process, to perform essential job functions, and/or to receive other benefits and privileges of employment, please contact careers@unigroup.com","https://us231.dayforcehcm.com/CandidatePortal/en-US/unigroup/Posting/View/5819?source=Indeed","cb4729396df2d9d4",,"Full-time",,"St. Louis, MO","STAFF SOFTWARE ENGINEER, DATA","Today","2023-10-25T11:46:17.428Z","3.6","52",,"2023-10-25T11:46:17.431Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=cb4729396df2d9d4&from=jasx&tk=1hdjafq3r2cc7000&vjs=3"
"Xlysi","W2 Contract ONLY. 

 Senior Azure Data Engineer 
Fully remote 
Need USC/GC 
12+ months contract 

 
Requirements:
 
 
 10 years of experience in data engineering, with at least 5 years focus on Azure technologies. 
 Strong expertise in designing and implementing data solutions using Azure Data Factory, Azure Databricks, Azure Synapse Analytics, and other relevant Azure services. 
 Proficiency in programming languages such asPython, Java, or C#. 
 Solid understanding of data modeling, data warehousing, and ETL/ELT processes. 
 Experience with SQL and NoSQL databases, such as Azure SQL Database and Azure Cosmos DB. 
 Strong problem-solving skills and the ability to analyze complex data requirements. 
 Excellent communication and collaboration skills, with the ability to work effectively in cross-functional teams. 
 Expert level 
- 
Azure Data Lake Storage 
Azure Data Factory 
Azure Databricks 
Azure Synapse 
Python 
PySpark 
Azure DevOps 

 Nice to have 
- 
Meaningful, demonstrable experience contributing to a large-scale data warehousing platform in Azure. 
Development of PowerBI dashboards providing analytic visualizations to business function.","W2 Contract ONLY. 
<br>
<br> Senior Azure Data Engineer 
<br>Fully remote 
<br>Need USC/GC 
<br>12+ months contract 
<br>
<br> 
<b>Requirements:</b>
<br> 
<ul> 
 <li>10 years of experience in data engineering, with at least 5 years focus on Azure technologies.</li> 
 <li>Strong expertise in designing and implementing data solutions using Azure Data Factory, Azure Databricks, Azure Synapse Analytics, and other relevant Azure services.</li> 
 <li>Proficiency in programming languages such asPython, Java, or C#.</li> 
 <li>Solid understanding of data modeling, data warehousing, and ETL/ELT processes.</li> 
 <li>Experience with SQL and NoSQL databases, such as Azure SQL Database and Azure Cosmos DB.</li> 
 <li>Strong problem-solving skills and the ability to analyze complex data requirements.</li> 
 <li>Excellent communication and collaboration skills, with the ability to work effectively in cross-functional teams.</li> 
</ul> Expert level 
<br>- 
<br>Azure Data Lake Storage 
<br>Azure Data Factory 
<br>Azure Databricks 
<br>Azure Synapse 
<br>Python 
<br>PySpark 
<br>Azure DevOps 
<br>
<br> Nice to have 
<br>- 
<br>Meaningful, demonstrable experience contributing to a large-scale data warehousing platform in Azure. 
<br>Development of PowerBI dashboards providing analytic visualizations to business function.","https://xlysi.catsone.com/careers/2106-General/jobs/16317291-Sr-Azure-Data-Engineer-Only-USCGC-W2-contract-only/?ref=Indeed","5d468f5d6aca2501",,"Contract",,"Remote","Sr. Azure Data Engineer (Only USC/GC)- W2 contract only","Today","2023-10-25T11:46:19.239Z",,,,"2023-10-25T11:46:19.241Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=5d468f5d6aca2501&from=jasx&tk=1hdjafq3r2cc7000&vjs=3"
"DRT Strategies, Inc.","Overview
  DRT Strategies delivers expert management consulting and information technology (IT) solutions to large federal agencies, the U.S. Navy, state and local government and commercial clients in health care, technology, and financial services industries.
  The three letters of our name, DRT, stand for Driving Resolution Together, which is the core philosophy on which the company was founded. That is, we collaborate with our clients to solve their most pressing challenges - together.
  We are problem solvers dedicated to your success, combining Fortune 500 experience with small business responsiveness. We have established a reputation with our clients as a forward-thinking consulting firm with demonstrated success in implementing solutions that lead to meaningful results. Our world-class consultants unite people to work collaboratively to achieve project goals and make your vision a reality.
  Project Description
  DRT provides data, technology, and mission support to its federal public health partners. We collaborate to address diverse, complex, and emerging problems within the public health domains of environmental health, infectious disease, chronic disease, and emergency preparedness/response. Our support drives the adoption of emerging best practices and technologies to increase the usefulness and value of public health data. Our team develops innovative data capture and analysis techniques in the field to enhance emergency response efforts all around the world. 
 Job Summary
  The Public Health Data Engineer works directly with public health professionals, data analysts, project coordinators, and developers. The Engineer supports the end-to-end data life cycle for public health data repositories ensuring accuracy and efficiency. They collaborate with developers and data team to enable customization of capability features and functionality to meet requirements. They access public health databases, data structures, and data flows to recommend and perform the import, structuring, and encoding of public health data. 
 Responsibilities
  Configures and ingests structured, unstructured, and semi-structured data repositories.  Designs, implements, and manages databases and data delivery systems for insights, analysis, and reporting.  Works with analysts, subject matter experts, and technologists to determine and implement database systems.  Applies expertise in database design and implementation tools, such as entity-relationship data modeling and SQL, distributed computing architectures, operating systems, storage technologies, memory management and networking.  Performs streaming and batch data processing, ETL, data wrangling, data ingest, and data access.  Develops and manages a schedule to meet deliverables and establish estimated completion dates.  Follows the government’s software development lifecycle framework and provides agreed upon project documentation.  Completes tasks on time with minimal supervision but supports collaboration across teams.  Recommends and implements solutions to support access, security, and recovery of data.  Monitors data repository performance and recommends and implements enhancements. 
 Required Experience
  Must have or be able to obtain a Public Trust with the US federal government  Experience in database or data repository administration roles  Advanced understanding of relational database design, development, and management  Experience with Microsoft SQL Server  Familiarity with common API languages 
 Preferred Experience
  Recent experience with HHS (FDA, CDC, etc.) environment and standards: SDLC, technical architecture, release processes  Knowledge of public health data and related privacy requirements (HIPAA, PII, etc.)  Experience with Agile and DevOps methodologies  Completed trainings or certifications in Microsoft SQL Server  Experience with cloud service providers such as Azure and AWS 
 Education and Training
  Degree in Computer Science, Data Science, or related field 
 DRT’s culture is reflective of our core values: 
 
  Professionalism: Be a leader and someone your customers and colleagues can count on by taking ownership and accountability for your work. Demonstrate a solution-oriented mindset and bias for action. Show empathy, dignity, and respect for each other. Be ""high minded"", maintain a calm demeanor when dealing with ambiguity or adversity and stay out of the fray (i.e., avoid drama).
   Quality: Do things right and do the right thing. Pursue excellence in your work by delivering quality services and products that provide high value and return on investment (ROI).
   Teamwork: Work with each other, the customer, and DRT. Demonstrate flexibility, a positive attitude and willingness to work collaboratively to help others and share information.
   Customer Mission Focused: Commit to customer success and strive to exceed expectations by understanding the mission, executing the customer's vision, and solving challenges to drive results and achieve mission goals. Build meaningful customer relationships by asking thoughtful questions, listening, and operating transparently.
   Growth Mindedness: Commit to company, team, and personal growth. Embrace an entrepreneurial spirit by actively seeking opportunities to support new customer projects and/or solve problems. Actively work to improve your skills, learn to be comfortable with ambiguity, and be resourceful and willing to figure things out and add value.
 
  Our culture fosters teamwork and transparency, empowering you to do great work. Our talented employees drive the success of the company – so our leadership is devoted to your success. We respect you as an individual. We recognize your contributions. We invest in your professional growth. Join us and let's continue to build your career together.
  Currently due to the remote working status of the federal government, most DRT employees are teleworking from home and leveraging video conferencing along with other virtual productivity tools. Under normal circumstances, most of our positions are required to report onsite. However, DRT has not yet determined when employees will be required to return to their onsite workspaces. Please be aware that at the time a date is determined to return to the worksite, the requirement of this position may be to report in person to the work location. 
 DRT Strategies, Inc. (DRT) celebrates diversity and is proud to provide Equal Employment Opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, genetics, disability, or protected veteran status. In addition to federal law requirements, DRT complies with applicable state and local laws governing non-discrimination in employment in every location in which the company has facilities.
  
 xEhfWILf9w","<div>
 <p><b>Overview</b></p>
 <p> DRT Strategies delivers expert management consulting and information technology (IT) solutions to large federal agencies, the U.S. Navy, state and local government and commercial clients in health care, technology, and financial services industries.</p>
 <p> The three letters of our name, DRT, stand for Driving Resolution Together, which is the core philosophy on which the company was founded. That is, we collaborate with our clients to solve their most pressing challenges - together.</p>
 <p> We are problem solvers dedicated to your success, combining Fortune 500 experience with small business responsiveness. We have established a reputation with our clients as a forward-thinking consulting firm with demonstrated success in implementing solutions that lead to meaningful results. Our world-class consultants unite people to work collaboratively to achieve project goals and make your vision a reality.</p>
 <p><b> Project Description</b></p>
 <p> DRT provides data, technology, and mission support to its federal public health partners. We collaborate to address diverse, complex, and emerging problems within the public health domains of environmental health, infectious disease, chronic disease, and emergency preparedness/response. Our support drives the adoption of emerging best practices and technologies to increase the usefulness and value of public health data. Our team develops innovative data capture and analysis techniques in the field to enhance emergency response efforts all around the world. </p>
 <p><b>Job Summary</b></p>
 <p> The Public Health Data Engineer works directly with public health professionals, data analysts, project coordinators, and developers. The Engineer supports the end-to-end data life cycle for public health data repositories ensuring accuracy and efficiency. They collaborate with developers and data team to enable customization of capability features and functionality to meet requirements. They access public health databases, data structures, and data flows to recommend and perform the import, structuring, and encoding of public health data. </p>
 <p><b>Responsibilities</b></p>
 <p> Configures and ingests structured, unstructured, and semi-structured data repositories.<br> <br> Designs, implements, and manages databases and data delivery systems for insights, analysis, and reporting.<br> <br> Works with analysts, subject matter experts, and technologists to determine and implement database systems.<br> <br> Applies expertise in database design and implementation tools, such as entity-relationship data modeling and SQL, distributed computing architectures, operating systems, storage technologies, memory management and networking.<br> <br> Performs streaming and batch data processing, ETL, data wrangling, data ingest, and data access.<br> <br> Develops and manages a schedule to meet deliverables and establish estimated completion dates.<br> <br> Follows the government&#x2019;s software development lifecycle framework and provides agreed upon project documentation.<br> <br> Completes tasks on time with minimal supervision but supports collaboration across teams.<br> <br> Recommends and implements solutions to support access, security, and recovery of data.<br> <br> Monitors data repository performance and recommends and implements enhancements. </p>
 <p><b>Required Experience</b></p>
 <p> Must have or be able to obtain a Public Trust with the US federal government<br> <br> Experience in database or data repository administration roles<br> <br> Advanced understanding of relational database design, development, and management<br> <br> Experience with Microsoft SQL Server<br> <br> Familiarity with common API languages </p>
 <p><b>Preferred Experience</b></p>
 <p> Recent experience with HHS (FDA, CDC, etc.) environment and standards: SDLC, technical architecture, release processes<br> <br> Knowledge of public health data and related privacy requirements (HIPAA, PII, etc.)<br> <br> Experience with Agile and DevOps methodologies<br> <br> Completed trainings or certifications in Microsoft SQL Server<br> <br> Experience with cloud service providers such as Azure and AWS </p>
 <p><b>Education and Training</b></p>
 <p> Degree in Computer Science, Data Science, or related field </p>
 <p>DRT&#x2019;s culture is reflective of our core values: </p>
 <ul>
  <li><b>Professionalism: </b>Be a leader and someone your customers and colleagues can count on by taking ownership and accountability for your work. Demonstrate a solution-oriented mindset and bias for action. Show empathy, dignity, and respect for each other. Be &quot;high minded&quot;, maintain a calm demeanor when dealing with ambiguity or adversity and stay out of the fray (i.e., avoid drama).</li>
  <li><b> Quality: </b>Do things right and do the right thing. Pursue excellence in your work by delivering quality services and products that provide high value and return on investment (ROI).</li>
  <li><b> Teamwork: </b>Work with each other, the customer, and DRT. Demonstrate flexibility, a positive attitude and willingness to work collaboratively to help others and share information.</li>
  <li><b> Customer Mission Focused: </b>Commit to customer success and strive to exceed expectations by understanding the mission, executing the customer&apos;s vision, and solving challenges to drive results and achieve mission goals. Build meaningful customer relationships by asking thoughtful questions, listening, and operating transparently.</li>
  <li><b> Growth Mindedness: </b>Commit to company, team, and personal growth. Embrace an entrepreneurial spirit by actively seeking opportunities to support new customer projects and/or solve problems. Actively work to improve your skills, learn to be comfortable with ambiguity, and be resourceful and willing to figure things out and add value.</li>
 </ul>
 <p> Our culture fosters teamwork and transparency, empowering you to do great work. Our talented employees drive the success of the company &#x2013; so our leadership is devoted to your success. We respect you as an individual. We recognize your contributions. We invest in your professional growth. Join us and let&apos;s continue to build your career together.</p>
 <p> Currently due to the remote working status of the federal government, most DRT employees are teleworking from home and leveraging video conferencing along with other virtual productivity tools. Under normal circumstances, most of our positions are required to report onsite. However, DRT has not yet determined when employees will be required to return to their onsite workspaces. Please be aware that at the time a date is determined to return to the worksite, the requirement of this position may be to report in person to the work location. </p>
 <p>DRT Strategies, Inc. (DRT) celebrates diversity and is proud to provide Equal Employment Opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, genetics, disability, or protected veteran status. In addition to federal law requirements, DRT complies with applicable state and local laws governing non-discrimination in employment in every location in which the company has facilities.</p>
 <p> </p>
 <p>xEhfWILf9w</p>
</div>","https://drtstrategies.applytojob.com/apply/xEhfWILf9w/Public-Health-Data-Engineer?source=INDE","a6e6be032d3fd1bd",,"Full-time",,"Remote","Public Health Data Engineer","Today","2023-10-25T11:46:21.141Z",,,,"2023-10-25T11:46:21.143Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=a6e6be032d3fd1bd&from=jasx&tk=1hdjafq3r2cc7000&vjs=3"
"AllStars-IT","Data Engineer
  
  
   
    
      Level
    
    
      Senior
    
   
   
    
      Department
    
    
      Other IT Positions
    
   
   
    
      Type
    
    
      Full Time
    
   
   
    
      Project
    
    
      WinWard
    
   
  
 
 
  
    Locations:
   
    
     
      
        Remote
      
     
    
   
  
 
 
 
   Job Details
  
   
    
      Posted on:
    
    
      October 24, 2023
    
   
   
    
      Job ID: 
    
    
     148
    
   
  
 
 
   About the Company
   ALLSTARSIT is an international outstaffing service provider that helps businesses recruit, pay, insure, and support top global talent with payroll, benefits, and more. The company specializes in software development services for clients across diverse industries such as cybersecurity, healthcare, fintech, telecommunications, media, and more.  ALLSTARSIT operates development hubs across Central and Eastern Europe, Israel, the UAE, India, the Philippines, and LATAM, with headquarters in San Francisco, US. The company has over 1,000 talented software engineers and tech specialists across all locations.
 
 
  
    About the Project
  
  
  
    Our client is a pioneering maritime AI company offering a comprehensive platform for risk management and maritime domain awareness. With advanced technology and extensive industry expertise, we help organizations overcome maritime challenges, predict future events,and drive success. Windward’s AI-powered solution allows stakeholders to make real-time, predictive intelligence-driven decisions, providing a 360° view of the maritime ecosystem. 
  
  
  Required skills:
  
   
     Bachelor's degree in Computer Science, Engineering, or related field 
    6+ years of professional experience as a Data Engineer, including experience in remote work
     Full proficiency in Kafka and Java (Spring Boot), with a proven track record of building data pipelines and streaming solutions
     Strong experience in designing and constructing ETL processes for data transformation and integration
     Proficiency in AWS services for data storage, processing, and analytics
     Familiarity with Python and NodeJS (a plus) 
    Demonstrated ability to challenge decisions, work independently, and mentor junior team members
     Excellent communication skills in English, both written and verbal 
    Strong problem-solving skills and meticulous attention to detail
   
  
   Scope of work:
  
    As a Senior Data Engineer, you will hold a crucial role in designing, developing, and maintaining our data platform, supporting critical data-driven decision-making processes across the organization. You will collaborate closely with cross-functional teams, including data scientists,analysts, and software engineers, ensuring the smooth flow of data and optimizing data pipelines. The ideal candidate will possess a strong background in data engineering, with proven expertise in Kafka, Java (including Spring Boot), Data Pipelines, AWS, and ETL processes. Familiarity with Python, NodeJS, and a willingness to challenge decisions, work independently, and mentor junior team members will be highly valued. 
   
    Lead the design, development, and maintenance of efficient and scalable data pipelines, facilitating data collection, processing, and transformation from diverse sources 
    Implement real-time data streaming solutions using Kafka, ensuring timely data ingestion and availability 
    Utilize Java, including Spring Boot, to build robust and high-performance data processing applications within our data platform 
    Collaborate closely with cross-functional teams to comprehend data requirements, identify opportunities for data optimization, and support data-driven initiatives 
    Uphold data integrity, reliability, and availability by implementing effective ETL processes and conducting data quality checks 
    Leverage AWS services for data storage, processing, and analytics, adhering to security and performance best practices 
    Monitor and troubleshoot data pipeline performance, proactively identifying bottlenecks and implementing optimizations 
    Create comprehensive documentation for data engineering processes, best practices, and internal guidelines 
    Stay updated with industry trends and emerging technologies in data engineering, contributing to continuous improvement
   
  
  
    Why AllSTARSIT?
    Your health always comes first ‍We prioritize your well-being by providing comprehensive medical insurance coverage. We cover 100% of your medical insurance costs and 75% of the insurance costs for your dependents.   Grow with us both professionally and personally ‍Our commitment to your growth includes an individual education budget, English, Spanish, and Portuguese language courses, tech meetups, online subscriptions, mentorship, internal schools, and engaging webinars. ‍ We take care of your mental health ‍In addition to our comprehensive benefits package, we offer sport/hobby compensation, access to a personal psychologist, and regular mental health webinars. ‍ Teams we are proud of ‍Seize the opportunity to collaborate with ambitious individuals at the forefront of driving innovation across diverse industries. Our team comprises 85% senior-level professionals, ensuring expertise and experience. We foster an environment where every teammate's ideas are valued, encouraging each individual to make a meaningful impact. ‍ People-centric office ‍Located in the center of Kyiv at BC Gulliver, our office boasts three floors with a fantastic terrace. Enjoy daily breakfasts, special team brunches on Fridays, and access to amenities such as a massage room and music lessons. With well-equipped meeting rooms, cozy kitchens stocked with snacks and coffee, shower facilities, gaming consoles, billiards, and table soccer, our office provides a comfortable and enjoyable workspace. Rest assured, we prioritize safety with a certified bomb shelter on the -3 floor.  Flexible working hours ‍We prioritize your comfort and flexibility by offering a schedule tailored to your needs. Whether you prefer working from the office or remotely, the choice is yours. We have partnered with leading international co-working spaces, ensuring that you can enjoy the office experience no matter where you are in the world.  Unleash the spirit of corporate life  Our dedicated Happiness Manager ensures your sense of belonging to corporate life through a combination of virtual and offline social activities, along with occasional gifts to celebrate your achievements. ‍ Work for victory ‍We are committed to supporting Ukraine during the war, even if not directly on the frontlines. We strive to make a positive impact through various charitable activities, such as clearing rubble, organizing events for orphans, caring for pet shelters, assisting refugees, participating in charity sports activities, hosting charity auctions, and promoting blood donations. Together, step by step, we work towards an unyielding victory in helping those in need during these challenging times.","<p></p>
<div>
 <div>
  <div>
   <p>Data Engineer</p>
  </div>
  <div>
   <div>
    <div>
     <p> Level</p>
    </div>
    <div>
      Senior
    </div>
   </div>
   <div>
    <div>
     <p> Department</p>
    </div>
    <div>
      Other IT Positions
    </div>
   </div>
   <div>
    <div>
     <p> Type</p>
    </div>
    <div>
      Full Time
    </div>
   </div>
   <div>
    <div>
     <p> Project</p>
    </div>
    <div>
      WinWard
    </div>
   </div>
  </div>
 </div>
 <div>
  <div>
   <p> Locations:</p>
   <div>
    <div>
     <div>
      <div>
        Remote
      </div>
     </div>
    </div>
   </div>
  </div>
 </div>
 <p></p>
 <div>
  <p> Job Details</p>
  <div>
   <div>
    <div>
      Posted on:
    </div>
    <div>
      October 24, 2023
    </div>
   </div>
   <div>
    <div>
      Job ID: 
    </div>
    <div>
     148
    </div>
   </div>
  </div>
 </div>
 <div>
  <p> About the Company</p>
  <p> ALLSTARSIT is an international outstaffing service provider that helps businesses recruit, pay, insure, and support top global talent with payroll, benefits, and more. The company specializes in software development services for clients across diverse industries such as cybersecurity, healthcare, fintech, telecommunications, media, and more.<br> <br> ALLSTARSIT operates development hubs across Central and Eastern Europe, Israel, the UAE, India, the Philippines, and LATAM, with headquarters in San Francisco, US. The company has over 1,000 talented software engineers and tech specialists across all locations.</p>
 </div>
 <div>
  <div>
   <p> About the Project</p>
  </div>
  <p></p>
  <div>
   <p> Our client is a pioneering maritime AI company offering a comprehensive platform for risk management and maritime domain awareness. With advanced technology and extensive industry expertise, we help organizations overcome maritime challenges, predict future events,and drive success. Windward&#x2019;s AI-powered solution allows stakeholders to make real-time, predictive intelligence-driven decisions, providing a 360&#xb0; view of the maritime ecosystem. </p>
  </div>
  <p></p>
  <p>Required skills:</p>
  <div>
   <ul>
    <li> Bachelor&apos;s degree in Computer Science, Engineering, or related field </li>
    <li>6+ years of professional experience as a Data Engineer, including experience in remote work</li>
    <li> Full proficiency in Kafka and Java (Spring Boot), with a proven track record of building data pipelines and streaming solutions</li>
    <li> Strong experience in designing and constructing ETL processes for data transformation and integration</li>
    <li> Proficiency in AWS services for data storage, processing, and analytics</li>
    <li> Familiarity with Python and NodeJS (a plus) </li>
    <li>Demonstrated ability to challenge decisions, work independently, and mentor junior team members</li>
    <li> Excellent communication skills in English, both written and verbal </li>
    <li>Strong problem-solving skills and meticulous attention to detail</li>
   </ul>
  </div>
  <p> Scope of work:</p>
  <div>
   <p> As a Senior Data Engineer, you will hold a crucial role in designing, developing, and maintaining our data platform, supporting critical data-driven decision-making processes across the organization. You will collaborate closely with cross-functional teams, including data scientists,analysts, and software engineers, ensuring the smooth flow of data and optimizing data pipelines. The ideal candidate will possess a strong background in data engineering, with proven expertise in Kafka, Java (including Spring Boot), Data Pipelines, AWS, and ETL processes. Familiarity with Python, NodeJS, and a willingness to challenge decisions, work independently, and mentor junior team members will be highly valued. </p>
   <ul>
    <li>Lead the design, development, and maintenance of efficient and scalable data pipelines, facilitating data collection, processing, and transformation from diverse sources </li>
    <li>Implement real-time data streaming solutions using Kafka, ensuring timely data ingestion and availability </li>
    <li>Utilize Java, including Spring Boot, to build robust and high-performance data processing applications within our data platform </li>
    <li>Collaborate closely with cross-functional teams to comprehend data requirements, identify opportunities for data optimization, and support data-driven initiatives </li>
    <li>Uphold data integrity, reliability, and availability by implementing effective ETL processes and conducting data quality checks </li>
    <li>Leverage AWS services for data storage, processing, and analytics, adhering to security and performance best practices </li>
    <li>Monitor and troubleshoot data pipeline performance, proactively identifying bottlenecks and implementing optimizations </li>
    <li>Create comprehensive documentation for data engineering processes, best practices, and internal guidelines </li>
    <li>Stay updated with industry trends and emerging technologies in data engineering, contributing to continuous improvement</li>
   </ul>
  </div>
  <div>
   <p> Why AllSTARSIT?</p>
   <p><b> Your health always comes first</b><br> <b>&#x200d;</b>We prioritize your well-being by providing comprehensive medical insurance coverage. We cover 100% of your medical insurance costs and 75% of the insurance costs for your dependents. <br> <br> <b>Grow with us both professionally and personally</b><br> <b>&#x200d;</b>Our commitment to your growth includes an individual education budget, English, Spanish, and Portuguese language courses, tech meetups, online subscriptions, mentorship, internal schools, and engaging webinars.<br> &#x200d;<br> <b>We take care of your mental health</b><br> <b>&#x200d;</b>In addition to our comprehensive benefits package, we offer sport/hobby compensation, access to a personal psychologist, and regular mental health webinars.<br> &#x200d;<br> <b>Teams we are proud of</b><br> <b>&#x200d;</b>Seize the opportunity to collaborate with ambitious individuals at the forefront of driving innovation across diverse industries. Our team comprises 85% senior-level professionals, ensuring expertise and experience. We foster an environment where every teammate&apos;s ideas are valued, encouraging each individual to make a meaningful impact.<br> &#x200d;<br> <b>People-centric office</b><br> <b>&#x200d;</b>Located in the center of Kyiv at BC Gulliver, our office boasts three floors with a fantastic terrace. Enjoy daily breakfasts, special team brunches on Fridays, and access to amenities such as a massage room and music lessons. With well-equipped meeting rooms, cozy kitchens stocked with snacks and coffee, shower facilities, gaming consoles, billiards, and table soccer, our office provides a comfortable and enjoyable workspace. Rest assured, we prioritize safety with a certified bomb shelter on the -3 floor.<br> <br> <b>Flexible working hours</b><br> <b>&#x200d;</b>We prioritize your comfort and flexibility by offering a schedule tailored to your needs. Whether you prefer working from the office or remotely, the choice is yours. We have partnered with leading international co-working spaces, ensuring that you can enjoy the office experience no matter where you are in the world.<br> <br> <b>Unleash the spirit of corporate life</b> <br> Our dedicated Happiness Manager ensures your sense of belonging to corporate life through a combination of virtual and offline social activities, along with occasional gifts to celebrate your achievements.<br> &#x200d;<br> <b>Work for victory</b><br> <b>&#x200d;</b>We are committed to supporting Ukraine during the war, even if not directly on the frontlines. We strive to make a positive impact through various charitable activities, such as clearing rubble, organizing events for orphans, caring for pet shelters, assisting refugees, participating in charity sports activities, hosting charity auctions, and promoting blood donations. Together, step by step, we work towards an unyielding victory in helping those in need during these challenging times.</p>
  </div>
 </div>
</div>
<p></p>","https://www.allstarsit.com/job-posts/data-engineer-148","025f409cbb85bbd8",,"Full-time",,"Remote","Data Engineer","Today","2023-10-25T11:46:29.365Z",,,,"2023-10-25T11:46:29.366Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=025f409cbb85bbd8&from=jasx&tk=1hdjafq3r2cc7000&vjs=3"
"Priority Technology Holdings, LLC","Job title: Staff Data Engineer Reports to: Sr Manager, Data Engineering Department: Data Location: Remote Grade: 19  Our Company: Priority Technology Holdings, Inc. is a leading financial technology company that specializes in providing integrated payments and banking solutions. Our innovative native platform empowers businesses to effortlessly collect, store, and send money in a scalable manner. We are committed to revolutionizing the way companies handle their financial transactions by offering cutting-edge technology and exceptional customer service.  Job Summary: As a Staff Data Engineer, you will play a pivotal role in our data infrastructure team. Your primary responsibilities include designing, developing, and implementing ETL data pipelines from diverse sources, ensuring their optimization and security. You'll collaborate closely with internal teams to identify and resolve operational challenges through the creation of data products, and you'll establish data marts for reliable and speedy data access. Your role also involves designing flexible API-based data services, implementing unit tests and monitoring systems to ensure data quality and efficiency, and working with data scientists, analysts, and stakeholders to fulfill data infrastructure needs. In addition to mentoring junior data engineers and advocating best practices within the team, you'll remain up-to-date with industry trends, technologies, and best practices in data engineering, introducing new tools and techniques to enhance the data platform's capabilities and efficiency. Your contributions will be essential in maintaining and advancing our data infrastructure.
  
  
    RESPONSIBILITIES:
    
   
     Design, develop, and implement optimized and secure ETL data pipelines from various sources.
     Collaborate with internal teams to identify and address operational challenges by designing and implementing data products.
     Create data marts for reliable and fast data access.
     Design flexible API-based data services.
     Establish unit tests and monitoring systems for data quality and efficient correction operations.
     Collaborate with data scientists, analysts, and stakeholders to meet data infrastructure needs.
     Mentor junior data engineers and promote best practices within the team.
     Stay updated on industry trends, technologies, and best practices in data engineering.
     Introduce new tools and techniques to enhance the data platform’s capabilities and efficiency.
   
  
  
   MINIMUM REQUIREMENTS:
  
    8 years of hands-on technical experience in software development, data structures, and algorithms.
    1 year of technical leadership experience, leading project teams and setting technical direction.
    Strong analytical skills with a focus on collecting, organizing, and analyzing information with precision.
    Expertise in cloud platforms and services such as AWS (e.g., Redshift, Kinesis, EMR) or GCP (e.g., BigQuery, Dataflow, Pub/Sub).
    Proficiency in programming languages like Python, Node, Scala, or Go.
    Proficiency in SQL and NoSQL databases including PostgreSQL, MySQL, MongoDB, and Cassandra.
    Staying updated with emerging data technologies, methodologies, and best practices.
    Proven ability to work in a dynamic, fast-paced environment and manage multiple projects concurrently.
  
  
  
    PREFERRED REQUIREMENTS:
  
  
    Proficient in big data tools: Hadoop, Spark, Kafka, etc.
    Skilled in workflow tools like Airflow, AWS Step Function, etc.
    Experienced in developing data solutions for payment processing.
    Proficient in data warehousing solutions, including Snowflake and Redshift.
    Experience in designing REST APIs.
    Proficient in ML libraries like Scrapy (preferred), scikit-learn, TensorFlow, etc.
  
  
  
    Compensation & Benefits:
    
   
     Compensation range: $92,000 - $162,000
     Unlimited PTO after year 1 (3 weeks to start)
     Medical, Dental & Vision
     401k Match
     Education Expense Reimbursement
     Gym Membership Reimbursement
     HSA and FSA for US based employees
     Employee assistance program (EAP)
   
    
    Traditional Physical Requirements:
    
   
     Requires prolonged sitting, standing, bending, stooping and stretching.
     Requires the ability to lift 10 pounds.
     Requires eye-hand coordination, manual dexterity and a normal range of hearing and vision (with or without correction).
   
  
  
    Join our team at Priority Technology Holdings, Inc. and be part of a dynamic and innovative company that is transforming the financial technology landscape. Together, we can shape the future of payments and banking solutions while providing unmatched value to our clients.","<div>
 <div>
  <p><b>Job title:</b> Staff Data Engineer<br> <b>Reports to:</b> Sr Manager, Data Engineering<br> <b>Department:</b> Data<br> <b>Location: </b>Remote<br> <b>Grade:</b> 19<br> <br> <b>Our Company: </b>Priority Technology Holdings, Inc. is a leading financial technology company that specializes in providing integrated payments and banking solutions. Our innovative native platform empowers businesses to effortlessly collect, store, and send money in a scalable manner. We are committed to revolutionizing the way companies handle their financial transactions by offering cutting-edge technology and exceptional customer service.<br> <br> <b>Job Summary:</b> As a Staff Data Engineer, you will play a pivotal role in our data infrastructure team. Your primary responsibilities include designing, developing, and implementing ETL data pipelines from diverse sources, ensuring their optimization and security. You&apos;ll collaborate closely with internal teams to identify and resolve operational challenges through the creation of data products, and you&apos;ll establish data marts for reliable and speedy data access. Your role also involves designing flexible API-based data services, implementing unit tests and monitoring systems to ensure data quality and efficiency, and working with data scientists, analysts, and stakeholders to fulfill data infrastructure needs. In addition to mentoring junior data engineers and advocating best practices within the team, you&apos;ll remain up-to-date with industry trends, technologies, and best practices in data engineering, introducing new tools and techniques to enhance the data platform&apos;s capabilities and efficiency. Your contributions will be essential in maintaining and advancing our data infrastructure.</p>
  <div></div>
  <div>
   <b><br> RESPONSIBILITIES:</b>
   <br> 
   <ul>
    <li> Design, develop, and implement optimized and secure ETL data pipelines from various sources.</li>
    <li> Collaborate with internal teams to identify and address operational challenges by designing and implementing data products.</li>
    <li> Create data marts for reliable and fast data access.</li>
    <li> Design flexible API-based data services.</li>
    <li> Establish unit tests and monitoring systems for data quality and efficient correction operations.</li>
    <li> Collaborate with data scientists, analysts, and stakeholders to meet data infrastructure needs.</li>
    <li> Mentor junior data engineers and promote best practices within the team.</li>
    <li> Stay updated on industry trends, technologies, and best practices in data engineering.</li>
    <li> Introduce new tools and techniques to enhance the data platform&#x2019;s capabilities and efficiency.</li>
   </ul>
  </div>
  <p></p>
  <p><b><br> MINIMUM REQUIREMENTS:</b></p>
  <ul>
   <li> 8 years of hands-on technical experience in software development, data structures, and algorithms.</li>
   <li> 1 year of technical leadership experience, leading project teams and setting technical direction.</li>
   <li> Strong analytical skills with a focus on collecting, organizing, and analyzing information with precision.</li>
   <li> Expertise in cloud platforms and services such as AWS (e.g., Redshift, Kinesis, EMR) or GCP (e.g., BigQuery, Dataflow, Pub/Sub).</li>
   <li> Proficiency in programming languages like Python, Node, Scala, or Go.</li>
   <li> Proficiency in SQL and NoSQL databases including PostgreSQL, MySQL, MongoDB, and Cassandra.</li>
   <li> Staying updated with emerging data technologies, methodologies, and best practices.</li>
   <li> Proven ability to work in a dynamic, fast-paced environment and manage multiple projects concurrently.</li>
  </ul>
  <div></div>
  <div>
   <b><br> PREFERRED REQUIREMENTS:</b>
  </div>
  <ul>
   <li> Proficient in big data tools: Hadoop, Spark, Kafka, etc.</li>
   <li> Skilled in workflow tools like Airflow, AWS Step Function, etc.</li>
   <li> Experienced in developing data solutions for payment processing.</li>
   <li> Proficient in data warehousing solutions, including Snowflake and Redshift.</li>
   <li> Experience in designing REST APIs.</li>
   <li> Proficient in ML libraries like Scrapy (preferred), scikit-learn, TensorFlow, etc.</li>
  </ul>
  <div></div>
  <div>
   <b><br> Compensation &amp; Benefits:</b>
   <br> 
   <ul>
    <li> Compensation range: &#x24;92,000 - &#x24;162,000</li>
    <li> Unlimited PTO after year 1 (3 weeks to start)</li>
    <li> Medical, Dental &amp; Vision</li>
    <li> 401k Match</li>
    <li> Education Expense Reimbursement</li>
    <li> Gym Membership Reimbursement</li>
    <li> HSA and FSA for US based employees</li>
    <li> Employee assistance program (EAP)</li>
   </ul>
   <br> 
   <b> Traditional Physical Requirements:</b>
   <br> 
   <ul>
    <li> Requires prolonged sitting, standing, bending, stooping and stretching.</li>
    <li> Requires the ability to lift 10 pounds.</li>
    <li> Requires eye-hand coordination, manual dexterity and a normal range of hearing and vision (with or without correction).</li>
   </ul>
  </div>
  <div>
   <br> Join our team at Priority Technology Holdings, Inc. and be part of a dynamic and innovative company that is transforming the financial technology landscape. Together, we can shape the future of payments and banking solutions while providing unmatched value to our clients.
  </div>
 </div>
</div>","https://workforcenow.adp.com/mascsr/default/mdf/recruitment/recruitment.html?cid=94f16522-9f0f-4a13-b72c-b0eaf60d119d&ccId=19000101_000001&jobId=491350&source=IN&lang=en_US&ittk=M0X9CG1P5K","06686c071e983d15",,,,"Remote","Staff Data Engineer","Today","2023-10-25T11:46:34.034Z",,,"$92,000 - $162,000 a year","2023-10-25T11:46:34.036Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=06686c071e983d15&from=jasx&tk=1hdjafq3r2cc7000&vjs=3"
"Limble CMMS","At Limble, we empower the unsung heroes that support the world. We've built the #1 Maintenance Management SaaS (CMMS) platform in an industry projected to double over the next five years. Organizations like McDonalds, Nike, Unilever and Rite Aid rely on Limble every day to streamline, operationalize and improve their maintenance programs, while empowering employees.
  What sets us apart? Our software is easy to use and we truly care about the success of our customers. They see us as a partner rather than just another vendor due to a relationship built on trust. We continue to experience triple-digit growth year after year and are just getting started!
  We are looking for a Data Warehouse Engineer to join our growing Data team at Limble. Ideal candidates will have a strong grasp of the modern data stack including tools such as Snowflake, dbt and Fivetran. Cloud platform experience such as AWS, GCP or Azure is a huge plus. As a Data Warehouse Engineer at Limble, your goal is to provide a best in class data warehouse that will be used to unlock key insights for our customers and business stakeholders
 
  How you’ll make an impact:
 
   Design, build, and maintain our data warehouse infrastructure, focusing on optimizing data storage, data pipelines, and data transformation processes.
   Leverage your expertise with Snowflake, dbt, and Fivetran to ensure efficient data loading, transformation, and accessibility.
   Utilize SQL and Python for scripting, automation, and data transformation to enhance our data warehouse's efficiency and accuracy.
   Collaborate closely with internal teams and business stakeholders to understand data requirements and guide stakeholders toward effective data solutions.
   Participate in architectural decisions.
   Ensure the data platform runs smoothly without bugs, implement data quality checks, and address anomalies promptly to maintain a high standard of data integrity.
   Maintain comprehensive documentation of data warehouse processes, data pipelines, and configurations to ensure transparency and continuity.
 
 
  Requirements:
 
   5+ years of experience in data engineering, data warehouse engineering, or a related role.
   Demonstrated experience with data modeling including modeling methodologies such as Kimball.
   Proficiency in SQL and Python as well as a cloud platform experience, preferably AWS.
   Prior experience in a start-up environment is preferred.
   Demonstrated adaptability and problem-solving skills.
   Excellent stakeholder management, communication skills, and the ability to convey technical challenges and solutions clearly.
   A commitment to ensuring data warehouse stability and stakeholder satisfaction in terms of the data platform.
 
  Benefits:
 
   $120,000 - $150,000, depending on experience
   Stock options
   Fully remote role
   Flexible PTO
   Paid parental leave
   Health, Dental, Vision, and Life insurance
   HSA with company contribution match
   401k with company contribution match
   Opportunities to grow with us!
 
  At Limble we are solution-oriented and customer-obsessed. We hire with a people-first approach, and we understand there’s no such thing as a perfect candidate. Limble’s company culture and values are based on collaboration and transparency. Our customers come from all different backgrounds and so do our employees. If you’re results-driven, enjoy solving complex problems, and are curious about what you could accomplish at a rapidly scaling startup, we’d love to hear from you.","<div>
 <p>At Limble, we empower the unsung heroes that support the world. We&apos;ve built the #1 Maintenance Management SaaS (CMMS) platform in an industry projected to double over the next five years. Organizations like McDonalds, Nike, Unilever and Rite Aid rely on Limble every day to streamline, operationalize and improve their maintenance programs, while empowering employees.</p>
 <p><br> What sets us apart? Our software is easy to use and we truly care about the success of our customers. They see us as a partner rather than just another vendor due to a relationship built on trust. We continue to experience triple-digit growth year after year and are just getting started!</p>
 <p><br> We are looking for a Data Warehouse Engineer to join our growing Data team at Limble. Ideal candidates will have a strong grasp of the modern data stack including tools such as Snowflake, dbt and Fivetran. Cloud platform experience such as AWS, GCP or Azure is a huge plus. As a Data Warehouse Engineer at Limble, your goal is to provide a best in class data warehouse that will be used to unlock key insights for our customers and business stakeholders</p>
 <p></p>
 <p><b> How you&#x2019;ll make an impact:</b></p>
 <ul>
  <li><p> Design, build, and maintain our data warehouse infrastructure, focusing on optimizing data storage, data pipelines, and data transformation processes.</p></li>
  <li><p> Leverage your expertise with Snowflake, dbt, and Fivetran to ensure efficient data loading, transformation, and accessibility.</p></li>
  <li><p> Utilize SQL and Python for scripting, automation, and data transformation to enhance our data warehouse&apos;s efficiency and accuracy.</p></li>
  <li><p> Collaborate closely with internal teams and business stakeholders to understand data requirements and guide stakeholders toward effective data solutions.</p></li>
  <li><p> Participate in architectural decisions.</p></li>
  <li><p> Ensure the data platform runs smoothly without bugs, implement data quality checks, and address anomalies promptly to maintain a high standard of data integrity.</p></li>
  <li><p> Maintain comprehensive documentation of data warehouse processes, data pipelines, and configurations to ensure transparency and continuity.</p></li>
 </ul>
 <p></p>
 <p><b> Requirements:</b></p>
 <ul>
  <li><p> 5+ years of experience in data engineering, data warehouse engineering, or a related role.</p></li>
  <li><p> Demonstrated experience with data modeling including modeling methodologies such as Kimball.</p></li>
  <li><p> Proficiency in SQL and Python as well as a cloud platform experience, preferably AWS.</p></li>
  <li><p> Prior experience in a start-up environment is preferred.</p></li>
  <li><p> Demonstrated adaptability and problem-solving skills.</p></li>
  <li><p> Excellent stakeholder management, communication skills, and the ability to convey technical challenges and solutions clearly.</p></li>
  <li><p> A commitment to ensuring data warehouse stability and stakeholder satisfaction in terms of the data platform.</p></li>
 </ul>
 <p><b><br> Benefits:</b></p>
 <ul>
  <li><p> &#x24;120,000 - &#x24;150,000, depending on experience</p></li>
  <li><p> Stock options</p></li>
  <li><p> Fully remote role</p></li>
  <li><p> Flexible PTO</p></li>
  <li><p> Paid parental leave</p></li>
  <li><p> Health, Dental, Vision, and Life insurance</p></li>
  <li><p> HSA with company contribution match</p></li>
  <li><p> 401k with company contribution match</p></li>
  <li><p> Opportunities to grow with us!</p></li>
 </ul>
 <p><br> At Limble we are solution-oriented and customer-obsessed. We hire with a people-first approach, and we understand there&#x2019;s no such thing as a perfect candidate. Limble&#x2019;s company culture and values are based on collaboration and transparency. Our customers come from all different backgrounds and so do our employees. If you&#x2019;re results-driven, enjoy solving complex problems, and are curious about what you could accomplish at a rapidly scaling startup, we&#x2019;d love to hear from you.</p>
</div>","https://jobs.ashbyhq.com/limble/c31183f0-48d1-4ef3-a62f-2d5e0d997231?utm_source=Ddov3JQjQG","2c977bb9946a4f1b",,"Full-time",,"Remote","Data Warehouse Engineer","Today","2023-10-25T11:46:32.395Z",,,,"2023-10-25T11:46:32.397Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=2c977bb9946a4f1b&from=jasx&tk=1hdjafq3r2cc7000&vjs=3"
"Kyte","About us
 
 
 
   Kyte is a Series B technology platform that brings magic and seamless delivery into the experience of getting a car in cities for any trip longer than a rideshare. We unlock the freedom to go places by committing to relentless customer centricity, technology-powered operational excellence and capital efficiency. The sky's the limit when it comes to Kyte.
 
 
 
   At Kyte, we love what we do and we believe our diverse team of ridiculously ambitious individuals is what makes our company unstoppable. We are a transparency first, egoless, and down-to-earth group of people, scattered across the globe, with a true passion for the Kyte mission. No matter where you are or what you are working on, you will have a significant level of ownership and impact on our company's trajectory and growth into the future.
 
 
 
   Help us unlock the freedom to go places!
 
 
 
   About The Role
 
 
 
   As Senior Software Engineer on the Data team, you'll be responsible for building reliable analytics data modes, pipelines for internal data consumers and engineers at Kyte. You will lead and partner with our engineering team and other analysts to develop data ingestion pipelines that assist with enabling better metrics and insights for the rest of the business. In this role, you will lead highly ambiguous projects and will need to make sense of requirements from stakeholder teams, without the help of a product or program representative. You'll partner with our Staff Data Infrastructure Engineer to build solutions that scale data engineering and consumption to Kyte. 
 
 What You'll Do
 
   Architect, build, and test data pipelines using ETL workflows to pull data from a wide variety of backend systems and tools including Postgres, S3, CSV, and other data sources
   Help design and write event-driven architecture solutions for Kyte’s software and systems applications
   Help us build a world-class data platform so we can achieve our goal of becoming the world’s largest fleet operator by 2030
   Work cross functionally with Engineering, Product, Operations, Finance, and other teams as applicable to foster a strong data culture
   Leave the code in a better state than when you found it (progressive refactor)
   Contribute to new architecture designs to fit Kyte’s growing marketplace challenges
 
  Required
 
   Minimum 5-7 years of data engineering experienceFamiliar with AWS (Lambda, S3, Glue), event streaming services (Kafka, Spark, etc)
   Experience with data analytics storage solutions such as PostgreSQL, Redshift, or Athena
   Experience with data platform technologies, like Airflow Experience with Python, Node, Java and/or Go
   BS/MS degree in Computer Science, related field or equivalent technical experience
   Experience working in an agile setup using continuous deployment and delivery
 
  Preferred
 
   Experience architecting microservices at scale
   Extensive experience with AWS or related cloud providers
   You’ve built and improved on database architectures, data warehousing/lake solutions, or related tech
   Experience with containerization (eg., Docker, Kubernetes)
 
 
   Perks and benefits
 
 
  Comprehensive Medical, Dental, & Vision Health Insurance (Individuals 100% Covered, Dependents 80% Covered)
  401(k) Retirement Plan
  $200 Monthly Commuter Benefit
  $75 Monthly Flexible Wellness Benefit
  Annual WFH Stipend (Remote Employees)
  Annual Learning and Development Budget
  Paid Github Copilot license
 
 
 
   #LI-KYTEJOBS
 
 
 
   Don't meet every single requirement? Studies have shown that women and people of color are less likely to apply for jobs unless they meet every single requirement. At Kyte, we're building an inclusive, diverse, and authentic workplace, so if you're excited about our vision/mission and this role caught your eye - we encourage you to apply! Don't worry if your previous experience doesn't match perfectly, you may be the right candidate for this role, or even another role at Kyte!","<div>
 <div>
  <b>About us</b>
 </div>
 <div></div>
 <div>
  <br> Kyte is a Series B technology platform that brings magic and seamless delivery into the experience of getting a car in cities for any trip longer than a rideshare. We unlock the freedom to go places by committing to relentless customer centricity, technology-powered operational excellence and capital efficiency. The sky&apos;s the limit when it comes to Kyte.
 </div>
 <div></div>
 <div>
  <br> At Kyte, we love what we do and we believe our diverse team of ridiculously ambitious individuals is what makes our company unstoppable. We are a transparency first, egoless, and down-to-earth group of people, scattered across the globe, with a true passion for the Kyte mission. No matter where you are or what you are working on, you will have a significant level of ownership and impact on our company&apos;s trajectory and growth into the future.
 </div>
 <div></div>
 <div>
  <br> Help us unlock the freedom to go places!
 </div>
 <div></div>
 <div>
  <b><br> About The Role</b>
 </div>
 <div></div>
 <div>
  <br> As Senior Software Engineer on the Data team, you&apos;ll be responsible for building reliable analytics data modes, pipelines for internal data consumers and engineers at Kyte. You will lead and partner with our engineering team and other analysts to develop data ingestion pipelines that assist with enabling better metrics and insights for the rest of the business. In this role, you will lead highly ambiguous projects and will need to make sense of requirements from stakeholder teams, without the help of a product or program representative. You&apos;ll partner with our Staff Data Infrastructure Engineer to build solutions that scale data engineering and consumption to Kyte. 
 </div>
 <h3 class=""jobSectionHeader""><b>What You&apos;ll Do</b></h3>
 <ul>
  <li> Architect, build, and test data pipelines using ETL workflows to pull data from a wide variety of backend systems and tools including Postgres, S3, CSV, and other data sources</li>
  <li> Help design and write event-driven architecture solutions for Kyte&#x2019;s software and systems applications</li>
  <li> Help us build a world-class data platform so we can achieve our goal of becoming the world&#x2019;s largest fleet operator by 2030</li>
  <li> Work cross functionally with Engineering, Product, Operations, Finance, and other teams as applicable to foster a strong data culture</li>
  <li> Leave the code in a better state than when you found it (progressive refactor)</li>
  <li> Contribute to new architecture designs to fit Kyte&#x2019;s growing marketplace challenges</li>
 </ul>
 <h3 class=""jobSectionHeader""><b> Required</b></h3>
 <ul>
  <li> Minimum 5-7 years of data engineering experienceFamiliar with AWS (Lambda, S3, Glue), event streaming services (Kafka, Spark, etc)</li>
  <li> Experience with data analytics storage solutions such as PostgreSQL, Redshift, or Athena</li>
  <li> Experience with data platform technologies, like Airflow Experience with Python, Node, Java and/or Go</li>
  <li> BS/MS degree in Computer Science, related field or equivalent technical experience</li>
  <li> Experience working in an agile setup using continuous deployment and delivery</li>
 </ul>
 <h3 class=""jobSectionHeader""><b> Preferred</b></h3>
 <ul>
  <li> Experience architecting microservices at scale</li>
  <li> Extensive experience with AWS or related cloud providers</li>
  <li> You&#x2019;ve built and improved on database architectures, data warehousing/lake solutions, or related tech</li>
  <li> Experience with containerization (eg., Docker, Kubernetes)</li>
 </ul>
 <div>
  <b> Perks and benefits</b>
 </div>
 <ul>
  <li>Comprehensive Medical, Dental, &amp; Vision Health Insurance (Individuals 100% Covered, Dependents 80% Covered)</li>
  <li>401(k) Retirement Plan</li>
  <li>&#x24;200 Monthly Commuter Benefit</li>
  <li>&#x24;75 Monthly Flexible Wellness Benefit</li>
  <li>Annual WFH Stipend (<i>Remote Employees)</i></li>
  <li>Annual Learning and Development Budget</li>
  <li>Paid Github Copilot license</li>
 </ul>
 <div></div>
 <div>
  <br> #LI-KYTEJOBS
 </div>
 <div></div>
 <div>
  <i><br> Don&apos;t meet every single requirement? Studies have shown that women and people of color are less likely to apply for jobs unless they meet every single requirement. At Kyte, we&apos;re building an inclusive, diverse, and authentic workplace, so if you&apos;re excited about our vision/mission and this role caught your eye - we encourage you to apply! Don&apos;t worry if your previous experience doesn&apos;t match perfectly, you may be the right candidate for this role, or even another role at Kyte!</i>
 </div>
</div>","https://www.indeed.com/rc/clk?jk=bb79f219418b4893&atk=&xpse=SoDD67I3JzdL8ARTop0LbzkdCdPP","bb79f219418b4893",,"Full-time",,"San Francisco, CA","Senior Software Engineer - Data","Just posted","2023-10-25T11:46:40.130Z","2.8","21",,"2023-10-25T11:46:40.219Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=bb79f219418b4893&from=jasx&tk=1hdjafq3r2cc7000&vjs=3"
"Cyberjin","Hybrid/Remote role
  Looking for a talented Data Engineer to support the acquisition of mission critical and mission support data sets. The preferred candidate will have a background in supporting cyber and/or network related missions within the military spaces, as either a developer, analyst or engineer. Work is mostly on customer site in San Antonio, TX with some hybrid support.
 
  Essential Job Responsibilities
  The ideal candidate will have worked with big data systems, complex structured and unstructured data sets, and have supported government data acquisition, analysis, and/or sharing efforts in the past. 
 To excel in the position, the candidate shall have a strong attention to detail, be able to understand technical complexities, and have the willingness to learn and adapt to the situation. 
 The candidate will work both independently and as part of a large team to accomplish client objectives. 
 Minimum Qualifications
  Security Clearance - Must have a current TS/SCI level security clearance and therefore all candidates must be a U.S. Citizen. 
 5 years experience as a developer, analyst, or engineer with a Bachelors in related field; OR 3 years relevant experience with Masters in related field; OR High School Diploma or equivalent and 9 years relevant experience.
  Experience with programming languages such as Python and Java.
  Proficiency with acquisition and understanding of network data and the associated metadata.
  Fluency with data extraction, translation, and loading including data prep and labeling to enable data analytics.
  Experience with Kibana and Elasticsearch.
  Familiarity with various log formats such as JSON, XML, and others.
  Experience with data flow, management, and storage solutions (i.e. Kafka, NiFi, and AWS S3 and SQS solutions).
  Ability to decompose technical problems and troubleshoot system and dataflow issues.
  Must be able to work on customer site most of the time.
  Preferred Requirements
  Experience with NOSQL databases such as Accumulo desired
  Prior Experience supporting cyber and/or network security operations within a large enterprise, as either an analyst, engineer, architect, or developer.
  
 c9sKXV1qkh","<div>
 <p><b>Hybrid/Remote role</b></p>
 <p> Looking for a talented Data Engineer to support the acquisition of mission critical and mission support data sets. The preferred candidate will have a background in supporting cyber and/or network related missions within the military spaces, as either a developer, analyst or engineer. Work is mostly on customer site in San Antonio, TX with some hybrid support.</p>
 <p></p>
 <p><br> Essential Job Responsibilities</p>
 <p> The ideal candidate will have worked with big data systems, complex structured and unstructured data sets, and have supported government data acquisition, analysis, and/or sharing efforts in the past. </p>
 <p>To excel in the position, the candidate shall have a strong attention to detail, be able to understand technical complexities, and have the willingness to learn and adapt to the situation. </p>
 <p>The candidate will work both independently and as part of a large team to accomplish client objectives. </p>
 <p>Minimum Qualifications</p>
 <p> Security Clearance - Must have a current TS/SCI level security clearance and therefore all candidates must be a U.S. Citizen. </p>
 <p>5 years experience as a developer, analyst, or engineer with a Bachelors in related field; OR 3 years relevant experience with Masters in related field; OR High School Diploma or equivalent and 9 years relevant experience.</p>
 <p> Experience with programming languages such as Python and Java.</p>
 <p> Proficiency with acquisition and understanding of network data and the associated metadata.</p>
 <p> Fluency with data extraction, translation, and loading including data prep and labeling to enable data analytics.</p>
 <p> Experience with Kibana and Elasticsearch.</p>
 <p> Familiarity with various log formats such as JSON, XML, and others.</p>
 <p> Experience with data flow, management, and storage solutions (i.e. Kafka, NiFi, and AWS S3 and SQS solutions).</p>
 <p> Ability to decompose technical problems and troubleshoot system and dataflow issues.</p>
 <p> Must be able to work on customer site most of the time.</p>
 <p> Preferred Requirements</p>
 <p> Experience with NOSQL databases such as Accumulo desired</p>
 <p> Prior Experience supporting cyber and/or network security operations within a large enterprise, as either an analyst, engineer, architect, or developer.</p>
 <p> </p>
 <p>c9sKXV1qkh</p>
</div>","https://cyberjin.applytojob.com/apply/c9sKXV1qkh/Data-Engineer-TSSCI?source=INDE","699df5b9ae819cbf",,"Full-time",,"Remote","Data Engineer TS/SCI","2 days ago","2023-10-23T11:46:44.333Z",,,,"2023-10-25T11:46:44.344Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=699df5b9ae819cbf&from=jasx&tk=1hdjagqsjjgbm800&vjs=3"
"Data Ideology","Data Ideology
  At DI, we provide Data & Analytics expertise to drive measurable business outcomes, often solving complex business problems for our clients. Our data analytics advisory services enable our customers to transform data into insights by driving a culture of empowerment and ownership of results. Our team consists of highly motivated individuals passionate about learning, understanding, collaborating, and intellectually curious. For more information about Data Ideology, visit www.dataideology.com
  Senior Data Engineer - Full-time
  We are looking for a Data Engineer to join our growing team. Data Engineer will leverage their business and technical knowledge to develop production-ready data models by integrating multiple data sources while working with business and technical teams to understand business strategy and objectives, gather information, and ensure business requirements are being fulfilled throughout the entire data & analytics lifecycle.
  Key Responsibilities
  To perform in this position successfully, an individual must be able to perform each essential duty satisfactorily. Reasonable accommodations may be made to enable individuals with disabilities to perform essential functions. Other duties may be assigned to meet business needs.
 
   Ability to collect and understand business requirements and translate those requirements into an actionable data warehouse plan.
   Knowledge of multi-dimensional and tabular design patterns and ability to identify solutions that leverage these modeling techniques.
   Ability to work within the SDLC framework in multiple environments and understand the complexities and dependencies of the data warehouse built within those constraints.
   Ability to define and implement best practices across database design and ETL.
   Ability to direct the work of others, including but not limited to directing ETL development, demonstrating an understanding of key concepts of ETL/ELT, including best practices for optimization and scheduling.
 
  Supervisory Responsibilities: None
  Qualifications
  Education and Experience:
 
   Proven understanding of Data Warehousing, Data Architecture, and BI.
   Experience with data pipelines and architecture/engineering.
   Knowledge of modern apps and data platforms.
   Cloud-based project implementation.
   Azure Data Factory experience is a plus
 
  Knowledge, Skills, and Abilities:
 
   BI/Data Warehousing (3+ years)
   Cloud platforms (2+ years)
   ETL (3+ years)
   SQL (3+ years)
   Data Modeling
   Data Vault Modeling
   Healthcare experience a plus
   Consulting experience a plus
 
  Work Environment:
 
   Remote work from home.
   Hours of work and days are generally Monday through Friday. Specific business hours will depend on client needs.
 
  Physical Demands:
 
   Must be able to remain in a stationary position 50% of the time.
   The person in this position must occasionally move about inside the office to access file cabinets, library stacks, office machinery, etc.
   Constantly operates a computer and other office productivity machinery, such as a calculator, copy machine, and printer.
   The person in this position frequently communicates with clients and coworkers. Must be able to exchange accurate information in these situations.
 
  Benefits:
 
   Unlimited Discretionary Time Off Policy
   Insurance (medical, dental, vision) for employees
   100% company paid - short and long-term disability insurance for employees
   100% company paid - life insurance and AD&D insurance for employees
   100% company paid – employee assistance program
   Retirement plans with company match
   Training and Certification Reimbursement annually
   Performance-based incentive program
   Commission incentive program
   Profit Sharing Plan
   Referral Bonuses
 
  Data Ideology is an EEO Employer
  
 HZVKHsjsPY","<div>
 <p><b>Data Ideology</b></p>
 <p> At DI, we provide Data &amp; Analytics expertise to drive measurable business outcomes, often solving complex business problems for our clients. Our data analytics advisory services enable our customers to transform data into insights by driving a culture of empowerment and ownership of results. Our team consists of highly motivated individuals passionate about learning, understanding, collaborating, and intellectually curious. For more information about Data Ideology, visit www.dataideology.com</p>
 <p><b> Senior Data Engineer - Full-time</b></p>
 <p> We are looking for a Data Engineer to join our growing team. Data Engineer will leverage their business and technical knowledge to develop production-ready data models by integrating multiple data sources while working with business and technical teams to understand business strategy and objectives, gather information, and ensure business requirements are being fulfilled throughout the entire data &amp; analytics lifecycle.</p>
 <p><b> Key Responsibilities</b></p>
 <p> To perform in this position successfully, an individual must be able to perform each essential duty satisfactorily. Reasonable accommodations may be made to enable individuals with disabilities to perform essential functions. Other duties may be assigned to meet business needs.</p>
 <ul>
  <li> Ability to collect and understand business requirements and translate those requirements into an actionable data warehouse plan.</li>
  <li> Knowledge of multi-dimensional and tabular design patterns and ability to identify solutions that leverage these modeling techniques.</li>
  <li> Ability to work within the SDLC framework in multiple environments and understand the complexities and dependencies of the data warehouse built within those constraints.</li>
  <li> Ability to define and implement best practices across database design and ETL.</li>
  <li> Ability to direct the work of others, including but not limited to directing ETL development, demonstrating an understanding of key concepts of ETL/ELT, including best practices for optimization and scheduling.</li>
 </ul>
 <p> Supervisory Responsibilities: None</p>
 <p><b> Qualifications</b></p>
 <p><i> Education and Experience:</i></p>
 <ul>
  <li> Proven understanding of Data Warehousing, Data Architecture, and BI.</li>
  <li> Experience with data pipelines and architecture/engineering.</li>
  <li> Knowledge of modern apps and data platforms.</li>
  <li> Cloud-based project implementation.</li>
  <li> Azure Data Factory experience is a plus</li>
 </ul>
 <p><i> Knowledge, Skills, and Abilities:</i></p>
 <ul>
  <li> BI/Data Warehousing (3+ years)</li>
  <li> Cloud platforms (2+ years)</li>
  <li> ETL (3+ years)</li>
  <li> SQL (3+ years)</li>
  <li> Data Modeling</li>
  <li> Data Vault Modeling</li>
  <li> Healthcare experience a plus</li>
  <li> Consulting experience a plus</li>
 </ul>
 <p> Work Environment:</p>
 <ul>
  <li> Remote work from home.</li>
  <li> Hours of work and days are generally Monday through Friday. Specific business hours will depend on client needs.</li>
 </ul>
 <p> Physical Demands:</p>
 <ul>
  <li> Must be able to remain in a stationary position 50% of the time.</li>
  <li> The person in this position must occasionally move about inside the office to access file cabinets, library stacks, office machinery, etc.</li>
  <li> Constantly operates a computer and other office productivity machinery, such as a calculator, copy machine, and printer.</li>
  <li> The person in this position frequently communicates with clients and coworkers. Must be able to exchange accurate information in these situations.</li>
 </ul>
 <p> Benefits:</p>
 <ul>
  <li> Unlimited Discretionary Time Off Policy</li>
  <li> Insurance (medical, dental, vision) for employees</li>
  <li> 100% company paid - short and long-term disability insurance for employees</li>
  <li> 100% company paid - life insurance and AD&amp;D insurance for employees</li>
  <li> 100% company paid &#x2013; employee assistance program</li>
  <li> Retirement plans with company match</li>
  <li> Training and Certification Reimbursement annually</li>
  <li> Performance-based incentive program</li>
  <li> Commission incentive program</li>
  <li> Profit Sharing Plan</li>
  <li> Referral Bonuses</li>
 </ul>
 <p> Data Ideology is an EEO Employer</p>
 <p> </p>
 <p>HZVKHsjsPY</p>
</div>","https://dataideology.applytojob.com/apply/HZVKHsjsPY/Senior-Data-Engineer?source=INDE","b856abef702a00db",,"Full-time",,"Remote","Senior Data Engineer","1 day ago","2023-10-24T11:46:45.477Z","5","9",,"2023-10-25T11:46:45.478Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=b856abef702a00db&from=jasx&tk=1hdjagu34k269800&vjs=3"
"Medtronic","Position Description:  Sr. Cloud Data Engineer for Medtronic, Inc. Minneapolis, MN. Multiple positions available. Designs, develops and maintains secure platforms for analytics & data science algorithms that support diabetes applications. Optimize security, robustness, scalability, performance, and cost to build and develop data driven products with prediction and forecasting capabilities. Develop on-premise and cloud-based software architecture systems. Provide critical support for AWS cloud environment and its associated services. Perform relational & NoSQL database design & optimization. Coordinate machine learning and statistical modelling for production systems using Python and Java. Develop ETL tools and logic to connect data pipelines to centralized datalake. Utilize real-time streaming data & processing using Apache Streams & Kafka. Develop analytical algorithms, reporting tools & dashboards to aid research, product strategy & business intelligence including attrition prediction & customer retention. Understand real-world and clinical diabetes solutions and navigates and uses data from continuous glucose monitoring systems and insulin pumps. Design, develop, debug and deploy Devops pipeline components related to technical infrastructure and ML/AIOps software programs. Create designs for multilayer infrastructure access management, security and system health. Monitor application performance, develop correlations for the existing automations and provide technical software solutions to isolate and resolve issues with the code, network and infrastructure. Coordinate SDLC (Software Development Life Cycle) processes and tool chains. Position is eligible for telecommuting status from anywhere in the United States. Multiple Positions available. #LI-DNI. 
 
 Basic Qualifications:  Requires a Master’s degree in Computer Science, Computer Information Systems, Statistics, Biomedical Engineering, Applied Mathematics, or related engineering or technical field and 2 years of experience as a cloud engineer or any occupation in data engineering or data science; or a Bachelor’s degree in Computer Science, Computer Information Systems, Statistics, Biomedical Engineering, Applied Mathematics, or related engineering or technical field and 5 years of experience as a cloud engineer or any occupation in data engineering or data science. Must possess 2 years of experience with each of the following: developing of on-premise and cloud-based software architecture systems; relational and NoSQL database design and optimization; development for production grade analytical systems using Python and Java; developing ETL tools and logic to connect data pipelines; real-time streaming data & processing; developing analytical algorithms, reporting tools & dashboards for research, product strategy and business intelligence; developing, designing, debugging and deploying Devops pipeline components related to technical infrastructure; developing designs for multilayer infrastructure access management, security and system health; monitor application performance and provide technical software solutions to isolate and resolve issues with the code, network and infrastructure; and SDLC (Software Development Life Cycle) processes and tool chains.","<div>
 <div>
  <p>Position Description: <br> Sr. Cloud Data Engineer for Medtronic, Inc. Minneapolis, MN. Multiple positions available. Designs, develops and maintains secure platforms for analytics &amp; data science algorithms that support diabetes applications. Optimize security, robustness, scalability, performance, and cost to build and develop data driven products with prediction and forecasting capabilities. Develop on-premise and cloud-based software architecture systems. Provide critical support for AWS cloud environment and its associated services. Perform relational &amp; NoSQL database design &amp; optimization. Coordinate machine learning and statistical modelling for production systems using Python and Java. Develop ETL tools and logic to connect data pipelines to centralized datalake. Utilize real-time streaming data &amp; processing using Apache Streams &amp; Kafka. Develop analytical algorithms, reporting tools &amp; dashboards to aid research, product strategy &amp; business intelligence including attrition prediction &amp; customer retention. Understand real-world and clinical diabetes solutions and navigates and uses data from continuous glucose monitoring systems and insulin pumps. Design, develop, debug and deploy Devops pipeline components related to technical infrastructure and ML/AIOps software programs. Create designs for multilayer infrastructure access management, security and system health. Monitor application performance, develop correlations for the existing automations and provide technical software solutions to isolate and resolve issues with the code, network and infrastructure. Coordinate SDLC (Software Development Life Cycle) processes and tool chains. Position is eligible for telecommuting status from anywhere in the United States. Multiple Positions available. #LI-DNI. </p>
 </div>
 <p>Basic Qualifications: <br> Requires a Master&#x2019;s degree in Computer Science, Computer Information Systems, Statistics, Biomedical Engineering, Applied Mathematics, or related engineering or technical field and 2 years of experience as a cloud engineer or any occupation in data engineering or data science; or a Bachelor&#x2019;s degree in Computer Science, Computer Information Systems, Statistics, Biomedical Engineering, Applied Mathematics, or related engineering or technical field and 5 years of experience as a cloud engineer or any occupation in data engineering or data science. Must possess 2 years of experience with each of the following: developing of on-premise and cloud-based software architecture systems; relational and NoSQL database design and optimization; development for production grade analytical systems using Python and Java; developing ETL tools and logic to connect data pipelines; real-time streaming data &amp; processing; developing analytical algorithms, reporting tools &amp; dashboards for research, product strategy and business intelligence; developing, designing, debugging and deploying Devops pipeline components related to technical infrastructure; developing designs for multilayer infrastructure access management, security and system health; monitor application performance and provide technical software solutions to isolate and resolve issues with the code, network and infrastructure; and SDLC (Software Development Life Cycle) processes and tool chains.</p>
</div>","https://medtronic.eightfold.ai/careers?pid=18737484&domain=medtronic.com&src=JB-10288","c2e2d3550b6d5721",,,,"710 MEDTRONIC PKWY, Minneapolis, MN 55432","Sr. Cloud Data Engineer","1 day ago","2023-10-24T11:46:47.795Z","3.9","5059",,"2023-10-25T11:46:47.796Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=c2e2d3550b6d5721&from=jasx&tk=1hdjagu34k269800&vjs=3"
"SLT","SLT Job Listing
Position: ETL Data EngineerLocation: RemoteType: Contract / 1099Length of Contract: 1.5 yearsApplication Deadline: October 25, 2023
About SLT:SLT specializes in providing custom software solutions and high-precision drone services. Our expertise ranges from software services like cloud infrastructure and generative AI solutions to high-precision drone services such as mapping, 3D modeling, and infrastructure inspection.
Job Description:SLT is seeking a full-time resource specializing in software engineering for a critical data migration project. The primary goal is to transition, test, and deploy Data Warehouse ETLs fromIBM DataStage V 8.5 on Windows 2008 Server to IBM DataStage V 11.7 on Red Hat Linux 8.
The DataStage ETL tool under consideration plays a pivotal role in facilitating data transfer from distinct source systems to the EPM Data Warehouse. This warehouse is instrumental for numerous agency users across various governmental departments, aiding in reporting activities.The prevailing DataStage software operates on Windows 2008 physical machines, which have reached the end of their support life. The data engineer will undertake these migration duties, complementing the efforts of existing staff members also engaged in this endeavor. While in-house staff will oversee project management aspects, the engineer will collaborate extensively with the Data Warehouse Team lead alongside other project team members.
Responsibilities:● Test migrated ETL jobs, including data validation and data integrity testing.
● Fix issues found during migration or testing.● Create new ETL jobs and update existing ETL jobs.● Migrate ETL jobs from DataStage V8.5 server to DataStage V11.7 server.● Document processes and details as directed by the team lead.● Assist with implementation of ETL jobs into the production batch process as needed.● Performance tune ETL jobs as needed.● Provide knowledge transfer to State staff as needed.● Perform other related duties as assigned.
Minimum Qualifications:● Three (3) years’ experience in DataStage ETL developer role(s).● Two (2) years’ experience with Oracle PL/SQL development.
Desired Qualifications● Experience with DataStage Server and Parallel jobs, Routines, hash files, and/or sequences.● Experience testing and/or performance tuning ETL jobs.● Experience with batch scheduling tools, e.g., Automic, Control-M, etc.● Experience with Activity Diagrams and/or Process Modeling.● Experience with DataStage migration projects.● Experience developing Oracle Business Intelligence reports.● Experience with PeopleSoft Human Resources and/or Finance modules/data.
Compensation:$110,000 - $140,000 per year
How to Apply:Interested candidates should send their resume and cover letter to info@sltaeronautics.com.
Job Types: Full-time, Contract
Pay: $110,000.00 - $140,000.00 per year
Experience level:

 3 years

Work Location: Remote","<p>SLT Job Listing</p>
<p><b>Position: </b>ETL Data Engineer<br><b>Location: </b>Remote<br><b>Type: </b>Contract / 1099<br><b>Length of Contract: </b>1.5 years<br><b>Application Deadline: </b>October 25, 2023</p>
<p>About SLT:<br>SLT specializes in providing custom software solutions and high-precision drone services. Our expertise ranges from software services like cloud infrastructure and generative AI solutions to high-precision drone services such as mapping, 3D modeling, and infrastructure inspection.</p>
<p>Job Description:<br>SLT is seeking a full-time resource specializing in software engineering for a critical data migration project. The primary goal is to transition, test, and deploy Data Warehouse ETLs from<br>IBM DataStage V 8.5 on Windows 2008 Server to IBM DataStage V 11.7 on Red Hat Linux 8.</p>
<p>The DataStage ETL tool under consideration plays a pivotal role in facilitating data transfer from distinct source systems to the EPM Data Warehouse. This warehouse is instrumental for numerous agency users across various governmental departments, aiding in reporting activities.<br>The prevailing DataStage software operates on Windows 2008 physical machines, which have reached the end of their support life. The data engineer will undertake these migration duties, complementing the efforts of existing staff members also engaged in this endeavor. While in-house staff will oversee project management aspects, the engineer will collaborate extensively with the Data Warehouse Team lead alongside other project team members.</p>
<p>Responsibilities:<br>&#x25cf; Test migrated ETL jobs, including data validation and data integrity testing.</p>
<p>&#x25cf; Fix issues found during migration or testing.<br>&#x25cf; Create new ETL jobs and update existing ETL jobs.<br>&#x25cf; Migrate ETL jobs from DataStage V8.5 server to DataStage V11.7 server.<br>&#x25cf; Document processes and details as directed by the team lead.<br>&#x25cf; Assist with implementation of ETL jobs into the production batch process as needed.<br>&#x25cf; Performance tune ETL jobs as needed.<br>&#x25cf; Provide knowledge transfer to State staff as needed.<br>&#x25cf; Perform other related duties as assigned.</p>
<p>Minimum Qualifications:<br>&#x25cf; Three (3) years&#x2019; experience in DataStage ETL developer role(s).<br>&#x25cf; Two (2) years&#x2019; experience with Oracle PL/SQL development.</p>
<p>Desired Qualifications<br>&#x25cf; Experience with DataStage Server and Parallel jobs, Routines, hash files, and/or sequences.<br>&#x25cf; Experience testing and/or performance tuning ETL jobs.<br>&#x25cf; Experience with batch scheduling tools, e.g., Automic, Control-M, etc.<br>&#x25cf; Experience with Activity Diagrams and/or Process Modeling.<br>&#x25cf; Experience with DataStage migration projects.<br>&#x25cf; Experience developing Oracle Business Intelligence reports.<br>&#x25cf; Experience with PeopleSoft Human Resources and/or Finance modules/data.</p>
<p>Compensation:<br>&#x24;110,000 - &#x24;140,000 per year</p>
<p>How to Apply:<br>Interested candidates should send their resume and cover letter to info@sltaeronautics.com.</p>
<p>Job Types: Full-time, Contract</p>
<p>Pay: &#x24;110,000.00 - &#x24;140,000.00 per year</p>
<p>Experience level:</p>
<ul>
 <li>3 years</li>
</ul>
<p>Work Location: Remote</p>",,"a07bc29d201610eb",,"Full-time","Contract","Remote","Data Engineer","1 day ago","2023-10-24T11:46:57.390Z","3.4","29","$110,000 - $140,000 a year","2023-10-25T11:46:57.396Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=a07bc29d201610eb&from=jasx&tk=1hdjagu34k269800&vjs=3"
"Zenotis Technologies Inc","Main skills – Data Engineering, Matillion, SQL, Python or Java, Cloud, Snowflake
Job Description:
He will design, build, and implement data integration solutions, including data pipelines and ETL jobs to meet the data of applications. Working with data architects, application development, data analytics teams and the data governance COE, in a combination of cloud and on premise platforms.
Duties:
· Develops and maintains scalable data pipelines and builds out new integration to support continuing increases in data volume and complexity.
· Designs and develops scalable ETL packages for point-to-point integration of data between source systems, extraction, and integration of data into various data assets.
· Perform data analysis required to troubleshoot data related issues and assist in the resolution of data issues.
· Assist in planning, coordinating, and executing engineering!
· Design and develop data migration in support of enterprise application and system implementation from legacy systems.
Addition experience
· Experience in ETL tools SAP business objects data service preferences
· Experience in python or java development experience in SQL
· Experience in data virtualization, tibco data visualization preferences
· Experience in cloud platform. Snowflakes are preferred.
Job Type: Contract
Salary: From $60.00 per hour
Schedule:

 Monday to Friday
 No nights
 No weekends

Work Location: Remote","<p><b>Main skills &#x2013; Data Engineering, Matillion, SQL, Python or Java, Cloud, Snowflake</b></p>
<p><b>Job Description:</b></p>
<p>He will design, build, and implement data integration solutions, including data pipelines and ETL jobs to meet the data of applications. Working with data architects, application development, data analytics teams and the data governance COE, in a combination of cloud and on premise platforms.</p>
<p><b>Duties:</b></p>
<p>&#xb7; Develops and maintains scalable data pipelines and builds out new integration to support continuing increases in data volume and complexity.</p>
<p>&#xb7; Designs and develops scalable ETL packages for point-to-point integration of data between source systems, extraction, and integration of data into various data assets.</p>
<p>&#xb7; Perform data analysis required to troubleshoot data related issues and assist in the resolution of data issues.</p>
<p>&#xb7; Assist in planning, coordinating, and executing engineering!</p>
<p>&#xb7; Design and develop data migration in support of enterprise application and system implementation from legacy systems.</p>
<p><b>Addition experience</b></p>
<p>&#xb7; Experience in ETL tools SAP business objects data service preferences</p>
<p>&#xb7; Experience in python or java development experience in SQL</p>
<p>&#xb7; Experience in data virtualization, tibco data visualization preferences</p>
<p>&#xb7; Experience in cloud platform. Snowflakes are preferred.</p>
<p>Job Type: Contract</p>
<p>Salary: From &#x24;60.00 per hour</p>
<p>Schedule:</p>
<ul>
 <li>Monday to Friday</li>
 <li>No nights</li>
 <li>No weekends</li>
</ul>
<p>Work Location: Remote</p>",,"5b050846e01d4f3b",,"Contract",,"Remote","Sr. Data Engineer","1 day ago","2023-10-24T11:47:02.404Z",,,"From $60 an hour","2023-10-25T11:47:02.405Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=5b050846e01d4f3b&from=jasx&tk=1hdjagu34k269800&vjs=3"
"Bestow","ABOUT BESTOW
 
 
 
   Bestow is a leader in the modern life insurance space. As both a direct-to-consumer destination and a leading enterprise SaaS provider, Bestow is on a mission to increase financial stability for everyone. We’re a series-C start-up with a remote/hybrid workforce offering work-life balance and equity to all employees.
   
 
 
  
 
  ABOUT THE TEAM
 
 
 
   The Bestow Data Engineering team plays a significant role within the organization, working across the entire company to provide scalable data solutions within the platform and toward integrations with external partners. The data engineering team works closely with internal analytics team members to improve data architecture and serve data science predictions. In addition, data engineers work closely with stakeholder members from product and engineering to design and launch new systems for extracting, transforming, and storing data. You’ll be called upon to improve Bestow’s data reliability, efficiency, and quality. You will be expected to scale your solutions to the cloud environment of a SaaS company, iterate quickly, and make pragmatic choices around what tools and technologies to adopt.
   
 
 
  
 
  We are looking for a 
  Sr. Data Engineer to join the team! 
  Open to
   
  Dallas, TX or Remote (US) #LI-Remote
 
 
  ABOUT THE ROLE
 
   Build robust solutions for transferring data from first and third-party applications to and from our data warehouse.
   Passionate about data quality and availability, driving to resolution through high collaboration with team members.
   Decision-making is done as a team. The things you build will be maintained and improved upon by others; there is a shared responsibility to make defensible design considerations.
   Develop hardened and repeatable (CI/CD) data models and pipelines to enable reporting, modeling, and machine learning.
   Improve data availability to our enterprise clients through a mix of traditional push delivery, cloud, and event-driven data-sharing methods.
   Ensure data quality through automated monitoring and alerting.
   Leverage Google Cloud (GCP) tools (eg: Cloud Run, Cloud Function, Vertex AI, App Engine, Cloud Storage, IAM, etc.) and services (eg: Astronomer - Apache Airflow) to bring data workloads to production.
   Collaborate with product, engineering, stakeholders, and data teams to deliver informed solutions to platform and client needs.
 
  THIS ROLE REPORTS TO
 
   VP of Data & Analytics 
 
 YOUR EXPERIENCE
 
   6+ years working in a data engineering role that supports incoming/outgoing feeds as well as analytics and data science teams
   Deep SQL experience with columnar databases such as Google BigQuery, Snowflake, or Amazon Redshift
   4+ years of Python or similar experience writing efficient, testable, and readable code
   Comfortable designing an end-to-end data pipeline in cloud frameworks (such as GCP, AWS, Azure) with requirements from multiple stakeholders
   Experience building CICD pipelines for data processing using tools such as Docker, CircleCI, dbt, git, etc
   Able to manage infrastructure using IAC tools such as Terraform or Pulumi
   Experience with standard data orchestration tools such as Apache Airflow (or similar) to manage SLOs and processing dependencies
   Experience in building streaming / real-time ingestion pipelines
   Experience with creating alerts and monitoring pipelines, which contribute to overall data governance.
   Experience with containerization and container orchestration technologies with cloud architecture and implementation features (single- and multi-tenancy, orchestration, elastic scalability)
   Familiarity with standard IT security practices such as identity and access management (IAM), data protection, encryption, certificate, and key management.
   Adaptability to learn new technologies and products as the job demands.
   Nice to have: experience with data contracts, data lakes, and API development.
 
 
 
   Starting compensation may vary based on geographic location, work experience, and skills.
 
 
   TOTAL REWARDS
 
 
 
   Competitive salary and equity-based on role
 
 
   Flexible PTO plan
 
 
   100% paid premiums for medical, dental, and vision insurance
 
 
   Paid parental leave
 
 
   Annual lifestyle spending account to support your physical, emotional, and financial wellbeing
 
 
   Flexible work-from-home policy and open to remote
 
 
 
   We are proud to prioritize the employee experience at Bestow and to see that awarded by our team members and the industry:
 
 
 
   Best Place for Working Parents 2023
 
 
   Great Place to Work Certified, 2022 + 2023
 
 
   Built in Best Places to Work, 2022 + 2023
 
 
   Fortunes Best Workplaces in Texas 2022 + 2023
 
 
   Fortunes Best Workplaces in Financial Services and Insurance 2022 + 2023
 
 
   Forbes Best Startup Employers 2022
 
 
   Fortune Best Medium Workplaces 2022
 
 
   Best Workplaces for Millennials 2022
 
 
 
   We value diversity at Bestow. The company will hire, recruit, and promote without regard to race, color, religion, sex, sexual orientation, gender identity or expression, national origin, pregnancy or maternity, veteran status or any other status protected by applicable law. We understand the importance of creating a safe and comfortable work environment and encourage individualism and authenticity in every member of our team.","<div>
 <div>
  <b>ABOUT BESTOW</b>
 </div>
 <div></div>
 <div>
  <br> Bestow is a leader in the modern life insurance space. As both a direct-to-consumer destination and a leading enterprise SaaS provider, Bestow is on a mission to increase financial stability for everyone. We&#x2019;re a series-C start-up with a remote/hybrid workforce offering work-life balance and equity to all employees.
  <br> 
 </div>
 <div></div>
 <br> 
 <div>
  <b>ABOUT THE TEAM</b>
 </div>
 <div></div>
 <div>
  <br> The Bestow Data Engineering team plays a significant role within the organization, working across the entire company to provide scalable data solutions within the platform and toward integrations with external partners. The data engineering team works closely with internal analytics team members to improve data architecture and serve data science predictions. In addition, data engineers work closely with stakeholder members from product and engineering to design and launch new systems for extracting, transforming, and storing data. You&#x2019;ll be called upon to improve Bestow&#x2019;s data reliability, efficiency, and quality. You will be expected to scale your solutions to the cloud environment of a SaaS company, iterate quickly, and make pragmatic choices around what tools and technologies to adopt.
  <br> 
 </div>
 <div></div>
 <br> 
 <div>
  We are looking for a 
  <b>Sr. Data Engineer</b> to join the team! 
  <b>Open to</b>
  <b> </b>
  <b>Dallas, TX or Remote (US) #LI-Remote</b>
 </div>
 <div></div>
 <h3 class=""jobSectionHeader""><b><br> ABOUT THE ROLE</b></h3>
 <ul>
  <li> Build robust solutions for transferring data from first and third-party applications to and from our data warehouse.</li>
  <li> Passionate about data quality and availability, driving to resolution through high collaboration with team members.</li>
  <li> Decision-making is done as a team. The things you build will be maintained and improved upon by others; there is a shared responsibility to make defensible design considerations.</li>
  <li> Develop hardened and repeatable (CI/CD) data models and pipelines to enable reporting, modeling, and machine learning.</li>
  <li> Improve data availability to our enterprise clients through a mix of traditional push delivery, cloud, and event-driven data-sharing methods.</li>
  <li> Ensure data quality through automated monitoring and alerting.</li>
  <li> Leverage Google Cloud (GCP) tools (eg: Cloud Run, Cloud Function, Vertex AI, App Engine, Cloud Storage, IAM, etc.) and services (eg: Astronomer - Apache Airflow) to bring data workloads to production.</li>
  <li> Collaborate with product, engineering, stakeholders, and data teams to deliver informed solutions to platform and client needs.</li>
 </ul>
 <h3 class=""jobSectionHeader""><b> THIS ROLE REPORTS TO</b></h3>
 <ul>
  <li> VP of Data &amp; Analytics </li>
 </ul>
 <h3 class=""jobSectionHeader""><b>YOUR EXPERIENCE</b></h3>
 <ul>
  <li> 6+ years working in a data engineering role that supports incoming/outgoing feeds as well as analytics and data science teams</li>
  <li> Deep SQL experience with columnar databases such as Google BigQuery, Snowflake, or Amazon Redshift</li>
  <li> 4+ years of Python or similar experience writing efficient, testable, and readable code</li>
  <li> Comfortable designing an end-to-end data pipeline in cloud frameworks (such as GCP, AWS, Azure) with requirements from multiple stakeholders</li>
  <li> Experience building CICD pipelines for data processing using tools such as Docker, CircleCI, dbt, git, etc</li>
  <li> Able to manage infrastructure using IAC tools such as Terraform or Pulumi</li>
  <li> Experience with standard data orchestration tools such as Apache Airflow (or similar) to manage SLOs and processing dependencies</li>
  <li> Experience in building streaming / real-time ingestion pipelines</li>
  <li> Experience with creating alerts and monitoring pipelines, which contribute to overall data governance.</li>
  <li> Experience with containerization and container orchestration technologies with cloud architecture and implementation features (single- and multi-tenancy, orchestration, elastic scalability)</li>
  <li> Familiarity with standard IT security practices such as identity and access management (IAM), data protection, encryption, certificate, and key management.</li>
  <li> Adaptability to learn new technologies and products as the job demands.</li>
  <li> Nice to have: experience with data contracts, data lakes, and API development.</li>
 </ul>
 <div></div>
 <div>
  <br> Starting compensation may vary based on geographic location, work experience, and skills.
 </div>
 <div>
  <b> TOTAL REWARDS</b>
 </div>
 <div></div>
 <div>
  <br> Competitive salary and equity-based on role
 </div>
 <div>
   Flexible PTO plan
 </div>
 <div>
   100% paid premiums for medical, dental, and vision insurance
 </div>
 <div>
   Paid parental leave
 </div>
 <div>
   Annual lifestyle spending account to support your physical, emotional, and financial wellbeing
 </div>
 <div>
   Flexible work-from-home policy and open to remote
 </div>
 <div></div>
 <div>
  <br> We are proud to prioritize the employee experience at Bestow and to see that awarded by our team members and the industry:
 </div>
 <div></div>
 <div>
  <br> Best Place for Working Parents 2023
 </div>
 <div>
   Great Place to Work Certified, 2022 + 2023
 </div>
 <div>
   Built in Best Places to Work, 2022 + 2023
 </div>
 <div>
   Fortunes Best Workplaces in Texas 2022 + 2023
 </div>
 <div>
   Fortunes Best Workplaces in Financial Services and Insurance 2022 + 2023
 </div>
 <div>
   Forbes Best Startup Employers 2022
 </div>
 <div>
   Fortune Best Medium Workplaces 2022
 </div>
 <div>
   Best Workplaces for Millennials 2022
 </div>
 <div></div>
 <div>
  <br> We value diversity at Bestow. The company will hire, recruit, and promote without regard to race, color, religion, sex, sexual orientation, gender identity or expression, national origin, pregnancy or maternity, veteran status or any other status protected by applicable law. We understand the importance of creating a safe and comfortable work environment and encourage individualism and authenticity in every member of our team.
 </div>
</div>","https://jobs.lever.co/hellobestow/5ca192d7-206f-43e0-9eda-b424745a69ef?lever-source=Indeed&lever-source=Indeed","261c524a08808e40",,"Full-time",,"Remote","Sr. Data Engineer","1 day ago","2023-10-24T11:46:48.370Z",,,"$116,000 - $145,000 a year","2023-10-25T11:46:48.373Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=261c524a08808e40&from=jasx&tk=1hdjagu34k269800&vjs=3"
"Mayo Clinic","Why Mayo Clinic 
  
 
   Mayo Clinic is top-ranked in more specialties than any other care provider according to U.S. News & World Report. As we work together to put the needs of the patient first, we are also dedicated to our employees, investing in competitive compensation and comprehensive benefit plans – to take care of you and your family, now and in the future. And with continuing education and advancement opportunities at every turn, you can build a long, successful career with Mayo Clinic. You’ll thrive in an environment that supports innovation, is committed to ending racism and supporting diversity, equity and inclusion, and provides the resources you need to succeed.
 
  
  
 Responsibilities
  
  As a member of the Data and Analytics organization, you will be responsible for building and delivering best-in-class clinical data initiatives aimed at driving best-in-class solutions. You will collaborate with analytic partners and business partners from product strategy, program management, IT, data strategy, and predictive analytics teams to develop effective solutions for our partners. 
  Lead data design, prototype, and development of data pipeline architecture pipelines. Lead implementation of internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability. Lead cause analysis on external and internal processes and data to identify opportunities for improvement and answer questions. Excellent analytic skills associated with working on unstructured datasets. Understand the architecture, be a team player, lead technical discussions and communicate the technical discussion. Be a senior Individual contributor of the Data or Software Engineering teams. Be part of Technical Review Board along with Manager and Principal Engineer. Be a technical liaison between Manager, Software Engineers and Principal Engineers. Collaborate with software engineers to analyze, develop and test functional requirements. Write clean, maintainable code 30% of the time and performing peer code-reviews. Mentor and Coach Engineers. Work with team members to investigate design approaches, prototype new technology and evaluate technical feasibility. Work in an Agile/Safe/Scrum environment to deliver high quality software. Establish architectural principles, select design patterns, and then mentor team members on their appropriate application. Facilitate and drive communication between front-end, back-end, data and platform engineers. Play a formal Engineering lead role in the area of expertise. Keep up-to-date with industry trends and developments. 
  Job Responsibilities: 
  
  Act as Product Owner for Data platform’s and Lead the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. 
  Evaluate the full technology stack of services required including PaaS, IaaS, SaaS, DataOps, operations, availability, and automation. 
  Research, design, and develop Public & Private Data Solutions, including impacts to enterprise architecture 
  Build high-performing clinical data processing frameworks leveraging Google Cloud Platform, GCP Shared Services like Google Healthcare API, Big Query, and HL7 FHIR store. 
  Participate in evaluation of supporting technologies and industry best practices with our cloud partners and peer teams. 
  Lead Modern Data Warehouse Solutions and Sizing efforts to create defined plans and work estimates for customer proposals and Statements of work. 
  Conduct full technical discovery, identifying pain points, business, and technical requirements, “as is” and “to be” scenarios! 
  Design and Develop clinical data pipelines integrating ingestion, harmonization, and consumption frameworks for onboarding clinical data from various data sources formatted in various industry standards (FHIR, C-CDA, HL7 V2, JSON, XML, etc.). 
  Build state-of-the-art data pipelines supporting both batch and real-time streams to enable Clinical data collection, storage, processing, transformation, aggregation, and dissemination through heterogeneous channels. 
  Build design specifications for health care data objects and surrounding data processing logic. 
  Lead innovation and research building proof of concepts for complex transformations, notification engines, analytical engines, and self-service analytics 
  Bring a DevOps mindset to enable big data and batch/real-time analytical solutions that leverage emerging technologies.
 
  
  
 Qualifications
  
  Bachelor’s Degree in Computer Science/Engineering or related field with 6 years of experience OR an Associate’s degree in Computer Science/Engineering or related field with 8 years of experience.
  Knowledge of professional software engineering practices and best practices for the full software development life cycle (SDLC), including coding standards, code reviews, source control management, build processes, testing, and operations. Have in-depth knowledge of data engineering and building data pipelines with a minimum of 5 years of experience in data engineering, data science or analytical modeling and basic knowledge of related disciplines. Worked and lead Data Engineering teams in Continuous Integration / Continuous Delivery model. Build/Lead Data products highly resilient in nature. Build/Lead Test Automation suites, Unit Testing coverage, Data Quality, Monitoring & Observability. A minimum experience of 5 years using relational databases and NoSQL Databases. Experience with cloud platforms such as GCP, Azure, AWS.
  Continuous Integration using Jenkins, Git Hub Actions or Azure Pipelines. Experience with cloud technologies, development and deployment. Experience with tools like Jira, GitHub, SharePoint, Azure Boards. Experience using advanced data processing solutions/capabilities such as Apache Spark, Hive, Airflow and Kafka, GCP Dataflow. Experience using big data, statistics and knowledge of data related aspects of machine learning. Experience with Google BigQuery, FHIR APIs, and Vertex AI. Knowledge of how workflow scheduling solutions such as Apache Airflow and Google Composer related to data systems. Knowledge of using Infrastructure as code (Kubernetes, Docker) in a cloud environment.
 
   Hands-on experience in architecture, design, and development of enterprise data applications and analytics solutions within the health care domain
   Experience in Google Cloud Platform/Shared Services such as Cloud Dataflow, Cloud Storage, Pub/sub, Cloud Composer, Big Query, and Health care API (FHIR store)
   They should be able to deliver an ingestion framework for relational data sources, understand layers and rules of a data lake and carry out all the tasks to operationalize data pipelines.
   Experience in Python, Java, Spark, Airflow, and Kafka development
   Hands-on experience working with “Big Data” technologies and experience with traditional RDBMS, Python, Unix Shell scripting, JSON, and XML
   Experience working with tools to automate CI/CD pipelines (e.g., Jenkins, GIT)
   Must have great articulation and communication skills.
   Working in a fluid environment, defining, and owning priorities that adapt to our larger goals. You can bring clarity to ambiguity while remaining open-minded to new information that might change your mind.
   Should have a strong understanding of healthcare data, including clinical data in proprietary and industry-standard formats.
   Participate in architectural discussions, perform system analysis which involves a review of the existing systems and operating methodologies. Participate in the analysis of newest technologies and suggest the optimal solutions which will be best suited for satisfying the current requirements and will simplify the future modifications
   Design appropriate data models for the use in transactional and big data environments as an input into Machine Learning processing.
   Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability
   Design and Build the necessary infrastructure for optimal ETL from a variety of data sources to be used on GCP services
   Collaborate with multiple stakeholders including Product Teams, Data Domain Owners, Infrastructure, Security and Global IT
   Identify, Implement, and continuously enhance the data automation process
   Develop proper Data Governance and Data Security
   Demonstrate strategic thinking and strong planning skills to establish long term roadmap and business plan
   Work with stakeholders to establish and meet data quality requirements, SLAs and SLOs for data ingestion
   Experience in Self-service Analytics/Visualization tools like PowerBI, Looker, Tableau
   Proven knowledge in implementing security & IAM requirements
   Experience building and maintaining a Data-Lake with DeltaLake
   Experience with ETL/ELT/DataMesh frameworks
   Experience with GCP Dataplex (Data Catalog, Clean Rooms)
 
  Authorization to work and remain in the United States, without necessity for Mayo Clinic sponsorship now, or in the future (for example, be a U.S. Citizen, national, or permanent resident, refugee, or asylee). Also, Mayo Clinic does not participate in the F-1 STEM OPT extension program.
  Exemption Status
  
  Exempt
  
  
 Compensation Detail
  
  $138,236.80 - $200,408.00 / year
  
  
 Benefits Eligible
  
  Yes
  
  
 Schedule
  
  Full Time
  
  
 Hours/Pay Period
  
  80
  
  
 Schedule Details
  
  Monday - Friday, 8:00 am - 5:00 pm
  
  
 Weekend Schedule
  
  As needed
  
  
 International Assignment
  
  No
  
  
 Site Description
  
 
  Just as our reputation has spread beyond our Minnesota roots, so have our locations. Today, our employees are located at our three major campuses in Phoenix/Scottsdale, Arizona, Jacksonville, Florida, Rochester, Minnesota, and at Mayo Clinic Health System campuses throughout Midwestern communities, and at our international locations. Each Mayo Clinic location is a special place where our employees thrive in both their work and personal lives. Learn more about what each unique Mayo Clinic campus has to offer, and where your best fit is.
   
 
 
   
  
   Affirmative Action and Equal Opportunity Employer 
  
  
    As an Affirmative Action and Equal Opportunity Employer Mayo Clinic is committed to creating an inclusive environment that values the diversity of its employees and does not discriminate against any employee or candidate. Women, minorities, veterans, people from the LGBTQ communities and people with disabilities are strongly encouraged to apply to join our teams. Reasonable accommodations to access job openings or to apply for a job are available.
    
  
 
  
  
 Recruiter
  
  Miranda Grabner","<div>
 <div>
  <b>Why Mayo Clinic</b> 
 </div> 
 <div>
  <br> Mayo Clinic is top-ranked in more specialties than any other care provider according to U.S. News &amp; World Report. As we work together to put the needs of the patient first, we are also dedicated to our employees, investing in competitive compensation and comprehensive benefit plans &#x2013; to take care of you and your family, now and in the future. And with continuing education and advancement opportunities at every turn, you can build a long, successful career with Mayo Clinic. You&#x2019;ll thrive in an environment that supports innovation, is committed to ending racism and supporting diversity, equity and inclusion, and provides the resources you need to succeed.
 </div>
 <br> 
 <br> 
 <b>Responsibilities</b>
 <br> 
 <p> As a member of the Data and Analytics organization, you will be responsible for building and delivering best-in-class clinical data initiatives aimed at driving best-in-class solutions. You will collaborate with analytic partners and business partners from product strategy, program management, IT, data strategy, and predictive analytics teams to develop effective solutions for our partners.</p> 
 <p> Lead data design, prototype, and development of data pipeline architecture pipelines. Lead implementation of internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability. Lead cause analysis on external and internal processes and data to identify opportunities for improvement and answer questions. Excellent analytic skills associated with working on unstructured datasets. Understand the architecture, be a team player, lead technical discussions and communicate the technical discussion. Be a senior Individual contributor of the Data or Software Engineering teams. Be part of Technical Review Board along with Manager and Principal Engineer. Be a technical liaison between Manager, Software Engineers and Principal Engineers. Collaborate with software engineers to analyze, develop and test functional requirements. Write clean, maintainable code 30% of the time and performing peer code-reviews. Mentor and Coach Engineers. Work with team members to investigate design approaches, prototype new technology and evaluate technical feasibility. Work in an Agile/Safe/Scrum environment to deliver high quality software. Establish architectural principles, select design patterns, and then mentor team members on their appropriate application. Facilitate and drive communication between front-end, back-end, data and platform engineers. Play a formal Engineering lead role in the area of expertise. Keep up-to-date with industry trends and developments.</p> 
 <p> Job Responsibilities:</p> 
 <ul> 
  <li>Act as Product Owner for Data platform&#x2019;s and Lead the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.</li> 
  <li>Evaluate the full technology stack of services required including PaaS, IaaS, SaaS, DataOps, operations, availability, and automation.</li> 
  <li>Research, design, and develop Public &amp; Private Data Solutions, including impacts to enterprise architecture</li> 
  <li>Build high-performing clinical data processing frameworks leveraging Google Cloud Platform, GCP Shared Services like Google Healthcare API, Big Query, and HL7 FHIR store.</li> 
  <li>Participate in evaluation of supporting technologies and industry best practices with our cloud partners and peer teams.</li> 
  <li>Lead Modern Data Warehouse Solutions and Sizing efforts to create defined plans and work estimates for customer proposals and Statements of work.</li> 
  <li>Conduct full technical discovery, identifying pain points, business, and technical requirements, &#x201c;as is&#x201d; and &#x201c;to be&#x201d; scenarios!</li> 
  <li>Design and Develop clinical data pipelines integrating ingestion, harmonization, and consumption frameworks for onboarding clinical data from various data sources formatted in various industry standards (FHIR, C-CDA, HL7 V2, JSON, XML, etc.).</li> 
  <li>Build state-of-the-art data pipelines supporting both batch and real-time streams to enable Clinical data collection, storage, processing, transformation, aggregation, and dissemination through heterogeneous channels.</li> 
  <li>Build design specifications for health care data objects and surrounding data processing logic.</li> 
  <li>Lead innovation and research building proof of concepts for complex transformations, notification engines, analytical engines, and self-service analytics</li> 
  <li>Bring a DevOps mindset to enable big data and batch/real-time analytical solutions that leverage emerging technologies.</li>
 </ul>
 <br> 
 <br> 
 <b>Qualifications</b>
 <br> 
 <p> Bachelor&#x2019;s Degree in Computer Science/Engineering or related field with 6 years of experience OR an Associate&#x2019;s degree in Computer Science/Engineering or related field with 8 years of experience.</p>
 <p> Knowledge of professional software engineering practices and best practices for the full software development life cycle (SDLC), including coding standards, code reviews, source control management, build processes, testing, and operations. Have in-depth knowledge of data engineering and building data pipelines with a minimum of 5 years of experience in data engineering, data science or analytical modeling and basic knowledge of related disciplines. Worked and lead Data Engineering teams in Continuous Integration / Continuous Delivery model. Build/Lead Data products highly resilient in nature. Build/Lead Test Automation suites, Unit Testing coverage, Data Quality, Monitoring &amp; Observability. A minimum experience of 5 years using relational databases and NoSQL Databases. Experience with cloud platforms such as GCP, Azure, AWS.</p>
 <p> Continuous Integration using Jenkins, Git Hub Actions or Azure Pipelines. Experience with cloud technologies, development and deployment. Experience with tools like Jira, GitHub, SharePoint, Azure Boards. Experience using advanced data processing solutions/capabilities such as Apache Spark, Hive, Airflow and Kafka, GCP Dataflow. Experience using big data, statistics and knowledge of data related aspects of machine learning. Experience with Google BigQuery, FHIR APIs, and Vertex AI. Knowledge of how workflow scheduling solutions such as Apache Airflow and Google Composer related to data systems. Knowledge of using Infrastructure as code (Kubernetes, Docker) in a cloud environment.</p>
 <ul>
  <li> Hands-on experience in architecture, design, and development of enterprise data applications and analytics solutions within the health care domain</li>
  <li> Experience in Google Cloud Platform/Shared Services such as Cloud Dataflow, Cloud Storage, Pub/sub, Cloud Composer, Big Query, and Health care API (FHIR store)</li>
  <li> They should be able to deliver an ingestion framework for relational data sources, understand layers and rules of a data lake and carry out all the tasks to operationalize data pipelines.</li>
  <li> Experience in Python, Java, Spark, Airflow, and Kafka development</li>
  <li> Hands-on experience working with &#x201c;Big Data&#x201d; technologies and experience with traditional RDBMS, Python, Unix Shell scripting, JSON, and XML</li>
  <li> Experience working with tools to automate CI/CD pipelines (e.g., Jenkins, GIT)</li>
  <li> Must have great articulation and communication skills.</li>
  <li> Working in a fluid environment, defining, and owning priorities that adapt to our larger goals. You can bring clarity to ambiguity while remaining open-minded to new information that might change your mind.</li>
  <li> Should have a strong understanding of healthcare data, including clinical data in proprietary and industry-standard formats.</li>
  <li> Participate in architectural discussions, perform system analysis which involves a review of the existing systems and operating methodologies. Participate in the analysis of newest technologies and suggest the optimal solutions which will be best suited for satisfying the current requirements and will simplify the future modifications</li>
  <li> Design appropriate data models for the use in transactional and big data environments as an input into Machine Learning processing.</li>
  <li> Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability</li>
  <li> Design and Build the necessary infrastructure for optimal ETL from a variety of data sources to be used on GCP services</li>
  <li> Collaborate with multiple stakeholders including Product Teams, Data Domain Owners, Infrastructure, Security and Global IT</li>
  <li> Identify, Implement, and continuously enhance the data automation process</li>
  <li> Develop proper Data Governance and Data Security</li>
  <li> Demonstrate strategic thinking and strong planning skills to establish long term roadmap and business plan</li>
  <li> Work with stakeholders to establish and meet data quality requirements, SLAs and SLOs for data ingestion</li>
  <li> Experience in Self-service Analytics/Visualization tools like PowerBI, Looker, Tableau</li>
  <li> Proven knowledge in implementing security &amp; IAM requirements</li>
  <li> Experience building and maintaining a Data-Lake with DeltaLake</li>
  <li> Experience with ETL/ELT/DataMesh frameworks</li>
  <li> Experience with GCP Dataplex (Data Catalog, Clean Rooms)</li>
 </ul>
 <p> Authorization to work and remain in the United States, without necessity for Mayo Clinic sponsorship now, or in the future (for example, be a U.S. Citizen, national, or permanent resident, refugee, or asylee). Also, Mayo Clinic does not participate in the F-1 STEM OPT extension program.</p>
 <b><br> Exemption Status</b>
 <br> 
 <br> Exempt
 <br> 
 <br> 
 <b>Compensation Detail</b>
 <br> 
 <br> &#x24;138,236.80 - &#x24;200,408.00 / year
 <br> 
 <br> 
 <b>Benefits Eligible</b>
 <br> 
 <br> Yes
 <br> 
 <br> 
 <b>Schedule</b>
 <br> 
 <br> Full Time
 <br> 
 <br> 
 <b>Hours/Pay Period</b>
 <br> 
 <br> 80
 <br> 
 <br> 
 <b>Schedule Details</b>
 <br> 
 <br> Monday - Friday, 8:00 am - 5:00 pm
 <br> 
 <br> 
 <b>Weekend Schedule</b>
 <br> 
 <br> As needed
 <br> 
 <br> 
 <b>International Assignment</b>
 <br> 
 <br> No
 <br> 
 <br> 
 <b>Site Description</b>
 <br> 
 <div>
  Just as our reputation has spread beyond our Minnesota roots, so have our locations. Today, our employees are located at our three major campuses in Phoenix/Scottsdale, Arizona, Jacksonville, Florida, Rochester, Minnesota, and at Mayo Clinic Health System campuses throughout Midwestern communities, and at our international locations. Each Mayo Clinic location is a special place where our employees thrive in both their work and personal lives. Learn more about what each unique Mayo Clinic campus has to offer, and where your best fit is.
  <br> 
 </div>
 <div>
  <br> 
  <div>
   <b>Affirmative Action and Equal Opportunity Employer</b> 
  </div>
  <div>
   <br> As an Affirmative Action and Equal Opportunity Employer Mayo Clinic is committed to creating an inclusive environment that values the diversity of its employees and does not discriminate against any employee or candidate. Women, minorities, veterans, people from the LGBTQ communities and people with disabilities are strongly encouraged to apply to join our teams. Reasonable accommodations to access job openings or to apply for a job are available.
   <br> 
  </div>
 </div>
 <br> 
 <br> 
 <b>Recruiter</b>
 <br> 
 <br> Miranda Grabner
</div>","https://jobs.mayoclinic.org/job/-/-/33647/56073605920","90ca40677b46e6ea",,"Full-time",,"3636 Technology Dr NW, Rochester, MN 55901","IT Lead Data Engineer - Remote","2 days ago","2023-10-23T11:47:02.666Z","3.9","3035","$138,237 - $200,408 a year","2023-10-25T11:47:02.667Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=90ca40677b46e6ea&from=jasx&tk=1hdjagqsjjgbm800&vjs=3"
"Booz Allen Hamilton","Job Description 
  
 
 
  
   
    
     
      
       
        
         Location: 
        
        
         Bethesda,MD,US 
        
       
       
        
         Remote Work: 
        
        
         Yes 
        
       
       
        
         Job Number: 
        
        
         R0182802
        
       
      
     
    
    
    
      
     
       
      
     
    
    
    
     
      
       
        
         Health Data Engineer
          The Opportunity:
          Ever-expanding technology like IoT, machine learning, and artificial intelligence means that there’s more structured and unstructured data available today than ever before. As a data engineer, you know that organizing big data can yield pivotal insights when it’s gathered from disparate sources. We need an experienced data engineer like you to help our clients find answers in their big data to impact important missions—from fraud detection, cancer research to national intelligence.
         
          As a big data engineer at Booz Allen, you’ll implement data engineering activities on some of the most mission-driven projects in the industry. You’ll deploy and develop pipelines and platforms that organize and make disparate data meaningful. Here, you’ll work with and guide a multi-disciplinary team of analysts, data engineers, developers, and data consumers in a fast-paced, agile environment. You’ll use your experience in analytical exploration and data examination while you manage the assessment, design, building, and maintenance of scalable platforms for your clients. Work with us to use big data for good.
         
          Join us. The world can’t wait.
         
          You Have:
         
           2+ years of experience in application development
           2+ years of experience designing, developing, operationalizing and maintaining complex data applications at enterprise scale
           2+ years of experience with developing and maintaining scalable data stores that supply big data in forms needed for business analysis
           2+ years of experience with a public Cloud, including AWS, Microsoft Azure, or Google Cloud
           Experience creating software for retrieving, parsing and processing structured and unstructured data
           Experience building scalable ETL/ELT workflows for reporting and analytics
           Experience creating solutions within a collaborative, cross-functional team environment
           Experience with Agile engineering practices
           Public Trust
           Bachelor’s degree
         
         
          Nice If You Have:
         
           3+ years of experience with utilizing programming languages, including C++, Java, or Python
           Experience in application development utilizing SQL or Scala
           Experience with distributed data and computing tools such as Spark, Databricks, Hadoop, Hive, AWS EMR, or Kafka
           Experience with working on real-time data and streaming applications
           Experience with data warehousing, including AWS Redshift, MySQL, or Snowflake
           Experience with UNIX and Linux, including basic commands and Shell scripting
           Ability to develop scripts and programs for converting various types of data into usable formats and support project team to scale, monitor and operate data platforms
         
         
          Vetting:
          Applicants selected will be subject to a government investigation and may need to meet eligibility requirements of the U.S. government client; Public Trust determination is required.
         
          Create Your Career:
          Grow With Us
          Your growth matters to us—that’s why we offer a variety of ways for you to develop your career. With professional and leadership development opportunities like upskilling programs, tuition reimbursement, mentoring, and firm-sponsored networking, you can chart a unique and fulfilling career path on your own terms.
         
          A Place Where You Belong
          Diverse perspectives cultivate collective ingenuity. Booz Allen’s culture of respect, equity, and opportunity means that, here, you are free to bring your whole self to work. With an array of business resource groups and other opportunities for connection, you’ll build your community in no time.
         
          Support Your Well-Being
          Our comprehensive benefits package includes wellness programs with HSA contributions, paid holidays, paid parental leave, a generous 401(k) match, and more. With these benefits, plus the option for flexible schedules and remote and hybrid locations, we’ll support you as you pursue a balanced, fulfilling life—at work and at home.
         
          Your Candidate Journey
          At Booz Allen, we know our people are what propel us forward, and we value relationships most of all. Here, we’ve compiled a list of resources so you’ll know what to expect as we forge a connection with you during your journey as a candidate with us.
         
          Compensation
          At Booz Allen, we celebrate your contributions, provide you with opportunities and choices, and support your total well-being. Our offerings include health, life, disability, financial, and retirement benefits, as well as paid leave, professional development, tuition assistance, work-life programs, and dependent care. Our recognition awards program acknowledges employees for exceptional performance and superior demonstration of our values. Full-time and part-time employees working at least 20 hours a week on a regular basis are eligible to participate in Booz Allen’s benefit programs. Individuals that do not meet the threshold are only eligible for select offerings, not inclusive of health benefits. We encourage you to learn more about our total benefits by visiting the Resource page on our Careers site and reviewing Our Employee Benefits page.
          Salary at Booz Allen is determined by various factors, including but not limited to location, the individual’s particular combination of education, knowledge, skills, competencies, and experience, as well as contract-specific affordability and organizational requirements. The projected compensation range for this position is $73,100.00 to $166,000.00 (annualized USD). The estimate displayed represents the typical salary range for this position and is just one component of Booz Allen’s total compensation package for employees.
         
          Work Model Our people-first culture prioritizes the benefits of flexibility and collaboration, whether that happens in person or remotely.
         
           If this position is listed as remote or hybrid, you’ll periodically work from a Booz Allen or client site facility.
           If this position is listed as onsite, you’ll work with colleagues and clients in person, as needed for the specific role.
         
         
          EEO Commitment
          We’re an equal employment opportunity/affirmative action employer that empowers our people to fearlessly drive change – no matter their race, color, ethnicity, religion, sex (including pregnancy, childbirth, lactation, or related medical conditions), national origin, ancestry, age, marital status, sexual orientation, gender identity and expression, disability, veteran status, military or uniformed service member status, genetic information, or any other status protected by applicable federal, state, local, or international law.","<div>
 <div>
  <div>
   <h2 class=""jobSectionHeader""><b>Job Description</b></h2> 
  </div>
 </div>
 <div>
  <div>
   <div>
    <div>
     <div>
      <div>
       <div>
        <div>
         Location: 
        </div>
        <div>
         Bethesda,MD,US 
        </div>
       </div>
       <div>
        <div>
         Remote Work: 
        </div>
        <div>
         Yes 
        </div>
       </div>
       <div>
        <div>
         Job Number: 
        </div>
        <div>
         R0182802
        </div>
       </div>
      </div>
     </div>
    </div>
    <p></p>
    <div>
     <br> 
     <div>
      <div> 
      </div>
     </div>
    </div>
    <div></div>
    <div>
     <div>
      <div>
       <div>
        <div>
         Health Data Engineer
         <p><b> The Opportunity:</b></p>
         <p> Ever-expanding technology like IoT, machine learning, and artificial intelligence means that there&#x2019;s more structured and unstructured data available today than ever before. As a data engineer, you know that organizing big data can yield pivotal insights when it&#x2019;s gathered from disparate sources. We need an experienced data engineer like you to help our clients find answers in their big data to impact important missions&#x2014;from fraud detection, cancer research to national intelligence.</p>
         <p></p>
         <p> As a big data engineer at Booz Allen, you&#x2019;ll implement data engineering activities on some of the most mission-driven projects in the industry. You&#x2019;ll deploy and develop pipelines and platforms that organize and make disparate data meaningful. Here, you&#x2019;ll work with and guide a multi-disciplinary team of analysts, data engineers, developers, and data consumers in a fast-paced, agile environment. You&#x2019;ll use your experience in analytical exploration and data examination while you manage the assessment, design, building, and maintenance of scalable platforms for your clients. Work with us to use big data for good.</p>
         <p></p>
         <p> Join us. The world can&#x2019;t wait.</p>
         <p></p>
         <p><b> You Have:</b></p>
         <ul>
          <li><p> 2+ years of experience in application development</p></li>
          <li><p> 2+ years of experience designing, developing, operationalizing and maintaining complex data applications at enterprise scale</p></li>
          <li><p> 2+ years of experience with developing and maintaining scalable data stores that supply big data in forms needed for business analysis</p></li>
          <li><p> 2+ years of experience with a public Cloud, including AWS, Microsoft Azure, or Google Cloud</p></li>
          <li><p> Experience creating software for retrieving, parsing and processing structured and unstructured data</p></li>
          <li><p> Experience building scalable ETL/ELT workflows for reporting and analytics</p></li>
          <li><p> Experience creating solutions within a collaborative, cross-functional team environment</p></li>
          <li><p> Experience with Agile engineering practices</p></li>
          <li><p> Public Trust</p></li>
          <li><p> Bachelor&#x2019;s degree</p></li>
         </ul>
         <p></p>
         <p><b> Nice If You Have:</b></p>
         <ul>
          <li><p> 3+ years of experience with utilizing programming languages, including C++, Java, or Python</p></li>
          <li><p> Experience in application development utilizing SQL or Scala</p></li>
          <li><p> Experience with distributed data and computing tools such as Spark, Databricks, Hadoop, Hive, AWS EMR, or Kafka</p></li>
          <li><p> Experience with working on real-time data and streaming applications</p></li>
          <li><p> Experience with data warehousing, including AWS Redshift, MySQL, or Snowflake</p></li>
          <li><p> Experience with UNIX and Linux, including basic commands and Shell scripting</p></li>
          <li><p> Ability to develop scripts and programs for converting various types of data into usable formats and support project team to scale, monitor and operate data platforms</p></li>
         </ul>
         <p></p>
         <p><b> Vetting:</b></p>
         <p> Applicants selected will be subject to a government investigation and may need to meet eligibility requirements of the U.S. government client; Public Trust determination is required.</p>
         <p></p>
         <p><b> Create Your Career:</b></p>
         <p><b> Grow With Us</b></p>
         <p> Your growth matters to us&#x2014;that&#x2019;s why we offer a variety of ways for you to develop your career. With professional and leadership development opportunities like upskilling programs, tuition reimbursement, mentoring, and firm-sponsored networking, you can chart a unique and fulfilling career path on your own terms.</p>
         <p></p>
         <p><b> A Place Where You Belong</b></p>
         <p> Diverse perspectives cultivate collective ingenuity. Booz Allen&#x2019;s culture of respect, equity, and opportunity means that, here, you are free to bring your whole self to work. With an array of business resource groups and other opportunities for connection, you&#x2019;ll build your community in no time.</p>
         <p></p>
         <p><b> Support Your Well-Being</b></p>
         <p> Our comprehensive benefits package includes wellness programs with HSA contributions, paid holidays, paid parental leave, a generous 401(k) match, and more. With these benefits, plus the option for flexible schedules and remote and hybrid locations, we&#x2019;ll support you as you pursue a balanced, fulfilling life&#x2014;at work and at home.</p>
         <p></p>
         <p><b> Your Candidate Journey</b></p>
         <p> At Booz Allen, we know our people are what propel us forward, and we value relationships most of all. Here, we&#x2019;ve compiled a list of resources so you&#x2019;ll know what to expect as we forge a connection with you during your journey as a candidate with us.</p>
         <p></p>
         <p><b> Compensation</b></p>
         <p> At Booz Allen, we celebrate your contributions, provide you with opportunities and choices, and support your total well-being. Our offerings include health, life, disability, financial, and retirement benefits, as well as paid leave, professional development, tuition assistance, work-life programs, and dependent care. Our recognition awards program acknowledges employees for exceptional performance and superior demonstration of our values. Full-time and part-time employees working at least 20 hours a week on a regular basis are eligible to participate in Booz Allen&#x2019;s benefit programs. Individuals that do not meet the threshold are only eligible for select offerings, not inclusive of health benefits. We encourage you to learn more about our total benefits by visiting the Resource page on our Careers site and reviewing Our Employee Benefits page.</p>
         <p></p> Salary at Booz Allen is determined by various factors, including but not limited to location, the individual&#x2019;s particular combination of education, knowledge, skills, competencies, and experience, as well as contract-specific affordability and organizational requirements. The projected compensation range for this position is &#x24;73,100.00 to &#x24;166,000.00 (annualized USD). The estimate displayed represents the typical salary range for this position and is just one component of Booz Allen&#x2019;s total compensation package for employees.
         <p></p>
         <p><b> Work Model</b><br> Our people-first culture prioritizes the benefits of flexibility and collaboration, whether that happens in person or remotely.</p>
         <ul>
          <li> If this position is listed as remote or hybrid, you&#x2019;ll periodically work from a Booz Allen or client site facility.</li>
          <li> If this position is listed as onsite, you&#x2019;ll work with colleagues and clients in person, as needed for the specific role.</li>
         </ul>
         <p></p>
         <p><b> EEO Commitment</b></p>
         <p> We&#x2019;re an equal employment opportunity/affirmative action employer that empowers our people to fearlessly drive change &#x2013; no matter their race, color, ethnicity, religion, sex (including pregnancy, childbirth, lactation, or related medical conditions), national origin, ancestry, age, marital status, sexual orientation, gender identity and expression, disability, veteran status, military or uniformed service member status, genetic information, or any other status protected by applicable federal, state, local, or international law.</p>
        </div>
       </div>
      </div>
     </div>
    </div>
   </div>
  </div>
 </div>
</div>
<p></p>","https://careers.boozallen.com/jobs/JobDetail/Bethesda-Health-Data-Engineer-R0182802/87076?source=JB-14400","755becb02c3bdf0a",,,,"Bethesda, MD","Health Data Engineer","1 day ago","2023-10-24T11:47:00.749Z","3.9","2515","$73,100 - $166,000 a year","2023-10-25T11:47:00.752Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=755becb02c3bdf0a&from=jasx&tk=1hdjagu34k269800&vjs=3"
"VXForward","VXForward has a great opportunity for a Azure Data Engineer -100% Remote with one of our clients.
  
  
 Roles and Responsibilities:
  
  
  Design and develop Azure solutions 
  Implement automated unit and integration testing 
  Collaborate with architecture and lead engineers to ensure consistent development practices 
  Participate in retrospective reviews 
  Participate in the estimation process for new work and releases 
  Collaborate with other engineers to solve and bring new perspectives to complex problems 
  Drive improvements in data engineering practices, procedures, and ways of working 
  Embrace new technologies and an ever-changing environment 
 
 Requirements
  
  
  5+ years proven ability of professional Data Development experience 
  3+ years of proven ability to develop with Azure and SQL (Oracle, SQL Server) 
  3+ years of experience with PySpark/Spark 
  2+ years of experience in Azure Data Factory and/or Azure Databricks 
  Experience in working with large-scale data sets and distributed systems 
  Full understanding of ETL concepts and Data Warehousing concepts 
  Exposure to version control software (Git, GitHub SaaS) 
  Strong understanding of Agile Principles (Scrum) 
  Bachelor’s Degree (Computer Science, Management Information Systems, Mathematics, Business Analytics, or STEM)
 
  
 This is a remote position.","<div>
 VXForward has a great opportunity for a Azure Data Engineer -100% Remote with one of our clients.
 <br> 
 <br> 
 <b>Roles and Responsibilities:</b>
 <br> 
 <ul> 
  <li>Design and develop Azure solutions</li> 
  <li>Implement automated unit and integration testing</li> 
  <li>Collaborate with architecture and lead engineers to ensure consistent development practices</li> 
  <li>Participate in retrospective reviews</li> 
  <li>Participate in the estimation process for new work and releases</li> 
  <li>Collaborate with other engineers to solve and bring new perspectives to complex problems</li> 
  <li>Drive improvements in data engineering practices, procedures, and ways of working</li> 
  <li>Embrace new technologies and an ever-changing environment</li> 
 </ul>
 <b>Requirements</b>
 <br> 
 <ul> 
  <li>5+ years proven ability of professional Data Development experience</li> 
  <li>3+ years of proven ability to develop with Azure and SQL (Oracle, SQL Server)</li> 
  <li>3+ years of experience with PySpark/Spark</li> 
  <li>2+ years of experience in Azure Data Factory and/or Azure Databricks</li> 
  <li>Experience in working with large-scale data sets and distributed systems</li> 
  <li>Full understanding of ETL concepts and Data Warehousing concepts</li> 
  <li>Exposure to version control software (Git, GitHub SaaS)</li> 
  <li>Strong understanding of Agile Principles (Scrum)</li> 
  <li>Bachelor&#x2019;s Degree (Computer Science, Management Information Systems, Mathematics, Business Analytics, or STEM)</li>
 </ul>
 <br> 
 <p>This is a remote position.</p>
</div>","https://vxforward.careerplug.com/jobs/2201600/apps/new","dc33a006f7173ce2",,"Full-time",,"Remote","Azure Data Engineer","1 day ago","2023-10-24T11:46:52.156Z",,,,"2023-10-25T11:46:52.158Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=dc33a006f7173ce2&from=jasx&tk=1hdjagu34k269800&vjs=3"
"Freemind solutions","Job Title: Senior Data Migration Specialist / Data Engineer (GoldenGate)
Location: Remote
Job Type: Full-time
Experience Level: Senior Level (Minimum 12 years)
Job Summary:
As a Senior Data Migration Specialist / Data Engineer, you will play a pivotal role in designing, implementing, and managing data migration solutions using Oracle GoldenGate. You will work closely with cross-functional teams to ensure seamless data migration, replication, and integration across various systems, databases, and platforms.
General Requirements:

 Minimum 12 years of professional experience in data migration, data engineering, or a related field.
 Strong expertise in Oracle GoldenGate, including installation, configuration, and administration.
 In-depth knowledge of database management systems (e.g., Oracle, SQL Server, MySQL) and SQL.
 Experience in data mapping, transformation, and integration.
 Proficiency in performance tuning and optimization of GoldenGate processes.
 Excellent problem-solving skills and the ability to troubleshoot and resolve complex data migration issues.
 Strong understanding of data security and compliance.
 Effective communication and teamwork skills to collaborate with cross-functional teams and stakeholders.
 Proven track record of successfully leading data migration projects from inception to completion.
 Bachelor's or Master's degree in computer science, information technology, or a related field is preferred.

Job Type: Full-time
Pay: $50.00 - $55.00 per hour
Experience level:

 11+ years

Schedule:

 8 hour shift

Experience:

 Informatica: 10 years (Preferred)
 SQL: 10 years (Preferred)
 Data warehouse: 10 years (Preferred)

Work Location: Remote","<p>Job Title: Senior Data Migration Specialist / Data Engineer (GoldenGate)</p>
<p>Location: Remote</p>
<p>Job Type: Full-time</p>
<p>Experience Level: Senior Level (Minimum 12 years)</p>
<p>Job Summary:</p>
<p>As a Senior Data Migration Specialist / Data Engineer, you will play a pivotal role in designing, implementing, and managing data migration solutions using Oracle <b>GoldenGate</b>. You will work closely with cross-functional teams to ensure seamless data migration, replication, and integration across various systems, databases, and platforms.</p>
<p>General Requirements:</p>
<ul>
 <li>Minimum 12 years of professional experience in data migration, data engineering, or a related field.</li>
 <li>Strong expertise in Oracle GoldenGate, including installation, configuration, and administration.</li>
 <li>In-depth knowledge of database management systems (e.g., Oracle, SQL Server, MySQL) and SQL.</li>
 <li>Experience in data mapping, transformation, and integration.</li>
 <li>Proficiency in performance tuning and optimization of GoldenGate processes.</li>
 <li>Excellent problem-solving skills and the ability to troubleshoot and resolve complex data migration issues.</li>
 <li>Strong understanding of data security and compliance.</li>
 <li>Effective communication and teamwork skills to collaborate with cross-functional teams and stakeholders.</li>
 <li>Proven track record of successfully leading data migration projects from inception to completion.</li>
 <li>Bachelor&apos;s or Master&apos;s degree in computer science, information technology, or a related field is preferred.</li>
</ul>
<p>Job Type: Full-time</p>
<p>Pay: &#x24;50.00 - &#x24;55.00 per hour</p>
<p>Experience level:</p>
<ul>
 <li>11+ years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Informatica: 10 years (Preferred)</li>
 <li>SQL: 10 years (Preferred)</li>
 <li>Data warehouse: 10 years (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,"efb4cc43afb6f8dd",,"Full-time",,"Remote","Senior Data Migration Specialist / Data Engineer","1 day ago","2023-10-24T11:47:15.003Z",,,"$50 - $55 an hour","2023-10-25T11:47:15.004Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=efb4cc43afb6f8dd&from=jasx&tk=1hdjagqsjjgbm800&vjs=3"
"Right Talents","Estimated Best in Market 
   
  
 
 
  
   Long Term
  
  
   
     Posted on: 12/22/2022 
   
  
 
 
  Job Description:
  
   Looking for candidates on W2
   Need to attend Initial Screening Test
 
 
 
   Responsibilities:
  
   
   Working closely with business stakeholders and product owners on requirement gathering & assessment. 
   Working with QA & Business Analysts on Integration & User Acceptance Testing. 
   Working using Agile Delivery Methodology, to deliver in Quick Delivery Cycles. 
   Developing Power BI datasets, dataflows and dashboard creation from various data sources. 
   Developing visual reports, dashboards and KPI scorecards using BI tools like Power BI, Qlik Sense and Tableau. 
   Developing paginated reports using Power Bi Build. 
   Connecting to data sources, importing data and transforming data for Business Intelligence using Qlik Sense and Power BI. Create and Maintain Design specifications and Support documentation. 
   Involve in Design Reviews & Post-Production Support Activities. 
   Troubleshoot systems production/performance issues. 
   Must be passionate about contributing to an organization focused on continuously improving consumer experiences. 
   Must be open to new data technologies and self-learning for new business needs. 
   
 
 
 
  Requirements:
  
   
   5+ years of overall IT Experience working with Business Intelligence and Data understanding. 
   3 + years designing and developing reports and dashboards in Power BI. 
   3 + Strong SQL skills (complex queries on relational databases, stored procedures, automation) required. 
   2+ years designing and developing reports and dashboards using BI tools like Power bi , DAX functions, Qlik Sense and Tableau. 
   Experience with Designing Data Visualization for Business Needs. 
   Excellent communication skills and understanding Business requirements. 
   Hands-On experience with GitHub, JIRA, and CI/CD tools. 
   Experience using statistical computer languages (Python, SQL, R, etc.) to manipulate data and draw insights from large data sets 
   Experience working in an Agile Scrum environment or good familiarity with the Agile methodologies. 
   Good understanding of Data Warehousing & Data Modeling fundamentals. 
   Strong ability to communicate clearly and professionally with end customers. 
   Any Cloud Application, Database, Data Integration, Business Intelligence Application Certifications are preferable. 
   Exposure to Data Streaming, and Data API Design & Development experience would be preferable. 
   Familiarity or working experience with one of tools like SSIS, ER Studio.","<div></div>
<div>
 <div>
  <div>
   <div>
    Estimated Best in Market 
   </div>
  </div>
 </div>
 <div>
  <div>
   Long Term
  </div>
  <div>
   <div>
     Posted on: 12/22/2022 
   </div>
  </div>
 </div>
 <div>
  <h6 class=""jobSectionHeader""><b>Job Description:</b></h6>
  <p></p>
  <p> Looking for candidates on W2</p>
  <p> Need to attend Initial Screening Test</p>
 </div>
 <p></p>
 <div>
  <h6 class=""jobSectionHeader""><b> Responsibilities:</b></h6>
  <p></p>
  <ul> 
   <li>Working closely with business stakeholders and product owners on requirement gathering &amp; assessment.</li> 
   <li>Working with QA &amp; Business Analysts on Integration &amp; User Acceptance Testing.</li> 
   <li>Working using Agile Delivery Methodology, to deliver in Quick Delivery Cycles.</li> 
   <li>Developing Power BI datasets, dataflows and dashboard creation from various data sources.</li> 
   <li>Developing visual reports, dashboards and KPI scorecards using BI tools like Power BI, Qlik Sense and Tableau.</li> 
   <li>Developing paginated reports using Power Bi Build.</li> 
   <li>Connecting to data sources, importing data and transforming data for Business Intelligence using Qlik Sense and Power BI.<br> Create and Maintain Design specifications and Support documentation.</li> 
   <li>Involve in Design Reviews &amp; Post-Production Support Activities.</li> 
   <li>Troubleshoot systems production/performance issues.</li> 
   <li>Must be passionate about contributing to an organization focused on continuously improving consumer experiences.</li> 
   <li>Must be open to new data technologies and self-learning for new business needs.</li> 
  </ul> 
 </div>
 <p></p>
 <div>
  <h6 class=""jobSectionHeader""><b>Requirements:</b></h6>
  <p></p>
  <ul> 
   <li>5+ years of overall IT Experience working with Business Intelligence and Data understanding.</li> 
   <li>3 + years designing and developing reports and dashboards in Power BI.</li> 
   <li>3 + Strong SQL skills (complex queries on relational databases, stored procedures, automation) required.</li> 
   <li>2+ years designing and developing reports and dashboards using BI tools like Power bi , DAX functions, Qlik Sense and Tableau.</li> 
   <li>Experience with Designing Data Visualization for Business Needs.</li> 
   <li>Excellent communication skills and understanding Business requirements.</li> 
   <li>Hands-On experience with GitHub, JIRA, and CI/CD tools.</li> 
   <li>Experience using statistical computer languages (Python, SQL, R, etc.) to manipulate data and draw insights from large data sets</li> 
   <li>Experience working in an Agile Scrum environment or good familiarity with the Agile methodologies.</li> 
   <li>Good understanding of Data Warehousing &amp; Data Modeling fundamentals.</li> 
   <li>Strong ability to communicate clearly and professionally with end customers.</li> 
   <li>Any Cloud Application, Database, Data Integration, Business Intelligence Application Certifications are preferable.</li> 
   <li>Exposure to Data Streaming, and Data API Design &amp; Development experience would be preferable.</li> 
   <li>Familiarity or working experience with one of tools like SSIS, ER Studio.</li>
  </ul>
 </div>
</div>
<p></p>","https://righttalents.net/openpositions/jobdetails/Data%20Analyst%2FEngineer%2FScientist_CBJOBID0314","639c90e33f632674",,,,"Remote","Data Analyst/Engineer/Scientist","2 days ago","2023-10-23T11:47:09.382Z",,,,"2023-10-25T11:47:09.383Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=639c90e33f632674&from=jasx&tk=1hdjagqsjjgbm800&vjs=3"
"iQuasar LLC","Data Engineer (Sr) 
Job Type: Full-time
Scheduled Weekly Hours: 40
Location: Remote
Work Status: US citizens only due to federal/government requirement.
Required Skills/Experience

 You have more than eight (8) years of experience in Data/ML engineering. If school experience is used, at most that would contribute to 2 years of actual experience


 You have experience with ETL, Data Labeling and Data Prep


 You have experience designing, implementing, and maintaining data architecture and services to be used for AI/ML. Additionally, operationalizing and maintaining AI/ML models in production


 You have experience with Python, Databricks, SSIS or Pentaho, Postgres, Spark, Airflow, and Kafka • You have advanced-level experience with SQL and SQL functions. •

You have the ability to perform data analytics on program related or system related activities. This will include assessing performance and manual processes implementing methods/algorithms to automate/optimize •
You have at a minimum, a bachelor’s degree in Computer Science, Information Technology Management or Engineering, or other comparable degree or experience

 You have the ability to obtain and maintain DHS Suitability Preferred Skills/Experience


 You have experience within Big Data environments (managing, accessing)


 You have experience operating in a DevSecOps and MLOps environment


 You have familiarity with cloud services, CPUs and GPUs


 You possess the ability to work independently/with minimal oversight


 You're a team player, passionate, and have a strong work ethic!

Education: • Bachelor's degree in Computer Science or related discipline
Job Type: Full-time
Salary: $105,000.00 - $145,000.00 per year
Benefits:

 401(k)
 Dental insurance
 Health insurance
 Life insurance
 Paid time off
 Parental leave
 Vision insurance

Experience level:

 8 years

Schedule:

 8 hour shift

Education:

 Bachelor's (Preferred)

Experience:

 Data Engineering: 8 years (Preferred)
 SQL: 5 years (Preferred)
 ETL: 3 years (Preferred)
 Python: 2 years (Preferred)

Work Location: Remote","<p><b>Data Engineer (Sr) </b></p>
<p>Job Type: Full-time</p>
<p>Scheduled Weekly Hours: 40</p>
<p>Location: Remote</p>
<p>Work Status: US citizens only due to federal/government requirement.</p>
<p>Required Skills/Experience</p>
<ul>
 <li>You have more than eight (8) years of experience in Data/ML engineering. If school experience is used, at most that would contribute to 2 years of actual experience</li>
</ul>
<ul>
 <li>You have experience with ETL, Data Labeling and Data Prep</li>
</ul>
<ul>
 <li>You have experience designing, implementing, and maintaining data architecture and services to be used for AI/ML. Additionally, operationalizing and maintaining AI/ML models in production</li>
</ul>
<ul>
 <li>You have experience with Python, Databricks, SSIS or Pentaho, Postgres, Spark, Airflow, and Kafka &#x2022; You have advanced-level experience with SQL and SQL functions. &#x2022;</li>
</ul>
<p>You have the ability to perform data analytics on program related or system related activities. This will include assessing performance and manual processes implementing methods/algorithms to automate/optimize &#x2022;</p>
<p>You have at a minimum, a bachelor&#x2019;s degree in Computer Science, Information Technology Management or Engineering, or other comparable degree or experience</p>
<ul>
 <li>You have the ability to obtain and maintain DHS Suitability Preferred Skills/Experience</li>
</ul>
<ul>
 <li>You have experience within Big Data environments (managing, accessing)</li>
</ul>
<ul>
 <li>You have experience operating in a DevSecOps and MLOps environment</li>
</ul>
<ul>
 <li>You have familiarity with cloud services, CPUs and GPUs</li>
</ul>
<ul>
 <li>You possess the ability to work independently/with minimal oversight</li>
</ul>
<ul>
 <li>You&apos;re a team player, passionate, and have a strong work ethic!</li>
</ul>
<p><b>Education</b>: &#x2022; Bachelor&apos;s degree in Computer Science or related discipline</p>
<p>Job Type: Full-time</p>
<p>Salary: &#x24;105,000.00 - &#x24;145,000.00 per year</p>
<p>Benefits:</p>
<ul>
 <li>401(k)</li>
 <li>Dental insurance</li>
 <li>Health insurance</li>
 <li>Life insurance</li>
 <li>Paid time off</li>
 <li>Parental leave</li>
 <li>Vision insurance</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>8 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
</ul>
<p>Education:</p>
<ul>
 <li>Bachelor&apos;s (Preferred)</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Data Engineering: 8 years (Preferred)</li>
 <li>SQL: 5 years (Preferred)</li>
 <li>ETL: 3 years (Preferred)</li>
 <li>Python: 2 years (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,"b58aac4026ea6a1e",,"Full-time",,"Remote","Senior Data Engineer","1 day ago","2023-10-24T11:47:18.896Z",,,"$105,000 - $145,000 a year","2023-10-25T11:47:18.897Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=b58aac4026ea6a1e&from=jasx&tk=1hdjagqsjjgbm800&vjs=3"
"Paramount+","OVERVIEW & RESPONSIBILITIES 
We are looking for someone who is thrilled to build Data Science and ML products that formulate our business strategy, optimize our content, and guide Marketing investment decisions. To build these products, you will leverage data collected from tens of millions of Paramount+ content users, including video consumption data from Adobe Analytics™ ClickStream, purchase / subscription data, and Ad revenue data from Doubleclick™. Efficiently building Data Science and ML products at Paramount+ requires experience in conceiving and deploying statistical and/or machine learning models, deep Python and SQL experience, perspicacity writing code in a collaborative setting, and excellent communication skills. 

 RESPONSIBILITIES 
Translate complex business problems into coherent, actionable quantitative solutions 
Implement, automate, and maintain reliable, performant, and end-to-end ML systems using software engineering and MLOps standard methodologies 
Deliver clear and actionable insights to collaborators 
Collaborate with partners across the org to implement data science solutions that inform the business strategy, optimize our content, and guide marketing investment decisions 
Build models to guide our content strategy such as content affinity and valuation models 
Build MLOps products used to monitor, persist, and expose ML models 
Build models to guide our customer lifecycle strategy, such as subscriber churn models 
Build models to guide our marketing strategy, such as audience segmentation models 

 BASIC QUALIFICATIONS 
STEM undergraduate degree in Statistics, Engineering, Informatics, Computer Science or similar 
4+ years experience in Data Science or ML Engineering 
Broad experience with both supervised and unsupervised machine learning techniques 
Ability to break complex problems into simple, coherent solutions 
Deep knowledge of the range and breadth of Data Science tools best suited for a given business problem 
Deep experience writing robust Python (Pandas, Airflow) and complex SQL code in a collaborative team setting, using Git™ distributed version control system 
Full stack Data Science experience, ranging from data pipeline to model deployment and maintenance 
Strong detail orientation with a penchant for data accuracy 
An ability to communicate concisely and persuasively with engineers, product managers, partners and collaborators 

 ADDITIONAL QUALIFICATIONS 
STEM graduate or post-graduate degree in Statistics, Engineering, Informatics, Computer Science or similar 
Experience with media or subscription businesses 
Experience using Google Cloud Platform (BigQuery, ML Engine, and APIs) 
Experience using project management tools (JIRA, Confluence) 
#LI-FV 37877 
#LI-REMOTE 

 Paramount+, a direct-to-consumer digital subscription video on-demand and live streaming service from Paramount Global, combines live sports, breaking news, and a mountain of entertainment. The premium streaming service features an expansive library of original series, hit shows and popular movies across every genre from world-renowned brands and production studios, including BET, CBS, Comedy Central, MTV, Nickelodeon, Paramount Pictures and the Smithsonian Channel. The service is also the streaming home to unmatched sports programming, including every CBS Sports event, from golf to football to basketball and more, plus exclusive streaming rights for major sports properties, including some of the world’s biggest and most popular soccer leagues. Paramount+ also enables subscribers to stream local CBS stations live across the U.S. in addition to the ability to stream Paramount Streaming’s other live channels: CBSN for 24/7 news, CBS Sports HQ for sports news and analysis, and ET Live for entertainment coverage. 

 
Paramount Global (NASDAQ: PARA, PARAA) is a leading global media and entertainment company that creates premium content and experiences for audiences worldwide. Driven by iconic studios, networks and streaming services, Paramount's portfolio of consumer brands includes CBS, Showtime Networks, Paramount Pictures, Nickelodeon, MTV, Comedy Central, BET, Paramount+, Pluto TV and Simon & Schuster, among others. Paramount delivers the largest share of the U.S. television audience and boasts one of the industry's most important and extensive libraries of TV and film titles. In addition to offering innovative streaming services and digital video products, the company provides powerful capabilities in production, distribution and advertising solutions. 

 ADDITIONAL INFORMATION 

 
Hiring Salary Range: $124,000.00 - 165,000.00. 

 The hiring salary range for this position applies to New York City, California, Colorado, Washington state, and most other geographies. Starting pay for the successful applicant depends on a variety of job-related factors, including but not limited to geographic location, market demands, experience, training, and education. The benefits available for this position include medical, dental, vision, 401(k) plan, life insurance coverage, disability benefits, tuition assistance program and PTO or, if applicable, as otherwise dictated by the appropriate Collective Bargaining Agreement. This position is bonus eligible. 

 https://www.paramount.com/careers/benefits 

 Paramount is an equal opportunity employer (EOE) including disability/vet. 

 At Paramount, the spirit of inclusion feeds into everything that we do, on-screen and off. From the programming and movies we create to employee benefits/programs and social impact outreach initiatives, we believe that opportunity, access, resources and rewards should be available to and for the benefit of all. Paramount is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ethnicity, ancestry, religion, creed, sex, national origin, sexual orientation, age, citizenship status, marital status, disability, gender identity, gender expression, and Veteran status. 

 If you are a qualified individual with a disability or a disabled veteran, you may request a reasonable accommodation if you are unable or limited in your ability to use or access. https://www.paramount.com/careers as a result of your disability. You can request reasonable accommodations by calling 212.846.5500 or by sending an email to paramountaccommodations@paramount.com. Only messages left for this purpose will be returned.","OVERVIEW &amp; RESPONSIBILITIES 
<br>We are looking for someone who is thrilled to build Data Science and ML products that formulate our business strategy, optimize our content, and guide Marketing investment decisions. To build these products, you will leverage data collected from tens of millions of Paramount+ content users, including video consumption data from Adobe Analytics&#x2122; ClickStream, purchase / subscription data, and Ad revenue data from Doubleclick&#x2122;. Efficiently building Data Science and ML products at Paramount+ requires experience in conceiving and deploying statistical and/or machine learning models, deep Python and SQL experience, perspicacity writing code in a collaborative setting, and excellent communication skills. 
<br>
<br> RESPONSIBILITIES 
<br>Translate complex business problems into coherent, actionable quantitative solutions 
<br>Implement, automate, and maintain reliable, performant, and end-to-end ML systems using software engineering and MLOps standard methodologies 
<br>Deliver clear and actionable insights to collaborators 
<br>Collaborate with partners across the org to implement data science solutions that inform the business strategy, optimize our content, and guide marketing investment decisions 
<br>Build models to guide our content strategy such as content affinity and valuation models 
<br>Build MLOps products used to monitor, persist, and expose ML models 
<br>Build models to guide our customer lifecycle strategy, such as subscriber churn models 
<br>Build models to guide our marketing strategy, such as audience segmentation models 
<br>
<br> BASIC QUALIFICATIONS 
<br>STEM undergraduate degree in Statistics, Engineering, Informatics, Computer Science or similar 
<br>4+ years experience in Data Science or ML Engineering 
<br>Broad experience with both supervised and unsupervised machine learning techniques 
<br>Ability to break complex problems into simple, coherent solutions 
<br>Deep knowledge of the range and breadth of Data Science tools best suited for a given business problem 
<br>Deep experience writing robust Python (Pandas, Airflow) and complex SQL code in a collaborative team setting, using Git&#x2122; distributed version control system 
<br>Full stack Data Science experience, ranging from data pipeline to model deployment and maintenance 
<br>Strong detail orientation with a penchant for data accuracy 
<br>An ability to communicate concisely and persuasively with engineers, product managers, partners and collaborators 
<br>
<br> ADDITIONAL QUALIFICATIONS 
<br>STEM graduate or post-graduate degree in Statistics, Engineering, Informatics, Computer Science or similar 
<br>Experience with media or subscription businesses 
<br>Experience using Google Cloud Platform (BigQuery, ML Engine, and APIs) 
<br>Experience using project management tools (JIRA, Confluence) 
<br>#LI-FV 37877 
<br>#LI-REMOTE 
<br>
<br> Paramount+, a direct-to-consumer digital subscription video on-demand and live streaming service from Paramount Global, combines live sports, breaking news, and a mountain of entertainment. The premium streaming service features an expansive library of original series, hit shows and popular movies across every genre from world-renowned brands and production studios, including BET, CBS, Comedy Central, MTV, Nickelodeon, Paramount Pictures and the Smithsonian Channel. The service is also the streaming home to unmatched sports programming, including every CBS Sports event, from golf to football to basketball and more, plus exclusive streaming rights for major sports properties, including some of the world&#x2019;s biggest and most popular soccer leagues. Paramount+ also enables subscribers to stream local CBS stations live across the U.S. in addition to the ability to stream Paramount Streaming&#x2019;s other live channels: CBSN for 24/7 news, CBS Sports HQ for sports news and analysis, and ET Live for entertainment coverage. 
<br>
<br> 
<b>Paramount Global (NASDAQ:</b> PARA, PARAA) is a leading global media and entertainment company that creates premium content and experiences for audiences worldwide. Driven by iconic studios, networks and streaming services, Paramount&apos;s portfolio of consumer brands includes CBS, Showtime Networks, Paramount Pictures, Nickelodeon, MTV, Comedy Central, BET, Paramount+, Pluto TV and Simon &amp; Schuster, among others. Paramount delivers the largest share of the U.S. television audience and boasts one of the industry&apos;s most important and extensive libraries of TV and film titles. In addition to offering innovative streaming services and digital video products, the company provides powerful capabilities in production, distribution and advertising solutions. 
<br>
<br> ADDITIONAL INFORMATION 
<br>
<br> 
<b>Hiring Salary Range:</b> &#x24;124,000.00 - 165,000.00. 
<br>
<br> The hiring salary range for this position applies to New York City, California, Colorado, Washington state, and most other geographies. Starting pay for the successful applicant depends on a variety of job-related factors, including but not limited to geographic location, market demands, experience, training, and education. The benefits available for this position include medical, dental, vision, 401(k) plan, life insurance coverage, disability benefits, tuition assistance program and PTO or, if applicable, as otherwise dictated by the appropriate Collective Bargaining Agreement. This position is bonus eligible. 
<br>
<br> https://www.paramount.com/careers/benefits 
<br>
<br> Paramount is an equal opportunity employer (EOE) including disability/vet. 
<br>
<br> At Paramount, the spirit of inclusion feeds into everything that we do, on-screen and off. From the programming and movies we create to employee benefits/programs and social impact outreach initiatives, we believe that opportunity, access, resources and rewards should be available to and for the benefit of all. Paramount is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ethnicity, ancestry, religion, creed, sex, national origin, sexual orientation, age, citizenship status, marital status, disability, gender identity, gender expression, and Veteran status. 
<br>
<br> If you are a qualified individual with a disability or a disabled veteran, you may request a reasonable accommodation if you are unable or limited in your ability to use or access. https://www.paramount.com/careers as a result of your disability. You can request reasonable accommodations by calling 212.846.5500 or by sending an email to paramountaccommodations@paramount.com. Only messages left for this purpose will be returned.","https://careers.paramount.com/job/New-York-Senior-Data-Scientist-Machine-Learning-Engineer-NY-10036/1078906800/?feedId=341000&utm_source=Indeed&utm_campaign=Paramount_Indeed","241a80b3883d2e12",,"Full-time",,"New York, NY 10036","Senior Data Scientist / Machine Learning Engineer","2 days ago","2023-10-23T11:47:12.028Z","3.9","2002","$124,000 - $165,000 a year","2023-10-25T11:47:12.029Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=241a80b3883d2e12&from=jasx&tk=1hdjagqsjjgbm800&vjs=3"
"Land Intelligence Inc","Who We Are Land Intelligence is a software technology company serving the commercial real estate industry. We focus on Land Development. We have been recognized as an industry technology leader in providing solutions on a national scale. Our team are visionaries that see a better, faster, and more valuable way to research, finance, and trade land.  Our Culture We are entrepreneurs first. Which means we manage the people, processes and product. We create new ways of doing things to drive value. We are builders and growth minded. Our leadership team has been recognized as a Best Places to work in the industry nationally. Our team drives for personal and professional development, as personal growth is instrumental to our success. Your learning will be supported by specialized in-house training programs and mentoring by the industry’s leading experts, many of whom are our investors and strategic partners.
  Job Overview Land Intelligence is seeking a savvy Data Engineer to join our growing team and help us continue to enhance our SaaS platform, LandSUITE®. The hire will be responsible for expanding and optimizing our data and data pipeline architecture to support product development and internal tools. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. They must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products. The right candidate will be excited by the prospect of re-designing our company's data architecture to support our next generation of products and data initiatives.
  Responsibilities
 
   Create and maintain optimal data pipeline architecture
   Assemble large, complex datasets that meet business requirements
   Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
   Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS 'big data' technologies
   Build analytics tools that use the data pipeline to provide actionable insights into user behavior and market trends
   Work with stakeholders including the executive and product development teams to assist with data-related technical issues and support their data infrastructure needs
   Keep our data separated and secure across AWS regions
   Create data tools for analytics and team members that assist them in building and optimizing LandSUITE® into an innovative industry leader
 
  Qualifications
 
   We are looking for a candidate with 5+ years of experience in a Data Engineer role who has attained a degree in Computer Science, Statistics, Informatics, Information Systems, or another quantitative field
   Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases
   Experience building and optimizing ‘big data’ data pipelines, architectures and data sets
   Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement
   Strong analytic skills related to working with unstructured datasets
   Build processes supporting data transformation, data structures, metadata, dependency and workload management
   A successful history of manipulating, processing and extracting value from large, disconnected datasets
   Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores
   Strong project management and organizational skills
   Experience supporting and working with cross-functional teams in a dynamic environment
   Experience with the following software/tools:
   
     Experience with relational SQL and NoSQL databases, including Postgres, MySQL, and Cassandra
     Experience with data pipeline and workflow management tools
     Experience with stream-processing systems
     Experience with AWS cloud services: EC2, EMR, RDS, Redshift
     Experience with object-oriented/object function scripting languages: Python, Java
   
  We are a startup, but this isn't our first time doing this. As a result, you can get the thrill of working at a startup, with the resources of a publicly traded company. We offer a best-in-class benefits package, as we are a Professional Employment Organization (PEO) with our partner Insperity that includes medical, vision, dental and life insurance. Our 401 (k) program offers an employer match, along with a 401(k)-profit sharing and performance-based bonuses. • Generous paid time off
  
  Land Intelligence is an EOE/Affirmative Action Employer M/F/D/V. If you are interested in applying for employment and need special assistance to apply for a posted position, please send an e-mail to careers@landintelligence.net.","<div>
 <p><b>Who We Are</b><br> Land Intelligence is a software technology company serving the commercial real estate industry. We focus on Land Development. We have been recognized as an industry technology leader in providing solutions on a national scale. Our team are visionaries that see a better, faster, and more valuable way to research, finance, and trade land.<br> <br> <b>Our Culture</b><br> We are entrepreneurs first. Which means we manage the people, processes and product. We create new ways of doing things to drive value. We are builders and growth minded. Our leadership team has been recognized as a Best Places to work in the industry nationally. Our team drives for personal and professional development, as personal growth is instrumental to our success. Your learning will be supported by specialized in-house training programs and mentoring by the industry&#x2019;s leading experts, many of whom are our investors and strategic partners.</p>
 <h2 class=""jobSectionHeader""><b> Job Overview</b></h2> Land Intelligence is seeking a savvy Data Engineer to join our growing team and help us continue to enhance our SaaS platform, LandSUITE&#xae;. The hire will be responsible for expanding and optimizing our data and data pipeline architecture to support product development and internal tools. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. They must be self-directed and comfortable supporting the data needs of multiple teams, systems, and products. The right candidate will be excited by the prospect of re-designing our company&apos;s data architecture to support our next generation of products and data initiatives.
 <h2 class=""jobSectionHeader""><b> Responsibilities</b></h2>
 <ul>
  <li> Create and maintain optimal data pipeline architecture</li>
  <li> Assemble large, complex datasets that meet business requirements</li>
  <li> Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.</li>
  <li> Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS &apos;big data&apos; technologies</li>
  <li> Build analytics tools that use the data pipeline to provide actionable insights into user behavior and market trends</li>
  <li> Work with stakeholders including the executive and product development teams to assist with data-related technical issues and support their data infrastructure needs</li>
  <li> Keep our data separated and secure across AWS regions</li>
  <li> Create data tools for analytics and team members that assist them in building and optimizing LandSUITE&#xae; into an innovative industry leader</li>
 </ul>
 <h2 class=""jobSectionHeader""><b> Qualifications</b></h2>
 <ul>
  <li> We are looking for a candidate with 5+ years of experience in a Data Engineer role who has attained a degree in Computer Science, Statistics, Informatics, Information Systems, or another quantitative field</li>
  <li> Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases</li>
  <li> Experience building and optimizing &#x2018;big data&#x2019; data pipelines, architectures and data sets</li>
  <li> Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement</li>
  <li> Strong analytic skills related to working with unstructured datasets</li>
  <li> Build processes supporting data transformation, data structures, metadata, dependency and workload management</li>
  <li> A successful history of manipulating, processing and extracting value from large, disconnected datasets</li>
  <li> Working knowledge of message queuing, stream processing, and highly scalable &#x2018;big data&#x2019; data stores</li>
  <li> Strong project management and organizational skills</li>
  <li> Experience supporting and working with cross-functional teams in a dynamic environment</li>
  <li> Experience with the following software/tools:
   <ul>
    <li> Experience with relational SQL and NoSQL databases, including Postgres, MySQL, and Cassandra</li>
    <li> Experience with data pipeline and workflow management tools</li>
    <li> Experience with stream-processing systems</li>
    <li> Experience with AWS cloud services: EC2, EMR, RDS, Redshift</li>
    <li> Experience with object-oriented/object function scripting languages: Python, Java</li>
   </ul></li>
 </ul> We are a startup, but this isn&apos;t our first time doing this. As a result, you can get the thrill of working at a startup, with the resources of a publicly traded company. We offer a best-in-class benefits package, as we are a Professional Employment Organization (PEO) with our partner Insperity that includes medical, vision, dental and life insurance. Our 401 (k) program offers an employer match, along with a 401(k)-profit sharing and performance-based bonuses. &#x2022; Generous paid time off
 <br> 
 <br> Land Intelligence is an EOE/Affirmative Action Employer M/F/D/V. If you are interested in applying for employment and need special assistance to apply for a posted position, please send an e-mail to careers@landintelligence.net.
</div>","http://j.brt.mv/PortalViewRequirement.do?source=Indeed&reqGK=27709072","e84faeb02d3b9031",,"Full-time",,"Remote","Data Engineer","1 day ago","2023-10-24T11:47:16.029Z",,,,"2023-10-25T11:47:16.031Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=e84faeb02d3b9031&from=jasx&tk=1hdjagqsjjgbm800&vjs=3"
"Cox Communications","Cox Communications is the largest private telecom company in America, and we proudly serve six million homes and businesses across 18 states. At Cox, we are committed to creating meaningful moments of human connection, not only with our products and services, but also with our career opportunities. Come connect with us and let's build a better future together.
  
  As a Software Engineer II you will write and maintain data-driven code, create solutions to implement capabilities and features, improve performance and maintainability while reducing technical debt in alignment with technology roadmaps.
  
  
 Primary Responsibilities and Essential Functions: 
  
  
 
  Aid in design and maintain the integrity of solutions.
 
  
 
  Work directly with stakeholders to understand real world problems in our domain.
 
  
 
  Design and deliver technical solutions to business and product problems.
 
  
 
  Evolve problem statements into actionable items that enable the team to deliver measurable value.
 
  
 
  Demonstrate ownership of initiatives and drive them through to completion.
 
  
 
  Gather and processes raw, structured, semi-structured, and unstructured data using batch and real-time data processing frameworks.
 
  
 
  Working in a continuous integration, testing, and delivery SDLC employing automation
 
  
 
  Eager to dig into problems and bring proposed solutions to group discussion.
 
  
 
  Open to feedback and able to creatively adapt multiple ideas into a solution.
 
  
 
  Analytical skills and the ability to pay careful attention to detail.
 
  
 
  Technical writing including high- and low-level diagramming techniques.
 
  
 
  Performs trouble-shooting efforts and investigations when necessary.
 
  
 
  Actively participates in the engineering community, staying up to date on new software technologies and best practices and shares insights with others in the organization.
 
  
  Qualifications
  
  
 Minimum: 
  
  
 
  Bachelor's degree in a related discipline and 2 years' experience in a related field. The right candidate could also have a different combination, such as a master's degree and up to 2 years' experience; or 14 years' experience in a related field.
 
  
 
  Demonstrated experience with data platforms: Big Data (Java, Python, PySpark..) and Cloud AWS Serverless Computing stack (e.g., Lambda, DynamoDB, S3, SQS, API Gateway, AWS Glue et al.)
 
  
 
  Excellent analytical, decision-making, problem-solving, team and time management.
 
  
 
  Knowledge of end-to-end systems development life cycles for software development
 
  
 
  Excellent verbal and written communication skills to technical and non-technical audiences of various levels in the organization
 
  
 
  Ability to estimate work effort for project sub-plans or small projects
 
  
 
  Positive outlook, strong work ethic, and responsive to stakeholders and partners
 
  
 
  Resourceful and proactive in gathering information and sharing ideas
 
  
  Preferred: 
  
  
 
  Azure, AWS, or Snowflake Cloud skills (preferably AWS) based around the design and development of multiple data pipeline.
 
  
 
  Hadoop ecosystem tools and concepts like Hive, Kafka, Spark, Scala, Hbase or Sqoop.
 
  
 
  Experience working in an Agile Development environment.
 
  
 
  Experience working on a DevOps Team.
 
  #LI-104
  
  
 About Cox Communications
  
  Cox Communications is the largest private telecom company in America, serving six million homes and businesses. That's a lot, but we also proudly serve our employees. Our benefits and our award-winning culture are just two of the things that make Cox a coveted place to work. If you're interested in bringing people closer through broadband, smart home tech and more, join Cox Communications today!
  
  
 About Cox
  
  Cox empowers employees to build a better future and has been doing so for over 120 years. With exciting investments and innovations across transportation, communications, cleantech and healthcare, our family of businesses - which includes Cox Automotive and Cox Communications - is forging a better future for us all. Ready to make your mark? Join us today!
  
  Benefits of working at Cox may include health care insurance (medical, dental, vision), retirement planning (401(k)), and paid days off (sick leave, parental leave, flexible vacation/wellness days, and/or PTO). For more details on what benefits you may be offered, visit our benefits page .
  
  Cox is an Equal Employment Opportunity employer - All qualified applicants/employees will receive consideration for employment without regard to that individual's age, race, color, religion or creed, national origin or ancestry, sex (including pregnancy), sexual orientation, gender, gender identity, physical or mental disability, veteran status, genetic information, ethnicity, citizenship, or any other characteristic protected by law.
  
  Statement to ALL Third-Party Agencies and Similar Organizations: Cox accepts resumes only from agencies with which we formally engage their services. Please do not forward resumes to our applicant tracking system, Cox employees, Cox hiring manager, or send to any Cox facility. Cox is not responsible for any fees or charges associated with unsolicited resumes.","<div>
 Cox Communications is the largest private telecom company in America, and we proudly serve six million homes and businesses across 18 states. At Cox, we are committed to creating meaningful moments of human connection, not only with our products and services, but also with our career opportunities. Come connect with us and let&apos;s build a better future together.
 <br> 
 <br> As a Software Engineer II you will write and maintain data-driven code, create solutions to implement capabilities and features, improve performance and maintainability while reducing technical debt in alignment with technology roadmaps.
 <br> 
 <br> 
 <b>Primary Responsibilities and Essential Functions: </b>
 <br> 
 <br> 
 <ul>
  <li>Aid in design and maintain the integrity of solutions.</li>
 </ul>
 <br> 
 <ul>
  <li>Work directly with stakeholders to understand real world problems in our domain.</li>
 </ul>
 <br> 
 <ul>
  <li>Design and deliver technical solutions to business and product problems.</li>
 </ul>
 <br> 
 <ul>
  <li>Evolve problem statements into actionable items that enable the team to deliver measurable value.</li>
 </ul>
 <br> 
 <ul>
  <li>Demonstrate ownership of initiatives and drive them through to completion.</li>
 </ul>
 <br> 
 <ul>
  <li>Gather and processes raw, structured, semi-structured, and unstructured data using batch and real-time data processing frameworks.</li>
 </ul>
 <br> 
 <ul>
  <li>Working in a continuous integration, testing, and delivery SDLC employing automation</li>
 </ul>
 <br> 
 <ul>
  <li>Eager to dig into problems and bring proposed solutions to group discussion.</li>
 </ul>
 <br> 
 <ul>
  <li>Open to feedback and able to creatively adapt multiple ideas into a solution.</li>
 </ul>
 <br> 
 <ul>
  <li>Analytical skills and the ability to pay careful attention to detail.</li>
 </ul>
 <br> 
 <ul>
  <li>Technical writing including high- and low-level diagramming techniques.</li>
 </ul>
 <br> 
 <ul>
  <li>Performs trouble-shooting efforts and investigations when necessary.</li>
 </ul>
 <br> 
 <ul>
  <li>Actively participates in the engineering community, staying up to date on new software technologies and best practices and shares insights with others in the organization.</li>
 </ul>
 <br> 
 <b> Qualifications</b>
 <br> 
 <br> 
 <b>Minimum: </b>
 <br> 
 <br> 
 <ul>
  <li>Bachelor&apos;s degree in a related discipline and 2 years&apos; experience in a related field. The right candidate could also have a different combination, such as a master&apos;s degree and up to 2 years&apos; experience; or 14 years&apos; experience in a related field.</li>
 </ul>
 <br> 
 <ul>
  <li>Demonstrated experience with data platforms: Big Data (Java, Python, PySpark..) and Cloud AWS Serverless Computing stack (e.g., Lambda, DynamoDB, S3, SQS, API Gateway, AWS Glue et al.)</li>
 </ul>
 <br> 
 <ul>
  <li>Excellent analytical, decision-making, problem-solving, team and time management.</li>
 </ul>
 <br> 
 <ul>
  <li>Knowledge of end-to-end systems development life cycles for software development</li>
 </ul>
 <br> 
 <ul>
  <li>Excellent verbal and written communication skills to technical and non-technical audiences of various levels in the organization</li>
 </ul>
 <br> 
 <ul>
  <li>Ability to estimate work effort for project sub-plans or small projects</li>
 </ul>
 <br> 
 <ul>
  <li>Positive outlook, strong work ethic, and responsive to stakeholders and partners</li>
 </ul>
 <br> 
 <ul>
  <li>Resourceful and proactive in gathering information and sharing ideas</li>
 </ul>
 <br> 
 <b> Preferred: </b>
 <br> 
 <br> 
 <ul>
  <li>Azure, AWS, or Snowflake Cloud skills (preferably AWS) based around the design and development of multiple data pipeline.</li>
 </ul>
 <br> 
 <ul>
  <li>Hadoop ecosystem tools and concepts like Hive, Kafka, Spark, Scala, Hbase or Sqoop.</li>
 </ul>
 <br> 
 <ul>
  <li>Experience working in an Agile Development environment.</li>
 </ul>
 <br> 
 <ul>
  <li>Experience working on a DevOps Team.</li>
 </ul>
 <br> #LI-104
 <br> 
 <br> 
 <b>About Cox Communications</b>
 <br> 
 <br> Cox Communications is the largest private telecom company in America, serving six million homes and businesses. That&apos;s a lot, but we also proudly serve our employees. Our benefits and our award-winning culture are just two of the things that make Cox a coveted place to work. If you&apos;re interested in bringing people closer through broadband, smart home tech and more, join Cox Communications today!
 <br> 
 <br> 
 <b>About Cox</b>
 <br> 
 <br> Cox empowers employees to build a better future and has been doing so for over 120 years. With exciting investments and innovations across transportation, communications, cleantech and healthcare, our family of businesses - which includes Cox Automotive and Cox Communications - is forging a better future for us all. Ready to make your mark? Join us today!
 <br> 
 <br> Benefits of working at Cox may include health care insurance (medical, dental, vision), retirement planning (401(k)), and paid days off (sick leave, parental leave, flexible vacation/wellness days, and/or PTO). For more details on what benefits you may be offered, visit our benefits page .
 <br> 
 <br> Cox is an Equal Employment Opportunity employer - All qualified applicants/employees will receive consideration for employment without regard to that individual&apos;s age, race, color, religion or creed, national origin or ancestry, sex (including pregnancy), sexual orientation, gender, gender identity, physical or mental disability, veteran status, genetic information, ethnicity, citizenship, or any other characteristic protected by law.
 <br> 
 <br> Statement to ALL Third-Party Agencies and Similar Organizations: Cox accepts resumes only from agencies with which we formally engage their services. Please do not forward resumes to our applicant tracking system, Cox employees, Cox hiring manager, or send to any Cox facility. Cox is not responsible for any fees or charges associated with unsolicited resumes.
</div>","https://click.appcast.io/track/hsu6xyn-org?cs=5c&jg=65v4&bid=lUf2CslKyPxm6i440ZgUYA==&ittk=OZADTQT6NI","4fda6e90f7b9b8d5",,"Full-time",,"Pensacola, FL","Big Data/Cloud Software Engineer ll","1 day ago","2023-10-24T11:47:22.244Z","3.8","3581",,"2023-10-25T11:47:22.246Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=4fda6e90f7b9b8d5&from=jasx&tk=1hdjagqsjjgbm800&vjs=3"
"Nike","Become a Part of the NIKE, Inc. Team
  
  \r
  
  \r
  
  NIKE, Inc. does more than outfit the world's best athletes. It is a place to explore potential, obliterate boundaries and push out the edges of what can be. The company looks for people who can grow, think, dream and create. Its culture thrives by embracing diversity and rewarding imagination. The brand seeks achievers, leaders and visionaries. At NIKE, Inc. it's about each person bringing skills and passion to a challenging and constantly evolving game.
  
  Open to remote work except in South Dakota, Vermont and West Virginia.
  
  The annual base salary for this position ranges from $112,600.00 in our lowest geographic market to $251,800.00 in our highest geographic market. Actual salary will vary based on a candidate's location, qualifications, skills and experience.
  
  Information about benefits can be found here .
  
  Become a Part of the NIKE, Inc. Team
  
  NIKE, Inc. does more than outfit the world's best athletes. It is a place to explore potential, obliterate boundaries and push out the edges of what can be. The company looks for people who can grow, think, dream and create. Its culture thrives by embracing diversity and rewarding imagination. The brand seeks achievers, leaders and visionaries. At Nike, it's about each person bringing skills and passion to a challenging and constantly evolving game!
  
  WHO WE ARE LOOKING FOR
  
  Now, more than ever, Technology needs to respond quickly to turn market disruptions into opportunities for our brand. To achieve this, we must continue to develop our Enterprise Analytics, Data Science & Machine Learning capabilities and team to ensure we're improving the power of the Nike enterprise in the analytics/machine learning space and managing data.
  
  We are seeking a hard-working Lead Data Engineer to play a meaningful role in the development of Nike's Enterprise Data & Analytics capabilities. You will direct a team of engineers with a focus on the development of new solutions and capabilities for various business functions across NIKE. As a Lead Engineer, you are thoughtful, aspirational, and adept at solutioning, architecture and delivery for ED&AI teams.
  
  WHAT YOU WILL WORK ON
  
 
   Leading the Technical delivery of scalable data and analytics solutions
   Produce outstanding code that meets all defined coding standards
   Implementing new and open-source technologies
   Craft and build reusable components, frameworks, and libraries at scale to support analytics data products
   Leading and evolving data and analytics products.
   Lead, encourage, mentor and grow a team of engineers
   Partner with Engineering Manager, Product Managers to help prioritize work
   Guide technology decisions that drive Nike standard methodologies.
   Drive new futuristic technology onboarding as per the business use cases
   Partner with other engineering teams across Nike, to collaborate on specific tools and technologies.
   Lead design and data architecture to process large data sets and solve various data challenges.
   Partner with enterprise platform and architecture to craft scalable data products and use standards and platforms at Nike.
   Optimize performance of existing artifacts including data pipelines.
   Implement action items against existing cost saving measures and develop strategy around visibility and cost reduction.
 
  WHO YOU WILL WORK WITH
  
  At Nike, we embrace and celebrate unique talent and experience of our teammates. You will work with diverse team of inquisitive, thoughtful, and creative people such as the Engineering Manager, Product Management team, Architects, Engineers, Data Analysts, Internal partners and other departments. They will span across various groups including Demand and Supply Management, Supply Chain, Commercial Analytics, and Consumer Insights. You will be in contact with a wide range of great people. One of Nike's maxims is ""Win as a Team"". In all interactions, you will find success in teamwork, a positive demeanor, and hard work.
  
  WHAT YOU BRING
  
 
   Bachelors degree in Computer Science or related technical subject area or equivalent combination of education and experience
   7+ years relevant work experience in the Data Engineering field
   4+ years proven experience working with AWS, EMR, S3, Python, Databricks - DBSql/Lakehouse (desired), Snowflake.
   2+ years experience building scalable, real-time and high-performance cloud data lake solutions
   Strong experience with relational SQL and programming languages such as Python, Scala, or Java
   Experience with source control tools such as GitHub and related CI/CD processes
   Experience working in AWS environment primarily EMR, S3, Kinesis, Redshift, Athena, etc
   Experience with data warehouses/RDBMS like Snowflake & Teradata
   Experience with workflow scheduling tools like Airflow
   Strong understanding of algorithms, data structures, data architecture, and technical designs.
   Proven track record to influence and communicate effectively with team members and business partners
   Ability to partner with teams in solving sophisticated problems by taking a broad perspective to identify innovative solutions
 
  #LI-KP1
  
  #REMOTE
  
  NIKE, Inc. is a growth company that looks for team members to grow with it. Nike offers a generous total rewards package, casual work environment, a diverse and inclusive culture, and an electric atmosphere for professional development. No matter the location, or the role, every Nike employee shares one galvanizing mission: To bring inspiration and innovation to every athlete* in the world.
  
  NIKE, Inc. is committed to employing a diverse workforce. Qualified applicants will receive consideration without regard to race, color, religion, sex, national origin, age, sexual orientation, gender identity, gender expression, veteran status, or disability.
  
  NIKE, Inc. is a growth company that looks for team members to grow with it. Nike offers a generous total rewards package, casual work environment, a diverse and inclusive culture, and an electric atmosphere for professional development. No matter the location, or the role, every Nike employee shares one galvanizing mission: To bring inspiration and innovation to every athlete* in the world.
  
  NIKE, Inc. is committed to employing a diverse workforce. Qualified applicants will receive consideration without regard to race, color, religion, sex, national origin, age, sexual orientation, gender identity, gender expression, veteran status, or disability.
  
  
 How We Hire
  
  At NIKE, Inc. we promise to provide a premium, inclusive, compelling and authentic candidate experience. Delivering on this promise means we allow you to be at your best - and to do that, you need to understand how the hiring process works. Transparency is key.
  
  
 
  This overview explains our hiring process for corporate roles. Note there may be different hiring steps involved for non-corporate roles.
 
  
  Benefits
  
  Whether it's transportation or financial health, we continually invest in our employees to help them achieve greatness - inside and outside of work. All who work here should be able to realize their full potential.","<div>
 Become a Part of the NIKE, Inc. Team
 <br> 
 <br> \r
 <br> 
 <br> \r
 <br> 
 <br> NIKE, Inc. does more than outfit the world&apos;s best athletes. It is a place to explore potential, obliterate boundaries and push out the edges of what can be. The company looks for people who can grow, think, dream and create. Its culture thrives by embracing diversity and rewarding imagination. The brand seeks achievers, leaders and visionaries. At NIKE, Inc. it&apos;s about each person bringing skills and passion to a challenging and constantly evolving game.
 <br> 
 <br> Open to remote work except in South Dakota, Vermont and West Virginia.
 <br> 
 <br> The annual base salary for this position ranges from &#x24;112,600.00 in our lowest geographic market to &#x24;251,800.00 in our highest geographic market. Actual salary will vary based on a candidate&apos;s location, qualifications, skills and experience.
 <br> 
 <br> Information about benefits can be found here .
 <br> 
 <br> Become a Part of the NIKE, Inc. Team
 <br> 
 <br> NIKE, Inc. does more than outfit the world&apos;s best athletes. It is a place to explore potential, obliterate boundaries and push out the edges of what can be. The company looks for people who can grow, think, dream and create. Its culture thrives by embracing diversity and rewarding imagination. The brand seeks achievers, leaders and visionaries. At Nike, it&apos;s about each person bringing skills and passion to a challenging and constantly evolving game!
 <br> 
 <br> WHO WE ARE LOOKING FOR
 <br> 
 <br> Now, more than ever, Technology needs to respond quickly to turn market disruptions into opportunities for our brand. To achieve this, we must continue to develop our Enterprise Analytics, Data Science &amp; Machine Learning capabilities and team to ensure we&apos;re improving the power of the Nike enterprise in the analytics/machine learning space and managing data.
 <br> 
 <br> We are seeking a hard-working Lead Data Engineer to play a meaningful role in the development of Nike&apos;s Enterprise Data &amp; Analytics capabilities. You will direct a team of engineers with a focus on the development of new solutions and capabilities for various business functions across NIKE. As a Lead Engineer, you are thoughtful, aspirational, and adept at solutioning, architecture and delivery for ED&amp;AI teams.
 <br> 
 <br> WHAT YOU WILL WORK ON
 <br> 
 <ul>
  <li> Leading the Technical delivery of scalable data and analytics solutions</li>
  <li> Produce outstanding code that meets all defined coding standards</li>
  <li> Implementing new and open-source technologies</li>
  <li> Craft and build reusable components, frameworks, and libraries at scale to support analytics data products</li>
  <li> Leading and evolving data and analytics products.</li>
  <li> Lead, encourage, mentor and grow a team of engineers</li>
  <li> Partner with Engineering Manager, Product Managers to help prioritize work</li>
  <li> Guide technology decisions that drive Nike standard methodologies.</li>
  <li> Drive new futuristic technology onboarding as per the business use cases</li>
  <li> Partner with other engineering teams across Nike, to collaborate on specific tools and technologies.</li>
  <li> Lead design and data architecture to process large data sets and solve various data challenges.</li>
  <li> Partner with enterprise platform and architecture to craft scalable data products and use standards and platforms at Nike.</li>
  <li> Optimize performance of existing artifacts including data pipelines.</li>
  <li> Implement action items against existing cost saving measures and develop strategy around visibility and cost reduction.</li>
 </ul>
 <br> WHO YOU WILL WORK WITH
 <br> 
 <br> At Nike, we embrace and celebrate unique talent and experience of our teammates. You will work with diverse team of inquisitive, thoughtful, and creative people such as the Engineering Manager, Product Management team, Architects, Engineers, Data Analysts, Internal partners and other departments. They will span across various groups including Demand and Supply Management, Supply Chain, Commercial Analytics, and Consumer Insights. You will be in contact with a wide range of great people. One of Nike&apos;s maxims is &quot;Win as a Team&quot;. In all interactions, you will find success in teamwork, a positive demeanor, and hard work.
 <br> 
 <br> WHAT YOU BRING
 <br> 
 <ul>
  <li> Bachelors degree in Computer Science or related technical subject area or equivalent combination of education and experience</li>
  <li> 7+ years relevant work experience in the Data Engineering field</li>
  <li> 4+ years proven experience working with AWS, EMR, S3, Python, Databricks - DBSql/Lakehouse (desired), Snowflake.</li>
  <li> 2+ years experience building scalable, real-time and high-performance cloud data lake solutions</li>
  <li> Strong experience with relational SQL and programming languages such as Python, Scala, or Java</li>
  <li> Experience with source control tools such as GitHub and related CI/CD processes</li>
  <li> Experience working in AWS environment primarily EMR, S3, Kinesis, Redshift, Athena, etc</li>
  <li> Experience with data warehouses/RDBMS like Snowflake &amp; Teradata</li>
  <li> Experience with workflow scheduling tools like Airflow</li>
  <li> Strong understanding of algorithms, data structures, data architecture, and technical designs.</li>
  <li> Proven track record to influence and communicate effectively with team members and business partners</li>
  <li> Ability to partner with teams in solving sophisticated problems by taking a broad perspective to identify innovative solutions</li>
 </ul>
 <br> #LI-KP1
 <br> 
 <br> #REMOTE
 <br> 
 <br> NIKE, Inc. is a growth company that looks for team members to grow with it. Nike offers a generous total rewards package, casual work environment, a diverse and inclusive culture, and an electric atmosphere for professional development. No matter the location, or the role, every Nike employee shares one galvanizing mission: To bring inspiration and innovation to every athlete* in the world.
 <br> 
 <br> NIKE, Inc. is committed to employing a diverse workforce. Qualified applicants will receive consideration without regard to race, color, religion, sex, national origin, age, sexual orientation, gender identity, gender expression, veteran status, or disability.
 <br> 
 <br> NIKE, Inc. is a growth company that looks for team members to grow with it. Nike offers a generous total rewards package, casual work environment, a diverse and inclusive culture, and an electric atmosphere for professional development. No matter the location, or the role, every Nike employee shares one galvanizing mission: To bring inspiration and innovation to every athlete* in the world.
 <br> 
 <br> NIKE, Inc. is committed to employing a diverse workforce. Qualified applicants will receive consideration without regard to race, color, religion, sex, national origin, age, sexual orientation, gender identity, gender expression, veteran status, or disability.
 <br> 
 <br> 
 <b>How We Hire</b>
 <br> 
 <br> At NIKE, Inc. we promise to provide a premium, inclusive, compelling and authentic candidate experience. Delivering on this promise means we allow you to be at your best - and to do that, you need to understand how the hiring process works. Transparency is key.
 <br> 
 <br> 
 <ul>
  <li>This overview explains our hiring process for corporate roles. Note there may be different hiring steps involved for non-corporate roles.</li>
 </ul>
 <br> 
 <b> Benefits</b>
 <br> 
 <br> Whether it&apos;s transportation or financial health, we continually invest in our employees to help them achieve greatness - inside and outside of work. All who work here should be able to realize their full potential.
</div>","https://click.appcast.io/track/hsy4yq6-org?cs=5c&jg=6uyq&bid=lUf2CslKyPxm6i440ZgUYA==&apstr=src%3DJB-11022&ittk=ERDJEL0QKH","8d140012013c13d1",,"Full-time",,"Salem, OR 97308","Lead Data Engineer - Consumer Analytics (Remote Work Option)","1 day ago","2023-10-24T11:47:22.078Z","4.1","11792","$112,600 - $251,800 a year","2023-10-25T11:47:22.080Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=8d140012013c13d1&from=jasx&tk=1hdjagqsjjgbm800&vjs=3"
"Disney","Principal Software Engineer - Data Platforms 
  Apply Later 
  
   
    Job ID
    10040221 
  
  
   
    Location
    Orlando, Florida, United States / Glendale, California, United States / Seattle, Washington, United States 
  
  
   
    Business
    Disney Parks, Experiences and Products 
  
  
   
    Date posted
    Oct. 23, 2023 
  
  
   Job Summary: 
   
    “We Power the Magic!” That’s our motto at Disney Parks, Experiences and Products Technology & Digital. Our team creates world-class immersive digital experiences for the Company’s premier vacation brands including Disney’s Parks & Resorts worldwide, Disney Cruise Line, Aulani, A Disney Resort & Spa, and Disney Vacation Club. We build and are responsible for the end-to-end digital and physical Guest experience for all technology & digital-led initiatives across the Attractions & Entertainment, Food & Beverage, Resorts & Transportation and Merchandise lines of business as well as other initiatives including MyDisneyExperience and Hey, Disney!
     We are looking for a Principal Software Engineer who will use their deep technical expertise to define engineering approaches and robust test automation for a data organization. The ideal candidate has broad skills that are balanced between interpersonal and technical capabilities to enable them to influence standards across the organization.
     This role sits in Data Products & Platforms within Technology & Digital for Disney Parks, Experiences, and Products.
     Responsibilities:
    
      Lead engineering practices for data environments that enable the collection, ingest, storage, and processing of large data products
      Collaborate with engineers to make decisions across teams on their engineering work and practices
      Stay up to date on the latest technologies and industry leading approaches in the field of data and software engineering
      Provide technical mentorship and guidance to data and software engineers at different levels
      Conduct code review pull-requests with consistent scrutiny on engineering practices and alignment with architecture guidance
    
     Qualifications:
    
      10+ years of software development experience across diverse domains and passionate about engineering practices in the data space.
      Expert level coder, able to develop solutions that solves sophisticated problems
      Authority in Python development and DevOps practices, e.g.,: CI/CD, docker, testing, and automation
      Detailed experience in at least one of the following languages - Typescript, Java, or another strongly typed language
      Strong experience implementing data software solutions using AWS and Data Warehouses like Snowflake and Google Cloud Platforms
      Experience with and believer of agile development approaches, including DevOps concepts
    
     Required Education
    
      Bachelor's degree in Computer Science, Information Systems, Software, Electrical or Electronics Engineering, or comparable field of study, and/or equivalent work experience
    
     Preferred Education:
    
      Master’s degree in Computer Science, Information Systems, Software, Electrical or Electronics Engineering, or comparable field of study, and/or equivalent work experience
    
     #DISNEYTECH
     The hiring range for this remote position is $172,036.00 to $265,430.00 per year, which factors in various geographic regions. The base pay actually offered will take into account internal equity and also may vary depending on the candidate’s geographic region, job-related knowledge, skills, and experience among other factors. A bonus and/or long-term incentive units may be provided as part of the compensation package, in addition to the full range of medical, financial, and/or other benefits, dependent on the level and position offered.","<div>
 <div>
  <p>Principal Software Engineer - Data Platforms</p> 
  <p></p>Apply Later 
  <div>
   <div>
    <b>Job ID</b>
   </div> 10040221 
  </div>
  <div>
   <div>
    <b>Location</b>
   </div> Orlando, Florida, United States / Glendale, California, United States / Seattle, Washington, United States 
  </div>
  <div>
   <div>
    <b>Business</b>
   </div> Disney Parks, Experiences and Products 
  </div>
  <div>
   <div>
    <b>Date posted</b>
   </div> Oct. 23, 2023 
  </div>
  <div>
   <h4 class=""jobSectionHeader""><b>Job Summary:</b></h4> 
   <div>
    <p>&#x201c;We Power the Magic!&#x201d; That&#x2019;s our motto at Disney Parks, Experiences and Products Technology &amp; Digital. Our team creates world-class immersive digital experiences for the Company&#x2019;s premier vacation brands including Disney&#x2019;s Parks &amp; Resorts worldwide, Disney Cruise Line, Aulani, A Disney Resort &amp; Spa, and Disney Vacation Club. We build and are responsible for the end-to-end digital and physical Guest experience for all technology &amp; digital-led initiatives across the Attractions &amp; Entertainment, Food &amp; Beverage, Resorts &amp; Transportation and Merchandise lines of business as well as other initiatives including MyDisneyExperience and Hey, Disney!</p>
    <p> We are looking for a Principal Software Engineer who will use their deep technical expertise to define engineering approaches and robust test automation for a data organization. The ideal candidate has broad skills that are balanced between interpersonal and technical capabilities to enable them to influence standards across the organization.</p>
    <p> This role sits in Data Products &amp; Platforms within Technology &amp; Digital for Disney Parks, Experiences, and Products.</p>
    <p><b> Responsibilities:</b></p>
    <ul>
     <li><p> Lead engineering practices for data environments that enable the collection, ingest, storage, and processing of large data products</p></li>
     <li><p> Collaborate with engineers to make decisions across teams on their engineering work and practices</p></li>
     <li><p> Stay up to date on the latest technologies and industry leading approaches in the field of data and software engineering</p></li>
     <li><p> Provide technical mentorship and guidance to data and software engineers at different levels</p></li>
     <li><p> Conduct code review pull-requests with consistent scrutiny on engineering practices and alignment with architecture guidance</p></li>
    </ul>
    <p><b> Qualifications:</b></p>
    <ul>
     <li><p> 10+ years of software development experience across diverse domains and passionate about engineering practices in the data space.</p></li>
     <li><p> Expert level coder, able to develop solutions that solves sophisticated problems</p></li>
     <li><p> Authority in Python development and DevOps practices, e.g.,: CI/CD, docker, testing, and automation</p></li>
     <li><p> Detailed experience in at least one of the following languages - Typescript, Java, or another strongly typed language</p></li>
     <li><p> Strong experience implementing data software solutions using AWS and Data Warehouses like Snowflake and Google Cloud Platforms</p></li>
     <li><p> Experience with and believer of agile development approaches, including DevOps concepts</p></li>
    </ul>
    <p><b> Required Education</b></p>
    <ul>
     <li><p> Bachelor&apos;s degree in Computer Science, Information Systems, Software, Electrical or Electronics Engineering, or comparable field of study, and/or equivalent work experience</p></li>
    </ul>
    <p><b> Preferred Education:</b></p>
    <ul>
     <li><p> Master&#x2019;s degree in Computer Science, Information Systems, Software, Electrical or Electronics Engineering, or comparable field of study, and/or equivalent work experience</p></li>
    </ul>
    <p> #DISNEYTECH</p>
    <br> The hiring range for this remote position is &#x24;172,036.00 to &#x24;265,430.00 per year, which factors in various geographic regions. The base pay actually offered will take into account internal equity and also may vary depending on the candidate&#x2019;s geographic region, job-related knowledge, skills, and experience among other factors. A bonus and/or long-term incentive units may be provided as part of the compensation package, in addition to the full range of medical, financial, and/or other benefits, dependent on the level and position offered.
   </div>
  </div>
 </div>
</div>","https://www.indeed.com/rc/clk?jk=530b87a7867afc6f&atk=&xpse=SoA467I3JzdF3LSDwB0LbzkdCdPP","530b87a7867afc6f",,,,"1050 Century Dr, Orlando, FL 32830","Principal Software Engineer - Data Platforms","1 day ago","2023-10-24T11:47:26.922Z","4.1","4471","$172,036 - $265,430 a year","2023-10-25T11:47:26.924Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=530b87a7867afc6f&from=jasx&tk=1hdjagqsjjgbm800&vjs=3"
"Octo","Octo, an IBM company, is an industry-leading, award-winning provider of technical solutions for the federal government. At Octo, we specialize in providing agile software engineering, user experience design, cloud services, and digital strategy services that address government's most pressing missions. Octo delivers intelligent solutions and rapid results, yielding lower costs and measurable outcomes. 
  Our team is what makes Octo great. At Octo you'll work beside some of the smartest and most accomplished staff you'll find in your career. Octo offers fantastic benefits and an amazing workplace culture where you will feel valued while you perform mission critical work for our government. Voted one of the region’s best places to work multiple times, Octo is an employer of choice! 
 You…
  
  
  As a 
 Senior Data Engineer at Octo, you will drive interactions with government stakeholders and teaming partners to gather requirements and propose analytic solutions.You will work with Octo business analysts and data analysts, as well as government stakeholders, to understand requirements and compile structured and unstructured data, ensuring its quality, accuracy, and reasonableness. You are comfortable leading technical teams comprised of individuals with varying skillsets and levels of experience. You have a passion for detail and are an excellent communicator. You are agile and curious and not afraid to identify what we are doing wrong so we can fix it, and what we are doing right so we can improve on it.
  
  
 Us…
  
  We were founded as a fresh alternative in the Government Consulting Community and are dedicated to the belief that results are a product of analytical thinking, agile design principles and that solutions are built in collaboration with, not for, our customers. This mantra drives us to succeed and act as true partners in advancing our client’s missions.
  
  
 Program Mission…
  
  
  You will be working on a high-profile data analytics program for the Department of Homeland of Security (DHS) maintaining and transforming mission critical applications. This program is the reporting system of record for a wide variety of agency and component data, including financial, human capital, and government assets.In addition to geospatial tools, this program relies on business intelligence and data analytics tools.
  
 
 Octo is an Equal Opportunity/Affirmative Action employer. All qualified candidates will receive consideration for employment without regard to disability, protected veteran status, race, color, religious creed, national origin, citizenship, marital status, sex, sexual orientation/gender identity, age, or genetic information. Selected applicant will be subject to a background investigation. 
  Octo is an IBM subsidiary which has been acquired by IBM and will be integrated into the IBM organization. Octo will be the hiring entity. By proceeding with this application, you understand that Octo will share your personal information with other IBM affiliates involved in your recruitment process, wherever these are located. More Information on how IBM protects your personal information, including the safeguards in case of cross-border data transfer, are available here: https://www.ibm.com/careers/us-en/privacy-policy/”.
  
  
 Requirements: 
  
  Designs and builds strategic solutions to move data from operational and external environments to the business intelligence environment using a variety of tools including Oracle,Informatica, Python, ArcGIS pro, ArcGIS Online, Tableau and PowerBI. 
  Designs and develops extract, transform and load (ETL) processes, and has experience with full lifecycle implementation of the technical components of a business intelligence solution. 
  Applies data governance policies and standards and incorporates these into data transformation logic in ETL processes to enforce data standards and quality rules. 
  Can drive conversations input to strategic program-level initiatives and roadmaps; 
  Experience ingesting large datasets so that they can accurately report findings to internal and external customers. 
  Experience creating complex data pipelines using Python, Python Jupyter notebook, Azure Data Factory and Azure Synapse Analytics. 
  Identifies database structural necessities by evaluating client operations, applications, and programming. 
  Possesses strong problem-solving and analytical skills. 
  Possesses excellent written and verbal communication skills, including presentation skills. 
  Proven ability to lead teams to collaboratively deliver work within time constraints. 
  Possesses effective organizational, teamwork, and interpersonal skills. 
  Parse the data in delimited files, XML and Json and process them into data warehouse and dashboards. 
  Interested in learning and has an ability to understand new technology concepts quickly and apply them accurately through an evolving, dynamic environment. 
  Actively participates in Agile ceremonies such as stand ups, backlog refinement, sprint planning and sprint review, retrospectives, and demos. 
  Has a passion and ability to make a difference. 
  
 Desired Skills: 
  
  Experience working with Oracle & Informatica 
  Experience in Cloud Architecture, Azure Cloud services, Azure Data Factory, Azure Synapse 
  Experience working with SQL Queries and Power Apps 
  Experience with Dashboard creation in ArcGIS, PowerBI, and Tableau 
  Experience with JIRAand Confluence 
  
 Years of Experience: 8+ years of experience in design, development, testing, and execution of ETL processes. 
  Education: Bachelor’s or Master’s in computer science, or a related field (preferred) 
  Location: Remote 
  Clearance: U.S. Citizenship required, ability to receive a DHS EOD","<div>
 <p>Octo, an IBM company, is an industry-leading, award-winning provider of technical solutions for the federal government. At Octo, we specialize in providing agile software engineering, user experience design, cloud services, and digital strategy services that address government&apos;s most pressing missions. Octo delivers intelligent solutions and rapid results, yielding lower costs and measurable outcomes.</p> 
 <p> Our team is what makes Octo great. At Octo you&apos;ll work beside some of the smartest and most accomplished staff you&apos;ll find in your career. Octo offers fantastic benefits and an amazing workplace culture where you will feel valued while you perform mission critical work for our government. Voted one of the region&#x2019;s best places to work multiple times, Octo is an employer of choice!</p> 
 <b>You&#x2026;</b>
 <br> 
 <b> </b>
 <br> As a 
 <b>Senior Data Engineer</b> at Octo, you will drive interactions with government stakeholders and teaming partners to gather requirements and propose analytic solutions.You will work with Octo business analysts and data analysts, as well as government stakeholders, to understand requirements and compile structured and unstructured data, ensuring its quality, accuracy, and reasonableness. You are comfortable leading technical teams comprised of individuals with varying skillsets and levels of experience. You have a passion for detail and are an excellent communicator. You are agile and curious and not afraid to identify what we are doing wrong so we can fix it, and what we are doing right so we can improve on it.
 <br> 
 <br> 
 <b>Us&#x2026;</b>
 <br> 
 <br> We were founded as a fresh alternative in the Government Consulting Community and are dedicated to the belief that results are a product of analytical thinking, agile design principles and that solutions are built in collaboration with, not for, our customers. This mantra drives us to succeed and act as true partners in advancing our client&#x2019;s missions.
 <br> 
 <br> 
 <b>Program Mission&#x2026;</b>
 <br> 
 <b> </b>
 <br> You will be working on a high-profile data analytics program for the Department of Homeland of Security (DHS) maintaining and transforming mission critical applications. This program is the reporting system of record for a wide variety of agency and component data, including financial, human capital, and government assets.In addition to geospatial tools, this program relies on business intelligence and data analytics tools.
 <br> 
 <p></p>
 <p>Octo is an Equal Opportunity/Affirmative Action employer. All qualified candidates will receive consideration for employment without regard to disability, protected veteran status, race, color, religious creed, national origin, citizenship, marital status, sex, sexual orientation/gender identity, age, or genetic information. Selected applicant will be subject to a background investigation.</p> 
 <p><i> Octo is an IBM subsidiary which has been acquired by IBM and will be integrated into the IBM organization. Octo will be the hiring entity. By proceeding with this application, you understand that Octo will share your personal information with other IBM affiliates involved in your recruitment process, wherever these are located. More Information on how IBM protects your personal information, including the safeguards in case of cross-border data transfer, are available here:</i> <i>https://www.ibm.com/careers/us-en/privacy-policy/</i><i>&#x201d;.</i></p>
 <br> 
 <p></p> 
 <p><b>Requirements:</b></p> 
 <ul> 
  <li>Designs and builds strategic solutions to move data from operational and external environments to the business intelligence environment using a variety of tools including Oracle,Informatica, Python, ArcGIS pro, ArcGIS Online, Tableau and PowerBI.</li> 
  <li>Designs and develops extract, transform and load (ETL) processes, and has experience with full lifecycle implementation of the technical components of a business intelligence solution.</li> 
  <li>Applies data governance policies and standards and incorporates these into data transformation logic in ETL processes to enforce data standards and quality rules.</li> 
  <li>Can drive conversations input to strategic program-level initiatives and roadmaps;</li> 
  <li>Experience ingesting large datasets so that they can accurately report findings to internal and external customers.</li> 
  <li>Experience creating complex data pipelines using Python, Python Jupyter notebook, Azure Data Factory and Azure Synapse Analytics.</li> 
  <li>Identifies database structural necessities by evaluating client operations, applications, and programming.</li> 
  <li>Possesses strong problem-solving and analytical skills.</li> 
  <li>Possesses excellent written and verbal communication skills, including presentation skills.</li> 
  <li>Proven ability to lead teams to collaboratively deliver work within time constraints.</li> 
  <li>Possesses effective organizational, teamwork, and interpersonal skills.</li> 
  <li>Parse the data in delimited files, XML and Json and process them into data warehouse and dashboards.</li> 
  <li>Interested in learning and has an ability to understand new technology concepts quickly and apply them accurately through an evolving, dynamic environment.</li> 
  <li>Actively participates in Agile ceremonies such as stand ups, backlog refinement, sprint planning and sprint review, retrospectives, and demos.</li> 
  <li>Has a passion and ability to make a difference.</li> 
 </ul> 
 <p><b>Desired Skills:</b></p> 
 <ul> 
  <li>Experience working with Oracle &amp; Informatica</li> 
  <li>Experience in Cloud Architecture, Azure Cloud services, Azure Data Factory, Azure Synapse</li> 
  <li>Experience working with SQL Queries and Power Apps</li> 
  <li>Experience with Dashboard creation in ArcGIS, PowerBI, and Tableau</li> 
  <li>Experience with JIRAand Confluence</li> 
 </ul> 
 <p><b>Years of Experience</b>: 8+ years of experience in design, development, testing, and execution of ETL processes.</p> 
 <p><b> Education</b>: Bachelor&#x2019;s or Master&#x2019;s in computer science, or a related field (preferred)</p> 
 <p><b> Location</b>: Remote</p> 
 <p><b> Clearance</b>: U.S. Citizenship required, ability to receive a DHS EOD</p>
</div>
<div></div>","https://recruit.hirebridge.com/v3/careercenter/v2/details.aspx?jid=617044&cid=6763&locvalue=1143","6ad6e6a2d776c4ca",,,,"Reston, VA 20191","Senior Data Engineer","1 day ago","2023-10-24T11:47:26.378Z","4","30",,"2023-10-25T11:47:26.380Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=6ad6e6a2d776c4ca&from=jasx&tk=1hdjagqsjjgbm800&vjs=3"
"ArchWell Health","ArchWell Health is a new, innovative healthcare provider devoted to improving the lives of our senior members. We deliver best-in-class care at comfortable, accessible neighborhood clinics where seniors can feel at home and become part of a vibrant, wellness-focused community. Our members experience greater continuity of care, as well as the comfort of knowing they will be treated with respect by people who genuinely care about them, their families, and their communities. 

 
Duties/Responsibilities:
 
 
 Build data integrations from internal and external sources to centralize data into a Data Warehouse environment. 
 Monitor data integration operations, data quality, troubleshoot, and resolve problems. 
 Profile data sources and map to target table formats. 
 Develop and monitor data quality processes and address problems. 
 Develop, unit test and system test integration components. 
 Create support documentation describing the functionality of the integrations. 
 Participating in technical design & requirements gathering meetings. 
 Participate in planning and implementing data integration and data migration activities. 
 Perform QA tests to ensure data integrity and quality. 
 Research data issues between source systems and the data warehouse. 
 
Required Skills/Experience:
 
 
 Bachelor’s degree required; Master's degree (in data science, computer science or MIS, mathematics, engineering, or related field) preferred. 
 5+ years of prior experience in Data Management / ETL / ELT / Data Warehousing 
 Experience in writing Data Quality routines for cleansing of data and capturing confidence score 
 Experience with master data management 
 Strong knowledge of Structured Query Language (SQL) and Transact-SQL (T-SQL) 
 Experience using scripting languages such as JavaScript or Python 
 Experience Healthcare data models, datasets, and source systems (e.g. EHR, claims, labs, etc.) 
 Experience with healthcare reference data (ICD, CPT etc.) 
 Experience with agile delivery methodologies 
 Data Modeling experience preferred. 
 Strong organizational, administrative, and analytical skills required. 
 Experience managing and working in cloud environments such as Amazon Web Services or Azure 
 Knowledge of HIPAA; ability to implement systems and processes in accordance with regulations 
 Excellent interpersonal communication skills, both written and verbal 
 ArchWell Health is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to their race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran status, or any other protected classification.","ArchWell Health is a new, innovative healthcare provider devoted to improving the lives of our senior members. We deliver best-in-class care at comfortable, accessible neighborhood clinics where seniors can feel at home and become part of a vibrant, wellness-focused community. Our members experience greater continuity of care, as well as the comfort of knowing they will be treated with respect by people who genuinely care about them, their families, and their communities. 
<br>
<br> 
<b>Duties/Responsibilities:</b>
<br> 
<ul> 
 <li>Build data integrations from internal and external sources to centralize data into a Data Warehouse environment.</li> 
 <li>Monitor data integration operations, data quality, troubleshoot, and resolve problems.</li> 
 <li>Profile data sources and map to target table formats.</li> 
 <li>Develop and monitor data quality processes and address problems.</li> 
 <li>Develop, unit test and system test integration components.</li> 
 <li>Create support documentation describing the functionality of the integrations.</li> 
 <li>Participating in technical design &amp; requirements gathering meetings.</li> 
 <li>Participate in planning and implementing data integration and data migration activities.</li> 
 <li>Perform QA tests to ensure data integrity and quality.</li> 
 <li>Research data issues between source systems and the data warehouse.</li> 
</ul> 
<b>Required Skills/Experience:</b>
<br> 
<ul> 
 <li>Bachelor&#x2019;s degree required; Master&apos;s degree (in data science, computer science or MIS, mathematics, engineering, or related field) preferred.</li> 
 <li>5+ years of prior experience in Data Management / ETL / ELT / Data Warehousing</li> 
 <li>Experience in writing Data Quality routines for cleansing of data and capturing confidence score</li> 
 <li>Experience with master data management</li> 
 <li>Strong knowledge of Structured Query Language (SQL) and Transact-SQL (T-SQL)</li> 
 <li>Experience using scripting languages such as JavaScript or Python</li> 
 <li>Experience Healthcare data models, datasets, and source systems (e.g. EHR, claims, labs, etc.)</li> 
 <li>Experience with healthcare reference data (ICD, CPT etc.)</li> 
 <li>Experience with agile delivery methodologies</li> 
 <li>Data Modeling experience preferred.</li> 
 <li>Strong organizational, administrative, and analytical skills required.</li> 
 <li>Experience managing and working in cloud environments such as Amazon Web Services or Azure</li> 
 <li>Knowledge of HIPAA; ability to implement systems and processes in accordance with regulations</li> 
 <li>Excellent interpersonal communication skills, both written and verbal</li> 
</ul> ArchWell Health is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to their race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran status, or any other protected classification.","https://us232.dayforcehcm.com/CandidatePortal/en-US/archwellhealth/Posting/View/6192?source=Indeed","5c4370f776c5467f",,"Full-time",,"Remote","Data Engineer","1 day ago","2023-10-24T11:47:34.323Z","3.4","19",,"2023-10-25T11:47:34.325Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=5c4370f776c5467f&from=jasx&tk=1hdjagqsjjgbm800&vjs=3"
"Sev1Tech","Overview/ Job Responsibilities: 
 
   We are seeking a highly experienced and skilled Senior Data Lake Engineer to join our team. As the Senior Data Lake Engineer, you will play a critical role in establishing and configuring an enterprise-level Databricks solution to support our federal customer organization's data lake initiatives. This position offers a unique opportunity to work with cutting-edge technologies and shape the future of our federal customer's data infrastructure.
 
 
 
   This position requires onsite presence at the customer location (Arlington, VA) one day per week.
 
 
 
   If you are a highly skilled and experienced Senior Data Lake Engineer with expertise in Databricks and passion for building scalable and secure data lake solutions, we would like to hear from you.
 
 
 
   Responsibilities:
 
 
   Lead the design, implementation, and configuration of an enterprise Data Lake solution utilizing Databricks, ensuring scalability, reliability, and optimal performance.
   Collaborate with cross-functional teams to gather requirements, understand data integration needs, and define data lake architecture and governance policies.
   Establish and configure Databricks workspaces, clusters, and storage components, optimizing the solution for efficient data processing, query performance, and data governance.
   Design and implement data ingestion pipelines to efficiently extract, transform, and load data from various sources into the data lake using Databricks tools and services.
   Develop and maintain data lake security frameworks, including access controls, encryption solutions, and data masking techniques to protect sensitive data.
   Collaborate with data engineers and data scientists to optimize data pipelines, develop data transformations, and ensure data quality and integrity.
   Monitor and tune Databricks clusters and workloads to ensure performance, reliability, and cost optimization, utilizing automated scaling and resource management techniques.
   Implement best practices for data governance, data cataloging, metadata management, and data lineage within Databricks, adhering to regulatory and compliance requirements.
   Collaborate with infrastructure teams to ensure data lake infrastructure meets scalability and availability requirements, leveraging Databricks cluster management and AWS/Azure services.
   Develop and maintain documentation and guidelines related to the Databricks solution, including architecture diagrams, standards, and processes.
   Stay up to date with the latest advancements in Databricks, big data technologies, and cloud platforms, continuously evaluating and implementing new features and capabilities.
   Provide technical guidance and mentorship to junior data engineers, promoting best practices and fostering a culture of continuous learning and growth.
   Collaborate with stakeholders to understand their data analytics and reporting needs and develop scalable data models and data transformation processes to support these requirements.
   Support data lake-related incident resolutions, troubleshooting data quality issues, performance bottlenecks, and other data-related challenges.
   Collaborate with data governance and compliance teams to ensure data privacy, security, and compliance guidelines are adhered to within the data lake solution.
   Participate in the evaluation and selection of new tools, technologies, and services to enhance the data lake infrastructure.
  Minimum Qualifications: 
 
  Bachelor's degree in computer science, information technology, or a related field. Equivalent experience will also be considered.
   Proven experience in building and configuring enterprise-level data lake solutions using Databricks in an AWS or Azure environment.
   In-depth knowledge of Databricks architecture, including workspaces, clusters, storage, notebook development, and automation capabilities.
   Strong expertise in designing and implementing data ingestion pipelines, data transformations, and data quality processes using Databricks.
   Experience with big data technologies such as Apache Spark, Apache Hive, Delta Lake, and Hadoop.
   Solid understanding of data governance principles, data modeling, data cataloging, and metadata management.
   Hands-on experience with cloud platforms like AWS or Azure, including relevant services like S3, EMR, Glue, Data Factory, etc.
   Proficiency in SQL and one or more programming languages (Python, Scala, or Java) for data manipulation and transformation.
   Knowledge of data security and privacy best practices, including data access controls, encryption, and data masking techniques.
   Strong problem-solving and analytical skills, with the ability to identify and resolve complex data-related issues.
   Excellent interpersonal and communication skills, with the ability to collaborate effectively with technical and non-technical stakeholders.
   Experience in a senior or lead role, providing technical guidance and mentorship to junior team members.
   Relevant certifications such as Databricks Certified Developer or Databricks Certified Professional are highly desirable.
   Eligibility/Clearance Requirements: Must be able to provide proof of U.S. Citizenship.
  Desired Qualifications: 
  Clearance Preference:
 
   Active DHS/CISA suitability - 1st priority
   Any DHS badge + DoD Top Secret - 2nd choice
   DoD Top Secret + willingness to obtain DHS/CISA suitability - 3rd choice (it can take 10-60 days to obtain suitability – work can only begin once suitability is fully adjudicated).
  About Sev1Tech LLC: 
  Founded in 2010, Sev1Tech provides IT, engineering, and program management solutions delivery. Sev1Tech focuses on providing program and IT support services to critical missions across Federal and Commercial Clients. Our Mission is to Build better companies. Enable better government. Protect our nation. Build better humans across the country.
 
   Join the Sev1Tech family where you can achieve great accomplishments while fostering a satisfying and rewarding career progression. Please apply directly through the website at: https://careers-sev1tech.icims.com/#joinSev1tech
   For any additional questions or to submit any referrals, please contact: Caitlin.maupin@sev1tech.com
   Sev1Tech is an Equal Opportunity and Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.","<div>
 Overview/ Job Responsibilities: 
 <div>
   We are seeking a highly experienced and skilled Senior Data Lake Engineer to join our team. As the Senior Data Lake Engineer, you will play a critical role in establishing and configuring an enterprise-level Databricks solution to support our federal customer organization&apos;s data lake initiatives. This position offers a unique opportunity to work with cutting-edge technologies and shape the future of our federal customer&apos;s data infrastructure.
 </div>
 <div></div>
 <div>
  <br> This position requires onsite presence at the customer location (Arlington, VA) one day per week.
 </div>
 <div></div>
 <div>
  <br> If you are a highly skilled and experienced Senior Data Lake Engineer with expertise in Databricks and passion for building scalable and secure data lake solutions, we would like to hear from you.
 </div>
 <div></div>
 <div>
  <b><br> Responsibilities:</b>
 </div>
 <ul>
  <li> Lead the design, implementation, and configuration of an enterprise Data Lake solution utilizing Databricks, ensuring scalability, reliability, and optimal performance.</li>
  <li> Collaborate with cross-functional teams to gather requirements, understand data integration needs, and define data lake architecture and governance policies.</li>
  <li> Establish and configure Databricks workspaces, clusters, and storage components, optimizing the solution for efficient data processing, query performance, and data governance.</li>
  <li> Design and implement data ingestion pipelines to efficiently extract, transform, and load data from various sources into the data lake using Databricks tools and services.</li>
  <li> Develop and maintain data lake security frameworks, including access controls, encryption solutions, and data masking techniques to protect sensitive data.</li>
  <li> Collaborate with data engineers and data scientists to optimize data pipelines, develop data transformations, and ensure data quality and integrity.</li>
  <li> Monitor and tune Databricks clusters and workloads to ensure performance, reliability, and cost optimization, utilizing automated scaling and resource management techniques.</li>
  <li> Implement best practices for data governance, data cataloging, metadata management, and data lineage within Databricks, adhering to regulatory and compliance requirements.</li>
  <li> Collaborate with infrastructure teams to ensure data lake infrastructure meets scalability and availability requirements, leveraging Databricks cluster management and AWS/Azure services.</li>
  <li> Develop and maintain documentation and guidelines related to the Databricks solution, including architecture diagrams, standards, and processes.</li>
  <li> Stay up to date with the latest advancements in Databricks, big data technologies, and cloud platforms, continuously evaluating and implementing new features and capabilities.</li>
  <li> Provide technical guidance and mentorship to junior data engineers, promoting best practices and fostering a culture of continuous learning and growth.</li>
  <li> Collaborate with stakeholders to understand their data analytics and reporting needs and develop scalable data models and data transformation processes to support these requirements.</li>
  <li> Support data lake-related incident resolutions, troubleshooting data quality issues, performance bottlenecks, and other data-related challenges.</li>
  <li> Collaborate with data governance and compliance teams to ensure data privacy, security, and compliance guidelines are adhered to within the data lake solution.</li>
  <li> Participate in the evaluation and selection of new tools, technologies, and services to enhance the data lake infrastructure.</li>
 </ul> Minimum Qualifications: 
 <ul>
  <li>Bachelor&apos;s degree in computer science, information technology, or a related field. Equivalent experience will also be considered.</li>
  <li> Proven experience in building and configuring enterprise-level data lake solutions using Databricks in an AWS or Azure environment.</li>
  <li> In-depth knowledge of Databricks architecture, including workspaces, clusters, storage, notebook development, and automation capabilities.</li>
  <li> Strong expertise in designing and implementing data ingestion pipelines, data transformations, and data quality processes using Databricks.</li>
  <li> Experience with big data technologies such as Apache Spark, Apache Hive, Delta Lake, and Hadoop.</li>
  <li> Solid understanding of data governance principles, data modeling, data cataloging, and metadata management.</li>
  <li> Hands-on experience with cloud platforms like AWS or Azure, including relevant services like S3, EMR, Glue, Data Factory, etc.</li>
  <li> Proficiency in SQL and one or more programming languages (Python, Scala, or Java) for data manipulation and transformation.</li>
  <li> Knowledge of data security and privacy best practices, including data access controls, encryption, and data masking techniques.</li>
  <li> Strong problem-solving and analytical skills, with the ability to identify and resolve complex data-related issues.</li>
  <li> Excellent interpersonal and communication skills, with the ability to collaborate effectively with technical and non-technical stakeholders.</li>
  <li> Experience in a senior or lead role, providing technical guidance and mentorship to junior team members.</li>
  <li> Relevant certifications such as Databricks Certified Developer or Databricks Certified Professional are highly desirable.</li>
  <li><b> Eligibility/Clearance Requirements</b>: Must be able to provide proof of U.S. Citizenship.</li>
 </ul> Desired Qualifications: 
 <p><b> Clearance Preference</b>:</p>
 <ul>
  <li> Active DHS/CISA suitability - 1st priority</li>
  <li> Any DHS badge + DoD Top Secret - 2nd choice</li>
  <li> DoD Top Secret + willingness to obtain DHS/CISA suitability - 3rd choice (it can take 10-60 days to obtain suitability &#x2013; work can only begin once suitability is fully adjudicated).</li>
 </ul> About Sev1Tech LLC: 
 <p> Founded in 2010, Sev1Tech provides IT, engineering, and program management solutions delivery. Sev1Tech focuses on providing program and IT support services to critical missions across Federal and Commercial Clients. Our Mission is to Build better companies. Enable better government. Protect our nation. Build better humans across the country.</p>
 <ul>
  <li> Join the Sev1Tech family where you can achieve great accomplishments while fostering a satisfying and rewarding career progression. Please apply directly through the website at: https://careers-sev1tech.icims.com/#joinSev1tech</li>
  <li> For any additional questions or to submit any referrals, please contact: Caitlin.maupin@sev1tech.com</li>
  <li> Sev1Tech is an Equal Opportunity and Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.</li>
 </ul>
</div>","https://careers-sev1tech.icims.com/jobs/7593/job?utm_source=indeed_integration&iis=Job%20Board&iisn=Indeed&indeed-apply-token=73a2d2b2a8d6d5c0a62696875eaebd669103652d3f0c2cd5445d3e66b1592b0f","fa48b110ceeae5b9",,"Full-time",,"Rosslyn, VA","Senior Data Lake Engineer - Databricks","1 day ago","2023-10-24T11:47:36.959Z",,,,"2023-10-25T11:47:36.961Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=fa48b110ceeae5b9&from=jasx&tk=1hdjagqsjjgbm800&vjs=3"
"NTT DATA","Company Overview: Req ID: 258248 NTT DATA Services strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now. NTT DATA's Client is currently seeking a Data Engineer to join their team in Pittsburgh, Pennsylvania (US-PA), United States (US). REMOTE!!
  
  Job Description: Responsibilities
  
 
  Lead, design, develop, deploy, and maintain mission critical data applications for Enterprise data platform
  Responsible for delivery of various data driven applications by leading several consultants.
  Technical liaison to one or two Agile delivery teams.
  Participate in all phases of the Enterprise Data platform development life cycle as appropriate; including, but not limited to gathering customer requirements, defining technical requirements, creating high level architecture diagrams, data validation and training sessions
  Engage in Data solutions and Business Intelligence Projects and drive them to closure
  Lead different aspects of data analytics, data quality, machine learning, data acquisition, visualization, and some design and analysis tasks
  Coordinate & work closely with architecture and data operations teams
  For Agile Projects, collaborate with Product Owner on epic and user story definitions
  Develop documentation and training materials, participate with customer groups in the planning for longer-term systems enhancements. Experience
  Total 10+ years of experience in the IT, Leading and developing BI and DW applications.
  5+ Years of Experience in Data Lake, Data Analytics & Business Intelligence Solutions
  Strong Experience in ETL Tools preferably Informatica, Databricks & AWS Glue
  Excellent experience in Data Lake using AWS Databricks, Apache Spark & Python
  2+ years of working experience in a DevOps environment, data integration and pipeline development.
  2+ years of Experience with AWS Cloud on data integration with Apache Spark, EMR, Glue, Kafka, Kinesis, and Lambda in S3, Redshift, RDS, MongoDB/DynamoDB ecosystems
  Demonstrated skill and ability in the development of data warehouse projects/applications (Oracle & SQL Server)
  Strong real-life experience in python development especially in pySpark in AWS Cloud environment.
  Experience in Python and common python libraries.
  Strong analytical experience with database in writing complex queries, query optimization, debugging, user defined functions, views, indexes etc.
  Experience with source control systems such as GitHub, Bit bucket, and Jenkins build and continuous integration tools.
  Knowledge of extract development against ERPs - SAP, Siebel, JDE, BAAN preferred
  Strong understanding of AWS Data lake and data bricks.
  Experience in SAP ERP application, data and processes desired
  Exposure to AWS Data Lake, AWS Lambda, AWS S3, Kafka, Redshift, Sage Maker would be added advantage
  Experience in Supply Chain, supplier invoice, purchase order is a plus. Skills and Abilities
  Full life cycle implementation experience in AWS using Pyspark/EMR, Athena, S3, Redshift, AWS API Gateway, Lambda, Glue and other managed services
  Experience with agile development methodologies by following DevOps, Data Ops and Dev Sec Ops practices.
  Manage life cycle of ETL Pipelines and other cloud platform tools, including GitHub, Jenkins, Terraform, Jira, and Confluence.
  Excellent written, verbal and inter-personal and stakeholder communication skills.
  Ability to analyze trends associated with huge datasets.
  Ability to work with cross functional teams from multiple regions/ time zones by effectively leveraging multi-form communication (Email, MS Teams for voice and chat, meetings)
  Excellent prioritization and problem-solving skills.
  Ability to work independently and as a member of a cross-functional team
  Good administration and time capabilities to deal with multiple projects and prioritize effectively.
  Willingness to learn, be mentored and improve
  Ability to effectively interpret data and translate into information and be able to effectively communicate the information both verbally or visually.
  Ability to multi-task and apply initiative and creativity on challenging projects.
  Strong problems solving and troubleshooting skills. Ability to transform a complex problem into smaller, manageable problems
 
  About NTT DATA Services:
  
  NTT DATA Services is a recognized leader in IT and business services, including cloud, data and applications, headquartered in Texas. As part of NTT DATA, a $30 billion trusted global innovator with a combined global reach of over 80 countries, we help clients transform through business and technology consulting, industry and digital solutions, applications development and management, managed edge-to-cloud infrastructure services, BPO, systems integration and global data centers. We are committed to our clients' long-term success. Visit nttdata.com or LinkedIn to learn more.
  
  NTT DATA Services is an equal opportunity employer and considers all applicants without regarding to race, color, religion, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other characteristic protected by law. We are committed to creating a diverse and inclusive environment for all employees. If you need assistance or an accommodation due to a disability, please inform your recruiter so that we may connect you with the appropriate team.
  
  Where required by law, NTT DATA provides a reasonable range of compensation for specific roles. The starting hourly range for this remote role is 
 ($50/hour TO $65/hour ). This range reflects the minimum and maximum target compensation for the position across all US locations. Actual compensation will depend on several factors, including the candidate's actual work location, relevant experience, technical skills, and other qualifications. This position may also be eligible for incentive compensation based on individual and/or company performance. This position is eligible for company benefits that will depend on the nature of the role offered. Company benefits may include medical, dental, and vision insurance, flexible spending or health savings account, life, and AD&D insurance, short-and long-term disability coverage, paid time off, employee assistance, participation in a 401k program with company match, and additional voluntary or legally required benefits.","<div>
 Company Overview: Req ID: 258248 NTT DATA Services strives to hire exceptional, innovative and passionate individuals who want to grow with us. If you want to be part of an inclusive, adaptable, and forward-thinking organization, apply now. NTT DATA&apos;s Client is currently seeking a Data Engineer to join their team in Pittsburgh, Pennsylvania (US-PA), United States (US). REMOTE!!
 <br> 
 <br> Job Description: Responsibilities
 <br> 
 <ul>
  <li>Lead, design, develop, deploy, and maintain mission critical data applications for Enterprise data platform</li>
  <li>Responsible for delivery of various data driven applications by leading several consultants.</li>
  <li>Technical liaison to one or two Agile delivery teams.</li>
  <li>Participate in all phases of the Enterprise Data platform development life cycle as appropriate; including, but not limited to gathering customer requirements, defining technical requirements, creating high level architecture diagrams, data validation and training sessions</li>
  <li>Engage in Data solutions and Business Intelligence Projects and drive them to closure</li>
  <li>Lead different aspects of data analytics, data quality, machine learning, data acquisition, visualization, and some design and analysis tasks</li>
  <li>Coordinate &amp; work closely with architecture and data operations teams</li>
  <li>For Agile Projects, collaborate with Product Owner on epic and user story definitions</li>
  <li>Develop documentation and training materials, participate with customer groups in the planning for longer-term systems enhancements. Experience</li>
  <li>Total 10+ years of experience in the IT, Leading and developing BI and DW applications.</li>
  <li>5+ Years of Experience in Data Lake, Data Analytics &amp; Business Intelligence Solutions</li>
  <li>Strong Experience in ETL Tools preferably Informatica, Databricks &amp; AWS Glue</li>
  <li>Excellent experience in Data Lake using AWS Databricks, Apache Spark &amp; Python</li>
  <li>2+ years of working experience in a DevOps environment, data integration and pipeline development.</li>
  <li>2+ years of Experience with AWS Cloud on data integration with Apache Spark, EMR, Glue, Kafka, Kinesis, and Lambda in S3, Redshift, RDS, MongoDB/DynamoDB ecosystems</li>
  <li>Demonstrated skill and ability in the development of data warehouse projects/applications (Oracle &amp; SQL Server)</li>
  <li>Strong real-life experience in python development especially in pySpark in AWS Cloud environment.</li>
  <li>Experience in Python and common python libraries.</li>
  <li>Strong analytical experience with database in writing complex queries, query optimization, debugging, user defined functions, views, indexes etc.</li>
  <li>Experience with source control systems such as GitHub, Bit bucket, and Jenkins build and continuous integration tools.</li>
  <li>Knowledge of extract development against ERPs - SAP, Siebel, JDE, BAAN preferred</li>
  <li>Strong understanding of AWS Data lake and data bricks.</li>
  <li>Experience in SAP ERP application, data and processes desired</li>
  <li>Exposure to AWS Data Lake, AWS Lambda, AWS S3, Kafka, Redshift, Sage Maker would be added advantage</li>
  <li>Experience in Supply Chain, supplier invoice, purchase order is a plus. Skills and Abilities</li>
  <li>Full life cycle implementation experience in AWS using Pyspark/EMR, Athena, S3, Redshift, AWS API Gateway, Lambda, Glue and other managed services</li>
  <li>Experience with agile development methodologies by following DevOps, Data Ops and Dev Sec Ops practices.</li>
  <li>Manage life cycle of ETL Pipelines and other cloud platform tools, including GitHub, Jenkins, Terraform, Jira, and Confluence.</li>
  <li>Excellent written, verbal and inter-personal and stakeholder communication skills.</li>
  <li>Ability to analyze trends associated with huge datasets.</li>
  <li>Ability to work with cross functional teams from multiple regions/ time zones by effectively leveraging multi-form communication (Email, MS Teams for voice and chat, meetings)</li>
  <li>Excellent prioritization and problem-solving skills.</li>
  <li>Ability to work independently and as a member of a cross-functional team</li>
  <li>Good administration and time capabilities to deal with multiple projects and prioritize effectively.</li>
  <li>Willingness to learn, be mentored and improve</li>
  <li>Ability to effectively interpret data and translate into information and be able to effectively communicate the information both verbally or visually.</li>
  <li>Ability to multi-task and apply initiative and creativity on challenging projects.</li>
  <li>Strong problems solving and troubleshooting skills. Ability to transform a complex problem into smaller, manageable problems</li>
 </ul>
 <br> About NTT DATA Services:
 <br> 
 <br> NTT DATA Services is a recognized leader in IT and business services, including cloud, data and applications, headquartered in Texas. As part of NTT DATA, a &#x24;30 billion trusted global innovator with a combined global reach of over 80 countries, we help clients transform through business and technology consulting, industry and digital solutions, applications development and management, managed edge-to-cloud infrastructure services, BPO, systems integration and global data centers. We are committed to our clients&apos; long-term success. Visit nttdata.com or LinkedIn to learn more.
 <br> 
 <br> NTT DATA Services is an equal opportunity employer and considers all applicants without regarding to race, color, religion, citizenship, national origin, ancestry, age, sex, sexual orientation, gender identity, genetic information, physical or mental disability, veteran or marital status, or any other characteristic protected by law. We are committed to creating a diverse and inclusive environment for all employees. If you need assistance or an accommodation due to a disability, please inform your recruiter so that we may connect you with the appropriate team.
 <br> 
 <br> Where required by law, NTT DATA provides a reasonable range of compensation for specific roles. The starting hourly range for this remote role is 
 <b>(&#x24;50/hour TO &#x24;65/hour ).</b> This range reflects the minimum and maximum target compensation for the position across all US locations. Actual compensation will depend on several factors, including the candidate&apos;s actual work location, relevant experience, technical skills, and other qualifications. This position may also be eligible for incentive compensation based on individual and/or company performance. This position is eligible for company benefits that will depend on the nature of the role offered. Company benefits may include medical, dental, and vision insurance, flexible spending or health savings account, life, and AD&amp;D insurance, short-and long-term disability coverage, paid time off, employee assistance, participation in a 401k program with company match, and additional voluntary or legally required benefits.
</div>","https://click.appcast.io/track/hsufn0u-org?cs=fzm&jg=2guz&bid=lUf2CslKyPxm6i440ZgUYA==&jobPipeline=Indeed&ittk=RH0E8INKOM","aeeb71d0cc5d40c9",,,,"Dallas, TX","Data Engineer","1 day ago","2023-10-24T11:47:36.692Z","3.5","3978","$50 an hour","2023-10-25T11:47:36.694Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=aeeb71d0cc5d40c9&from=jasx&tk=1hdjagqsjjgbm800&vjs=3"
"Optum","Optum is a global organization that delivers care, aided by technology to help millions of people live healthier lives. The work you do with our team will directly improve health outcomes by connecting people with the care, pharmacy benefits, data and resources they need to feel their best. Here, you will find a culture guided by diversity and inclusion, talented peers, comprehensive benefits and career development opportunities. Come make an impact on the communities we serve as you help us advance health equity on a global scale. Join us to start Caring. Connecting. Growing together.
  
  
 Functions may include database architecture, engineering, design, optimization, security, and administration, data modeling, big data development, Extract, Transform, and Load (ETL) development, storage engineering, data warehousing, data provisioning, and other similar roles.
  
  
  Responsibilities may include Platform-as-a-Service and Cloud solutions with a focus on data stores and associated ecosystems. Duties may include: 
  
  Management of design services 
  Providing sizing and configuration assistance 
  Ensuring strict data quality 
  Performing needs assessments
 
  
  
  Position in this function analyzes current business practices, processes, and procedures and identifies future business opportunities for leveraging data storage and retrieval system capabilities. Manages relationships with software and hardware vendors to understand the potential architectural impact of different vendor strategies and data acquisition. May design schemas: write SQL or other data markup scripting and helps to support the development of Analytics and Applications that build on top of data. Selects, develops, and evaluates personnel to ensure the efficient operation of the function.
  
  
  You’ll enjoy the flexibility to work remotely * from anywhere within the U.S. as you take on some tough challenges.
  
  
 Primary Responsibilities: 
  
  Developing ETL solutions and supporting data strategy leveraging acquired subject matter expertise to identify/inform opportunities and risks 
  Creating physical and logical data models supporting new data warehouse development and architecture 
  Developing/leading complex dashboards, scorecards, reports, and data analysis 
  Performing peer-reviews and providing technical support/guidance to the project team 
  Leading the creating/maintaining of Teradata database, ETL, reporting architecture/environment, processes/procedures, coding standards, and data management/security activities 
  Supporting/Standing up the enterprise data environment for disaster situations or annual disaster recovery drill 
  Supporting the project manager and leadership in project planning and executing activities, including presentations, risks/issues, and status reporting 
  Demonstrating deliverables to the team or the customers for approvals and gain buy-in to implement solutions or adapt to new products/services/programs 
  Working with the DBAs, data modelers, and application/system admins to support maintaining the Teradata and application server hardware and software (including patches, fixes, and upgrades) and leading industry level standards or best practices
 
  
  
 
   You’ll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in.
 
  Required Qualifications: 
  
  Bachelor’s degree or higher in engineering, related field, or equivalent experience 
  7+ years of ETL and enterprise data warehouse technical development  
  7+ years of experience gathering requirements, exploring data, SQL/DB development 
  7+ years of experience in reporting, analysis, and/or data management/governance 
  4+ years of Informatica PowerCenter 
  Demonstrated experience working with high-performing remote teams and delivering to external customer 
  Advanced skills in Microsoft Office (Excel, Word, PowerPoint, etc.) with solid communication skills
 
  
  
  Preferred Qualifications: 
  
  MBA or advanced degree 
  Certified Business Intelligence Professional (CBIP) 
  4+ years of experience leading a team as a technical lead or similar capacity 
  Experience in Medicaid/Managed Care/health insurance industry 
  Experience with visualization tools (Power B.I., Tableau, SAS, etc.) 
  Experience using Snowflake 
  Experience in scripting language experience (Python, R, SAS, T-SQL, etc.) 
  Familiarity with A.I. and machine learning
 
  
  
 
  All employees working remotely will be required to adhere to UnitedHealth Group’s Telecommuter Policy
 
  
  
 California, Colorado, Connecticut, Nevada, New Jersey, New York, Rhode Island, or Washington Residents Only: The salary range for California/Colorado/Connecticut/Nevada/New Jersey/New York/Rhode Island/Washington residents is $85,000 to $167,300. Pay is based on several factors including but not limited to education, work experience, certifications, etc. In addition to your salary, UnitedHealth Group offers benefits such as, a comprehensive benefits package, incentive and recognition programs, equity stock purchase and 401k contribution (all benefits are subject to eligibility requirements). No matter where or when you begin a career with UnitedHealth Group, you’ll find a far-reaching choice of benefits and incentives.
  
  
  At UnitedHealth Group, our mission is to help people live healthier lives and make the health system work better for everyone. We believe everyone–of every race, gender, sexuality, age, location and income–deserves the opportunity to live their healthiest life. Today, however, there are still far too many barriers to good health which are disproportionately experienced by people of color, historically marginalized groups and those with lower incomes. We are committed to mitigating our impact on the environment and enabling and delivering equitable care that addresses health disparities and improves health outcomes — an enterprise priority reflected in our mission.
  
  
  
 Diversity creates a healthier atmosphere: UnitedHealth Group is an Equal Employment Opportunity/Affirmative Action employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law.
  
  
 
   UnitedHealth Group is a drug - free workplace. Candidates are required to pass a drug test before beginning employment.","<div>
 <p>Optum is a global organization that delivers care, aided by technology to help millions of people live healthier lives. The work you do with our team will directly improve health outcomes by connecting people with the care, pharmacy benefits, data and resources they need to feel their best. Here, you will find a culture guided by diversity and inclusion, talented peers, comprehensive benefits and career development opportunities. Come make an impact on the communities we serve as you help us advance health equity on a global scale. Join us to start <b>Caring. Connecting. Growing together.</b></p>
 <br> 
 <p> </p>
 <p>Functions may include database architecture, engineering, design, optimization, security, and administration, data modeling, big data development, Extract, Transform, and Load (ETL) development, storage engineering, data warehousing, data provisioning, and other similar roles.</p>
 <br> 
 <p></p> 
 <p> Responsibilities may include Platform-as-a-Service and Cloud solutions with a focus on data stores and associated ecosystems. Duties may include:</p> 
 <ul> 
  <li>Management of design services</li> 
  <li>Providing sizing and configuration assistance</li> 
  <li>Ensuring strict data quality</li> 
  <li>Performing needs assessments</li>
 </ul>
 <br> 
 <p></p> 
 <p> Position in this function analyzes current business practices, processes, and procedures and identifies future business opportunities for leveraging data storage and retrieval system capabilities. Manages relationships with software and hardware vendors to understand the potential architectural impact of different vendor strategies and data acquisition. May design schemas: write SQL or other data markup scripting and helps to support the development of Analytics and Applications that build on top of data. Selects, develops, and evaluates personnel to ensure the efficient operation of the function.</p>
 <br> 
 <p></p> 
 <p> You&#x2019;ll enjoy the flexibility to work remotely * from anywhere within the U.S. as you take on some tough challenges.</p>
 <br> 
 <p> </p>
 <p><b>Primary Responsibilities:</b></p> 
 <ul> 
  <li>Developing ETL solutions and supporting data strategy leveraging acquired subject matter expertise to identify/inform opportunities and risks</li> 
  <li>Creating physical and logical data models supporting new data warehouse development and architecture</li> 
  <li>Developing/leading complex dashboards, scorecards, reports, and data analysis</li> 
  <li>Performing peer-reviews and providing technical support/guidance to the project team</li> 
  <li>Leading the creating/maintaining of Teradata database, ETL, reporting architecture/environment, processes/procedures, coding standards, and data management/security activities</li> 
  <li>Supporting/Standing up the enterprise data environment for disaster situations or annual disaster recovery drill</li> 
  <li>Supporting the project manager and leadership in project planning and executing activities, including presentations, risks/issues, and status reporting</li> 
  <li>Demonstrating deliverables to the team or the customers for approvals and gain buy-in to implement solutions or adapt to new products/services/programs</li> 
  <li>Working with the DBAs, data modelers, and application/system admins to support maintaining the Teradata and application server hardware and software (including patches, fixes, and upgrades) and leading industry level standards or best practices</li>
 </ul>
 <br> 
 <p></p> 
 <div>
   You&#x2019;ll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in.
 </div>
 <p><b> Required Qualifications:</b></p> 
 <ul> 
  <li>Bachelor&#x2019;s degree or higher in engineering, related field, or equivalent experience</li> 
  <li>7+ years of ETL and enterprise data warehouse technical development<br> </li> 
  <li>7+ years of experience gathering requirements, exploring data, SQL/DB development</li> 
  <li>7+ years of experience in reporting, analysis, and/or data management/governance</li> 
  <li>4+ years of Informatica PowerCenter</li> 
  <li>Demonstrated experience working with high-performing remote teams and delivering to external customer</li> 
  <li>Advanced skills in Microsoft Office (Excel, Word, PowerPoint, etc.) with solid communication skills</li>
 </ul>
 <br> 
 <p></p> 
 <p><b> Preferred Qualifications:</b></p> 
 <ul> 
  <li>MBA or advanced degree</li> 
  <li>Certified Business Intelligence Professional (CBIP)</li> 
  <li>4+ years of experience leading a team as a technical lead or similar capacity</li> 
  <li>Experience in Medicaid/Managed Care/health insurance industry</li> 
  <li>Experience with visualization tools (Power B.I., Tableau, SAS, etc.)</li> 
  <li>Experience using Snowflake</li> 
  <li>Experience in scripting language experience (Python, R, SAS, T-SQL, etc.)</li> 
  <li>Familiarity with A.I. and machine learning</li>
 </ul>
 <br> 
 <p></p> 
 <ul>
  <li>All employees working remotely will be required to adhere to UnitedHealth Group&#x2019;s Telecommuter Policy</li>
 </ul>
 <br> 
 <p> </p>
 <p><b>California, Colorado, Connecticut, Nevada, New Jersey, New York, Rhode Island, or Washington Residents Only: </b>The salary range for California/Colorado/Connecticut/Nevada/New Jersey/New York/Rhode Island/Washington residents is &#x24;85,000 to &#x24;167,300. Pay is based on several factors including but not limited to education, work experience, certifications, etc. In addition to your salary, UnitedHealth Group offers benefits such as, a comprehensive benefits package, incentive and recognition programs, equity stock purchase and 401k contribution (all benefits are subject to eligibility requirements). No matter where or when you begin a career with UnitedHealth Group, you&#x2019;ll find a far-reaching choice of benefits and incentives.</p>
 <br> 
 <p></p> 
 <p><i> At UnitedHealth Group, our mission is to help people live healthier lives and make the health system work better for everyone. We believe everyone&#x2013;of every race, gender, sexuality, age, location and income&#x2013;deserves the opportunity to live their healthiest life. Today, however, there are still far too many barriers to good health which are disproportionately experienced by people of color, historically marginalized groups and those with lower incomes. We are committed to mitigating our impact on the environment and enabling and delivering equitable care that addresses health disparities and improves health outcomes &#x2014; an enterprise priority reflected in our mission.</i></p>
 <br> 
 <p></p> 
 <p> </p>
 <p><i>Diversity creates a healthier atmosphere: UnitedHealth Group is an Equal Employment Opportunity/Affirmative Action employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law.</i></p>
 <br> 
 <p></p> 
 <div>
  <i> UnitedHealth Group is a drug - free workplace. Candidates are required to pass a drug test before beginning employment.</i>
 </div>
</div>","https://careers.unitedhealthgroup.com/job/18979713/sr-informatica-data-engineer-remote-remote/?src=JB-22473","a2753a057927718e",,"Full-time",,"Alton, IL 62002","Sr Informatica Data Engineer - Remote","1 day ago","2023-10-24T11:47:32.798Z","3.3","5629",,"2023-10-25T11:47:32.801Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=a2753a057927718e&from=jasx&tk=1hdjagqsjjgbm800&vjs=3"
"ASCENDING","Location: 100% Remote (Driving distance to Ashburn, VA within 5 hours preferred) Contract Duration: 5 years. Security Clearance Requirement: Must be a U.S. Citizen to obtain clearance
  Job Description:
  We are seeking a highly skilled and experienced Senior Azure Data Engineer to join our team on a long-term contract. The successful candidate will play a critical role in designing, implementing, and optimizing data solutions using Microsoft Azure technologies. This position is 100% remote, but proximity to Ashburn, VA within a 5-hour driving distance is preferred.
  Responsibilities:
 
   Collaborate with cross-functional teams to design, develop, and maintain data solutions on the Azure cloud platform.
   Build and optimize data pipelines using Azure Data Factory, Azure Databricks, and related services.
   Develop, test, and deploy ETL processes for data transformation and integration.
   Work with programming languages such as Python and SQL to manipulate and analyze data.
   Ensure data security and compliance with Homeland Security requirements.
 
  Qualifications:
  The ideal candidate will possess the following qualifications:
 
   Bachelor's or Master's degree in Computer Science, Engineering, or a related field.
   A minimum of 7 years of experience in the field of Information Technology.
   Proven experience of at least 3 years working as a Data Engineer with a primary focus on Microsoft Azure technologies, with recent hands-on experience being highly preferred.
   Strong expertise in designing, building, and optimizing data pipelines using Azure Data Factory, Azure Databricks, and related services.
   Proficiency in programming languages such as Python and SQL, with a solid understanding of ETL frameworks.
   Possession of relevant Azure certifications, such as Microsoft Certified: Azure Data Engineer, is a plus.
 
  We are looking for a self-motivated and detail-oriented individual who can work independently and collaboratively to meet the unique data engineering needs of Homeland Security. If you are a U.S. Citizen with a passion for cutting-edge technology and a desire to contribute to national security, we encourage you to apply.
  To apply for this position, please submit your resume and a cover letter outlining your relevant experience and qualifications.
  
 ebrOM6Ntph","<div>
 <p>Location: <b>100% Remote</b> (Driving distance to Ashburn, VA within 5 hours preferred)<br> Contract Duration: 5 years.<br> Security Clearance Requirement: Must be a<b> U.S. Citizen to obtain clearance</b></p>
 <p><b> Job Description</b>:</p>
 <p> We are seeking a highly skilled and experienced Senior Azure Data Engineer to join our team on a long-term contract. The successful candidate will play a critical role in designing, implementing, and optimizing data solutions using Microsoft Azure technologies. This position is 100% remote, but proximity to Ashburn, VA within a 5-hour driving distance is preferred.</p>
 <p><b> Responsibilities</b>:</p>
 <ul>
  <li> Collaborate with cross-functional teams to design, develop, and maintain data solutions on the Azure cloud platform.</li>
  <li> Build and optimize data pipelines using <b>Azure</b> Data Factory, Azure Databricks, and related services.</li>
  <li> Develop, test, and deploy <b>ETL</b> processes for data transformation and integration.</li>
  <li> Work with programming languages such as <b>Python</b> and <b>SQL</b> to manipulate and analyze data.</li>
  <li> Ensure data security and compliance with Homeland Security requirements.</li>
 </ul>
 <p><b> Qualifications</b>:</p>
 <p> The ideal candidate will possess the following qualifications:</p>
 <ul>
  <li><b> Bachelor&apos;s</b> or <b>Master&apos;s</b> degree in Computer Science, Engineering, or a related field.</li>
  <li> A minimum of <b>7 years of experience</b> in the field of Information Technology.</li>
  <li> Proven experience of at least <b>3 years</b> working as a <b>Data Engineer</b> with a primary focus on Microsoft Azure technologies, with recent hands-on experience being highly preferred.</li>
  <li> Strong expertise in designing, building, and optimizing data pipelines using <b>Azure Data Factory, Azure Databricks</b>, and related services.</li>
  <li> Proficiency in programming languages such as <b>Python</b> and <b>SQL</b>, with a solid understanding of <b>ETL</b> frameworks.</li>
  <li> Possession of relevant <b>Azure</b> certifications, such as <b>Microsoft Certified: Azure Data Engineer, is a </b><b><i>plus</i></b>.</li>
 </ul>
 <p> We are looking for a self-motivated and detail-oriented individual who can work independently and collaboratively to meet the unique data engineering needs of Homeland Security. If you are a U.S. Citizen with a passion for cutting-edge technology and a desire to contribute to national security, we encourage you to apply.</p>
 <p> To apply for this position, please submit your resume and a cover letter outlining your relevant experience and qualifications.</p>
 <p> </p>
 <p>ebrOM6Ntph</p>
</div>","https://ascendingdc.applytojob.com/apply/ebrOM6Ntph/Azure-Data-Engineer?source=INDE","282a7c727e0fad73",,"Full-time",,"Ashburn, VA","Azure Data Engineer","4 days ago","2023-10-21T11:47:51.908Z",,,,"2023-10-25T11:47:51.909Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=282a7c727e0fad73&from=jasx&tk=1hdjaj11gi6lr801&vjs=3"
"Sporty Group","Sporty's sites are some of the most popular on the internet, consistently staying in Alexa's list of top websites for the countries they operate in
 
 
 
   As a Data Engineer at Sporty, you will play a critical role in ensuring the smooth processing and handling of data for our machine learning and data science initiatives. Your primary responsibilities will include designing, building, testing, optimising, and maintaining data pipelines and architectures for various aspects of our rapidly growing business.
 
 
 
   Who We Are
 
 
 
   Sporty Group is a consumer internet and technology business with an unrivalled sports media, gaming, social and fintech platform which serves millions of daily active users across the globe via technology and operations hubs across more than 10 countries and 3 continents.
 
 
   The recipe for our success is to discover intelligent and energetic people, who are passionate about our products and serving our users, and attract and retain them with a dynamic and flexible work life which empowers them to create value and rewards them generously based upon their contribution.
 
 
   We have already built a capable and proven team of 300+ high achievers from a diverse set of backgrounds and we are looking for more talented individuals to drive further growth and contribute to the innovation, creativity and hard work that currently serves our users further via their grit and innovation.
 
 
 
   Responsibilities
 
 
 
   Design, develop and maintain scalable batch ETL and near-real-time data pipelines and architectures for various parts of our business, on fast and versatile data sources with millions of changes per day
 
 
   Ensure all data provided is of the highest quality, accuracy, and consistency
 
 
   Identify, design, and implement internal process improvements for optimising data delivery and re-designing infrastructure for greater scalability
 
 
   Builds out new API integrations to support continuing increases in data volume and complexity
 
 
   Communicate with data scientist, MLOps engineers, product owners and BI analysts in order to understand business processes and system architecture for specific product features
 
 
 
   Requirements
 
 
 
   Bachelor’s degree, or equivalent experience, in Computer Science, Engineering, Mathematics, or a related technical field
 
 
   3+ years of experience in data engineering, data platforms, BI or related domain
 
 
   Experience in successfully implementing data-centric applications, such as data warehouses, operational data stores, and data integration projects
 
 
   Experience with large-scale production relational and NoSQL databases
 
 
   Experience with data modelling
 
 
   General understanding of data architectures and event-driven architectures
 
 
   Proficient in SQL
 
 
   Familiarity with one scripting language, preferably Python
 
 
   Experience with Apache Airflow
 
 
   Solid understanding of cloud data services: AWS services such as S3, Athena, EC2, RedShift, EMR (Elastic MapReduce), EKS, RDS (Relational Database Services) and Lambda
 
 
 
   Nice to have:
 
 
 
   Apache Spark
 
 
   Understanding of containerisation and orchestration technologies like Docker/Kubernetes
 
 
   Relevant knowledge or experience in the gaming industry
  
 
  Benefits
 
 
 
   Quarterly and flash bonuses
 
 
   We have core hours of 10am-3pm in a local timezone, but flexible hours outside of this
 
 
   Education allowance
 
 
   Referral bonuses
 
 
   28 days paid annual leave
 
 
   2 x annual company retreats (Lisbon + Dubai in 2022 / Phuket in Q2 2023 + 1 more TBC!)
 
 
   Highly talented, dependable co-workers in a global, multicultural organisation
 
 
   Payment via world class online wallet system DEEL
 
 
   Top of the line equipment supplied by market leader Hofy
 
 
   We score 100% on The Joel Test 
 
 
  Our teams are small enough for you to be impactful
 
 
   Our business is globally established and successful, offering stability and security to our Team Members
 
 
 
   Our Mission
 
 
 
   Our mission is to be an everyday entertainment platform for everyone
 
 
 
   Our Operating Principles
 
 
 
   1. Create Value for Users
 
 
   2. Act in the Long-Term Interests of Sporty
 
 
   3. Focus on Product Improvements & Innovation
 
 
   4. Be Responsible
 
 
   5. Preserve Integrity & Honesty
 
 
   6. Respect Confidentiality & Privacy
 
 
   7. Ensure Stability, Security & Scalability
 
 
   8. Work Hard with Passion & Pride
 
 
 
   Interview Process
 
 
 
   HackerRank Test
 
 
   Remote video screening with our Talent Acquisition Team + live ID check 
 
 
  Remote 90 min video interview loop with 3 x Team Members (30 mins each) 
 
 
  Pre offer call with Talent Acquisition Team
 
 
   ID check via Zinc 
 
 
  24-72 hour feedback loops throughout process
   
 
 
  
 
  Working at Sporty
 
 
 
   The top-down mentality at Sporty is high performance based, meaning we trust you to do your job with an emphasis on support to help you achieve, grow and de-block any issues when they're in your way.
 
 
   Generally employees can choose their own hours, as long as they are collaborating and doing stand-ups etc. The emphasis is really on results.
 
 
 
   As we are a highly structured and established company we are able to offer the security and support of a global business with the allure of a startup environment. Sporty is independently managed and financed, meaning we don’t have arbitrary shareholder or VC targets to cater to.
 
 
 
   We literally build, spend and make decisions based on the ethos of building THE best platform of its kind. We are truly a tech company to the core and take excellent care of our Team Members.","<div>
 <div>
  Sporty&apos;s sites are some of the most popular on the internet, consistently staying in Alexa&apos;s list of top websites for the countries they operate in
 </div>
 <div></div>
 <div>
  <br> As a Data Engineer at Sporty, you will play a critical role in ensuring the smooth processing and handling of data for our machine learning and data science initiatives. Your primary responsibilities will include designing, building, testing, optimising, and maintaining data pipelines and architectures for various aspects of our rapidly growing business.
 </div>
 <div></div>
 <div>
  <b><br> Who We Are</b>
 </div>
 <div></div>
 <div>
  <br> Sporty Group is a consumer internet and technology business with an unrivalled sports media, gaming, social and fintech platform which serves millions of daily active users across the globe via technology and operations hubs across more than 10 countries and 3 continents.
 </div>
 <div>
   The recipe for our success is to discover intelligent and energetic people, who are passionate about our products and serving our users, and attract and retain them with a dynamic and flexible work life which empowers them to create value and rewards them generously based upon their contribution.
 </div>
 <div>
   We have already built a capable and proven team of 300+ high achievers from a diverse set of backgrounds and we are looking for more talented individuals to drive further growth and contribute to the innovation, creativity and hard work that currently serves our users further via their grit and innovation.
 </div>
 <div></div>
 <div>
  <b><br> Responsibilities</b>
 </div>
 <div></div>
 <div>
  <br> Design, develop and maintain scalable batch ETL and near-real-time data pipelines and architectures for various parts of our business, on fast and versatile data sources with millions of changes per day
 </div>
 <div>
   Ensure all data provided is of the highest quality, accuracy, and consistency
 </div>
 <div>
   Identify, design, and implement internal process improvements for optimising data delivery and re-designing infrastructure for greater scalability
 </div>
 <div>
   Builds out new API integrations to support continuing increases in data volume and complexity
 </div>
 <div>
   Communicate with data scientist, MLOps engineers, product owners and BI analysts in order to understand business processes and system architecture for specific product features
 </div>
 <div></div>
 <div>
  <b><br> Requirements</b>
 </div>
 <div></div>
 <div>
  <br> Bachelor&#x2019;s degree, or equivalent experience, in Computer Science, Engineering, Mathematics, or a related technical field
 </div>
 <div>
   3+ years of experience in data engineering, data platforms, BI or related domain
 </div>
 <div>
   Experience in successfully implementing data-centric applications, such as data warehouses, operational data stores, and data integration projects
 </div>
 <div>
   Experience with large-scale production relational and NoSQL databases
 </div>
 <div>
   Experience with data modelling
 </div>
 <div>
   General understanding of data architectures and event-driven architectures
 </div>
 <div>
   Proficient in SQL
 </div>
 <div>
   Familiarity with one scripting language, preferably Python
 </div>
 <div>
   Experience with Apache Airflow
 </div>
 <div>
   Solid understanding of cloud data services: AWS services such as S3, Athena, EC2, RedShift, EMR (Elastic MapReduce), EKS, RDS (Relational Database Services) and Lambda
 </div>
 <div></div>
 <div>
  <b><br> Nice to have:</b>
 </div>
 <div></div>
 <div>
  <br> Apache Spark
 </div>
 <div>
   Understanding of containerisation and orchestration technologies like Docker/Kubernetes
 </div>
 <div>
   Relevant knowledge or experience in the gaming industry
 </div> 
 <div>
  <b>Benefits</b>
 </div>
 <div></div>
 <div>
  <br> Quarterly and flash bonuses
 </div>
 <div>
   We have core hours of 10am-3pm in a local timezone, but flexible hours outside of this
 </div>
 <div>
   Education allowance
 </div>
 <div>
   Referral bonuses
 </div>
 <div>
   28 days paid annual leave
 </div>
 <div>
   2 x annual company retreats (Lisbon + Dubai in 2022 / Phuket in Q2 2023 + 1 more TBC!)
 </div>
 <div>
   Highly talented, dependable co-workers in a global, multicultural organisation
 </div>
 <div>
   Payment via world class online wallet system DEEL
 </div>
 <div>
   Top of the line equipment supplied by market leader Hofy
 </div>
 <div>
   We score 100% on The Joel Test 
 </div>
 <div>
  Our teams are small enough for you to be impactful
 </div>
 <div>
   Our business is globally established and successful, offering stability and security to our Team Members
 </div>
 <div></div>
 <div>
  <b><br> Our Mission</b>
 </div>
 <div></div>
 <div>
  <br> Our mission is to be an everyday entertainment platform for everyone
 </div>
 <div></div>
 <div>
  <b><br> Our Operating Principles</b>
 </div>
 <div></div>
 <div>
  <br> 1. Create Value for Users
 </div>
 <div>
   2. Act in the Long-Term Interests of Sporty
 </div>
 <div>
   3. Focus on Product Improvements &amp; Innovation
 </div>
 <div>
   4. Be Responsible
 </div>
 <div>
   5. Preserve Integrity &amp; Honesty
 </div>
 <div>
   6. Respect Confidentiality &amp; Privacy
 </div>
 <div>
   7. Ensure Stability, Security &amp; Scalability
 </div>
 <div>
   8. Work Hard with Passion &amp; Pride
 </div>
 <div></div>
 <div>
  <b><br> Interview Process</b>
 </div>
 <div></div>
 <div>
  <br> HackerRank Test
 </div>
 <div>
   Remote video screening with our Talent Acquisition Team + live ID check 
 </div>
 <div>
  Remote 90 min video interview loop with 3 x Team Members (30 mins each) 
 </div>
 <div>
  Pre offer call with Talent Acquisition Team
 </div>
 <div>
   ID check via Zinc 
 </div>
 <div>
  24-72 hour feedback loops throughout process
  <br> 
 </div>
 <div></div>
 <br> 
 <div>
  <b>Working at Sporty</b>
 </div>
 <div></div>
 <div>
  <br> The top-down mentality at Sporty is high performance based, meaning we trust you to do your job with an emphasis on support to help you achieve, grow and de-block any issues when they&apos;re in your way.
 </div>
 <div>
   Generally employees can choose their own hours, as long as they are collaborating and doing stand-ups etc. The emphasis is really on results.
 </div>
 <div></div>
 <div>
  <br> As we are a highly structured and established company we are able to offer the security and support of a global business with the allure of a startup environment. Sporty is independently managed and financed, meaning we don&#x2019;t have arbitrary shareholder or VC targets to cater to.
 </div>
 <div></div>
 <div>
  <br> We literally build, spend and make decisions based on the ethos of building THE best platform of its kind. We are truly a tech company to the core and take excellent care of our Team Members.
 </div>
</div>","https://jobs.lever.co/sporty/6fb86e93-5a9f-4c1f-8983-f51f3bcc6ecd?lever-source=Indeed","2d1de91e6e9422b6",,"Full-time",,"Remote","Data Engineer","4 days ago","2023-10-21T11:47:54.698Z",,,,"2023-10-25T11:47:54.699Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=2d1de91e6e9422b6&from=jasx&tk=1hdjaj0h4j4gl800&vjs=3"
"OM Group Inc.","Data Engineer
  
 
 
  OM Group, Inc. is a fast-growing Microsoft Azure and AWS Partner delivering full-stack cloud services for Federal & DoD clients. We are currently expanding support for the Enterprise Data Warehouse to continue and evolve on-prem/cloud/hybrid data migration and enterprise reporting platforms.
  
 
 
  We are hiring a 
  Data Engineer with experience creating and maintaining complex shell and SQL scripts used to expand and populate data into the enterprise data warehouse. The successful candidate will be responsible for managing and maintaining the Extract, Transform and Load (ETL) processes to populate the Enterprise Data Warehouse/ Enterprise Virtual Viewer (EDW/EVV). The EDW/EVV is powered by a multi-tier environment encapsulation of Windows/Unix and Linux Environments. The primary data repository, as well as host to the virtualization/governance and catalog layers live on the IBM Cloud Pak for Data System (CP4D) and Netezza Performance Server (NPS) PostgreSQL based database. The CP4D system is Linux-based on a combined OpenShift and Redhat 7 platform. The EDW/EVV also utilizes SAP Business Objects (BOBJ) to do reporting based on a Windows Virtual Machine (VM) architecture. The Oracle 19c database is on Oracle Solaris VMs and acts as repository databases for SAP BOBJ.
  
 
 
  This position is remote on Eastern Time zone schedule.
  
 
 
  Responsibilities
  
 
  Work with stakeholders to evaluate business needs and develop tasks to meet requirements and objectives 
  Provide quality assurance and identify bugs or required fixes and communicate to respective teams. Ensure data being migrated to the EDW/EVV production environment is documented 
  Enforce EDW/EVV standards / policies and provide quality assurance for universe, star schema/build, report, and dashboard migrations from the EDW/EVV development environment into the EDW/EVV production environment (see Appendix A for migration metrics) 
  Maintain existing Unix Shell and Structured Query Language (SQL) script based ETL processes. Design, Develop, document, and maintain new and/or expanding script based ETL processes e.g., Unix Shell and SQL scripts 
  Coordinate, test, implement, document, and manage required upgrades and capability enhancements for the EDW/EVV. Perform tasks for data repository expansion as more data content is transitioned to the warehouse 
  Work with infrastructure team to ensure that all the required monitoring, exception handling and fault tolerance is in place for a production-quality data platform 
  Team up with analysts, product managers, and other stakeholders to understand evolving business needs and translate reporting capabilities accordingly 
  Assist with process improvement with a customer-focused, progressive mindset 
  Understand data classification and adhere to the information protection and privacy restrictions 
  Troubleshoot and provide technical support for staff and back-end system users 
  Document and support code migrations and provide quality assurance / control of EDW/EVV star schemas/builds, universes, local data, and reports
 
  
 
 
  Requirements
  
 
  Active DoD Secret Security clearance 
  Hold DoD IAT-III, IAM-II and IASAE-II certifications and Data related industry certifications 
  5+ years experience with Linux Shell Script development, testing, debugging and deployment 
  5+ years experience with SQL, relational database and data warehouse technologies 
  3+ years experience developing and maintaining Python or similar scripting language 
  3+ years experience in multiple subversion technologies such as Subversion, GitHub or Tortoise 
  Knowledge of AWS technologies such as S3, EC2, Redshift, Glue, Athena and Step Functions preferred 
  Experience in data mining and development of ETL processes, distributed data architectures and big data processing technologies 
  Knowledge of production / environment control 
  Knowledge of Agile software development lifecycle 
  Excellent problem-solving skills and attention to detail 
  Strong documentation and training skills 
  
 
 
  OM Group, Inc. recently voted one of the top workplaces in the Washington, DC area, is a fast-growing Microsoft Azure and AWS Partner delivering full-stack cloud services for Federal & DoD clients. We are a growing company that values your skills, training, and ideas and strives to foster a welcoming, diverse, and inclusive environment. OM Group provides competitive compensation and benefits including health insurance coverage, 401(k), paid time off, as well as support for continuous education and training.
  
 
 
  OM Group, Inc. is an equal opportunity employer (EEO) and does not discriminate on the basis of race, color, religion, sex, national origin, age, disability, veteran status, or any other characteristic protected by law. We are committed to creating a diverse and inclusive workplace where all employees are treated with respect and dignity","<div>
 <div>
  <b>Data Engineer</b>
 </div> 
 <div></div>
 <div>
  <b>OM Group, Inc.</b> is a fast-growing Microsoft Azure and AWS Partner delivering full-stack cloud services for Federal &amp; DoD clients. We are currently expanding support for the Enterprise Data Warehouse to continue and evolve on-prem/cloud/hybrid data migration and enterprise reporting platforms.
 </div> 
 <div></div>
 <div>
  We are hiring a 
  <b>Data Engineer </b>with experience creating and maintaining complex shell and SQL scripts used to expand and populate data into the enterprise data warehouse. The successful candidate will be responsible for managing and maintaining the Extract, Transform and Load (ETL) processes to populate the Enterprise Data Warehouse/ Enterprise Virtual Viewer (EDW/EVV). The EDW/EVV is powered by a multi-tier environment encapsulation of Windows/Unix and Linux Environments. The primary data repository, as well as host to the virtualization/governance and catalog layers live on the IBM Cloud Pak for Data System (CP4D) and Netezza Performance Server (NPS) PostgreSQL based database. The CP4D system is Linux-based on a combined OpenShift and Redhat 7 platform. The EDW/EVV also utilizes SAP Business Objects (BOBJ) to do reporting based on a Windows Virtual Machine (VM) architecture. The Oracle 19c database is on Oracle Solaris VMs and acts as repository databases for SAP BOBJ.
 </div> 
 <div></div>
 <div>
  This position is remote on Eastern Time zone schedule.
 </div> 
 <div></div>
 <div>
  <b>Responsibilities</b>
 </div> 
 <ul>
  <li>Work with stakeholders to evaluate business needs and develop tasks to meet requirements and objectives</li> 
  <li>Provide quality assurance and identify bugs or required fixes and communicate to respective teams. Ensure data being migrated to the EDW/EVV production environment is documented</li> 
  <li>Enforce EDW/EVV standards / policies and provide quality assurance for universe, star schema/build, report, and dashboard migrations from the EDW/EVV development environment into the EDW/EVV production environment (see Appendix A for migration metrics)</li> 
  <li>Maintain existing Unix Shell and Structured Query Language (SQL) script based ETL processes. Design, Develop, document, and maintain new and/or expanding script based ETL processes e.g., Unix Shell and SQL scripts</li> 
  <li>Coordinate, test, implement, document, and manage required upgrades and capability enhancements for the EDW/EVV. Perform tasks for data repository expansion as more data content is transitioned to the warehouse</li> 
  <li>Work with infrastructure team to ensure that all the required monitoring, exception handling and fault tolerance is in place for a production-quality data platform</li> 
  <li>Team up with analysts, product managers, and other stakeholders to understand evolving business needs and translate reporting capabilities accordingly</li> 
  <li>Assist with process improvement with a customer-focused, progressive mindset</li> 
  <li>Understand data classification and adhere to the information protection and privacy restrictions</li> 
  <li>Troubleshoot and provide technical support for staff and back-end system users</li> 
  <li>Document and support code migrations and provide quality assurance / control of EDW/EVV star schemas/builds, universes, local data, and reports</li>
 </ul>
 <br> 
 <div></div>
 <div>
  <b>Requirements</b>
 </div> 
 <ul>
  <li>Active DoD Secret Security clearance</li> 
  <li>Hold DoD IAT-III, IAM-II and IASAE-II certifications and Data related industry certifications</li> 
  <li>5+ years experience with Linux Shell Script development, testing, debugging and deployment</li> 
  <li>5+ years experience with SQL, relational database and data warehouse technologies</li> 
  <li>3+ years experience developing and maintaining Python or similar scripting language</li> 
  <li>3+ years experience in multiple subversion technologies such as Subversion, GitHub or Tortoise</li> 
  <li>Knowledge of AWS technologies such as S3, EC2, Redshift, Glue, Athena and Step Functions preferred</li> 
  <li>Experience in data mining and development of ETL processes, distributed data architectures and big data processing technologies</li> 
  <li>Knowledge of production / environment control</li> 
  <li>Knowledge of Agile software development lifecycle</li> 
  <li>Excellent problem-solving skills and attention to detail</li> 
  <li>Strong documentation and training skills</li> 
 </ul> 
 <div></div>
 <div>
  <b>OM Group, Inc.</b> recently voted one of the top workplaces in the Washington, DC area, is a fast-growing Microsoft Azure and AWS Partner delivering full-stack cloud services for Federal &amp; DoD clients. We are a growing company that values your skills, training, and ideas and strives to foster a welcoming, diverse, and inclusive environment. OM Group provides competitive compensation and benefits including health insurance coverage, 401(k), paid time off, as well as support for continuous education and training.
 </div> 
 <div></div>
 <div>
  <b>OM Group, Inc. is an equal opportunity employer (EEO)</b> and does not discriminate on the basis of race, color, religion, sex, national origin, age, disability, veteran status, or any other characteristic protected by law. We are committed to creating a diverse and inclusive workplace where all employees are treated with respect and dignity
 </div>
</div>
<div></div>","https://www.omgroupinc.us/careers.html?gnk=job&gni=8a7883ac8afcf316018b4d84371508e9&gns=Indeed+Free","e5a378ed11848b80",,,,"Remote","Senior Data Engineer","4 days ago","2023-10-21T11:47:55.978Z","3.2","6",,"2023-10-25T11:47:55.986Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=e5a378ed11848b80&from=jasx&tk=1hdjaj2rsih37802&vjs=3"
"FreeWheel","Comcast brings together the best in media and technology. We drive innovation to create the world's best entertainment and online experiences. As a Fortune 50 leader, we set the pace in a variety of innovative and fascinating businesses and create career opportunities across a wide range of locations and disciplines. We are at the forefront of change and move at an amazing pace, thanks to our remarkable people, who bring cutting-edge products and services to life for millions of customers every day. If you share in our passion for teamwork, our vision to revolutionize industries and our goal to lead the future in media and technology, we want you to fast-forward your career at Comcast.
  Job Summary
  Job Description
  DUTIES: Provide technical leadership to a team responsible for developing software components for FreeWheel’s advertising platform products; build data platforms, Video Integration products, Linear Integration products, and new web frontend frameworks; develop software using Java, Python, Scala, and Go programming languages, which are run on big data platforms including Apache Hadoop, Spark, and Snowflake on AWS cloud platform; design, develop, test, and maintain software that extracts, transforms, and loads large volumes of data; create dashboards and monitors on Datadog to ensure 24x7 availability of critical software deployments; manage data held in relational database management systems (RDBMS) using SQL; develop and deploy complex SQL queries to validate impressions from set top boxes on a massive scale; write scripts for CI/CD to enable software artifacts to be built and deployed on AWS and Databricks, using Jenkins or similar tools; debug functional and performance issues on software modules running on Databricks and Spark; analyze product specifications, write technical specs, create monitoring dashboards, develop test suites, design workflows, and setup database schemas and tables; interface with global engineering, operations, services, and business operations teams to execute proof of concepts and incorporate new requirements; improve system performance and ensure availability and scalability of services; and guide and mentor junior-level engineers. Position is eligible for 100% remote work.
 
  REQUIREMENTS: Bachelor’s degree, or foreign equivalent, in Computer Science, Engineering, or related technical field, and five (5) years of experience developing software using Python; managing data held in relational database management systems (RDBMS) using SQL; and developing data architecture; of which one (1) year includes developing software using Spark; and working with cloud technologies, including AWS.
 
  Disclaimer:
 
 
   This information has been designed to indicate the general nature and level of work performed by employees in this role. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications.
 
 
  Comcast is proud to be an equal opportunity workplace. We will consider all qualified applicants for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran status, genetic information, or any other basis protected by applicable law.
  Base pay is one part of the Total Rewards that Comcast provides to compensate and recognize employees for their work. Most sales positions are eligible for a Commission under the terms of an applicable plan, while most non-sales positions are eligible for a Bonus. Additionally, Comcast provides best-in-class Benefits. We believe that benefits should connect you to the support you need when it matters most, and should help you care for those who matter most. That’s why we provide an array of options, expert guidance and always-on tools, that are personalized to meet the needs of your reality – to help support you physically, financially and emotionally through the big milestones and in your everyday life. Please visit the compensation and benefits summary on our careers site for more details.","<div>
 Comcast brings together the best in media and technology. We drive innovation to create the world&apos;s best entertainment and online experiences. As a Fortune 50 leader, we set the pace in a variety of innovative and fascinating businesses and create career opportunities across a wide range of locations and disciplines. We are at the forefront of change and move at an amazing pace, thanks to our remarkable people, who bring cutting-edge products and services to life for millions of customers every day. If you share in our passion for teamwork, our vision to revolutionize industries and our goal to lead the future in media and technology, we want you to fast-forward your career at Comcast.
 <p><b><br> Job Summary</b></p>
 <p><b> Job Description</b></p>
 <p> DUTIES: Provide technical leadership to a team responsible for developing software components for FreeWheel&#x2019;s advertising platform products; build data platforms, Video Integration products, Linear Integration products, and new web frontend frameworks; develop software using Java, Python, Scala, and Go programming languages, which are run on big data platforms including Apache Hadoop, Spark, and Snowflake on AWS cloud platform; design, develop, test, and maintain software that extracts, transforms, and loads large volumes of data; create dashboards and monitors on Datadog to ensure 24x7 availability of critical software deployments; manage data held in relational database management systems (RDBMS) using SQL; develop and deploy complex SQL queries to validate impressions from set top boxes on a massive scale; write scripts for CI/CD to enable software artifacts to be built and deployed on AWS and Databricks, using Jenkins or similar tools; debug functional and performance issues on software modules running on Databricks and Spark; analyze product specifications, write technical specs, create monitoring dashboards, develop test suites, design workflows, and setup database schemas and tables; interface with global engineering, operations, services, and business operations teams to execute proof of concepts and incorporate new requirements; improve system performance and ensure availability and scalability of services; and guide and mentor junior-level engineers. Position is eligible for 100% remote work.</p>
 <p></p>
 <p> REQUIREMENTS: Bachelor&#x2019;s degree, or foreign equivalent, in Computer Science, Engineering, or related technical field, and five (5) years of experience developing software using Python; managing data held in relational database management systems (RDBMS) using SQL; and developing data architecture; of which one (1) year includes developing software using Spark; and working with cloud technologies, including AWS.</p>
 <p></p>
 <p><b> Disclaimer:</b></p>
 <p></p>
 <ul>
  <li> This information has been designed to indicate the general nature and level of work performed by employees in this role. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications.</li>
 </ul>
 <p></p>
 <p> Comcast is proud to be an equal opportunity workplace. We will consider all qualified applicants for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran status, genetic information, or any other basis protected by applicable law.</p>
 <p><br> Base pay is one part of the Total Rewards that Comcast provides to compensate and recognize employees for their work. Most sales positions are eligible for a Commission under the terms of an applicable plan, while most non-sales positions are eligible for a Bonus. Additionally, Comcast provides best-in-class Benefits. We believe that benefits should connect you to the support you need when it matters most, and should help you care for those who matter most. That&#x2019;s why we provide an array of options, expert guidance and always-on tools, that are personalized to meet the needs of your reality &#x2013; to help support you physically, financially and emotionally through the big milestones and in your everyday life. Please visit the compensation and benefits summary on our careers site for more details.</p>
</div>","https://jobs.comcast.com/jobs/description?external_or_internal=External&job_id=R374527&source=ind_orga_at&jobPipeline=Indeed","8d363231c84d5af6",,,,"Philadelphia, PA 19103","Sr. Software Engineer (Data) [Multiple Openings]-9199","4 days ago","2023-10-21T11:47:54.203Z","3.8","4",,"2023-10-25T11:47:54.205Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=8d363231c84d5af6&from=jasx&tk=1hdjaj11gi6lr801&vjs=3"
"KinderCare Education","Futures start here. Where first steps, new friendships, and confident learners are born. At KinderCare Learning Companies, the first and only early childhood education provider recognized with the Gallup Exceptional Workplace Award, we offer a variety of early education and child care options for families. We pave the way for their lifelong learning journey ahead. And we want you to join us in shaping a future we can all be proud to share—in neighborhoods, at work, and in schools nationwide. 
 At KinderCare Learning Companies, you’ll use your skills and expertise to support the work (and fun) that happens in our sites and centers every day. From marketers and strategists to financial analysts and data engineers, and so much more, we’re all passionate about crafting a world where children, families, and organizations can thrive. 
 As the Data Engineer, you will be responsible for the design, development, and implementation of data pipelines, data lake solutions, and data models using an Agile methodology. This role will also partner with team members to define and operationalize data governance standards, data quality and reliability processes, and data lake architecture design sessions, with a goal of optimizing the effectiveness and efficiency of product development and delivery.
   RESPONSIBILITIES: 
  
  Collaborate with team to craft, develop, and implement data pipelines and ETL systems for the data lake 
  Develop data models that support Business Intelligence products 
  Perform data validation to ensure data quality and reliability 
  Work with BI architect and Data Engineering Lead to create technical requirements and specifications for Business Intelligence products 
  Identify, tackle, and fix data pipeline Production issues 
  Build clear documentation of procedures and protocols 
  Implement data governance, data security and privacy policies and collaborate multi-functionally with technical and non-technical team members 
 
 Qualifications: 
 
  Bachelor’s degree in Computer Science, Information Systems, Engineering, Statistics, related field; or equivalent experience 
  5-7 years’ experience as a Data Engineer designing, developing, and implementing data pipelines and ETL systems and 3 years’ experience with data lake technologies and architectures (S3, Glue, Redshift, RDS, DynamoDB, Data Pipeline, EMR) 
  Ambitious knowledge in data warehousing solutions (Panoply, Redshift, etc.) with hands on experience using Cloud based database systems (Azure SQL, Bigtable, etc.) 
  Hands on development experience using Python, Perl, Scala, Java, SAS, C++ 
  Fluent in SQL, ability to write new, complex queries with no supervision; strong written and verbal communication skills 
  Experience with Agile development methodologies, CI/CD automation, Test Driven Development 
  Knowledge of standard processes in database engineering and data security with high level cloud migration & tools knowledge 
  Familiarity with machine learning methodologies and strong problem-solving skills 
  
 Our benefits meet you where you are. We’re here to help our employees navigate the integration of work and life: 
  
  Know your whole family is supported with discounted child care benefits. 
  Breathe easy with medical, dental, and vision benefits for your family (and pets, too!). 
  Feel supported in your mental health and personal growth with employee assistance programs. 
  Feel great and thrive with access to health and wellness programs, unlimited paid time off and discounts for work necessities, such as cell phones. 
  … and much more. 
  
 We operate research-backed, accredited, and customizable programs in more than 2,000 sites and centers across 40 states and the District of Columbia. As we expand, we’re matching the needs of more and more families, dynamic work environments, and diverse communities from coast to coast. Because we believe every family deserves access to high-quality child care, no matter who they are or where they live. Every day, you’ll help bring this mission to life by building community and delivering exceptional experiences. And if you’re anything like us, you’ll come for the work, and stay for the people. 
  KinderCare Learning Companies is an Equal Opportunity employer. All qualified applicants will receive consideration for employment without regard to race, national origin, age, sex, religion, disability, sexual orientation, marital status, military or veteran status, gender identity or expression, or any other basis protected by local, state, or federal law.
  Primary Location : Portland, Oregon, United States
  Job : Corporate","<div>
 <p>Futures start here. Where first steps, new friendships, and confident learners are born. At KinderCare Learning Companies, the first and only early childhood education provider recognized with the <b>Gallup Exceptional Workplace Award</b>, we offer a variety of early education and child care options for families. We pave the way for their lifelong learning journey ahead. And we want you to join us in shaping a future we can all be proud to share&#x2014;in neighborhoods, at work, and in schools nationwide. </p>
 <p>At KinderCare Learning Companies, you&#x2019;ll use your skills and expertise to support the work (and fun) that happens in our sites and centers every day. From marketers and strategists to financial analysts and data engineers, and so much more, we&#x2019;re all passionate about crafting a world where children, families, and organizations can thrive. </p>
 <p>As the Data Engineer, you will be responsible for the design, development, and implementation of data pipelines, data lake solutions, and data models using an Agile methodology. This role will also partner with team members to define and operationalize data governance standards, data quality and reliability processes, and data lake architecture design sessions, with a goal of optimizing the effectiveness and efficiency of product development and delivery.</p>
 <p><br> <b> RESPONSIBILITIES:</b></p> 
 <ul> 
  <li>Collaborate with team to craft, develop, and implement data pipelines and ETL systems for the data lake</li> 
  <li>Develop data models that support Business Intelligence products</li> 
  <li>Perform data validation to ensure data quality and reliability</li> 
  <li>Work with BI architect and Data Engineering Lead to create technical requirements and specifications for Business Intelligence products</li> 
  <li>Identify, tackle, and fix data pipeline Production issues</li> 
  <li>Build clear documentation of procedures and protocols</li> 
  <li>Implement data governance, data security and privacy policies and collaborate multi-functionally with technical and non-technical team members</li> 
 </ul>
 <h4 class=""jobSectionHeader""><b>Qualifications: </b></h4>
 <ul>
  <li>Bachelor&#x2019;s degree in Computer Science, Information Systems, Engineering, Statistics, related field; or equivalent experience</li> 
  <li>5-7 years&#x2019; experience as a Data Engineer designing, developing, and implementing data pipelines and ETL systems and 3 years&#x2019; experience with data lake technologies and architectures (S3, Glue, Redshift, RDS, DynamoDB, Data Pipeline, EMR)</li> 
  <li>Ambitious knowledge in data warehousing solutions (Panoply, Redshift, etc.) with hands on experience using Cloud based database systems (Azure SQL, Bigtable, etc.)</li> 
  <li>Hands on development experience using Python, Perl, Scala, Java, SAS, C++</li> 
  <li>Fluent in SQL, ability to write new, complex queries with no supervision; strong written and verbal communication skills</li> 
  <li>Experience with Agile development methodologies, CI/CD automation, Test Driven Development</li> 
  <li>Knowledge of standard processes in database engineering and data security with high level cloud migration &amp; tools knowledge</li> 
  <li>Familiarity with machine learning methodologies and strong problem-solving skills</li> 
 </ul> 
 <p><b>Our benefits meet you where you are.</b> We&#x2019;re here to help our employees navigate the integration of work and life:</p> 
 <ul> 
  <li>Know your whole family is supported with <b>discounted child care benefits.</b></li> 
  <li>Breathe easy with <b>medical, dental, and vision benefits</b> for your family (and pets, too!).</li> 
  <li>Feel supported in your mental health and personal growth with <b>employee assistance programs.</b></li> 
  <li>Feel great and thrive with access to <b>health and wellness programs, unlimited paid time off </b>and <b>discounts</b> for work necessities, such as cell phones.</li> 
  <li>&#x2026; and much more.</li> 
 </ul> 
 <p>We operate research-backed, accredited, and customizable programs in more than 2,000 sites and centers across 40 states and the District of Columbia. As we expand, we&#x2019;re matching the needs of more and more families, dynamic work environments, and diverse communities from coast to coast. Because we believe every family deserves access to high-quality child care, no matter who they are or where they live. Every day, you&#x2019;ll help bring this mission to life by building community and delivering exceptional experiences. And if you&#x2019;re anything like us, you&#x2019;ll come for the work, and stay for the people.</p> 
 <p> KinderCare Learning Companies is an Equal Opportunity employer. All qualified applicants will receive consideration for employment without regard to race, national origin, age, sex, religion, disability, sexual orientation, marital status, military or veteran status, gender identity or expression, or any other basis protected by local, state, or federal law.</p>
 <p> Primary Location : Portland, Oregon, United States</p>
 <p> Job : Corporate</p>
</div>","https://www.indeed.com/rc/clk?jk=707d3fdd870a23bd&atk=&xpse=SoDf67I3JzdBTkwTH50LbzkdCdPP","707d3fdd870a23bd",,"Full-time",,"Portland, OR 97204","Data Engineer - Remote Opportunity","4 days ago","2023-10-21T11:48:05.257Z","2.7","769",,"2023-10-25T11:48:05.259Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=707d3fdd870a23bd&from=jasx&tk=1hdjaj11gi6lr801&vjs=3"
"Conversant Group","Conversant Group is an IT infrastructure and security consulting company founded in 2009 and based in Chattanooga, TN. We are the 
  world’s first Civilian Cybersecurity Force, dedicated to defeating cyber terrorists. To do so, Conversant Group is organized into three battalions: 
  Athena7
   which provides infrastructure assessment, strategy and remediation, 
  Grypho5
   which offers ongoing managed protection, and 
  Fenix24
   which provides rapid restoration in case of a cyberattack.
 
 
 
   The Rapid Betterment Engineer IV role is a Senior / Technical Lead, principally responsible for the rapid implementation of Conversant Group’s Managed Security Solutions. This role reports to the Director of Rapid Betterment and is 100% Remote with a potential requirement to travel to client locations when necessary and if applicable (subject to availability). An ideal candidate will be able to; [1] lead technical design and architectural workshops, [2] deploy and configure complex technical solutions using our internal specifications, documentation, and best practices, with minimal supervision, [3] resolve implementation issues with a combination of skills, experience, vendor support, and internal consultations, [4] maintain an exhaustive understanding of our products and solutions, [5] create and maintain implementation documentation leveraging Conversant Group’s security standards, [6] maintain a high degree of proficiency in the products and software to be implemented, [7] act as a subject matter expert for Conversant Group’s product and service solutions, [8] provide mentoring and be a technical escalation for junior / mid-level engineers.
  
 Preferred Skills
 
   Lead client facing technical architecture and design workshops.
   Implement complex technical security solutions with minimal supervision.
   Resolve implementation-related technical issues.
   Work closely with Project Managers on defining task details, timelines, status updates, and working within project scope.
   Understand project scope definition, validation, and rapid deployment of services.
   Maintain an exhaustive understanding of the guiding principles for Conversant Group product & service offerings.
   Act as a SME for Conversant Group’s Managed Security Solutions portfolio.
   Provide mentoring and be a technical escalation for team members.
   Maintain select vendor technical certifications.
   Provide client education for deployed solutions.
   Collaborate with clients, partners, and vendors to aid in successful deployment of security solutions.
 
  Qualifications
 
   7+ years of Managed Services Support Engineering and/or Architectural Design & Implementation Services Engineering.
   7+ years of Security Product experience Implementing and Supporting On-premises, Hybrid, Cloud based Security Solutions such as Cohesity, Microsoft Azure, AWS and VMWare.
   Service Delivery in a Managed Services / Professional Services organization highly desired.
   Working Knowledge and Understanding of Microsoft and VMWare platforms highly desired.
   Industry Certifications in Cohesity, Microsoft, and VMware highly desired.
   Bachelor’s Degree in Information Technology, related field, or equivalent work history.
   Experience working for and within IT for the Legal Industry a plus.
 
 
   Why work for us?
 
 
 
   We offer a dynamic, innovative work environment with rewarding work - help save our clients from disaster!
 
 
   We truly value our employees and provide an extraordinary package to prove it, including:
   
 
 
  
 
  Internal and external learning & development opportunities, including career advancement.
 
 
   Competitive compensation & benefits.
 
 
   Scheduled & flexible PTO programs.
 
 
   Fully remote work options.
 
 
   Family friendly programs
 
 
   Care packages 
 
 
  Regular team building events.
 
 
 
   Join the world's first Civilian Cybersecurity Force and take your career to the next level!","<div>
 <div>
  Conversant Group is an IT infrastructure and security consulting company founded in 2009 and based in Chattanooga, TN. We are the 
  <b>world&#x2019;s first Civilian Cybersecurity Force</b>, dedicated to defeating cyber terrorists. To do so, Conversant Group is organized into three battalions: 
  <b><i>Athena7</i></b>
  <i> </i>which provides infrastructure assessment, strategy and remediation, 
  <b><i>Grypho5</i></b>
  <i> </i>which offers ongoing managed protection, and 
  <b><i>Fenix24</i></b>
  <i> </i>which provides rapid restoration in case of a cyberattack.
 </div>
 <div></div>
 <div>
  <br> The Rapid Betterment Engineer IV role is a Senior / Technical Lead, principally responsible for the rapid implementation of Conversant Group&#x2019;s Managed Security Solutions. This role reports to the Director of Rapid Betterment and is 100% Remote with a potential requirement to travel to client locations when necessary and if applicable (subject to availability). An ideal candidate will be able to; [1] lead technical design and architectural workshops, [2] deploy and configure complex technical solutions using our internal specifications, documentation, and best practices, with minimal supervision, [3] resolve implementation issues with a combination of skills, experience, vendor support, and internal consultations, [4] maintain an exhaustive understanding of our products and solutions, [5] create and maintain implementation documentation leveraging Conversant Group&#x2019;s security standards, [6] maintain a high degree of proficiency in the products and software to be implemented, [7] act as a subject matter expert for Conversant Group&#x2019;s product and service solutions, [8] provide mentoring and be a technical escalation for junior / mid-level engineers.
 </div> 
 <h3 class=""jobSectionHeader""><b>Preferred Skills</b></h3>
 <ul>
  <li> Lead client facing technical architecture and design workshops.</li>
  <li> Implement complex technical security solutions with minimal supervision.</li>
  <li> Resolve implementation-related technical issues.</li>
  <li> Work closely with Project Managers on defining task details, timelines, status updates, and working within project scope.</li>
  <li> Understand project scope definition, validation, and rapid deployment of services.</li>
  <li> Maintain an exhaustive understanding of the guiding principles for Conversant Group product &amp; service offerings.</li>
  <li> Act as a SME for Conversant Group&#x2019;s Managed Security Solutions portfolio.</li>
  <li> Provide mentoring and be a technical escalation for team members.</li>
  <li> Maintain select vendor technical certifications.</li>
  <li> Provide client education for deployed solutions.</li>
  <li> Collaborate with clients, partners, and vendors to aid in successful deployment of security solutions.</li>
 </ul>
 <h3 class=""jobSectionHeader""><b> Qualifications</b></h3>
 <ul>
  <li> 7+ years of Managed Services Support Engineering and/or Architectural Design &amp; Implementation Services Engineering.</li>
  <li> 7+ years of Security Product experience Implementing and Supporting On-premises, Hybrid, Cloud based Security Solutions such as Cohesity, Microsoft Azure, AWS and VMWare.</li>
  <li> Service Delivery in a Managed Services / Professional Services organization highly desired.</li>
  <li> Working Knowledge and Understanding of Microsoft and VMWare platforms highly desired.</li>
  <li> Industry Certifications in Cohesity, Microsoft, and VMware highly desired.</li>
  <li> Bachelor&#x2019;s Degree in Information Technology, related field, or equivalent work history.</li>
  <li> Experience working for and within IT for the Legal Industry a plus.</li>
 </ul>
 <div>
  <b> Why work for us?</b>
 </div>
 <div></div>
 <div>
  <br> We offer a dynamic, innovative work environment with rewarding work - help save our clients from disaster!
 </div>
 <div>
   We truly value our employees and provide an extraordinary package to prove it, including:
  <br> 
 </div>
 <div></div>
 <br> 
 <div>
  Internal and external learning &amp; development opportunities, including career advancement.
 </div>
 <div>
   Competitive compensation &amp; benefits.
 </div>
 <div>
   Scheduled &amp; flexible PTO programs.
 </div>
 <div>
   Fully remote work options.
 </div>
 <div>
   Family friendly programs
 </div>
 <div>
   Care packages 
 </div>
 <div>
  Regular team building events.
 </div>
 <div></div>
 <div>
  <br> Join the world&apos;s first Civilian Cybersecurity Force and take your career to the next level!
 </div>
</div>
<div></div>","https://jobs.lever.co/conversantgroup/997f5e26-2f70-4e11-a84f-6e7f72b06d23?lever-source=Indeed","326aecf5562629b4",,"Full-time",,"Remote","Data Protection & Cloud Engineer (Cohesity)","4 days ago","2023-10-21T11:48:07.564Z","3.9","7",,"2023-10-25T11:48:07.565Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=326aecf5562629b4&from=jasx&tk=1hdjaj11gi6lr801&vjs=3"
"CVS Health","Bring your heart to CVS Health. Every one of us at CVS Health shares a single, clear purpose: Bringing our heart to every moment of your health. This purpose guides our commitment to deliver enhanced human-centric health care for a rapidly changing world. Anchored in our brand — with heart at its center — our purpose sends a personal message that how we deliver our services is just as important as what we deliver.  Our Heart At Work Behaviors™ support this purpose. We want everyone who works at CVS Health to feel empowered by the role they play in transforming our culture and accelerating our ability to innovate and deliver solutions to make health care more personal, convenient and affordable.
 
  Position Summary
  Manages the creation and/or implementation of information security policies, programs, and procedures to cost-effectively and efficiently protect information and information systems assets from Intentional or inadvertent modification; disclosure or destruction; unauthorized access; reduced, interrupted or terminated, processing capability; malicious logic or virus activity; or loss, theft, damage or destruction of any IT resources.
 
  Designs, implements and deploys information security policies, procedures and guidelines. Responsible for developing, maintaining, publishing and/or enforcing information security standards and guidelines encompassing data, and intellectual security. Provides reports to management regarding the effectiveness of network and data security and making recommendations for the adoption of new procedures and technologies, as required. Monitors changes in legislation and accreditation standards that affect information security. Monitors internal control systems to ensure that appropriate information access levels and security clearances are maintained. Establishes meaningful metrics on key critical infrastructure components of information security and monitoring of these to ensure the confidentiality, integrity and availability of information and processes. Ensures awareness of organization's information security policies and procedures among employees, contractors, alliances and other third parties. Initiates, facilitates, and promotes activities to foster information security awareness within the organization. Provides direct information security training to all employees, contractors, alliances, and other third parties. Coordinates internal and external audits and follow up with implementation, based audit recommendations. Serves as an internal information security consultant to the organization. Monitors advancements in information security technologies. Communicates unresolved information security exposures, misuse, or non-compliance situations to senior management. Participates in the activities of the Information Security Committee, responsible for the organization's information security program.
  Extensive experience with data ingest process, Hadoop, Apache Nifi, Cloud platform (AWS, GCP, Azure)
 
 
  Support the design, development and implementation of Security Data Analytic (SDA) capabilities that leverage cybersecurity data and big data technologies to deliver efficiencies and new discovery capabilities
  Design and implement resilient data pipelines and architecture for ingesting unbounded data from multiple sources with different formats leveraging Hadoop, Spark, NiFi and Kafka, Cloud-based applications (AWS, GCP, Azure)
  Develop capabilities for ingesting and indexing large volumes of security-related data in Hadoop, AWS, GCP, Azure environments
  Stay apprised of the latest data science technologies and techniques in order to identify areas where new tools can be applied to support SDA use cases
  Design and implement batch pipelines using Hive, Spark, Pig and Python for global security analytics
  Design and implement discrete and behavioral analytic capabilities to support cybersecurity operations
  Develop visualization capabilities to data using Tableau
  Support the SDA infrastructure design and implementation and ensure integration with other GS systems and operational workflows
  Stay apprised of the latest industry security trends and threats and apply that knowledge to the design and implementation of SDA software solutions
 
  Required Qualifications
  4+ years experience designing and implementing capabilities in a Hadoop-based, Spark big data environment
  4+ years experience in design and implementing real-time data pipelines using NiFi, Kafka and Spark Streaming 
 4+ years experience processing large volumes of data with Hive, Pig, Spark, NiFi, Kafka and Python 
   Preferred Qualifications
 
   Familiarity with streaming big data applications like, Kafka, NiFi, Spark streaming/ Strom
   Strong background in Information System Security
   Excellent written and verbal communication skills
   Familiarity with statistical computing techniques, and statistics software (e.g. R and Tableau)
   Experience using source code management systems (e.g. Git)
   Interest and passion for learning about new technologies
   Experience in processing TB’s worth of data
 
  Education
  Bachelor in engineering, computer science, or related fields
 
  Pay Range
  The typical pay range for this role is:
  $94,500.00 - $196,000.00
 
  This pay range represents the base hourly rate or base annual full-time salary for all positions in the job grade within which this position falls. The actual base salary offer will depend on a variety of factors including experience, education, geography and other relevant factors. This position is eligible for a CVS Health bonus, commission or short-term incentive program in addition to the base pay range listed above.  In addition to your compensation, enjoy the rewards of an organization that puts our heart into caring for our colleagues and our communities. The Company offers a full range of medical, dental, and vision benefits. Eligible employees may enroll in the Company’s 401(k) retirement savings plan, and an Employee Stock Purchase Plan is also available for eligible employees. The Company provides a fully-paid term life insurance plan to eligible employees, and short-term and long term disability benefits. CVS Health also offers numerous well-being programs, education assistance, free development courses, a CVS store discount, and discount programs with participating partners. As for time off, Company employees enjoy Paid Time Off (“PTO”) or vacation pay, as well as paid holidays throughout the calendar year. Number of paid holidays, sick time and other time off are provided consistent with relevant state law and Company policies.  For more detailed information on available benefits, please visit jobs.CVSHealth.com/benefits
 
  CVS Health requires certain colleagues to be fully vaccinated against COVID-19 (including any booster shots if required), where allowable under the law, unless they are approved for a reasonable accommodation based on disability, medical condition, religious belief, or other legally recognized reasons that prevents them from being vaccinated.
 
  You are required to have received at least one COVID-19 shot prior to your first day of employment and to provide proof of your vaccination status or apply for a reasonable accommodation within the first 10 days of your employment. Please note that in some states and roles, you may be required to provide proof of full vaccination or an approved reasonable accommodation before you can begin to actively work.
 
  CVS Health is committed to recruiting, hiring, developing, advancing, and retaining individuals with disabilities. As such, we strive to provide equal access to the benefits and privileges of employment, including the provision of a reasonable accommodation to perform essential job functions. CVS Health can provide a request for a reasonable accommodation, including a qualified interpreter, written information in other formats, translation or other services through ColleagueRelations@CVSHealth.com If you have a speech or hearing disability, please call 7-1-1 to utilize Telecommunications Relay Services (TRS). We will make every effort to respond to your request within 48 business hours and do everything we can to work towards a solution.","<div>
 <p>Bring your heart to CVS Health. Every one of us at CVS Health shares a single, clear purpose: Bringing our heart to every moment of your health. This purpose guides our commitment to deliver enhanced human-centric health care for a rapidly changing world. Anchored in our brand &#x2014; with heart at its center &#x2014; our purpose sends a personal message that how we deliver our services is just as important as what we deliver.<br> <br> Our Heart At Work Behaviors&#x2122; support this purpose. We want everyone who works at CVS Health to feel empowered by the role they play in transforming our culture and accelerating our ability to innovate and deliver solutions to make health care more personal, convenient and affordable.</p>
 <p></p>
 <p><b> Position Summary</b></p>
 <p> Manages the creation and/or implementation of information security policies, programs, and procedures to cost-effectively and efficiently protect information and information systems assets from Intentional or inadvertent modification; disclosure or destruction; unauthorized access; reduced, interrupted or terminated, processing capability; malicious logic or virus activity; or loss, theft, damage or destruction of any IT resources.</p>
 <p></p>
 <p> Designs, implements and deploys information security policies, procedures and guidelines. Responsible for developing, maintaining, publishing and/or enforcing information security standards and guidelines encompassing data, and intellectual security. Provides reports to management regarding the effectiveness of network and data security and making recommendations for the adoption of new procedures and technologies, as required. Monitors changes in legislation and accreditation standards that affect information security. Monitors internal control systems to ensure that appropriate information access levels and security clearances are maintained. Establishes meaningful metrics on key critical infrastructure components of information security and monitoring of these to ensure the confidentiality, integrity and availability of information and processes. Ensures awareness of organization&apos;s information security policies and procedures among employees, contractors, alliances and other third parties. Initiates, facilitates, and promotes activities to foster information security awareness within the organization. Provides direct information security training to all employees, contractors, alliances, and other third parties. Coordinates internal and external audits and follow up with implementation, based audit recommendations. Serves as an internal information security consultant to the organization. Monitors advancements in information security technologies. Communicates unresolved information security exposures, misuse, or non-compliance situations to senior management. Participates in the activities of the Information Security Committee, responsible for the organization&apos;s information security program.</p>
 <p> Extensive experience with data ingest process, Hadoop, Apache Nifi, Cloud platform (AWS, GCP, Azure)</p>
 <p></p>
 <ul>
  <li>Support the design, development and implementation of Security Data Analytic (SDA) capabilities that leverage cybersecurity data and big data technologies to deliver efficiencies and new discovery capabilities</li>
  <li>Design and implement resilient data pipelines and architecture for ingesting unbounded data from multiple sources with different formats leveraging Hadoop, Spark, NiFi and Kafka, Cloud-based applications (AWS, GCP, Azure)</li>
  <li>Develop capabilities for ingesting and indexing large volumes of security-related data in Hadoop, AWS, GCP, Azure environments</li>
  <li>Stay apprised of the latest data science technologies and techniques in order to identify areas where new tools can be applied to support SDA use cases</li>
  <li>Design and implement batch pipelines using Hive, Spark, Pig and Python for global security analytics</li>
  <li>Design and implement discrete and behavioral analytic capabilities to support cybersecurity operations</li>
  <li>Develop visualization capabilities to data using Tableau</li>
  <li>Support the SDA infrastructure design and implementation and ensure integration with other GS systems and operational workflows</li>
  <li>Stay apprised of the latest industry security trends and threats and apply that knowledge to the design and implementation of SDA software solutions</li>
 </ul>
 <p><b><br> Required Qualifications</b></p>
 <p> 4+ years experience designing and implementing capabilities in a Hadoop-based, Spark big data environment</p>
 <p> 4+ years experience in design and implementing real-time data pipelines using NiFi, Kafka and Spark Streaming </p>
 <p>4+ years experience processing large volumes of data with Hive, Pig, Spark, NiFi, Kafka and Python<br> </p>
 <p><br> <br> <b>Preferred Qualifications</b></p>
 <ul>
  <li> Familiarity with streaming big data applications like, Kafka, NiFi, Spark streaming/ Strom</li>
  <li> Strong background in Information System Security</li>
  <li> Excellent written and verbal communication skills</li>
  <li> Familiarity with statistical computing techniques, and statistics software (e.g. R and Tableau)</li>
  <li> Experience using source code management systems (e.g. Git)</li>
  <li> Interest and passion for learning about new technologies</li>
  <li> Experience in processing TB&#x2019;s worth of data</li>
 </ul>
 <p><b><br> Education</b></p>
 <p> Bachelor in engineering, computer science, or related fields</p>
 <p></p>
 <p><b> Pay Range</b></p>
 <p> The typical pay range for this role is:</p>
 <p></p> &#x24;94,500.00 - &#x24;196,000.00
 <p></p>
 <p> This pay range represents the base hourly rate or base annual full-time salary for all positions in the job grade within which this position falls. The actual base salary offer will depend on a variety of factors including experience, education, geography and other relevant factors. This position is eligible for a CVS Health bonus, commission or short-term incentive program in addition to the base pay range listed above.<br> <br> In addition to your compensation, enjoy the rewards of an organization that puts our heart into caring for our colleagues and our communities. The Company offers a full range of medical, dental, and vision benefits. Eligible employees may enroll in the Company&#x2019;s 401(k) retirement savings plan, and an Employee Stock Purchase Plan is also available for eligible employees. The Company provides a fully-paid term life insurance plan to eligible employees, and short-term and long term disability benefits. CVS Health also offers numerous well-being programs, education assistance, free development courses, a CVS store discount, and discount programs with participating partners. As for time off, Company employees enjoy Paid Time Off (&#x201c;PTO&#x201d;) or vacation pay, as well as paid holidays throughout the calendar year. Number of paid holidays, sick time and other time off are provided consistent with relevant state law and Company policies.<br> <br> For more detailed information on available benefits, please visit jobs.CVSHealth.com/benefits</p>
 <p></p>
 <p> CVS Health requires certain colleagues to be fully vaccinated against COVID-19 (including any booster shots if required), where allowable under the law, unless they are approved for a reasonable accommodation based on disability, medical condition, religious belief, or other legally recognized reasons that prevents them from being vaccinated.</p>
 <p></p>
 <p> You are required to have received at least one COVID-19 shot prior to your first day of employment and to provide proof of your vaccination status or apply for a reasonable accommodation within the first 10 days of your employment. Please note that in some states and roles, you may be required to provide proof of full vaccination or an approved reasonable accommodation before you can begin to actively work.</p>
 <p></p>
 <p> CVS Health is committed to recruiting, hiring, developing, advancing, and retaining individuals with disabilities. As such, we strive to provide equal access to the benefits and privileges of employment, including the provision of a reasonable accommodation to perform essential job functions. CVS Health can provide a request for a reasonable accommodation, including a qualified interpreter, written information in other formats, translation or other services through ColleagueRelations@CVSHealth.com If you have a speech or hearing disability, please call 7-1-1 to utilize Telecommunications Relay Services (TRS). We will make every effort to respond to your request within 48 business hours and do everything we can to work towards a solution.</p>
</div>","https://jobs.cvshealth.com/job/19309567/mgr-security-data-engineer-remote/?rx_a=0&rx_c=&rx_group=103511&rx_job=R0062474&rx_medium=cpc&rx_source=indeed&rx_ts=20231025T100803Z&rx_p=5AK1AFHFZW","b1473aeba6e7b340",,"Full-time",,"Blue Bell, PA","Mgr, Security Data Engineer","4 days ago","2023-10-21T11:47:53.607Z","3.2","44878","$90,000 - $196,000 a year","2023-10-25T11:47:53.609Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=b1473aeba6e7b340&from=jasx&tk=1hdjaj11gi6lr801&vjs=3"
"OneStudyTeam","At OneStudyTeam (a Reify Health company), we specialize in speeding up clinical trials and increasing the chance of new therapies being approved with the ultimate goal of improving patient outcomes. Our cloud-based platform, StudyTeam, brings research site workflows online and enables sites, sponsors, and other key stakeholders to work together more effectively. StudyTeam is trusted by the largest global biopharmaceutical companies, used in over 6,000 research sites, and is available in over 100 countries. Join us in our mission to advance clinical research and improve patient care. 
   One mission. One team. That's OneStudyTeam.
 
  Our unique, rapidly growing data streams are enabling novel opportunities to manage clinical trials more efficiently and predictably. The Data Engineering team is looking for talented Senior Data Engineers to build, expand, and support a cutting-edge data architecture which is the analytical backbone of our company. If you are empathetic, business-driven, and want to use your data engineering and data architecture skills to make a tangible impact in the clinical research community then this may be the role for you. 
  We're looking for people who can effectively balance rapid execution and delivery with sustainable and scalable architectural initiatives to serve the business most effectively. You have strong opinions, weakly held, and while well-versed technically know when to choose the right tool, for the right job, at the right level of complexity. You will work closely with the rest of our Data Engineering team and our Data Science teams to help collect, stream, transform, and effectively manage data for integration into critical reporting, data visualizations, and ModelOps systems for emerging data science products. 
  What You'll Be Working On 
 
  Build reliable and robust data integrations with external partners 
  Supporting the development and expansion of modern, privacy-aware, data warehouse and data mesh architectures 
  Helping to build, manage, orchestrate, and integrate streaming data sources, data lakes, ELT processes, columnar storage systems, and distributed query execution solutions 
  Establishing proactive data quality/freshness dashboards, monitoring, alerting, and anomaly remediation systems 
  Building practical data onboarding tooling and process automation solutions 
  Learning to effectively understand and deftly navigate the global compliance ecosystem (HIPAA, GDPR, etc.) to ensure your work respects the rights, regulations, and consent preferences of all stakeholders, including historical underserved or underrepresented populations 
  Developing a deep understanding of the clinical ecosystem, our products, and our business and how they all uniquely interact to help people 
 
 What You'll Bring to OneStudyTeam 
 
  4+ years of experience successfully developing and deploying data pipelines and distributed architectures, ideally in a space similar to ours (startup, healthcare, regulated data) 
  Hands-on experience implementing ETL/ELT best practices at scale and demonstrated practical experience or familiarity with a good portion of our stack, including: AWS services (Redshift, MSK, Lambda, ECS, ECR, EC2, Glue, Quicksight, Spectrum, S3, etc.), Postgres, dbt, Kafka, Prefect, Docker, Terraform 
  Excellent programming skills in Python and deep comfort with SQL. Clojure experience is also highly appreciated 
  Experience or interest in developing and managing enterprise-scale data, distributed data architectures 
  Able to independently ship medium-to-large features and start to support or participate in architectural design 
  Excellent written and verbal communication skills 
  Strong attention to detail is key, especially when considering correctness, security, and compliance 
  Solid software testing, documentation, and debugging practices in the context of distributed systems 
 
 
   Learn more about our global benefits offerings on our careers site: https://careers.onestudyteam.com/us-benefits 
   We value diversity and believe the unique contributions each of us brings drives our success. We do not discriminate on the basis of race, sex, religion, color, national origin, gender identity, age, marital status, veteran status, or disability status. 
   Note: OneStudyTeam is unable to sponsor work visas at this time. If you are a non-U.S. resident applicant, please note that OST works with a Professional Employer Organization. 
   As a condition of employment, you will abide by all organizational security and privacy policies. 
   For a detailed overview of OneStudyTeam's candidate privacy policy, please visit https://careers.onestudyteam.com/candidate-privacy-policy. This organization participates in E-Verify (E-Verify's Right to Work guidance can be found here).","<div>
 <div>
  <p>At OneStudyTeam (a Reify Health company), we specialize in speeding up clinical trials and increasing the chance of new therapies being approved with the ultimate goal of improving patient outcomes. Our cloud-based platform, StudyTeam, brings research site workflows online and enables sites, sponsors, and other key stakeholders to work together more effectively. StudyTeam is trusted by the largest global biopharmaceutical companies, used in over 6,000 research sites, and is available in over 100 countries. Join us in our mission to advance clinical research and improve patient care.</p> 
  <p><b> One mission. One team. That&apos;s OneStudyTeam.</b></p>
 </div>
 <p> Our unique, rapidly growing data streams are enabling novel opportunities to manage clinical trials more efficiently and predictably. The Data Engineering team is looking for talented <b>Senior Data Engineers</b> to build, expand, and support a cutting-edge data architecture which is the analytical backbone of our company. If you are empathetic, business-driven, and want to use your data engineering and data architecture skills to make a tangible impact in the clinical research community then this may be the role for you.</p> 
 <p> We&apos;re looking for people who can effectively balance rapid execution and delivery with sustainable and scalable architectural initiatives to serve the business most effectively. You have strong opinions, weakly held, and while well-versed technically know when to choose the right tool, for the right job, at the right level of complexity. You will work closely with the rest of our Data Engineering team and our Data Science teams to help collect, stream, transform, and effectively manage data for integration into critical reporting, data visualizations, and ModelOps systems for emerging data science products.</p> 
 <h3 class=""jobSectionHeader""><b> What You&apos;ll Be Working On</b></h3> 
 <ul>
  <li>Build reliable and robust data integrations with external partners</li> 
  <li>Supporting the development and expansion of modern, privacy-aware, data warehouse and data mesh architectures</li> 
  <li>Helping to build, manage, orchestrate, and integrate streaming data sources, data lakes, ELT processes, columnar storage systems, and distributed query execution solutions</li> 
  <li>Establishing proactive data quality/freshness dashboards, monitoring, alerting, and anomaly remediation systems</li> 
  <li>Building practical data onboarding tooling and process automation solutions</li> 
  <li>Learning to effectively understand and deftly navigate the global compliance ecosystem (HIPAA, GDPR, etc.) to ensure your work respects the rights, regulations, and consent preferences of all stakeholders, including historical underserved or underrepresented populations</li> 
  <li>Developing a deep understanding of the clinical ecosystem, our products, and our business and how they all uniquely interact to help people</li> 
 </ul>
 <h3 class=""jobSectionHeader""><b>What You&apos;ll Bring to OneStudyTeam</b></h3> 
 <ul>
  <li>4+ years of experience successfully developing and deploying data pipelines and distributed architectures, ideally in a space similar to ours (startup, healthcare, regulated data)</li> 
  <li>Hands-on experience implementing ETL/ELT best practices at scale and demonstrated practical experience or familiarity with a good portion of our stack, including: AWS services (Redshift, MSK, Lambda, ECS, ECR, EC2, Glue, Quicksight, Spectrum, S3, etc.), Postgres, dbt, Kafka, Prefect, Docker, Terraform</li> 
  <li>Excellent programming skills in Python and deep comfort with SQL. Clojure experience is also highly appreciated</li> 
  <li>Experience or interest in developing and managing enterprise-scale data, distributed data architectures</li> 
  <li>Able to independently ship medium-to-large features and start to support or participate in architectural design</li> 
  <li>Excellent written and verbal communication skills</li> 
  <li>Strong attention to detail is key, especially when considering correctness, security, and compliance</li> 
  <li>Solid software testing, documentation, and debugging practices in the context of distributed systems</li> 
 </ul>
 <div>
  <p> Learn more about our global benefits offerings on our careers site: https://careers.onestudyteam.com/us-benefits</p> 
  <p><i> We value diversity and believe the unique contributions each of us brings drives our success. We do not discriminate on the basis of race, sex, religion, color, national origin, gender identity, age, marital status, veteran status, or disability status.</i></p> 
  <p><b><i> Note</i></b><i>: OneStudyTeam</i><i> is unable to sponsor work visas at this time. If you are a non-U.S. resident applicant, please note that OST works with a Professional Employer Organization.</i></p> 
  <p><i> As a condition of employment, you will abide by all organizational security and privacy policies.</i></p> 
  <p><i> For a detailed overview of OneStudyTeam&apos;s candidate privacy policy, please visit</i><i> </i><i>https://careers.onestudyteam.com/candidate-privacy-policy</i><i>. This organization participates in </i><i>E-Verify</i><i> (E-Verify&apos;s Right to Work guidance can be found </i><i>here</i><i>).</i></p>
 </div>
</div>","https://www.indeed.com/applystart?jk=6e452e2e53e76bbd&from=vj&pos=top&mvj=0&spon=0&sjdu=YmZE5d5THV8u75cuc0H6Y26AwfY51UOGmh3Z9h4OvXhtYkhs4NuygT4127jCqIQEQPTx7ack05kPwIeOSvQquQ&vjfrom=serp&astse=80f476eaf9f1ae42&assa=4268","6e452e2e53e76bbd",,,,"Remote","Senior Data Engineer","4 days ago","2023-10-21T11:48:05.006Z",,,,"2023-10-25T11:48:05.007Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=6e452e2e53e76bbd&from=jasx&tk=1hdjaj2rsih37802&vjs=3"
"Vision Government Solutions Inc","About Vision
  Vision Government Solutions is a leading government technology firm providing cutting-edge software to the public sector. We are at an incredible inflection point of growth and are looking for exceptional individuals to join our Software Implementation team to help us successfully welcome new communities to our Vision software.
  Our software implementation philosophy emphasizes the importance of customer delight, speed, and long-term partnership. To that end, we are searching for ambitious, motivated, detail-oriented individuals looking to further a career in Implementation Engineering. The right candidate will be driven by customer happiness, be obsessed with continuous improvement, and have strong data engineering and ETL toolkit to master the vast landscape of unique implementation projects we encounter.
  Summary of Role & Responsibilities
  The Data Engineer (Software Implementation) will primarily be responsible for writing and deploying custom data conversions, working in concert with technical project managers and architects. Sample responsibilities include:
  Data Engineering
 
   Deliver custom database conversions during new customer implementation projects
   Migrate data from competitor solutions to our in-house software
   Upgrade existing customers from our previous Oracle/VB6 software version to our current SQL Server/C#/.Net product
   Assist with the development of ad hoc T-SQL scripts for occasional data engineering needs
   Collaborate with the Continuous Improvement team to streamline future processes, automate repeatable tasks, improve quality assurance, and minimize rework
   Develop custom routines to convert graphical building drawings from various formats, including (but not limited to) Traverse, SVG, AutoCAD DXF, and binary, to our proprietary Sketch XML format
   Create and alter client analytics dashboards using PowerBI
   Embrace agile methodologies to achieve industry-leading project delivery schedules while maintaining ongoing customer engagement and excitement
   Partner with project teams to provide a seamless customer transition experience
 
 
  Role Evolution
  This role will begin with a primary focus on municipal assessment software implementation projects (~90% of the time), with the remaining 10% focused on continuous improvement initiatives and team growth. In addition, role evolution may include calibration and modeling efforts, branching into our SaaS product world, advanced data engineering opportunities, and exciting continuous improvement projects.
  Who We Are Looking For
  The ideal person for this role will have demonstrated the following abilities and traits:
  Skills and Ambitions:
 
   Ability to thrive in a fast-paced, ever-changing landscape
   5 + years of experience working in a SQL-focused conversion role
   Solid knowledge of T-SQL and SQL Server Management Studio
   Familiarity with Oracle SQL*Plus is desired but not required
   Experience with ETL and Data Migration
   Understanding of relational database concepts
   Impressive oral and written communication skills
   Exceptional time management capabilities; deadlines are not flexible
   Ability to communicate and translate ideas between technical and non-technical parties
   SQL Server 2012+/SSRS/SSIS/Jira/GitHub/Confluence
   Skills using Apache NiFi, R, or Python are a plus but not required
   BS/BA in Computer Science or related field preferred
 
  Total Compensation Package: To be discussed
  Benefits Package: Vision offers health, dental, and vision plans, as well as a 401(k)-matching program.
  Work Location: Remote
  Equal Employment Opportunity 
 Vision Government Solutions is an Equal Opportunity Employer and committed to a diverse and inclusive workplace. All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law. 
 We're proud to be an equal opportunity employer and celebrate our employees' differences, including race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability and Veteran status. 
 Vision Government Solutions maintains a drug-free workplace.
  
 R8YtQTsBA6","<div>
 <p><b>About Vision</b></p>
 <p> Vision Government Solutions is a leading government technology firm providing cutting-edge software to the public sector. We are at an incredible inflection point of growth and are looking for exceptional individuals to join our Software Implementation team to help us successfully welcome new communities to our Vision software.</p>
 <p> Our software implementation philosophy emphasizes the importance of customer delight, speed, and long-term partnership. To that end, we are searching for ambitious, motivated, detail-oriented individuals looking to further a career in Implementation Engineering. The right candidate will be driven by customer happiness, be obsessed with continuous improvement, and have strong data engineering and ETL toolkit to master the vast landscape of unique implementation projects we encounter.</p>
 <p><b> Summary of Role &amp; Responsibilities</b></p>
 <p> The Data Engineer (Software Implementation) will primarily be responsible for writing and deploying custom data conversions, working in concert with technical project managers and architects. Sample responsibilities include:</p>
 <p><b> Data Engineering</b></p>
 <ul>
  <li> Deliver custom database conversions during new customer implementation projects</li>
  <li> Migrate data from competitor solutions to our in-house software</li>
  <li> Upgrade existing customers from our previous Oracle/VB6 software version to our current SQL Server/C#/.Net product</li>
  <li> Assist with the development of ad hoc T-SQL scripts for occasional data engineering needs</li>
  <li> Collaborate with the Continuous Improvement team to streamline future processes, automate repeatable tasks, improve quality assurance, and minimize rework</li>
  <li> Develop custom routines to convert graphical building drawings from various formats, including (but not limited to) Traverse, SVG, AutoCAD DXF, and binary, to our proprietary Sketch XML format</li>
  <li> Create and alter client analytics dashboards using PowerBI</li>
  <li> Embrace agile methodologies to achieve industry-leading project delivery schedules while maintaining ongoing customer engagement and excitement</li>
  <li> Partner with project teams to provide a seamless customer transition experience</li>
 </ul>
 <p></p>
 <p><b><br> Role Evolution</b></p>
 <p> This role will begin with a primary focus on municipal assessment software implementation projects (~90% of the time), with the remaining 10% focused on continuous improvement initiatives and team growth. In addition, role evolution may include calibration and modeling efforts, branching into our SaaS product world, advanced data engineering opportunities, and exciting continuous improvement projects.</p>
 <p><b> Who We Are Looking For</b></p>
 <p> The ideal person for this role will have demonstrated the following abilities and traits:</p>
 <p><b> Skills and Ambitions:</b></p>
 <ul>
  <li> Ability to thrive in a fast-paced, ever-changing landscape</li>
  <li> 5 + years of experience working in a SQL-focused conversion role</li>
  <li> Solid knowledge of T-SQL and SQL Server Management Studio</li>
  <li> Familiarity with Oracle SQL*Plus is desired but not required</li>
  <li> Experience with ETL and Data Migration</li>
  <li> Understanding of relational database concepts</li>
  <li> Impressive oral and written communication skills</li>
  <li> Exceptional time management capabilities; deadlines are not flexible</li>
  <li> Ability to communicate and translate ideas between technical and non-technical parties</li>
  <li> SQL Server 2012+/SSRS/SSIS/Jira/GitHub/Confluence</li>
  <li> Skills using Apache NiFi, R, or Python are a plus but not required</li>
  <li> BS/BA in Computer Science or related field preferred</li>
 </ul>
 <p><b> Total Compensation Package:</b> To be discussed</p>
 <p><b> Benefits Package:</b> Vision offers health, dental, and vision plans, as well as a 401(k)-matching program.</p>
 <p><b> Work Location:</b> Remote</p>
 <p><b><i> Equal Employment Opportunity</i></b></p> 
 <p><i>Vision Government Solutions is an Equal Opportunity Employer and committed to a diverse and inclusive workplace. All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.</i></p> 
 <p><i>We&apos;re proud to be an equal opportunity employer and celebrate our employees&apos; differences, including race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability and Veteran status.</i></p> 
 <p><i>Vision Government Solutions maintains a drug-free workplace.</i></p>
 <p> </p>
 <p>R8YtQTsBA6</p>
</div>","https://www.indeed.com/applystart?jk=8a288fca6ecd6019&from=vj&pos=top&mvj=0&spon=0&sjdu=YmZE5d5THV8u75cuc0H6Y26AwfY51UOGmh3Z9h4OvXj2SygBTdTHT3n0pV4DcbDYnUUKYs5yKqp3Fg7KgmoxhA&vjfrom=serp&astse=d34b37c2c8a53463&assa=1398","8a288fca6ecd6019",,"Full-time",,"Remote","Data Engineer","4 days ago","2023-10-21T11:48:11.779Z","3.4","13",,"2023-10-25T11:48:11.781Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=8a288fca6ecd6019&from=jasx&tk=1hdjaj11gi6lr801&vjs=3"
"OneStudyTeam","At OneStudyTeam (a Reify Health company), we specialize in speeding up clinical trials and increasing the chance of new therapies being approved with the ultimate goal of improving patient outcomes. Our cloud-based platform, StudyTeam, brings research site workflows online and enables sites, sponsors, and other key stakeholders to work together more effectively. StudyTeam is trusted by the largest global biopharmaceutical companies, used in over 6,000 research sites, and is available in over 100 countries. Join us in our mission to advance clinical research and improve patient care. 
   One mission. One team. That’s OneStudyTeam.
 
  Our unique, rapidly growing data streams are enabling novel opportunities to manage clinical trials more efficiently and predictably. The Data Engineering team is looking for talented Senior Data Engineers to build, expand, and support a cutting-edge data architecture which is the analytical backbone of our company. If you are empathetic, business-driven, and want to use your data engineering and data architecture skills to make a tangible impact in the clinical research community then this may be the role for you. 
  We're looking for people who can effectively balance rapid execution and delivery with sustainable and scalable architectural initiatives to serve the business most effectively. You have strong opinions, weakly held, and while well-versed technically know when to choose the right tool, for the right job, at the right level of complexity. You will work closely with the rest of our Data Engineering team and our Data Science teams to help collect, stream, transform, and effectively manage data for integration into critical reporting, data visualizations, and ModelOps systems for emerging data science products. 
  What You’ll Be Working On 
 
  Build reliable and robust data integrations with external partners 
  Supporting the development and expansion of modern, privacy-aware, data warehouse and data mesh architectures 
  Helping to build, manage, orchestrate, and integrate streaming data sources, data lakes, ELT processes, columnar storage systems, and distributed query execution solutions 
  Establishing proactive data quality/freshness dashboards, monitoring, alerting, and anomaly remediation systems 
  Building practical data onboarding tooling and process automation solutions 
  Learning to effectively understand and deftly navigate the global compliance ecosystem (HIPAA, GDPR, etc.) to ensure your work respects the rights, regulations, and consent preferences of all stakeholders, including historical underserved or underrepresented populations 
  Developing a deep understanding of the clinical ecosystem, our products, and our business and how they all uniquely interact to help people 
 
 What You’ll Bring to OneStudyTeam 
 
  4+ years of experience successfully developing and deploying data pipelines and distributed architectures, ideally in a space similar to ours (startup, healthcare, regulated data) 
  Hands-on experience implementing ETL/ELT best practices at scale and demonstrated practical experience or familiarity with a good portion of our stack, including: AWS services (Redshift, MSK, Lambda, ECS, ECR, EC2, Glue, Quicksight, Spectrum, S3, etc.), Postgres, dbt, Kafka, Prefect, Docker, Terraform 
  Excellent programming skills in Python and deep comfort with SQL. Clojure experience is also highly appreciated 
  Experience or interest in developing and managing enterprise-scale data, distributed data architectures 
  Able to independently ship medium-to-large features and start to support or participate in architectural design 
  Excellent written and verbal communication skills 
  Strong attention to detail is key, especially when considering correctness, security, and compliance 
  Solid software testing, documentation, and debugging practices in the context of distributed systems 
 
 
   Learn more about our global benefits offerings on our careers site: https://careers.onestudyteam.com/us-benefits 
   We value diversity and believe the unique contributions each of us brings drives our success. We do not discriminate on the basis of race, sex, religion, color, national origin, gender identity, age, marital status, veteran status, or disability status. 
   Note: OneStudyTeam is unable to sponsor work visas at this time. If you are a non-U.S. resident applicant, please note that OST works with a Professional Employer Organization. 
   As a condition of employment, you will abide by all organizational security and privacy policies. 
   For a detailed overview of OneStudyTeam's candidate privacy policy, please visit https://careers.onestudyteam.com/candidate-privacy-policy. This organization participates in E-Verify (E-Verify's Right to Work guidance can be found here).","<div>
 <div>
  <p>At OneStudyTeam (a Reify Health company), we specialize in speeding up clinical trials and increasing the chance of new therapies being approved with the ultimate goal of improving patient outcomes. Our cloud-based platform, StudyTeam, brings research site workflows online and enables sites, sponsors, and other key stakeholders to work together more effectively. StudyTeam is trusted by the largest global biopharmaceutical companies, used in over 6,000 research sites, and is available in over 100 countries. Join us in our mission to advance clinical research and improve patient care.</p> 
  <p><b> One mission. One team. That&#x2019;s OneStudyTeam.</b></p>
 </div>
 <p> Our unique, rapidly growing data streams are enabling novel opportunities to manage clinical trials more efficiently and predictably. The Data Engineering team is looking for talented <b>Senior Data Engineers</b> to build, expand, and support a cutting-edge data architecture which is the analytical backbone of our company. If you are empathetic, business-driven, and want to use your data engineering and data architecture skills to make a tangible impact in the clinical research community then this may be the role for you.</p> 
 <p> We&apos;re looking for people who can effectively balance rapid execution and delivery with sustainable and scalable architectural initiatives to serve the business most effectively. You have strong opinions, weakly held, and while well-versed technically know when to choose the right tool, for the right job, at the right level of complexity. You will work closely with the rest of our Data Engineering team and our Data Science teams to help collect, stream, transform, and effectively manage data for integration into critical reporting, data visualizations, and ModelOps systems for emerging data science products.</p> 
 <h3 class=""jobSectionHeader""><b> What You&#x2019;ll Be Working On</b></h3> 
 <ul>
  <li>Build reliable and robust data integrations with external partners</li> 
  <li>Supporting the development and expansion of modern, privacy-aware, data warehouse and data mesh architectures</li> 
  <li>Helping to build, manage, orchestrate, and integrate streaming data sources, data lakes, ELT processes, columnar storage systems, and distributed query execution solutions</li> 
  <li>Establishing proactive data quality/freshness dashboards, monitoring, alerting, and anomaly remediation systems</li> 
  <li>Building practical data onboarding tooling and process automation solutions</li> 
  <li>Learning to effectively understand and deftly navigate the global compliance ecosystem (HIPAA, GDPR, etc.) to ensure your work respects the rights, regulations, and consent preferences of all stakeholders, including historical underserved or underrepresented populations</li> 
  <li>Developing a deep understanding of the clinical ecosystem, our products, and our business and how they all uniquely interact to help people</li> 
 </ul>
 <h3 class=""jobSectionHeader""><b>What You&#x2019;ll Bring to OneStudyTeam</b></h3> 
 <ul>
  <li>4+ years of experience successfully developing and deploying data pipelines and distributed architectures, ideally in a space similar to ours (startup, healthcare, regulated data)</li> 
  <li>Hands-on experience implementing ETL/ELT best practices at scale and demonstrated practical experience or familiarity with a good portion of our stack, including: AWS services (Redshift, MSK, Lambda, ECS, ECR, EC2, Glue, Quicksight, Spectrum, S3, etc.), Postgres, dbt, Kafka, Prefect, Docker, Terraform</li> 
  <li>Excellent programming skills in Python and deep comfort with SQL. Clojure experience is also highly appreciated</li> 
  <li>Experience or interest in developing and managing enterprise-scale data, distributed data architectures</li> 
  <li>Able to independently ship medium-to-large features and start to support or participate in architectural design</li> 
  <li>Excellent written and verbal communication skills</li> 
  <li>Strong attention to detail is key, especially when considering correctness, security, and compliance</li> 
  <li>Solid software testing, documentation, and debugging practices in the context of distributed systems</li> 
 </ul>
 <div>
  <p> Learn more about our global benefits offerings on our careers site: https://careers.onestudyteam.com/us-benefits</p> 
  <p><i> We value diversity and believe the unique contributions each of us brings drives our success. We do not discriminate on the basis of race, sex, religion, color, national origin, gender identity, age, marital status, veteran status, or disability status.</i></p> 
  <p><b><i> Note</i></b><i>: OneStudyTeam</i><i> is unable to sponsor work visas at this time. If you are a non-U.S. resident applicant, please note that OST works with a Professional Employer Organization.</i></p> 
  <p><i> As a condition of employment, you will abide by all organizational security and privacy policies.</i></p> 
  <p><i> For a detailed overview of OneStudyTeam&apos;s candidate privacy policy, please visit</i><i> </i><i>https://careers.onestudyteam.com/candidate-privacy-policy</i><i>. This organization participates in </i><i>E-Verify</i><i> (E-Verify&apos;s Right to Work guidance can be found </i><i>here</i><i>).</i></p>
 </div>
</div>","https://careers.onestudyteam.com/jobs/bbfa1bc3-dc1c-48eb-8dd4-9b4cfb58d6f0?source=indeed","38af24928b1158e6",,,,"Remote","Senior Data Engineer","4 days ago","2023-10-21T11:48:13.172Z",,,,"2023-10-25T11:48:13.173Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=38af24928b1158e6&from=jasx&tk=1hdjaj11gi6lr801&vjs=3"
"InfoMagnus","Data Governance Engineer
This is a contract remote role must reside in US.
We are people-oriented technologists, analysts and designers helping companies solve complex business problems with technology.
InfoMagnus was built to provide an environment where passion is celebrated, personal time is respected and hard work is rewarded.
Our Beliefs:
Integrity, dignity and respect in every interaction. Commitment to our employees. Trust and responsibility in all relationships. Giving back to our community.
Job Description:
We are looking for a skilled Data Governance Engineer who is well-versed in data governance principles and practices, with expertise in utilizing data quality platforms like Anomalo or data catalog platforms like Alation. In this role, you will play a crucial part in ensuring the integrity, security, and quality of our data assets.
Key Responsibilities:
· Collaborate with cross-functional teams to establish and maintain data governance policies, standards, and procedures.
· Implement and manage data quality checks and data lineage using Anomalo or Alation.
· Design and execute data quality audits and assessments to identify and resolve data issues.
· Develop and maintain data cataloging and metadata management processes.
Qualifications:
· Bachelor’s degree in Computer Science, Information Systems, or a related field. Master's degree preferred.
· Proven experience in data governance and data quality management.
· Proficiency in utilizing data quality platforms like Anomalo or data catalog platform like Alation.
· Strong understanding of data governance frameworks, policies and procedures.
· Excellent problem-solving skills and attention to detail.
· Strong clear communication skills and interpersonal skills to collaborate with various teams.
Equal Opportunity Employer
Job Type: Contract
Pay: $60.00 - $70.00 per hour
Experience level:

 3 years

Schedule:

 Monday to Friday

Application Question(s):

 Do you have an employer that sponsors

your Visa?

 Do you have experience using the data catalog platform Alation?
 Do you have experience using the data quality platform Anomalo?

Work Location: Remote","<p><b>Data Governance Engineer</b></p>
<p><i><b>This is a contract remote role must reside in US</b></i><i>.</i></p>
<p>We are people-oriented technologists, analysts and designers helping companies solve complex business problems with technology.</p>
<p>InfoMagnus was built to provide an environment where passion is celebrated, personal time is respected and hard work is rewarded.</p>
<p>Our Beliefs:</p>
<p>Integrity, dignity and respect in every interaction. Commitment to our employees. Trust and responsibility in all relationships. Giving back to our community.</p>
<p><b>Job Description:</b></p>
<p>We are looking for a skilled Data Governance Engineer who is well-versed in data governance principles and practices, with expertise in utilizing data quality platforms like <b>Anomalo</b> or data catalog platforms like <b>Alation</b>. In this role, you will play a crucial part in ensuring the integrity, security, and quality of our data assets.</p>
<p><b>Key Responsibilities:</b></p>
<p>&#xb7; Collaborate with cross-functional teams to establish and maintain data governance policies, standards, and procedures.</p>
<p>&#xb7; Implement and manage data quality checks and data lineage using <b>Anomalo</b> or <b>Alation</b>.</p>
<p>&#xb7; Design and execute data quality audits and assessments to identify and resolve data issues.</p>
<p>&#xb7; Develop and maintain data cataloging and metadata management processes.</p>
<p><b>Qualifications:</b></p>
<p>&#xb7; Bachelor&#x2019;s degree in Computer Science, Information Systems, or a related field. Master&apos;s degree preferred.</p>
<p>&#xb7; Proven experience in data governance and data quality management.</p>
<p>&#xb7; Proficiency in utilizing data quality platforms like <b>Anomalo</b> or data catalog platform like <b>Alation.</b></p>
<p>&#xb7; Strong understanding of data governance frameworks, policies and procedures.</p>
<p>&#xb7; Excellent problem-solving skills and attention to detail.</p>
<p>&#xb7; Strong clear communication skills and interpersonal skills to collaborate with various teams.</p>
<p>Equal Opportunity Employer</p>
<p>Job Type: Contract</p>
<p>Pay: &#x24;60.00 - &#x24;70.00 per hour</p>
<p>Experience level:</p>
<ul>
 <li>3 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>Monday to Friday</li>
</ul>
<p>Application Question(s):</p>
<ul>
 <li>Do you have an employer that sponsors</li>
</ul>
<p>your Visa?</p>
<ul>
 <li>Do you have experience using the data catalog platform Alation?</li>
 <li>Do you have experience using the data quality platform Anomalo?</li>
</ul>
<p>Work Location: Remote</p>",,"19d5fafc9fb74494",,"Contract",,"Remote","Data Governance Engineer","3 days ago","2023-10-22T11:48:22.015Z",,,"$60 - $70 an hour","2023-10-25T11:48:22.019Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=19d5fafc9fb74494&from=jasx&tk=1hdjaj0h4j4gl800&vjs=3"
"ServiceNow","Company Description
  At ServiceNow, our technology makes the world work for everyone, and our people make it possible. We move fast because the world can’t wait, and we innovate in ways no one else can for our customers and communities. By joining ServiceNow, you are part of an ambitious team of change makers who have a restless curiosity and a drive for ingenuity. We know that your best work happens when you live your best life and share your unique talents, so we do everything we can to make that possible. We dream big together, supporting each other to make our individual and collective dreams come true. The future is ours, and it starts with you. 
 With more than 7,700+ customers, we serve approximately 85% of the Fortune 500®, and we're proud to be one of FORTUNE 100 Best Companies to Work For® and World's Most Admired Companies™. 
 Learn more on Life at Now blog and hear from our employees about their experiences working at ServiceNow. 
 Unsure if you meet all the qualifications of a job description but are deeply excited about the role? We still encourage you to apply! At ServiceNow, we are committed to creating an inclusive environment where all voices are heard, valued, and respected. We welcome all candidates, including individuals from non-traditional, varied backgrounds, that might not come from a typical path connected to this role. We believe skills and experience are transferrable, and the desire to dream big makes for great candidates.
  Job Description
  You will be part of the ServiceNow Cloud Services Big Data Team. The Big Data team is building the next-generation platform that collects, stores and provides real-time access to large amounts of data.
  You will be driving the design and implementation of ServiceNow in-house real-time data visualization and analytics platform to support the growth of ServiceNow to 10B+ revenue.
  What you get to do in this role:
 
   Bring your innovation and experience in designing and developing the next generation data analytics platform using cutting-edge technologies.
   Standardize processes for complete development cycle including design, implementation, unit testing, code review, testing automation etc.
   Research and adopt the right technologies to improve the scalability and productivity of the engineering group.
   Work closely with key stakeholders and product owners to drive technical design for requirements of various use cases.
   Coordinate with cross-function teams (DevOps, network, QA, etc) to ensure a smooth cycle from development to deployment.
 
  
  Qualifications
  To be successful in this role you have:
 
   Expert-level skills JavaScript, NodeJS, Webpack, ReactJS or other modern UI frameworks.
   4+ years of software development experience & strong troubleshooting and debugging skills
   Experience with data analytics, data visualization, BI tools and Hadoop ecosystem.
   Ability to drive projects end to end.
   Ability to produce high-quality software that is unit tested, code reviewed, and checked in regularly for continuous integration.
   Familiarity with backend Restful API development, preferable in Java
   Solid background in complicated SQL & data analytics
   Zeal for learning and adopting new ideas and patterns
   Strong Computer Science fundamentals, data structures, algorithms, and software design.
 
  GCS-23
  For positions in the Bay Area, we offer a base pay of $133,300 - $226,700, plus equity (when applicable), variable/incentive compensation and benefits. Sales positions generally offer a competitive On Target Earnings (OTE) incentive compensation structure. Please note that the base pay shown is a guideline, and individual total compensation will vary based on factors such as qualifications, skill level, competencies and work location. We also offer health plans, including flexible spending accounts, a 401(k) Plan with company match, ESPP, matching donations, a flexible time away plan and family leave programs (subject to eligibility requirements). Compensation is based on the geographic location in which the role is located, and is subject to change based on work location.
  Additional Information
  ServiceNow is an Equal Employment Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, creed, religion, sex, sexual orientation, national origin or nationality, ancestry, age, disability, gender identity or expression, marital status, veteran status or any other category protected by law. 
 At ServiceNow, we lead with flexibility and trust in our distributed world of work. Click here to learn about our work personas: flexible, remote and required-in-office. 
 If you require a reasonable accommodation to complete any part of the application process, or are limited in the ability or unable to access or use this online application process and need an alternative method for applying, you may contact us at talent.acquisition@servicenow.com for assistance. 
 For positions requiring access to technical data subject to export control regulations, including Export Administration Regulations (EAR), ServiceNow may have to obtain export licensing approval from the U.S. Government for certain individuals. All employment is contingent upon ServiceNow obtaining any export license or other approval that may be required by the U.S. Government. 
 Please Note: Fraudulent job postings/job scams are increasingly common. Click here to learn what to watch out for and how to protect yourself. All genuine ServiceNow job postings can be found through the ServiceNow Careers site.
  
 From Fortune. © 2022 Fortune Media IP Limited All rights reserved. Used under license. 
 Fortune and Fortune Media IP Limited are not affiliated with, and do not endorse products or services of, ServiceNow.","<div>
 <b>Company Description</b>
 <p><br> At ServiceNow, our technology makes the world work for everyone, and our people make it possible. We move fast because the world can&#x2019;t wait, and we innovate in ways no one else can for our customers and communities. By joining ServiceNow, you are part of an ambitious team of change makers who have a restless curiosity and a drive for ingenuity. We know that your best work happens when you live your best life and share your unique talents, so we do everything we can to make that possible. We dream big together, supporting each other to make our individual and collective dreams come true. The future is ours, and it starts with you.</p> 
 <p>With more than 7,700+ customers, we serve approximately 85% of the Fortune 500&#xae;, and we&apos;re proud to be one of FORTUNE 100 Best Companies to Work For&#xae; and World&apos;s Most Admired Companies&#x2122;.</p> 
 <p>Learn more on Life at Now blog and hear from our employees about their experiences working at ServiceNow.</p> 
 <p>Unsure if you meet all the qualifications of a job description but are deeply excited about the role? We still encourage you to apply! At ServiceNow, we are committed to creating an inclusive environment where all voices are heard, valued, and respected. We welcome all candidates, including individuals from non-traditional, varied backgrounds, that might not come from a typical path connected to this role. We believe skills and experience are transferrable, and the desire to dream big makes for great candidates.</p>
 <b><br> Job Description</b>
 <p><br> You will be part of the ServiceNow Cloud Services Big Data Team. The Big Data team is building the next-generation platform that collects, stores and provides real-time access to large amounts of data.</p>
 <p> You will be driving the design and implementation of ServiceNow in-house real-time data visualization and analytics platform to support the growth of ServiceNow to 10B+ revenue.</p>
 <p><b> What you get to do in this role:</b></p>
 <ul>
  <li> Bring your innovation and experience in designing and developing the next generation data analytics platform using cutting-edge technologies.</li>
  <li> Standardize processes for complete development cycle including design, implementation, unit testing, code review, testing automation etc.</li>
  <li> Research and adopt the right technologies to improve the scalability and productivity of the engineering group.</li>
  <li> Work closely with key stakeholders and product owners to drive technical design for requirements of various use cases.</li>
  <li> Coordinate with cross-function teams (DevOps, network, QA, etc) to ensure a smooth cycle from development to deployment.</li>
 </ul>
 <br> 
 <b> Qualifications</b>
 <p><b><br> To be successful in this role you have:</b></p>
 <ul>
  <li> Expert-level skills JavaScript, NodeJS, Webpack, ReactJS or other modern UI frameworks.</li>
  <li> 4+ years of software development experience &amp; strong troubleshooting and debugging skills</li>
  <li> Experience with data analytics, data visualization, BI tools and Hadoop ecosystem.</li>
  <li> Ability to drive projects end to end.</li>
  <li> Ability to produce high-quality software that is unit tested, code reviewed, and checked in regularly for continuous integration.</li>
  <li> Familiarity with backend Restful API development, preferable in Java</li>
  <li> Solid background in complicated SQL &amp; data analytics</li>
  <li> Zeal for learning and adopting new ideas and patterns</li>
  <li> Strong Computer Science fundamentals, data structures, algorithms, and software design.</li>
 </ul>
 <p> GCS-23</p>
 <p> For positions in the Bay Area, we offer a base pay of &#x24;133,300 - &#x24;226,700, plus equity (when applicable), variable/incentive compensation and benefits. Sales positions generally offer a competitive On Target Earnings (OTE) incentive compensation structure. Please note that the base pay shown is a guideline, and individual total compensation will vary based on factors such as qualifications, skill level, competencies and work location. We also offer health plans, including flexible spending accounts, a 401(k) Plan with company match, ESPP, matching donations, a flexible time away plan and family leave programs (subject to eligibility requirements). Compensation is based on the geographic location in which the role is located, and is subject to change based on work location.</p>
 <b><br> Additional Information</b>
 <p><br> ServiceNow is an Equal Employment Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, creed, religion, sex, sexual orientation, national origin or nationality, ancestry, age, disability, gender identity or expression, marital status, veteran status or any other category protected by law.</p> 
 <p>At ServiceNow, we lead with flexibility and trust in our distributed world of work. Click here to learn about our work personas: flexible, remote and required-in-office.</p> 
 <p>If you require a reasonable accommodation to complete any part of the application process, or are limited in the ability or unable to access or use this online application process and need an alternative method for applying, you may contact us at talent.acquisition@servicenow.com for assistance.</p> 
 <p>For positions requiring access to technical data subject to export control regulations, including Export Administration Regulations (EAR), ServiceNow may have to obtain export licensing approval from the U.S. Government for certain individuals. All employment is contingent upon ServiceNow obtaining any export license or other approval that may be required by the U.S. Government.</p> 
 <p>Please Note: Fraudulent job postings/job scams are increasingly common. Click here to learn what to watch out for and how to protect yourself. All genuine ServiceNow job postings can be found through the ServiceNow Careers site.</p>
 <p><br> </p>
 <p>From Fortune. &#xa9; 2022 Fortune Media IP Limited All rights reserved. Used under license.</p> 
 <p>Fortune and Fortune Media IP Limited are not affiliated with, and do not endorse products or services of, ServiceNow.</p>
</div>","https://www.indeed.com/rc/clk?jk=e43d9fc0f405d53b&atk=&xpse=SoBs67I3JzdeytgH3j0LbzkdCdPP","e43d9fc0f405d53b",,"Full-time",,"2225 Lawson Ln, Santa Clara, CA 95054","Sr Software Engineer - Front End - Big Data","1 day ago","2023-10-24T11:48:26.482Z","3.7","241","$133,300 - $226,700 a year","2023-10-25T11:48:26.485Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=e43d9fc0f405d53b&from=jasx&tk=1hdjaj0h4j4gl800&vjs=3"
"TEKletics","BUSINESS INTELLIGENCE (ETL)
Job Responsibilities:The successful candidate will work with high level BI-ETL developers to process data from various source systems into an Enterprise Data Warehouse.
Primary duties include, but are not limited to:

 Building/interpreting design documents
 Building/interpreting mapping documents
 Building/completing test documents
 Participate in technical design reviews and code reviews
 Understand and adhere to ETL best practices
 Develop and troubleshoot ETL code
 Participate in peer review sessions

Required Skills: Comfortable working in a team environment  Strong work ethic*

 Excellent time management
 Excellent oral/written communication
 Prior experience using MS Office (word, excel, outlook)

Preferred Skills: Fundamental understanding of database concepts * Experience with UNIX scripting

 Experience with Object-Oriented Programming
 ETL experience
 Hadoop Experience
 Leadership Experience

Job Type: Full-time
Pay: $15.00 - $30.00 per hour
Benefits:

 Paid time off

Schedule:

 8 hour shift

COVID-19 considerations:All employees are currently working remotely
Experience:

 SQL: 1 year (Preferred)
 Python: 1 year (Preferred)
 Azure: 1 year (Preferred)

Work Location: Remote","<p><b>BUSINESS INTELLIGENCE (ETL)</b></p>
<p><b>Job Responsibilities:</b>The successful candidate will work with high level BI-ETL developers to process data from various source systems into an Enterprise Data Warehouse.</p>
<p><b>Primary duties include, but are not limited to:</b></p>
<ul>
 <li>Building/interpreting design documents</li>
 <li>Building/interpreting mapping documents</li>
 <li>Building/completing test documents</li>
 <li>Participate in technical design reviews and code reviews</li>
 <li>Understand and adhere to ETL best practices</li>
 <li>Develop and troubleshoot ETL code</li>
 <li>Participate in peer review sessions</li>
</ul>
<p><b>Required Skills:</b> <b>Comfortable working in a team environment </b> Strong work ethic<br>*</p>
<ul>
 <li>Excellent time management</li>
 <li>Excellent oral/written communication</li>
 <li>Prior experience using MS Office (word, excel, outlook)</li>
</ul>
<p>Preferred Skills: Fundamental understanding of database concepts * Experience with UNIX scripting</p>
<ul>
 <li>Experience with Object-Oriented Programming</li>
 <li>ETL experience</li>
 <li>Hadoop Experience</li>
 <li>Leadership Experience</li>
</ul>
<p>Job Type: Full-time</p>
<p>Pay: &#x24;15.00 - &#x24;30.00 per hour</p>
<p>Benefits:</p>
<ul>
 <li>Paid time off</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
</ul>
<p>COVID-19 considerations:<br>All employees are currently working remotely</p>
<p>Experience:</p>
<ul>
 <li>SQL: 1 year (Preferred)</li>
 <li>Python: 1 year (Preferred)</li>
 <li>Azure: 1 year (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,"5de691f5f49787f9",,"Full-time",,"Remote","Junior Data Engineer","5 days ago","2023-10-20T11:48:30.733Z","2.5","2","$15 - $30 an hour","2023-10-25T11:48:30.736Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=5de691f5f49787f9&from=jasx&tk=1hdjak60ij4ij800&vjs=3"
"Concentrix","Job Title: Big Data Engineer Job Ref #: 981496
 
  Job Description Concentrix CVG Customer Management Group Inc., Cincinnati OH, has multiple openings for the position of Big Data Engineer. Work will be performed in various unanticipated locations throughout the U.S. Travel and/or relocation is required. Telecommuting may be permitted.
  The Big Data Engineer will write, update, and maintain software applications; perform production maintenance of code; gather solutions requirements. Own technical commitments to clients and work with the team to successful delivery of solutions. Analyze, design, and code for complex requirements as well as write programs of complexity. Responsible for defining problems, collecting data, establishing facts, drawing valid conclusions, and preparing appropriate reports.
 
  The position requires a Master’s degree in Computer Science, Engineering (any), or any technical/analytical field that is closely related to the specialty, plus knowledge of: HTML5, CSS3, MySQL, and jQuery.
  To apply, send resume to ctlyst_postings@concentrix.com with Job Ref# 981496 in the subject line of the email.
 
  Location: USA, OH, Work-at-Home
 
  Language Requirements:
 
  Time Type:
  If you are a California resident, by submitting your information, you acknowledge that you have read and have access to the Job Applicant Privacy Notice for California Residents
  Concentrix is an Equal Opportunity/Affirmative Action Employer including Disabled/Vets.
 
  For more information regarding your EEO rights as an applicant, please visit the following websites: 
 
  English
  Spanish
 
 
  To request a reasonable accommodation please click here.
 
  If you wish to review the Affirmative Action Plan, please click here.","<p></p>
<div>
 <p>Job Title:</p> Big Data Engineer Job Ref #: 981496
 <p></p>
 <p> Job Description</p> Concentrix CVG Customer Management Group Inc., Cincinnati OH, has multiple openings for the position of Big Data Engineer. Work will be performed in various unanticipated locations throughout the U.S. Travel and/or relocation is required. Telecommuting may be permitted.
 <p> The Big Data Engineer will write, update, and maintain software applications; perform production maintenance of code; gather solutions requirements. Own technical commitments to clients and work with the team to successful delivery of solutions. Analyze, design, and code for complex requirements as well as write programs of complexity. Responsible for defining problems, collecting data, establishing facts, drawing valid conclusions, and preparing appropriate reports.</p>
 <p></p>
 <p> The position requires a Master&#x2019;s degree in Computer Science, Engineering (any), or any technical/analytical field that is closely related to the specialty, plus knowledge of: HTML5, CSS3, MySQL, and jQuery.</p>
 <p> To apply, send resume to ctlyst_postings@concentrix.com with Job Ref# 981496 in the subject line of the email.</p>
 <p></p>
 <p> Location:</p> USA, OH, Work-at-Home
 <p></p>
 <p> Language Requirements:</p>
 <p></p>
 <p> Time Type:</p>
 <p><b> If you are a California resident, by submitting your information, you acknowledge that you have read and have access to the </b><b>Job Applicant Privacy Notice for California Residents</b></p>
 <p><br> Concentrix is an Equal Opportunity/Affirmative Action Employer including Disabled/Vets.</p>
 <p></p>
 <p> For more information regarding your EEO rights as an applicant, please visit the following websites: </p>
 <ul>
  <li>English</li>
  <li>Spanish</li>
 </ul>
 <p></p>
 <p> To request a reasonable accommodation please click here.</p>
 <p></p>
 <p> If you wish to review the Affirmative Action Plan, please click here.</p>
</div>
<p></p>","https://www.indeed.com/rc/clk?jk=2ba6006821dec6ec&atk=&xpse=SoB567I3Jzdeq2SQCR0LbzkdCdPP","2ba6006821dec6ec",,,,"Remote","Big Data Engineer Job Ref #: 981496","4 days ago","2023-10-21T11:48:21.563Z","3.4","31697",,"2023-10-25T11:48:21.565Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=2ba6006821dec6ec&from=jasx&tk=1hdjaj0h4j4gl800&vjs=3"
"Olsson","Company Description
  We are Olsson, a team-based, purpose-driven engineering and design firm. Our solutions improve communities and our people make it possible.
  Our most meaningful asset is our people, and we are dedicated to providing an environment where they can continue to learn, grow, and thrive. Our entrepreneurial spirit is what has allowed us — and will continue to allow us — to grow. The result? Inspired people, amazing designs, and projects with purpose.
 
 

 Job Description
  Olsson provides multidisciplinary design services for some of the largest and most forward-thinking and desirable companies in the world to work for. The large hyperscale data center campuses we design throughout the U.S. will give you the opportunity to work on some of the largest and most complex engineering-driven projects being built today. Our clients are relationship based and truly value the work we do for them, affording us the opportunity to contribute to society’s technological and connected community through the design of the critical infrastructure that is the foundation of these projects.
  As an experienced engineer on our Data Center Civil Team, you will be a part of the firm’s largest and most complex projects. You will serve as a project manager on some projects and lead design engineer on others. Prepare planning and design documents, process design calculations, and develop and maintain team and client standards. You may lead quality assurance/quality control and act as an advisor on complex projects. You will also coordinate with other Olsson teams, professional staff, technical staff, clients, and other consultants.
  You may travel to job sites for observation and attend client meetings.
 
  Olsson currently has one opportunity for a Senior Civil Engineer. This role offers flexible work options, including remote and hybrid opportunities, to accommodate diverse working preferences and promote work-life balance. Candidates can live in Lincoln, Omaha, Overland Park or Dallas-Fort Worth area and work remotely or work out of any Olsson office location in these regions/areas.
 
 
 

 Qualifications
  You are passionate about:
 
   Working collaboratively with others
   Having ownership in the work you do
   Using your talents to positively affect communities
   Solving problems
   Providing excellence in client service
 
  You bring to the team:
 
   Strong communication skills
   Ability to contribute and work well on a team
   Bachelor's Degree in civil engineering
   At least 6 years of related civil engineering experience
   Proficient in Civil 3D software
   Must be a registered professional engineer
 
  Additional Information
  Olsson is a nationally recognized, employee-owned firm specializing in planning and design, engineering, field services, environmental, and technology. Founded in 1956 on the very mindset that drives us today, we’re here to improve communities by making them more sustainable, better connected, and more efficient. Simply put, we work to leave the world better than we found it.
  As an Olsson employee, you’ll receive our traditional benefits package (health care, vision, dental, paid time off, etc.), plus you’ll:
 
   Become an owner in the company after your first year through our Employee Stock Ownership Plan (ESOP)
   Engage in work that has a positive impact in communities
   Receive an excellent 401(k) match
   Participate in a wellness program promoting balanced lifestyles
   Benefit from a bonus system that rewards performance
   Have the possibility for flexible work arrangements
 
  Olsson is an EEO employer. We encourage qualified minority, female, veteran and disabled candidates to apply and be considered for open positions. We do not discriminate against any applicant for employment, or any employee because of race, color, religion, national origin, age, sex, sexual orientation, gender identity, gender, disability, age, or military status.
  #LI-LA1
  #LI-Remote","<div>
 Company Description
 <p><br> We are Olsson, a team-based, purpose-driven engineering and design firm. Our solutions improve communities and our people make it possible.</p>
 <p> Our most meaningful asset is our people, and we are dedicated to providing an environment where they can continue to learn, grow, and thrive. Our entrepreneurial spirit is what has allowed us &#x2014; and will continue to allow us &#x2014; to grow. The result? Inspired people, amazing designs, and projects with purpose.</p>
</div> 
<br> 
<div>
 Job Description
 <p><br> Olsson provides multidisciplinary design services for some of the largest and most forward-thinking and desirable companies in the world to work for. The large hyperscale data center campuses we design throughout the U.S. will give you the opportunity to work on some of the largest and most complex engineering-driven projects being built today. Our clients are relationship based and truly value the work we do for them, affording us the opportunity to contribute to society&#x2019;s technological and connected community through the design of the critical infrastructure that is the foundation of these projects.</p>
 <p> As an experienced engineer on our Data Center Civil Team, you will be a part of the firm&#x2019;s largest and most complex projects. You will serve as a project manager on some projects and lead design engineer on others. Prepare planning and design documents, process design calculations, and develop and maintain team and client standards. You may lead quality assurance/quality control and act as an advisor on complex projects. You will also coordinate with other Olsson teams, professional staff, technical staff, clients, and other consultants.</p>
 <p><i> You may travel to job sites for observation and attend client meetings.</i></p>
 <ul>
  <li><i>Olsson currently has one opportunity for a Senior Civil Engineer. This role offers flexible work options, including remote and hybrid opportunities, to accommodate diverse working preferences and promote work-life balance. Candidates can live in Lincoln, Omaha, Overland Park or Dallas-Fort Worth area and work remotely or work out of any Olsson office location in these regions/areas.</i></li>
 </ul>
</div> 
<br> 
<div>
 Qualifications
 <p><b><br> You are passionate about:</b></p>
 <ul>
  <li> Working collaboratively with others</li>
  <li> Having ownership in the work you do</li>
  <li> Using your talents to positively affect communities</li>
  <li> Solving problems</li>
  <li> Providing excellence in client service</li>
 </ul>
 <p><b> You bring to the team:</b></p>
 <ul>
  <li> Strong communication skills</li>
  <li> Ability to contribute and work well on a team</li>
  <li> Bachelor&apos;s Degree in civil engineering</li>
  <li> At least 6 years of related civil engineering experience</li>
  <li> Proficient in Civil 3D software</li>
  <li> Must be a registered professional engineer</li>
 </ul>
 <br> Additional Information
 <p><br> Olsson is a nationally recognized, employee-owned firm specializing in planning and design, engineering, field services, environmental, and technology. Founded in 1956 on the very mindset that drives us today, we&#x2019;re here to improve communities by making them more sustainable, better connected, and more efficient. Simply put, we work to leave the world better than we found it.</p>
 <p> As an Olsson employee, you&#x2019;ll receive our traditional benefits package (health care, vision, dental, paid time off, etc.), plus you&#x2019;ll:</p>
 <ul>
  <li> Become an owner in the company after your first year through our Employee Stock Ownership Plan (ESOP)</li>
  <li> Engage in work that has a positive impact in communities</li>
  <li> Receive an excellent 401(k) match</li>
  <li> Participate in a wellness program promoting balanced lifestyles</li>
  <li> Benefit from a bonus system that rewards performance</li>
  <li> Have the possibility for flexible work arrangements</li>
 </ul>
 <p> Olsson is an EEO employer. We encourage qualified minority, female, veteran and disabled candidates to apply and be considered for open positions. We do not discriminate against any applicant for employment, or any employee because of race, color, religion, national origin, age, sex, sexual orientation, gender identity, gender, disability, age, or military status.</p>
 <p> #LI-LA1</p>
 <p> #LI-Remote</p>
</div>","https://jobs.smartrecruiters.com/Olsson/743999938556883-senior-civil-engineer-data-center-remote-","81d8b8cee52ced8a",,"Full-time",,"601 P Street, Lincoln, NE 68508","Senior Civil Engineer - Data Center (Remote)","4 days ago","2023-10-21T11:48:35.463Z","3.2","23",,"2023-10-25T11:48:35.464Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=81d8b8cee52ced8a&from=jasx&tk=1hdjak60ij4ij800&vjs=3"
"Olsson","Company Description
  We are Olsson, a team-based, purpose-driven engineering and design firm. Our solutions improve communities and our people make it possible.
  Our most meaningful asset is our people, and we are dedicated to providing an environment where they can continue to learn, grow, and thrive. Our entrepreneurial spirit is what has allowed us — and will continue to allow us — to grow. The result? Inspired people, amazing designs, and projects with purpose.
 
 

 Job Description
  Olsson provides multidisciplinary design services for some of the largest and most forward-thinking and desirable companies in the world to work for. The large hyperscale data center campuses we design throughout the U.S. will give you the opportunity to work on some of the largest and most complex engineering-driven projects being built today. Our clients are relationship based and truly value the work we do for them, affording us the opportunity to contribute to society’s technological and connected community through the design of the critical infrastructure that is the foundation of these projects.
  As a Licensed Civil Engineer on our Data Center Civil Team, you will be a part of the firm’s largest and most complex projects. You will serve as a project manager on some projects and lead design engineer on others. Prepare planning and design documents, process design calculations, and develop and maintain team and client standards. You may lead quality assurance/quality control and act as an advisor on complex projects. You will also coordinate with other Olsson teams, professional staff, technical staff, clients, and other consultants.
  You may travel to job sites for observation and attend client meetings.
 
  Olsson currently has one opportunity for a Licensed Civil Engineer. This role offers flexible work options, including remote and hybrid opportunities, to accommodate diverse working preferences and promote work-life balance. Candidates can live in Lincoln, Omaha, Overland Park, Denver or Dallas-Fort Worth area and work remotely or work out of any Olsson office location in these regions/areas.
 
 
 

 Qualifications
  You are passionate about:
 
   Working collaboratively with others
   Having ownership in the work you do
   Using your talents to positively affect communities
   Solving problems
   Providing excellence in client service
 
  You bring to the team:
 
   Strong communication skills
   Ability to contribute and work well on a team
   Bachelor's Degree in civil engineering
   At least 6 years of related civil engineering experience
   Proficient in Civil 3D software
   Must be a registered professional engineer
 
  Additional Information
  Olsson is a nationally recognized, employee-owned firm specializing in planning and design, engineering, field services, environmental, and technology. Founded in 1956 on the very mindset that drives us today, we’re here to improve communities by making them more sustainable, better connected, and more efficient. Simply put, we work to leave the world better than we found it.
  As an Olsson employee, you’ll receive our traditional benefits package (health care, vision, dental, paid time off, etc.), plus you’ll:
 
   Become an owner in the company after your first year through our Employee Stock Ownership Plan (ESOP)
   Engage in work that has a positive impact in communities
   Receive an excellent 401(k) match
   Participate in a wellness program promoting balanced lifestyles
   Benefit from a bonus system that rewards performance
   Have the possibility for flexible work arrangements
 
  Olsson is an EEO employer. We encourage qualified minority, female, veteran and disabled candidates to apply and be considered for open positions. We do not discriminate against any applicant for employment, or any employee because of race, color, religion, national origin, age, sex, sexual orientation, gender identity, gender, disability, age, or military status.
  #LI-LA1
  #LI-Remote","<div>
 Company Description
 <p><br> We are Olsson, a team-based, purpose-driven engineering and design firm. Our solutions improve communities and our people make it possible.</p>
 <p> Our most meaningful asset is our people, and we are dedicated to providing an environment where they can continue to learn, grow, and thrive. Our entrepreneurial spirit is what has allowed us &#x2014; and will continue to allow us &#x2014; to grow. The result? Inspired people, amazing designs, and projects with purpose.</p>
</div> 
<br> 
<div>
 Job Description
 <p><br> Olsson provides multidisciplinary design services for some of the largest and most forward-thinking and desirable companies in the world to work for. The large hyperscale data center campuses we design throughout the U.S. will give you the opportunity to work on some of the largest and most complex engineering-driven projects being built today. Our clients are relationship based and truly value the work we do for them, affording us the opportunity to contribute to society&#x2019;s technological and connected community through the design of the critical infrastructure that is the foundation of these projects.</p>
 <p> As a Licensed Civil Engineer on our Data Center Civil Team, you will be a part of the firm&#x2019;s largest and most complex projects. You will serve as a project manager on some projects and lead design engineer on others. Prepare planning and design documents, process design calculations, and develop and maintain team and client standards. You may lead quality assurance/quality control and act as an advisor on complex projects. You will also coordinate with other Olsson teams, professional staff, technical staff, clients, and other consultants.</p>
 <p><i> You may travel to job sites for observation and attend client meetings.</i></p>
 <ul>
  <li><i>Olsson currently has one opportunity for a Licensed Civil Engineer. This role offers flexible work options, including remote and hybrid opportunities, to accommodate diverse working preferences and promote work-life balance. Candidates can live in Lincoln, Omaha, Overland Park, Denver or Dallas-Fort Worth area and work remotely or work out of any Olsson office location in these regions/areas.</i></li>
 </ul>
</div> 
<br> 
<div>
 Qualifications
 <p><b><br> You are passionate about:</b></p>
 <ul>
  <li> Working collaboratively with others</li>
  <li> Having ownership in the work you do</li>
  <li> Using your talents to positively affect communities</li>
  <li> Solving problems</li>
  <li> Providing excellence in client service</li>
 </ul>
 <p><b> You bring to the team:</b></p>
 <ul>
  <li> Strong communication skills</li>
  <li> Ability to contribute and work well on a team</li>
  <li> Bachelor&apos;s Degree in civil engineering</li>
  <li> At least 6 years of related civil engineering experience</li>
  <li> Proficient in Civil 3D software</li>
  <li> Must be a registered professional engineer</li>
 </ul>
 <br> Additional Information
 <p><br> Olsson is a nationally recognized, employee-owned firm specializing in planning and design, engineering, field services, environmental, and technology. Founded in 1956 on the very mindset that drives us today, we&#x2019;re here to improve communities by making them more sustainable, better connected, and more efficient. Simply put, we work to leave the world better than we found it.</p>
 <p> As an Olsson employee, you&#x2019;ll receive our traditional benefits package (health care, vision, dental, paid time off, etc.), plus you&#x2019;ll:</p>
 <ul>
  <li> Become an owner in the company after your first year through our Employee Stock Ownership Plan (ESOP)</li>
  <li> Engage in work that has a positive impact in communities</li>
  <li> Receive an excellent 401(k) match</li>
  <li> Participate in a wellness program promoting balanced lifestyles</li>
  <li> Benefit from a bonus system that rewards performance</li>
  <li> Have the possibility for flexible work arrangements</li>
 </ul>
 <p> Olsson is an EEO employer. We encourage qualified minority, female, veteran and disabled candidates to apply and be considered for open positions. We do not discriminate against any applicant for employment, or any employee because of race, color, religion, national origin, age, sex, sexual orientation, gender identity, gender, disability, age, or military status.</p>
 <p> #LI-LA1</p>
 <p> #LI-Remote</p>
</div>","https://jobs.smartrecruiters.com/Olsson/743999938557225-licensed-civil-engineer-data-center-remote-","6bbc2980914165f8",,"Full-time",,"601 P Street, Lincoln, NE 68508","Licensed Civil Engineer - Data Center (Remote)","4 days ago","2023-10-21T11:48:33.983Z","3.2","23",,"2023-10-25T11:48:33.985Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=6bbc2980914165f8&from=jasx&tk=1hdjak60ij4ij800&vjs=3"
"Team Velocity","Fast-paced, high-growth, technology driven marketing company serving the digital marketing automotive industry seeks hard working Senior Data Engineer. 
 Strong MSSQL experience, a must! 
 Ideal candidate has very strong knowledge of the Microsoft SQL Server engine with a focus on the ability to troubleshoot, optimize and support existing pipelines (SSIS) and processes, and a desire to learn (or already possess and grow) popular cloud technologies like Snowflake, BigQuery, Matillion, and more. 
 The ideal candidate is analytical, results-oriented, self-driven, confident, and able to meaningfully contribute to solution design among a team of other skilled data engineers. 
 This is a full-time, salaried, remote position headquartered in Herndon, VA. Applicants must be located within the continental U.S., eastern or central time zones highly preferred. 
 RESPONSIBILITIES: 
 
  Maintain and support existing MSSQL processes, pipelines (SSIS), and schemas 
  Migrate existing pipelines and processes to the cloud (Snowflake, GCP) 
  Analyze and organize raw data sets to meet both functional and non-functional requirements 
  Develop and test new pipelines and processes, both in MSSQL and cloud environments 
  With the data engineering team, design and implement solutions that scale to meet business needs 
  Design and implement data models to support analytics and reporting initiatives 
  With the software team, develop products that deliver value to our clients 
  Participate in peer-reviews of solution designs and related artifacts 
  Package and support deployment of releases 
  Analyze and resolve technical and application problems 
  Adhere to high-quality development principles while delivering solutions on-time and on-budget 
  Provide third-level support to business users 
  Participate in business operations as necessary to meet business goals 
  
 REQUIREMENTS: 
 
  Bachelor’s degree in Information Systems, Computer Science, or Information Technology (a 4-year bachelor’s degree is acceptable) - OR - expertise in above subject matters matched by experience 
  7+ years’ experience in MSSQL data engineering and development 
  Excellent understanding of T-SQL, stored procedures, indexes, stats, functions, and views 
  Expert understanding of relational and warehousing database design and querying concepts 
  Experience with SSIS, data integration and ETL/ELT procedures 
  Exposure to agile development methodology 
  Understanding of version control concepts 
  Strong desire to learn 
  Strong desire to be world-class data and SQL troubleshooter 
  
 BENEFICIAL: 
 
  Experience with Snowflake cloud data warehouse 
  Experience with Matillion (Snowflake/BigQuery) 
  Experience with Google Cloud Platform technology stack: BigQuery, Data Flow, Data Fusion 
  Experience with BI tools like Sigma, Tableau or others 
  Experience with NoSQL databases 
  
 COMPENSATION  This is a full-time, salaried, remote position headquartered in Herndon, VA. Eastern or central time zones preferred. Competitive compensation commensurate with experience. Participation in company benefit offerings include paid time off, medical, dental, vision, 401(k)/matching, wellness, and more. 
 NEXT STEPS  If you meet the requirements, and are interested in applying for this role, please complete the online application, be sure to include a current resume, contact information, and salary requirements. NO PHONE CALLS PLEASE. 
 
 ABOUT TEAM VELOCITY  Team Velocity is a SaaS technology provider serving the automotive industry. We provide an omni-channel marketing automation platform and retailing solutions to OEMs and dealerships nationwide. We are revolutionizing the automotive industry with cutting-edge technology to help dealers sell and service more cars. Made by dealers for dealers, Team Velocity’s proprietary technology platform Apollo® analyzes consumer behavior to predict who will buy, what they will buy, and when they are ready to service. Apollo automates the entire communication process by delivering hyper-personalized campaigns across every touchpoint, maximizing ROI, and lifetime revenue.   Our vision is to serve our clients with a single technology platform that empowers them to execute intelligent marketing across every online and offline channel. We aim to deliver a frictionless consumer experience, from the initial engagement to final transaction.   Our team members are hard-working and driven to achieve success for our clients and our unique culture promotes creativity, camaraderie, and success.","<div>
 <p>Fast-paced, high-growth, technology driven marketing company serving the digital marketing automotive industry seeks hard working <b>Senior Data Engineer</b>. </p>
 <p><b><i>Strong MSSQL experience, a must! </i></b></p>
 <p>Ideal candidate has very strong knowledge of the Microsoft SQL Server engine with a focus on the ability to troubleshoot, optimize and support existing pipelines (SSIS) and processes, and a desire to learn (or already possess and grow) popular cloud technologies like Snowflake, BigQuery, Matillion, and more. </p>
 <p>The ideal candidate is analytical, results-oriented, self-driven, confident, and able to meaningfully contribute to solution design among a team of other skilled data engineers.</p> 
 <p><b><i>This is a full-time, salaried, remote position headquartered in Herndon, VA. Applicants must be located within the continental U.S., eastern or central time zones highly preferred. </i></b></p>
 <p><b>RESPONSIBILITIES:</b></p> 
 <ul>
  <li>Maintain and support existing MSSQL processes, pipelines (SSIS), and schemas</li> 
  <li>Migrate existing pipelines and processes to the cloud (Snowflake, GCP)</li> 
  <li>Analyze and organize raw data sets to meet both functional and non-functional requirements</li> 
  <li>Develop and test new pipelines and processes, both in MSSQL and cloud environments</li> 
  <li>With the data engineering team, design and implement solutions that scale to meet business needs</li> 
  <li>Design and implement data models to support analytics and reporting initiatives</li> 
  <li>With the software team, develop products that deliver value to our clients</li> 
  <li>Participate in peer-reviews of solution designs and related artifacts</li> 
  <li>Package and support deployment of releases</li> 
  <li>Analyze and resolve technical and application problems</li> 
  <li>Adhere to high-quality development principles while delivering solutions on-time and on-budget</li> 
  <li>Provide third-level support to business users</li> 
  <li>Participate in business operations as necessary to meet business goals</li> 
 </ul> 
 <p><b>REQUIREMENTS:</b></p> 
 <ul>
  <li>Bachelor&#x2019;s degree in Information Systems, Computer Science, or Information Technology (a 4-year bachelor&#x2019;s degree is acceptable) <b>- OR -</b> expertise in above subject matters matched by experience</li> 
  <li>7+ years&#x2019; experience in MSSQL data engineering and development</li> 
  <li>Excellent understanding of T-SQL, stored procedures, indexes, stats, functions, and views</li> 
  <li>Expert understanding of relational and warehousing database design and querying concepts</li> 
  <li>Experience with SSIS, data integration and ETL/ELT procedures</li> 
  <li>Exposure to agile development methodology</li> 
  <li>Understanding of version control concepts</li> 
  <li>Strong desire to learn</li> 
  <li>Strong desire to be world-class data and SQL troubleshooter</li> 
 </ul> 
 <p><b>BENEFICIAL:</b></p> 
 <ul>
  <li>Experience with Snowflake cloud data warehouse</li> 
  <li>Experience with Matillion (Snowflake/BigQuery)</li> 
  <li>Experience with Google Cloud Platform technology stack: BigQuery, Data Flow, Data Fusion</li> 
  <li>Experience with BI tools like Sigma, Tableau or others</li> 
  <li>Experience with NoSQL databases</li> 
 </ul> 
 <p><b>COMPENSATION </b><br> This is a full-time, salaried, remote position headquartered in Herndon, VA. Eastern or central time zones preferred. Competitive compensation commensurate with experience. Participation in company benefit offerings include paid time off, medical, dental, vision, 401(k)/matching, wellness, and more.</p> 
 <p><b>NEXT STEPS </b><br> If you meet the requirements, and are interested in applying for this role, <b>please complete the online application</b>, be sure to include a current resume, contact information, and salary requirements. NO PHONE CALLS PLEASE.<br> </p>
 <p></p>
 <p><b>ABOUT TEAM VELOCITY </b><br> Team Velocity is a SaaS technology provider serving the automotive industry. We provide an omni-channel marketing automation platform and retailing solutions to OEMs and dealerships nationwide. We are revolutionizing the automotive industry with cutting-edge technology to help dealers sell and service more cars. Made by dealers for dealers, Team Velocity&#x2019;s proprietary technology platform Apollo&#xae; analyzes consumer behavior to predict who will buy, what they will buy, and when they are ready to service. Apollo automates the entire communication process by delivering hyper-personalized campaigns across every touchpoint, maximizing ROI, and lifetime revenue. <br> <br> Our vision is to serve our clients with a single technology platform that empowers them to execute intelligent marketing across every online and offline channel. We aim to deliver a frictionless consumer experience, from the initial engagement to final transaction. <br> <br> Our team members are hard-working and driven to achieve success for our clients and our unique culture promotes creativity, camaraderie, and success.</p>
</div>","https://teamvelocitymarketing.hrmdirect.com/employment/view.php?req=2800220&jbsrc=1014","a777e3c8aa805996",,"Full-time",,"Herndon, VA 20171","Senior Data Engineer","4 days ago","2023-10-21T11:48:37.597Z","3.1","12",,"2023-10-25T11:48:37.599Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=a777e3c8aa805996&from=jasx&tk=1hdjak60ij4ij800&vjs=3"
"G Associates LLC","Title: Sr Data Engineer
Location: Remote
Length: Contract-To-Hire
WHAT YOU'LL NEED:

 Bachelor’s degree in computer science, Management information systems (MIS) or related degree / experience commensurate to a degree preferred
 5+ years of hands-on experience implementing, maintaining, and supporting data management solutions including program/project delivery
 7+ years of experience with SQL & T-SQL code development - experience with Snowflake preferred
 5+ years of experience with Python – A MUST HAVE!
 4+ years of experience with Databricks
 4+ years of experience designing, building and deploying solutions with Azure Data Factory – A MUST HAVE !
 4+ years of experience with logical modeling for visualization tools (Tableau, Power BI, Sigma)
 4+ years of Experience with Data lake technologies including ADLS Gen. 2, AWS S3, AWS Glue preferred
 Need to differentiate between a Senior Data Engineer versus a Data Analyst (e.g., BI) … don’t want an Analyst
 Needs to be heavy Python (libraries include Pandas) and have worked with very large data sets; Understands OOP (Object-Oriented Programming language).
 Candidates need to own the entire process creation (business requirements through production). Also need to be comfortable wearing multiple hats (ie., jumping off the Innovation team to help with something on the Run team (production/maintenance).
 Must be able and comfortable with mentoring other Data Engineers
 Senior enough to know how to deal with ambiguity … they are creating a totally new data warehousing environment, moving to Azure Data Lake. Be able to jump in and build pipelines …

Job Type: Full-time
Pay: $83,492.68 - $135,000.00 per year
Experience level:

 9 years

Schedule:

 8 hour shift
 Monday to Friday

Experience:

 Azure Data Lake: 7 years (Required)
 SQL: 7 years (Required)
 Data Engineer: 10 years (Required)
 Databricks: 7 years (Required)

Work Location: Remote","<p><b>Title: Sr Data Engineer</b></p>
<p><b>Location: Remote</b></p>
<p><b>Length: Contract-To-Hire</b></p>
<p><b>WHAT YOU&apos;LL NEED:</b></p>
<ul>
 <li>Bachelor&#x2019;s degree in computer science, Management information systems (MIS) or related degree / experience commensurate to a degree preferred</li>
 <li>5+ years of hands-on experience implementing, maintaining, and supporting data management solutions including program/project delivery</li>
 <li>7+ years of experience with SQL &amp; T-SQL code development - experience with Snowflake preferred</li>
 <li>5+ years of experience with Python &#x2013; A MUST HAVE!</li>
 <li>4+ years of experience with Databricks</li>
 <li>4+ years of experience designing, building and deploying solutions with Azure Data Factory &#x2013; A MUST HAVE !</li>
 <li>4+ years of experience with logical modeling for visualization tools (Tableau, Power BI, Sigma)</li>
 <li>4+ years of Experience with Data lake technologies including ADLS Gen. 2, AWS S3, AWS Glue preferred</li>
 <li>Need to differentiate between a Senior Data Engineer versus a Data Analyst (e.g., BI) &#x2026; don&#x2019;t want an Analyst</li>
 <li>Needs to be heavy Python (libraries include Pandas) and have worked with very large data sets; Understands OOP (Object-Oriented Programming language).</li>
 <li>Candidates need to own the entire process creation (business requirements through production). Also need to be comfortable wearing multiple hats (ie., jumping off the Innovation team to help with something on the Run team (production/maintenance).</li>
 <li>Must be able and comfortable with mentoring other Data Engineers</li>
 <li>Senior enough to know how to deal with ambiguity &#x2026; they are creating a totally new data warehousing environment, moving to Azure Data Lake. Be able to jump in and build pipelines &#x2026;</li>
</ul>
<p>Job Type: Full-time</p>
<p>Pay: &#x24;83,492.68 - &#x24;135,000.00 per year</p>
<p>Experience level:</p>
<ul>
 <li>9 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
 <li>Monday to Friday</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Azure Data Lake: 7 years (Required)</li>
 <li>SQL: 7 years (Required)</li>
 <li>Data Engineer: 10 years (Required)</li>
 <li>Databricks: 7 years (Required)</li>
</ul>
<p>Work Location: Remote</p>",,"ead9ad56f5aa8781",,"Full-time",,"Remote","Sr. Azure Data Engineer","5 days ago","2023-10-20T11:48:49.281Z",,,"$83,493 - $135,000 a year","2023-10-25T11:48:49.283Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=ead9ad56f5aa8781&from=jasx&tk=1hdjaknrkj4ij800&vjs=3"
"AvidXchange","Overview 

 As a Principal Software Manager - Master Data Management, You will lead the Master Data Management initiative from a Technology perspective, working closely with dedicated engineering/architecture teams to deliver next-generation master data. The goal of this program is the unify disparate data sources via master data and entity relationship concepts to unlock business value, revenue generation, and cost savings. 

 What you'll do 

 
 Lead the design, build, and implementation of the Reltio Master Data Management platform with strong collaboration and teamwork with architecture, engineering and product partners. 
 Serve as AvidXchange’s technical owner of Reltio platform and services, leveraging expertise in system configurations, data modeling, tuning, and health/observability. 
 Develop (MDM) technology-enabled solutions that address the needs of clients, including the design, automation, and orchestration of enterprise Master, Relationship, and Reference Data 
 Engineer cross-domain solutions, including integration patterns (with customer-facing, operational, and analytic systems), workflows, policies, support, and reporting associated with an enterprise MDM capability. 
 Coach and mentor Engineers in data management and MDM practices and mindset. 
 Collaborate closely with the product organization to align product roadmaps with platform service capabilities. 
 Influence and educate partners and upstream/downstream domains to support adoption. 
 Design and develop data quality metrics, processes, and reporting. 
 Maintain up-to-date knowledge of MDM best practices and emerging technologies. 
 Interface closely with Reltio support and customer success to ensure a strong partnership. 
 What we're looking for 

 
 Bachelor's or master's degree in computer science, engineering, or related field. 
 Reltio Configuration Specialist certification is required. 
 7-10+ years of relevant work experience in MDM architecture and implementation. 
 10+ years experience in solutions and/or data architecture role or equivalent 
 3-5+ years experience leading cross-functional teams and large-scale projects. 
 Demonstrated ability to influence leadership and key stakeholders at varying levels within the organization. 
 Deep understanding and hands-on experience with MDM tools and technologies, such as Reltio, Informatica MDM or similar (Reltio strongly preferred). 
 Strong knowledge of data analysis, data modeling, data governance, and data quality principles. 
 Architectural level knowledge of Integrations leveraging APIs, Events, Messages and ETL. 
 Deep understanding of Cloud Platforms preferably Azure. 
 Proficient in SQL and programming languages such as Java or Python. 
 Excellent communication and collaboration skills - Ability to work independently and within a team environment. 
 Strong analytical, problem-solving, and critical thinking skills. 
 About AvidXchange 

 AvidXchange is a leading provider of accounts payable (“AP”) automation software and payment solutions for middle-market businesses and their suppliers. By trade, we are a technology company, but if you ask anyone who works here, they’ll tell you our people are at the core of who we are. We focus on creating a culture of Diversity, Inclusion & Belonging, and are proud to be a safe place where teammates can bring their whole selves to work. At AvidXchange, mindset is everything. We are Connected as People, Growth Minded, and Customer Obsessed. These three mindsets represent our culture – who we are, who we’ve always been, and they guide us to improve every day. Since our founding in 2000 in Charlotte, NC, we’ve created a company of over 1,600 teammates working in one of our 5 offices across the U.S., or remotely. AvidXchange is proud to be Certified™ as a Great Place to Work®. The prestigious recognition is based on anonymous data from our teammates and makes official what our teammates have known for years – that AvidXchange is a Great Place to Work®. 

 
Who you are:
 
 
 A go-getter with an entrepreneurial mindset – that means you are not afraid of taking risks, winning big or facing the unknown. 
 Someone who understands that business is people centric. Connecting with others as humans first allows you to develop mutually beneficial working relationships. 
 Focused on making a difference for our customers. AvidXchange exists to help solve complex problems for our customers so we can all realize our potential. 
 
What you’ll get:
 AvidXchange teammates (we call them AvidXers) get the perks and prestige of a publicly traded tech company paired with the flexibility of a founder-led startup. We help our AvidXers develop as professionals and as human beings, providing work/life balance, development programs, competitive benefits and equity options. At AvidXchange, we are building more than a tech company – we are building an experience. We remain committed to a culture where you can fully be 'you’ – connected with others, chasing big goals, and making a meaningful impact. If you want to help us grow while realizing your potential and creating stories you’ll tell for years, you’ve come to the right place. 

 
AvidXers enjoy:
 
 
 18 days PTO* 
 11 Holidays (8 company recognized & 3 floating holidays) 
 16 hours per year of paid Volunteer Time Off (VTO) 
 Competitive Healthcare 
 High Deductible Heath Plan Option that has $0 monthly premium for teammate-only coverage 
 100% AvidXchange paid Dental Base Plan Coverage 
 100% AvidXchange paid Life Insurance 
 100% AvidXchange paid Long-Term Disability 
 100% AvidXchange paid Short-Term Disability 
 Employee Assistance Program (EAP) - Provides counseling services, legal and financial consultations and health advocacy for Teammates and their eligible dependents 
 Onsite Health Clinic with Atrium Health** - available to Teammates and their eligible dependents 
 Retirement 401k Match up to 4% 
 Parental Leave: 8 weeks 100% paid by AvidXchange*** 
 Discounts on Pet, Home, and Auto insurance 
 BrightDime Financial Wellness Tool, offered free to teammates 
 WeeCare Childcare Service: helps teammates find affordable daycare, childcare, and tutors 40% less expensive than traditional daycare centers 
 Perks at Work: free discount program that provides teammates the opportunity to save on items from electronics, movie tickets, car buying, vacations, and more 
 Onsite gym fitness center, yoga studio, and basketball court**** 
 Tuition Reimbursement up to the federal maximum of $5,250***** 
 Hybrid Workplace Flexibility 
 Free parking 
 Fully granted from beginning of year, pro-rated if hired mid-year 
 **Charlotte location only 
***Must be full-time for at least 3 months 
****Charlotte location only 
*****Must be full-time for at least one year 

 Equal Employment Opportunity 

 AvidXchange is an equal opportunity employer. AvidXchange is committed to equal employment opportunity in accordance with applicable federal, state, and local laws. AvidXchange will not discriminate against applicants for employment on any legally recognized basis. This includes, but is not limited to veteran status, race, color, religion, sex, sexual orientation, gender identity, gender expression, national origin, age and physical or mental disability.","Overview 
<br>
<br> As a Principal Software Manager - Master Data Management, You will lead the Master Data Management initiative from a Technology perspective, working closely with dedicated engineering/architecture teams to deliver next-generation master data. The goal of this program is the unify disparate data sources via master data and entity relationship concepts to unlock business value, revenue generation, and cost savings. 
<br>
<br> What you&apos;ll do 
<br>
<ul> 
 <li>Lead the design, build, and implementation of the Reltio Master Data Management platform with strong collaboration and teamwork with architecture, engineering and product partners.</li> 
 <li>Serve as AvidXchange&#x2019;s technical owner of Reltio platform and services, leveraging expertise in system configurations, data modeling, tuning, and health/observability.</li> 
 <li>Develop (MDM) technology-enabled solutions that address the needs of clients, including the design, automation, and orchestration of enterprise Master, Relationship, and Reference Data</li> 
 <li>Engineer cross-domain solutions, including integration patterns (with customer-facing, operational, and analytic systems), workflows, policies, support, and reporting associated with an enterprise MDM capability.</li> 
 <li>Coach and mentor Engineers in data management and MDM practices and mindset.</li> 
 <li>Collaborate closely with the product organization to align product roadmaps with platform service capabilities.</li> 
 <li>Influence and educate partners and upstream/downstream domains to support adoption.</li> 
 <li>Design and develop data quality metrics, processes, and reporting.</li> 
 <li>Maintain up-to-date knowledge of MDM best practices and emerging technologies.</li> 
 <li>Interface closely with Reltio support and customer success to ensure a strong partnership.</li> 
</ul> What we&apos;re looking for 
<br>
<ul> 
 <li>Bachelor&apos;s or master&apos;s degree in computer science, engineering, or related field.</li> 
 <li>Reltio Configuration Specialist certification is required.</li> 
 <li>7-10+ years of relevant work experience in MDM architecture and implementation.</li> 
 <li>10+ years experience in solutions and/or data architecture role or equivalent</li> 
 <li>3-5+ years experience leading cross-functional teams and large-scale projects.</li> 
 <li>Demonstrated ability to influence leadership and key stakeholders at varying levels within the organization.</li> 
 <li>Deep understanding and hands-on experience with MDM tools and technologies, such as Reltio, Informatica MDM or similar (Reltio strongly preferred).</li> 
 <li>Strong knowledge of data analysis, data modeling, data governance, and data quality principles.</li> 
 <li>Architectural level knowledge of Integrations leveraging APIs, Events, Messages and ETL.</li> 
 <li>Deep understanding of Cloud Platforms preferably Azure.</li> 
 <li>Proficient in SQL and programming languages such as Java or Python.</li> 
 <li>Excellent communication and collaboration skills - Ability to work independently and within a team environment.</li> 
 <li>Strong analytical, problem-solving, and critical thinking skills.</li> 
</ul> About AvidXchange 
<br>
<br> AvidXchange is a leading provider of accounts payable (&#x201c;AP&#x201d;) automation software and payment solutions for middle-market businesses and their suppliers. By trade, we are a technology company, but if you ask anyone who works here, they&#x2019;ll tell you our people are at the core of who we are. We focus on creating a culture of Diversity, Inclusion &amp; Belonging, and are proud to be a safe place where teammates can bring their whole selves to work. At AvidXchange,&#x202f;mindset is everything. We are Connected as People, Growth Minded, and Customer Obsessed. These&#x202f;three mindsets represent our culture &#x2013; who we&#x202f;are, who we&#x2019;ve always been, and they guide us&#x202f;to improve every day.&#x202f;Since our founding in 2000 in Charlotte, NC, we&#x2019;ve created a company of over 1,600 teammates working in one of our 5 offices across the U.S., or remotely. AvidXchange is proud to be Certified&#x2122; as a&#x202f;Great Place to Work&#xae;. The prestigious recognition is based on anonymous data from our teammates and makes official what our teammates have known for years &#x2013; that AvidXchange is a Great Place to Work&#xae;. 
<br>
<br> 
<b>Who you are:</b>
<br> 
<ul> 
 <li>A go-getter with an entrepreneurial mindset &#x2013; that means you are not afraid of taking risks, winning big or facing the unknown.</li> 
 <li>Someone who understands that business is people centric. Connecting with others as humans first allows you to develop mutually beneficial working relationships.</li> 
 <li>Focused on making a difference for our customers. AvidXchange exists to help solve complex problems for our customers so we can all realize our potential.</li> 
</ul> 
<b>What you&#x2019;ll get:</b>
<br> AvidXchange teammates (we call them AvidXers) get the perks and prestige of a publicly traded tech company paired with the flexibility of a founder-led startup. We help our AvidXers develop as professionals and as human beings, providing work/life balance, development programs, competitive benefits and equity options. At AvidXchange, we are building more than a tech company &#x2013; we are building an experience. We remain committed to a culture where you can fully be &apos;you&#x2019; &#x2013; connected with others, chasing big goals, and making a meaningful impact. If you want to help us grow while realizing your potential and creating stories you&#x2019;ll tell for years, you&#x2019;ve come to the right place. 
<br>
<br> 
<b>AvidXers enjoy:</b>
<br> 
<ul> 
 <li>18 days PTO*</li> 
 <li>11 Holidays (8 company recognized &amp; 3 floating holidays)</li> 
 <li>16 hours per year of paid Volunteer Time Off (VTO)</li> 
 <li>Competitive Healthcare</li> 
 <li>High Deductible Heath Plan Option that has &#x24;0 monthly premium for teammate-only coverage</li> 
 <li>100% AvidXchange paid Dental Base Plan Coverage</li> 
 <li>100% AvidXchange paid Life Insurance</li> 
 <li>100% AvidXchange paid Long-Term Disability</li> 
 <li>100% AvidXchange paid Short-Term Disability</li> 
 <li>Employee Assistance Program (EAP) - Provides counseling services, legal and financial consultations and health advocacy for Teammates and their eligible dependents</li> 
 <li>Onsite Health Clinic with Atrium Health** - available to Teammates and their eligible dependents</li> 
 <li>Retirement 401k Match up to 4%</li> 
 <li>Parental Leave: 8 weeks 100% paid by AvidXchange***</li> 
 <li>Discounts on Pet, Home, and Auto insurance</li> 
 <li>BrightDime Financial Wellness Tool, offered free to teammates</li> 
 <li>WeeCare Childcare Service: helps teammates find affordable daycare, childcare, and tutors 40% less expensive than traditional daycare centers</li> 
 <li>Perks at Work: free discount program that provides teammates the opportunity to save on items from electronics, movie tickets, car buying, vacations, and more</li> 
 <li>Onsite gym fitness center, yoga studio, and basketball court****</li> 
 <li>Tuition Reimbursement up to the federal maximum of &#x24;5,250*****</li> 
 <li>Hybrid Workplace Flexibility</li> 
 <li>Free parking</li> 
 <li>Fully granted from beginning of year, pro-rated if hired mid-year</li> 
</ul> **Charlotte location only 
<br>***Must be full-time for at least 3 months 
<br>****Charlotte location only 
<br>*****Must be full-time for at least one year 
<br>
<br> Equal Employment Opportunity 
<br>
<br> AvidXchange is an equal opportunity employer. AvidXchange is committed to equal employment opportunity in accordance with applicable federal, state, and local laws. AvidXchange will not discriminate against applicants for employment on any legally recognized basis. This includes, but is not limited to veteran status, race, color, religion, sex, sexual orientation, gender identity, gender expression, national origin, age and physical or mental disability.","https://us232.dayforcehcm.com/CandidatePortal/en-US/avidxchange/Posting/View/9634?source=Indeed","66dd540a4c2f5995",,"Full-time",,"Remote","Principal Software Engineer - Master Data Management (MDM)","4 days ago","2023-10-21T11:48:44.917Z","3.3","137",,"2023-10-25T11:48:44.919Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=66dd540a4c2f5995&from=jasx&tk=1hdjak60ij4ij800&vjs=3"
"MetroSys","Responsibilities:
 
   Collaborate with stakeholders to understand data migration requirements, including scope, timelines, and specific data sets.
   Design and develop a comprehensive data migration plan, considering factors such as data volume, complexity, and business continuity.
   Utilize Komprise data management software to perform assessments, identify target data, and orchestrate the migration process.
   Configure and optimize Komprise settings to align with migration goals, ensuring efficient and secure data transfer.
   Conduct pre-migration validation tests to ensure data integrity and accuracy before the actual migration process.
   Monitor the data migration process in real-time, addressing any issues or discrepancies as they arise.
   Implement data validation and reconciliation procedures to verify the successful completion of the migration.
   Collaborate with cross-functional teams, including storage administrators and system engineers, to ensure seamless integration with the NetApp environment.
   Document the entire migration process, including configurations, settings, and any custom scripts or workflows used.
   Provide knowledge transfer and training to internal teams for ongoing management and maintenance of the migrated data.
 
  Requirements:
 
   Bachelor's degree in Information Technology, Computer Science, or a related field (preferred) or equivalent work experience.
   Proven work experience as a Data Migration Engineer with specific expertise in migrating data from Isilon to NetApp using Komprise.
   Strong proficiency in Komprise data management software and related tools.
   In-depth knowledge of Isilon and NetApp storage platforms, including file systems, protocols, and administration.
   Experience with scripting languages (e.g., Python, PowerShell) for automation and customization of migration processes.
   Excellent problem-solving and analytical skills, with the ability to diagnose and resolve complex data migration issues.
   Strong communication and interpersonal skills, with the ability to collaborate effectively with technical and non-technical stakeholders.
 
  
 T7ZuUtppMf","<div>
 <p>Responsibilities:</p>
 <ul>
  <li> Collaborate with stakeholders to understand data migration requirements, including scope, timelines, and specific data sets.</li>
  <li> Design and develop a comprehensive data migration plan, considering factors such as data volume, complexity, and business continuity.</li>
  <li> Utilize Komprise data management software to perform assessments, identify target data, and orchestrate the migration process.</li>
  <li> Configure and optimize Komprise settings to align with migration goals, ensuring efficient and secure data transfer.</li>
  <li> Conduct pre-migration validation tests to ensure data integrity and accuracy before the actual migration process.</li>
  <li> Monitor the data migration process in real-time, addressing any issues or discrepancies as they arise.</li>
  <li> Implement data validation and reconciliation procedures to verify the successful completion of the migration.</li>
  <li> Collaborate with cross-functional teams, including storage administrators and system engineers, to ensure seamless integration with the NetApp environment.</li>
  <li> Document the entire migration process, including configurations, settings, and any custom scripts or workflows used.</li>
  <li> Provide knowledge transfer and training to internal teams for ongoing management and maintenance of the migrated data.</li>
 </ul>
 <p> Requirements:</p>
 <ul>
  <li> Bachelor&apos;s degree in Information Technology, Computer Science, or a related field (preferred) or equivalent work experience.</li>
  <li> Proven work experience as a Data Migration Engineer with specific expertise in migrating data from Isilon to NetApp using Komprise.</li>
  <li> Strong proficiency in Komprise data management software and related tools.</li>
  <li> In-depth knowledge of Isilon and NetApp storage platforms, including file systems, protocols, and administration.</li>
  <li> Experience with scripting languages (e.g., Python, PowerShell) for automation and customization of migration processes.</li>
  <li> Excellent problem-solving and analytical skills, with the ability to diagnose and resolve complex data migration issues.</li>
  <li> Strong communication and interpersonal skills, with the ability to collaborate effectively with technical and non-technical stakeholders.</li>
 </ul>
 <p> </p>
 <p>T7ZuUtppMf</p>
</div>","https://www.indeed.com/applystart?jk=7f7a185825ef7e9f&from=vj&pos=top&mvj=0&spon=0&sjdu=YmZE5d5THV8u75cuc0H6Y26AwfY51UOGmh3Z9h4OvXjRthTD0dPYW0v_bVGP2vP4VAq81dAHm7GpE1MqdfeWCQ&vjfrom=serp&astse=9fd00ca04dcec5f3&assa=8483","7f7a185825ef7e9f",,,,"Remote","Data Migration Engineer","5 days ago","2023-10-20T11:48:48.871Z",,,"$60 - $80 an hour","2023-10-25T11:48:48.872Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=7f7a185825ef7e9f&from=jasx&tk=1hdjaknrkj4ij800&vjs=3"
"Gridiron IT","Seeking a Junior Data Engineer on a remote basis. Secret clearance is required. 
Overview: We are looking to immediately fill a Junior Data Engineer on our team. The ADE is one of the major pillars of MyNavy HR Transformation and serves as an enterprise-wide centralized repository that provides seamless and secure data access. This pilot’s objective is to support defining a comprehensive future-state ADE data model by informing the total number of unique data elements and to assess the ability to accelerate the data integration process using new data tools available following the Authority to Operate (ATO).
Minimum Qualifications:

 2+ years of experience with scalable ETL workflows/development, extract, cleanse, and process disparate data sources
 Secret Clearance is required.
 HS Diploma required (Bachelor's preferred)
 Experience with cleaning and transforming data utilizing Python and/or SQL, specifically complex SQL queries
 Familiarity with acquiring data from disparate data sources using APIs and SQL
 Ability to develop scripts and programs for converting various types of data into usable formats and support project team to scale, monitor, and operate data platforms

Job Type: Full-time
Pay: $48.00 - $52.00 per hour
Benefits:

 401(k)
 401(k) matching
 Dental insurance
 Health insurance
 Vision insurance

Schedule:

 8 hour shift

Work Location: Remote","<p><b>Seeking a Junior Data Engineer on a remote basis. </b><br><b>Secret clearance is required. </b></p>
<p><b>Overview:</b> We are looking to immediately fill a Junior Data Engineer on our team. The ADE is one of the major pillars of MyNavy HR Transformation and serves as an enterprise-wide centralized repository that provides seamless and secure data access. This pilot&#x2019;s objective is to support defining a comprehensive future-state ADE data model by informing the total number of unique data elements and to assess the ability to accelerate the data integration process using new data tools available following the Authority to Operate (ATO).</p>
<p><b>Minimum Qualifications:</b></p>
<ul>
 <li>2+ years of experience with scalable ETL workflows/development, extract, cleanse, and process disparate data sources</li>
 <li>Secret Clearance is required.</li>
 <li>HS Diploma required (Bachelor&apos;s preferred)</li>
 <li>Experience with cleaning and transforming data utilizing Python and/or SQL, specifically complex SQL queries</li>
 <li>Familiarity with acquiring data from disparate data sources using APIs and SQL</li>
 <li>Ability to develop scripts and programs for converting various types of data into usable formats and support project team to scale, monitor, and operate data platforms</li>
</ul>
<p>Job Type: Full-time</p>
<p>Pay: &#x24;48.00 - &#x24;52.00 per hour</p>
<p>Benefits:</p>
<ul>
 <li>401(k)</li>
 <li>401(k) matching</li>
 <li>Dental insurance</li>
 <li>Health insurance</li>
 <li>Vision insurance</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
</ul>
<p>Work Location: Remote</p>",,"45a231ffe43c577a",,"Full-time",,"Remote","Jr Data Engineer - Secret Cleared","5 days ago","2023-10-20T11:48:54.149Z","4.2","17","$48 - $52 an hour","2023-10-25T11:48:54.151Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=45a231ffe43c577a&from=jasx&tk=1hdjaktftk278800&vjs=3"
"Metric5","Sr. Databricks Data Engineer 
 Description 
 Metric5 is currently seeking a Databricks Data Engineer to work within a team providing Data Warehouse and Business Intelligence services to our government customer using Agile processes. As a Data Engineer, you will work with varying huge data sources with different schemas and data elements to design and implement solutions while aligning the technical roadmap for expanding the usage of Databricks Lakehouse Platform. You will be a Big Data Analytics expert on aspects of architecture and design; support the program by authoring reference architectures, how-tos, and demo applications. You have an eye for spotting data correlations and a desire to dig into large datasets to find technical solutions and deliver business value. 
 Program Details 
 The program you will be supporting has a mission to provide development, security, and operations (DevSecOps) support to U.S. Citizenship and Immigration Services (USCIS) with a focus on development, operations, and modernization of the Agency’s Enterprise Data Warehouse/Data Lake. The team utilizes open-source, AWS Cloud, and Big Data technologies, agile project management practices, and modern DevSecOps delivery to provide the business intelligence support systems to meet the reporting, data analytics, and machine learning/artificial intelligence needs critical to USCIS leadership, data/business analysts, data scientists, and other decision-makers. 
 Requirements: 
 
  Designing and implementing data ingestion pipelines from multiple sources using big data technologies 
  Developing scalable and re-usable frameworks for ingesting of data sets 
  Integrating the end-to-end data pipeline - to take data from source systems to target data repositories ensuring the quality and consistency of data is maintained at all times 
  Working with event based / streaming technologies to ingest and process data 
  Working with other members of the project team to support delivery of additional project components (API interfaces, Search) 
  Evaluating the performance and applicability of multiple tools against customer requirements 
  Implementing Databricks Unity Catalog and Delta Share features 
  
 
 Required Skills: 
 
  3+ years in a customer-facing, technical architecture role with expertise in at least one of the following technologies: Big data engineering (Ex: Spark, Delta, Hadoop, Kafka) 
  5+ years of experience with ETL development ingesting data from diverse and huge data sources Data Warehousing & ETL (Ex: SQL, OLTP/OLAP/DSS) 
  5+ years of experience Data Applications (Ex: Logs Analysis, Threat Detection, Real-time Systems Monitoring, Risk Analysis and more) 
  5+ years of experience with relational databases used to support BI analytics 
  3+ years of experience producing and consuming Rest APIs. 
  4+ years of experience with coding and scripting (Bash, Python, SQL, Java and Scala) 
  4+ years of experience using build and deployment tools (Jenkins, Docker). 
  Experience in Data Science and Machine Learning (Ex: pandas, scikit-learn, HPO) 
  Experience with structured streaming with Delta Sharing and integration with Unity Catalog 
  Experience in a Data Warehouse/Data Lake and Business Intelligence environment 
  Self-driven with the ability to adapt quickly, work in a challenging and fast paced environment within cross-functional teams, and to promote creative problem solving within their team 
  Hands on deep working knowledge with AWS including ECS/EC2/EKS, Security groups IAM roles, Instance profile and AWS S3 bucket security. 
  Experience of architecting and development of event-based architecture use cases using SQS/SNS including efficient usage of serverless functions (AWS Lambda) 
  Strong working knowledge and implementation of CI, CD pipelines using (Maven/Jenkins/Github/Gitlabs) including Code quality, code smells and configure reporting (e.g.: SONAR) 
  Experience with Agile development practices, including Scrum and Kanban, and management tools (e.g., Jira, Confluence) 
  Experience building solutions with public cloud providers such as AWS, Azure, or GCP 
  Strong teamwork, co-ordination, planning and influencing skills 
  Excellent communication and organizational skills 
  Experience with Microsoft Office Suite including Excel, PowerPoint, and Visio 
  
 
 Desired Skills: 
 
 
  Experience demonstrating technical concepts, including presenting and white-boarding 
  Experience with AWS Database Migration Service (DMS), Databricks/Apache Spark, and/or Kafka experience 
  Experience with Postgres and Oracle 
  Experience with Scaled Agile Framework SAFe 
  
 
 Education: Bachelor’s degree in a technical discipline preferred – Computer Science, Mathematics, or equivalent technical degree, or the equivalent combination of education, professional training, and work experience. 
 Location: Reston, VA – Currently fully remote 
 Clearance: Must be a US Citizen and be able to obtain a government agency Suitability Clearance. USCIS Entry on Duty (EOD) preferred.","<div>
 <p><b>Sr. Databricks Data Engineer</b></p> 
 <p>Description</p> 
 <p>Metric5 is currently seeking a Databricks Data Engineer to work within a team providing Data Warehouse and Business Intelligence services to our government customer using Agile processes. As a Data Engineer, you will work with varying huge data sources with different schemas and data elements to design and implement solutions while aligning the technical roadmap for expanding the usage of Databricks Lakehouse Platform. You will be a Big Data Analytics expert on aspects of architecture and design; support the program by authoring reference architectures, how-tos, and demo applications. You have an eye for spotting data correlations and a desire to dig into large datasets to find technical solutions and deliver business value. </p>
 <p><b>Program Details</b></p> 
 <p>The program you will be supporting has a mission to provide development, security, and operations (DevSecOps) support to U.S. Citizenship and Immigration Services (USCIS) with a focus on development, operations, and modernization of the Agency&#x2019;s Enterprise Data Warehouse/Data Lake. The team utilizes open-source, AWS Cloud, and Big Data technologies, agile project management practices, and modern DevSecOps delivery to provide the business intelligence support systems to meet the reporting, data analytics, and machine learning/artificial intelligence needs critical to USCIS leadership, data/business analysts, data scientists, and other decision-makers.</p> 
 <p><b>Requirements:</b></p> 
 <ul>
  <li>Designing and implementing data ingestion pipelines from multiple sources using big data technologies </li>
  <li>Developing scalable and re-usable frameworks for ingesting of data sets</li> 
  <li>Integrating the end-to-end data pipeline - to take data from source systems to target data repositories ensuring the quality and consistency of data is maintained at all times</li> 
  <li>Working with event based / streaming technologies to ingest and process data</li> 
  <li>Working with other members of the project team to support delivery of additional project components (API interfaces, Search)</li> 
  <li>Evaluating the performance and applicability of multiple tools against customer requirements</li> 
  <li>Implementing Databricks Unity Catalog and Delta Share features</li> 
 </ul> 
 <p></p>
 <p><b>Required Skills: </b></p>
 <ul>
  <li>3+ years in a customer-facing, technical architecture role with expertise in at least one of the following technologies: Big data engineering (Ex: Spark, Delta, Hadoop, Kafka)</li> 
  <li>5+ years of experience with ETL development ingesting data from diverse and huge data sources Data Warehousing &amp; ETL (Ex: SQL, OLTP/OLAP/DSS)</li> 
  <li>5+ years of experience Data Applications (Ex: Logs Analysis, Threat Detection, Real-time Systems Monitoring, Risk Analysis and more)</li> 
  <li>5+ years of experience with relational databases used to support BI analytics</li> 
  <li>3+ years of experience producing and consuming Rest APIs. </li>
  <li>4+ years of experience with coding and scripting (Bash, Python, SQL, Java and Scala)</li> 
  <li>4+ years of experience using build and deployment tools (Jenkins, Docker).</li> 
  <li>Experience in Data Science and Machine Learning (Ex: pandas, scikit-learn, HPO)</li> 
  <li>Experience with structured streaming with Delta Sharing and integration with Unity Catalog</li> 
  <li>Experience in a Data Warehouse/Data Lake and Business Intelligence environment</li> 
  <li>Self-driven with the ability to adapt quickly, work in a challenging and fast paced environment within cross-functional teams, and to promote creative problem solving within their team</li> 
  <li>Hands on deep working knowledge with AWS including ECS/EC2/EKS, Security groups IAM roles, Instance profile and AWS S3 bucket security.</li> 
  <li>Experience of architecting and development of event-based architecture use cases using SQS/SNS including efficient usage of serverless functions (AWS Lambda)</li> 
  <li>Strong working knowledge and implementation of CI, CD pipelines using (Maven/Jenkins/Github/Gitlabs) including Code quality, code smells and configure reporting (e.g.: SONAR)</li> 
  <li>Experience with Agile development practices, including Scrum and Kanban, and management tools (e.g., Jira, Confluence)</li> 
  <li>Experience building solutions with public cloud providers such as AWS, Azure, or GCP</li> 
  <li>Strong teamwork, co-ordination, planning and influencing skills</li> 
  <li>Excellent communication and organizational skills</li> 
  <li>Experience with Microsoft Office Suite including Excel, PowerPoint, and Visio</li> 
 </ul> 
 <p></p>
 <p><b>Desired Skills:</b></p> 
 <p></p>
 <ul>
  <li>Experience demonstrating technical concepts, including presenting and white-boarding</li> 
  <li>Experience with AWS Database Migration Service (DMS), Databricks/Apache Spark, and/or Kafka experience</li> 
  <li>Experience with Postgres and Oracle</li> 
  <li>Experience with Scaled Agile Framework SAFe</li> 
 </ul> 
 <p></p>
 <p><b>Education:</b> Bachelor&#x2019;s degree in a technical discipline preferred &#x2013; Computer Science, Mathematics, or equivalent technical degree, or the equivalent combination of education, professional training, and work experience.</p> 
 <p><b>Location</b>: Reston, VA &#x2013; Currently fully remote </p>
 <p><b>Clearance</b>: Must be a US Citizen and be able to obtain a government agency Suitability Clearance. USCIS Entry on Duty (EOD) preferred.</p>
</div>","https://metric5.hrmdirect.com/employment/view.php?req=2614082&jbsrc=1014","b93b81bddd338416",,,,"Reston, VA","Sr. Databricks Data Engineer","5 days ago","2023-10-20T11:48:57.717Z",,,,"2023-10-25T11:48:57.718Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=b93b81bddd338416&from=jasx&tk=1hdjakuh7je3s800&vjs=3"
"ServiceNow","Company Description
  At ServiceNow, our technology makes the world work for everyone, and our people make it possible. We move fast because the world can’t wait, and we innovate in ways no one else can for our customers and communities. By joining ServiceNow, you are part of an ambitious team of change makers who have a restless curiosity and a drive for ingenuity. We know that your best work happens when you live your best life and share your unique talents, so we do everything we can to make that possible. We dream big together, supporting each other to make our individual and collective dreams come true. The future is ours, and it starts with you. 
 With more than 7,700+ customers, we serve approximately 85% of the Fortune 500®, and we're proud to be one of FORTUNE 100 Best Companies to Work For® and World's Most Admired Companies™. 
 Learn more on Life at Now blog and hear from our employees about their experiences working at ServiceNow. 
 Unsure if you meet all the qualifications of a job description but are deeply excited about the role? We still encourage you to apply! At ServiceNow, we are committed to creating an inclusive environment where all voices are heard, valued, and respected. We welcome all candidates, including individuals from non-traditional, varied backgrounds, that might not come from a typical path connected to this role. We believe skills and experience are transferrable, and the desire to dream big makes for great candidates.
  Job Description
  As a Senior Staff Data and Software Engineer, you will be responsible for developing and implementing cutting-edge technical solutions that align with our organization's business objectives. You will work closely with stakeholders to understand their needs, assess existing systems and infrastructure, and design robust and scalable data and mircroservices solutions that drive innovation and efficiency. Your role will require a combination of technical expertise, strategic thinking, and effective communication to bridge the gap between business and technology.
 
  Key Responsibilities:
 
   Solution Design: Collaborate with business leaders, project managers, and technical teams to understand requirements and design holistic technical solutions that address current and future needs.
   Architecture Planning: Develop and maintain technology roadmaps, ensuring alignment with organizational goals and industry best practices.
   Technical Leadership: Provide technical leadership and guidance to development teams, ensuring adherence to architectural standards and best practices.
   Risk Assessment: Identify and evaluate technical risks and propose mitigation strategies to ensure project success and data security.
   Documentation: Create and maintain comprehensive architecture documentation, including diagrams, guidelines, and standards for development teams to follow.
   Vendor Evaluation: Assess and recommend third-party tools, products, and services that can enhance our technical solutions.
   Prototyping: Develop proof-of-concept and prototype solutions to validate architectural decisions and demonstrate feasibility.
   Performance Optimization: Continuously monitor and analyze system performance, identifying areas for improvement and optimizing existing solutions.
   Security and Compliance: Ensure that solutions comply with industry regulations and security standards, and proactively address security vulnerabilities.
   Collaboration: Foster collaboration and effective communication between cross-functional teams, promoting a culture of innovation and excellence.
 
  
  Qualifications
  Qualifications:
 
   Bachelor's degree in Computer Science, Information Technology, or related field (Master's degree preferred).
   Proven experience as a Lead Engineer and Solution Architect or a similar role.
   Strong knowledge of enterprise architecture principles and best practices.
   Proficiency in designing and implementing solutions using various technologies and platforms.
   Excellent problem-solving and analytical skills.
   Outstanding communication and interpersonal abilities.
   Project management skills and experience in managing complex technical projects.
   Certification in relevant technologies or architecture frameworks (e.g., TOGAF, AWS Certified Solutions Architect, Microsoft Certified: Azure Solutions Architect Expert) is a plus.
 
  Preferred Skills:
 
   Cloud computing expertise (e.g., AWS, Azure, Google Cloud Platform).
   Knowledge of DevOps practices and tools.
   Familiarity with microservices architecture, expertise a plus.
   Familiarity with graph databases, expertise a plus.
   Experience with containerization and orchestration technologies (e.g., Docker, Kubernetes).
   Strong understanding of data architecture and database technologies.
   Knowledge of cybersecurity best practices.
   Excellent presentation and facilitation skills.
 
  #DTjobs
  For positions in the Bay Area, we offer a base pay of $184,700 - $323,300, plus equity (when applicable), variable/incentive compensation and benefits. Sales positions generally offer a competitive On Target Earnings (OTE) incentive compensation structure. Please note that the base pay shown is a guideline, and individual total compensation will vary based on factors such as qualifications, skill level, competencies and work location. We also offer health plans, including flexible spending accounts, a 401(k) Plan with company match, ESPP, matching donations, a flexible time away plan and family leave programs (subject to eligibility requirements). Compensation is based on the geographic location in which the role is located, and is subject to change based on work location.
  Additional Information
  ServiceNow is an Equal Employment Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, creed, religion, sex, sexual orientation, national origin or nationality, ancestry, age, disability, gender identity or expression, marital status, veteran status or any other category protected by law. 
 At ServiceNow, we lead with flexibility and trust in our distributed world of work. Click here to learn about our work personas: flexible, remote and required-in-office. 
 If you require a reasonable accommodation to complete any part of the application process, or are limited in the ability or unable to access or use this online application process and need an alternative method for applying, you may contact us at talent.acquisition@servicenow.com for assistance. 
 For positions requiring access to technical data subject to export control regulations, including Export Administration Regulations (EAR), ServiceNow may have to obtain export licensing approval from the U.S. Government for certain individuals. All employment is contingent upon ServiceNow obtaining any export license or other approval that may be required by the U.S. Government. 
 Please Note: Fraudulent job postings/job scams are increasingly common. Click here to learn what to watch out for and how to protect yourself. All genuine ServiceNow job postings can be found through the ServiceNow Careers site.
  
 From Fortune. © 2022 Fortune Media IP Limited All rights reserved. Used under license. 
 Fortune and Fortune Media IP Limited are not affiliated with, and do not endorse products or services of, ServiceNow.","<div>
 <b>Company Description</b>
 <p><br> At ServiceNow, our technology makes the world work for everyone, and our people make it possible. We move fast because the world can&#x2019;t wait, and we innovate in ways no one else can for our customers and communities. By joining ServiceNow, you are part of an ambitious team of change makers who have a restless curiosity and a drive for ingenuity. We know that your best work happens when you live your best life and share your unique talents, so we do everything we can to make that possible. We dream big together, supporting each other to make our individual and collective dreams come true. The future is ours, and it starts with you.</p> 
 <p>With more than 7,700+ customers, we serve approximately 85% of the Fortune 500&#xae;, and we&apos;re proud to be one of FORTUNE 100 Best Companies to Work For&#xae; and World&apos;s Most Admired Companies&#x2122;.</p> 
 <p>Learn more on Life at Now blog and hear from our employees about their experiences working at ServiceNow.</p> 
 <p>Unsure if you meet all the qualifications of a job description but are deeply excited about the role? We still encourage you to apply! At ServiceNow, we are committed to creating an inclusive environment where all voices are heard, valued, and respected. We welcome all candidates, including individuals from non-traditional, varied backgrounds, that might not come from a typical path connected to this role. We believe skills and experience are transferrable, and the desire to dream big makes for great candidates.</p>
 <b><br> Job Description</b>
 <p><br> As a Senior Staff Data and Software Engineer, you will be responsible for developing and implementing cutting-edge technical solutions that align with our organization&apos;s business objectives. You will work closely with stakeholders to understand their needs, assess existing systems and infrastructure, and design robust and scalable data and mircroservices solutions that drive innovation and efficiency. Your role will require a combination of technical expertise, strategic thinking, and effective communication to bridge the gap between business and technology.</p>
 <p></p>
 <p><b><br> Key Responsibilities:</b></p>
 <ul>
  <li><b> Solution Design:</b> Collaborate with business leaders, project managers, and technical teams to understand requirements and design holistic technical solutions that address current and future needs.</li>
  <li><b> Architecture Planning:</b> Develop and maintain technology roadmaps, ensuring alignment with organizational goals and industry best practices.</li>
  <li><b> Technical Leadership:</b> Provide technical leadership and guidance to development teams, ensuring adherence to architectural standards and best practices.</li>
  <li><b> Risk Assessment:</b> Identify and evaluate technical risks and propose mitigation strategies to ensure project success and data security.</li>
  <li><b> Documentation:</b> Create and maintain comprehensive architecture documentation, including diagrams, guidelines, and standards for development teams to follow.</li>
  <li><b> Vendor Evaluation:</b> Assess and recommend third-party tools, products, and services that can enhance our technical solutions.</li>
  <li><b> Prototyping:</b> Develop proof-of-concept and prototype solutions to validate architectural decisions and demonstrate feasibility.</li>
  <li><b> Performance Optimization:</b> Continuously monitor and analyze system performance, identifying areas for improvement and optimizing existing solutions.</li>
  <li><b> Security and Compliance:</b> Ensure that solutions comply with industry regulations and security standards, and proactively address security vulnerabilities.</li>
  <li><b> Collaboration:</b> Foster collaboration and effective communication between cross-functional teams, promoting a culture of innovation and excellence.</li>
 </ul>
 <br> 
 <b> Qualifications</b>
 <p><b><br> Qualifications:</b></p>
 <ul>
  <li> Bachelor&apos;s degree in Computer Science, Information Technology, or related field (Master&apos;s degree preferred).</li>
  <li> Proven experience as a Lead Engineer and Solution Architect or a similar role.</li>
  <li> Strong knowledge of enterprise architecture principles and best practices.</li>
  <li> Proficiency in designing and implementing solutions using various technologies and platforms.</li>
  <li> Excellent problem-solving and analytical skills.</li>
  <li> Outstanding communication and interpersonal abilities.</li>
  <li> Project management skills and experience in managing complex technical projects.</li>
  <li> Certification in relevant technologies or architecture frameworks (e.g., TOGAF, AWS Certified Solutions Architect, Microsoft Certified: Azure Solutions Architect Expert) is a plus.</li>
 </ul>
 <p><b> Preferred Skills:</b></p>
 <ul>
  <li> Cloud computing expertise (e.g., AWS, Azure, Google Cloud Platform).</li>
  <li> Knowledge of DevOps practices and tools.</li>
  <li> Familiarity with microservices architecture, expertise a plus.</li>
  <li> Familiarity with graph databases, expertise a plus.</li>
  <li> Experience with containerization and orchestration technologies (e.g., Docker, Kubernetes).</li>
  <li> Strong understanding of data architecture and database technologies.</li>
  <li> Knowledge of cybersecurity best practices.</li>
  <li> Excellent presentation and facilitation skills.</li>
 </ul>
 <p> #DTjobs</p>
 <p> For positions in the Bay Area, we offer a base pay of &#x24;184,700 - &#x24;323,300, plus equity (when applicable), variable/incentive compensation and benefits. Sales positions generally offer a competitive On Target Earnings (OTE) incentive compensation structure. Please note that the base pay shown is a guideline, and individual total compensation will vary based on factors such as qualifications, skill level, competencies and work location. We also offer health plans, including flexible spending accounts, a 401(k) Plan with company match, ESPP, matching donations, a flexible time away plan and family leave programs (subject to eligibility requirements). Compensation is based on the geographic location in which the role is located, and is subject to change based on work location.</p>
 <b><br> Additional Information</b>
 <p><br> ServiceNow is an Equal Employment Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, creed, religion, sex, sexual orientation, national origin or nationality, ancestry, age, disability, gender identity or expression, marital status, veteran status or any other category protected by law.</p> 
 <p>At ServiceNow, we lead with flexibility and trust in our distributed world of work. Click here to learn about our work personas: flexible, remote and required-in-office.</p> 
 <p>If you require a reasonable accommodation to complete any part of the application process, or are limited in the ability or unable to access or use this online application process and need an alternative method for applying, you may contact us at talent.acquisition@servicenow.com for assistance.</p> 
 <p>For positions requiring access to technical data subject to export control regulations, including Export Administration Regulations (EAR), ServiceNow may have to obtain export licensing approval from the U.S. Government for certain individuals. All employment is contingent upon ServiceNow obtaining any export license or other approval that may be required by the U.S. Government.</p> 
 <p>Please Note: Fraudulent job postings/job scams are increasingly common. Click here to learn what to watch out for and how to protect yourself. All genuine ServiceNow job postings can be found through the ServiceNow Careers site.</p>
 <p><br> </p>
 <p>From Fortune. &#xa9; 2022 Fortune Media IP Limited All rights reserved. Used under license.</p> 
 <p>Fortune and Fortune Media IP Limited are not affiliated with, and do not endorse products or services of, ServiceNow.</p>
</div>","https://www.indeed.com/rc/clk?jk=226571fd23411b51&atk=&xpse=SoDE67I3Jzda1tgcWb0LbzkdCdPP","226571fd23411b51",,"Full-time",,"176 N Racine Ave, Chicago, IL 60607","Senior Staff Data and Services Software Engineer","6 days ago","2023-10-19T11:48:58.630Z","3.7","241","$184,700 - $323,300 a year","2023-10-25T11:48:58.631Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=226571fd23411b51&from=jasx&tk=1hdjakuh7je3s800&vjs=3"
"Care.com","About Care.com 
  Care.com is a consumer tech company with heart. We're on a mission to solve a human challenge we all face: finding great care for the ones we love. We're moms and dads and pet parents. We have parents and grandparents, so we understand that everyone, at some point in their lives, could use a helping hand. Our culture and our products reflect that. 
  Here, entrepreneurs, self-starters, team players, and big thinkers unite behind a common cause. Here, we're applying data analytics, AI, and the latest technologies to solve universal problems and connect people in new ways. If you like having autonomy, if you thrive on collaboration and building new things, and if you're all about using your talent for good, Care.com is the place for you. 
  Office Locations: (This is a hybrid position) 
 
  NY, NY 10011 
  Austin, TX 78746 
  Shelton, CT 06484 
  
 What Your Days Will be Like: 
  The Data Engineer will be focusing on building out data feeds and tooling from our application platform and enable rapid ingestion into our centralized Data Lake/Data Warehouse. The Data Engineer will work across business areas and application teams at Care.com to rationalize data and design, build and maintain reusable data feeds which and ultimately empower analytics consumption at Care.com. 
  The ideal candidate will have professional experience building data pipelines in a technical environment. S/he will have an understanding of application development, data warehousing, demonstrate strong business judgment, and be able to prioritize in a fast-paced environment. 
  What You'll Be Working On: 
  
  Collaborate and partner with application teams to understand data collection/generation and design and partner to build and implement data feeds from our product tech stack 
  Design, develop, and build code for rapid feeds and ingestion into the Data Warehouse 
  Identify data sources used for building out data architecture diagrams/models 
  Establish engineering practices and setup frameworks for ""Data as a Service"" 
  Collaborate with relevant delivery teams, including infrastructure, operations, site reliability engineering, product development, and others to perform evaluations, POCs, and ultimately implement and operationalize new technology. 
  Solve code level problems quickly and efficiently 
  Participate in demos and code reviews 
  Promote software best approach, standards, and processes 
  Shape development processes to promote a high-quality output while continuing to iterate quickly 
  Incorporate best practices for security, performance, and data privacy into data pipelines 
  
 What You'll Need to Succeed: 
  
  BS or MS in Computer Science or relevant engineering experience 
  5+ years work experience in Data Engineering/data pipelines 
  3+ years SQL experience is a must 
  1+ years Unix/batch scripting preferred 
  1+ years Python experience is a plus 
  1+ years Windows server admin experience is a plus 
  Experience interfacing with business teams and turning requirements and vision into a technical reality 
  MySQL & Vertica Experience a plus/preferred 
  AWS experience is a plus 
  Ability to drive efforts from start to finish as a self-motivator 
  Knowledge in Data Warehousing is a MUST 
  Proven ability to maintain performance level in a fast-paced agile environment 
  Pragmatic and realistic with solutions 
  
 For a list of our Perks + Benefits, click here! 
  Care.com supports diverse families and communities and seeks employees who are just as diverse. As an equal opportunity employer, Care.com recognizes the power of a diverse and inclusive workforce and encourages applications from individuals with varied experiences, perspectives, and backgrounds. Care.com is committed to providing reasonable accommodations for qualified individuals with disabilities. If you need assistance or accommodation, please reach out to talent@care.com. 
  Company Overview: 
  Available in more than 20 countries, Care.com is the world's leading platform for finding and managing high-quality family care. Care.com is designed to meet the evolving needs of today's families and caregivers, offering everything from household tax and payroll services and customized corporate benefits packages covering the care needs of working families, to innovating new ways for caregivers to be paid and obtain professional benefits. Since 2007, families have relied on Care.com's industry-leading products—from child and elder care to pet care and home care. Care.com is an IAC company (NASDAQ: IAC). 
  Salary Range: 110,000 to 145,000. The base salary range above represents the anticipated low and high end of the national salary range for this position. Actual salaries may vary and may be above or below the range based on various factors including but not limited to work location, experience, and performance. The range listed is just one component of Care.com's total compensation package for employees. Other rewards may include annual bonuses and short- and long-term incentives. In addition, Care.com provides a variety of benefits to employees, including health insurance coverage, life, and disability insurance, a generous 401K employer matching program, paid holidays, and paid time off (PTO).","<div>
 <p><b>About Care.com</b></p> 
 <p> Care.com is a consumer tech company with heart. We&apos;re on a mission to solve a human challenge we all face: finding great care for the ones we love. We&apos;re moms and dads and pet parents. We have parents and grandparents, so we understand that everyone, at some point in their lives, could use a helping hand. Our culture and our products reflect that.</p> 
 <p> Here, entrepreneurs, self-starters, team players, and big thinkers unite behind a common cause. Here, we&apos;re applying data analytics, AI, and the latest technologies to solve universal problems and connect people in new ways. If you like having autonomy, if you thrive on collaboration and building new things, and if you&apos;re all about using your talent for good, Care.com is the place for you.</p> 
 <h2 class=""jobSectionHeader""><b> Office Locations: (This is a hybrid position)</b></h2> 
 <ul>
  <li><b>NY, NY</b> 10011</li> 
  <li><b>Austin, TX</b> 78746</li> 
  <li><b>Shelton, CT</b> 06484</li> 
 </ul> 
 <p><b>What Your Days Will be Like:</b></p> 
 <p> The Data Engineer will be focusing on building out data feeds and tooling from our application platform and enable rapid ingestion into our centralized Data Lake/Data Warehouse. The Data Engineer will work across business areas and application teams at Care.com to rationalize data and design, build and maintain reusable data feeds which and ultimately empower analytics consumption at Care.com.</p> 
 <p> The ideal candidate will have professional experience building data pipelines in a technical environment. S/he will have an understanding of application development, data warehousing, demonstrate strong business judgment, and be able to prioritize in a fast-paced environment.</p> 
 <p><b> What You&apos;ll Be Working On:</b></p> 
 <ul> 
  <li>Collaborate and partner with application teams to understand data collection/generation and design and partner to build and implement data feeds from our product tech stack</li> 
  <li>Design, develop, and build code for rapid feeds and ingestion into the Data Warehouse</li> 
  <li>Identify data sources used for building out data architecture diagrams/models</li> 
  <li>Establish engineering practices and setup frameworks for &quot;Data as a Service&quot;</li> 
  <li>Collaborate with relevant delivery teams, including infrastructure, operations, site reliability engineering, product development, and others to perform evaluations, POCs, and ultimately implement and operationalize new technology.</li> 
  <li>Solve code level problems quickly and efficiently</li> 
  <li>Participate in demos and code reviews</li> 
  <li>Promote software best approach, standards, and processes</li> 
  <li>Shape development processes to promote a high-quality output while continuing to iterate quickly</li> 
  <li>Incorporate best practices for security, performance, and data privacy into data pipelines</li> 
 </ul> 
 <p><b>What You&apos;ll Need to Succeed:</b></p> 
 <ul> 
  <li>BS or MS in Computer Science or relevant engineering experience</li> 
  <li>5+ years work experience in Data Engineering/data pipelines</li> 
  <li>3+ years SQL experience is a must</li> 
  <li>1+ years Unix/batch scripting preferred</li> 
  <li>1+ years Python experience is a plus</li> 
  <li>1+ years Windows server admin experience is a plus</li> 
  <li>Experience interfacing with business teams and turning requirements and vision into a technical reality</li> 
  <li>MySQL &amp; Vertica Experience a plus/preferred</li> 
  <li>AWS experience is a plus</li> 
  <li>Ability to drive efforts from start to finish as a self-motivator</li> 
  <li>Knowledge in Data Warehousing is a MUST</li> 
  <li>Proven ability to maintain performance level in a fast-paced agile environment</li> 
  <li>Pragmatic and realistic with solutions</li> 
 </ul> 
 <p><b>For a list of our Perks + Benefits, click</b> <b>here!</b></p> 
 <p> Care.com supports diverse families and communities and seeks employees who are just as diverse. As an equal opportunity employer, Care.com recognizes the power of a diverse and inclusive workforce and encourages applications from individuals with varied experiences, perspectives, and backgrounds. Care.com is committed to providing reasonable accommodations for qualified individuals with disabilities. If you need assistance or accommodation, please reach out to talent@care.com.</p> 
 <p><b> Company Overview:</b></p> 
 <p> Available in more than 20 countries, Care.com is the world&apos;s leading platform for finding and managing high-quality family care. Care.com is designed to meet the evolving needs of today&apos;s families and caregivers, offering everything from household tax and payroll services and customized corporate benefits packages covering the care needs of working families, to innovating new ways for caregivers to be paid and obtain professional benefits. Since 2007, families have relied on Care.com&apos;s industry-leading products&#x2014;from child and elder care to pet care and home care. Care.com is an IAC company (NASDAQ: IAC).</p> 
 <p><i> Salary Range: 110,000 to 145,000. The base salary range above represents the anticipated low and high end of the national salary range for this position. </i><b><i>Actual salaries may vary and may be above or below the range based on various factors including but not limited to work location, experience, and performance.</i></b><i> The range listed is just one component of Care.com&apos;s total compensation package for employees. Other rewards may include annual bonuses and short- and long-term incentives. In addition, Care.com provides a variety of benefits to employees, including health insurance coverage, life, and disability insurance, a generous 401K employer matching program, paid holidays, and paid time off (PTO).</i></p>
</div>
<p></p>","https://www.care.com/vis/careers/job/5451121?gh_jid=5451121&gh_src=d04190671us&utm_medium=organic","87c74988107a90d3",,,,"Hybrid remote","Data Engineer","5 days ago","2023-10-20T11:48:57.392Z","4.2","1861","$110,000 - $145,000 a year","2023-10-25T11:48:57.394Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=87c74988107a90d3&from=jasx&tk=1hdjakuh7je3s800&vjs=3"
"Amivero","Description: 
  The Amivero Team
  Amivero’s team of IT professionals delivers digital services that elevate the federal government, whether national security or improved government services. Our human-centered, data-driven approach is focused on truly understanding the environment and the challenge, and reimagining with our customer how outcomes can be achieved.
  Our team of technologists leverage modern, agile methods to design and develop equitable, accessible, and innovative data and software services that impact hundreds of millions of people.
  As a member of the Amivero team you will use your empathy for a customer’s situation, your passion for service, your energy for solutioning, and your bias towards action to bring modernization to very important, mission-critical, and public service government IT systems.
  Special Requirements
 
   Active Top Secret Security Clearance
   Bachelor’s degree in Computer Science, Mathematics, Engineering, or related field
   Practical working experience and advanced knowledge of cyber threats, tools, techniques, and processes.
   Experience in data modeling and working with datasets of all sizes using a variety of data mining and data analysis methods/tools
   This role is remote, but will require occasional meetings at the client location (Arlington, VA or Pensacola, FL)
 
  The Gist…
  We are looking for a Junior Data Mining and Analytics Engineer, your skillset will create useful and actionable insight for the customer through the development of analytic solutions (hardware, analytics, tools, techniques, practices, deployment, standards, performance specifications, etc.) for analytic use cases developed during the performance of this project. You will work closely with the Analytics Research team to identify platform enhancements that support the forward-looking analytics under consideration. The ideal candidate has extensive knowledge of a wide variety of systems and networks to include high-volume/high-availability systems. You are focused on results, a self-starter, and have demonstrated success for using analytics to drive the understanding, growth, and success of the analysis.
  What Your Day Might Include…
 
   Perform knowledge elicitation from customer subject matter experts and convert that to build analytic solutions
   Design, engineer, and optimize sustainment of large-scale distributed computation platforms and supporting environment (ecosystems) for various stakeholders, business owners, and industry partner
   Oversee the transition of services from third-party vendors to the analytic environment and be responsible for ad hoc and formal end-user training
   Identify applicable data to perform analytics and create solutions to acquire, transform, and load or correlate data components to and from the analytic environment
   Develop custom data modeling procedures to assist with data mining, modeling, and production
   Assess the effectiveness and accuracy of new data sources and data gathering techniques
   Develop processes and tools to monitor and analyze model performance and data accuracy
   Interpret and communicate results to non-technical customers
  Requirements:
   You’ll Bring These Qualifications…
 
   Interpersonal skills and the ability to communicate effectively with various clients in order to explain and elaborate on technical details
   Experience in developing analytic tools, processes, and governance for storing, modeling, capturing, and delivering data to the client’s enterprise
   Experience with computational notebook software such as Zeppelin or Jupyter
   Experience with the application of visual analytics to computational analytic results
   Fluency in one or more programming languages (e.g., Python, JavaScript, R, etc.)
   Experience with database querying like SQL
   Readiness to collaborate with engineering teams, product teams, and customers to develop prototypes and software products
   Scaled Agile Framework (SAFe) experience
   Amazon Web Services (AWS) Certified Cloud Practitioner or higher desired
   Master’s degree in Computer Science, Mathematics, Engineering, or related field
   CompTIA Security+ or higher cybersecurity certification preferred
 
  EOE/M/F/VET/DISABLEDAll qualified applicants will receive consideration without regard to race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, disability, genetic information, marital status, amnesty, or status as a covered veteran in accordance with applicable federal, state and local laws. Amivero complies with applicable state and local laws governing non-discrimination in employment in every location in which the company has facilities.","<div>
 Description: 
 <p><b> The Amivero Team</b></p>
 <p> Amivero&#x2019;s team of IT professionals delivers digital services that elevate the federal government, whether national security or improved government services. Our human-centered, data-driven approach is focused on truly understanding the environment and the challenge, and reimagining with our customer how outcomes can be achieved.</p>
 <p> Our team of technologists leverage modern, agile methods to design and develop equitable, accessible, and innovative data and software services that impact hundreds of millions of people.</p>
 <p> As a member of the Amivero team you will use your empathy for a customer&#x2019;s situation, your passion for service, your energy for solutioning, and your bias towards action to bring modernization to very important, mission-critical, and public service government IT systems.</p>
 <p><b> Special Requirements</b></p>
 <ul>
  <li> Active Top Secret Security Clearance</li>
  <li> Bachelor&#x2019;s degree in Computer Science, Mathematics, Engineering, or related field</li>
  <li> Practical working experience and advanced knowledge of cyber threats, tools, techniques, and processes.</li>
  <li> Experience in data modeling and working with datasets of all sizes using a variety of data mining and data analysis methods/tools</li>
  <li> This role is remote, but will require occasional meetings at the client location (Arlington, VA or Pensacola, FL)</li>
 </ul>
 <p><b> The Gist&#x2026;</b></p>
 <p> We are looking for a Junior Data Mining and Analytics Engineer, your skillset will create useful and actionable insight for the customer through the development of analytic solutions (hardware, analytics, tools, techniques, practices, deployment, standards, performance specifications, etc.) for analytic use cases developed during the performance of this project. You will work closely with the Analytics Research team to identify platform enhancements that support the forward-looking analytics under consideration. The ideal candidate has extensive knowledge of a wide variety of systems and networks to include high-volume/high-availability systems. You are focused on results, a self-starter, and have demonstrated success for using analytics to drive the understanding, growth, and success of the analysis.</p>
 <p><b> What Your Day Might Include&#x2026;</b></p>
 <ul>
  <li> Perform knowledge elicitation from customer subject matter experts and convert that to build analytic solutions</li>
  <li> Design, engineer, and optimize sustainment of large-scale distributed computation platforms and supporting environment (ecosystems) for various stakeholders, business owners, and industry partner</li>
  <li> Oversee the transition of services from third-party vendors to the analytic environment and be responsible for ad hoc and formal end-user training</li>
  <li> Identify applicable data to perform analytics and create solutions to acquire, transform, and load or correlate data components to and from the analytic environment</li>
  <li> Develop custom data modeling procedures to assist with data mining, modeling, and production</li>
  <li> Assess the effectiveness and accuracy of new data sources and data gathering techniques</li>
  <li> Develop processes and tools to monitor and analyze model performance and data accuracy</li>
  <li> Interpret and communicate results to non-technical customers</li>
 </ul> Requirements:
 <p><br> <br> <b>You&#x2019;ll Bring These Qualifications&#x2026;</b></p>
 <ul>
  <li> Interpersonal skills and the ability to communicate effectively with various clients in order to explain and elaborate on technical details</li>
  <li> Experience in developing analytic tools, processes, and governance for storing, modeling, capturing, and delivering data to the client&#x2019;s enterprise</li>
  <li> Experience with computational notebook software such as Zeppelin or Jupyter</li>
  <li> Experience with the application of visual analytics to computational analytic results</li>
  <li> Fluency in one or more programming languages (e.g., Python, JavaScript, R, etc.)</li>
  <li> Experience with database querying like SQL</li>
  <li> Readiness to collaborate with engineering teams, product teams, and customers to develop prototypes and software products</li>
  <li> Scaled Agile Framework (SAFe) experience</li>
  <li> Amazon Web Services (AWS) Certified Cloud Practitioner or higher desired</li>
  <li> Master&#x2019;s degree in Computer Science, Mathematics, Engineering, or related field</li>
  <li> CompTIA Security+ or higher cybersecurity certification preferred</li>
 </ul>
 <p> EOE/M/F/VET/DISABLEDAll qualified applicants will receive consideration without regard to race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, disability, genetic information, marital status, amnesty, or status as a covered veteran in accordance with applicable federal, state and local laws. Amivero complies with applicable state and local laws governing non-discrimination in employment in every location in which the company has facilities.</p>
</div>","https://recruiting.paylocity.com/recruiting/jobs/Details/2019212/Amivero/Junior-Data-Mining-and-Analytics-Engineer?source=Indeed_Feed","d4c658ebe903afda",,"Full-time",,"Remote","Junior Data Mining and Analytics Engineer","6 days ago","2023-10-19T11:48:59.553Z","4.8","5",,"2023-10-25T11:48:59.554Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=d4c658ebe903afda&from=jasx&tk=1hdjakuh7je3s800&vjs=3"
"Oncora Medical","About us:
Oncora is an oncology software and data company dedicated to helping physicians and scientists collect and use real-world data to improve outcomes for cancer patients. Our machine learning algorithms, which are deployed in active clinical environments, accurately predict oncology outcomes such as unplanned hospitalization, survival, and recurrence. Our software products include: a clinical workflow and data entry system for oncology clinical care, a data warehouse that leverages connections to other healthcare software systems such as EMRs, PACS, to amass real-world, regulatory-grade oncology data, a machine learning platform to train and validate predictive models of key oncology events, a machine learning API to power external software tools, and a virtual clinical trial platform that allows pharma and device companies to leverage automated medical image analysis to advance new technologies in the fight to cure cancer. We work with world-leading cancer centers such as MD Anderson and Northwell Health, global device companies such as Varian Medical Systems, and innovative biopharma companies. Our team is mission-driven to its core.
About the role:
We are looking for an experienced engineer to join our mission driven team to help develop our data platform that integrates and transforms multiple imperfect and messy healthcare data sources into clean, usable data so that we can learn from every cancer patient.
As a core member of our Platform and Integrations team, you will play a vital role in designing and building our core data platform and helping scale it to serve additional hospitals. You will lead major improvements to our data infrastructure and apply your skills in software development, architecture and data modeling to get things done.
We are a small team trying to tackle a very large problem, so we need teammates that are ultimately accountable to themselves and continuously push themselves, the product and the organization forward.
What you will be doing:

 Designing and implementing improvements to our data extraction and transformation processes to increase performance and stability
 Architecting our data warehousing and reporting capabilities to support real-time analysis of tens of thousands of patients representing millions of data points
 Incrementally evolving our platform architecture and infrastructure without disrupting operations
 Overseeing and monitoring our existing data platforms for stability, performance and accuracy
 Working with Product and Engineering to define, document, and build transformations to extract intelligence from multiple incomplete and siloed data sources
 Building pipelines to de-identify and consolidate cross-institutional data to fuel predictive analytics, research, and clinical trials
 Building visibility into all aspects of our data platform (workflow status, system health, data lineage, etc.)

About you:

 Demonstrated experience independently leading complex software projects
 A strong base of software engineering experience, typically 5-8+ years, with a good portion of that leading the development of data platforms, pipelines or data-intensive projects
 Deep understanding of multiple database technologies, for example relational, document, key/value, columnar, etc (we use Postgres, MongoDB, Redis, and ElasticSearch)
 Experience building asynchronous and distributed systems (we use RabbitMQ)
 Fluency with a functional or imperative language (we use Python)
 A focus on writing understandable, testable, and maintainable code
 Familiarity with modern containerized environments (we use Docker & Kubernetes)

Bonuses:

 Experience with healthcare data standards and integrations (HL7, FHIR, DICOM, etc.)

Compensation, Benefits, and Perks:

 Salary: $145k-180k plus equity compensation
 401k, health and dental insurance, flexible vacation, paid parental leave
 eBooks, online courses, home office budget
 Events: happy hours, team dinners, conversations with oncologists
 Work with smart, passionate people on a product that will have a direct impact on the lives of cancer patients

What to expect in the hiring process:

 Introductory phone call (15-30 minutes zoom call)
 Phone interview with CTO/CEO (60 min zoom call)
 Virtual onsite, including a pair programming session, engineering team meet, and co-founder meet (90-120 minute zoom call)
 Final stages, potential follow-up interviews, and offer discussions

Job Type: Full-time
Pay: $120,000.00 - $185,000.00 per year
Benefits:

 401(k)
 Dental insurance
 Flexible schedule
 Health insurance
 Paid time off
 Parental leave

Experience level:

 4 years

Schedule:

 Monday to Friday

Work Location: Remote","<p><b>About us:</b></p>
<p>Oncora is an oncology software and data company dedicated to helping physicians and scientists collect and use real-world data to improve outcomes for cancer patients. Our machine learning algorithms, which are deployed in active clinical environments, accurately predict oncology outcomes such as unplanned hospitalization, survival, and recurrence. Our software products include: a clinical workflow and data entry system for oncology clinical care, a data warehouse that leverages connections to other healthcare software systems such as EMRs, PACS, to amass real-world, regulatory-grade oncology data, a machine learning platform to train and validate predictive models of key oncology events, a machine learning API to power external software tools, and a virtual clinical trial platform that allows pharma and device companies to leverage automated medical image analysis to advance new technologies in the fight to cure cancer. We work with world-leading cancer centers such as MD Anderson and Northwell Health, global device companies such as Varian Medical Systems, and innovative biopharma companies. Our team is mission-driven to its core.</p>
<p><b>About the role:</b></p>
<p>We are looking for an experienced engineer to join our mission driven team to help develop our data platform that integrates and transforms multiple imperfect and messy healthcare data sources into clean, usable data so that we can learn from every cancer patient.</p>
<p>As a core member of our Platform and Integrations team, you will play a vital role in designing and building our core data platform and helping scale it to serve additional hospitals. You will lead major improvements to our data infrastructure and apply your skills in software development, architecture and data modeling to get things done.</p>
<p>We are a small team trying to tackle a very large problem, so we need teammates that are ultimately accountable to themselves and continuously push themselves, the product and the organization forward.</p>
<p><b>What you will be doing:</b></p>
<ul>
 <li>Designing and implementing improvements to our data extraction and transformation processes to increase performance and stability</li>
 <li>Architecting our data warehousing and reporting capabilities to support real-time analysis of tens of thousands of patients representing millions of data points</li>
 <li>Incrementally evolving our platform architecture and infrastructure without disrupting operations</li>
 <li>Overseeing and monitoring our existing data platforms for stability, performance and accuracy</li>
 <li>Working with Product and Engineering to define, document, and build transformations to extract intelligence from multiple incomplete and siloed data sources</li>
 <li>Building pipelines to de-identify and consolidate cross-institutional data to fuel predictive analytics, research, and clinical trials</li>
 <li>Building visibility into all aspects of our data platform (workflow status, system health, data lineage, etc.)</li>
</ul>
<p><b>About you:</b></p>
<ul>
 <li>Demonstrated experience independently leading complex software projects</li>
 <li>A strong base of software engineering experience, typically 5-8+ years, with a good portion of that leading the development of data platforms, pipelines or data-intensive projects</li>
 <li>Deep understanding of multiple database technologies, for example relational, document, key/value, columnar, etc (we use Postgres, MongoDB, Redis, and ElasticSearch)</li>
 <li>Experience building asynchronous and distributed systems (we use RabbitMQ)</li>
 <li>Fluency with a functional or imperative language (we use Python)</li>
 <li>A focus on writing understandable, testable, and maintainable code</li>
 <li>Familiarity with modern containerized environments (we use Docker &amp; Kubernetes)</li>
</ul>
<p><b>Bonuses:</b></p>
<ul>
 <li>Experience with healthcare data standards and integrations (HL7, FHIR, DICOM, etc.)</li>
</ul>
<p><b>Compensation, Benefits, and Perks:</b></p>
<ul>
 <li>Salary: &#x24;145k-180k plus equity compensation</li>
 <li>401k, health and dental insurance, flexible vacation, paid parental leave</li>
 <li>eBooks, online courses, home office budget</li>
 <li>Events: happy hours, team dinners, conversations with oncologists</li>
 <li>Work with smart, passionate people on a product that will have a direct impact on the lives of cancer patients</li>
</ul>
<p><b>What to expect in the hiring process:</b></p>
<ul>
 <li>Introductory phone call (15-30 minutes zoom call)</li>
 <li>Phone interview with CTO/CEO (60 min zoom call)</li>
 <li>Virtual onsite, including a pair programming session, engineering team meet, and co-founder meet (90-120 minute zoom call)</li>
 <li>Final stages, potential follow-up interviews, and offer discussions</li>
</ul>
<p>Job Type: Full-time</p>
<p>Pay: &#x24;120,000.00 - &#x24;185,000.00 per year</p>
<p>Benefits:</p>
<ul>
 <li>401(k)</li>
 <li>Dental insurance</li>
 <li>Flexible schedule</li>
 <li>Health insurance</li>
 <li>Paid time off</li>
 <li>Parental leave</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>4 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>Monday to Friday</li>
</ul>
<p>Work Location: Remote</p>",,"72b83dc7d8aad2ef",,"Full-time",,"Remote","Senior Software Engineer (Data Platform)","5 days ago","2023-10-20T11:49:06.279Z",,,"$120,000 - $185,000 a year","2023-10-25T11:49:06.280Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=72b83dc7d8aad2ef&from=jasx&tk=1hdjakuh7je3s800&vjs=3"
"GTECH LLC","Responsibilities

 Understand business requirements.
 Understand source systems, source data and source data formats that are available on-prem / cloud.
 Design and build data ingestion pipeline.
 Design and build complex data processing pipelines.
 Work with relevant stakeholders to assist with data-related technical issues and support their data needs.
 Build programs for data quality checks.
 Provide operational support.
 Work with data architecture, data governance and data analytics. teams to ensure pipelines adhere to enterprise standards, usability, and performance.
 Involve in System Testing, UAT, code deployment activities.
 Coordinate with offshore team on regular basis

Primary Skills
Azure Databricks
PySpark,
Scala
Snowflake (at least for 1 resource)
Secondary Skills
ADF
CICD
Airflow
SQL
Cloud Databases
Understanding of Agile methodologies
Experience Level
4-6 years of working experience in primary skills
Overall 8-10 years in ETL/Data Engineering
Job Type: Full-time
Salary: $83,492.68 - $130,739.06 per year
Benefits:

 401(k)
 Dental insurance
 Flexible schedule
 Health insurance
 Paid time off
 Tuition reimbursement
 Vision insurance

Experience level:

 10 years
 11+ years
 4 years
 5 years
 6 years
 7 years
 8 years
 9 years

Experience:

 Informatica: 1 year (Preferred)
 SQL: 1 year (Preferred)
 Data warehouse: 1 year (Preferred)

Work Location: Remote","<p>Responsibilities</p>
<ul>
 <li>Understand business requirements.</li>
 <li>Understand source systems, source data and source data formats that are available on-prem / cloud.</li>
 <li>Design and build data ingestion pipeline.</li>
 <li>Design and build complex data processing pipelines.</li>
 <li>Work with relevant stakeholders to assist with data-related technical issues and support their data needs.</li>
 <li>Build programs for data quality checks.</li>
 <li>Provide operational support.</li>
 <li>Work with data architecture, data governance and data analytics. teams to ensure pipelines adhere to enterprise standards, usability, and performance.</li>
 <li>Involve in System Testing, UAT, code deployment activities.</li>
 <li>Coordinate with offshore team on regular basis</li>
</ul>
<p>Primary Skills</p>
<p>Azure Databricks</p>
<p>PySpark,</p>
<p>Scala</p>
<p>Snowflake (at least for 1 resource)</p>
<p>Secondary Skills</p>
<p>ADF</p>
<p>CICD</p>
<p>Airflow</p>
<p>SQL</p>
<p>Cloud Databases</p>
<p>Understanding of Agile methodologies</p>
<p>Experience Level</p>
<p>4-6 years of working experience in primary skills</p>
<p>Overall 8-10 years in ETL/Data Engineering</p>
<p>Job Type: Full-time</p>
<p>Salary: &#x24;83,492.68 - &#x24;130,739.06 per year</p>
<p>Benefits:</p>
<ul>
 <li>401(k)</li>
 <li>Dental insurance</li>
 <li>Flexible schedule</li>
 <li>Health insurance</li>
 <li>Paid time off</li>
 <li>Tuition reimbursement</li>
 <li>Vision insurance</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>10 years</li>
 <li>11+ years</li>
 <li>4 years</li>
 <li>5 years</li>
 <li>6 years</li>
 <li>7 years</li>
 <li>8 years</li>
 <li>9 years</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Informatica: 1 year (Preferred)</li>
 <li>SQL: 1 year (Preferred)</li>
 <li>Data warehouse: 1 year (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,"d4db43c4028922ab",,"Full-time",,"Remote","Azure Data Engineer","5 days ago","2023-10-20T11:49:12.746Z",,,"$83,493 - $130,739 a year","2023-10-25T11:49:12.748Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=d4db43c4028922ab&from=jasx&tk=1hdjakuh7je3s800&vjs=3"
"Bright Vision Technologies","Bright Vision Technologies has an immediate opportunity for Lead Data Engineer in Santa Clara, CA.(Remote)
  Title: Lead Data Engineer Role: Remote Location: Santa Clara, CA.
  As a consultant within the DIE team, you will work with our clients to define their digital strategy and execution roadmap, and design and implement differentiated digital solutions to help deliver measurable value.
  Must have 12+years of experience and should have worked as Lead.
  This is W2 role.
  Your responsibilities in this role will include:
 
   Primary focus is on Glue, S3, Redshift, Lambda, PySpark, Spark.
   Then added skillset which could add value are AWS Step function, NoSQL DB like Dynamo DB and AWS Data Migration Service in that order of priority.
   Data engineer should have at least 2 years of relevant AWS experience with their services mentioned above.
   Experience in data security or governance and performance improvement is an added benefit
   Only focus is on AWS services and tech stack.
 
  Would you like to know more about our new opportunity? For immediate consideration, please send your resume directly to Venkata Raju at venkat.r@bvteck.com or Phone +1 (732) 298-7641 
 https://calendly.com/venkat-bvteck/15min 
 At BVTeck, we are committed to providing equal employment opportunities and fostering an inclusive work environment. We encourage applications from all qualified individuals regardless of race, ethnicity, religion, gender identity, sexual orientation, age, disability, or any other protected status. If you require accommodations during the recruitment process, please let us know.
  
 5S8nYMOotm","<div>
 <p>Bright Vision Technologies has an immediate opportunity for Lead Data Engineer in Santa Clara, CA.(Remote)</p>
 <p> Title: Lead Data Engineer<br> Role: Remote<br> Location: Santa Clara, CA.</p>
 <p> As a consultant within the DIE team, you will work with our clients to define their digital strategy and execution roadmap, and design and implement differentiated digital solutions to help deliver measurable value.</p>
 <p><b> Must have 12+years of experience and should have worked as Lead.</b></p>
 <p><b> This is W2 role.</b></p>
 <p> Your responsibilities in this role will include:</p>
 <ul>
  <li> Primary focus is on Glue, S3, Redshift, Lambda, PySpark, Spark.</li>
  <li> Then added skillset which could add value are AWS Step function, NoSQL DB like Dynamo DB and AWS Data Migration Service in that order of priority.</li>
  <li> Data engineer should have at least 2 years of relevant AWS experience with their services mentioned above.</li>
  <li> Experience in data security or governance and performance improvement is an added benefit</li>
  <li> Only focus is on AWS services and tech stack.</li>
 </ul>
 <p><i> Would you like to know more about our new opportunity? For immediate consideration, please send your resume directly to Venkata Raju at venkat.r@bvteck.com or Phone +1 (732) 298-7641</i></p> 
 <p><i>https://calendly.com/venkat-bvteck/15min</i></p> 
 <p><i>At BVTeck, we are committed to providing equal employment opportunities and fostering an inclusive work environment. We encourage applications from all qualified individuals regardless of race, ethnicity, religion, gender identity, sexual orientation, age, disability, or any other protected status. If you require accommodations during the recruitment process, please let us know.</i></p>
 <p> </p>
 <p>5S8nYMOotm</p>
</div>","https://brightvisiontechnologies.applytojob.com/apply/5S8nYMOotm/Lead-Data-Engineer?source=INDE","67ac721633aee65d",,,,"Remote","Lead Data Engineer","5 days ago","2023-10-20T11:49:07.566Z",,,"$70 an hour","2023-10-25T11:49:07.586Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=67ac721633aee65d&from=jasx&tk=1hdjakuh7je3s800&vjs=3"
"CyberCoders","Data Engineer 
 
  Job Title: Senior Data Engineer
   
  Remote: Yes, 100% Remote
   
  Job Type: Direct Hire
   
  Hours: Full-Time
   
  Base Salary Range: $140-185k (base) / year
   
   
  Given the clients work with government contracts, you must be an active US Citizen to apply. You will need to obtain a security clearance after hire, but do not need to have one currently. US CITIZENSHIP REQUIRED is for all Government Clearance. Thank you.
   
   Our team is looking for an experienced Senior Data Engineer to join us in working on top secret DOD cleared projects. Ideally someone with experience in time series and sensor data collecting with the ability to program in Python or R. We are building out a new data engineering team and this role would support our existing 4 data scientists in their efforts by building data pipelines, shaping data, etl, etc.
 
  What You Need for this Position
 
  Data Engineering / Cloud Engineering / Database Management Experience
  Incredible SQL Skills (MySQL / SQLite / Oracle)
  Programming experience (Python or R)
 
  
  Bonus Point For:
  
 
  Sensor Data / Imaging Data / Time Series Data
  IOT 
  Azure Gov / AWS
  Government Clearance
 
  What's In It for You
 
  Generous Base Salary
  401k (+match)
  Flexible Remote Schedule
  Health / Dental / Vision
  Vacation / PTO
  14 Paid holidays + Holiday break around new year / end of year
 
 
   So, if you are a Data Engineer with experience, please apply today!
 
 
   Colorado employees will receive paid sick leave. For additional information about available benefits, please contact Hanna Frauen
 
 
 Applicants must be authorized to work in the U.S.
  CyberCoders is proud to be an Equal Opportunity Employer
  
  All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any other characteristic protected by law.
  
  
 Your Right to Work – In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.","<div>
 Data Engineer 
 <div>
  <b>Job Title:</b> Senior Data Engineer
  <br> 
  <b>Remote:</b> Yes, 100% Remote
  <br> 
  <b>Job Type:</b> Direct Hire
  <br> 
  <b>Hours:</b> Full-Time
  <br> 
  <b>Base Salary Range:</b> &#x24;140-185k (base) / year
  <br> 
  <br> 
  <b>Given the clients work with government contracts, you must be an active US Citizen to apply. You will need to obtain a security clearance after hire, but do not need to have one currently. US CITIZENSHIP REQUIRED is for all Government Clearance. Thank you.</b>
  <br> 
  <br> Our team is looking for an experienced Senior Data Engineer to join us in working on top secret DOD cleared projects. Ideally someone with experience in time series and sensor data collecting with the ability to program in Python or R. We are building out a new data engineering team and this role would support our existing 4 data scientists in their efforts by building data pipelines, shaping data, etl, etc.
 </div>
 <h4 class=""jobSectionHeader""><b> What You Need for this Position</b></h4>
 <ul>
  <li>Data Engineering / Cloud Engineering / Database Management Experience</li>
  <li>Incredible SQL Skills (MySQL / SQLite / Oracle)</li>
  <li>Programming experience (Python or R)</li>
 </ul>
 <br> 
 <b> Bonus Point For:</b>
 <br> 
 <ul>
  <li>Sensor Data / Imaging Data / Time Series Data</li>
  <li>IOT </li>
  <li>Azure Gov / AWS</li>
  <li>Government Clearance</li>
 </ul>
 <h4 class=""jobSectionHeader""><b> What&apos;s In It for You</b></h4>
 <ul>
  <li>Generous Base Salary</li>
  <li>401k (+match)</li>
  <li>Flexible Remote Schedule</li>
  <li>Health / Dental / Vision</li>
  <li>Vacation / PTO</li>
  <li>14 Paid holidays + Holiday break around new year / end of year</li>
 </ul>
 <div>
   So, if you are a Data Engineer with experience, please apply today!
 </div>
 <div>
   Colorado employees will receive paid sick leave. For additional information about available benefits, please contact Hanna Frauen
 </div>
 <ul></ul>
 <p>Applicants must be authorized to work in the U.S.</p>
 <b><br> CyberCoders is proud to be an Equal Opportunity Employer</b>
 <br> 
 <br> All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any other characteristic protected by law.
 <br> 
 <br> 
 <b>Your Right to Work</b> &#x2013; In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.
</div>","https://www.cybercoders.com/jobs/details/717927/?ad=recruiticsindeed","cc4dd34c8fc5c188",,"Full-time",,"Raleigh, NC 27603","Data Engineer","1 day ago","2023-10-24T11:48:41.494Z","3.9","83","$145,000 - $185,000 a year","2023-10-25T11:48:41.496Z","US","remote","data engineer","https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0CpFJQzrgRR8WqXWK1qKKEqALWJw739KlKqr2H-MSI4ehQhkyt7GrkYFAdLCKqicVDWCL9ucYwrKT5Fe9OXxDBCh_wR0uZJBStm41vZmDvyUjKUlhD0Xmop5LZ48E3VmYHfKQVgyDV8cwhOOLxBxoGVQ9WVzvXciKJ6qVT8F-LlacrRvVkIAz0ld79gRHd7cM8bWk8jKo7U3lq-6nVEXeRyD4mebf3cWrZOp23ieFGeiTH2Pi9R2MBEiCOtSTtHIxMCEtTGezCEi7_EoNlz4tSG8fDDIiWxrtRek2D9y3ktvT2lW0JzwUV9_6XGb8YUj8PmnVphTwqqsFyXCsvV0tHEr55P84tvEc074e_EjtmD_5n3N8S55OuoVsCdFKlIrUSPTehle8oTqh6GSgFS439f8CkbqccR9M8Yw_qzTIxeyhjUpC4bWEjPawhtAW5k3R2kyJXJ5FYlBN332KeGYEiwrw86HUwWbsCpH4X9TRaPJJWiJEJ8rAy5wELOnPwHD09Rspn5ufsadJO4zArVzfIyDat0CLbCbDAVwbXIk89v6MI-oqR0F0b6KslJhdnMB97Aja9Ke66vOAhE6Kns-OQU6dKf-3yuIQEBR4B7XMO0E5V1iSYhpPwfh6ZTFCg3OCU45DoVWjEWkFTFTYUiMchVeX8EOBqL3iM37SIwh-qOMcxR8XXhRGq_wDOSx87UKgWknz-lLri6iWc3njIEL7Jie6SfE7tWjNQ_pRWcVtcB6BY3_rADwHDFNYV9O94vjVZBdVQQGDpzUZuyTtSaXqIFQgkoDT7yMrgw7I7u60BdRdZdB9oVplpLD8PZYbAQ1yOk8JYy7sqQ8aS2ymIpoblg8kHPGQmmoTYpA2iLy2l3dqdcB3fEpUEr_V72kC0nEKXERAkwHS6Vb_8npQoztJgp0NcejkgZL9R5xuZDcverMEW5lVzE9NvQl1-JokTJzKgmzoJQwmo30kvHN9jzHlGZ70CAYln-CCei7N2aszvLi9ZEMXOOyAsX4ScjV3p7tlcgcuFRUO1IlFp0B1IciCdRg73yQUqocE7GkdW5Pp5NsUXsdcGHFLfrytQIX5EJEOgLz2R37XW0Y2D2NdfeD0yNH8NKffB21r8UmvXyb6X6oqIUdcnKdH3xuGzJWZNOgRGYg62YETxhJmdHjBhPPSyhlSo2X1tJXgEbCOsiko7YzdqAAcGNonFn&xkcb=SoDI-_M3JzdfZfg2lj0JbzkdCdPP&p=2&fvj=0&vjs=3&jsa=5086&tk=1hdjak5rpj4ih800&from=jasx&wvign=1"
"Vista Global","Job Profile: 
 
   About Team
 
 
   The Data Foundation Team is highly critical for the organization to provide timely, accurate and most up to date data so that the business can take decisions accordingly. The team works with several application teams, data analytics, data science team etc. We are looking for a highly skilled and experienced 
  Senior Data Engineer to design, implement, and maintain robust and scalable data pipelines.
 
 
 
   About Company
 
 
   Vista Tech plays a vital role in the Vista group operations by delivering and accelerating comprehensive technology solutions across all brands. Vista’s end-to-end and click-to-flight solutions offer the industry's only comprehensive flight booking platform, seamlessly integrating global operations, and leveraging AI and machine learning to optimize pricing and fleet movement. Comprised of the Product Management, Engineering, and IT teams, Vista Tech’s mission is to enhance transparency and accessibility in private aviation through the development of the world's largest digital private aviation marketplace. In achieving this, Vista Tech always ensures the utmost safety and efficiency for FLIGHT CREW, EMPLOYEES and Members, while fostering a culture of innovation and excellence.
 
 
 
   You will report to Engineering Manager and play a crucial role in driving the technical direction of our projects and guiding the team in adopting best practices and cutting-edge technologies. This position is a 100% remote role with regular shif timings (9 AM to 6 AM EST). You will collaborate with cross-functional teams, provide technical leadership, and contribute to the entire software development lifecycle.
  Your Responsibilities: 
 
  Scalable Data Infrastructure: Lead the development and maintenance of highly scalable data pipelines, playing a crucial role in fortifying our data foundation.
   Technical Excellence: Demonstrate hands-on technical expertise in designing, building, and documenting complex data pipelines while adhering to data engineering best practices.
   Cross-Functional Collaboration: Collaborate closely with data engineering, analytics, and data science leadership to continuously enhance the functionality and capabilities of our data systems.
   Process Optimization: Identify opportunities for internal process improvements, spearheading automation of manual tasks, optimizing data delivery mechanisms, and redesigning infrastructure to ensure greater scalability and efficiency.
   Data Integration Mastery: Define and construct the infrastructure necessary to facilitate efficient extraction, transformation, and loading (ETL) of data from a diverse array of sources.
  Required Skills, Qualifications, and Experience: 
 
  
   
     Strong Analytical Foundation: A robust background in mathematics, statistics, computer science, data science, or a related discipline, showcasing your analytical prowess.
   
  
   
     Programming Proficiency: Advanced expertise in programming languages, particularly Python and SQL, to tackle complex data challenges.
   
  
   
     Production Experience: Proven experience in the production environment with a range of essential tools and platforms, including Snowflake, DBT, Airflow, Amazon Web Services (AWS), Docker/Kubernetes, and PostgreSQL.
   
  
   
     Database Mastery: Proficiency in database technologies, including Snowflake, PostgreSQL, Redshift, and others, enabling efficient data management.
   
  
   
     Exceptional Organizational Skills: Strong organizational capabilities, allowing you to manage multiple projects and priorities concurrently while consistently meeting deadlines.
   
  
   
     Additional Assets: Familiarity and experience with additional tools and technologies, such as AWS certification, Kafka Streaming/Kafka Connect, MongoDB, and CI/CD tools like GitLab, Jira, and Confluence, are highly advantageous.","<div>
 Job Profile: 
 <div>
  <b> About Team</b>
 </div>
 <div>
   The Data Foundation Team is highly critical for the organization to provide timely, accurate and most up to date data so that the business can take decisions accordingly. The team works with several application teams, data analytics, data science team etc. We are looking for a highly skilled and experienced 
  <b>Senior Data Engineer</b> to design, implement, and maintain robust and scalable data pipelines.
 </div>
 <div></div>
 <div>
  <b><br> About Company</b>
 </div>
 <div>
   Vista Tech plays a vital role in the Vista group operations by delivering and accelerating comprehensive technology solutions across all brands. Vista&#x2019;s end-to-end and click-to-flight solutions offer the industry&apos;s only comprehensive flight booking platform, seamlessly integrating global operations, and leveraging AI and machine learning to optimize pricing and fleet movement. Comprised of the Product Management, Engineering, and IT teams, Vista Tech&#x2019;s mission is to enhance transparency and accessibility in private aviation through the development of the world&apos;s largest digital private aviation marketplace. In achieving this, Vista Tech always ensures the utmost safety and efficiency for FLIGHT CREW, EMPLOYEES and Members, while fostering a culture of innovation and excellence.
 </div>
 <div></div>
 <div>
  <br> You will report to Engineering Manager and play a crucial role in driving the technical direction of our projects and guiding the team in adopting best practices and cutting-edge technologies. This position is a 100% remote role with regular shif timings (9 AM to 6 AM EST). You will collaborate with cross-functional teams, provide technical leadership, and contribute to the entire software development lifecycle.
 </div> Your Responsibilities: 
 <ul>
  <li><b>Scalable Data Infrastructure:</b> Lead the development and maintenance of highly scalable data pipelines, playing a crucial role in fortifying our data foundation.</li>
  <li><b> Technical Excellence:</b> Demonstrate hands-on technical expertise in designing, building, and documenting complex data pipelines while adhering to data engineering best practices.</li>
  <li><b> Cross-Functional Collaboration:</b> Collaborate closely with data engineering, analytics, and data science leadership to continuously enhance the functionality and capabilities of our data systems.</li>
  <li><b> Process Optimization:</b> Identify opportunities for internal process improvements, spearheading automation of manual tasks, optimizing data delivery mechanisms, and redesigning infrastructure to ensure greater scalability and efficiency.</li>
  <li><b> Data Integration Mastery:</b> Define and construct the infrastructure necessary to facilitate efficient extraction, transformation, and loading (ETL) of data from a diverse array of sources.</li>
 </ul> Required Skills, Qualifications, and Experience: 
 <ul>
  <li>
   <div>
    <b> Strong Analytical Foundation:</b> A robust background in mathematics, statistics, computer science, data science, or a related discipline, showcasing your analytical prowess.
   </div></li>
  <li>
   <div>
    <b> Programming Proficiency:</b> Advanced expertise in programming languages, particularly Python and SQL, to tackle complex data challenges.
   </div></li>
  <li>
   <div>
    <b> Production Experience:</b> Proven experience in the production environment with a range of essential tools and platforms, including Snowflake, DBT, Airflow, Amazon Web Services (AWS), Docker/Kubernetes, and PostgreSQL.
   </div></li>
  <li>
   <div>
    <b> Database Mastery:</b> Proficiency in database technologies, including Snowflake, PostgreSQL, Redshift, and others, enabling efficient data management.
   </div></li>
  <li>
   <div>
    <b> Exceptional Organizational Skills:</b> Strong organizational capabilities, allowing you to manage multiple projects and priorities concurrently while consistently meeting deadlines.
   </div></li>
  <li>
   <div>
    <b> Additional Assets:</b> Familiarity and experience with additional tools and technologies, such as AWS certification, Kafka Streaming/Kafka Connect, MongoDB, and CI/CD tools like GitLab, Jira, and Confluence, are highly advantageous.
   </div></li>
 </ul>
</div>","https://careers-vistaglobal.icims.com/jobs/3757/job?utm_source=indeed_integration&iis=Job%20Board&iisn=Indeed&indeed-apply-token=73a2d2b2a8d6d5c0a62696875eaebd669103652d3f0c2cd5445d3e66b1592b0f","85b86aa2f4ff1163",,"Full-time",,"Fort Lauderdale, FL 33394","Senior Data Engineer","5 days ago","2023-10-20T11:49:09.157Z",,,,"2023-10-25T11:49:09.158Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=85b86aa2f4ff1163&from=jasx&tk=1hdjakuh7je3s800&vjs=3"
"Microagility","We are seeking an experienced ETL (Extract, Transform, and Load) Data Engineer with expertise in Google Cloud Platform (GCP) to join our client data engineering team.
Key Responsibilities:
· Design, develop, and maintain ETL pipelines on Google Cloud Platform (GCP) to ensure efficient data extraction, transformation, and loading processes.
· Extract data from various sources, including databases, APIs, and cloud storage, and ensure data quality and consistency.
· Collaborate with data scientists, data analysts, and other stakeholders to understand their data requirements and ensure data pipelines meet their needs.
· Implement data transformations, including cleaning, aggregation, and enrichment, to prepare data for analysis and reporting.
· Design, build, and maintain ETL pipelines using GCP services like Dataflow, Dataprep, or other relevant tools.
· Performance Optimization: Continuously optimize ETL processes for speed, efficiency, and scalability within the GCP environment.
Key Requirements:
· Proven experience in designing and developing ETL pipelines on Google Cloud Platform (GCP).
· Experience with GCP data tools and services, such as BigQuery, Dataflow, Dataprep, and Data Studio.
· Knowledge of ETL tools, methodologies, and best practices.
· Experience with data warehousing concepts.
· Proficiency in data scripting languages, such as Python or SQL.
· Strong problem-solving skills and attention to detail.
· Excellent communication and teamwork skills.
Education:

 Bachelor’s Degree in Computer Science or related field

Why Work with Us
Competitive compensation.
Opportunity to work on innovative projects for high-profile clients.
A collaborative, inclusive, and supportive work environment.
Opportunities for professional growth and development.
Work with a team of passionate, talented, and creative individuals.
Job Type: Contract
Schedule:

 8 hour shift

Experience:

 ETL: 7 years (Preferred)
 Google Cloud Platform: 5 years (Preferred)

Work Location: Remote","<p>We are seeking an experienced ETL (Extract, Transform, and Load) Data Engineer with expertise in Google Cloud Platform (GCP) to join our client data engineering team.</p>
<p><b>Key Responsibilities:</b></p>
<p>&#xb7; Design, develop, and maintain ETL pipelines on Google Cloud Platform (GCP) to ensure efficient data extraction, transformation, and loading processes.</p>
<p>&#xb7; Extract data from various sources, including databases, APIs, and cloud storage, and ensure data quality and consistency.</p>
<p>&#xb7; Collaborate with data scientists, data analysts, and other stakeholders to understand their data requirements and ensure data pipelines meet their needs.</p>
<p>&#xb7; Implement data transformations, including cleaning, aggregation, and enrichment, to prepare data for analysis and reporting.</p>
<p>&#xb7; Design, build, and maintain ETL pipelines using GCP services like Dataflow, Dataprep, or other relevant tools.</p>
<p>&#xb7; Performance Optimization: Continuously optimize ETL processes for speed, efficiency, and scalability within the GCP environment.</p>
<p><b>Key Requirements:</b></p>
<p>&#xb7; Proven experience in designing and developing ETL pipelines on Google Cloud Platform (GCP).</p>
<p>&#xb7; Experience with GCP data tools and services, such as BigQuery, Dataflow, Dataprep, and Data Studio.</p>
<p>&#xb7; Knowledge of ETL tools, methodologies, and best practices.</p>
<p>&#xb7; Experience with data warehousing concepts.</p>
<p>&#xb7; Proficiency in data scripting languages, such as Python or SQL.</p>
<p>&#xb7; Strong problem-solving skills and attention to detail.</p>
<p>&#xb7; Excellent communication and teamwork skills.</p>
<p><b>Education:</b></p>
<ul>
 <li>Bachelor&#x2019;s Degree in Computer Science or related field</li>
</ul>
<p><b>Why Work with Us</b></p>
<p>Competitive compensation.</p>
<p>Opportunity to work on innovative projects for high-profile clients.</p>
<p>A collaborative, inclusive, and supportive work environment.</p>
<p>Opportunities for professional growth and development.</p>
<p>Work with a team of passionate, talented, and creative individuals.</p>
<p>Job Type: Contract</p>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
</ul>
<p>Experience:</p>
<ul>
 <li>ETL: 7 years (Preferred)</li>
 <li>Google Cloud Platform: 5 years (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,"e981504596ce7e00",,"Contract",,"Remote","Data Engineer GCP ETL","4 days ago","2023-10-21T11:49:21.981Z",,,,"2023-10-25T11:49:21.985Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=e981504596ce7e00&from=jasx&tk=1hdjaj0h4j4gl800&vjs=3"
"PrizePicks","At PrizePicks, we are the fastest growing sports company in North America, as recognized by Inc. 5000. As the leading platform for Daily Fantasy Sports, we cover a diverse range of sports leagues, including the NFL, NBA, and Esports titles like League of Legends and CS:GO. Our team of over 350 employees thrives in an inclusive culture that values individuals from diverse backgrounds, regardless of their level of sports fandom. Ready to reimagine the DFS industry together?
 
  The Analytics Team is responsible for building and maintaining analytics tools and workflows to support the PrizePicks business across all departments — at the core of these operations is data. As a Data Science Engineer, you will be developing, maintaining, and testing streaming data and MLOps infrastructures to enable PrizePicks to offer real-time priced markets within the product. 
  What you'll do: 
 
  Create and maintain optimal sport data stream architecture, ensuring data reliability in both speed and quality for both raw and transformed data pipelines. 
  Partner with Data Science to determine the best paths for operationalization of DS/ML assets, ensuring model output quality, stability, and scalability. 
  Lead the design and implementation of the data and MLOps stack required for real-time pricing models and contribute to architecture evaluations and decisions for our growing data product roadmap. 
  Work cross-functionally with Engineering, QA, and Product teams to enable the creation and distribution of highly-visible, real-time, in-game micromarket offerings to the PrizePicks platform. 
  Build and own rigorous monitoring, alerting, and documentation processes, and work with Engineering teams to ensure complete feature uptime. 
 
 What you have: 
 
  3+ years of experience in a data science, machine learning engineer, or data-oriented software engineering role creating and pushing end-to-end data science pipelines and MLOps assets to production. 
  Experience building and optimizing cloud-based data streaming pipelines and infrastructure. 
  Experience exposing real-time predictive model outputs to production-grade systems leveraging large-scale distributed data processing and model training. 
  Experience with the following: 
  Strong organizational, communication, presentation, and collaboration experience with organizational technical and non-technical teams 
  Graduate degree in Computer Science, Statistics, Mathematics, Informatics, Information Systems or other quantitative field 
  Preferred: experience building real-time production data science pipelines in a daily fantasy sports or oddsmaking business 
 
 Where you'll live: 
 
  Anywhere in the US is fine but Atlanta would be preferred. 
 
 Benefits you'll receive: 
  In addition to your great compensation package, company subsidized medical/dental/vision coverage plans and matching 401(k), we'll shower you with perks including: 
  
  Break room with ping pong, endless snacks and in-office lunch once a week 
  Unlimited PTO to encourage a healthy work/life balance (2 week min required!) 
  Modern work schedule focused on getting the job done, not hours clocked 
  Workplace flexibility 
  Company and team outings, we encourage a tight-knit workplace 
  Generous Maternity AND Paternity leave (16 weeks!) 
  Annual bonus & stock options 
  Wellness program 
  Company equipment provided (Windows & Mac options) 
  Annual performance reviews with opportunity for growth and career development
 
 
 
   You must be authorized to work for any employer in the U.S. We are unable to sponsor or take over sponsorship of an employment Visa at this time. 
   PrizePicks is an Equal Opportunity Employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.","<div>
 <div>
  <p>At PrizePicks, we are the fastest growing sports company in North America, as recognized by Inc. 5000. As the leading platform for Daily Fantasy Sports, we cover a diverse range of sports leagues, including the NFL, NBA, and Esports titles like League of Legends and CS:GO. Our team of over 350 employees thrives in an inclusive culture that values individuals from diverse backgrounds, regardless of their level of sports fandom. Ready to reimagine the DFS industry together?</p>
 </div>
 <p> The Analytics Team is responsible for building and maintaining analytics tools and workflows to support the PrizePicks business across all departments &#x2014; at the core of these operations is data. As a Data Science Engineer, you will be developing, maintaining, and testing streaming data and MLOps infrastructures to enable PrizePicks to offer real-time priced markets within the product.</p> 
 <h1 class=""jobSectionHeader""><b> What you&apos;ll do:</b></h1> 
 <ul>
  <li>Create and maintain optimal sport data stream architecture, ensuring data reliability in both speed and quality for both raw and transformed data pipelines.</li> 
  <li>Partner with Data Science to determine the best paths for operationalization of DS/ML assets, ensuring model output quality, stability, and scalability.</li> 
  <li>Lead the design and implementation of the data and MLOps stack required for real-time pricing models and contribute to architecture evaluations and decisions for our growing data product roadmap.</li> 
  <li>Work cross-functionally with Engineering, QA, and Product teams to enable the creation and distribution of highly-visible, real-time, in-game micromarket offerings to the PrizePicks platform.</li> 
  <li>Build and own rigorous monitoring, alerting, and documentation processes, and work with Engineering teams to ensure complete feature uptime.</li> 
 </ul>
 <h1 class=""jobSectionHeader""><b>What you have:</b></h1> 
 <ul>
  <li>3+ years of experience in a data science, machine learning engineer, or data-oriented software engineering role creating and pushing end-to-end data science pipelines and MLOps assets to production.</li> 
  <li>Experience building and optimizing cloud-based data streaming pipelines and infrastructure.</li> 
  <li>Experience exposing real-time predictive model outputs to production-grade systems leveraging large-scale distributed data processing and model training.</li> 
  <li>Experience with the following:</li> 
  <li>Strong organizational, communication, presentation, and collaboration experience with organizational technical and non-technical teams</li> 
  <li>Graduate degree in Computer Science, Statistics, Mathematics, Informatics, Information Systems or other quantitative field</li> 
  <li>Preferred: experience building real-time production data science pipelines in a daily fantasy sports or oddsmaking business</li> 
 </ul>
 <h1 class=""jobSectionHeader""><b>Where you&apos;ll live:</b></h1> 
 <ul>
  <li>Anywhere in the US is fine but Atlanta would be preferred.</li> 
 </ul>
 <h1 class=""jobSectionHeader""><b>Benefits you&apos;ll receive:</b></h1> 
 <p> In addition to your great compensation package, company subsidized medical/dental/vision coverage plans and matching 401(k), we&apos;ll shower you with perks including:</p> 
 <ul> 
  <li>Break room with ping pong, endless snacks and in-office lunch once a week</li> 
  <li>Unlimited PTO to encourage a healthy work/life balance (2 week min required!)</li> 
  <li>Modern work schedule focused on getting the job done, not hours clocked</li> 
  <li>Workplace flexibility </li>
  <li>Company and team outings, we encourage a tight-knit workplace</li> 
  <li>Generous Maternity AND Paternity leave (16 weeks!)</li> 
  <li>Annual bonus &amp; stock options </li>
  <li>Wellness program</li> 
  <li>Company equipment provided (Windows &amp; Mac options)</li> 
  <li>Annual performance reviews with opportunity for growth and career development</li>
 </ul>
 <p></p>
 <div>
  <p><b><br> You must be authorized to work for any employer in the U.S. We are unable to sponsor or take over sponsorship of an employment Visa at this time.</b></p> 
  <p><i> PrizePicks is an Equal Opportunity Employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.</i></p>
 </div>
</div>","https://www.prizepicks.com/position?gh_jid=5780099003&gh_src=a30715853us","f6025098ab97e827",,,,"Atlanta, GA 30309","Data Science Engineer","5 days ago","2023-10-20T11:49:09.844Z","4.3","6",,"2023-10-25T11:49:09.846Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=f6025098ab97e827&from=jasx&tk=1hdjakuh7je3s800&vjs=3"
"Twin Health","Twin Health 
   At Twin Health, we empower people to reverse, prevent and improve chronic metabolic diseases. Twin Health invented The Whole Body Digital Twin™ , a dynamic representation of each individual's unique metabolism, built from thousands of data points collected daily via non-invasive sensors and self-reported preferences. The Whole Body Digital Twin delivers a new standard of care, empowering physicians and patients to make personalized data-driven decisions. 
   Working here 
   Our team is passionate, talented, and driven by our purpose to improve the health and happiness of our members. Our culture empowers each Twin to do what's needed to create value for our customers and our company, and enjoy their experience at work. Twin Health was awarded Innovator of the Year by Employer Health Innovation Roundtable (EHIR) (out of 358 companies), named to the 2021 CB Insights Digital Health 150, and recognized by Built In's 2022 Best Places To Work Awards. In October 2021, Twin Health announced its Series C funding round of $155M, led by ICONIQ Growth, enabling us to scale services in the U.S. and globally, helping to solve the global chronic metabolic disease health crisis. We have recently announced broad and growing partnerships with premier employers, such as Blackstone and Berkshire Hathaway. We are building the company you always wished you worked for. Join us in revolutionizing healthcare and building the most impactful digital health company in the world! 
   Excited to join us and do your part in improving people's health and happiness?
 
  Opportunity 
  Twin Heath is expanding rapidly across health providers nationwide in the US as well as India. We are seeking an experienced Data Warehouse Engineer to join our growing team to own the entire process of building a data warehouse from scratch and managing ETL pipelines, working collaboratively with internal business stakeholders and customers. We are looking for candidates physically located in PST. 
  Responsibilities 
  
  Drive analysis, architecture, design, and development of Cloud based data warehouse and business intelligence solutions. 
   Design and develop data flows and models for data warehousing and self-serve reporting. 
   Design and build extensible data acquisition and integration solutions to meet business reporting and analytics needs. 
   Implement a comprehensive framework to support logging, profile and audit high volume high frequency file processing supporting multiple formats. 
   Automate and optimize existing data processing workloads to integrate with the enterprise data warehouse. 
   Understand and translate business needs into data models to support long-term, scalable, and reliable solutions. 
   Create data models for self-serve reporting. 
   Build data pipelines from systems such as CRM, ERP and internal applications with the emphasis on scalability and reliability. 
   Partner with business users, senior architects, and infrastructure engineers to form complete end-to end-solutions. 
   Drive data quality across the organization; develop best practices for standard naming conventions and coding practices to ensure consistency of data models and tracking. 
   Prepare to respond to ad hoc reporting needs of business functions. 
   Work in a fast-paced environment and perform effectively in a sprint based agile development environment. 
   Collaborate with developers and analysts on technical and functional designs. 
   Analyze query performance and perform query tuning to assist development engineers in designing and optimizing queries. 
  Applicants must be authorized to work for any employer in the U.S. We are unable to sponsor or take over sponsorship of an employment Visa at this time. 
  This remote opportunity is available to US based persons 
  
 Qualifications 
  
  Bachelor's degree in computers Ince or related field. 
   At least 8 year of overall experience building data warehouse solutions with solid data modeling skills. 
   At least 3 years of hands-on experience in building Data Warehouses in Snowflake or Redshift 
   At least 4 years of experience in developing data pipelines using Python, PySpark or Scala. 
   Experience with API design and development using RESTful and/or SOAP protocols. 
   At least 2 years of experience creating DBT models and in a cloud, data Warehouses platform. 
   Knowledge of advanced SQL scripting and ability to write complex queries. 
   Experience using Rivery or new age ELT tools like Fivetran, Matillion etc. 
   Understanding of agile methodologies such as CI/CD, Applicant Resiliency, and Security 
   Experience working with various file formats at scale. 
   Experience with AWS. Familiarity with AWS Athena, Glue, Data Pipeline is a plus. 
   At least 2 years of experience with Salesforce Health Cloud, Marketing Cloud and NetSuite Schemas. 
   Experience designing highly scalable ETL/ELT processes with complex data transformations, data formats including error handling and monitoring. 
   Excellent analytical, problem solving, and troubleshooting skills, and solid communication skills. 
  
 Compensation and Benefits 
  The compensation range for this position is $140,000 annually. 
  In addition, Twin has an ambitious vision to empower people to live healthier and happier lives, and to achieve this purpose, we need the very best people to enhance our cutting-edge technology and medical science, deliver the best possible care, and turn our passion into value for our members, partners and investors. We are committed to delivering an outstanding culture and experience for every Twin employee through a company based on the values of passion, talent, and trust. We offer comprehensive benefits and perks in line with these principles, as well as a high level of flexibility for every Twin 
  
  A competitive compensation package in line with leading technology companies 
  As a remote friendly company we are committed to providing opportunities for all who join to further build relationships, increase cross-functional collaboration, and celebrate our accomplishments. 
  Opportunity for equity participation 
  Unlimited vacation with manager approval 
  16 weeks of 100% paid parental leave for delivering parents; 8 weeks of 100% paid parental leave for non-delivering parents 
  100% Employer sponsored healthcare, dental, and vision for you, and 80% coverage for your family; Health Savings Account and Flexible Spending Account options 
  401k retirement savings plan 
 
 
  
   Salary range for US jobs
  
   US Salary Range
  
    $140,000—$140,000 USD","<div>
 <div>
  <p><b>Twin Health</b></p> 
  <p> At Twin Health, we empower people to reverse, prevent and improve chronic metabolic diseases. Twin Health invented The Whole Body Digital Twin&#x2122; , a dynamic representation of each individual&apos;s unique metabolism, built from thousands of data points collected daily via non-invasive sensors and self-reported preferences. The Whole Body Digital Twin delivers a new standard of care, empowering physicians and patients to make personalized data-driven decisions.</p> 
  <p><b> Working here</b></p> 
  <p> Our team is passionate, talented, and driven by our purpose to improve the health and happiness of our members. Our culture empowers each Twin to do what&apos;s needed to create value for our customers and our company, and enjoy their experience at work. Twin Health was awarded Innovator of the Year by Employer Health Innovation Roundtable (EHIR) (out of 358 companies), named to the 2021 CB Insights Digital Health 150, and recognized by Built In&apos;s 2022 Best Places To Work Awards. In October 2021, Twin Health announced its Series C funding round of &#x24;155M, led by ICONIQ Growth, enabling us to scale services in the U.S. and globally, helping to solve the global chronic metabolic disease health crisis. We have recently announced broad and growing partnerships with premier employers, such as Blackstone and Berkshire Hathaway. We are building the company you always wished you worked for. Join us in revolutionizing healthcare and building the most impactful digital health company in the world!</p> 
  <p><b> Excited to join us and do your part in improving people&apos;s health and happiness?</b></p>
 </div>
 <p><b> Opportunity</b></p> 
 <p> Twin Heath is expanding rapidly across health providers nationwide in the US as well as India. We are seeking an experienced Data Warehouse Engineer to join our growing team to own the entire process of building a data warehouse from scratch and managing ETL pipelines, working collaboratively with internal business stakeholders and customers. We are looking for candidates physically located in PST.</p> 
 <p><b> Responsibilities</b></p> 
 <ul> 
  <li>Drive analysis, architecture, design, and development of Cloud based data warehouse and business intelligence solutions.</li> 
  <li> Design and develop data flows and models for data warehousing and self-serve reporting.</li> 
  <li> Design and build extensible data acquisition and integration solutions to meet business reporting and analytics needs.</li> 
  <li> Implement a comprehensive framework to support logging, profile and audit high volume high frequency file processing supporting multiple formats.</li> 
  <li> Automate and optimize existing data processing workloads to integrate with the enterprise data warehouse.</li> 
  <li> Understand and translate business needs into data models to support long-term, scalable, and reliable solutions.</li> 
  <li> Create data models for self-serve reporting.</li> 
  <li> Build data pipelines from systems such as CRM, ERP and internal applications with the emphasis on scalability and reliability.</li> 
  <li> Partner with business users, senior architects, and infrastructure engineers to form complete end-to end-solutions.</li> 
  <li> Drive data quality across the organization; develop best practices for standard naming conventions and coding practices to ensure consistency of data models and tracking.</li> 
  <li> Prepare to respond to ad hoc reporting needs of business functions.</li> 
  <li> Work in a fast-paced environment and perform effectively in a sprint based agile development environment.</li> 
  <li> Collaborate with developers and analysts on technical and functional designs.</li> 
  <li> Analyze query performance and perform query tuning to assist development engineers in designing and optimizing queries.</li> 
  <li>Applicants must be authorized to work for any employer in the U.S. We are unable to sponsor or take over sponsorship of an employment Visa at this time.</li> 
  <li>This remote opportunity is available to US based persons</li> 
 </ul> 
 <p><b>Qualifications</b></p> 
 <ul> 
  <li>Bachelor&apos;s degree in computers Ince or related field.</li> 
  <li> At least 8 year of overall experience building data warehouse solutions with solid data modeling skills.</li> 
  <li> At least 3 years of hands-on experience in building Data Warehouses in Snowflake or Redshift</li> 
  <li> At least 4 years of experience in developing data pipelines using Python, PySpark or Scala.</li> 
  <li> Experience with API design and development using RESTful and/or SOAP protocols.</li> 
  <li> At least 2 years of experience creating DBT models and in a cloud, data Warehouses platform.</li> 
  <li> Knowledge of advanced SQL scripting and ability to write complex queries.</li> 
  <li> Experience using Rivery or new age ELT tools like Fivetran, Matillion etc.</li> 
  <li> Understanding of agile methodologies such as CI/CD, Applicant Resiliency, and Security</li> 
  <li> Experience working with various file formats at scale.</li> 
  <li> Experience with AWS. Familiarity with AWS Athena, Glue, Data Pipeline is a plus.</li> 
  <li> At least 2 years of experience with Salesforce Health Cloud, Marketing Cloud and NetSuite Schemas.</li> 
  <li> Experience designing highly scalable ETL/ELT processes with complex data transformations, data formats including error handling and monitoring.</li> 
  <li> Excellent analytical, problem solving, and troubleshooting skills, and solid communication skills.</li> 
 </ul> 
 <p><b>Compensation and Benefits</b></p> 
 <p> The compensation range for this position is &#x24;140,000 annually.</p> 
 <p> In addition, Twin has an ambitious vision to empower people to live healthier and happier lives, and to achieve this purpose, we need the very best people to enhance our cutting-edge technology and medical science, deliver the best possible care, and turn our passion into value for our members, partners and investors. We are committed to delivering an outstanding culture and experience for every Twin employee through a company based on the values of passion, talent, and trust. We offer comprehensive benefits and perks in line with these principles, as well as a high level of flexibility for every Twin</p> 
 <ul> 
  <li>A competitive compensation package in line with leading technology companies</li> 
  <li>As a remote friendly company we are committed to providing opportunities for all who join to further build relationships, increase cross-functional collaboration, and celebrate our accomplishments.</li> 
  <li>Opportunity for equity participation</li> 
  <li>Unlimited vacation with manager approval</li> 
  <li>16 weeks of 100% paid parental leave for delivering parents; 8 weeks of 100% paid parental leave for non-delivering parents</li> 
  <li>100% Employer sponsored healthcare, dental, and vision for you, and 80% coverage for your family; Health Savings Account and Flexible Spending Account options</li> 
  <li>401k retirement savings plan</li> 
 </ul>
 <div>
  <div>
   Salary range for US jobs
  </div>
  <p><b> US Salary Range</b></p>
  <div>
    &#x24;140,000&#x2014;&#x24;140,000 USD
  </div>
 </div>
</div>","https://boards.greenhouse.io/twinhealth/jobs/5002329004?gh_src=3d8c6d624us","3b61441b9b1ff9fa",,,,"Remote","Senior Data Warehouse Engineer","5 days ago","2023-10-20T11:49:16.814Z","3.6","9","$140,000 a year","2023-10-25T11:49:16.816Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=3b61441b9b1ff9fa&from=jasx&tk=1hdjakuh7je3s800&vjs=3"
"Kellanova","As a Senior Data Engineer , you will play a pivotal role in collecting, processing, and preparing enterprise data for analytics and reporting, to support Kellanova’s digital business initiatives. You’ll have the opportunity to lead the design and implementation of data products, working with a range of AWS services, including S3, Redshift, EMR, Lambda, Glue and SageMaker. 

 You will be scoping and designing technical solutions, working with a variety of internal customers, including functional stakeholders, data scientists and data analysts. Developing and maintaining solutions to ingest raw data from enterprise systems and third parties into the Kellanova data lake and designing and building data warehouse models using AWS services. Location preference is Chicago, IL or Battle Creek, MI but the position can also be remote. 

 A Taste of What You’ll Be Doing 

 Cross-Functional Partnerships - Scoping and designing technical solutions, working with a variety of internal customers, including functional stakeholders, data scientists and data analysts. 
Project Management - Leading data engineering projects with internal and external partners. Supporting technical aspects of projects from requirements through to implementation, by serving as an SME and educator to the organization and supplier partners. 
Data Expertise - Analysis of incoming data and trends. Development of monitoring systems to track anomalies and data quality issues. Engineering data transformations to prepare data for analytics according to business requirements. Developing and maintaining solutions to ingest raw data from enterprise systems and third parties into the Kellanova data lake. 
Innovation Mindset - Driving innovation and continuous improvement of the global data engineering practice, including evaluation of emerging technologies and approaches. Establishing and championing data engineering best practices within the engineering community. 

 Your Recipe for Success 

 Bachelor’s degree in computer science, Information Management, Engineering, or related field with 7 to 10 years’ hands on experience in a technical field 
Demonstrated technical/business experience in problem-framing and problem-solving with required project management skills. 
Experience in data analysis, modeling, and mining. 
Strong experience with data engineering tools and technology (E.g., Redshift, Spark, dbt, PowerBI) 
Experience with SQL, Hive, Python, Scala, or other data querying languages. 
Demonstrated experience translating complex and technical subject matter. 
Ability to manage multiple and sometimes conflicting priorities. 

 What’s Next 

 After you apply, your application will be reviewed by a real recruiter – not a bot. This means it could take us a little while to get back with you so watch your inbox for updates. In the meantime, visit our How We Hire page to get insights into our hiring process and how to best prepare for a Kellanova interview. 

 If we can help you with a reasonable accommodation throughout the application or hiring process, please email USA.Recruitment@Kellanova.com . 

 About Kellanova 

 Kellanova is a leading company in global snacking, international cereal and noodles, plant-based foods, and North America frozen breakfast, and a portfolio of iconic, world-class brands, including Pringles, Cheez-It, Pop-Tarts, Kellogg’s Rice Krispies Treats, MorningStar Farms, Incogmeato, Gardenburger, Nutri-Grain, RXBAR, and Eggo. We also steward a suite of beloved international cereal brands, including Kellogg’s, Frosties, Zucaritas, Special K, Krave, Miel Pops, Coco Pops, and Crunchy Nut, among others. 

 At Kellanova, we are committed to Equity, Diversity, and Inclusion (ED&I), uplifting each other and embracing our differences to achieve our common goals. Our focus on ED&I enables us to build a culture where all employees are inspired to share their passion, talents, ideas, and can bring their authentic selves to work. Learn more here . 

 We’re proud to offer industry competitive Total Health benefits (Physical, Financial, Emotional, and Social) that vary depending on region and type of role. Be sure to ask your recruiter for more information! 

 The Finer Print 

 Kellanova is an Equal Opportunity Employer that strives to provide an inclusive work environment, a seat for everyone at the table, and embraces the diverse talent of its people. All qualified applicants will receive consideration for employment without regard to race, color, ethnicity, disability, religion, national origin, gender, gender identity, gender expression, marital status, sexual orientation, age, protected veteran status, or any other characteristic protected by law. For more information regarding our efforts to advance Equity, Diversity & Inclusion, please visit our website here . #LIRemote 

 Ready to Taste the Future of Food? 

 
 Kellanova Recruitment","As a Senior Data Engineer , you will play a pivotal role in collecting, processing, and preparing enterprise data for analytics and reporting, to support Kellanova&#x2019;s digital business initiatives. You&#x2019;ll have the opportunity to lead the design and implementation of data products, working with a range of AWS services, including S3, Redshift, EMR, Lambda, Glue and SageMaker. 
<br>
<br> You will be scoping and designing technical solutions, working with a variety of internal customers, including functional stakeholders, data scientists and data analysts. Developing and maintaining solutions to ingest raw data from enterprise systems and third parties into the Kellanova data lake and designing and building data warehouse models using AWS services. Location preference is Chicago, IL or Battle Creek, MI but the position can also be remote. 
<br>
<br> A Taste of What You&#x2019;ll Be Doing 
<br>
<br> Cross-Functional Partnerships - Scoping and designing technical solutions, working with a variety of internal customers, including functional stakeholders, data scientists and data analysts. 
<br>Project Management - Leading data engineering projects with internal and external partners. Supporting technical aspects of projects from requirements through to implementation, by serving as an SME and educator to the organization and supplier partners. 
<br>Data Expertise - Analysis of incoming data and trends. Development of monitoring systems to track anomalies and data quality issues. Engineering data transformations to prepare data for analytics according to business requirements. Developing and maintaining solutions to ingest raw data from enterprise systems and third parties into the Kellanova data lake. 
<br>Innovation Mindset - Driving innovation and continuous improvement of the global data engineering practice, including evaluation of emerging technologies and approaches. Establishing and championing data engineering best practices within the engineering community. 
<br>
<br> Your Recipe for Success 
<br>
<br> Bachelor&#x2019;s degree in computer science, Information Management, Engineering, or related field with 7 to 10 years&#x2019; hands on experience in a technical field 
<br>Demonstrated technical/business experience in problem-framing and problem-solving with required project management skills. 
<br>Experience in data analysis, modeling, and mining. 
<br>Strong experience with data engineering tools and technology (E.g., Redshift, Spark, dbt, PowerBI) 
<br>Experience with SQL, Hive, Python, Scala, or other data querying languages. 
<br>Demonstrated experience translating complex and technical subject matter. 
<br>Ability to manage multiple and sometimes conflicting priorities. 
<br>
<br> What&#x2019;s Next 
<br>
<br> After you apply, your application will be reviewed by a real recruiter &#x2013; not a bot. This means it could take us a little while to get back with you so watch your inbox for updates. In the meantime, visit our How We Hire page to get insights into our hiring process and how to best prepare for a Kellanova interview. 
<br>
<br> If we can help you with a reasonable accommodation throughout the application or hiring process, please email USA.Recruitment@Kellanova.com . 
<br>
<br> About Kellanova 
<br>
<br> Kellanova is a leading company in global snacking, international cereal and noodles, plant-based foods, and North America frozen breakfast, and a portfolio of iconic, world-class brands, including Pringles, Cheez-It, Pop-Tarts, Kellogg&#x2019;s Rice Krispies Treats, MorningStar Farms, Incogmeato, Gardenburger, Nutri-Grain, RXBAR, and Eggo. We also steward a suite of beloved international cereal brands, including Kellogg&#x2019;s, Frosties, Zucaritas, Special K, Krave, Miel Pops, Coco Pops, and Crunchy Nut, among others. 
<br>
<br> At Kellanova, we are committed to Equity, Diversity, and Inclusion (ED&amp;I), uplifting each other and embracing our differences to achieve our common goals. Our focus on ED&amp;I enables us to build a culture where all employees are inspired to share their passion, talents, ideas, and can bring their authentic selves to work. Learn more here . 
<br>
<br> We&#x2019;re proud to offer industry competitive Total Health benefits (Physical, Financial, Emotional, and Social) that vary depending on region and type of role. Be sure to ask your recruiter for more information! 
<br>
<br> The Finer Print 
<br>
<br> Kellanova is an Equal Opportunity Employer that strives to provide an inclusive work environment, a seat for everyone at the table, and embraces the diverse talent of its people. All qualified applicants will receive consideration for employment without regard to race, color, ethnicity, disability, religion, national origin, gender, gender identity, gender expression, marital status, sexual orientation, age, protected veteran status, or any other characteristic protected by law. For more information regarding our efforts to advance Equity, Diversity &amp; Inclusion, please visit our website here . #LIRemote 
<br>
<br> Ready to Taste the Future of Food? 
<br>
<ul> 
 <li>Kellanova Recruitment</li> 
</ul>","https://click.appcast.io/track/hq57jf8-org?cs=5c&jg=5ry0&bid=lUf2CslKyPxm6i440ZgUYA==&ittk=YPMSA3RUMR","5c9800845ac3e7c9",,,,"Naperville, IL 60564","Senior Data Engineer","5 days ago","2023-10-20T11:49:20.092Z","3.6","2682",,"2023-10-25T11:49:20.097Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=5c9800845ac3e7c9&from=jasx&tk=1hdjakuh7je3s800&vjs=3"
"Lucayan Technology Solutions","On-Site Job
Description:
12 Months Contract
MUST HAVE's:Database developmentWorking knowledge on AWSOracle, Snowflake, Python, CICD, Jenkins8 years' experience in the space
NICE TO HAVE's:Docker containers and EKS knowledgeReporting tools – Tableau or Power BICandidate that has worked in a complex financial environment
DescriptionIf you have a passion for working with data using multiple emerging technologies on the cloud, this might be the right opportunity for you! The Cloud Data Engineer will be working as part of a core team building solutions for the data analytics platform for Client’s Asset Management Division. This will involve designing and developing solutions for a variety of data lake needs using Snowflake as the data store for structured/semi-structured data and AWS s3 for unstructured data.We are seeking a highly motivated expert data engineer with a strong agile mentality, who’s looking for new challenges, we have an exciting opportunity for you to join our fast paced and highly collaborative group. This role will be involved in the full end-to-end process through planning, design, development, quality, and implementation of solutions.This position can be in Merrimack, Boston, Raleigh, and Westlake.
The Expertise and Skills You BringExpertise in SQL, identifying patterns and trends in data, recommend and define data requirements, mastery in implementing data quality checks to ensure accuracy and completeness.You enjoy learning new technologies, data analysis, identifying data patterns and trends and you can independently resolve technical challenges.Experience in processing and exposing data using AWS technologies like ec2, s3, Lambda, API Gateway, Load Balancers, Auto scaling etc.Expertise in building data ingestion tools using technologies like Python to extract data from Relational Databases/Web Scraping/External API’sExperience in Snowflake or any MPP and columnar database on the Cloud.Experience in CI/CD release automation and deployment (Jenkins, Concourse, CloudFormation etc.)Experience and good understating of databases (Oracle, Netezza) and ETL toolsExperience in scheduling tools like Autosys, Control-M, Airflow etc.Excellent programming skills in SQL, PL/SQL, Python, shell etc.Exposure to Big Data technologies (Hadoop, Spark, Hive, presto etc.)Exposure to streaming services like Kafka and containers will be an advantage.Good understanding of overall AWS security services like KMS, IAM, Security groups etc.At least 8 years of software development experience with at least 3 years working on Cloud/Big Data technologies.BS in Computer Science or related degree, or equivalent experienceAt Client, you can find it all here. We reward ambitious, passionate individuals with a work environment that furthers diversity, partnership and collaboration as well as encourages innovative ideas and fresh thinking. We recognize the value that employees’ individual differences can give to the forward-thinking and strong future of our company.
Education: Bachelors Degree
Job Type: Contract
Experience level:

 10 years

Work Location: On the road","<p>On-Site Job</p>
<p><b>Description:</b></p>
<p>12 Months Contract</p>
<p><b>MUST HAVE&apos;s:</b><br>Database development<br>Working knowledge on AWS<br>Oracle, Snowflake, Python, CICD, Jenkins<br>8 years&apos; experience in the space</p>
<p><b>NICE TO HAVE&apos;s:</b><br>Docker containers and EKS knowledge<br>Reporting tools &#x2013; Tableau or Power BI<br>Candidate that has worked in a complex financial environment</p>
<p><b>Description</b><br>If you have a passion for working with data using multiple emerging technologies on the cloud, this might be the right opportunity for you! The Cloud Data Engineer will be working as part of a core team building solutions for the data analytics platform for Client&#x2019;s Asset Management Division. This will involve designing and developing solutions for a variety of data lake needs using Snowflake as the data store for structured/semi-structured data and AWS s3 for unstructured data.<br>We are seeking a highly motivated expert data engineer with a strong agile mentality, who&#x2019;s looking for new challenges, we have an exciting opportunity for you to join our fast paced and highly collaborative group. This role will be involved in the full end-to-end process through planning, design, development, quality, and implementation of solutions.<br>This position can be in Merrimack, Boston, Raleigh, and Westlake.</p>
<p><b>The Expertise and Skills You Bring</b><br>Expertise in SQL, identifying patterns and trends in data, recommend and define data requirements, mastery in implementing data quality checks to ensure accuracy and completeness.<br>You enjoy learning new technologies, data analysis, identifying data patterns and trends and you can independently resolve technical challenges.<br>Experience in processing and exposing data using AWS technologies like ec2, s3, Lambda, API Gateway, Load Balancers, Auto scaling etc.<br>Expertise in building data ingestion tools using technologies like Python to extract data from Relational Databases/Web Scraping/External API&#x2019;s<br>Experience in Snowflake or any MPP and columnar database on the Cloud.<br>Experience in CI/CD release automation and deployment (Jenkins, Concourse, CloudFormation etc.)<br>Experience and good understating of databases (Oracle, Netezza) and ETL tools<br>Experience in scheduling tools like Autosys, Control-M, Airflow etc.<br>Excellent programming skills in SQL, PL/SQL, Python, shell etc.<br>Exposure to Big Data technologies (Hadoop, Spark, Hive, presto etc.)<br>Exposure to streaming services like Kafka and containers will be an advantage.<br>Good understanding of overall AWS security services like KMS, IAM, Security groups etc.<br>At least 8 years of software development experience with at least 3 years working on Cloud/Big Data technologies.<br>BS in Computer Science or related degree, or equivalent experience<br>At Client, you can find it all here. We reward ambitious, passionate individuals with a work environment that furthers diversity, partnership and collaboration as well as encourages innovative ideas and fresh thinking. We recognize the value that employees&#x2019; individual differences can give to the forward-thinking and strong future of our company.</p>
<p><b>Education:</b> Bachelors Degree</p>
<p>Job Type: Contract</p>
<p>Experience level:</p>
<ul>
 <li>10 years</li>
</ul>
<p>Work Location: On the road</p>",,"fb7783846e97f42c",,"Contract",,"West Lake Hills, TX","Data Engineer","5 days ago","2023-10-20T11:49:30.166Z",,,,"2023-10-25T11:49:30.178Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=fb7783846e97f42c&from=jasx&tk=1hdjaluvpjc8r800&vjs=3"
"Crossover Health Management Services, Inc.","About Crossover Health
 
  Crossover Health is creating the future of health as it should be. A national, team-based medical group with a focus on wellbeing and prevention that extends beyond traditional sick care, the company delivers an entirely new model of healthcare—Primary Health—built on the foundation of trusted relationships, an interdisciplinary care team approach, and outcomes-based payment. Crossover’s Primary Health model integrates primary care, physical medicine, mental health, health coaching, care navigation and more, and delivers care in surround-sound—in-person, virtually and via asynchronous messaging. Together we are building a community of members that embraces healthcare as a proactive part of their lifestyle.
 
  Job Summary
  Crossover’s Data Engineer is responsible for managing and developing data sources for analytics at scale. This is a critical role for supporting Crossover’s growing analytics team, and serves as the connection between Crossover’s Product and Technology teams, Data Science team, and data infrastructure vendors. In this role, the successful candidate will build out new data sources within the enterprise data warehouse, guide data modeling efforts for new and existing projects, and manage data ingress and egress between Crossover teams, clients, partners, and vendors. The ideal candidate will have experience with both clinical healthcare data as well as healthcare claims data.
 
  Job Responsibilities
 
   Develop and maintain data sources within Crossover’s enterprise data warehouse (inclusive of our current vendor and/or future data infrastructure)
   Assist with recommendations for data architecture, data storage, data integration, data quality, and data models
   Contribute to design sessions based on technical requirements, and build data models to clean and transform datasets for use by Crossover’s Data Science and Analytics teams
   Assist with ETL, ELT, and reverse-ETL design and development initiatives including data analysis, source-target mapping, data profiling, change data capture, QA testing, and performance tuning to guarantee quality and repeatability of data model results
   Create and maintain data model standards, including MDM (Master Data Management) and codebase standardization
   Migrate Enterprise Workloads to Snowflake using industry standard methodologies
   Automate and deploy as well as build CI/CD pipelines to support cloud based workload
   Design, deliver cloud native, hybrid, and multi-cloud Workloads
   Invest in documentation, including all system design, architecture and ongoing changes
   Design and support production job schedules, including alerting, monitoring, break fixes, and performance tuning
   Build solutions that are automated, scalable, and sustainable while minimizing defects and technical debt
   Assist stakeholders including analytics, design, product, and executive teams with data-related technical issues
   Ability to work independently with little instruction or direct oversight
   Perform other duties as assigned
 
 
  Minimum Qualifications
 
   Bachelors in Computer Science or Data Engineering, related degree, or equivalent professional experience
   3+ years relevant work experience within a complex, dynamic environment, with preference for experience with clinical healthcare data
   3+ years architecting , implementing, and supporting data infrastructure and topologies
 
 
   Experience building and operating highly available, distributed systems of extraction, ingestion, and processing of large data sets across a variety of applications (OLTP, OLAP and DSS)
   3+ years Experience with Data warehousing, methodologies, modeling techniques, design patterns, and technologies.
   Experience with data migration tools and deploying cloudbase solutions
   Experience in writing advanced SQL (DML & DDL), including Stored Procedures, Indexes, user defined functions, windows functions, correlated subqueries and CTE's, and related data query and management technology
   Coding ability in R, Python, and Shell Scripting to build and deploy Pipelines
   Working knowledge of Git, or similar collaborative code management software
   Experience with data integration tools such as FiveTran, DBT, Informatica, Matillion, or similar ETL/ELT tools
   Experience with Snowflake’s data platform
 
 
  Preferred Qualifications
 
   Masters in Computer Science, Data Engineering, or related degree
   Healthcare data acquisition, ingestion, processing, and analytics knowledge highly preferred
   Previous experience with health informatics, taxonomies, terminologies, and code sets
   Knowledge and understanding of product features: IAAS, PAAS and SAAS solutions
   Experience with healthcare claims data, formats, and analytics
   Experience with Health Catalyst’s data and analytics platform
   Experience with Tableau Cloud administration
   Experience with Master Data Management
   Understand Cloud Ecosystem
 
  The base pay range for this position is $91,428.00 to $105,143.00 per year. Pay range may vary depending on work location, applicable knowledge, skills, and experience. This position will be eligible for an annual bonus opportunity and comprehensive benefits package that includes Medical Insurance, Dental Insurance, Vision Insurance, Short- and Long-Term Disability, Life Insurance, Paid Time Off and 401K.
 
  Crossover Health is committed to Equal Employment Opportunity regardless of race, color, national origin, gender, sexual orientation, age, religion, veteran status, disability, history of disability or perceived disability. If you need assistance or an accommodation due to a disability, you may email us at careers@crossoverhealth.com.
 
  To all recruitment agencies: We do not accept unsolicited agency resumes and are not responsible for any fees related to unsolicited resumes.
  #LI-Remote","<div>
 <p><b>About Crossover Health</b></p>
 <p></p>
 <p> Crossover Health is creating the future of health as it should be. A national, team-based medical group with a focus on wellbeing and prevention that extends beyond traditional sick care, the company delivers an entirely new model of healthcare&#x2014;Primary Health&#x2014;built on the foundation of trusted relationships, an interdisciplinary care team approach, and outcomes-based payment. Crossover&#x2019;s Primary Health model integrates primary care, physical medicine, mental health, health coaching, care navigation and more, and delivers care in surround-sound&#x2014;in-person, virtually and via asynchronous messaging. Together we are building a community of members that embraces healthcare as a proactive part of their lifestyle.</p>
 <p></p>
 <p><b> Job Summary</b></p>
 <br> Crossover&#x2019;s Data Engineer is responsible for managing and developing data sources for analytics at scale. This is a critical role for supporting Crossover&#x2019;s growing analytics team, and serves as the connection between Crossover&#x2019;s Product and Technology teams, Data Science team, and data infrastructure vendors. In this role, the successful candidate will build out new data sources within the enterprise data warehouse, guide data modeling efforts for new and existing projects, and manage data ingress and egress between Crossover teams, clients, partners, and vendors. The ideal candidate will have experience with both clinical healthcare data as well as healthcare claims data.
 <p></p>
 <p><b> Job Responsibilities</b></p>
 <ul>
  <li><p> Develop and maintain data sources within Crossover&#x2019;s enterprise data warehouse (inclusive of our current vendor and/or future data infrastructure)</p></li>
  <li><p> Assist with recommendations for data architecture, data storage, data integration, data quality, and data models</p></li>
  <li><p> Contribute to design sessions based on technical requirements, and build data models to clean and transform datasets for use by Crossover&#x2019;s Data Science and Analytics teams</p></li>
  <li><p> Assist with ETL, ELT, and reverse-ETL design and development initiatives including data analysis, source-target mapping, data profiling, change data capture, QA testing, and performance tuning to guarantee quality and repeatability of data model results</p></li>
  <li><p> Create and maintain data model standards, including MDM (Master Data Management) and codebase standardization</p></li>
  <li><p> Migrate Enterprise Workloads to Snowflake using industry standard methodologies</p></li>
  <li><p> Automate and deploy as well as build CI/CD pipelines to support cloud based workload</p></li>
  <li><p> Design, deliver cloud native, hybrid, and multi-cloud Workloads</p></li>
  <li><p> Invest in documentation, including all system design, architecture and ongoing changes</p></li>
  <li><p> Design and support production job schedules, including alerting, monitoring, break fixes, and performance tuning</p></li>
  <li><p> Build solutions that are automated, scalable, and sustainable while minimizing defects and technical debt</p></li>
  <li><p> Assist stakeholders including analytics, design, product, and executive teams with data-related technical issues</p></li>
  <li><p> Ability to work independently with little instruction or direct oversight</p></li>
  <li><p> Perform other duties as assigned</p></li>
 </ul>
 <p></p>
 <p><b> Minimum Qualifications</b></p>
 <ul>
  <li><p> Bachelors in Computer Science or Data Engineering, related degree, or equivalent professional experience</p></li>
  <li><p> 3+ years relevant work experience within a complex, dynamic environment, with preference for experience with clinical healthcare data</p></li>
  <li><p> 3+ years architecting , implementing, and supporting data infrastructure and topologies</p></li>
 </ul>
 <ul>
  <li><p> Experience building and operating highly available, distributed systems of extraction, ingestion, and processing of large data sets across a variety of applications (OLTP, OLAP and DSS)</p></li>
  <li><p> 3+ years Experience with Data warehousing, methodologies, modeling techniques, design patterns, and technologies.</p></li>
  <li><p> Experience with data migration tools and deploying cloudbase solutions</p></li>
  <li><p> Experience in writing advanced SQL (DML &amp; DDL), including Stored Procedures, Indexes, user defined functions, windows functions, correlated subqueries and CTE&apos;s, and related data query and management technology</p></li>
  <li><p> Coding ability in R, Python, and Shell Scripting to build and deploy Pipelines</p></li>
  <li><p> Working knowledge of Git, or similar collaborative code management software</p></li>
  <li><p> Experience with data integration tools such as FiveTran, DBT, Informatica, Matillion, or similar ETL/ELT tools</p></li>
  <li><p> Experience with Snowflake&#x2019;s data platform</p></li>
 </ul>
 <p></p>
 <p><b> Preferred Qualifications</b></p>
 <ul>
  <li><p> Masters in Computer Science, Data Engineering, or related degree</p></li>
  <li><p> Healthcare data acquisition, ingestion, processing, and analytics knowledge highly preferred</p></li>
  <li><p> Previous experience with health informatics, taxonomies, terminologies, and code sets</p></li>
  <li><p> Knowledge and understanding of product features: IAAS, PAAS and SAAS solutions</p></li>
  <li><p> Experience with healthcare claims data, formats, and analytics</p></li>
  <li><p> Experience with Health Catalyst&#x2019;s data and analytics platform</p></li>
  <li><p> Experience with Tableau Cloud administration</p></li>
  <li><p> Experience with Master Data Management</p></li>
  <li><p> Understand Cloud Ecosystem</p></li>
 </ul>
 <p></p> The base pay range for this position is &#x24;91,428.00 to &#x24;105,143.00 per year. Pay range may vary depending on work location, applicable knowledge, skills, and experience. This position will be eligible for an annual bonus opportunity and comprehensive benefits package that includes Medical Insurance, Dental Insurance, Vision Insurance, Short- and Long-Term Disability, Life Insurance, Paid Time Off and 401K.
 <p></p>
 <p> Crossover Health is committed to Equal Employment Opportunity regardless of race, color, national origin, gender, sexual orientation, age, religion, veteran status, disability, history of disability or perceived disability. If you need assistance or an accommodation due to a disability, you may email us at careers@crossoverhealth.com.</p>
 <p></p>
 <p><b> To all recruitment agencies</b>: We do not accept unsolicited agency resumes and are not responsible for any fees related to unsolicited resumes.</p>
 <p></p> #LI-Remote
</div>","https://www.indeed.com/rc/clk?jk=d7a6a168405310c5&atk=&xpse=SoCW67I3JzdXZAw9XR0LbzkdCdPP","d7a6a168405310c5",,"Full-time",,"101 West Avenida Vista Hermosa, San Clemente, CA 92672","Data Engineer","5 days ago","2023-10-20T11:49:29.293Z","3.4","47","$91,428 - $105,143 a year","2023-10-25T11:49:29.297Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=d7a6a168405310c5&from=jasx&tk=1hdjaluvpjc8r800&vjs=3"
"Inclusively","Inclusively is partnering with a multinational financial services company to hire a Senior Data Engineer - Hybrid.
ABOUT INCLUSIVELY:
Inclusively is a digital tech platform that connects candidates with disabilities, who may benefit from workplace accommodations, to inclusive employers. This includes all disabilities under the ADA, including mental health conditions (e.g. anxiety, depression, PTSD), chronic illnesses (e.g. diabetes, Long COVID), and neurodivergence (e.g. autism, ADHD). Applicants with one or more of these conditions are encouraged to apply; Inclusively does not require applicants to disclose their specific disability.
The Role
Cloud and Platform Engineering (CAPE) is looking for a database/automation engineer to work in the Oracle squad. The role involves general maintenance task such as rehydration, upgrades, backup, recovery, and troubleshooting. Responsibilities also involve database security, monitoring and performance tuning of the database and application queries.
The Expertise
Bachelor or Master degree in Computer Science, Information Technology or Equivalent

 8+ years of IT database experience
 Requires in depth knowledge of Oracle RAC, Oracle Performance & Tuning and SQL Tuning.
 Experience working on various Cloud Options (AWS/Azure).
 Build, deliver, configure, and maintain Oracle databases on-premises and Cloud Infrastructure in AWS and Azure using industry standard deployment tools.
 Standardize and implement Cloud user management, security, capacity, monitoring and configuring application tools
 Experience with Aurora PostgreSQL, Cosmos DB, Cockroach DB and other cloud native database skills preferred in AWS or Azure.
 Strong experience in production support and maintenance of high availability database systems with ability to manage large scale database projects.
 Experience in Golden Gate replication on Oracle Real Application Clusters (RAC) & Exadata
 Analytical skill in solving complex technology challenges including solid troubleshooting skills with advanced performance SQL tuning expertise.
 Proven expertise in Realtime Application Testing, Oracle diagnostic tools.
 Experience in configuring Monitoring tools like Data Dog, OEM and proactive detection.
 Aurora PostgreSQL, Cosmos DB, Cockroach DB and other cloud native database skills preferred in AWS RDS and Azure.

The Skills You Bring

 Your ability to learn and adapt to new technologies.
 You have experience collaborating with multiple teams and stakeholders.
 Your ability to work in fast paced, highly demanding environment.

Job Type: Full-time
Experience level:

 8 years

Schedule:

 Monday to Friday

Work Location: Remote","<p><b>Inclusively is partnering with a </b><i><b>multinational financial services company</b></i> <b>to hire a </b><i><b>Senior Data Engineer - Hybrid.</b></i></p>
<p><b>ABOUT INCLUSIVELY:</b></p>
<p>Inclusively is a digital tech platform that connects candidates with disabilities, who may benefit from workplace accommodations, to inclusive employers. This includes all disabilities under the ADA, including mental health conditions (e.g. anxiety, depression, PTSD), chronic illnesses (e.g. diabetes, Long COVID), and neurodivergence (e.g. autism, ADHD). <b>Applicants with one or more of these conditions are encouraged to apply; Inclusively does not require applicants to disclose their specific disability.</b></p>
<p><b>The Role</b></p>
<p>Cloud and Platform Engineering (CAPE) is looking for a database/automation engineer to work in the Oracle squad. The role involves general maintenance task such as rehydration, upgrades, backup, recovery, and troubleshooting. Responsibilities also involve database security, monitoring and performance tuning of the database and application queries.</p>
<p><b>The Expertise</b></p>
<p>Bachelor or Master degree in Computer Science, Information Technology or Equivalent</p>
<ul>
 <li>8+ years of IT database experience</li>
 <li>Requires in depth knowledge of Oracle RAC, Oracle Performance &amp; Tuning and SQL Tuning.</li>
 <li>Experience working on various Cloud Options (AWS/Azure).</li>
 <li>Build, deliver, configure, and maintain Oracle databases on-premises and Cloud Infrastructure in AWS and Azure using industry standard deployment tools.</li>
 <li>Standardize and implement Cloud user management, security, capacity, monitoring and configuring application tools</li>
 <li>Experience with Aurora PostgreSQL, Cosmos DB, Cockroach DB and other cloud native database skills preferred in AWS or Azure.</li>
 <li>Strong experience in production support and maintenance of high availability database systems with ability to manage large scale database projects.</li>
 <li>Experience in Golden Gate replication on Oracle Real Application Clusters (RAC) &amp; Exadata</li>
 <li>Analytical skill in solving complex technology challenges including solid troubleshooting skills with advanced performance SQL tuning expertise.</li>
 <li>Proven expertise in Realtime Application Testing, Oracle diagnostic tools.</li>
 <li>Experience in configuring Monitoring tools like Data Dog, OEM and proactive detection.</li>
 <li>Aurora PostgreSQL, Cosmos DB, Cockroach DB and other cloud native database skills preferred in AWS RDS and Azure.</li>
</ul>
<p><i>The Skills You Bring</i></p>
<ul>
 <li>Your ability to learn and adapt to new technologies.</li>
 <li>You have experience collaborating with multiple teams and stakeholders.</li>
 <li>Your ability to work in fast paced, highly demanding environment.</li>
</ul>
<p>Job Type: Full-time</p>
<p>Experience level:</p>
<ul>
 <li>8 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>Monday to Friday</li>
</ul>
<p>Work Location: Remote</p>",,"7bffd3c2d5800e2e",,"Full-time",,"Westlake, TX 76262","Senior Data Engineer - Hybrid","12 days ago","2023-10-13T11:49:34.161Z",,,,"2023-10-25T11:49:34.164Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=7bffd3c2d5800e2e&from=jasx&tk=1hdjaltphk26u800&vjs=3"
"Verizon","When you join Verizon
  Verizon is one of the world’s leading providers of technology and communications services, transforming the way we connect around the world. We’re a human network that reaches across the globe and works behind the scenes. We anticipate, lead, and believe that listening is where learning begins. In crisis and in celebration, we come together—lifting up our communities and striving to make an impact to move the world forward. If you’re fueled by purpose, and powered by persistence, explore a career with us. Here, you’ll discover the rigor it takes to make a difference and the fulfillment that comes with living the #NetworkLife.
 
  What you’ll be doing...
  We are seeking an experienced strategic technical leader with telecom and wireless expertise to join our team in a role as Distinguished Staff Engineer. You will leverage your deep hands-on expertise in cloud solution, data and ML to build solutions that support key functions like marketing, service assurance, and sales. This is a high-impact role shaping the future of our cloud data and AI platforms. The Ideal candidate is a driven technical leader who will spearhead data engineering research initiatives that will shape the future of our data infrastructure. As our Data Engineering Lead, you will explore cutting-edge technologies and techniques to build next-generation data platforms that power our products, operations, and strategic growth. Responcibilites inlcude...
 
 
   Architecting data pipelines, warehouses, and machine learning solutions to power marketing, network operations, customer insights, and other telecom use cases.
   Leading projects to productionize promising technologies like distributed computing, streaming analytics, and cloud-native data patterns in our environment
   Building, and producing petabyte-scale data pipelines, lakehouses, and machine learning infrastructure on Google Cloud Platform leveraging tools like BigQuery, Dataflow, Dataproc, and Vertex AI.
   Researching, prototype, and evaluating emerging data engineering tools, architectures, and methodologies to solve complex challenges for telco workloads
   Handling massive datasets related to call records, network traffic, customer usage, and other telecom data sources
   Building a strategy and execution plan for migration to the cloud
   Data Engineering Cloud Cost/Operation Optimization
   Analyzing cloud spending across data platforms and identify optimization opportunities
   Right-size data infrastructure resources based on usage analytics and workload profiling
   Implementing automation and policies to eliminate waste, stop unused resources, increase efficiency
   Promoting architectural and workload best practices for cost-efficient data systems.
   Creating and managing processes for cloud budgeting, allocation, monitoring, and reporting
   Negotiating contracts with cloud providers to improve spending effectiveness
   Developing cost awareness initiatives such as chargebacks, show backs, and financial training
   Driving major technical initiatives and architectural decisions for the data platform, analytics, and AI
   Continuously improve engineering best practices related to data security, testing, CI/CD, and reliability of large-scale distributed systems on the cloud.
   Sharing knowledge across the organization by publishing papers, giving tech talks, and maintaining reference architectures
   Developing and promoting standards for high-quality, well-tested, maintainable code. Foster a culture of technical excellence.
   Architecting highly automated solutions for data observability platform for validation, and data drift detection to maintain quality and reliability of company core data.
   Collaborating cross-functionally with product managers, data scientists, and business stakeholders to understand needs and deliver impactful data solutions.
   Mentoring and developing junior engineers through code reviews, architectural guidance, design frameworks, and coaching sessions.
   Acting as a trusted advisor to executives on strategic technology and data solution directions.
 
 
  Where you will be working…
  In this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager
 
  What we’re looking for...
 
  You’ll need to have:
 
   Bachelor's degree or four or more years of work experience.
   Six or more years of relevant work experience.
 
 
  Even better if you have one or more of the following:
 
   15+ years of industry experience in data engineering, analytics, and production machine learning systems.
   Deep hands-on experience with Google Cloud data services: BigQuery, Dataflow, Dataproc, PubSub, Storage, CloudFunctions, Cloud Run.
   Deep knowledge of large-scale distributed data architecture patterns and performance optimization techniques.
   Deep expertise in strategizing, planning, program management, and technical architecture for large-scale cloud data modernization.
   Deep expertise in building data pipelines, warehouses, and machine learning solutions to power marketing, network operations, customer insights, and other telecom use cases.
   Proficiency in developing complex data pipelines, ETLs, and workflows on cloud platforms optimized for high volumes of telecom data.
   Expertise in designing modern data architecture using Fivetran or Matillion, dbtlabs or dataform, BigQuery, DatabrickSQL, Snowflake
   Knowledge of tools like Composer, Spark, Kafka, BigQuery, Dataform, Azure Data Fabric, Databricks, Snowflake
   Expertise in Open source Data Technologies in processing, storage, data quality - Cloudera Hadoop, Presto HUDI, Iceberg, Delta Lake, Apache Flink, GreatExpectations, Airflow, Kubernetes, Kafka
   Proficiency using cloud platforms like GCP, AWS, or Azure.
   Proficiency with CI/CD tooling like Jenkins, Airflow, Kubernetes, and Git/GitHub to enable robust development pipelines for data and ML
   Strategic thinker able to solve complex technical challenges and mentor junior engineers working with telecom data
   Knowledge of data data observability solutions Elementary.io, MonteCarlo, HoneyComb
 
  If Verizon and this role sound like a fit for you, we encourage you to apply even if you don’t meet every “even better” qualification listed above.
 
  This role is eligible to be considered for the Department of Defense SkillBridge Program.
 
 
  
   
    
     
      
       
         Where you’ll be working
       
      
     
    
   
  
  In this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager.
 
  Scheduled Weekly Hours 40
 
  Equal Employment Opportunity
  We’re proud to be an equal opportunity employer - and celebrate our employees’ differences, including race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, and Veteran status. At Verizon, we know that diversity makes us stronger. We are committed to a collaborative, inclusive environment that encourages authenticity and fosters a sense of belonging. We strive for everyone to feel valued, connected, and empowered to reach their potential and contribute their best. Check out our diversity and inclusion page to learn more.","<div>
 <h3 class=""jobSectionHeader""><b>When you join Verizon</b></h3>
 <p> Verizon is one of the world&#x2019;s leading providers of technology and communications services, transforming the way we connect around the world. We&#x2019;re a human network that reaches across the globe and works behind the scenes. We anticipate, lead, and believe that listening is where learning begins. In crisis and in celebration, we come together&#x2014;lifting up our communities and striving to make an impact to move the world forward. If you&#x2019;re fueled by purpose, and powered by persistence, explore a career with us. Here, you&#x2019;ll discover the rigor it takes to make a difference and the fulfillment that comes with living the #NetworkLife.</p>
 <p></p>
 <h3 class=""jobSectionHeader""><b> What you&#x2019;ll be doing...</b></h3>
 <p> We are seeking an experienced strategic technical leader with telecom and wireless expertise to join our team in a role as Distinguished Staff Engineer. You will leverage your deep hands-on expertise in cloud solution, data and ML to build solutions that support key functions like marketing, service assurance, and sales. This is a high-impact role shaping the future of our cloud data and AI platforms. The Ideal candidate is a driven technical leader who will spearhead data engineering research initiatives that will shape the future of our data infrastructure. As our Data Engineering Lead, you will explore cutting-edge technologies and techniques to build next-generation data platforms that power our products, operations, and strategic growth. Responcibilites inlcude...</p>
 <p></p>
 <ul>
  <li><p> Architecting data pipelines, warehouses, and machine learning solutions to power marketing, network operations, customer insights, and other telecom use cases.</p></li>
  <li><p> Leading projects to productionize promising technologies like distributed computing, streaming analytics, and cloud-native data patterns in our environment</p></li>
  <li><p> Building, and producing petabyte-scale data pipelines, lakehouses, and machine learning infrastructure on Google Cloud Platform leveraging tools like BigQuery, Dataflow, Dataproc, and Vertex AI.</p></li>
  <li><p> Researching, prototype, and evaluating emerging data engineering tools, architectures, and methodologies to solve complex challenges for telco workloads</p></li>
  <li><p> Handling massive datasets related to call records, network traffic, customer usage, and other telecom data sources</p></li>
  <li><p> Building a strategy and execution plan for migration to the cloud</p></li>
  <li><p> Data Engineering Cloud Cost/Operation Optimization</p></li>
  <li><p> Analyzing cloud spending across data platforms and identify optimization opportunities</p></li>
  <li><p> Right-size data infrastructure resources based on usage analytics and workload profiling</p></li>
  <li><p> Implementing automation and policies to eliminate waste, stop unused resources, increase efficiency</p></li>
  <li><p> Promoting architectural and workload best practices for cost-efficient data systems.</p></li>
  <li><p> Creating and managing processes for cloud budgeting, allocation, monitoring, and reporting</p></li>
  <li><p> Negotiating contracts with cloud providers to improve spending effectiveness</p></li>
  <li><p> Developing cost awareness initiatives such as chargebacks, show backs, and financial training</p></li>
  <li><p> Driving major technical initiatives and architectural decisions for the data platform, analytics, and AI</p></li>
  <li><p> Continuously improve engineering best practices related to data security, testing, CI/CD, and reliability of large-scale distributed systems on the cloud.</p></li>
  <li><p> Sharing knowledge across the organization by publishing papers, giving tech talks, and maintaining reference architectures</p></li>
  <li><p> Developing and promoting standards for high-quality, well-tested, maintainable code. Foster a culture of technical excellence.</p></li>
  <li><p> Architecting highly automated solutions for data observability platform for validation, and data drift detection to maintain quality and reliability of company core data.</p></li>
  <li><p> Collaborating cross-functionally with product managers, data scientists, and business stakeholders to understand needs and deliver impactful data solutions.</p></li>
  <li><p> Mentoring and developing junior engineers through code reviews, architectural guidance, design frameworks, and coaching sessions.</p></li>
  <li><p> Acting as a trusted advisor to executives on strategic technology and data solution directions.</p></li>
 </ul>
 <p></p>
 <p><b> Where you will be working&#x2026;</b></p>
 <p> In this hybrid role, you&apos;ll have a defined work location that includes work from home and assigned office days set by your manager</p>
 <p></p>
 <h3 class=""jobSectionHeader""><b> What we&#x2019;re looking for...</b></h3>
 <p></p>
 <h3 class=""jobSectionHeader""><b> You&#x2019;ll need to have:</b></h3>
 <ul>
  <li><p> Bachelor&apos;s degree or four or more years of work experience.</p></li>
  <li><p> Six or more years of relevant work experience.</p></li>
 </ul>
 <p></p>
 <h3 class=""jobSectionHeader""><b> Even better if you have one or more of the following:</b></h3>
 <ul>
  <li><p> 15+ years of industry experience in data engineering, analytics, and production machine learning systems.</p></li>
  <li><p> Deep hands-on experience with Google Cloud data services: BigQuery, Dataflow, Dataproc, PubSub, Storage, CloudFunctions, Cloud Run.</p></li>
  <li><p> Deep knowledge of large-scale distributed data architecture patterns and performance optimization techniques.</p></li>
  <li><p> Deep expertise in strategizing, planning, program management, and technical architecture for large-scale cloud data modernization.</p></li>
  <li><p> Deep expertise in building data pipelines, warehouses, and machine learning solutions to power marketing, network operations, customer insights, and other telecom use cases.</p></li>
  <li><p> Proficiency in developing complex data pipelines, ETLs, and workflows on cloud platforms optimized for high volumes of telecom data.</p></li>
  <li><p> Expertise in designing modern data architecture using Fivetran or Matillion, dbtlabs or dataform, BigQuery, DatabrickSQL, Snowflake</p></li>
  <li><p> Knowledge of tools like Composer, Spark, Kafka, BigQuery, Dataform, Azure Data Fabric, Databricks, Snowflake</p></li>
  <li><p> Expertise in Open source Data Technologies in processing, storage, data quality - Cloudera Hadoop, Presto HUDI, Iceberg, Delta Lake, Apache Flink, GreatExpectations, Airflow, Kubernetes, Kafka</p></li>
  <li><p> Proficiency using cloud platforms like GCP, AWS, or Azure.</p></li>
  <li><p> Proficiency with CI/CD tooling like Jenkins, Airflow, Kubernetes, and Git/GitHub to enable robust development pipelines for data and ML</p></li>
  <li><p> Strategic thinker able to solve complex technical challenges and mentor junior engineers working with telecom data</p></li>
  <li><p> Knowledge of data data observability solutions Elementary.io, MonteCarlo, HoneyComb</p></li>
 </ul>
 <p> If Verizon and this role sound like a fit for you, we encourage you to apply even if you don&#x2019;t meet every &#x201c;even better&#x201d; qualification listed above.</p>
 <p></p>
 <p> This role is eligible to be considered for the Department of Defense SkillBridge Program.</p>
 <p></p>
 <div>
  <div>
   <div>
    <div>
     <div>
      <div>
       <div>
        <h3 class=""jobSectionHeader""><b> Where you&#x2019;ll be working</b></h3>
       </div>
      </div>
     </div>
    </div>
   </div>
  </div>
 </div> In this hybrid role, you&apos;ll have a defined work location that includes work from home and assigned office days set by your manager.
 <p></p>
 <h3 class=""jobSectionHeader""><b> Scheduled Weekly Hours</b></h3> 40
 <p></p>
 <h3 class=""jobSectionHeader""><b> Equal Employment Opportunity</b></h3>
 <p> We&#x2019;re proud to be an equal opportunity employer - and celebrate our employees&#x2019; differences, including race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, and Veteran status. At Verizon, we know that diversity makes us stronger. We are committed to a collaborative, inclusive environment that encourages authenticity and fosters a sense of belonging. We strive for everyone to feel valued, connected, and empowered to reach their potential and contribute their best. Check out our diversity and inclusion page to learn more.</p>
</div>
<p></p>","https://mycareer.verizon.com/jobs/r-1018298/distinguished-data-engineer/?source=jb-indeed&dclid=CIKVzOOQkYIDFcO0nwodJQkGfA","c4d220123b6667d7",,"Full-time",,"Irving, TX","Distinguished Data Engineer","13 days ago","2023-10-12T11:49:27.464Z","3.8","32098",,"2023-10-25T11:49:27.477Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=c4d220123b6667d7&from=jasx&tk=1hdjaltphk26u800&vjs=3"
"The Church of Jesus Christ of Latter-day Saints","As a Data Extraction/Machine Learning Engineer at FamilySearch.org, you will be at the forefront of developing and implementing advanced algorithms, models, and data-driven solutions that enhance the data extraction capabilities, accuracy, and user experience of our platform. Working in close collaboration with a team of talented engineers, data scientists, and domain experts, you will help create and refine the AI-driven features that enable millions of users worldwide to make remarkable discoveries about their ancestors and family history. 
 We are looking for a passionate and dedicated candidate who shares our vision of advancing the Lord's work. You will have the opportunity to use cutting-edge technologies, access high-performance computing resources, and collaborate with highly skilled peers. This is a rare and rewarding chance to grow your career and personal skills while making a positive impact. 
 Why Join FamilySearch.org? 
 
  Impactful Mission: Be part of a meaningful mission to help individuals discover their family history and create lasting connections. 
  Cutting-Edge Technology: Work with state-of-the-art AI and machine learning technologies to push the boundaries of genealogical research. 
  Collaborative Environment: Join a team of passionate engineers, data scientists, and domain experts who value collaboration, innovation, and knowledge sharing. 
  Continuous Growth: Engage in professional development opportunities, attend conferences, and stay at the forefront of AI advancements. 
  Work-Life Balance: FamilySearch.org values work-life balance and promotes a flexible and supportive work environment. 
 
 If you're a highly motivated and creative thinker who is excited to make a tangible impact in the world of artificial intelligence and family history, we would love to hear from you! Join us at FamilySearch.org and be part of a transformative journey that connects generations and unlocks the power of AI to bring the past to life.
 


 Responsibilities: 
 
  Design, develop and deploy algorithms to extract genealogical data from the web. 
  Design, develop, and deploy AI and machine learning models that extract genealogical data from various web sources. 
  Collaborate with cross-functional teams to understand business requirements and translate them into actionable AI solutions. 
  Build scalable and efficient data pipelines for processing and analyzing large-scale genealogical data. 
  Perform data exploration, feature engineering, and model evaluation to identify optimal solutions for complex problems. 
  Stay up-to-date with the latest advancements in AI and machine learning techniques and proactively explore their potential applications to enhance FamilySearch.org.
 
 


 Required: 
 Education: 
 
  Bachelor’s degree in Computer Science, Artificial Intelligence, Machine Learning, or a related field. 
 
 Work Experience: 
 
  8+ years of industry-recognized, progressive, and relevant professional experience 
   
    Significant experience designing, training, testing, and deploying deep learning models in Computer Vision and Natural Language Processing domains. Experience with Speech Recognition is a plus. 
   
 
 Demonstrated Skills & Abilities: 
 
  Solid understanding of deep learning concepts and practices. 
  Solid programming skills in Python on the Linux platform. 
  Solid Java programming skills. 
  Excellent communication skills including the ability to create, communicate, and direct work toward accomplishing an overall technical vision. 
  Demonstrated ability to mentor and train peers. 
  Keen interest in foreign languages, scripts, and historical documents. Beginner level of at least one foreign language desired. 
  Excellent data manipulation skills using tools on the Linux platform (grep, sed, awk, NumPy, etc.). 
  Solid command of TensorFlow and/or PyTorch deep learning frameworks. 
  Ability to think about human language structurally. 
  Familiarity with cloud compute environments such as AWS. 
  Track record of self-teaching significant new concepts. 
  Proven ability to work effectively with people of various educational levels and backgrounds. Comfortable conversing with scientists, executives, engineers, and end users alike. 
  Ability to self-direct and work independently for extended periods as required. 
 
 Preferred Qualifications: 
 
  Master’s or PhD in Computer Science or a related field desired. 
 
 #LI-KS1
 


 
  Church employees find joy and satisfaction in using their unique talents and abilities to further the Lord’s work. From the IT professional who develops an app that sends the gospel message worldwide, to the facilities manager who maintains our buildings— giving Church members places to worship, teach, learn, and receive sacred ordinances—our employees seek innovative ways to share the gospel of Jesus Christ with the world. They are literally working in His kingdom. 
 
 
  Only members of the Church who are worthy of a temple recommend qualify for employment. Apart from this, the Church is an equal opportunity employer and does not discriminate in its employment decisions on any basis that would violate U.S. or local law. 
 
 
  Qualified applicants will be considered for employment without regard to race, national origin, color, gender, pregnancy, marital status, age, disability, genetic information, veteran status, or other legally protected categories that apply to the Church. The Church will make reasonable accommodations for qualified individuals with known disabilities.","<div>
 <p>As a Data Extraction/Machine Learning Engineer at FamilySearch.org, you will be at the forefront of developing and implementing advanced algorithms, models, and data-driven solutions that enhance the data extraction capabilities, accuracy, and user experience of our platform. Working in close collaboration with a team of talented engineers, data scientists, and domain experts, you will help create and refine the AI-driven features that enable millions of users worldwide to make remarkable discoveries about their ancestors and family history. </p>
 <p>We are looking for a passionate and dedicated candidate who shares our vision of advancing the Lord&apos;s work. You will have the opportunity to use cutting-edge technologies, access high-performance computing resources, and collaborate with highly skilled peers. This is a rare and rewarding chance to grow your career and personal skills while making a positive impact. </p>
 <p>Why Join FamilySearch.org? </p>
 <ul>
  <li>Impactful Mission: Be part of a meaningful mission to help individuals discover their family history and create lasting connections. </li>
  <li>Cutting-Edge Technology: Work with state-of-the-art AI and machine learning technologies to push the boundaries of genealogical research. </li>
  <li>Collaborative Environment: Join a team of passionate engineers, data scientists, and domain experts who value collaboration, innovation, and knowledge sharing. </li>
  <li>Continuous Growth: Engage in professional development opportunities, attend conferences, and stay at the forefront of AI advancements. </li>
  <li>Work-Life Balance: FamilySearch.org values work-life balance and promotes a flexible and supportive work environment. </li>
 </ul>
 <p>If you&apos;re a highly motivated and creative thinker who is excited to make a tangible impact in the world of artificial intelligence and family history, we would love to hear from you! Join us at FamilySearch.org and be part of a transformative journey that connects generations and unlocks the power of AI to bring the past to life.</p>
</div> 
<br>
<div>
 <p>Responsibilities: </p>
 <ul>
  <li>Design, develop and deploy algorithms to extract genealogical data from the web. </li>
  <li>Design, develop, and deploy AI and machine learning models that extract genealogical data from various web sources. </li>
  <li>Collaborate with cross-functional teams to understand business requirements and translate them into actionable AI solutions. </li>
  <li>Build scalable and efficient data pipelines for processing and analyzing large-scale genealogical data. </li>
  <li>Perform data exploration, feature engineering, and model evaluation to identify optimal solutions for complex problems. </li>
  <li>Stay up-to-date with the latest advancements in AI and machine learning techniques and proactively explore their potential applications to enhance FamilySearch.org.</li>
 </ul>
</div> 
<br>
<div>
 <p>Required: </p>
 <p>Education: </p>
 <ul>
  <li>Bachelor&#x2019;s degree in Computer Science, Artificial Intelligence, Machine Learning, or a related field. </li>
 </ul>
 <p>Work Experience: </p>
 <ul>
  <li>8+ years of industry-recognized, progressive, and relevant professional experience 
   <ul>
    <li>Significant experience designing, training, testing, and deploying deep learning models in Computer Vision and Natural Language Processing domains. Experience with Speech Recognition is a plus. </li>
   </ul></li>
 </ul>
 <p>Demonstrated Skills &amp; Abilities: </p>
 <ul>
  <li>Solid understanding of deep learning concepts and practices. </li>
  <li>Solid programming skills in Python on the Linux platform. </li>
  <li>Solid Java programming skills. </li>
  <li>Excellent communication skills including the ability to create, communicate, and direct work toward accomplishing an overall technical vision. </li>
  <li>Demonstrated ability to mentor and train peers. </li>
  <li>Keen interest in foreign languages, scripts, and historical documents. Beginner level of at least one foreign language desired. </li>
  <li>Excellent data manipulation skills using tools on the Linux platform (grep, sed, awk, NumPy, etc.). </li>
  <li>Solid command of TensorFlow and/or PyTorch deep learning frameworks. </li>
  <li>Ability to think about human language structurally. </li>
  <li>Familiarity with cloud compute environments such as AWS. </li>
  <li>Track record of self-teaching significant new concepts. </li>
  <li>Proven ability to work effectively with people of various educational levels and backgrounds. Comfortable conversing with scientists, executives, engineers, and end users alike. </li>
  <li>Ability to self-direct and work independently for extended periods as required. </li>
 </ul>
 <p>Preferred Qualifications: </p>
 <ul>
  <li>Master&#x2019;s or PhD in Computer Science or a related field desired. </li>
 </ul>
 <p>#LI-KS1</p>
</div> 
<br>
<div>
 <div>
  Church employees find joy and satisfaction in using their unique talents and abilities to further the Lord&#x2019;s work. From the IT professional who develops an app that sends the gospel message worldwide, to the facilities manager who maintains our buildings&#x2014; giving Church members places to worship, teach, learn, and receive sacred ordinances&#x2014;our employees seek innovative ways to share the gospel of Jesus Christ with the world. They are literally working in His kingdom. 
 </div>
 <div>
  Only members of the Church who are worthy of a temple recommend qualify for employment. Apart from this, the Church is an equal opportunity employer and does not discriminate in its employment decisions on any basis that would violate U.S. or local law. 
 </div>
 <div>
  Qualified applicants will be considered for employment without regard to race, national origin, color, gender, pregnancy, marital status, age, disability, genetic information, veteran status, or other legally protected categories that apply to the Church. The Church will make reasonable accommodations for qualified individuals with known disabilities.
 </div>
</div>","https://epej.fa.us2.oraclecloud.com/hcmUI/CandidateExperience/en/sites/CX_1001/requisitions/preview/357347","da36bc301acf1248",,"Full-time",,"3201 N Garden Dr, Lehi, UT 84043","FamilySearch Data Extraction/Machine Learning Engineer (US-based, Remote Optional)","7 days ago","2023-10-18T11:49:35.605Z","4.7","2108",,"2023-10-25T11:49:35.608Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=da36bc301acf1248&from=jasx&tk=1hdjaltphk26u800&vjs=3"
"Redcloud Consulting","RedCloud Consulting is a business and IT consulting company with local Puget Sound Enterprise and Mid-sized clients. RedCloud seeks a 
 Senior Engineer – Data Center Infrastructure to support immediate client operations. Seattle Business Magazine has recognized us, ranked #1 on their ""Best Companies to Work for in Washington"" for Mid-Sized Businesses list, awarded #1 Fastest Growing Company in Washington by Puget Sound Business Journal, and named on the Inc. 500/5000 list.
  
  
 Job Description:
  We are seeking a highly skilled and experienced Senior Engineer specializing in Data Center Infrastructure to join our dynamic team. The ideal candidate will possess a strong background in Data Center Infrastructure Management (DCIM) with a focus on Sunbird DC Track software. This individual will play a crucial role in managing, maintaining, and optimizing our data center operations.
  
  
 Responsibilities include but are not limited to: 
 
  Serve as the primary owner and administrator of the DCIM platform, responsible for its day-to-day operations, maintenance, and periodic audits. 
  Collaborate with cross-functional teams to ensure seamless integration and operation of data center infrastructure. 
  Stay updated with industry best practices and emerging technologies to recommend and implement improvements in data center operations. 
  
  
 Required Knowledge, Skills, and Abilities: 
 
  Demonstrated expertise in Data Center Infrastructure Management (DCIM) with a deep understanding of data center operations, including power and cooling systems, rack and power configuration, and device management. 
  Proficient in using DCIM software, with a specific emphasis on Sunbird DC Track, to monitor and manage data center resources effectively. 
  Experience with ServiceNow is highly desirable, as it will be advantageous in streamlining operational processes. 
  Expertise in Subird DC Track DCIM Software is a critical factor for this role. The ability to leverage this specialization will be a substantial advantage in the hiring process. 
  Familiarity with other DCIM software solutions, such as PowerIQ and TigerEyes, is considered a plus. 
  
  
 Qualifications: 
 
  Bachelor's degree in data science, IT infrastructure, computer science, or other related field. 
  8+ years of proven experience in Data Center Infrastructure Management. 
  Extensive experience with Sunbird DC Track software is essential. 
  Strong proficiency in DCIM software applications. 
  Knowledge of physical hardware components and their integration within a data center environment. 
  Excellent problem-solving skills and attention to detail. 
  Effective communication and collaboration skills.
 
  
  
  Compensation range for position is $135,000 – 170,500 DOE.
  Benefits and bonus information can be found at 
 https://www.redcloudconsulting.com/careers.html
  
  RedCloud requires employees maintain permanent residency within the United States during their employment period. During onboarding, proof of eligibility to work in the United States will be requested. RedCloud does not provide visa sponsorship.
  
  
 About Us:
  RedCloud is a boutique, business and technology consulting firm providing local companies with expert-level support for over two decades. Whether it’s to solve a specific business challenge or to provide additional support for an ambitious project, we can help bring even the most visionary endeavors to fruition.
  
  Anchored by a foundation of ""integrity-based consulting"", the RedCloud team of subject matter experts collaborate closely with clients to develop and implement high-level solutions, bringing stability, growth, and innovation together for long-term success. We provide a broad array of business and technology consulting services through RedCloud’s core services: Empower Operations, Empower Sales and Marketing, Empower Customers, Empower Security and Privacy. 
  
  Visit 
 http://www.redcloudconsulting.com/ for more info. 
  #LI-Remote","<div>
 RedCloud Consulting is a business and IT consulting company with local Puget Sound Enterprise and Mid-sized clients. RedCloud seeks a 
 <b>Senior Engineer &#x2013; Data Center Infrastructure</b> to support immediate client operations. Seattle Business Magazine has recognized us, ranked #1 on their &quot;Best Companies to Work for in Washington&quot; for Mid-Sized Businesses list, awarded #1 Fastest Growing Company in Washington by Puget Sound Business Journal, and named on the Inc. 500/5000 list.
 <br> 
 <br> 
 <b>Job Description:</b>
 <br> We are seeking a highly skilled and experienced Senior Engineer specializing in Data Center Infrastructure to join our dynamic team. The ideal candidate will possess a strong background in Data Center Infrastructure Management (DCIM) with a focus on Sunbird DC Track software. This individual will play a crucial role in managing, maintaining, and optimizing our data center operations.
 <br> 
 <br> 
 <b>Responsibilities include but are not limited to:</b> 
 <ul>
  <li>Serve as the primary owner and administrator of the DCIM platform, responsible for its day-to-day operations, maintenance, and periodic audits. </li>
  <li>Collaborate with cross-functional teams to ensure seamless integration and operation of data center infrastructure. </li>
  <li>Stay updated with industry best practices and emerging technologies to recommend and implement improvements in data center operations. </li>
 </ul> 
 <br> 
 <b>Required Knowledge, Skills, and Abilities:</b> 
 <ul>
  <li>Demonstrated expertise in Data Center Infrastructure Management (DCIM) with a deep understanding of data center operations, including power and cooling systems, rack and power configuration, and device management. </li>
  <li>Proficient in using DCIM software, with a specific emphasis on Sunbird DC Track, to monitor and manage data center resources effectively. </li>
  <li>Experience with ServiceNow is highly desirable, as it will be advantageous in streamlining operational processes. </li>
  <li>Expertise in Subird DC Track DCIM Software is a critical factor for this role. The ability to leverage this specialization will be a substantial advantage in the hiring process. </li>
  <li>Familiarity with other DCIM software solutions, such as PowerIQ and TigerEyes, is considered a plus. </li>
 </ul> 
 <br> 
 <b>Qualifications:</b> 
 <ul>
  <li>Bachelor&apos;s degree in data science, IT infrastructure, computer science, or other related field. </li>
  <li>8+ years of proven experience in Data Center Infrastructure Management. </li>
  <li>Extensive experience with Sunbird DC Track software is essential. </li>
  <li>Strong proficiency in DCIM software applications. </li>
  <li>Knowledge of physical hardware components and their integration within a data center environment. </li>
  <li>Excellent problem-solving skills and attention to detail. </li>
  <li>Effective communication and collaboration skills.</li>
 </ul>
 <br> 
 <br> 
 <br> Compensation range for position is &#x24;135,000 &#x2013; 170,500 DOE.
 <br> Benefits and bonus information can be found at 
 <b>https://www.redcloudconsulting.com/careers.html</b>
 <br> 
 <br> RedCloud requires employees maintain permanent residency within the United States during their employment period. During onboarding, proof of eligibility to work in the United States will be requested. RedCloud does not provide visa sponsorship.
 <br> 
 <br> 
 <b>About Us:</b>
 <br> RedCloud is a boutique, business and technology consulting firm providing local companies with expert-level support for over two decades. Whether it&#x2019;s to solve a specific business challenge or to provide additional support for an ambitious project, we can help bring even the most visionary endeavors to fruition.
 <br> 
 <br> Anchored by a foundation of &quot;integrity-based consulting&quot;, the RedCloud team of subject matter experts collaborate closely with clients to develop and implement high-level solutions, bringing stability, growth, and innovation together for long-term success. We provide a broad array of business and technology consulting services through RedCloud&#x2019;s core services: Empower Operations, Empower Sales and Marketing, Empower Customers, Empower Security and Privacy. 
 <br> 
 <br> Visit 
 <b>http://www.redcloudconsulting.com/</b> for more info. 
 <br> #LI-Remote
</div>","https://www.indeed.com/rc/clk?jk=15a24cf7d36918c8&atk=&xpse=SoC867I3JzdWeJzAMp0LbzkdCdPP","15a24cf7d36918c8",,,,"Seattle, WA","Senior Engineer, Data Center Infrastructure","10 days ago","2023-10-15T11:49:36.202Z","4.7","6","$135,000 - $170,500 a year","2023-10-25T11:49:36.203Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=15a24cf7d36918c8&from=jasx&tk=1hdjaltphk26u800&vjs=3"
"Medidata Solutions","Requisition ID 
   533295
  
 
 
  
    
  
 
 
  Medidata: Powering Smarter Treatments and Healthier People 
   Medidata, a Dassault Systèmes company, is leading the digital transformation of life sciences, creating hope for millions of people. Medidata helps generate the evidence and insights to help pharmaceutical, biotech, medical device and diagnostics companies, and academic researchers accelerate value, minimize risk, and optimize outcomes. More than one million registered users across 2,000+ customers and partners access the world's most trusted platform for clinical development, commercial, and real-world data. Known for its ground-breaking technological innovations, Medidata has supported more than 30,000 clinical trials and 9 million study participants. And Medidata’s ongoing commitment to infusing the patient voice into trial designs and solutions is helping to create a better and more inclusive experience for all participants in clinical studies. Medidata is involved in nearly 40% of company-initiated trial starts globally, with studies conducted in more than 140 countries. More than 70% of novel drugs approved by the Food and Drug Administration (FDA) in 2022 were developed with Medidata software. Medidata is headquartered in New York City and has offices around the world to meet the needs of its customers. Discover more at www.medidata.com and follow us @medidata. 
   Our Team: 
   Medidata is looking for individuals who will help us tackle some of the most complex questions facing the industry today using our proprietary platform and advanced analytics. At Medidata, we never work alone. This role will partner heavily with all of the key stakeholder functions including product, delivery, data science, engineering, partnerships, and biostatistics. Successful Medidata AI candidates will be skilled in analytical/quantitative thinking, structured communication, and excited about building the next horizon of Medidata’s mission to power smarter treatments and healthier people. 
   Who We're Looking For: 
   
   Advanced skills in modern data architecture, data science engineering, data modeling and data quality using state-of-art cloud computing technologies (AWS). 
   Hands-on experience in the latest breed of data ETL, automation and 
   CICD technologies including Python, SQL and Git in a cloud setting. 
   2+ years of experience with cloud-native data warehouse technologies like Snowflake. 
   Skills in data analysis, insight generation and manipulation of structured and unstructured data sources. Experience with automated data quality frameworks. 
   Collaborate with all levels of data science engineering technology personnel and senior leadership. 
   Document and present work to all levels of technical and non-technical audiences. 
   Commitment to creating rigorous, high-quality insights from data, at scale. 
   You should be flexible / willing to work across matrixed delivery landscape which includes and not limited to Agile Applications 
   Development, Support and Deployment. 
   Design and implement secure data pipelines into a Snowflake data warehouse from on premise and cloud data sources. 
   Guide and review off-shore development team work providing coaching and coding feedback aligning to best practices set by the Data Science Engineering team. 
   
  Requirements (Education & Experience): 
   
   Undergraduate degree in a technical or scientific field, such as Statistics, Data Science, Computer Science, or similar  
   8+ years professional experience as a data scientist, data engineer, data analyst, or related role  
   Experience with clinical trial data is not required, but interest to learn and understand how these data improve medical research is paramount  
  
  Medidata is making a real difference in the lives of patients everywhere by accelerating critical drug and medical device development, enabling life-saving drugs and medical devices to get to market faster. Our products sit at the convergence of the Technology and Life Sciences industries, one of most exciting areas for global innovation. Nine of the top 10 best-selling drugs in 2017 were developed on the Medidata platform. 
   Medidata Solutions have powered close to 30,000 clinical trials giving us the largest collection of clinical trial data in the world. With this asset, we pioneer innovative, advanced applications and intelligent data analytics, bringing an unmatched level of quality and efficiency to clinical trials enabling treatments to reach waiting patients sooner. 
   As with all roles, Medidata sets ranges based on a number of factors including function, level, candidate expertise and experience, and geographic location. 
   The salary range for positions that will be physically based in the NYC Metro Area is $135,000 - $180,000 
   Base pay is one part of the Total Rewards that Medidata provides to compensate and recognize employees for their work. Most sales positions are eligible for a commission on the terms of applicable plan documents, and many of Medidata’s non-sales positions are eligible for annual bonuses. Medidata believes that benefits should connect you to the support you need when it matters most and provides best-in-class benefits, including medical, dental, life and disability insurance; 401(k) matching; unlimited paid time off; and 10 paid holidays per year. 
   #LI-ME1 #LI-Remote 
  
  Equal Employment Opportunity In order to provide equal employment and advancement opportunities to all individuals, employment decisions at Medidata are based on merit, qualifications and abilities. Medidata is committed to a policy of non-discrimination and equal opportunity for all employees and qualified applicants without regard to race, color, religion, gender, sex (including pregnancy, childbirth or medical or common conditions related to pregnancy or childbirth), sexual orientation, gender identity, gender expression, marital status, familial status, national origin, ancestry, age, disability, veteran status, military service, application for military service, genetic information, receipt of free medical care, or any other characteristic protected under applicable law. Medidata will make reasonable accommodations for qualified individuals with known disabilities, in accordance with applicable law.","<p></p>
<div>
 <div>
  <div>
   <h3 class=""jobSectionHeader""><b>Requisition ID </b></h3>
   <p>533295</p>
  </div>
 </div>
 <div>
  <div>
   <br> 
  </div>
 </div>
 <div>
  <p>Medidata: Powering Smarter Treatments and Healthier People</p> 
  <p> Medidata, a Dassault Syst&#xe8;mes company, is leading the digital transformation of life sciences, creating hope for millions of people. Medidata helps generate the evidence and insights to help pharmaceutical, biotech, medical device and diagnostics companies, and academic researchers accelerate value, minimize risk, and optimize outcomes. More than one million registered users across 2,000+ customers and partners access the world&apos;s most trusted platform for clinical development, commercial, and real-world data. Known for its ground-breaking technological innovations, Medidata has supported more than 30,000 clinical trials and 9 million study participants. And Medidata&#x2019;s ongoing commitment to infusing the patient voice into trial designs and solutions is helping to create a better and more inclusive experience for all participants in clinical studies. Medidata is involved in nearly 40% of company-initiated trial starts globally, with studies conducted in more than 140 countries. More than 70% of novel drugs approved by the Food and Drug Administration (FDA) in 2022 were developed with Medidata software. Medidata is headquartered in New York City and has offices around the world to meet the needs of its customers. Discover more at www.medidata.com and follow us @medidata.</p> 
  <p><b> Our Team:</b></p> 
  <p> Medidata is looking for individuals who will help us tackle some of the most complex questions facing the industry today using our proprietary platform and advanced analytics. At Medidata, we never work alone. This role will partner heavily with all of the key stakeholder functions including product, delivery, data science, engineering, partnerships, and biostatistics. Successful Medidata AI candidates will be skilled in analytical/quantitative thinking, structured communication, and excited about building the next horizon of Medidata&#x2019;s mission to power smarter treatments and healthier people.</p> 
  <p><b> Who We&apos;re Looking For:</b></p> 
  <ul> 
   <li>Advanced skills in modern data architecture, data science engineering, data modeling and data quality using state-of-art cloud computing technologies (AWS).</li> 
   <li>Hands-on experience in the latest breed of data ETL, automation and</li> 
   <li>CICD technologies including Python, SQL and Git in a cloud setting.</li> 
   <li>2+ years of experience with cloud-native data warehouse technologies like Snowflake.</li> 
   <li>Skills in data analysis, insight generation and manipulation of structured and unstructured data sources. Experience with automated data quality frameworks.</li> 
   <li>Collaborate with all levels of data science engineering technology personnel and senior leadership.</li> 
   <li>Document and present work to all levels of technical and non-technical audiences.</li> 
   <li>Commitment to creating rigorous, high-quality insights from data, at scale.</li> 
   <li>You should be flexible / willing to work across matrixed delivery landscape which includes and not limited to Agile Applications</li> 
   <li>Development, Support and Deployment.</li> 
   <li>Design and implement secure data pipelines into a Snowflake data warehouse from on premise and cloud data sources.</li> 
   <li>Guide and review off-shore development team work providing coaching and coding feedback aligning to best practices set by the Data Science Engineering team.</li> 
  </ul> 
  <p><b>Requirements (Education &amp; Experience):</b></p> 
  <ul> 
   <li><p>Undergraduate degree in a technical or scientific field, such as Statistics, Data Science, Computer Science, or similar</p> </li> 
   <li><p>8+ years professional experience as a data scientist, data engineer, data analyst, or related role</p> </li> 
   <li><p>Experience with clinical trial data is not required, but interest to learn and understand how these data improve medical research is paramount</p> </li> 
  </ul>
  <p>Medidata is making a real difference in the lives of patients everywhere by accelerating critical drug and medical device development, enabling life-saving drugs and medical devices to get to market faster. Our products sit at the convergence of the Technology and Life Sciences industries, one of most exciting areas for global innovation. Nine of the top 10 best-selling drugs in 2017 were developed on the Medidata platform.</p> 
  <p> Medidata Solutions have powered close to 30,000 clinical trials giving us the largest collection of clinical trial data in the world. With this asset, we pioneer innovative, advanced applications and intelligent data analytics, bringing an unmatched level of quality and efficiency to clinical trials enabling treatments to reach waiting patients sooner.</p> 
  <p> As with all roles, Medidata sets ranges based on a number of factors including function, level, candidate expertise and experience, and geographic location.</p> 
  <p> The salary range for positions that will be physically based in the NYC Metro Area is &#x24;135,000 - &#x24;180,000</p> 
  <p> Base pay is one part of the Total Rewards that Medidata provides to compensate and recognize employees for their work. Most sales positions are eligible for a commission on the terms of applicable plan documents, and many of Medidata&#x2019;s non-sales positions are eligible for annual bonuses. Medidata believes that benefits should connect you to the support you need when it matters most and provides best-in-class benefits, including medical, dental, life and disability insurance; 401(k) matching; unlimited paid time off; and 10 paid holidays per year.</p> 
  <p> #LI-ME1<br> #LI-Remote</p> 
  <p></p>
  <h3 class=""jobSectionHeader""><b>Equal Employment Opportunity</b></h3> In order to provide equal employment and advancement opportunities to all individuals, employment decisions at Medidata are based on merit, qualifications and abilities. Medidata is committed to a policy of non-discrimination and equal opportunity for all employees and qualified applicants without regard to race, color, religion, gender, sex (including pregnancy, childbirth or medical or common conditions related to pregnancy or childbirth), sexual orientation, gender identity, gender expression, marital status, familial status, national origin, ancestry, age, disability, veteran status, military service, application for military service, genetic information, receipt of free medical care, or any other characteristic protected under applicable law. Medidata will make reasonable accommodations for qualified individuals with known disabilities, in accordance with applicable law.
 </div>
</div>
<p></p>","https://www.medidata.com/en/careers/lead-data-science-engineer-533295/job/","dbf9b007f625f71b",,,,"New York, NY","Lead Data Science Engineer","10 days ago","2023-10-15T11:49:38.966Z","3.6","94","$135,000 - $180,000 a year","2023-10-25T11:49:38.974Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=dbf9b007f625f71b&from=jasx&tk=1hdjaltphk26u800&vjs=3"
"Gap Inc.","About Gap Inc. 
 Our brands bridge the gaps we see in the world. Old Navy democratizes style to ensure everyone has access to quality fashion at every price point. Athleta unleashes the potential of every woman, regardless of body size, age or ethnicity. Banana Republic believes in sustainable luxury for all. And Gap inspires the world to bring individuality to modern, responsibly made essentials. 
 This simple idea—that we all deserve to belong, and on our own terms—is core to who we are as a company and how we make decisions. Our team is made up of thousands of people across the globe who take risks, think big, and do good for our customers, communities, and the planet. Ready to learn fast, create with audacity and lead boldly? Join our team.
  About the Role
  In this role, you will design highly scalable and high performing technology solutions in an Agile work environment and produce and deliver code and/or test cases using your knowledge of software development and Agile practice. You will collaborate closely with business support teams, product managers, security and architecture to assist in resolving critical production issues to help simplify and improve business processes through the latest in technology and automation. You are a technical expert that will lead through the requirements gathering, design, development, deployment, and support phases of a product. You are proficient in at least one core programming languages or packages.  As a Senior Engineer - ML/Data you will be contributing to the build of applications and services off of our Gap Data Platform. The tools will be KPI based, focusing on trend analysis of the different brands we are servicing: GAP, Banana Republic, Old Navy and Athleta. Strong understanding of CICD pipelines, data operations and Dev Ops are beneficial to this role.   This is a remote role is based out Dallas, TX. However, the Company may require you in the future to work in an on-site location designated by Gap Inc. on a full-time or part-time basis.
  What You'll Do
 
   Define technical specifications and development requirements that result in high performing technologies that are also domain specific.
   Develop and enhance product and/or applications with limited direction to solve business problems of medium complexity by keeping customer experience at the forefront.
   Adopt and model a DevOps mindset by applying automation, continuous integration and continuous delivery in everything we do.
   Foster innovation by applying best practices and learning from emerging technologies and through collaboration with cross functional stakeholders.
   Serve as application expert in support of domain areas.
   Communicate difficult concepts, providing technical and professional interpretations and recommendations.
   Advise and mentor junior team members and enable collaboration to help teams achieve their best.
 
  Who You Are
 
   Strong working experience utilizing Python and Databricks
   Software Development experience and understanding of security, secure coding/testing and data structures and aware of industry and competitor practices.
   Comprehensive knowledge of software development, practice, concepts and technology.
   Proficiency with various software languages and platforms such as Java, Oracle, Azure etc.
   Experience with related technology stack and platforms.
   Experience with building and sustaining effective relationships with immediate team and stakeholders.
 
  Benefits at Gap Inc. 
 
  Merchandise discount for our brands: 50% off regular-priced merchandise at Old Navy, Gap, Banana Republic and Athleta, and 30% off at Outlet for all employees. 
  One of the most competitive Paid Time Off plans in the industry.* 
  Employees can take up to five “on the clock” hours each month to volunteer at a charity of their choice.* 
  Extensive 401(k) plan with company matching for contributions up to four percent of an employee’s base pay.* 
  Employee stock purchase plan.* 
  Medical, dental, vision and life insurance.* 
  See more of the benefits we offer. 
 
 
  For eligible employees
 
  Gap Inc. is an equal-opportunity employer and is committed to providing a workplace free from harassment and discrimination. We are committed to recruiting, hiring, training and promoting qualified people of all backgrounds, and make all employment decisions without regard to any protected status. We have received numerous awards for our long-held commitment to equality and will continue to foster a diverse and inclusive environment of belonging. In 2022, we were recognized by Forbes as one of the World's Best Employers and one of the Best Employers for Diversity.  Salary Range: $95,300 - $138,200 USD Employee pay will vary based on factors such as qualifications, experience, skill level, competencies and work location. We will meet minimum wage or minimum of the pay range (whichever is higher) based on city, county and state requirements.  US Candidates Please note that effective, June 30, 2022, Gap Inc. will no longer require any of its employees to wear face masks or require proof of COVID vaccination, unless required by local or state/provincial mandates or as part of Gap Inc’s quarantine guidelines after being exposed to or testing positive for COVID. Therefore, please disregard any language in any job posting that refers to Gap Inc.’s face mask and proof of vaccination policy as said policy is no longer effective.","<div>
 <h2 class=""jobSectionHeader""><b>About Gap Inc.</b></h2> 
 <p>Our brands bridge the gaps we see in the world. Old Navy democratizes style to ensure everyone has access to quality fashion at every price point. Athleta unleashes the potential of every woman, regardless of body size, age or ethnicity. Banana Republic believes in sustainable luxury for all. And Gap inspires the world to bring individuality to modern, responsibly made essentials. </p>
 <p>This simple idea&#x2014;that we all deserve to belong, and on our own terms&#x2014;is core to who we are as a company and how we make decisions. Our team<b> </b>is made up of thousands of people across the globe who take risks, think big, and do good for our customers, communities, and the planet. Ready to learn fast, create with audacity and lead boldly? Join our team.</p>
 <h2 class=""jobSectionHeader""><b> About the Role</b></h2>
 <p> In this role, you will design highly scalable and high performing technology solutions in an Agile work environment and produce and deliver code and/or test cases using your knowledge of software development and Agile practice. You will collaborate closely with business support teams, product managers, security and architecture to assist in resolving critical production issues to help simplify and improve business processes through the latest in technology and automation. You are a technical expert that will lead through the requirements gathering, design, development, deployment, and support phases of a product. You are proficient in at least one core programming languages or packages.<br> <br> As a Senior Engineer - ML/Data you will be contributing to the build of applications and services off of our Gap Data Platform. The tools will be KPI based, focusing on trend analysis of the different brands we are servicing: GAP, Banana Republic, Old Navy and Athleta. Strong understanding of CICD pipelines, data operations and Dev Ops are beneficial to this role. <br> <br> This is a remote role is based out Dallas, TX. However, the Company may require you in the future to work in an on-site location designated by Gap Inc. on a full-time or part-time basis.</p>
 <h2 class=""jobSectionHeader""><b> What You&apos;ll Do</b></h2>
 <ul>
  <li> Define technical specifications and development requirements that result in high performing technologies that are also domain specific.</li>
  <li> Develop and enhance product and/or applications with limited direction to solve business problems of medium complexity by keeping customer experience at the forefront.</li>
  <li> Adopt and model a DevOps mindset by applying automation, continuous integration and continuous delivery in everything we do.</li>
  <li> Foster innovation by applying best practices and learning from emerging technologies and through collaboration with cross functional stakeholders.</li>
  <li> Serve as application expert in support of domain areas.</li>
  <li> Communicate difficult concepts, providing technical and professional interpretations and recommendations.</li>
  <li> Advise and mentor junior team members and enable collaboration to help teams achieve their best.</li>
 </ul>
 <h2 class=""jobSectionHeader""><b> Who You Are</b></h2>
 <ul>
  <li> Strong working experience utilizing Python and Databricks</li>
  <li> Software Development experience and understanding of security, secure coding/testing and data structures and aware of industry and competitor practices.</li>
  <li> Comprehensive knowledge of software development, practice, concepts and technology.</li>
  <li> Proficiency with various software languages and platforms such as Java, Oracle, Azure etc.</li>
  <li> Experience with related technology stack and platforms.</li>
  <li> Experience with building and sustaining effective relationships with immediate team and stakeholders.</li>
 </ul>
 <h2 class=""jobSectionHeader""><b> Benefits at Gap Inc.</b></h2> 
 <ul>
  <li>Merchandise discount for our brands: 50% off regular-priced merchandise at Old Navy, Gap, Banana Republic and Athleta, and 30% off at Outlet for all employees.</li> 
  <li>One of the most competitive Paid Time Off plans in the industry.*</li> 
  <li>Employees can take up to five &#x201c;on the clock&#x201d; hours each month to volunteer at a charity of their choice.*</li> 
  <li>Extensive 401(k) plan with company matching for contributions up to four percent of an employee&#x2019;s base pay.*</li> 
  <li>Employee stock purchase plan.*</li> 
  <li>Medical, dental, vision and life insurance.*</li> 
  <li>See more of the benefits we offer.</li> 
 </ul>
 <ul>
  <li><i>For eligible employees</i></li>
 </ul>
 <p> Gap Inc. is an equal-opportunity employer and is committed to providing a workplace free from harassment and discrimination. We are committed to recruiting, hiring, training and promoting qualified people of all backgrounds, and make all employment decisions without regard to any protected status. We have received numerous awards for our long-held commitment to equality and will continue to foster a diverse and inclusive environment of belonging. In 2022, we were recognized by Forbes as one of the World&apos;s Best Employers and one of the Best Employers for Diversity.<br> <br> Salary Range: &#x24;95,300 - &#x24;138,200 USD<br> Employee pay will vary based on factors such as qualifications, experience, skill level, competencies and work location. We will meet minimum wage or minimum of the pay range (whichever is higher) based on city, county and state requirements.<br> <br> <b>US Candidates</b><br> Please note that effective, June 30, 2022, Gap Inc. will no longer require any of its employees to wear face masks or require proof of COVID vaccination, unless required by local or state/provincial mandates or as part of Gap Inc&#x2019;s quarantine guidelines after being exposed to or testing positive for COVID. Therefore, please disregard any language in any job posting that refers to Gap Inc.&#x2019;s face mask and proof of vaccination policy as said policy is no longer effective.</p>
</div>","https://www.gapinc.com/en-us/jobs/w73/16/senior-engineer-ml-data-remote-dallas,-tx?rx_campaign=indeed0&rx_ch=jobp4p&rx_group=116953&rx_job=R137316&rx_r=none&rx_source=Indeed&rx_ts=20231025T080341Z&rx_vp=cpc&src=JB-12080&src=JB-10324&rx_p=RZIF7AN3GV&rx_viewer=95e370ce732c11ee8d0f37c8975f6c20cd341655bf5b4ecab28e6f7b2d8d8233","18da6255e1db9629",,"Full-time",,"2 Folsom St, San Francisco, CA 94105","Senior Engineer - ML/Data (Remote","11 days ago","2023-10-14T11:49:37.618Z","3.7","3500","$95,300 - $138,200 a year","2023-10-25T11:49:37.620Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=18da6255e1db9629&from=jasx&tk=1hdjaltphk26u800&vjs=3"
"P3 Adaptive","P3 Adaptive (https://p3adaptive.com/) has been the pioneer of data empowerment since 2010. Our mission is to re-shape the entire analytics industry taking it from the current lumbering and friction-filled incarnation and turning it into the nimble and revolutionary movement it's always supposed to have been.
We are actively hiring data engineers with an intermediate or better working knowledge of Microsoft Azure Data and Analytics Services. If you are experienced in creating solutions with Azure Data Factory, Azure Synapse, Azure Data Lake, and Azure SQL Server, you would be a great fit for this position.As a P3 Adaptive Data Engineer, you'll be intimately involved every day providing remote development for organizations in diverse industries that are working to adopt this new and improved way of thinking.Why Work for P3 Adaptive?Overall, what we do is really cool and really important. As part of the P3 Adaptive team, you will work with the most exciting platforms in Microsoft's history to solve data issues across multiple business segments and industries. You will work with some of the brightest minds and most passionate people in the industry. You will make a difference from your first day and you won't stop there. With P3 Adaptive, you will have both the resources and the support you need to grow your career. If that isn't enough, you will also have direct access to leadership, the opportunity for advancement, and access to top-tier ongoing training.Finally, we're not your typical 9 to 5. P3 Adaptive is respectful of the need for work/life balance. With flexible schedules and the permanent ability to work remotely, you can make your schedule suit your personal needs.Job Duties:

 Support the execution of Power BI projects, working alongside expert Principal Consultants and Solution Architects.
 Create Data Storage Solutions with SQL Server and Data Lakes.
 Develop ETL Pipelines with Azure Data Factory.
 Provision Azure Subscriptions and Resources.
 Develop Automation Solutions using languages such as PowerShell and Python

Qualifications:

 Exceptional communication skills
 Excellent time management skills – multitasking is critical
 Experienced in Project Management
 Intermediate or better knowledge of T-SQL for DDL and DML applications.
 Experience with Azure Active Directory Security Groups and Role-Based Access Controls
 Experience with SSIS, SSAS preferred
 Experience with PowerShell and Python preferred
 Ability to connect with our clients
 Insatiable curiosity and love of learning
 Travel Optional

The ideal candidate will have:

 Prior Professional Services or Consulting

Benefits:

 Remote
 Dental, life, medical, vision, and 401k
 ST/LT Disability Insurance
 Generous PTO policy
 Continued training to level up your Power Skills
 Supplemental benefits available:
 Accident Insurance
 Critical Illness Insurance
 Hospital Indemnity
 Health Care FSA
 Dependent Day Care FSA
 Employee Assistance Program
 Paid parental leave
 Hardware and software allowance
 Personalized incentives

Do you meet the criteria? Are you ready to take control of your career?P3 Adaptive is proud to be an equal opportunity employer committed to diversity and inclusion. We consider all applications from suitably qualified persons regardless of their race, sex, disability, religion/belief, sexual orientation, or age. All employment decisions are decided on the basis of qualifications, merit, and business need.
Job Type: Full-time
Pay: $110,000.00 - $140,000.00 per year
Schedule:

 8 hour shift
 Choose your own hours
 Monday to Friday

Work Location: Remote","<p><b>P3 Adaptive</b> (https://p3adaptive.com/) has been the pioneer of data empowerment since 2010. Our mission is to re-shape the entire analytics industry taking it from the current lumbering and friction-filled incarnation and turning it into the nimble and revolutionary movement it&apos;s always supposed to have been.</p>
<p>We are actively hiring data engineers with an intermediate or better working knowledge of Microsoft Azure Data and Analytics Services. If you are experienced in creating solutions with Azure Data Factory, Azure Synapse, Azure Data Lake, and Azure SQL Server, you would be a great fit for this position.<br>As a P3 Adaptive Data Engineer, you&apos;ll be intimately involved every day providing remote development for organizations in diverse industries that are working to adopt this new and improved way of thinking.<br><b>Why Work for P3 Adaptive?</b><br>Overall, what we do is really cool and really important. As part of the P3 Adaptive team, you will work with the most exciting platforms in Microsoft&apos;s history to solve data issues across multiple business segments and industries. You will work with some of the brightest minds and most passionate people in the industry. You will make a difference from your first day and you won&apos;t stop there. With P3 Adaptive, you will have both the resources and the support you need to grow your career. If that isn&apos;t enough, you will also have direct access to leadership, the opportunity for advancement, and access to top-tier ongoing training.<br>Finally, we&apos;re not your typical 9 to 5. P3 Adaptive is respectful of the need for work/life balance. With flexible schedules and the permanent ability to work remotely, you can make your schedule suit your personal needs.<br><b>Job Duties:</b></p>
<ul>
 <li>Support the execution of Power BI projects, working alongside expert Principal Consultants and Solution Architects.</li>
 <li>Create Data Storage Solutions with SQL Server and Data Lakes.</li>
 <li>Develop ETL Pipelines with Azure Data Factory.</li>
 <li>Provision Azure Subscriptions and Resources.</li>
 <li>Develop Automation Solutions using languages such as PowerShell and Python</li>
</ul>
<p><b>Qualifications</b>:</p>
<ul>
 <li>Exceptional communication skills</li>
 <li>Excellent time management skills &#x2013; multitasking is critical</li>
 <li>Experienced in Project Management</li>
 <li>Intermediate or better knowledge of T-SQL for DDL and DML applications.</li>
 <li>Experience with Azure Active Directory Security Groups and Role-Based Access Controls</li>
 <li>Experience with SSIS, SSAS preferred</li>
 <li>Experience with PowerShell and Python preferred</li>
 <li>Ability to connect with our clients</li>
 <li>Insatiable curiosity and love of learning</li>
 <li>Travel Optional</li>
</ul>
<p><b>The ideal candidate will have:</b></p>
<ul>
 <li>Prior Professional Services or Consulting</li>
</ul>
<p><b>Benefits</b>:</p>
<ul>
 <li>Remote</li>
 <li>Dental, life, medical, vision, and 401k</li>
 <li>ST/LT Disability Insurance</li>
 <li>Generous PTO policy</li>
 <li>Continued training to level up your Power Skills</li>
 <li>Supplemental benefits available:</li>
 <li>Accident Insurance</li>
 <li>Critical Illness Insurance</li>
 <li>Hospital Indemnity</li>
 <li>Health Care FSA</li>
 <li>Dependent Day Care FSA</li>
 <li>Employee Assistance Program</li>
 <li>Paid parental leave</li>
 <li>Hardware and software allowance</li>
 <li>Personalized incentives</li>
</ul>
<p><b>Do you meet the criteria? Are you ready to take control of your career?</b><br><i>P3 Adaptive is proud to be an equal opportunity employer committed to diversity and inclusion. We consider all applications from suitably qualified persons regardless of their race, sex, disability, religion/belief, sexual orientation, or age. All employment decisions are decided on the basis of qualifications, merit, and business need.</i></p>
<p>Job Type: Full-time</p>
<p>Pay: &#x24;110,000.00 - &#x24;140,000.00 per year</p>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
 <li>Choose your own hours</li>
 <li>Monday to Friday</li>
</ul>
<p>Work Location: Remote</p>",,"45cf7e5b6edba0bb",,"Full-time",,"Remote","Principal Azure Data Engineer","11 days ago","2023-10-14T11:49:50.744Z","5","3","$110,000 - $140,000 a year","2023-10-25T11:49:50.746Z","US","remote","data engineer","https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0CseI6QC-n6SGIjnglYBZpqyqNdlYp4GeBzkExJj_HIyo2YLNARtzb12dMIosPRFJzisvvMZvEisAOFwppdBtYC148bvBIPs1CPxfYDaQO45Ty1o65CYwldJ6mvKhEtnAlQbJjGedOOdhH6r9h0AJ04ZBwnCN2X5ViCE7OA-nV0frKNZzwULVmYUsA4NwNPWS9lasVXeXadIOny4zUUIIVeISbJsUTAQ0C1EzLb_GDxIihs-YRB6LymxHWt-CeIis_YxdUlzFHUeGQ12JrhxRGAvYRPJqDOAXd31BS9B5-1EHw5xju1xMxKCtS9KO4WW94e64ozqZ5S7aFroBSCrarvqYyHgPjB6eFeQaa7DNwLuMg9yHiZiDS276EA_dz4_auL0uyzJe6NiMjlfE6WfWBJb2zrOlwzqbWVAtgNxib--WtpNK0KI4zEc00aw4Ro0M0fh7HdDKznZ8qo-RLY80y24-SUipRh9zG8dQWW5Zy6h0yHENe0ZmbAFGkAweebIjx8ToBr_GStykLAIgWmf4rf07oy3mTDm2bhmLwr2G0cld0fGqD7ZTKlht7HvlF41HMJ9vO1eNitySy-E6N0J6lxK5mllhMctWJ-aPjB_-4M7o38XbN5wjX3SEIq62A77b2QafeQlmHL-9ZEj8enYyH1GOCTcZ2ViQeoaU2Ycce48w%3D%3D&xkcb=SoDP-_M3JzdYZ-Q28B0IbzkdCdPP&p=3&fvj=1&vjs=3&jsa=2358&tk=1hdjaltphk26u800&from=jasx&wvign=1"
"Community Reinvestment Fund, USA","Posted on October 12, 2023 
       
      
     
    
   
  
 
 
  
   
    
     
      
       
        
         
          
           About the Position 
          
         
         
          
           The Data Engineer is responsible for supporting the implementation of projects focused on collecting, aggregating, storing, reconciling, and making data accessible from disparate sources to enable analysis and decision making. The Data Engineer will play a critical role in the data supply chain by ensuring stakeholders can access and manipulate data for routine and ad hoc analysis. The Data engineer will additionally support the full lifecycle of data from sources through analytics to action. 
            The Data Engineer must be an experienced user of Power BI. The Data Engineer will develop scalable, reliable, and high-impact solutions leveraging the Azure cloud platform to create a modern enterprise data platform. The Data Engineer will work closely with business stakeholders and data analysts to understand the data needs and design, implement, and maintain Power BI reports, dashboards, and visualizations. 
          
         
        
       
      
     
     
      
       
        
         
          
           Key Responsibilities & Accountabilities 
          
         
         
          
           
            Translate business requirements to technical solutions leveraging strong business acumen 
            Analyze current business practices, processes and procedures as well as identifying future business opportunities for leveraging Microsoft Azure Data & Analytics PaaS Services 
            Support the planning and implementation of data design services, providing sizing and configuration assistance and performing needs assessments. 
            Deliver of architectures for transformations and modernizations of enterprise data solutions using Azure cloud data technologies. 
            Design and Build Azure Data Pipelines using Databricks. 
            Develop and maintain data warehouse schematics, layouts, architectures and relational/non-relational databases for data access and Advanced Analytics. 
            Expose data to end users using Power BI or any other modern visualization platform or experience 
            Implement effective metrics and monitoring processes 
           
          
         
        
       
      
     
     
      
       
        
         
          
           About You 
          
         
         
          
           
            Bachelor’s degree in Computer Science (or related field), or equivalent work experience 
            Minimum of 4 years data engineering experience 
            Demonstrated experience of turning business use cases and requirements to technical solutions 
            Experience in business processing mapping of data and analytics solutions. 
            Ability to conduct data profiling, cataloging, and mapping for technical design and construction of technical data flows. 
            Strong team collaboration and experience working with remote teams 
            The ability to apply such methods to solve business problems using one or more Azure Data and Analytics services in combination with building data pipelines, data streams, and system integration 
            Experienced in Data Transformation using ETL/ELT tools such as AWS Glue, Azure Data Factory, Talend, EAI 
            Knowledge in business intelligence tools such as Power BI, Tableau, Qlik, Cognos TM1 
            Knowledge of Azure Data Factory, Azure Data Lake, Relational Databases SQL DW, and SQL, Azure App Service is required. Azure IoT, Azure HDInsight + Spark, Azure Cosmos DB, Azure Databricks, Azure Stream Analytics is a plus 
            Experienced in Cloud Data-related tool such as Microsoft Azure, Amazon S3 
            Ability to leverage on variety of programming languages & data management/processing tools to ensure data reliability, quality & efficiency 
            Knowledge of Python is a plus 
            Designing and building Data Pipelines using streams of IOT data 
            Knowledge of Dev-Ops processes (including CI/CD) and Infrastructure as code fundamentals 
           
          
         
         
         
          
           Organizational Policies 
          
         
         
          
           Community Reinvestment Fund USA, Inc is an affirmative action equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity or expression, pregnancy, age, national origin, disability status, genetic information, protected veteran status or any other characteristic protected by law. 
          
         
         
          
           Candidates must reside in and be authorized to work in the United States without sponsorship. 
          
         
         
          
           CRF’s Theory of Change CRF’s Theory of Change is the strategic framework used to guide its work. While CRF’s mission remains constant, the Theory of Change outlines how it contributes to the changes it seeks. 
          
         
         
          
           The challenge CRF works to address The current economic system, perpetuated by institutional racism, individual biases and disparities, is unjust. It fosters the inequities that are causing widening disparities in incomes, wealth, and opportunity gaps. 
          
         
         
          
           A just economy that works for all CRF’s Theory of Change is underpinned by the conviction that small businesses are the backbone of our economy, employing nearly half of the U.S. workforce and generating two-thirds of new jobs. CRF believes that providing small businesses equitable access to capital and support services is essential to creating a just economy that works for all. 
          
         
         
          
           CRF activates its mission by:
           
             Co-creating and deploying innovative financial products and services that address the barriers and inequities small businesses operated by historically excluded people face.
             Designing and managing financial programs that attract incremental impact capital to communities with a history of underinvestment.
             Orchestrating a network of trusted small business support organizations enabled by technology.
             Growing the capacity of community development finance organizations.
             Helping small businesses navigate the complexities of the small business support ecosystem.
            
          
         
         
          
           CRF STANDS IN SOLIDARITY Community Reinvestment Fund, USA (CRF) stands in solidarity with all fighting for social justice, equity, and transformational change. They know that the social changes taking place today will yield a lasting positive impact on the lives of millions. 
          
         
         
          
           DIVERSITY, EQUITY & INCLUSION: CRF is dedicated to building and sustaining a truly diverse, equitable, and inclusive culture. These are not just words on a page – Diversity, Equity & Inclusion are top priorities for the organization, and tie deeply to each of their core values and overall vision for the future. CRF is an equal opportunity employer that evaluate applicants without regard to race, color, national origin, religion, sex, age, marital status, disability, veteran status, sexual orientation, gender identity, or other characteristics protected by law. 
          
         
         
          
           CORE VALUES Create Equitable Economic Opportunities, Lead Through Collaboration, Transform Through Innovation, Excel In All They Do, Act with Integrity. 
          
         
         
          
           How We Help Together with its partners – including community leaders, nonprofit lenders, financial institutions, foundations and more – CRF is creating new strategies and technologies that build stronger local economies, create jobs and support economic mobility. 
          
         
         
          
           CRF is headquartered in Minneapolis, Minnesota. For a more detailed description of the incredible work we do, how we do it, and who we are, please visit www.crfusa.com. 
          
         
        
       
      
     
    
   
   
    
     
      
       
        
         
          
           
            
             
              
               
                Department: 
               
              
             
             
              
               
                Data and Analytics
                
              
             
            
           
          
         
         
          
           
            
             
              
               
                Location:
                
              
             
             
              
               
                Minneapolis, MN – remote work is available
                
              
             
            
           
          
         
         
          
           
            
             
              
               
                Salary:
                
              
             
             
              
               
                $85,000 to $105,000 Annually (exempt)
                
              
             
            
           
          
         
        
       
      
     
     
     
      
       
        
         
          
           What We Offer 
          
         
         
          
           A collaborative working environment comprised of driven and highly engaged individuals committed to diversity, equity, and inclusion and the mission, vision and values of CRF. CRF is proud to extend its employees a wide array of benefits including, but not limited to: 
          
         
         
          
           
            Health and dental insurance 
            403B and Roth IRA 
            Paid Time Off (PTO) 
            Wellness Program 
            Educational Assistance 
            Long-Term and Short-Term Disability 
            Life Insurance 
            Flexible schedules and telecommuting options 
            10 Federal Holidays plus 2 Floating Holidays 
           
          
         
        
       
      
     
     
      
       
        
         
          
           How to Apply 
          
         
         
          
           To apply for this or other positions, please send your resume to: recruiting@crfusa.com 
          
         
        
       
      
     
    
   
  
 
 
  
   
    
     
      
       Community Reinvestment Fund USA, Inc is an affirmative action equal opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity or expression, pregnancy, age, national origin, disability status, genetic information, protected veteran status or any other characteristic protected by law. 
      
     
     
      
       CRF USA, Inc. requires all employees to be vaccinated for COVID-19. As a condition of hire with CRF USA, Inc. candidates must be able to show proof of vaccination for COVID-19 prior to an extension of an offer of employment. Accommodations will be considered for disabilities or sincerely held religious beliefs. 
      
     
     
      
       Candidates must reside in and be authorized to work in the United States without sponsorship.","<div></div>
<div>
 <div>
  <div>
   <div>
    <div>
     <div>
      <div>
       <ul>
        <li>Posted on October 12, 2023 </li>
       </ul>
      </div>
     </div>
    </div>
   </div>
  </div>
 </div>
 <div>
  <div>
   <div>
    <div>
     <div>
      <div>
       <div>
        <div>
         <div>
          <div>
           <h3 class=""jobSectionHeader""><b>About the Position</b></h3> 
          </div>
         </div>
         <div>
          <div>
           <p>The Data Engineer is responsible for supporting the implementation of projects focused on collecting, aggregating, storing, reconciling, and making data accessible from disparate sources to enable analysis and decision making. The Data Engineer will play a critical role in the data supply chain by ensuring stakeholders can access and manipulate data for routine and ad hoc analysis. The Data engineer will additionally support the full lifecycle of data from sources through analytics to action.</p> 
           <p> The Data Engineer must be an experienced user of Power BI. The Data Engineer will develop scalable, reliable, and high-impact solutions leveraging the Azure cloud platform to create a modern enterprise data platform. The Data Engineer will work closely with business stakeholders and data analysts to understand the data needs and design, implement, and maintain Power BI reports, dashboards, and visualizations. </p>
          </div>
         </div>
        </div>
       </div>
      </div>
     </div>
     <div>
      <div>
       <div>
        <div>
         <div>
          <div>
           <h2 class=""jobSectionHeader""><b>Key Responsibilities &amp; Accountabilities</b></h2> 
          </div>
         </div>
         <div>
          <div>
           <ul>
            <li>Translate business requirements to technical solutions leveraging strong business acumen </li>
            <li>Analyze current business practices, processes and procedures as well as identifying future business opportunities for leveraging Microsoft Azure Data &amp; Analytics PaaS Services</li> 
            <li>Support the planning and implementation of data design services, providing sizing and configuration assistance and performing needs assessments.</li> 
            <li>Deliver of architectures for transformations and modernizations of enterprise data solutions using Azure cloud data technologies.</li> 
            <li>Design and Build Azure Data Pipelines using Databricks.</li> 
            <li>Develop and maintain data warehouse schematics, layouts, architectures and relational/non-relational databases for data access and Advanced Analytics.</li> 
            <li>Expose data to end users using Power BI or any other modern visualization platform or experience</li> 
            <li>Implement effective metrics and monitoring processes</li> 
           </ul>
          </div>
         </div>
        </div>
       </div>
      </div>
     </div>
     <div>
      <div>
       <div>
        <div>
         <div>
          <div>
           <h2 class=""jobSectionHeader""><b>About You</b></h2> 
          </div>
         </div>
         <div>
          <div>
           <ul>
            <li>Bachelor&#x2019;s degree in Computer Science (or related field), or equivalent work experience</li> 
            <li>Minimum of 4 years data engineering experience </li>
            <li>Demonstrated experience of turning business use cases and requirements to technical solutions </li>
            <li>Experience in business processing mapping of data and analytics solutions. </li>
            <li>Ability to conduct data profiling, cataloging, and mapping for technical design and construction of technical data flows.</li> 
            <li>Strong team collaboration and experience working with remote teams</li> 
            <li>The ability to apply such methods to solve business problems using one or more Azure Data and Analytics services in combination with building data pipelines, data streams, and system integration</li> 
            <li>Experienced in Data Transformation using ETL/ELT tools such as AWS Glue, Azure Data Factory, Talend, EAI</li> 
            <li>Knowledge in business intelligence tools such as Power BI, Tableau, Qlik, Cognos TM1</li> 
            <li>Knowledge of Azure Data Factory, Azure Data Lake, Relational Databases SQL DW, and SQL, Azure App Service is required. Azure IoT, Azure HDInsight + Spark, Azure Cosmos DB, Azure Databricks, Azure Stream Analytics is a plus</li> 
            <li>Experienced in Cloud Data-related tool such as Microsoft Azure, Amazon S3</li> 
            <li>Ability to leverage on variety of programming languages &amp; data management/processing tools to ensure data reliability, quality &amp; efficiency</li> 
            <li>Knowledge of Python is a plus</li> 
            <li>Designing and building Data Pipelines using streams of IOT data</li> 
            <li>Knowledge of Dev-Ops processes (including CI/CD) and Infrastructure as code fundamentals</li> 
           </ul>
          </div>
         </div>
         <div></div>
         <div>
          <div>
           <h3 class=""jobSectionHeader""><b>Organizational Policies</b></h3> 
          </div>
         </div>
         <div>
          <div>
           <p>Community Reinvestment Fund USA, Inc is an affirmative action equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity or expression, pregnancy, age, national origin, disability status, genetic information, protected veteran status or any other characteristic protected by law.</p> 
          </div>
         </div>
         <div>
          <div>
           <p>Candidates must reside in and be authorized to work in the United States without sponsorship.</p> 
          </div>
         </div>
         <div>
          <div>
           <p><b>CRF&#x2019;s Theory of Change</b><br> CRF&#x2019;s Theory of Change is the strategic framework used to guide its work. While CRF&#x2019;s mission remains constant, the Theory of Change outlines how it contributes to the changes it seeks.</p> 
          </div>
         </div>
         <div>
          <div>
           <p><b>The challenge CRF works to address</b><br> The current economic system, perpetuated by institutional racism, individual biases and disparities, is unjust. It fosters the inequities that are causing widening disparities in incomes, wealth, and opportunity gaps.</p> 
          </div>
         </div>
         <div>
          <div>
           <p><b>A just economy that works for all</b><br> CRF&#x2019;s Theory of Change is underpinned by the conviction that small businesses are the backbone of our economy, employing nearly half of the U.S. workforce and generating two-thirds of new jobs. CRF believes that providing small businesses equitable access to capital and support services is essential to creating a just economy that works for all.</p> 
          </div>
         </div>
         <div>
          <div>
           <p>CRF activates its mission by:</p>
           <ul>
            <li> Co-creating and deploying innovative financial products and services that address the barriers and inequities small businesses operated by historically excluded people face.</li>
            <li> Designing and managing financial programs that attract incremental impact capital to communities with a history of underinvestment.</li>
            <li> Orchestrating a network of trusted small business support organizations enabled by technology.</li>
            <li> Growing the capacity of community development finance organizations.</li>
            <li> Helping small businesses navigate the complexities of the small business support ecosystem.</li>
           </ul> 
          </div>
         </div>
         <div>
          <div>
           <p><b>CRF STANDS IN SOLIDARITY</b><br> Community Reinvestment Fund, USA (CRF) stands in solidarity with all fighting for social justice, equity, and transformational change. They know that the social changes taking place today will yield a lasting positive impact on the lives of millions.</p> 
          </div>
         </div>
         <div>
          <div>
           <p><b>DIVERSITY, EQUITY &amp; INCLUSION:</b><br> CRF is dedicated to building and sustaining a truly diverse, equitable, and inclusive culture. These are not just words on a page &#x2013; Diversity, Equity &amp; Inclusion are top priorities for the organization, and tie deeply to each of their core values and overall vision for the future. CRF is an equal opportunity employer that evaluate applicants without regard to race, color, national origin, religion, sex, age, marital status, disability, veteran status, sexual orientation, gender identity, or other characteristics protected by law.</p> 
          </div>
         </div>
         <div>
          <div>
           <p><b>CORE VALUES</b><br> Create Equitable Economic Opportunities, Lead Through Collaboration, Transform Through Innovation, Excel In All They Do, Act with Integrity.</p> 
          </div>
         </div>
         <div>
          <div>
           <p><b>How We Help</b><br> Together with its partners &#x2013; including community leaders, nonprofit lenders, financial institutions, foundations and more &#x2013; CRF is creating new strategies and technologies that build stronger local economies, create jobs and support economic mobility.</p> 
          </div>
         </div>
         <div>
          <div>
           <p>CRF is headquartered in Minneapolis, Minnesota. For a more detailed description of the incredible work we do, how we do it, and who we are, please visit www.crfusa.com.</p> 
          </div>
         </div>
        </div>
       </div>
      </div>
     </div>
    </div>
   </div>
   <div>
    <div>
     <div>
      <div>
       <div>
        <div>
         <div>
          <div>
           <div>
            <div>
             <div>
              <div>
               <div>
                Department: 
               </div>
              </div>
             </div>
             <div>
              <div>
               <div>
                Data and Analytics
               </div> 
              </div>
             </div>
            </div>
           </div>
          </div>
         </div>
         <div>
          <div>
           <div>
            <div>
             <div>
              <div>
               <div>
                Location:
               </div> 
              </div>
             </div>
             <div>
              <div>
               <div>
                Minneapolis, MN &#x2013; remote work is available
               </div> 
              </div>
             </div>
            </div>
           </div>
          </div>
         </div>
         <div>
          <div>
           <div>
            <div>
             <div>
              <div>
               <div>
                Salary:
               </div> 
              </div>
             </div>
             <div>
              <div>
               <div>
                &#x24;85,000 to &#x24;105,000 Annually (exempt)
               </div> 
              </div>
             </div>
            </div>
           </div>
          </div>
         </div>
        </div>
       </div>
      </div>
     </div>
     <div></div>
     <div>
      <div>
       <div>
        <div>
         <div>
          <div>
           <h3 class=""jobSectionHeader""><b>What We Offer</b></h3> 
          </div>
         </div>
         <div>
          <div>
           A collaborative working environment comprised of driven and highly engaged individuals committed to diversity, equity, and inclusion and the mission, vision and values of CRF. CRF is proud to extend its employees a wide array of benefits including, but not limited to: 
          </div>
         </div>
         <div>
          <div>
           <ul>
            <li>Health and dental insurance</li> 
            <li>403B and Roth IRA</li> 
            <li>Paid Time Off (PTO)</li> 
            <li>Wellness Program</li> 
            <li>Educational Assistance</li> 
            <li>Long-Term and Short-Term Disability</li> 
            <li>Life Insurance</li> 
            <li>Flexible schedules and telecommuting options</li> 
            <li>10 Federal Holidays plus 2 Floating Holidays</li> 
           </ul>
          </div>
         </div>
        </div>
       </div>
      </div>
     </div>
     <div>
      <div>
       <div>
        <div>
         <div>
          <div>
           <h2 class=""jobSectionHeader""><b>How to Apply</b></h2> 
          </div>
         </div>
         <div>
          <div>
           To apply for this or other positions, please send your resume to: recruiting@crfusa.com 
          </div>
         </div>
        </div>
       </div>
      </div>
     </div>
    </div>
   </div>
  </div>
 </div>
 <div>
  <div>
   <div>
    <div>
     <div>
      <div>
       Community Reinvestment Fund USA, Inc is an affirmative action equal opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity or expression, pregnancy, age, national origin, disability status, genetic information, protected veteran status or any other characteristic protected by law. 
      </div>
     </div>
     <div>
      <div>
       CRF USA, Inc. requires all employees to be vaccinated for COVID-19. As a condition of hire with CRF USA, Inc. candidates must be able to show proof of vaccination for COVID-19 prior to an extension of an offer of employment. Accommodations will be considered for disabilities or sincerely held religious beliefs. 
      </div>
     </div>
     <div>
      <div>
       Candidates must reside in and be authorized to work in the United States without sponsorship.
      </div>
     </div>
    </div>
   </div>
  </div>
 </div>
</div>
<div></div>","https://crfusa.com/data-engineer/","16c48fc80404f2f3",,,,"801 Nicollet Mall Ste 1700, Minneapolis, MN 55402","Data Engineer","12 days ago","2023-10-13T11:49:44.920Z","4","6","$85,000 - $105,000 a year","2023-10-25T11:49:44.922Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=16c48fc80404f2f3&from=jasx&tk=1hdjaltphk26u800&vjs=3"
"Velocity Black","Category 
     
      Engineering
      
    
   
   
   
    
     Experience 
     
      Sr. Manager
      
    
   
   
   
    
     Primary Address 
     
      
       Richmond, Virginia
       
     
    
   
  
 
 
  Overview West Creek 8 (12080), United States of America, Richmond, Virginia
   Senior Lead Data Engineer (Remote-Eligible)
  
   Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative,inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking Data Engineers who are passionate about marrying data with emerging technologies. As a Capital One Senior Lead Data Engineer, you’ll have the opportunity to be on the forefront of driving a major transformation within Capital One.
  
  
   
     What You’ll Do:
   
  
  
    Manage and develop a Java-based pipeline and query tools depending on HIve Metastore, AWS S3, Kafka and ORC
    Develop analytics tooling to solve business problems driven by scale and international expansion
    Optimize configurations for analytics tools to support growing business and organization
  
  
  
    “Capital One is open to hiring a Remote Employee for this opportunity.”
  
  
  
   
     Basic Qualifications:
   
  
  
    Bachelor’s Degree
    At least 8 years of experience in application development (Internship experience does not apply)
    At least 2 years of experience in big data technologies
    At least 1 year experience with cloud computing (AWS, Microsoft Azure, Google Cloud)
  
  
  
   
     Preferred Qualifications:
   
  
  
    9+ years of experience in application development including Python, Javascript, or Java
    4+ years of experience with AWS
    5+ years experience with Distributed data/computing tools (Trino, Hive, Kafka or Spark)
    4+ year experience working on real-time data and streaming applications
    4+ years of experience with NoSQL implementation (Cassandra)
    4+ years of experience with UNIX/Linux including basic commands and shell scripting
    2+ years of experience with Agile engineering practices
  
  
  
    Capital One will consider sponsoring a new qualified applicant for employment authorization for this position.
  
  
   The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked.
   New York City (Hybrid On-Site): $230,100 - $262,700 for Sr. Lead Data Engineer
   San Francisco, California (Hybrid On-Site): $243,800 - $278,200 for Sr. Lead Data Engineer
   Remote (Regardless of Location): $195,000 - $222,600 for Sr. Lead Data Engineer
  
   Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate’s offer letter.
   This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.
  
  
   
    
     
       Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at theCapital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.
     
    
   
  
   No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City’s Fair Chance Act; Philadelphia’s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.
  
   If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.
  
   For technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com
  
   Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.
  
   Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).","<div></div>
<div>
 <div>
  <ul>
   <div>
    <div>
     <p><b>Category</b></p> 
     <div>
      Engineering
     </div> 
    </div>
   </div>
   <div></div>
   <div>
    <div>
     <p><b>Experience</b></p> 
     <div>
      Sr. Manager
     </div> 
    </div>
   </div>
   <div></div>
   <div>
    <div>
     <p><b>Primary Address</b></p> 
     <div>
      <div>
       Richmond, Virginia
      </div> 
     </div>
    </div>
   </div>
  </ul>
 </div>
 <div>
  <p>Overview</p> West Creek 8 (12080), United States of America, Richmond, Virginia
  <p></p> Senior Lead Data Engineer (Remote-Eligible)
  <p></p>
  <p> Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative,inclusive, and iterative delivery environment? At Capital One, you&apos;ll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking Data Engineers who are passionate about marrying data with emerging technologies. As a Capital One Senior Lead Data Engineer, you&#x2019;ll have the opportunity to be on the forefront of driving a major transformation within Capital One.</p>
  <p></p>
  <div>
   <div>
    <p> What You&#x2019;ll Do:</p>
   </div>
  </div>
  <ul>
   <li><p> Manage and develop a Java-based pipeline and query tools depending on HIve Metastore, AWS S3, Kafka and ORC</p></li>
   <li><p> Develop analytics tooling to solve business problems driven by scale and international expansion</p></li>
   <li><p> Optimize configurations for analytics tools to support growing business and organization</p></li>
  </ul>
  <p></p>
  <div>
   <p><i> &#x201c;Capital One is open to hiring a Remote Employee for this opportunity.&#x201d;</i></p>
  </div>
  <p></p>
  <div>
   <div>
    <p> Basic Qualifications:</p>
   </div>
  </div>
  <ul>
   <li><p> Bachelor&#x2019;s Degree</p></li>
   <li><p> At least 8 years of experience in application development (Internship experience does not apply)</p></li>
   <li><p> At least 2 years of experience in big data technologies</p></li>
   <li><p> At least 1 year experience with cloud computing (AWS, Microsoft Azure, Google Cloud)</p></li>
  </ul>
  <p></p>
  <div>
   <div>
    <p> Preferred Qualifications:</p>
   </div>
  </div>
  <ul>
   <li><p> 9+ years of experience in application development including Python, Javascript, or Java</p></li>
   <li><p> 4+ years of experience with AWS</p></li>
   <li><p> 5+ years experience with Distributed data/computing tools (Trino, Hive, Kafka or Spark)</p></li>
   <li><p> 4+ year experience working on real-time data and streaming applications</p></li>
   <li><p> 4+ years of experience with NoSQL implementation (Cassandra)</p></li>
   <li><p> 4+ years of experience with UNIX/Linux including basic commands and shell scripting</p></li>
   <li><p> 2+ years of experience with Agile engineering practices</p></li>
  </ul>
  <p></p>
  <div>
   <p><i> Capital One will consider sponsoring a new qualified applicant for employment authorization for this position.</i></p>
  </div>
  <p></p>
  <p> The minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked.</p>
  <p></p> New York City (Hybrid On-Site): &#x24;230,100 - &#x24;262,700 for Sr. Lead Data Engineer
  <p></p> San Francisco, California (Hybrid On-Site): &#x24;243,800 - &#x24;278,200 for Sr. Lead Data Engineer
  <p></p> Remote (Regardless of Location): &#x24;195,000 - &#x24;222,600 for Sr. Lead Data Engineer
  <p></p>
  <p> Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate&#x2019;s offer letter.</p>
  <p></p> This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.
  <p></p>
  <div>
   <div>
    <div>
     <div>
      <p> Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at theCapital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.</p>
     </div>
    </div>
   </div>
  </div>
  <p></p> No agencies please. Capital One is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City&#x2019;s Fair Chance Act; Philadelphia&#x2019;s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.
  <p></p>
  <p> If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.</p>
  <p></p>
  <p> For technical support or questions about Capital One&apos;s recruiting process, please send an email to Careers@capitalone.com</p>
  <p></p>
  <p> Capital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.</p>
  <p></p>
  <p> Capital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).</p>
 </div>
</div>","https://www.capitalonecareers.com/job/richmond/senior-lead-data-engineer-remote-eligible/1732/55676321808","7d3659f970fdfd0b",,"Part-time",,"Richmond, VA","Senior Lead Data Engineer (Remote-Eligible)","12 days ago","2023-10-13T11:49:47.981Z","3.9","10311",,"2023-10-25T11:49:47.982Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=7d3659f970fdfd0b&from=jasx&tk=1hdjaltphk26u800&vjs=3"
"Ferguson Enterprises, LLC","Job Posting: 
 Ferguson is North America’s leading value-added distributor across residential, non-residential, new construction and repair, maintenance, and improvement (RMI) end markets. Spanning 34,000 suppliers and more than one million customers, we deliver local expertise, value-added solutions, and the industry’s most extensive portfolio of products. From infrastructure, plumbing, and appliances, to HVAC, fire protection, fabrication, and more, we make our customers’ complex projects simple, successful, and sustainable.
 
  Ferguson has an exciting opportunity for a Lead Azure Data Engineer to join the Enterprise Data & Analytics team, within the Ferguson IT organization. This role will support the customer domain and come with experience in large digital and data management transformations. This is a lead role, under our IT Enterprise Data & Analytics (EDA), Data Application Technology department. The ideal candidate will have prior experience in a MDM implementation for the wholesale/distributor industry.
  **This role is approved to be Remote or Hybrid in accordance with company policy.**
 
  The Lead Azure Data Engineer is accountable for the successful delivery of Ferguson’s Master Data Management Customer technology. The engineer will carry out the activities responsible for the design and configuration of the Customer MDM in Reltio. The ideal candidate will be well versed in Cloud based technologies such as AWS, Azure Cloud and GCP with experience in Databricks and Azure Data Factory. Knowledge in data integration, data quality, project life cycle phases, and best practices is expected. Furthermore, we expect you to help other developers through code reviews, pair programming, and technical leadership. Your passion will lead to our customers success!
  Duties and Responsibilities:
 
   Work with Users, and Business Analysts to understand and translate functional requirements to technical design.
   Analysis and resolution of sophisticated data relationships and data cleansing/profiling scenarios.
   Hands-on configuration experience of hierarchies, entity types, attributes, relationships, and crosswalks in an MDM.
   Solid understanding of high-level enterprise architecture patterns for data ingesting, storing, processing, and publishing
   Experience configuring Out-of-the-box and Custom Cleanse Functions within an MDM.
   Experience configuring External matches, and their usage to profile the incoming data for new sources.
   Configuration and usage of advanced queries, Workflow management, User Management, and UI Config. Experience with LCA configuration for customizations.
   Strategic problem solver - somebody who can conceptualize and lead the engineering of an MDM project.
   Experience in designing master data (Customer Master, Product Master, etc.) hierarchies and reference data preferred.
   Expert in Cloud & Data Technology and the market trends within Azure and AWS ecosystems.
   Work with project management to develop the overall implementation solution, roadmap and plan.
   Advanced Reltio experience desired but not required in areas of RIH (Reltio Integration Hub), Reltio L1, L2, L3 configuration layers, Reltio APIs, Reltio Open Collaboration System, etc.
 
  Qualifications and Requirements:
 
   12+ years of overall IT experience
   Experience with hands-on implementation of Master Data Management (MDM) solutions using one of the major MDM platforms (Reltio Customer 360 - preferred, Informatica MDM, Stibo Systems)
   5+ years’ Experience working as either: Software Engineer/Data Engineer: query tuning, performance tuning, troubleshooting, and debugging big data solutions.
   Experience using and designing solutions on cloud infrastructure and services, such as AWS, Azure, or GCP
   Experience with Development Tools for CI/CD, Unit and Integration testing, Automation and Orchestration, REST API, BI tools, and SQL Interfaces.
   Expert level experience developing designs, data specific crosswalks and end user training documents.
   History working with Databricks, Cloud Data Platforms, BI, Data Warehousing, Data Lakes, Data Science & Predictive Analytics
   Expert in building Databricks notebooks in extracting the data from various source systems and perform data cleansing, data wrangling, data ETL processing and loading to AZURE SQL DB.
   Expert in developing JSON Scripts for deploying the Pipeline in Azure Data Factory (ADF) that process the data.
   Expert in using Databricks with Azure Data Factory (ADF) to compute large volumes of data from different sources.
   Performed ETL operations in Azure Databricks by connecting to different relational database source systems.
   Solid grasp of environment management, release management, code versioning, and deployment methodologies.
   Familiarity with platform authentication patterns (SAML, SSO, OAuth).
   Experience using a No SQL database such as Mongo or Cosmos is a plus.
   Experience using development tools like JSON, Postman, Terraform, etc. is a plus.
   Experience working in both a Waterfall and Agile environment is a plus.
   Work directly to collaborate and mentor team members in differing levels across the world.
   Be a subject matter expert and a go-to team member that makes valuable contributions daily.
   Be willing to focus and contribute to seeking information, when needed.
 
  Ferguson is dedicated to providing meaningful benefits programs and products to our associates and their families—geared toward benefits, wellness, financial protection, and retirement savings. Ferguson offers a competitive benefits package that includes medical, dental, vision, retirement savings with company match, paid leave (vacation, sick, personal, holiday, and parental), employee assistance programs, associate discounts, community involvement opportunities, and much more!
 
  #LI-REMOTE
 
  Pay Range:
 
  Actual pay rate may vary depending upon location. The estimated pay range for this position is below. The specific rate will depend on a candidate’s qualifications and prior experience.
  $8,470.59 - $14,834.37
 
  Estimated Ranges displayed are Monthly for Salaried roles OR Hourly for all other roles.
 
  This role is Bonus or Incentive Plan eligible.
 
  The Company is an equal opportunity employer as well as a government contractor that shall abide by the requirements of 41 CFR 60-300.5(a), which prohibits discrimination against qualified protected Veterans and the requirements of 41 CFR 60-741.5(A), which prohibits discrimination against qualified individuals on the basis of disability.
 
  Ferguson Enterprises, LLC. is an equal employment employer F/M/Disability/Vet/Sexual Orientation/Gender Identity.
 
  Equal Employment Opportunity and Reasonable Accommodation Information","<div>
 <p><b>Job Posting: </b></p>
 <p>Ferguson is North America&#x2019;s leading value-added distributor across residential, non-residential, new construction and repair, maintenance, and improvement (RMI) end markets. Spanning 34,000 suppliers and more than one million customers, we deliver local expertise, value-added solutions, and the industry&#x2019;s most extensive portfolio of products. From infrastructure, plumbing, and appliances, to HVAC, fire protection, fabrication, and more, we make our customers&#x2019; complex projects simple, successful, and sustainable.</p>
 <p></p>
 <p> Ferguson has an exciting opportunity for a Lead Azure Data Engineer to join the Enterprise Data &amp; Analytics team, within the Ferguson IT organization. This role will support the customer domain and come with experience in large digital and data management transformations. This is a lead role, under our IT Enterprise Data &amp; Analytics (EDA), Data Application Technology department. The ideal candidate will have prior experience in a MDM implementation for the wholesale/distributor industry.</p>
 <h1 class=""jobSectionHeader""><b> **This role is approved to be Remote or Hybrid in accordance with company policy.**</b></h1>
 <p></p>
 <p> The Lead Azure Data Engineer is accountable for the successful delivery of Ferguson&#x2019;s Master Data Management Customer technology. The engineer will carry out the activities responsible for the design and configuration of the Customer MDM in Reltio. The ideal candidate will be well versed in Cloud based technologies such as AWS, Azure Cloud and GCP with experience in Databricks and Azure Data Factory. Knowledge in data integration, data quality, project life cycle phases, and best practices is expected. Furthermore, we expect you to help other developers through code reviews, pair programming, and technical leadership. Your passion will lead to our customers success!</p>
 <h2 class=""jobSectionHeader""><b> Duties and Responsibilities:</b></h2>
 <ul>
  <li> Work with Users, and Business Analysts to understand and translate functional requirements to technical design.</li>
  <li> Analysis and resolution of sophisticated data relationships and data cleansing/profiling scenarios.</li>
  <li> Hands-on configuration experience of hierarchies, entity types, attributes, relationships, and crosswalks in an MDM.</li>
  <li> Solid understanding of high-level enterprise architecture patterns for data ingesting, storing, processing, and publishing</li>
  <li> Experience configuring Out-of-the-box and Custom Cleanse Functions within an MDM.</li>
  <li> Experience configuring External matches, and their usage to profile the incoming data for new sources.</li>
  <li> Configuration and usage of advanced queries, Workflow management, User Management, and UI Config. Experience with LCA configuration for customizations.</li>
  <li> Strategic problem solver - somebody who can conceptualize and lead the engineering of an MDM project.</li>
  <li> Experience in designing master data (Customer Master, Product Master, etc.) hierarchies and reference data preferred.</li>
  <li> Expert in Cloud &amp; Data Technology and the market trends within Azure and AWS ecosystems.</li>
  <li> Work with project management to develop the overall implementation solution, roadmap and plan.</li>
  <li> Advanced Reltio experience desired but not required in areas of RIH (Reltio Integration Hub), Reltio L1, L2, L3 configuration layers, Reltio APIs, Reltio Open Collaboration System, etc.</li>
 </ul>
 <h2 class=""jobSectionHeader""><b> Qualifications and Requirements:</b></h2>
 <ul>
  <li> 12+ years of overall IT experience</li>
  <li> Experience with hands-on implementation of Master Data Management (MDM) solutions using one of the major MDM platforms (Reltio Customer 360 - preferred, Informatica MDM, Stibo Systems)</li>
  <li> 5+ years&#x2019; Experience working as either: Software Engineer/Data Engineer: query tuning, performance tuning, troubleshooting, and debugging big data solutions.</li>
  <li> Experience using and designing solutions on <b>cloud infrastructure and services, such as AWS, Azure, or GCP</b></li>
  <li> Experience with Development Tools for CI/CD, Unit and Integration testing, Automation and Orchestration, <b>REST API, BI tools, and SQL Interfaces.</b></li>
  <li> Expert level experience developing designs, data specific crosswalks and end user training documents.</li>
  <li> History working with Databricks, Cloud Data Platforms, BI, Data Warehousing, Data Lakes, Data Science &amp; Predictive Analytics</li>
  <li> Expert in building Databricks notebooks in extracting the data from various source systems and perform data cleansing, data wrangling, data ETL processing and loading to AZURE SQL DB.</li>
  <li> Expert in developing JSON Scripts for deploying the Pipeline in Azure Data Factory (ADF) that process the data.</li>
  <li> Expert in using Databricks with Azure Data Factory (ADF) to compute large volumes of data from different sources.</li>
  <li> Performed ETL operations in Azure Databricks by connecting to different relational database source systems.</li>
  <li> Solid grasp of environment management, release management, code versioning, and deployment methodologies.</li>
  <li> Familiarity with platform authentication patterns (SAML, SSO, OAuth).</li>
  <li> Experience using a No SQL database such as Mongo or Cosmos is a plus.</li>
  <li> Experience using development tools like JSON, Postman, Terraform, etc. is a plus.</li>
  <li> Experience working in both a Waterfall and Agile environment is a plus.</li>
  <li> Work directly to collaborate and mentor team members in differing levels across the world.</li>
  <li> Be a subject matter expert and a go-to team member that makes valuable contributions daily.</li>
  <li> Be willing to focus and contribute to seeking information, when needed.</li>
 </ul>
 <p> Ferguson is dedicated to providing meaningful benefits programs and products to our associates and their families&#x2014;geared toward benefits, wellness, financial protection, and retirement savings. Ferguson offers a competitive benefits package that includes medical, dental, vision, retirement savings with company match, paid leave (vacation, sick, personal, holiday, and parental), employee assistance programs, associate discounts, community involvement opportunities, and much more!</p>
 <p></p>
 <p> #LI-REMOTE</p>
 <ul></ul>
 <p><b> Pay Range:</b></p>
 <ul></ul>
 <p><i> Actual pay rate may vary depending upon location. The estimated pay range for this position is below. The specific rate will depend on a candidate&#x2019;s qualifications and prior experience.</i></p>
 <ul></ul> &#x24;8,470.59 - &#x24;14,834.37
 <ul></ul>
 <p><b><i> Estimated Ranges displayed are Monthly for Salaried roles </i></b><b>OR</b><b><i> Hourly for all other roles.</i></b></p>
 <ul></ul>
 <p> This role is Bonus or Incentive Plan eligible.</p>
 <ul></ul>
 <p><i> The Company is an equal opportunity employer as well as a government contractor that shall abide by the requirements of 41 CFR 60-300.5(a), which prohibits discrimination against qualified protected Veterans and the requirements of 41 CFR 60-741.5(A), which prohibits discrimination against qualified individuals on the basis of disability.</i></p>
 <p></p>
 <p><i> Ferguson Enterprises, LLC. is an equal employment employer </i><i>F/M/Disability/Vet/Sexual</i><i> Orientation/Gender</i><i> Identity.</i></p>
 <p></p>
 <p> Equal Employment Opportunity and Reasonable Accommodation Information</p>
</div>
<p></p>","https://ferguson.wd1.myworkdayjobs.com/Ferguson_Experienced/job/Remote/Lead-MDM-Software-Engineer---Reltio_R-106339","c80d7194fccc1e9d",,"Full-time",,"Richmond, VA 23219","Lead Azure Data Engineer - Remote","12 days ago","2023-10-13T11:49:33.023Z","3.3","1984",,"2023-10-25T11:49:33.025Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=c80d7194fccc1e9d&from=jasx&tk=1hdjaltphk26u800&vjs=3"
"Analytica","ANALYTICA is seeking a Senior Data Engineer to support a federal government client in the DC metro area (Note - your work location is REMOTE). In this assignment, you will be a team member serving the client in advancing the customers use data, metadata, as well as explore new technologies to better meet those needs.  This is a mission that takes some serious smarts, intense curiosity and a background in developing data solutions across the data lifecycle.  Analytica has been recognized by Inc. Magazine as the fastest-growing private US small business. We work with U.S. government customers in health, civilian, and national security missions. As a core member you’ll work with a diverse team of professionals to solution matters, architect nuisances, and come up with alternatives. We offer competitive compensation with opportunities for bonuses, employer paid health care, training and development funds, and 401k match.  Responsibilities include (But Are Not Necessarily Limited To):
 
   Research, design, build, optimize and maintain reliable, efficient, and accessible data models, systems and pipelines/APIs etc.
   Support, with guidance, the analytic and/or operational use of data.
   Align closely with Enterprise partners in data science, architecture, governance, infrastructure, and security to apply standards and optimize production environments and practices.
   Collaborate with business owners to optimize data collection, movement, storage, and usage to data process and data quality.
   Convert concepts & ideas into workable prototypes (custom or COTS products) for client reviews and acceptance.
   Translate business needs into:
   
     data architecture solutions development within supported data systems.
     data orchestration pipelines (source to target analysis & recommendations), data sourcing, cleansing, augmentation and quality control processes within supported data systems.
     Prototype, test and integrate new data tools (i.e. data features and functionality) as defined by the product owners and business teams
   
 
  Competency and skill set will determine level of placement within the posted job family.  Qualifications:
 
   Bachelor’s degree in computer science, information systems management or similarly related degree.
   7+ years of professional data solutions development and implementation experience with:
   
     AWS (Glue, Athena, API Gateway)
     SQL, NoSQL
     Data developments with modeling tools such as Neo4J, Erwin, Embarcadero, transforming logical, physical, conceptual, reverse engineering & forward engineering.
     Development with Alation and/or EASparx
     Data Movement tools such as Informatica & others…
     Unit testing
     RESTful API Development
     Desire and willingness to learn new data tools
   
   Has an Agile mindset and iterative development process background
   
     Help promote a culture of diversity and inclusion within the department and the larger organization
     Value different ideas and opinions
     Listen courageously and remain curious in all that you do
   
   CMS data experience a must
   CMS Public Trust clearance, EUA highly preferred
 
  Valuable Experience:
 
   AWS CDK and/or other AWS services (or comparable cloud data solutioning tools)
   Experience with Git and CICD pipelines
   Relational database design
   Microservices / Containers (Docker, Kubernetes)
   Informatica Intelligent Cloud Services (IICS)
   Prior experience with CMS, preferably within clinical quality or standards area
 
  About ANALYTICA: Analytica is a leading consulting and information technology solutions provider to public sector organizations supporting health, civilian, and national security missions. Founded in 2009 and headquartered in Bethesda, MD., the company is an established 8(a) small business that has been recognized by Inc. Magazine each of the past three years as one of the 250 fastest-growing companies in the U.S. Analytica specializes in providing software and systems engineering, information management, analytics & visualization, agile project management, and management consulting services. The company is appraised by the Software Engineering Institute (SEI) at CMMI® Maturity Level 3 and is an ISO 9001:2008 certified provider.  As a federal contractor, Analytica is required to verify that all employees are fully vaccinated against COVID-19. If you receive an offer and are unable to get vaccinated for religious or medical reasons, you may request a reasonable accommodation. 
  
 vR3cn1Uzfh","<div>
 <p>ANALYTICA is seeking a <b>Senior </b><b>Data Engineer </b>to support a federal government client in the DC metro area (Note - your work location is REMOTE). In this assignment, you will be a team member serving the client in advancing the customers use data, metadata, as well as explore new technologies to better meet those needs.<br> <br> This is a mission that takes some serious smarts, intense curiosity and a background in developing data solutions across the data lifecycle.<br> <br> Analytica has been recognized by Inc. Magazine as the fastest-growing private US small business. We work with U.S. government customers in health, civilian, and national security missions. As a core member you&#x2019;ll work with a diverse team of professionals to solution matters, architect nuisances, and come up with alternatives. We offer competitive compensation with opportunities for bonuses, employer paid health care, training and development funds, and 401k match.<br> <br> <b>Responsibilities include (But Are Not Necessarily Limited To):</b></p>
 <ul>
  <li> Research, design, build, optimize and maintain reliable, efficient, and accessible data models, systems and pipelines/APIs etc.</li>
  <li> Support, with guidance, the analytic and/or operational use of data.</li>
  <li> Align closely with Enterprise partners in data science, architecture, governance, infrastructure, and security to apply standards and optimize production environments and practices.</li>
  <li> Collaborate with business owners to optimize data collection, movement, storage, and usage to data process and data quality.</li>
  <li> Convert concepts &amp; ideas into workable prototypes (custom or COTS products) for client reviews and acceptance.</li>
  <li> Translate business needs into:
   <ul>
    <li> data architecture solutions development within supported data systems.</li>
    <li> data orchestration pipelines (source to target analysis &amp; recommendations), data sourcing, cleansing, augmentation and quality control processes within supported data systems.</li>
    <li> Prototype, test and integrate new data tools (i.e. data features and functionality) as defined by the product owners and business teams</li>
   </ul></li>
 </ul>
 <p> Competency and skill set will determine level of placement within the posted job family.<br> <br> <b>Qualifications:</b></p>
 <ul>
  <li> Bachelor&#x2019;s degree in computer science, information systems management or similarly related degree.</li>
  <li> 7+ years of professional data solutions development and implementation experience with:
   <ul>
    <li> AWS (Glue, Athena, API Gateway)</li>
    <li> SQL, NoSQL</li>
    <li> Data developments with modeling tools such as Neo4J, Erwin, Embarcadero, transforming logical, physical, conceptual, reverse engineering &amp; forward engineering.</li>
    <li> Development with Alation and/or EASparx</li>
    <li> Data Movement tools such as Informatica &amp; others&#x2026;</li>
    <li> Unit testing</li>
    <li> RESTful API Development</li>
    <li> Desire and willingness to learn new data tools</li>
   </ul></li>
  <li> Has an Agile mindset and iterative development process background
   <ul>
    <li> Help promote a culture of diversity and inclusion within the department and the larger organization</li>
    <li> Value different ideas and opinions</li>
    <li> Listen courageously and remain curious in all that you do</li>
   </ul></li>
  <li> CMS data experience a must</li>
  <li> CMS Public Trust clearance, EUA highly preferred</li>
 </ul>
 <p><b> Valuable Experience:</b></p>
 <ul>
  <li> AWS CDK and/or other AWS services (or comparable cloud data solutioning tools)</li>
  <li> Experience with Git and CICD pipelines</li>
  <li> Relational database design</li>
  <li> Microservices / Containers (Docker, Kubernetes)</li>
  <li> Informatica Intelligent Cloud Services (IICS)</li>
  <li> Prior experience with CMS, preferably within clinical quality or standards area</li>
 </ul>
 <p><b><br> About </b><b>ANALYTICA</b>: Analytica is a leading consulting and information technology solutions provider to public sector organizations supporting health, civilian, and national security missions. Founded in 2009 and headquartered in Bethesda, MD., the company is an established 8(a) small business that has been recognized by <i>Inc. Magazine</i> each of the past three years as one of the 250 fastest-growing companies in the U.S. Analytica specializes in providing software and systems engineering, information management, analytics &amp; visualization, agile project management, and management consulting services. The company is appraised by the Software Engineering Institute (SEI) at CMMI&#xae; Maturity Level 3 and is an ISO 9001:2008 certified provider.<br> <br> As a federal contractor, Analytica is required to verify that all employees are fully vaccinated against COVID-19. If you receive an offer and are unable to get vaccinated for religious or medical reasons, you may request a reasonable accommodation.<br> </p>
 <p> </p>
 <p>vR3cn1Uzfh</p>
</div>","https://analyticallc.applytojob.com/apply/vR3cn1Uzfh/Senior-Data-Engineer?source=INDE","814edb5947a6ddd9",,"Full-time",,"Remote","Senior Data Engineer","7 days ago","2023-10-18T11:49:49.711Z","3.5","10",,"2023-10-25T11:49:49.713Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=814edb5947a6ddd9&from=jasx&tk=1hdjaltphk26u800&vjs=3"
"Booz Allen Hamilton","Job Description 
  
 
 
  
   
    
     
      
       
        
         Location: 
        
        
         Springfield,VA,US 
        
       
       
        
         Remote Work: 
        
        
         Yes 
        
       
       
        
         Job Number: 
        
        
         R0182529
        
       
      
     
    
    
    
      
     
       
      
     
    
    
    
     
      
       
        
         Azure Data Engineer, Lead
          The Opportunity:
          The DHS Cube program is seeking an experienced Data Engineer for its data management, data integration, and business intelligence program. This high-visibility mission support data program is managed within Headquarters for the DHS and our numerous client stakeholders. Over the next year and beyond, this program will be looking to take on new challenges, including modernizing their business intelligence technologies through a move to cloud services, improving the data quality through the development of confidence scorecards, and development of a data governance or data standards prototype.
         
          As part of this role, our Azure ETL Engineer will be responsible for moving ETL pipelines from one framework to a new framework. The role will also perform ETL testing and troubleshooting, including testing the unit data models, system performance testing, uploading, downloading, or querying speed tests, and data flow validations. In this role, you will also be building repeatable documentation assets to enable implementation teams to take advantage of pre-built assets, identifying data storage requirements, determining the storage needs of the customer, and ongoing development of a repository of ETL knowledge and utilizing that to enhance the quality, speed, and productivity of the team. As a senior member of the team, you’ll also be supporting more mid-level engineers with the testing and QA of their code along with an expectation of enhanced code velocity, design thinking, and the ability to work independently and provide feedback to the Data Architect on required design updates throughout the process.
         
          Join us. The world can’t wait.
         
          You Have: 
         
          8+ years of experience in a role encompassing industry standard ETL development techniques
           8+ years of experience with data integration, and database technologies, including Oracle, Postgres, Cosmos, or SQL
           3+ years of experience with Azure Cloud SaaS solutions and managed services serverless technologies, including Azure Data Factory, Synapse Analytics, Logic Apps, or ADLS
           Experience with scripting and basic programming, including JavaScript, shell script, or Python
           Experience with data analysis and profiling of source data while developing or building robust ETL processes
           Knowledge of disparate data sources and targets
           Knowledge of data validation, cleansing, transformation, consolidation, de-duplication, aggregation, de-aggregation, and enrichment
           Knowledge of API development and testing
           DHS Suitability
           Bachelor’s degree
         
         
          Nice If You Have:
         
           Experience with Agile and Scrum methodologies
           Experience with Azure Platform CI/CD or DevOps
           Experience with reporting tools, including Tableau or Power BI
           Experience with industry standard ETL tools such as SQL, scripting languages, data modelling techniques, or relational and NoSQL database engineering and configuration, including document store, Azure Cosmos, or AWS DynamoDB
           Knowledge of data transactional requirements, business logic pertaining to commit and rollback cycles, and how to implement to preserve the integrity of related data elements in government, financial and similar systems
           Ability to build ETL processes that apply to data migration and data integration scenarios to know their key differences
           Ability to adapt to a rapidly changing product and respond strategically to client needs
           Ability to balance multiple efforts simultaneously and meet strict deadlines
           Ability to have a desire for learning and development, and a passion for exploratory analysis or exploratory learning
         
         
          Vetting: 
         Applicants selected will be subject to a government investigation and may need to meet eligibility requirements of the U.S. government client; DHS suitability is required.
         
          Create Your Career:
         
          Grow With Us
          Your growth matters to us—that’s why we offer a variety of ways for you to develop your career. With professional and leadership development opportunities like upskilling programs, tuition reimbursement, mentoring, and firm-sponsored networking, you can chart a unique and fulfilling career path on your own terms.
         
          A Place Where You Belong
          Diverse perspectives cultivate collective ingenuity. Booz Allen’s culture of respect, equity, and opportunity means that, here, you are free to bring your whole self to work. With an array of business resource groups and other opportunities for connection, you’ll develop your community in no time.
         
          Support Your Well-Being
          Our comprehensive benefits package includes wellness programs with HSA contributions, paid holidays, paid parental leave, a generous 401(k) match, and more. With these benefits, plus the option for flexible schedules and remote and hybrid locations, we’ll support you as you pursue a balanced, fulfilling life—at work and at home.
         
          Your Candidate Journey
          At Booz Allen, we know our people are what propel us forward, and we value relationships most of all. Here, we’ve compiled a list of resources so you’ll know what to expect as we forge a connection with you during your journey as a candidate with us.
         
          Compensation
          At Booz Allen, we celebrate your contributions, provide you with opportunities and choices, and support your total well-being. Our offerings include health, life, disability, financial, and retirement benefits, as well as paid leave, professional development, tuition assistance, work-life programs, and dependent care. Our recognition awards program acknowledges employees for exceptional performance and superior demonstration of our values. Full-time and part-time employees working at least 20 hours a week on a regular basis are eligible to participate in Booz Allen’s benefit programs. Individuals that do not meet the threshold are only eligible for select offerings, not inclusive of health benefits. We encourage you to learn more about our total benefits by visiting the Resource page on our Careers site and reviewing Our Employee Benefits page.
          Salary at Booz Allen is determined by various factors, including but not limited to location, the individual’s particular combination of education, knowledge, skills, competencies, and experience, as well as contract-specific affordability and organizational requirements. The projected compensation range for this position is $93,300.00 to $212,000.00 (annualized USD). The estimate displayed represents the typical salary range for this position and is just one component of Booz Allen’s total compensation package for employees.
         
          Work Model Our people-first culture prioritizes the benefits of flexibility and collaboration, whether that happens in person or remotely.
         
           If this position is listed as remote or hybrid, you’ll periodically work from a Booz Allen or client site facility.
           If this position is listed as onsite, you’ll work with colleagues and clients in person, as needed for the specific role.
         
         
          EEO Commitment
          We’re an equal employment opportunity/affirmative action employer that empowers our people to fearlessly drive change – no matter their race, color, ethnicity, religion, sex (including pregnancy, childbirth, lactation, or related medical conditions), national origin, ancestry, age, marital status, sexual orientation, gender identity and expression, disability, veteran status, military or uniformed service member status, genetic information, or any other status protected by applicable federal, state, local, or international law.","<div>
 <div>
  <div>
   <h2 class=""jobSectionHeader""><b>Job Description</b></h2> 
  </div>
 </div>
 <div>
  <div>
   <div>
    <div>
     <div>
      <div>
       <div>
        <div>
         Location: 
        </div>
        <div>
         Springfield,VA,US 
        </div>
       </div>
       <div>
        <div>
         Remote Work: 
        </div>
        <div>
         Yes 
        </div>
       </div>
       <div>
        <div>
         Job Number: 
        </div>
        <div>
         R0182529
        </div>
       </div>
      </div>
     </div>
    </div>
    <p></p>
    <div>
     <br> 
     <div>
      <div> 
      </div>
     </div>
    </div>
    <div></div>
    <div>
     <div>
      <div>
       <div>
        <div>
         Azure Data Engineer, Lead
         <p><b> The Opportunity:</b></p>
         <p> The DHS Cube program is seeking an experienced Data Engineer for its data management, data integration, and business intelligence program. This high-visibility mission support data program is managed within Headquarters for the DHS and our numerous client stakeholders. Over the next year and beyond, this program will be looking to take on new challenges, including modernizing their business intelligence technologies through a move to cloud services, improving the data quality through the development of confidence scorecards, and development of a data governance or data standards prototype.</p>
         <p></p>
         <p> As part of this role, our Azure ETL Engineer will be responsible for moving ETL pipelines from one framework to a new framework. The role will also perform ETL testing and troubleshooting, including testing the unit data models, system performance testing, uploading, downloading, or querying speed tests, and data flow validations. In this role, you will also be building repeatable documentation assets to enable implementation teams to take advantage of pre-built assets, identifying data storage requirements, determining the storage needs of the customer, and ongoing development of a repository of ETL knowledge and utilizing that to enhance the quality, speed, and productivity of the team. As a senior member of the team, you&#x2019;ll also be supporting more mid-level engineers with the testing and QA of their code along with an expectation of enhanced code velocity, design thinking, and the ability to work independently and provide feedback to the Data Architect on required design updates throughout the process.</p>
         <p></p>
         <p> Join us. The world can&#x2019;t wait.</p>
         <p></p>
         <p><b> You Have: </b></p>
         <ul>
          <li>8+ years of experience in a role encompassing industry standard ETL development techniques</li>
          <li> 8+ years of experience with data integration, and database technologies, including Oracle, Postgres, Cosmos, or SQL</li>
          <li> 3+ years of experience with Azure Cloud SaaS solutions and managed services serverless technologies, including Azure Data Factory, Synapse Analytics, Logic Apps, or ADLS</li>
          <li> Experience with scripting and basic programming, including JavaScript, shell script, or Python</li>
          <li> Experience with data analysis and profiling of source data while developing or building robust ETL processes</li>
          <li> Knowledge of disparate data sources and targets</li>
          <li> Knowledge of data validation, cleansing, transformation, consolidation, de-duplication, aggregation, de-aggregation, and enrichment</li>
          <li> Knowledge of API development and testing</li>
          <li> DHS Suitability</li>
          <li> Bachelor&#x2019;s degree</li>
         </ul>
         <p></p>
         <p><b> Nice If You Have:</b></p>
         <ul>
          <li> Experience with Agile and Scrum methodologies</li>
          <li> Experience with Azure Platform CI/CD or DevOps</li>
          <li> Experience with reporting tools, including Tableau or Power BI</li>
          <li> Experience with industry standard ETL tools such as SQL, scripting languages, data modelling techniques, or relational and NoSQL database engineering and configuration, including document store, Azure Cosmos, or AWS DynamoDB</li>
          <li> Knowledge of data transactional requirements, business logic pertaining to commit and rollback cycles, and how to implement to preserve the integrity of related data elements in government, financial and similar systems</li>
          <li> Ability to build ETL processes that apply to data migration and data integration scenarios to know their key differences</li>
          <li> Ability to adapt to a rapidly changing product and respond strategically to client needs</li>
          <li> Ability to balance multiple efforts simultaneously and meet strict deadlines</li>
          <li> Ability to have a desire for learning and development, and a passion for exploratory analysis or exploratory learning</li>
         </ul>
         <p></p>
         <p><b> Vetting: </b></p>
         <p>Applicants selected will be subject to a government investigation and may need to meet eligibility requirements of the U.S. government client; DHS suitability is required.</p>
         <p></p>
         <p><b> Create Your Career:</b></p>
         <p></p>
         <p><b> Grow With Us</b></p>
         <p> Your growth matters to us&#x2014;that&#x2019;s why we offer a variety of ways for you to develop your career. With professional and leadership development opportunities like upskilling programs, tuition reimbursement, mentoring, and firm-sponsored networking, you can chart a unique and fulfilling career path on your own terms.</p>
         <p></p>
         <p><b> A Place Where You Belong</b></p>
         <p> Diverse perspectives cultivate collective ingenuity. Booz Allen&#x2019;s culture of respect, equity, and opportunity means that, here, you are free to bring your whole self to work. With an array of business resource groups and other opportunities for connection, you&#x2019;ll develop your community in no time.</p>
         <p></p>
         <p><b> Support Your Well-Being</b></p>
         <p> Our comprehensive benefits package includes wellness programs with HSA contributions, paid holidays, paid parental leave, a generous 401(k) match, and more. With these benefits, plus the option for flexible schedules and remote and hybrid locations, we&#x2019;ll support you as you pursue a balanced, fulfilling life&#x2014;at work and at home.</p>
         <p></p>
         <p><b> Your Candidate Journey</b></p>
         <p> At Booz Allen, we know our people are what propel us forward, and we value relationships most of all. Here, we&#x2019;ve compiled a list of resources so you&#x2019;ll know what to expect as we forge a connection with you during your journey as a candidate with us.</p>
         <p></p>
         <p><b> Compensation</b></p>
         <p> At Booz Allen, we celebrate your contributions, provide you with opportunities and choices, and support your total well-being. Our offerings include health, life, disability, financial, and retirement benefits, as well as paid leave, professional development, tuition assistance, work-life programs, and dependent care. Our recognition awards program acknowledges employees for exceptional performance and superior demonstration of our values. Full-time and part-time employees working at least 20 hours a week on a regular basis are eligible to participate in Booz Allen&#x2019;s benefit programs. Individuals that do not meet the threshold are only eligible for select offerings, not inclusive of health benefits. We encourage you to learn more about our total benefits by visiting the Resource page on our Careers site and reviewing Our Employee Benefits page.</p>
         <p></p> Salary at Booz Allen is determined by various factors, including but not limited to location, the individual&#x2019;s particular combination of education, knowledge, skills, competencies, and experience, as well as contract-specific affordability and organizational requirements. The projected compensation range for this position is &#x24;93,300.00 to &#x24;212,000.00 (annualized USD). The estimate displayed represents the typical salary range for this position and is just one component of Booz Allen&#x2019;s total compensation package for employees.
         <p></p>
         <p><b> Work Model</b><br> Our people-first culture prioritizes the benefits of flexibility and collaboration, whether that happens in person or remotely.</p>
         <ul>
          <li> If this position is listed as remote or hybrid, you&#x2019;ll periodically work from a Booz Allen or client site facility.</li>
          <li> If this position is listed as onsite, you&#x2019;ll work with colleagues and clients in person, as needed for the specific role.</li>
         </ul>
         <p></p>
         <p><b> EEO Commitment</b></p>
         <p> We&#x2019;re an equal employment opportunity/affirmative action employer that empowers our people to fearlessly drive change &#x2013; no matter their race, color, ethnicity, religion, sex (including pregnancy, childbirth, lactation, or related medical conditions), national origin, ancestry, age, marital status, sexual orientation, gender identity and expression, disability, veteran status, military or uniformed service member status, genetic information, or any other status protected by applicable federal, state, local, or international law.</p>
        </div>
       </div>
      </div>
     </div>
    </div>
   </div>
  </div>
 </div>
</div>
<p></p>","https://careers.boozallen.com/jobs/JobDetail/Springfield-Azure-Data-Engineer-Lead-R0182529/86836?source=JB-14400","bcb3fa3a95822e7e",,,,"Springfield, VA","Azure Data Engineer, Lead","6 days ago","2023-10-19T11:49:54.787Z","3.9","2515","$93,300 - $212,000 a year","2023-10-25T11:49:54.801Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=bcb3fa3a95822e7e&from=jasx&tk=1hdjaltphk26u800&vjs=3"
"Brillio","Lead data engineer - R01530706 
 
 
  About Brillio: 
 
 
  Brillio is the partner of choice for many Fortune 1000 companies seeking to turn disruption into a competitive advantage through innovative digital adoption. Backed by Bain Capital, Brillio is one of the fastest growing digital technology service providers. We help clients harness the transformative potential of the four superpowers of technology - cloud computing, internet of things (IoT), artificial intelligence (AI), and mobility. Born digital in 2014, we apply Customer Experience Solutions, Data Analytics and AI, Digital Infrastructure and Security, and Platform and Product Engineering expertise to help clients quickly innovate for growth, create digital products, build service platforms, and drive smarter, data-driven performance. With delivery locations across United States, Romania, Canada, Mexico, and India, our growing global workforce of over 6,000 Brillians blends the latest technology and design thinking with digital fluency to solve complex business problems and drive competitive differentiation for our clients. Brillio was awarded ‘Great Place to Work’ in 2021 and 2022
 
 
 
   Lead data engineer
 
  Primary Skills
 
   SNS, SQS, Athena, CloudWatch, Kinesis, Redshift 
 
 Specialization
 
   AWS Data EngineerIng Advanced: Associate Data Engineer 
 
 Job requirements
 
   Role: Lead Data Engineer
 
 
   Years of Experience: 10+ years
 
 
   Travel Required: Yes
 
 
   Location: Remote
 
  
 
 
   As a consultant within the DIE team, you will work with our clients to define their digital strategy and execution roadmap, and design and implement differentiated digital solutions to help deliver measurable value.
 
  
 
 
   Your responsibilities in this role will include:
 
 
   Primary focus is on Glue, S3, Redshift, Lambda, PySpark, Spark. 
  Then added skillset which could add value are AWS Step function, NoSQL DB like Dynamo DB and AWS Data Migration Service in that order of priority.
   Data engineer should have at least 2 years of relevant AWS experience with their services mentioned above. 
  Experience in data security or governance and performance improvement is an added benefit
   Only focus is on AWS services and tech stack.""
 
 
 
   Why should you apply for this role?
 
 
   As Brillio continues to gain momentum as a trusted partner for our clients in their digital transformation journey, we strive to set new benchmarks for speed and value creation. The DIE team at Brillio is at the forefront of leading this charge by reimagining and executing how we structure, sell and deliver our services to better serve our clients.
 
  
 
 
   DAE: https://www.brillio.com/services-data-analytics/
 
 
 
   Know what it’s like to work and grow at Brillio:https://www.brillio.com/join-us/
 
 
 
   Equal Employment Opportunity Declaration
 
 
   Brillio is an equal opportunity employer to all, regardless of age, ancestry, colour, disability (mental and physical), exercising the right to family care and medical leave, gender, gender expression, gender identity, genetic information, marital status, medical condition, military or veteran status, national origin, political affiliation, race, religious creed, sex (includes pregnancy, childbirth, breastfeeding, and related medical conditions), and sexual orientation.
 
  
 
 
   #LI-RJ1
 
  
 
 
   Know what it’s like to work and grow at Brillio: Click here","<div>
 <h1 class=""jobSectionHeader""><b>Lead data engineer - R01530706</b></h1> 
 <p></p>
 <div>
  <b>About Brillio: </b>
 </div>
 <div>
  Brillio is the partner of choice for many Fortune 1000 companies seeking to turn disruption into a competitive advantage through innovative digital adoption. Backed by Bain Capital, Brillio is one of the fastest growing digital technology service providers. We help clients harness the transformative potential of the four superpowers of technology - cloud computing, internet of things (IoT), artificial intelligence (AI), and mobility. Born digital in 2014, we apply Customer Experience Solutions, Data Analytics and AI, Digital Infrastructure and Security, and Platform and Product Engineering expertise to help clients quickly innovate for growth, create digital products, build service platforms, and drive smarter, data-driven performance. With delivery locations across United States, Romania, Canada, Mexico, and India, our growing global workforce of over 6,000 Brillians blends the latest technology and design thinking with digital fluency to solve complex business problems and drive competitive differentiation for our clients. Brillio was awarded &#x2018;Great Place to Work&#x2019; in 2021 and 2022
 </div>
 <div></div>
 <div>
  <b><br> Lead data engineer</b>
 </div>
 <h6 class=""jobSectionHeader""><b> Primary Skills</b></h6>
 <ul>
  <li> SNS, SQS, Athena, CloudWatch, Kinesis, Redshift </li>
 </ul>
 <h6 class=""jobSectionHeader""><b>Specialization</b></h6>
 <ul>
  <li> AWS Data EngineerIng Advanced: Associate Data Engineer </li>
 </ul>
 <h6 class=""jobSectionHeader""><b>Job requirements</b></h6>
 <div>
  <b><br> Role:</b> Lead Data Engineer
 </div>
 <div>
  <b> Years of Experience:</b> 10+ years
 </div>
 <div>
  <b> Travel Required:</b> Yes
 </div>
 <div>
  <b> Location:</b> Remote
 </div>
 <div> 
 </div>
 <div>
   As a consultant within the DIE team, you will work with our clients to define their digital strategy and execution roadmap, and design and implement differentiated digital solutions to help deliver measurable value.
 </div>
 <div> 
 </div>
 <div>
  <b> Your responsibilities in this role will include:</b>
 </div>
 <ul>
  <li> Primary focus is on Glue, S3, Redshift, Lambda, PySpark, Spark. </li>
  <li>Then added skillset which could add value are AWS Step function, NoSQL DB like Dynamo DB and AWS Data Migration Service in that order of priority.</li>
  <li> Data engineer should have at least 2 years of relevant AWS experience with their services mentioned above. </li>
  <li>Experience in data security or governance and performance improvement is an added benefit</li>
  <li> Only focus is on AWS services and tech stack.&quot;</li>
 </ul>
 <div></div>
 <div>
  <b><br> Why should you apply for this role?</b>
 </div>
 <div>
   As Brillio continues to gain momentum as a trusted partner for our clients in their digital transformation journey, we strive to set new benchmarks for speed and value creation. The DIE team at Brillio is at the forefront of leading this charge by reimagining and executing how we structure, sell and deliver our services to better serve our clients.
 </div>
 <div> 
 </div>
 <div>
   DAE: https://www.brillio.com/services-data-analytics/
 </div>
 <div></div>
 <div>
  <br> Know what it&#x2019;s like to work and grow at Brillio:https://www.brillio.com/join-us/
 </div>
 <div></div>
 <div>
  <b><br> Equal Employment Opportunity Declaration</b>
 </div>
 <div>
   Brillio is an equal opportunity employer to all, regardless of age, ancestry, colour, disability (mental and physical), exercising the right to family care and medical leave, gender, gender expression, gender identity, genetic information, marital status, medical condition, military or veteran status, national origin, political affiliation, race, religious creed, sex (includes pregnancy, childbirth, breastfeeding, and related medical conditions), and sexual orientation.
 </div>
 <div> 
 </div>
 <div>
   #LI-RJ1
 </div>
 <div> 
 </div>
 <div>
   Know what it&#x2019;s like to work and grow at Brillio: Click here
 </div>
</div>
<p></p>","https://careers.brillio.com/job-details/?job-id=20799","1135c1400eced1b6",,,,"San Ramon, CA","Lead data engineer – R01530706","6 days ago","2023-10-19T11:49:56.321Z","3.4","170",,"2023-10-25T11:49:56.322Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=1135c1400eced1b6&from=jasx&tk=1hdjaltphk26u800&vjs=3"
"Verizon","When you join Verizon
  Verizon is one of the world’s leading providers of technology and communications services, transforming the way we connect around the world. We’re a human network that reaches across the globe and works behind the scenes. We anticipate, lead, and believe that listening is where learning begins. In crisis and in celebration, we come together—lifting up our communities and striving to make an impact to move the world forward. If you’re fueled by purpose, and powered by persistence, explore a career with us. Here, you’ll discover the rigor it takes to make a difference and the fulfillment that comes with living the #NetworkLife.
 
  What you’ll be doing...
 
  Are you ready for the challenge of driving transformation at the scale of a $130B+ Fortune 50 company through an industry leading technology stack?
 
  In this role, you will be responsible for Technical Product Management for Verizon’s Corporate end to end master data management capabilities, focused on building a robust human centered experience spanning across the Supply Chain, Finance, and HR data domains. Trusted data starts with complete, accurate and valid master data. Your work will focus on shaping human centered end to end processes from identification of the need for new master data, through to its request and eventual creation / maintenance including full accountability for product vision & strategy, roadmap and backlogs around key mission objectives, delivering features from concept to launch through agile delivery methodologies, ensuring value realization, run governance and support that delivers outstanding business results / user experience. Partnership is essential in this role, as we focus on shaping a federated data strategy that requires the definition of data standards in partnership with various GPOs within the Finance, Supply Chain and HR domains, collaboration in the establishment of data catalogs that enable end users to leverage plain business language to query their data, and connectivity with owners of commercial and other non-corporate data to which the corporate functions will subscribe.
 
  The ideal candidate for this role will be equal parts technical and functional leader to be successful, with the ability to set rigorous technical / architectural standards whilst speaking the language of our business partners to influence the strategic direction, master data and governance capabilities. Focusing on driving the innovation agenda, this leader will work with internal & external industry leaders to shape world-class capabilities and enable continuous improvement.
 
  This Principal Engineer will drive the MDM strategy with other MDM SME’s, Analysts and 3rd party partners, as well as influence Global Process Owners to deliver transformative solutions.
 
  Responsibilities 
 
  Collaborating with cross-functional teams to define and enforce data governance policies, standards, and best practices within Oracle EDMCS and SAP MDG.
   Utilizing your deep knowledge of Oracle EDMCS and SAP MDG to configure and maintain workflows, and validation rules.
   Being proficient in Google Cloud Platform (GCP) services and tools, including BigQuery, Dataflow, Dataproc, and Pub/Sub.
   Working closely with IT and Business teams to ensure seamless data integration between Oracle EDMCS, SAP MDG, and other enterprise systems.
   Developing and maintaining data mapping and transformation rules to ensure data consistency and compliance.
   Collaborating with data stewards and business users to resolve data-related issues and support data maintenance activities as well as own function design fo the end to end master data lifecycle mgmt
   Creating and maintaining comprehensive documentation for data management processes, configurations, and standard operating procedures.
 
 
  What we’re looking for...  You’ll need to have:
 
   Bachelor’s degree or four or more years of work experience.
   Six or more years of relevant work experience
   Experience with Oracle EDMCS/DRM and SAP MDG.
   Experience in managing or executing data cleansing, data mapping, and data governance areas, preferably in an SAP environment as well as integration across complex ERP landscapes.
   Experience of interfacing SAP with legacy and modern data ecosystems.
   Experience in data modeling, data mapping, and data transformation.
   Experience in data governance, data quality management, and MDM concepts.
 
 
  Even better if you have one or more of the following:
 
   SAP MDG or Oracle EDMCS certification is a plus.
   Working knowledge of SAP finance modules.
   Experience with Collibra workflows and master data modules.
   Experience in successful delivery of scaled agile.
   Scripting and programming skills in languages like Python, Java, or Scala.
   Experience as a Data Engineer or similar role with a focus on GCP. Strong knowledge of cloud-based data storage solutions, such as Cloud Storage, Cloud SQL, and Bigtable.
   Knowledge of data privacy regulations and compliance standards (e.g., GDPR, CCPA).
 
 
  If Verizon and this role sound like a fit for you, we encourage you to apply even if you don’t meet every “even better” qualification listed above.  This role may be considered as part of the Department of Defense SkillBridge Program.
 
 
  
   
    
     
      
       
         Where you’ll be working
       
      
     
    
   
  
  In this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager.
 
  Scheduled Weekly Hours 40
 
  Equal Employment Opportunity
  We’re proud to be an equal opportunity employer - and celebrate our employees’ differences, including race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, and Veteran status. At Verizon, we know that diversity makes us stronger. We are committed to a collaborative, inclusive environment that encourages authenticity and fosters a sense of belonging. We strive for everyone to feel valued, connected, and empowered to reach their potential and contribute their best. Check out our diversity and inclusion page to learn more.","<div>
 <h3 class=""jobSectionHeader""><b>When you join Verizon</b></h3>
 <p> Verizon is one of the world&#x2019;s leading providers of technology and communications services, transforming the way we connect around the world. We&#x2019;re a human network that reaches across the globe and works behind the scenes. We anticipate, lead, and believe that listening is where learning begins. In crisis and in celebration, we come together&#x2014;lifting up our communities and striving to make an impact to move the world forward. If you&#x2019;re fueled by purpose, and powered by persistence, explore a career with us. Here, you&#x2019;ll discover the rigor it takes to make a difference and the fulfillment that comes with living the #NetworkLife.</p>
 <p></p>
 <h3 class=""jobSectionHeader""><b> What you&#x2019;ll be doing...</b></h3>
 <p></p>
 <p> Are you ready for the challenge of driving transformation at the scale of a &#x24;130B+ Fortune 50 company through an industry leading technology stack?</p>
 <p></p>
 <p> In this role, you will be responsible for Technical Product Management for Verizon&#x2019;s Corporate end to end master data management capabilities, focused on building a robust human centered experience spanning across the Supply Chain, Finance, and HR data domains. Trusted data starts with complete, accurate and valid master data. Your work will focus on shaping human centered end to end processes from identification of the need for new master data, through to its request and eventual creation / maintenance including full accountability for product vision &amp; strategy, roadmap and backlogs around key mission objectives, delivering features from concept to launch through agile delivery methodologies, ensuring value realization, run governance and support that delivers outstanding business results / user experience. Partnership is essential in this role, as we focus on shaping a federated data strategy that requires the definition of data standards in partnership with various GPOs within the Finance, Supply Chain and HR domains, collaboration in the establishment of data catalogs that enable end users to leverage plain business language to query their data, and connectivity with owners of commercial and other non-corporate data to which the corporate functions will subscribe.</p>
 <p></p>
 <p> The ideal candidate for this role will be equal parts technical and functional leader to be successful, with the ability to set rigorous technical / architectural standards whilst speaking the language of our business partners to influence the strategic direction, master data and governance capabilities. Focusing on driving the innovation agenda, this leader will work with internal &amp; external industry leaders to shape world-class capabilities and enable continuous improvement.</p>
 <p></p>
 <p> This Principal Engineer will drive the MDM strategy with other MDM SME&#x2019;s, Analysts and 3rd party partners, as well as influence Global Process Owners to deliver transformative solutions.</p>
 <p></p>
 <p><b> Responsibilities</b><b> </b></p>
 <ul>
  <li><p>Collaborating with cross-functional teams to define and enforce data governance policies, standards, and best practices within Oracle EDMCS and SAP MDG.</p></li>
  <li><p> Utilizing your deep knowledge of Oracle EDMCS and SAP MDG to configure and maintain workflows, and validation rules.</p></li>
  <li><p> Being proficient in Google Cloud Platform (GCP) services and tools, including BigQuery, Dataflow, Dataproc, and Pub/Sub.</p></li>
  <li><p> Working closely with IT and Business teams to ensure seamless data integration between Oracle EDMCS, SAP MDG, and other enterprise systems.</p></li>
  <li><p> Developing and maintaining data mapping and transformation rules to ensure data consistency and compliance.</p></li>
  <li><p> Collaborating with data stewards and business users to resolve data-related issues and support data maintenance activities as well as own function design fo the end to end master data lifecycle mgmt</p></li>
  <li><p> Creating and maintaining comprehensive documentation for data management processes, configurations, and standard operating procedures.</p></li>
 </ul>
 <p></p>
 <p><b> What we&#x2019;re looking for...</b><br> <br> You&#x2019;ll need to have:</p>
 <ul>
  <li><p> Bachelor&#x2019;s degree or four or more years of work experience.</p></li>
  <li><p> Six or more years of relevant work experience</p></li>
  <li><p> Experience with Oracle EDMCS/DRM and SAP MDG.</p></li>
  <li><p> Experience in managing or executing data cleansing, data mapping, and data governance areas, preferably in an SAP environment as well as integration across complex ERP landscapes.</p></li>
  <li><p> Experience of interfacing SAP with legacy and modern data ecosystems.</p></li>
  <li><p> Experience in data modeling, data mapping, and data transformation.</p></li>
  <li><p> Experience in data governance, data quality management, and MDM concepts.</p></li>
 </ul>
 <p></p>
 <p> Even better if you have one or more of the following:</p>
 <ul>
  <li><p> SAP MDG or Oracle EDMCS certification is a plus.</p></li>
  <li><p> Working knowledge of SAP finance modules.</p></li>
  <li><p> Experience with Collibra workflows and master data modules.</p></li>
  <li><p> Experience in successful delivery of scaled agile.</p></li>
  <li><p> Scripting and programming skills in languages like Python, Java, or Scala.</p></li>
  <li><p> Experience as a Data Engineer or similar role with a focus on GCP. Strong knowledge of cloud-based data storage solutions, such as Cloud Storage, Cloud SQL, and Bigtable.</p></li>
  <li><p> Knowledge of data privacy regulations and compliance standards (e.g., GDPR, CCPA).</p></li>
 </ul>
 <p></p>
 <p> If Verizon and this role sound like a fit for you, we encourage you to apply even if you don&#x2019;t meet every &#x201c;even better&#x201d; qualification listed above.<br> <br> This role may be considered as part of the Department of Defense SkillBridge Program.</p>
 <p></p>
 <div>
  <div>
   <div>
    <div>
     <div>
      <div>
       <div>
        <h3 class=""jobSectionHeader""><b> Where you&#x2019;ll be working</b></h3>
       </div>
      </div>
     </div>
    </div>
   </div>
  </div>
 </div> In this hybrid role, you&apos;ll have a defined work location that includes work from home and assigned office days set by your manager.
 <p></p>
 <h3 class=""jobSectionHeader""><b> Scheduled Weekly Hours</b></h3> 40
 <p></p>
 <h3 class=""jobSectionHeader""><b> Equal Employment Opportunity</b></h3>
 <p> We&#x2019;re proud to be an equal opportunity employer - and celebrate our employees&#x2019; differences, including race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, and Veteran status. At Verizon, we know that diversity makes us stronger. We are committed to a collaborative, inclusive environment that encourages authenticity and fosters a sense of belonging. We strive for everyone to feel valued, connected, and empowered to reach their potential and contribute their best. Check out our diversity and inclusion page to learn more.</p>
</div>
<p></p>","https://mycareer.verizon.com/jobs/r-1019130/principal-engineer-master-data-management/?source=jb-indeed&dclid=CIDuj_KQkYIDFVQHaAgdwwUGvA","53f6056d9cfd3d9f",,"Full-time",,"Irving, TX","Principal Engineer - Master Data Management","6 days ago","2023-10-19T11:49:58.064Z","3.8","32098",,"2023-10-25T11:49:58.117Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=53f6056d9cfd3d9f&from=jasx&tk=1hdjaltphk26u800&vjs=3"
"Headspace","About the Staff Data Engineer at Headspace: 
  Headspace is seeking an experienced Staff Data Engineer to join our Data Products team (part of our Data Engineering org). In this role, you will be responsible for architecting and implementing a set of core data sets in our data lake. Your customers are our data consumers, including analysts, machine learning engineers and data scientists. 
  What you will do: 
  
  Design and implement mission critical data pipelines for our company 
  Help create a set of high-quality, composable data products for our data consumers 
  Write well designed, testable, performant, and efficient code that runs on Apache Spark and Delta Lake 
  Lead the development of a world-class data lake that meets the strict security, privacy, and compliance requirements of the healthcare industry 
  Collaborate with the data science and machine learning team to build data sets used for model training and development 
  Mentor and coach other engineers to build a data-first culture at the company 
  Write well designed, testable, performant, and efficient code 
  Contribute in all phases of the development lifecycle 
  
 What you will bring: 
  Required Skills: 
  
  6+ years professional software development 
  You’ve built high quality data pipelines before with comprehensive unit tests suites, data quality checks etc. 
  Has a solid grasp of building new frameworks, tools or systems. Able to bring creative technical solutions to the table and design solutions at scale. 
  Experience with Apache Spark and Delta Lake are a plus, but not required 
  Solid understanding of system topologies from machine architecture to network architecture. Ability to solve unique complex problems. 
  Ability to work independently with minimal supervision on system level projects. Identifies and corrects errors on their own. Assumes greater responsibilities and anticipates some team needs. A wide degree of creativity and latitude is expected. 
  Proposes new solutions, ideas, tools and techniques for moderately complex problems. 
  Begins to assume a lead role in team projects. Mentors and provides guidance. 
  Considers multiple approaches and recommends best technical direction including logic and reasoning to areas outside of the immediate team. 
  Proven hands-on Software Development experience, especially API and microservices architecture 
  
 Preferred Skills: 
  
  Having experience apache spark would be useful 
  Experience with data modeling 
  Python experience is a plus 
  
 Pay & Benefits: 
  The base salary range for this role is determined by a number of factors, including but not limited to skills and scope required, relevant licensure and certifications, and unique relevant experience and job-related skills. The base salary range for this role is $131,414 - $190,100. 
  At Headspace, cash salary is but one component of our Total Rewards package. We’re proud of our robust package inclusive of: base salary, stock awards, comprehensive healthcare coverage, monthly wellness stipend, retirement savings match, lifetime Headspace membership, unlimited, free mental health coaching, generous parental leave, and much more. Paid performance incentives are also included for those in eligible roles. Additional details about our Total Rewards package will be provided during the recruitment process. 
  How we feel about Diversity, Equity, Inclusion and Belonging: 
  Headspace is committed to bringing together humans from different backgrounds and perspectives, providing employees with a safe and welcoming work environment free of discrimination and harassment. We strive to create a diverse & inclusive environment where everyone can thrive, feel a sense of belonging, and do impactful work together. 
  As an equal opportunity employer, we prohibit any unlawful discrimination against a job applicant on the basis of their race, color, religion, gender, gender identity, gender expression, sexual orientation, national origin, family or parental status, disability*, age, veteran status, or any other status protected by the laws or regulations in the locations where we operate. We respect the laws enforced by the EEOC and are dedicated to going above and beyond in fostering diversity across our workplace. 
 
  Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and certain state or local laws. A reasonable accommodation is a change in the way things are normally done which will ensure an equal employment opportunity without imposing undue hardship on Headspace Health. Please inform our Talent team if you need any assistance completing any forms or to otherwise participate in the application or interview process.
  
  
 Headspace participates in the E-Verify Program. 
  Privacy Statement 
  All member records are protected according to our Privacy Policy. Further, while employees of Headspace (formerly Ginger) cannot access Headspace products/services, they will be offered benefits according to the company's benefit plan. To ensure we are adhering to best practice and ethical guidelines in the field of mental health, we take care to avoid dual relationships. A dual relationship occurs when a mental health care provider has a second, significantly different relationship with their client in addition to the traditional client-therapist relationship—including, for example, a managerial relationship. 
  As such, Headspace requests that individuals who have received coaching or clinical services at Headspace wait until their care with Headspace is complete before applying for a position. If someone with a Headspace account is hired for a position, please note their account will be deactivated and they will not be able to use Headspace services for the duration of their employment. 
  Further, if Headspace cannot find a role that fails to resolve an ethical issue associated with a dual relationship, Headspace may need to take steps to ensure ethical obligations are being adhered to, including a delayed start date or a potential leave of absence. Such steps would be taken to protect both the former member, as well as any relevant individuals from their care team, from impairment, risk of exploitation, or harm. 
  For how how we will use the personal information you provide as part of the application process, please see: https://organizations.headspace.com/page/applicant-notice. 
  #LI-LL1","<div>
 <p><b>About the Staff Data Engineer</b><b> at Headspace:</b></p> 
 <p><i> Headspace is seeking an experienced Staff Data Engineer to join our Data Products team (part of our Data Engineering org). In this role, you will be responsible for architecting and implementing a set of core data sets in our data lake. Your customers are our data consumers, including analysts, machine learning engineers and data scientists.</i></p> 
 <p><b> What you will do:</b></p> 
 <ul> 
  <li>Design and implement mission critical data pipelines for our company</li> 
  <li>Help create a set of high-quality, composable data products for our data consumers</li> 
  <li>Write well designed, testable, performant, and efficient code that runs on Apache Spark and Delta Lake</li> 
  <li>Lead the development of a world-class data lake that meets the strict security, privacy, and compliance requirements of the healthcare industry</li> 
  <li>Collaborate with the data science and machine learning team to build data sets used for model training and development</li> 
  <li>Mentor and coach other engineers to build a data-first culture at the company</li> 
  <li>Write well designed, testable, performant, and efficient code</li> 
  <li>Contribute in all phases of the development lifecycle</li> 
 </ul> 
 <p><b>What you will bring</b>:</p> 
 <p><b> Required Skills:</b></p> 
 <ul> 
  <li>6+ years professional software development</li> 
  <li>You&#x2019;ve built high quality data pipelines before with comprehensive unit tests suites, data quality checks etc.</li> 
  <li>Has a solid grasp of building new frameworks, tools or systems. Able to bring creative technical solutions to the table and design solutions at scale.</li> 
  <li>Experience with Apache Spark and Delta Lake are a plus, but not required</li> 
  <li>Solid understanding of system topologies from machine architecture to network architecture. Ability to solve unique complex problems.</li> 
  <li>Ability to work independently with minimal supervision on system level projects. Identifies and corrects errors on their own. Assumes greater responsibilities and anticipates some team needs. A wide degree of creativity and latitude is expected.</li> 
  <li>Proposes new solutions, ideas, tools and techniques for moderately complex problems.</li> 
  <li>Begins to assume a lead role in team projects. Mentors and provides guidance.</li> 
  <li>Considers multiple approaches and recommends best technical direction including logic and reasoning to areas outside of the immediate team.</li> 
  <li>Proven hands-on Software Development experience, especially API and microservices architecture</li> 
 </ul> 
 <p><b>Preferred Skills:</b></p> 
 <ul> 
  <li>Having experience apache spark would be useful</li> 
  <li>Experience with data modeling</li> 
  <li>Python experience is a plus</li> 
 </ul> 
 <p><b>Pay &amp; Benefits</b>:</p> 
 <p> The base salary range for this role is determined by a number of factors, including but not limited to skills and scope required, relevant licensure and certifications, and unique relevant experience and job-related skills. The base salary range for this role is <b>&#x24;131,414 - &#x24;190,100</b>.</p> 
 <p> At Headspace, cash salary is but one component of our Total Rewards package. We&#x2019;re proud of our robust package inclusive of: base salary, stock awards, comprehensive healthcare coverage, monthly wellness stipend, retirement savings match, lifetime Headspace membership, unlimited, free mental health coaching, generous parental leave, and much more. Paid performance incentives are also included for those in eligible roles. Additional details about our Total Rewards package will be provided during the recruitment process.</p> 
 <p><b> How we feel about Diversity, Equity, Inclusion and Belonging:</b></p> 
 <p> Headspace is committed to bringing together humans from different backgrounds and perspectives, providing employees with a safe and welcoming work environment free of discrimination and harassment. We strive to create a diverse &amp; inclusive environment where everyone can thrive, feel a sense of belonging, and do impactful work together.</p> 
 <p> As an equal opportunity employer, we prohibit any unlawful discrimination against a job applicant on the basis of their race, color, religion, gender, gender identity, gender expression, sexual orientation, national origin, family or parental status, disability*, age, veteran status, or any other status protected by the laws or regulations in the locations where we operate. We respect the laws enforced by the EEOC and are dedicated to going above and beyond in fostering diversity across our workplace.</p> 
 <ul>
  <li><i>Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and certain state or local laws. A reasonable accommodation is a change in the way things are normally done which will ensure an equal employment opportunity without imposing undue hardship on Headspace Health. </i><b><i>Please inform our Talent team</i></b><b><i> if you need any assistance completing any forms or to otherwise participate in the application or interview process.</i></b></li>
 </ul> 
 <p></p> 
 <p><i>Headspace participates in the </i><i>E-Verify Program</i><i>.</i></p> 
 <p><b><i> Privacy Statement</i></b></p> 
 <p><i> All member records are protected according to our</i><i> </i><i>Privacy Policy</i><i>. Further, while employees of Headspace (formerly Ginger) cannot access Headspace products/services, they will be offered benefits according to the company&apos;s benefit plan. To ensure we are adhering to best practice and ethical guidelines in the field of mental health, we take care to avoid dual relationships. A dual relationship occurs when a mental health care provider has a second, significantly different relationship with their client in addition to the traditional client-therapist relationship&#x2014;including, for example, a managerial relationship.</i></p> 
 <p><i> As such, Headspace requests that individuals who have received coaching or clinical services at Headspace wait until their care with Headspace is complete before applying for a position. If someone with a Headspace account is hired for a position, please note their account will be deactivated and they will not be able to use Headspace services for the duration of their employment.</i></p> 
 <p><i> Further, if Headspace cannot find a role that fails to resolve an ethical issue associated with a dual relationship, Headspace may need to take steps to ensure ethical obligations are being adhered to, including a delayed start date or a potential leave of absence. Such steps would be taken to protect both the former member, as well as any relevant individuals from their care team, from impairment, risk of exploitation, or harm.</i></p> 
 <p> For how how we will use the personal information you provide as part of the application process, please see: https://organizations.headspace.com/page/applicant-notice.</p> 
 <p> #LI-LL1</p>
</div>","https://boards.greenhouse.io/hs/jobs/5452319","f1db132bcf22f966",,,,"Remote","Staff Data Engineer","5 days ago","2023-10-20T11:50:04.697Z",,,"$131,414 - $190,100 a year","2023-10-25T11:50:04.698Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=f1db132bcf22f966&from=jasx&tk=1hdjan1i5jcbj800&vjs=3"
"OneSignal","OneSignal is a leading omnichannel customer engagement solution, powering personalized customer journeys across mobile and web push notifications, in-app messaging, SMS, and email. On a mission to democratize engagement, we enable over a million businesses to keep their users - including readers, fans, players and shoppers - engaged and up to date by delivering 12 billion messages daily. 
   1 in 5 new apps launches using OneSignal! We support companies in 140 countries, including Zynga, USA Today, Bitcoin.com, Eventbrite, Tribune, and many more - from startups and small businesses just getting off the ground to established companies communicating with millions of customers. 
   We’re venture-backed by SignalFire, Rakuten Ventures, Y Combinator, HubSpot, and BAM Elevate (read more about our recent Series C!). We offer remote work as the default option in the United States in California, New York, Pennsylvania, Texas, and Utah as well as in the UK and Singapore - with plans to expand the locations we support in the future. We also have offices in San Mateo, CA, New York City, and London, UK. Hiring in Singapore is done in partnership with a local PEO. 
   OneSignal has a lot of the great tech startup qualities you'd expect, but we don't stop there. Our massive scale and small team, emphasis on healthy life balance and kindness in all our interactions, and focus on ownership and personal growth make OneSignal a uniquely great place to work.
 
  Our blog contains more information about the OneSignal Engineering career ladder, and our diverse team. 
  About The Team: 
  Our User Data team empowers OneSignal customers with a Customer Data Platform that serves as a real-time system of record for user and audience data and provides timely and useful insights to our customers so that they can optimally understand and engage their users. 
  As a Senior Software Engineer, you'll have the autonomy to take ownership of significant projects and directly impact our platform's performance and features. Your expertise will shape the way businesses engage with their users. Working remotely, you'll have the flexibility to create a schedule that works best for you, allowing you to excel in both your professional and personal life. 
  What You'll Do: 
 
  Collaborate closely with Product Managers, Designers, and fellow engineers to design and implement new full-stack features and functionalities for our Customer Data Platform, using languages such as React/TypeScript, Ruby, Golang and Rust 
  Actively participate in peer code reviews and Technical Design Spec reviews, providing valuable technical insights to continuously improve our code base 
  Work together with the team to efficiently resolve production issues and ensure the system scales smoothly to meet the growing demands of our customers. 
  Conduct data analysis and performance monitoring to identify areas for optimization and enhancement 
  Stay up-to-date with the latest industry trends and technologies, incorporating new ideas into our engineering processes 
  Ability to work independently in uncertainty and drive multiple experiments to derive at a solution to unblock business and customer operations. 
  Work on customer driven product development 
 
 What You'll Bring: 
 
  6+ years of professional software development experience 
  Experience building backend frameworks at scale 
  Experience with Rust and/or Golang, or a strong willingness to learn these two languages quickly 
  Experience with distributed system event streaming framework such as Apache Kafka 
  Experience with Docker and Kubernetes 
  Experience designing RESTful API 
 
 We value a variety of experiences, and these are not required. It would be an added bonus if you have experience in any of the following: 
 
  Experience with ScyllaDB 
  Experience with Ruby/Rails 
  Experience building a robust React Web application 
  Experience with continuous build in an Agile Environment 
  Have a good understanding of clean software design principles 
  
 The New York and California base salary for this full time position is between $160,000 to $180,000. Your exact starting salary is determined by a number of factors such as your experience, skills, and qualifications. In addition to base salary, we also offer a competitive equity program and comprehensive and inclusive benefits.
 
   Qualities we look for: 
  
   Friendliness & Empathy 
   Accountability & Collaboration 
   Proactiveness & Urgency 
   Growth Mindset & Love of Learning 
   
  In keeping with our beliefs and goals, no employee or applicant will face discrimination/harassment based on: race, color, ancestry, national origin, religion, age, gender, marital domestic partner status, sexual orientation, gender identity, disability status, or veteran status. Above and beyond discrimination/harassment based on 'protected categories,' we also strive to prevent other, subtler forms of inappropriate behavior (e.g., stereotyping) from ever gaining a foothold in our office. Whether blatant or hidden, barriers to success have no place in our workplace. 
   Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and certain state or local laws. A reasonable accommodation is a change in the way things are normally done which will ensure an equal employment opportunity without imposing undue hardship on OneSignal. Please inform us if you need assistance completing any forms or to otherwise participate in the application and/or interview process. 
   OneSignal collects and processes personal data submitted by job applicants in accordance with our Privacy Policy - including GDPR and CCPA compliance.","<div>
 <div>
  <p>OneSignal is a leading omnichannel customer engagement solution, powering personalized customer journeys across mobile and web push notifications, in-app messaging, SMS, and email. On a mission to democratize engagement, we enable over a million businesses to keep their users - including readers, fans, players and shoppers - engaged and up to date by delivering 12 billion messages <i>daily.</i></p> 
  <p> 1 in 5 new apps launches using OneSignal! We support companies in 140 countries, including Zynga, USA Today, Bitcoin.com, Eventbrite, Tribune, and many more - from startups and small businesses just getting off the ground to established companies communicating with millions of customers.</p> 
  <p> We&#x2019;re venture-backed by SignalFire, Rakuten Ventures, Y Combinator, HubSpot, and BAM Elevate (read more about our recent Series C!). We offer remote work as the default option in the United States in California, New York, Pennsylvania, Texas, and Utah as well as in the UK and Singapore - with plans to expand the locations we support in the future. We also have offices in San Mateo, CA, New York City, and London, UK. Hiring in Singapore is done in partnership with a local PEO.</p> 
  <p> OneSignal has a lot of the great tech startup qualities you&apos;d expect, but we don&apos;t stop there. Our massive scale and small team, emphasis on healthy life balance and kindness in all our interactions, and focus on ownership and personal growth make OneSignal a uniquely great place to work.</p>
 </div>
 <p> Our blog contains more information about the OneSignal Engineering career ladder, and our diverse team.</p> 
 <h3 class=""jobSectionHeader""><b> About The Team:</b></h3> 
 <p> Our User Data team empowers OneSignal customers with a Customer Data Platform that serves as a real-time system of record for user and audience data and provides timely and useful insights to our customers so that they can optimally understand and engage their users.</p> 
 <p> As a Senior Software Engineer, you&apos;ll have the autonomy to take ownership of significant projects and directly impact our platform&apos;s performance and features. Your expertise will shape the way businesses engage with their users. Working remotely, you&apos;ll have the flexibility to create a schedule that works best for you, allowing you to excel in both your professional and personal life.</p> 
 <h3 class=""jobSectionHeader""><b> What You&apos;ll Do:</b></h3> 
 <ul>
  <li>Collaborate closely with Product Managers, Designers, and fellow engineers to design and implement new full-stack features and functionalities for our Customer Data Platform, using languages such as React/TypeScript, Ruby, Golang and Rust</li> 
  <li>Actively participate in peer code reviews and Technical Design Spec reviews, providing valuable technical insights to continuously improve our code base</li> 
  <li>Work together with the team to efficiently resolve production issues and ensure the system scales smoothly to meet the growing demands of our customers.</li> 
  <li>Conduct data analysis and performance monitoring to identify areas for optimization and enhancement</li> 
  <li>Stay up-to-date with the latest industry trends and technologies, incorporating new ideas into our engineering processes</li> 
  <li>Ability to work independently in uncertainty and drive multiple experiments to derive at a solution to unblock business and customer operations.</li> 
  <li>Work on customer driven product development</li> 
 </ul>
 <h3 class=""jobSectionHeader""><b>What You&apos;ll Bring:</b></h3> 
 <ul>
  <li>6+ years of professional software development experience</li> 
  <li>Experience building backend frameworks at scale</li> 
  <li>Experience with Rust and/or Golang, or a strong willingness to learn these two languages quickly</li> 
  <li>Experience with distributed system event streaming framework such as Apache Kafka</li> 
  <li>Experience with Docker and Kubernetes</li> 
  <li>Experience designing RESTful API</li> 
 </ul>
 <h3 class=""jobSectionHeader""><b>We value a variety of experiences, and these are not required. It would be an added bonus if you have experience in any of the following:</b></h3> 
 <ul>
  <li>Experience with ScyllaDB</li> 
  <li>Experience with Ruby/Rails</li> 
  <li>Experience building a robust React Web application</li> 
  <li>Experience with continuous build in an Agile Environment</li> 
  <li>Have a good understanding of clean software design principles</li> 
 </ul> 
 <p><i>The New York and California base salary for this full time position is between &#x24;160,000 to &#x24;180,000. Your exact starting salary is determined by a number of factors such as your experience, skills, and qualifications. In addition to base salary, we also offer a competitive equity program and comprehensive and inclusive benefits.</i></p>
 <div>
  <h3 class=""jobSectionHeader""><b> Qualities we look for:</b></h3> 
  <ul>
   <li>Friendliness &amp; Empathy</li> 
   <li>Accountability &amp; Collaboration</li> 
   <li>Proactiveness &amp; Urgency</li> 
   <li>Growth Mindset &amp; Love of Learning</li> 
  </ul> 
  <p><i>In keeping with our beliefs and goals, no employee or applicant will face discrimination/harassment based on: race, color, ancestry, national origin, religion, age, gender, marital domestic partner status, sexual orientation, gender identity, disability status, or veteran status. Above and beyond discrimination/harassment based on &apos;protected categories,&apos; we also strive to prevent other, subtler forms of inappropriate behavior (e.g., stereotyping) from ever gaining a foothold in our office. Whether blatant or hidden, barriers to success have no place in our workplace.</i></p> 
  <p><i> Applicants with disabilities may be entitled to reasonable accommodation under the terms of the Americans with Disabilities Act and certain state or local laws. A reasonable accommodation is a change in the way things are normally done which will ensure an equal employment opportunity without imposing undue hardship on OneSignal. Please inform us if you need assistance completing any forms or to otherwise participate in the application and/or interview process.</i></p> 
  <p><i> OneSignal collects and processes personal data submitted by job applicants in accordance with our</i><i> Privacy Policy</i><i> - including GDPR and CCPA compliance.</i></p>
 </div>
</div>","https://onesignal.com/careers/4294005006","011c8a21605190d4",,"Full-time",,"Remote","Senior Software Engineer, User Data Team","5 days ago","2023-10-20T11:50:04.994Z",,,,"2023-10-25T11:50:04.996Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=011c8a21605190d4&from=jasx&tk=1hdjan1i5jcbj800&vjs=3"
"Liberty Mutual","Pay Philosophy
  The typical starting salary range for this role is determined by a number of factors including skills, experience, education, certifications and location. The full salary range for this role reflects the competitive labor market value for all employees in these positions across the national market and provides an opportunity to progress as employees grow and develop within the role. Some roles at Liberty Mutual have a corresponding compensation plan which may include commission and/or bonus earnings at rates that vary based on multiple factors set forth in the compensation plan for the role.
  Description 
 
  Under direct supervision, responsible for the analysis, development, and execution of data solutions of low to moderate complexity that assists with the information lifecycle needs of an organization
   Assists with collecting, integrating, and analyzing organizational data with the purpose of drawing conclusions from that information
   Develops, constructs, tests, and maintains data architectures for data platform, database, analytical, reporting, or data science systems
   Recognizes opportunities to improve data reliability, quality, and efficiency and may make recommendations where appropriate 
  Designs and develops low complexity programs and tools to support ingestion, curation and provisioning of enterprise data to achieve analytics or reporting
   Builds and designs data models and data architecture that improve accessibility, efficiency, governance and quality of data
   Recognizes opportunities to improve data quality
   Assists with aspects of deployment of data solutions
   Helps identify possible process improvements that address technology gaps within a single business process of low to moderate complexity
   Analyzes and prepare low to moderately complex technology enabled recommendations to address gaps within a single business process
   Performs other projects and duties as assigned
   Telecommuting permitted up to 100%
 
  Qualifications
  The position requires a Bachelor’s degree, or foreign equivalent, in Electrical Engineering, or a related technical or business field plus two (2) years of experience in the job offered or a Associate Data Engineer-related occupation. Position also requires demonstrable experience with each of the following:
 
   New and emerging technologies including AWS SDK, and Docker/Kubernetes
   IT concepts, strategies and methodologies
   IT architectures and technical standards
   Business function and business operations
   Design and development tools
   Layered systems architectures and shared data engineering concepts
   Agile data engineering concepts and processes
   Applying customer requirements, including drawing out unforeseen implications and making recommendations for design, the ability to define design reasoning, understanding potential impacts of design requirements
   Telecommuting permitted up to 100%
 
  To apply, please visit https://jobs.libertymutualgroup.com/, select “Search Jobs,” enter job requisition #2023-61263 in the “Job ID or Keywords” field, and submit resume. Alternatively, you may apply by submitting a resume via e-mail to RecruitLM@LibertyMutual.com. Reference requisition number in subject of e-mail.
  About Us
  **This position may have in-office requirements depending on candidate location.**
 
  At Liberty Mutual, our purpose is to help people embrace today and confidently pursue tomorrow. That’s why we provide an environment focused on openness, inclusion, trust and respect. Here, you’ll discover our expansive range of roles, and a workplace where we aim to help turn your passion into a rewarding profession.
 
  Liberty Mutual has proudly been recognized as a “Great Place to Work” by Great Place to Work® US for the past several years. We were also selected as one of the “100 Best Places to Work in IT” on IDG’s Insider Pro and Computerworld’s 2020 list. For many years running, we have been named by Forbes as one of America’s Best Employers for Women and one of America’s Best Employers for New Graduates—as well as one of America’s Best Employers for Diversity. To learn more about our commitment to diversity and inclusion please visit: https://jobs.libertymutualgroup.com/diversity-equity-inclusion/
 
  We value your hard work, integrity and commitment to make things better, and we put people first by offering you benefits that support your life and well-being. To learn more about our benefit offerings please visit: https://LMI.co/Benefits
 
  Liberty Mutual is an equal opportunity employer. We will not tolerate discrimination on the basis of race, color, national origin, sex, sexual orientation, gender identity, religion, age, disability, veteran’s status, pregnancy, genetic information or on any basis prohibited by federal, state or local law.","<div>
 <b>Pay Philosophy</b>
 <p> The typical starting salary range for this role is determined by a number of factors including skills, experience, education, certifications and location. The full salary range for this role reflects the competitive labor market value for all employees in these positions across the national market and provides an opportunity to progress as employees grow and develop within the role. Some roles at Liberty Mutual have a corresponding compensation plan which may include commission and/or bonus earnings at rates that vary based on multiple factors set forth in the compensation plan for the role.</p>
 <b><br> Description </b>
 <ul>
  <li>Under direct supervision, responsible for the analysis, development, and execution of data solutions of low to moderate complexity that assists with the information lifecycle needs of an organization</li>
  <li> Assists with collecting, integrating, and analyzing organizational data with the purpose of drawing conclusions from that information</li>
  <li> Develops, constructs, tests, and maintains data architectures for data platform, database, analytical, reporting, or data science systems</li>
  <li> Recognizes opportunities to improve data reliability, quality, and efficiency and may make recommendations where appropriate </li>
  <li>Designs and develops low complexity programs and tools to support ingestion, curation and provisioning of enterprise data to achieve analytics or reporting</li>
  <li> Builds and designs data models and data architecture that improve accessibility, efficiency, governance and quality of data</li>
  <li> Recognizes opportunities to improve data quality</li>
  <li> Assists with aspects of deployment of data solutions</li>
  <li> Helps identify possible process improvements that address technology gaps within a single business process of low to moderate complexity</li>
  <li> Analyzes and prepare low to moderately complex technology enabled recommendations to address gaps within a single business process</li>
  <li> Performs other projects and duties as assigned</li>
  <li> Telecommuting permitted up to 100%</li>
 </ul>
 <b> Qualifications</b>
 <p> The position requires a Bachelor&#x2019;s degree, or foreign equivalent, in Electrical Engineering, or a related technical or business field plus two (2) years of experience in the job offered or a Associate Data Engineer-related occupation. Position also requires demonstrable experience with each of the following:</p>
 <ul>
  <li> New and emerging technologies including AWS SDK, and Docker/Kubernetes</li>
  <li> IT concepts, strategies and methodologies</li>
  <li> IT architectures and technical standards</li>
  <li> Business function and business operations</li>
  <li> Design and development tools</li>
  <li> Layered systems architectures and shared data engineering concepts</li>
  <li> Agile data engineering concepts and processes</li>
  <li> Applying customer requirements, including drawing out unforeseen implications and making recommendations for design, the ability to define design reasoning, understanding potential impacts of design requirements</li>
  <li> Telecommuting permitted up to 100%</li>
 </ul>
 <p> To apply, please visit https://jobs.libertymutualgroup.com/, select &#x201c;Search Jobs,&#x201d; enter job requisition #2023-61263 in the &#x201c;Job ID or Keywords&#x201d; field, and submit resume. Alternatively, you may apply by submitting a resume via e-mail to RecruitLM@LibertyMutual.com. Reference requisition number in subject of e-mail.</p>
 <b> About Us</b>
 <p> **This position may have in-office requirements depending on candidate location.**</p>
 <p></p>
 <p><br> At Liberty Mutual, our purpose is to help people embrace today and confidently pursue tomorrow. That&#x2019;s why we provide an environment focused on openness, inclusion, trust and respect. Here, you&#x2019;ll discover our expansive range of roles, and a workplace where we aim to help turn your passion into a rewarding profession.</p>
 <p></p>
 <p><br> Liberty Mutual has proudly been recognized as a &#x201c;Great Place to Work&#x201d; by Great Place to Work&#xae; US for the past several years. We were also selected as one of the &#x201c;100 Best Places to Work in IT&#x201d; on IDG&#x2019;s Insider Pro and Computerworld&#x2019;s 2020 list. For many years running, we have been named by Forbes as one of America&#x2019;s Best Employers for Women and one of America&#x2019;s Best Employers for New Graduates&#x2014;as well as one of America&#x2019;s Best Employers for Diversity. To learn more about our commitment to diversity and inclusion please visit: https://jobs.libertymutualgroup.com/diversity-equity-inclusion/</p>
 <p></p>
 <p><br> We value your hard work, integrity and commitment to make things better, and we put people first by offering you benefits that support your life and well-being. To learn more about our benefit offerings please visit: https://LMI.co/Benefits</p>
 <p></p>
 <p><br> Liberty Mutual is an equal opportunity employer. We will not tolerate discrimination on the basis of race, color, national origin, sex, sexual orientation, gender identity, religion, age, disability, veteran&#x2019;s status, pregnancy, genetic information or on any basis prohibited by federal, state or local law.</p>
</div>","https://app.eightfold.ai/careers/job/618494050518?domain=libertymutual.com&utm_source=indeed&microsite=libertymutual.com&mode=job&iis=Job+Board&iisn=Indeed+Organic","ae83164544567281",,"Full-time",,"Columbus, OH","Associate Data Engineer","5 days ago","2023-10-20T11:50:08.456Z","3.5","5358","$79,602 - $114,700 a year","2023-10-25T11:50:08.459Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=ae83164544567281&from=jasx&tk=1hdjan1i5jcbj800&vjs=3"
"Aflac, Incorporated","Salary Range: $55,000 - $140,000
 
  We’ve Got You Under Our Wing
  We are the duck. We develop and empower our people, cultivate relationships, give back to our community, and celebrate every success along the way. We do it all…The Aflac Way.
 
  Aflac, a Fortune 500 company, is an industry leader in voluntary insurance products that pay cash directly to policyholders and one of America's best-known brands. Aflac has been recognized as Fortune’s 50 Best Workplaces for Diversity and as one of World’s Most Ethical Companies by Ethisphere.com.
  
 Our business is about being there for people in need. So, ask yourself, are you the duck? If so, there’s a home, and a flourishing career for you at Aflac.
 
  Work Designation. Depending on your location within the continental US, this role may be hybrid or remote. 
 
  If you live within 50 miles of the Aflac offices located in Columbus, GA or Columbia, SC, this role will be hybrid. This means you will be expected to work in the office for at least 60% of the work week. You will work from your home (within the continental US) for the remaining portion of the work week. Details of this schedule will be discussed with your leadership.
   If you live more than 50 miles from the Aflac offices located in Columbus, GA or Columbia, SC, this role will be remote. This means you will be expected to work from your home, within the continental US. If the role is remote, there may be occasions that you are requested to come to the office based on business need. Any requests to come to the office would be communicated with you in advance.
 
 
  What does it take to be successful at Aflac?
 
   Acting with Integrity
   Communicating Effectively
   Pursuing Self-Development
   Serving Customers
   Supporting Change
   Supporting Organizational Goals
   Working with Diverse Populations
 
 
  What does it take to be successful in this role?
 
  AWS Data Platform - Cloud infrastructure, Datalake/Cloud Formation, Automation, CI/CD 
  Amazon Cloud Data Storage – S3, RedShift, DynamoDB, NoSQL 
  ETL Tools – AWS Glue, Informatica Suite, SSIS, Infoworks 
  SQL & Relational Databases – SQL Server, Teradata, MS Access, HIVE, HBase 
  XML 
  XSLT 
  .NET Framework 
  C# 
  Java 
  JavaScript 
  jQuery 
  LINQ 
  MVC Framework 
  ASPX 
  Angular.js 
  Bootstrap 
  Knockout 
  Business Intelligence 
  ETL Techniques 
  Data Modeling 
  Data Warehousing/Business Intelligence 
  Meta Data Repository 
  MS SQL Server
 
 
  Education & Experience Required
 
   Bachelor's Degree In Programming/systems or computer science, or related field
   Four or more years of programming experience
   Experience and understanding of multiple programming languages and applicable applications including SQL and ETL
   Experienced in Cloud data storage and consumption models such as S3 Buckets, Lake Formation, RedShift, Dynamo DB
   Experienced in working with compute engines such as Spark, EMR, Data bricks, Snowflake etc.
 
  Or an equivalent combination of education and experience
 
  Principal Duties & Responsibilities
 
  Works under minimum supervisor to exercise independent decision making; Creates processes which initiate the ETL or Batch cycle; develops streaming processes for extracted data loading to destination database, including on-the-fly processing where extract and transformation phase to no go to persistent storage; Performs data profiling of source data in order to identify data quality issues and anomalies, business knowledge embedded in data; natural keys, and meta data information
 
  
 
  Build repeatable, automated and sustainable Extract, Transform and Load (ETL) processes leveraging platforms such as AWS cloud native – AWS Glue, DMS, Informatica, Infoworks, Hadoop, Spark processing Engines
 
  
 
  Creates data validation rule on source data to confirm the data has correct and/or expected values; Writes alternate workflow steps or reports back to the source for further analysis and correction of incorrect record(s) when validation rules are not passed
 
  
 
  Develops processes to be applied to extracted source data to move to target state; Writes data cleansing functions to get data to proper prunes data set to include only fields needed; translates source code values to target value; Standardizes free form values to codes; Derives new values through calculations on existing fields; Merges data from multiple in order to generate on consolidated source for the target
 
  
 
  Sorts and Aggregates records into rollup where multiple records are represented; Creates surrogate-key values to use in place of multiple natural keys; Turns multiple columns into multiple rows or vice–versa (Transposing or Pivoting); Splits multi-valued column data into multiple columns; Disaggregates repeating columns into separate detail table(s); Creates lookup tables; Looks up and validates reference information as part of data validation
 
  
 
  Creates and applies data validation step process in order to perform partial, full or no record’s rejection; Writes processes which handle exceptions and/or move records exceptions to alternate Transform step(s)
 
  
 
  Develops processes which load the transformed data into end target systems (database, file, application, etc.); may apply different techniques based on business needs including inserting new data into target; Over write existing data with cumulative information; Updates existing data at some frequency; Creates data validation steps in this layer to ensure loaded data
 
  
 
  Creates process cleanup after complex ETL processes which release resources used to run ETL; Creates processes to archive data
 
  
 
  Participates in project collaboration meeting with clients, business analysts, and team members in order to analyze and clarify business requirements; Translates business requirements into detailed technical specifications
 
  
 
  Works with project teams to define and design scope for each project; Creates unit test cases to ensure the application meets the needs of the business
 
  
 
  Ensures proper configuration management and change controls are implemented; Provides technical assistance and cross training to other team members
 
  
 
  Designs and implements technology best practices, guidelines and repeatable processes; Prepares and presents status updates on various projects
 
  
 
  Performs other duties as required
 
 
  Total Rewards
  This compensation range is specific to the job level and takes into account the wide range of factors that are considered in making compensation decisions including, but not limited to: education, experience, licensure, certifications, geographic location, and internal equity. The range has been created in good faith based on information known to Aflac at the time of the posting. Compensation decisions are dependent on the circumstances of each case. This salary range does not include any potential incentive pay or benefits, however, such information will be provided separately when appropriate. The salary range for this position is $55,000 to $140,000.
 
  In addition to the base salary, we offer an array of benefits to meet your needs including medical, dental, and vision coverage, prescription drug coverage, health care flexible spending, dependent care flexible spending, Aflac supplemental policies (Accident, Cancer, Critical Illness and Hospital Indemnity offered at no costs to employee), 401(k) plans, annual bonuses, and an opportunity to purchase company stock. On an annual basis, you’ll also be offered 11 paid holidays, up to 20 days PTO to be used for any reason, and, if eligible, state mandated sick leave (Washington employees accrue 1 hour sick leave for every 40 hours worked) and other leaves of absence, if eligible, when needed to support your physical, financial, and emotional well-being. Aflac complies with all applicable leave laws, including, but not limited to sick and safe leave, and adoption and parental leave, in all states and localities.","<div>
 <p><b>Salary Range: </b>&#x24;55,000 - &#x24;140,000</p>
 <p></p>
 <p><b><br> We&#x2019;ve Got You Under Our Wing</b></p>
 <p> We are the duck. We develop and empower our people, cultivate relationships, give back to our community, and celebrate every success along the way. We do it all&#x2026;<i>The Aflac Way</i>.</p>
 <p></p>
 <p><br> Aflac, a Fortune 500 company, is an industry leader in voluntary insurance products that pay cash directly to policyholders and one of America&apos;s best-known brands. Aflac has been recognized as Fortune&#x2019;s 50 Best Workplaces for Diversity and as one of World&#x2019;s Most Ethical Companies by Ethisphere.com.</p>
 <p><br> </p>
 <p>Our business is about being there for people in need. So, ask yourself, are you the duck? If so, there&#x2019;s a home, and a flourishing career for you at Aflac.</p>
 <p></p>
 <p><b><br> Work Designation.</b> Depending on your location within the continental US, this role may be <b>hybrid</b> or <b>remote. </b></p>
 <ul>
  <li>If you live <i>within 50 miles</i> of the Aflac offices located in Columbus, GA or Columbia, SC, this role will be <b>hybrid.</b><b> </b>This means you will be expected to work in the office for at least 60% of the work week. You will work from your home (within the continental US) for the remaining portion of the work week. Details of this schedule will be discussed with your leadership.</li>
  <li> If you live <i>more than 50 miles</i> from the Aflac offices located in Columbus, GA or Columbia, SC, this role will be <b>remote.</b> This means you will be expected to work from your home, within the continental US. If the role is remote, there may be occasions that you are requested to come to the office based on business need. Any requests to come to the office would be communicated with you in advance.</li>
 </ul>
 <p></p>
 <p><b><br> What does it take to be successful at Aflac?</b></p>
 <ul>
  <li> Acting with Integrity</li>
  <li> Communicating Effectively</li>
  <li> Pursuing Self-Development</li>
  <li> Serving Customers</li>
  <li> Supporting Change</li>
  <li> Supporting Organizational Goals</li>
  <li> Working with Diverse Populations</li>
 </ul>
 <p></p>
 <p><b><br> What does it take to be successful in this role?</b></p>
 <ul>
  <li>AWS Data Platform - Cloud infrastructure, Datalake/Cloud Formation, Automation, CI/CD </li>
  <li>Amazon Cloud Data Storage &#x2013; S3, RedShift, DynamoDB, NoSQL </li>
  <li>ETL Tools &#x2013; AWS Glue, Informatica Suite, SSIS, Infoworks </li>
  <li>SQL &amp; Relational Databases &#x2013; SQL Server, Teradata, MS Access, HIVE, HBase </li>
  <li>XML </li>
  <li>XSLT </li>
  <li>.NET Framework </li>
  <li>C# </li>
  <li>Java </li>
  <li>JavaScript </li>
  <li>jQuery </li>
  <li>LINQ </li>
  <li>MVC Framework </li>
  <li>ASPX </li>
  <li>Angular.js </li>
  <li>Bootstrap </li>
  <li>Knockout </li>
  <li>Business Intelligence </li>
  <li>ETL Techniques </li>
  <li>Data Modeling </li>
  <li>Data Warehousing/Business Intelligence </li>
  <li>Meta Data Repository </li>
  <li>MS SQL Server</li>
 </ul>
 <p></p>
 <p><b><br> Education &amp; Experience Required</b></p>
 <ul>
  <li> Bachelor&apos;s Degree In Programming/systems or computer science, or related field</li>
  <li> Four or more years of programming experience</li>
  <li> Experience and understanding of multiple programming languages and applicable applications including SQL and ETL</li>
  <li> Experienced in Cloud data storage and consumption models such as S3 Buckets, Lake Formation, RedShift, Dynamo DB</li>
  <li> Experienced in working with compute engines such as Spark, EMR, Data bricks, Snowflake etc.</li>
 </ul>
 <p><i> Or an equivalent combination of education and experience</i></p>
 <p></p>
 <p><b><br> Principal Duties &amp; Responsibilities</b></p>
 <ul>
  <li>Works under minimum supervisor to exercise independent decision making; Creates processes which initiate the ETL or Batch cycle; develops streaming processes for extracted data loading to destination database, including on-the-fly processing where extract and transformation phase to no go to persistent storage; Performs data profiling of source data in order to identify data quality issues and anomalies, business knowledge embedded in data; natural keys, and meta data information</li>
 </ul>
 <p><br> </p>
 <ul>
  <li>Build repeatable, automated and sustainable Extract, Transform and Load (ETL) processes leveraging platforms such as AWS cloud native &#x2013; AWS Glue, DMS, Informatica, Infoworks, Hadoop, Spark processing Engines</li>
 </ul>
 <p><br> </p>
 <ul>
  <li>Creates data validation rule on source data to confirm the data has correct and/or expected values; Writes alternate workflow steps or reports back to the source for further analysis and correction of incorrect record(s) when validation rules are not passed</li>
 </ul>
 <p><br> </p>
 <ul>
  <li>Develops processes to be applied to extracted source data to move to target state; Writes data cleansing functions to get data to proper prunes data set to include only fields needed; translates source code values to target value; Standardizes free form values to codes; Derives new values through calculations on existing fields; Merges data from multiple in order to generate on consolidated source for the target</li>
 </ul>
 <p><br> </p>
 <ul>
  <li>Sorts and Aggregates records into rollup where multiple records are represented; Creates surrogate-key values to use in place of multiple natural keys; Turns multiple columns into multiple rows or vice&#x2013;versa (Transposing or Pivoting); Splits multi-valued column data into multiple columns; Disaggregates repeating columns into separate detail table(s); Creates lookup tables; Looks up and validates reference information as part of data validation</li>
 </ul>
 <p><br> </p>
 <ul>
  <li>Creates and applies data validation step process in order to perform partial, full or no record&#x2019;s rejection; Writes processes which handle exceptions and/or move records exceptions to alternate Transform step(s)</li>
 </ul>
 <p><br> </p>
 <ul>
  <li>Develops processes which load the transformed data into end target systems (database, file, application, etc.); may apply different techniques based on business needs including inserting new data into target; Over write existing data with cumulative information; Updates existing data at some frequency; Creates data validation steps in this layer to ensure loaded data</li>
 </ul>
 <p><br> </p>
 <ul>
  <li>Creates process cleanup after complex ETL processes which release resources used to run ETL; Creates processes to archive data</li>
 </ul>
 <p><br> </p>
 <ul>
  <li>Participates in project collaboration meeting with clients, business analysts, and team members in order to analyze and clarify business requirements; Translates business requirements into detailed technical specifications</li>
 </ul>
 <p><br> </p>
 <ul>
  <li>Works with project teams to define and design scope for each project; Creates unit test cases to ensure the application meets the needs of the business</li>
 </ul>
 <p><br> </p>
 <ul>
  <li>Ensures proper configuration management and change controls are implemented; Provides technical assistance and cross training to other team members</li>
 </ul>
 <p><br> </p>
 <ul>
  <li>Designs and implements technology best practices, guidelines and repeatable processes; Prepares and presents status updates on various projects</li>
 </ul>
 <p><br> </p>
 <ul>
  <li>Performs other duties as required</li>
 </ul>
 <p></p>
 <p><b><br> Total Rewards</b></p>
 <p><i> This compensation range is specific to the job level and takes into account the wide range of factors that are considered in making compensation decisions including, but not limited to: education, experience, licensure, certifications, geographic location, and internal equity. The range has been created in good faith based on information known to Aflac at the time of the posting. Compensation decisions are dependent on the circumstances of each case. This salary range does not include any potential incentive pay or benefits, however, such information will be provided separately when appropriate. The salary range for this position is &#x24;55,000 to &#x24;140,000.</i></p>
 <p></p>
 <p><i><br> In addition to the base salary, we offer an array of benefits to meet your needs including medical, dental, and vision coverage, prescription drug coverage, health care flexible spending, dependent care flexible spending, Aflac supplemental policies (Accident, Cancer, Critical Illness and Hospital Indemnity offered at no costs to employee), 401(k) plans, annual bonuses, and an opportunity to purchase company stock. On an annual basis, you&#x2019;ll also be offered 11 paid holidays, up to 20 days PTO to be used for any reason, and, if eligible, state mandated sick leave (Washington employees accrue 1 hour sick leave for every 40 hours worked) and other leaves of absence, if eligible, when needed to support your physical, financial, and emotional well-being. Aflac complies with all applicable leave laws, including, but not limited to sick and safe leave, and adoption and parental leave, in all states and localities.</i></p>
</div>
<p></p>","https://careers.aflac.com/job/Remote-Data-Engineer-%28AWSETL%29-OR-31999/1089155300/?feedId=342200&utm_source=Indeed&utm_campaign=Aflac_Indeed","6b0246ca2a37effe",,,,"Remote","Data Engineer (AWS/ETL)","5 days ago","2023-10-20T11:50:08.920Z","3.5","4235","$55,000 - $140,000 a year","2023-10-25T11:50:08.921Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=6b0246ca2a37effe&from=jasx&tk=1hdjan1i5jcbj800&vjs=3"
"TSR Consulting Services (Confidential Banking Client)","Our client, an leading software firm, has an opening for an Azure Data Engineer on a W2 Contract to Hire basis.
Work Locations: Can work remote as long as local to the following states: Florida, Georgia, Virginia, Texas, Tennessee, Colorado, North Carolina
Summary of Duties & Responsibilities:
This is a greenfield opportunity to develop an AI and Analytics application in the ERP and Work Management space. The Azure Data Engineer is responsible for designing and implementing Big Data and BI solutions using the Microsoft Azure platform. This role is 80% hands-on development and 20% architecting Big Data and BI solutions.The candidate is expected to evangelize and educate others on engineering design and development standards. Azure Data Architect is expected to function as a productive member of a team, working and communicating proactively with engineering peers, technical leads, project managers, product owners, and resource managers.The ideal candidate will have10 years of hands-on experience in designing, implementing, and delivering into production, large-scale near real-time data warehouses.
Essential Functions:

 Design and develop data warehouses and data pipelines within Azure
 Design and develop Azure ML model execution pipelines
 Evangelize engineering design and development standards
 Act as a key contributor to the design and development lifecycle of analytic applications utilizing Microsoft Azure and BI technology platforms
 Participate in Agile ceremonies including daily stand-ups, sprint planning, retrospectives, and product demonstrations
 Produce efficient and elegant code that meets business requirements
 Author unit tests that adhere to code coverage guidelines
 Proactively communicate progress, issues, and risks to stakeholders
 Accurately estimate assignments
 Create and maintain technical documentation
 Mentor less experienced engineers
 Contribute to the growth and maturity of the Software Engineering Group
 Performs other related duties as directed

Required Skills & Competencies

 10 years of hands-on experience designing and implementing large-scale distributed data architecture for BI and OLTP systems
 10 years of hands-on experience designing and implementing large-scale data pipelines
 5 years of hands-on experience in Azure data services
 5 years of hands-on experience with data integration using ETL / ELT tools
 5 years of hands-on experience with Python including object-oriented programming and unit testing
 Advanced experience with one or more Python parallel processing libraries
 Advanced experience with one or more Python data analysis libraries
 Advanced data modeling experience
 Broad experience in Microsoft SQL technologies
 Broad multi-tenant data architecture and implementation experience across different data stores, messaging systems and data processing engines
 Experience with data integration through APIs, Web Services, SOAP, and/or REST services
 Experience using Azure DevOps and CI/CD as well as Agile tools and processes including Git, Jenkins, Jira, and Confluence
 Knowledge of SOA and Micros Services Application Architecture
 Ability to work in a fast-paced, collaborative team environment
 Excellent written and verbal communication skills and ability to express ideas clearly and concisely

Desired Attributes

 Strong team player with ability to collaborate with all levels of the organization
 Possess a drive towards forward progress and delivering results while taking responsibility
 Multi-tasker with ability to set and manage priorities
 Proactively and transparently communicate challenges and successes to product leadership
 Exhibit attention to detail
 Flexibility and willingness to help in other areas as priorities shift
 Ability to effectively operate with minimal supervision, but knowing how to keep leadership in the loop

Education or Prior Work Experience:

 Bachelor's degree in CS or related field, master’s degree preferred
 10+ years of experience with designing and developing complex data architecture solutions
 5+ years of design and development experience with Microsoft Azure data architecture and related solutions
 5 years of hands-on experience with Python

Job Type: Contract
Pay: $80.00 - $90.00 per hour
Experience level:

 10 years

Work Location: Remote","<p>Our client, an leading software firm, has an opening for an <b>Azure Data Engineer</b> on a <b>W2 Contract to Hire</b> basis.</p>
<p><b>Work Locations: </b>Can work remote as long as local to the following states: Florida, Georgia, Virginia, Texas, Tennessee, Colorado, North Carolina</p>
<p><b>Summary of Duties &amp; Responsibilities:</b></p>
<p>This is a greenfield opportunity to develop an AI and Analytics application in the ERP and Work Management space. The Azure Data Engineer is responsible for designing and implementing Big Data and BI solutions using the Microsoft Azure platform. This role is 80% hands-on development and 20% architecting Big Data and BI solutions.<br>The candidate is expected to evangelize and educate others on engineering design and development standards. Azure Data Architect is expected to function as a productive member of a team, working and communicating proactively with engineering peers, technical leads, project managers, product owners, and resource managers.<br>The ideal candidate will have10 years of hands-on experience in designing, implementing, and delivering into production, large-scale near real-time data warehouses.</p>
<p><b>Essential Functions:</b></p>
<ul>
 <li>Design and develop data warehouses and data pipelines within Azure</li>
 <li>Design and develop Azure ML model execution pipelines</li>
 <li>Evangelize engineering design and development standards</li>
 <li>Act as a key contributor to the design and development lifecycle of analytic applications utilizing Microsoft Azure and BI technology platforms</li>
 <li>Participate in Agile ceremonies including daily stand-ups, sprint planning, retrospectives, and product demonstrations</li>
 <li>Produce efficient and elegant code that meets business requirements</li>
 <li>Author unit tests that adhere to code coverage guidelines</li>
 <li>Proactively communicate progress, issues, and risks to stakeholders</li>
 <li>Accurately estimate assignments</li>
 <li>Create and maintain technical documentation</li>
 <li>Mentor less experienced engineers</li>
 <li>Contribute to the growth and maturity of the Software Engineering Group</li>
 <li>Performs other related duties as directed</li>
</ul>
<p><b>Required Skills &amp; Competencies</b></p>
<ul>
 <li>10 years of hands-on experience designing and implementing large-scale distributed data architecture for BI and OLTP systems</li>
 <li>10 years of hands-on experience designing and implementing large-scale data pipelines</li>
 <li>5 years of hands-on experience in Azure data services</li>
 <li>5 years of hands-on experience with data integration using ETL / ELT tools</li>
 <li>5 years of hands-on experience with Python including object-oriented programming and unit testing</li>
 <li>Advanced experience with one or more Python parallel processing libraries</li>
 <li>Advanced experience with one or more Python data analysis libraries</li>
 <li>Advanced data modeling experience</li>
 <li>Broad experience in Microsoft SQL technologies</li>
 <li>Broad multi-tenant data architecture and implementation experience across different data stores, messaging systems and data processing engines</li>
 <li>Experience with data integration through APIs, Web Services, SOAP, and/or REST services</li>
 <li>Experience using Azure DevOps and CI/CD as well as Agile tools and processes including Git, Jenkins, Jira, and Confluence</li>
 <li>Knowledge of SOA and Micros Services Application Architecture</li>
 <li>Ability to work in a fast-paced, collaborative team environment</li>
 <li>Excellent written and verbal communication skills and ability to express ideas clearly and concisely</li>
</ul>
<p><b>Desired Attributes</b></p>
<ul>
 <li>Strong team player with ability to collaborate with all levels of the organization</li>
 <li>Possess a drive towards forward progress and delivering results while taking responsibility</li>
 <li>Multi-tasker with ability to set and manage priorities</li>
 <li>Proactively and transparently communicate challenges and successes to product leadership</li>
 <li>Exhibit attention to detail</li>
 <li>Flexibility and willingness to help in other areas as priorities shift</li>
 <li>Ability to effectively operate with minimal supervision, but knowing how to keep leadership in the loop</li>
</ul>
<p><b>Education or Prior Work Experience:</b></p>
<ul>
 <li>Bachelor&apos;s degree in CS or related field, master&#x2019;s degree preferred</li>
 <li>10+ years of experience with designing and developing complex data architecture solutions</li>
 <li>5+ years of design and development experience with Microsoft Azure data architecture and related solutions</li>
 <li>5 years of hands-on experience with Python</li>
</ul>
<p>Job Type: Contract</p>
<p>Pay: &#x24;80.00 - &#x24;90.00 per hour</p>
<p>Experience level:</p>
<ul>
 <li>10 years</li>
</ul>
<p>Work Location: Remote</p>",,"276dd72d5e5ffadd",,"Contract",,"Remote","Azure Data Engineer","4 days ago","2023-10-21T11:50:18.009Z","3.8","5","$80 - $90 an hour","2023-10-25T11:50:18.011Z","US","remote","data engineer","https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0CRsdBggKFjsd2I0WXN7jbirCZuQf4qBYKBRMDPXZZqyp2f8VIHX3n1cIS2cg4x8GGfMdAhLRXL3eXEOUYUqNGaa2M7GrsgWT4jNmftuYccL-veEfnXzrWwXbRfSmlpU1GLyuKtxQ5nQDZXYz6mEBEGjf8NTGgnGKAuX0A_oC0pAG_g4euwYOxgo_8syo_vWe4uKNWqCj4fluClnihoNgGauDOBICz0fSwUMnDsoe-b108Jsw2o5UaeFbOM3S_A9YuvDyFI0bFb_5tscOezgysVMqp_L0PAO0k8Ep8xSAgJYZ_vE_D4eOH2u1xCyzVkSttCX2WFwCMCQPfyCLDuXmVhkDAqGCiF0uOzh4YJxeAOVbuydBEv2LBgXLqAFcmdgYaq_kACLs9TksOP0ZAy-r5GBmuRYs2NFek0M9u9dPPbuVeHxDBEASSews-FPuL8Ifca8PhnCHKQnF6JL2Rf1Jqt0iqdh9QbTnUmDSurZaXE1_I_NtJ7xzJX_Iol3-P0IQ4T-WdstFH-OXH3JAtQSFMdAoKyIVdHQIha18qvZQbehxT_eMgE_kPQJEuv_yLWB6V9hC3BPOsWleXxKexoZduaKd2dZsBMkpYxTA6SyPS0-A%3D%3D&xkcb=SoBb-_M3JzdDyK2C2x0BbzkdCdPP&p=10&fvj=1&vjs=3&jsa=6857&tk=1hdjaj0h4j4gl800&from=jasx&wvign=1"
"USA Health","Overview:
 USA Health is Transforming Medicine along the Gulf Coast to care for the unique needs of our community. 

 USA Health is changing how medical care, education and research impact the health of people who live in Mobile and the surrounding area. Our team of doctors, advanced care providers, nurses, therapists and researchers provide the region's most advanced medicine at multiple facilities, campuses, clinics and classrooms. We offer patients convenient access to innovative treatments and advancements that improve the health and overall wellbeing of our community. 

 
Responsibilities:
 The Cerner Data Engineer will build and maintain data systems and construct datasets that are used to analyze and support USA Health Data and Analytics; implement methods to improve data reliability, quality, and efficiency - ensuring that data is collected accurately and consistently, y using standardized data practices; identify and correct any errors or inconsistencies in the data and ensure that the data is complete, accurate, and relevant to the intended use; conducts regular audits of data collection procedures; participate in data governance process by helping to establish policies and procedures for managing data, including data ownership, access, and security ensuring that data is used in a consistent and ethical manner; analyzing and organize raw data, removing any errors or inconsistencies, applying data standardization; combine data from multiple sources to provide a more complete picture of Health System practices; helping end users to identify trends and practices and improving the overall accuracy of the data; develop and maintain datasets by identifying the data that is needed, ensuring that it is cleaned and organized, and establishing a process for ensuring that the data will be refreshed periodically and monitored for accuracy; build data systems and pipelines by creating and managing the infrastructure that is used to collect, store, process, and analyze data and then establishing the processes that moves data from one place to another; build algorithms and prototypes by working with end users to help determine the problem that needs to be solved and for complex issues breaking it down into smaller, more manageable problems, and addressing those problems; then developing the appropriate algorithms or prototypes using programming languages, software development frameworks, and machine learning libraries as is applicable; develop and test architectures that enable data extraction and transformation to prepare data for prescriptive and predictive modeling; identify all of the data sources that will be used for the modeling project; this could include data from internal systems, external databases, or third-party APIs; ensure the data is cleaned and remove any errors or inconsistencies, and formatting the data in a way that is compatible with the modeling tools that will be used; develop an extraction and transformation (ETL) pipeline; develop and test the models and then deploy; develop analytical tools and programs by working with ream members and end users to define the problem, determine or design the appropriate solution as needed and then build the solution using a coding solution to a data visualization tool or machine learning library, then test and deploy the solution; collaborate with end users, team members, and vendors on assigned projects; works the assigned schedule as defined and overtime as required; completes all mandatory education requirements; attends meetings as required including administrative, committee, and staff meetings; cooperates with department staff to achieve objectives and maintain good interdepartmental relationships; abides by and enforces all compliance requirements; performs these responsibilities in an ethical manner consistent with the organization's values; adheres to USA Health policies including confidentiality; accepts and completes all duties positively and without conflict; cooperates, helps others, and improves the performance of the department; completes all mandatory department education and USA Health requirements; regular and prompt attendance; ability to work schedule as defined and overtime as required; related duties as required. 

 
Qualifications:
 The Bachelor’s degree from an accredited institution as approved and accepted by the University of South Alabama and four years of experience in IT HER programs, clinical applications and/or coding/programming with two years of experience specific to Cerner Plus.","<b>Overview:</b>
<br> USA Health is Transforming Medicine along the Gulf Coast to care for the unique needs of our community. 
<br>
<br> USA Health is changing how medical care, education and research impact the health of people who live in Mobile and the surrounding area. Our team of doctors, advanced care providers, nurses, therapists and researchers provide the region&apos;s most advanced medicine at multiple facilities, campuses, clinics and classrooms. We offer patients convenient access to innovative treatments and advancements that improve the health and overall wellbeing of our community. 
<br>
<br> 
<b>Responsibilities:</b>
<br> The Cerner Data Engineer will build and maintain data systems and construct datasets that are used to analyze and support USA Health Data and Analytics; implement methods to improve data reliability, quality, and efficiency - ensuring that data is collected accurately and consistently, y using standardized data practices; identify and correct any errors or inconsistencies in the data and ensure that the data is complete, accurate, and relevant to the intended use; conducts regular audits of data collection procedures; participate in data governance process by helping to establish policies and procedures for managing data, including data ownership, access, and security ensuring that data is used in a consistent and ethical manner; analyzing and organize raw data, removing any errors or inconsistencies, applying data standardization; combine data from multiple sources to provide a more complete picture of Health System practices; helping end users to identify trends and practices and improving the overall accuracy of the data; develop and maintain datasets by identifying the data that is needed, ensuring that it is cleaned and organized, and establishing a process for ensuring that the data will be refreshed periodically and monitored for accuracy; build data systems and pipelines by creating and managing the infrastructure that is used to collect, store, process, and analyze data and then establishing the processes that moves data from one place to another; build algorithms and prototypes by working with end users to help determine the problem that needs to be solved and for complex issues breaking it down into smaller, more manageable problems, and addressing those problems; then developing the appropriate algorithms or prototypes using programming languages, software development frameworks, and machine learning libraries as is applicable; develop and test architectures that enable data extraction and transformation to prepare data for prescriptive and predictive modeling; identify all of the data sources that will be used for the modeling project; this could include data from internal systems, external databases, or third-party APIs; ensure the data is cleaned and remove any errors or inconsistencies, and formatting the data in a way that is compatible with the modeling tools that will be used; develop an extraction and transformation (ETL) pipeline; develop and test the models and then deploy; develop analytical tools and programs by working with ream members and end users to define the problem, determine or design the appropriate solution as needed and then build the solution using a coding solution to a data visualization tool or machine learning library, then test and deploy the solution; collaborate with end users, team members, and vendors on assigned projects; works the assigned schedule as defined and overtime as required; completes all mandatory education requirements; attends meetings as required including administrative, committee, and staff meetings; cooperates with department staff to achieve objectives and maintain good interdepartmental relationships; abides by and enforces all compliance requirements; performs these responsibilities in an ethical manner consistent with the organization&apos;s values; adheres to USA Health policies including confidentiality; accepts and completes all duties positively and without conflict; cooperates, helps others, and improves the performance of the department; completes all mandatory department education and USA Health requirements; regular and prompt attendance; ability to work schedule as defined and overtime as required; related duties as required. 
<br>
<br> 
<b>Qualifications:</b>
<br> The Bachelor&#x2019;s degree from an accredited institution as approved and accepted by the University of South Alabama and four years of experience in IT HER programs, clinical applications and/or coding/programming with two years of experience specific to Cerner Plus.","https://careers.usahealthsystem.com/jobs/9524/job?utm_source=indeed_integration&iis=Job%20Board&iisn=Indeed&indeed-apply-token=73a2d2b2a8d6d5c0a62696875eaebd669103652d3f0c2cd5445d3e66b1592b0f","7dd9208ec61a79ed",,"Full-time",,"650 Clinic Dr, Mobile, AL 36688","Cerner Data Engineer","6 days ago","2023-10-19T11:50:14.723Z","3.5","16",,"2023-10-25T11:50:14.737Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=7dd9208ec61a79ed&from=jasx&tk=1hdjan9ea2cc7000&vjs=3"
"b.well Connected Health","Lead Data Engineer - Data Interoperability Team
   
   As a Lead Data Engineer, you will be a critical member of the Data Interoperability team responsible for building and maintaining data pipelines and data infrastructure that connects to thousands of data sources around the country to bring together a person’s health record in one place.
   
   b.well Connected Health has the largest set of connected health data for any person in the United States. By bringing a person’s health data into one place, we are able to help everyone get convenient and affordable health care.
   
   
  This position is available for fully remote work.
   
   
  What You'll Do:
  
 
  Design, build, and maintain b.well’s data pipeline infrastructure using Python, Spark, Prefect, Kubernetes and other modern technologies 
  Lead a team of data engineers to build data pipelines and infrastructure that connects to thousands of data sources around the country including health providers, insurance companies, pharmacies and labs. 
  Launch new projects from ideation to completion 
  Help lead other developers to improve their career development and coding abilities 
  You will safeguard sensitive data by following policies and training concerning your security and privacy responsibilities 
 
 
  Job Requirements:
  
 
  7+ years of professional programming experience (must include Python) 
  2+ years building microservices in Python 
  Exceptional and demonstrable data engineering experience 
  Experience in loading, validating, cleaning, and manipulating data files 
  Strong experience with unit testing and test-driven development 
  Strong experience with relational and/or NoSQL databases 
  Strong experience with cloud-based infrastructure 
  Comfort with Linux/Unix command line 
 
 
  Great to Have:
  
 
  7+ years of Advance Python experience 
  5+ years of data pipeline engineering experience 
  1+ years of experience with Spark 
  Experience with Airflow or Prefect 
  Experience with Docker 
  Experience with streaming data 
  Experience scaling technology solutions to hundreds of thousands active users 
  Experience mentoring other developers 
  Deep understanding of common API methodologies 
  Startup experience 
 
 
  Blow Us Away:
  
 
  Experience working with third-party healthcare APIs, HL7, data streams, and/or flat files 
  Experience in cybersecurity 
  Experience with HIPAA, HITECH, and HITRUST 
  An active GitHub profile or other public code portfolio 
  Active Stack Overflow profile 
  Documented work on open source projects 
  
 
  
   Data shows that women, people of color, and other underrepresented groups may be less likely to apply for jobs unless they believe they are a perfect match. But b.well holds diversity amongst its key values, and we have a strong commitment to building our workforce and products through that lens.
  
   
   
  
   You don't have to check every box in this job description to be a great fit for the role! If you're excited about this position and the prospect of working for b.well, please apply. If it turns out this role isn't for you, there may be other openings that could align with your experience and expertise!
  
 
  
 
   We are committed to an inclusive and diverse b.well. We are an equal opportunity employer. We do not discriminate based on race, ethnicity, color, ancestry, national origin, religion, sex, sexual orientation, gender identity, age, disability, veteran, genetic information, marital status or any other legally protected status.","<div>
 <div>
  <b>Lead Data Engineer - Data Interoperability Team</b>
  <br> 
  <br> As a Lead Data Engineer, you will be a critical member of the Data Interoperability team responsible for building and maintaining data pipelines and data infrastructure that connects to thousands of data sources around the country to bring together a person&#x2019;s health record in one place.
  <br> 
  <br> b.well Connected Health has the largest set of connected health data for any person in the United States. By bringing a person&#x2019;s health data into one place, we are able to help everyone get convenient and affordable health care.
  <br> 
  <br> 
  <b>This position is available for fully remote work</b>.
  <br> 
  <br> 
  <b>What You&apos;ll Do:</b>
 </div> 
 <ul>
  <li>Design, build, and maintain b.well&#x2019;s data pipeline infrastructure using Python, Spark, Prefect, Kubernetes and other modern technologies</li> 
  <li>Lead a team of data engineers to build data pipelines and infrastructure that connects to thousands of data sources around the country including health providers, insurance companies, pharmacies and labs.</li> 
  <li>Launch new projects from ideation to completion</li> 
  <li>Help lead other developers to improve their career development and coding abilities</li> 
  <li>You will safeguard sensitive data by following policies and training concerning your security and privacy responsibilities</li> 
 </ul>
 <div>
  <b>Job Requirements:</b>
 </div> 
 <ul>
  <li>7+ years of professional programming experience (must include Python)</li> 
  <li>2+ years building microservices in Python</li> 
  <li>Exceptional and demonstrable data engineering experience</li> 
  <li>Experience in loading, validating, cleaning, and manipulating data files</li> 
  <li>Strong experience with unit testing and test-driven development</li> 
  <li>Strong experience with relational and/or NoSQL databases</li> 
  <li>Strong experience with cloud-based infrastructure</li> 
  <li>Comfort with Linux/Unix command line</li> 
 </ul>
 <div>
  <b>Great to Have:</b>
 </div> 
 <ul>
  <li>7+ years of Advance Python experience</li> 
  <li>5+ years of data pipeline engineering experience</li> 
  <li>1+ years of experience with Spark</li> 
  <li>Experience with Airflow or Prefect</li> 
  <li>Experience with Docker</li> 
  <li>Experience with streaming data</li> 
  <li>Experience scaling technology solutions to hundreds of thousands active users</li> 
  <li>Experience mentoring other developers</li> 
  <li>Deep understanding of common API methodologies</li> 
  <li>Startup experience</li> 
 </ul>
 <div>
  <b>Blow Us Away:</b>
 </div> 
 <ul>
  <li>Experience working with third-party healthcare APIs, HL7, data streams, and/or flat files</li> 
  <li>Experience in cybersecurity</li> 
  <li>Experience with HIPAA, HITECH, and HITRUST</li> 
  <li>An active GitHub profile or other public code portfolio</li> 
  <li>Active Stack Overflow profile</li> 
  <li>Documented work on open source projects</li> 
 </ul> 
 <div>
  <div>
   Data shows that women, people of color, and other underrepresented groups may be less likely to apply for jobs unless they believe they are a perfect match. But b.well holds diversity amongst its key values, and we have a strong commitment to building our workforce and products through that lens.
  </div>
  <br> 
  <div></div> 
  <div>
   You don&apos;t have to check every box in this job description to be a great fit for the role! If you&apos;re excited about this position and the prospect of working for b.well, please apply. If it turns out this role isn&apos;t for you, there may be other openings that could align with your experience and expertise!
  </div>
 </div>
 <br> 
 <div>
  <br> We are committed to an inclusive and diverse b.well. We are an equal opportunity employer. We do not discriminate based on race, ethnicity, color, ancestry, national origin, religion, sex, sexual orientation, gender identity, age, disability, veteran, genetic information, marital status or any other legally protected status.
 </div>
</div>","https://www.indeed.com/applystart?jk=8a6fc1cf4a3aad4e&from=vj&pos=top&mvj=0&spon=0&sjdu=YmZE5d5THV8u75cuc0H6Y26AwfY51UOGmh3Z9h4OvXgtUhfZJtObR1siMnGjN7lhGQFsFDP3gSdjjHZnj9ZEfg&vjfrom=serp&astse=2527873ae4d85455&assa=6525","8a6fc1cf4a3aad4e",,,,"Remote","Lead Data Engineer - Data Interoperability","6 days ago","2023-10-19T11:50:27.017Z","1.7","3",,"2023-10-25T11:50:27.019Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=8a6fc1cf4a3aad4e&from=jasx&tk=1hdjannibirm6800&vjs=3"
"AIR HAMBURG Luftverkehrsgesellschaft mbH","Requisition ID
       
      
       2023-3757 
      
     
     
      
       City 
      
      
       Fort Lauderdale 
      
     
     
      
       Position Type
       
      
       Permanent Full-Time 
      
     
     
      
       Work Base
       
      
       Remote 
      
     
     
      
       Category
       
      
       Engineering 
      
     
    
   
  
 
 Job Profile 
 
  
   
    
     About Team
    
    
      The Data Foundation Team is highly critical for the organization to provide timely, accurate and most up to date data so that the business can take decisions accordingly. The team works with several application teams, data analytics, data science team etc. We are looking for a highly skilled and experienced 
     Senior Data Engineer to design, implement, and maintain robust and scalable data pipelines.
    
    
    
      About Company
    
    
      Vista Tech plays a vital role in the Vista group operations by delivering and accelerating comprehensive technology solutions across all brands. Vista’s end-to-end and click-to-flight solutions offer the industry's only comprehensive flight booking platform, seamlessly integrating global operations, and leveraging AI and machine learning to optimize pricing and fleet movement. Comprised of the Product Management, Engineering, and IT teams, Vista Tech’s mission is to enhance transparency and accessibility in private aviation through the development of the world's largest digital private aviation marketplace. In achieving this, Vista Tech always ensures the utmost safety and efficiency for FLIGHT CREW, EMPLOYEES and Members, while fostering a culture of innovation and excellence.
    
    
    
      You will report to Engineering Manager and play a crucial role in driving the technical direction of our projects and guiding the team in adopting best practices and cutting-edge technologies. This position is a 100% remote role with regular shif timings (9 AM to 6 AM EST). You will collaborate with cross-functional teams, provide technical leadership, and contribute to the entire software development lifecycle.
     
   
  
 
 Your Responsibilities 
 
  
   
    
     Scalable Data Infrastructure: Lead the development and maintenance of highly scalable data pipelines, playing a crucial role in fortifying our data foundation.
      Technical Excellence: Demonstrate hands-on technical expertise in designing, building, and documenting complex data pipelines while adhering to data engineering best practices.
      Cross-Functional Collaboration: Collaborate closely with data engineering, analytics, and data science leadership to continuously enhance the functionality and capabilities of our data systems.
      Process Optimization: Identify opportunities for internal process improvements, spearheading automation of manual tasks, optimizing data delivery mechanisms, and redesigning infrastructure to ensure greater scalability and efficiency.
      Data Integration Mastery: Define and construct the infrastructure necessary to facilitate efficient extraction, transformation, and loading (ETL) of data from a diverse array of sources.
     
   
  
 
 Required Skills, Qualifications, and Experience 
 
  
   
    
     
      
       Strong Analytical Foundation: A robust background in mathematics, statistics, computer science, data science, or a related discipline, showcasing your analytical prowess.
      
     
      
        Programming Proficiency: Advanced expertise in programming languages, particularly Python and SQL, to tackle complex data challenges.
      
     
      
        Production Experience: Proven experience in the production environment with a range of essential tools and platforms, including Snowflake, DBT, Airflow, Amazon Web Services (AWS), Docker/Kubernetes, and PostgreSQL.
      
     
      
        Database Mastery: Proficiency in database technologies, including Snowflake, PostgreSQL, Redshift, and others, enabling efficient data management.
      
     
      
        Exceptional Organizational Skills: Strong organizational capabilities, allowing you to manage multiple projects and priorities concurrently while consistently meeting deadlines.
      
     
      
        Additional Assets: Familiarity and experience with additional tools and technologies, such as AWS certification, Kafka Streaming/Kafka Connect, MongoDB, and CI/CD tools like GitLab, Jira, and Confluence, are highly advantageous.","<div></div>
<div>
 <div>
  <div>
   <div>
    <div>
     <div>
      <div>
       Requisition ID
      </div> 
      <div>
       2023-3757 
      </div>
     </div>
     <div>
      <div>
       City 
      </div>
      <div>
       Fort Lauderdale 
      </div>
     </div>
     <div>
      <div>
       Position Type
      </div> 
      <div>
       Permanent Full-Time 
      </div>
     </div>
     <div>
      <div>
       Work Base
      </div> 
      <div>
       Remote 
      </div>
     </div>
     <div>
      <div>
       Category
      </div> 
      <div>
       Engineering 
      </div>
     </div>
    </div>
   </div>
  </div>
 </div>
 <h2 class=""jobSectionHeader""><b>Job Profile </b></h2>
 <div>
  <div>
   <div>
    <div>
     <b>About Team</b>
    </div>
    <div>
      The Data Foundation Team is highly critical for the organization to provide timely, accurate and most up to date data so that the business can take decisions accordingly. The team works with several application teams, data analytics, data science team etc. We are looking for a highly skilled and experienced 
     <b>Senior Data Engineer</b> to design, implement, and maintain robust and scalable data pipelines.
    </div>
    <div></div>
    <div>
     <b><br> About Company</b>
    </div>
    <div>
      Vista Tech plays a vital role in the Vista group operations by delivering and accelerating comprehensive technology solutions across all brands. Vista&#x2019;s end-to-end and click-to-flight solutions offer the industry&apos;s only comprehensive flight booking platform, seamlessly integrating global operations, and leveraging AI and machine learning to optimize pricing and fleet movement. Comprised of the Product Management, Engineering, and IT teams, Vista Tech&#x2019;s mission is to enhance transparency and accessibility in private aviation through the development of the world&apos;s largest digital private aviation marketplace. In achieving this, Vista Tech always ensures the utmost safety and efficiency for FLIGHT CREW, EMPLOYEES and Members, while fostering a culture of innovation and excellence.
    </div>
    <div></div>
    <div>
     <br> You will report to Engineering Manager and play a crucial role in driving the technical direction of our projects and guiding the team in adopting best practices and cutting-edge technologies. This position is a 100% remote role with regular shif timings (9 AM to 6 AM EST). You will collaborate with cross-functional teams, provide technical leadership, and contribute to the entire software development lifecycle.
    </div> 
   </div>
  </div>
 </div>
 <h2 class=""jobSectionHeader""><b>Your Responsibilities </b></h2>
 <div>
  <div>
   <div>
    <ul>
     <li><b>Scalable Data Infrastructure:</b> Lead the development and maintenance of highly scalable data pipelines, playing a crucial role in fortifying our data foundation.</li>
     <li><b> Technical Excellence:</b> Demonstrate hands-on technical expertise in designing, building, and documenting complex data pipelines while adhering to data engineering best practices.</li>
     <li><b> Cross-Functional Collaboration:</b> Collaborate closely with data engineering, analytics, and data science leadership to continuously enhance the functionality and capabilities of our data systems.</li>
     <li><b> Process Optimization:</b> Identify opportunities for internal process improvements, spearheading automation of manual tasks, optimizing data delivery mechanisms, and redesigning infrastructure to ensure greater scalability and efficiency.</li>
     <li><b> Data Integration Mastery:</b> Define and construct the infrastructure necessary to facilitate efficient extraction, transformation, and loading (ETL) of data from a diverse array of sources.</li>
    </ul> 
   </div>
  </div>
 </div>
 <h2 class=""jobSectionHeader""><b>Required Skills, Qualifications, and Experience </b></h2>
 <div>
  <div>
   <div>
    <ul>
     <li>
      <div>
       <b>Strong Analytical Foundation:</b> A robust background in mathematics, statistics, computer science, data science, or a related discipline, showcasing your analytical prowess.
      </div></li>
     <li>
      <div>
       <b> Programming Proficiency:</b> Advanced expertise in programming languages, particularly Python and SQL, to tackle complex data challenges.
      </div></li>
     <li>
      <div>
       <b> Production Experience:</b> Proven experience in the production environment with a range of essential tools and platforms, including Snowflake, DBT, Airflow, Amazon Web Services (AWS), Docker/Kubernetes, and PostgreSQL.
      </div></li>
     <li>
      <div>
       <b> Database Mastery:</b> Proficiency in database technologies, including Snowflake, PostgreSQL, Redshift, and others, enabling efficient data management.
      </div></li>
     <li>
      <div>
       <b> Exceptional Organizational Skills:</b> Strong organizational capabilities, allowing you to manage multiple projects and priorities concurrently while consistently meeting deadlines.
      </div></li>
     <li>
      <div>
       <b> Additional Assets:</b> Familiarity and experience with additional tools and technologies, such as AWS certification, Kafka Streaming/Kafka Connect, MongoDB, and CI/CD tools like GitLab, Jira, and Confluence, are highly advantageous.
      </div></li>
    </ul>
   </div>
  </div>
 </div>
</div>
<div></div>","https://careers-vistaglobal.icims.com/jobs/3757/senior-data-engineer/job?in_iframe=1","70e539c357b570ae",,"Full-time",,"Fort Lauderdale, FL","SENIOR DATA ENGINEER","6 days ago","2023-10-19T11:50:20.401Z",,,,"2023-10-25T11:50:20.402Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=70e539c357b570ae&from=jasx&tk=1hdjakuh7je3s800&vjs=3"
"OTSI","OTSI (Object Technology Solutions, Inc) has an immediate opening Senior Data Engineer at Overland Park, KS (Remote) and it’s a Long Term Contract position.
This would be W2 position and consultant should go for F2F interview in Overland Park, KS location. (so please apply who are ready to work on W2 and go for a F2F interview)
Job Description:
· 7+ Year of experience as a Data Engineer with below skills
· Big Data,
· Spark,
· HIVE
· AWS
· Kubernetes
· Data Bricks
About us:
OTSI is a leading global technology company offering solutions, consulting, and managed services for businesses worldwide since 1999. OTSI serves clients from its 15 offices across 6 countries around the globe with a “Follow-the-Sun” model. Headquartered in Overland Park, Kansas, we have a strong presence in North America, Central America, and Asia-Pacific with a Global Delivery Center based in India. These strategic locations offer our customers the competitive advantages of onshore, nearshore, and offshore engagement and delivery options, with 24/7 support. OTSI works with 100+ enterprise customers, of which many are Fortune ranked, OTSI focuses on industry segments such as Banking, Financial Services & Insurance, Healthcare & Life Sciences, Energy & Utilities, Communications & Media Entertainment, Engineering & Telecom, Retail & Consumer Services, Hi-tech, Manufacturing, Engineering, transport logistics, Government, Defense & PSUs.
Our Center of Excellence:
· Data & Analytics
· Digital Transformation
· QA & Automation
· Enterprise Applications
· Disruptive Technologies
Job Type: Contract
Pay: $50.00 - $58.00 per hour
Experience level:

 7 years

Schedule:

 8 hour shift
 Monday to Friday

Experience:

 Data Engineer: 7 years (Preferred)
 Spark: 5 years (Preferred)
 AWS: 5 years (Preferred)
 DATA BRICKS: 5 years (Preferred)
 Kubernetes: 5 years (Preferred)

Work Location: Remote","<p><b>OTSI (Object Technology Solutions, Inc)</b> has an immediate opening <b>Senior Data Engineer </b>at <b>Overland Park, KS (Remote) </b>and it&#x2019;s a <b>Long Term Contract </b>position.</p>
<p><b>This would be W2 position and consultant should go for F2F interview in Overland Park, KS location. (so please apply who are ready to work on W2 and go for a F2F interview)</b></p>
<p><b>Job Description:</b></p>
<p>&#xb7; 7+ Year of experience as a Data Engineer with below skills</p>
<p>&#xb7; Big Data,</p>
<p>&#xb7; Spark,</p>
<p>&#xb7; HIVE</p>
<p>&#xb7; AWS</p>
<p>&#xb7; Kubernetes</p>
<p>&#xb7; Data Bricks</p>
<p><b>About us:</b></p>
<p>OTSI is a leading global technology company offering solutions, consulting, and managed services for businesses worldwide since 1999. OTSI serves clients from its 15 offices across 6 countries around the globe with a &#x201c;Follow-the-Sun&#x201d; model. Headquartered in Overland Park, Kansas, we have a strong presence in North America, Central America, and Asia-Pacific with a Global Delivery Center based in India. These strategic locations offer our customers the competitive advantages of onshore, nearshore, and offshore engagement and delivery options, with 24/7 support. OTSI works with 100+ enterprise customers, of which many are Fortune ranked, OTSI focuses on industry segments such as Banking, Financial Services &amp; Insurance, Healthcare &amp; Life Sciences, Energy &amp; Utilities, Communications &amp; Media Entertainment, Engineering &amp; Telecom, Retail &amp; Consumer Services, Hi-tech, Manufacturing, Engineering, transport logistics, Government, Defense &amp; PSUs.</p>
<p>Our Center of Excellence:</p>
<p>&#xb7; Data &amp; Analytics</p>
<p>&#xb7; Digital Transformation</p>
<p>&#xb7; QA &amp; Automation</p>
<p>&#xb7; Enterprise Applications</p>
<p>&#xb7; Disruptive Technologies</p>
<p>Job Type: Contract</p>
<p>Pay: &#x24;50.00 - &#x24;58.00 per hour</p>
<p>Experience level:</p>
<ul>
 <li>7 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
 <li>Monday to Friday</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Data Engineer: 7 years (Preferred)</li>
 <li>Spark: 5 years (Preferred)</li>
 <li>AWS: 5 years (Preferred)</li>
 <li>DATA BRICKS: 5 years (Preferred)</li>
 <li>Kubernetes: 5 years (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,"f5777b4056e4e7e0",,"Contract",,"Overland Park, KS","Data Engineer (Only W2)","6 days ago","2023-10-19T11:50:28.772Z","3.5","23","$50 - $58 an hour","2023-10-25T11:50:28.775Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=f5777b4056e4e7e0&from=jasx&tk=1hdjannibirm6800&vjs=3"
"Syngenta Crop Protection","Company Description
  As a world market leader in crop protection, we help farmers to counter these threats and ensure enough safe, nutritious, affordable food for all – while minimizing the use of land and other agricultural inputs.
  Syngenta Crop Protection keeps plants safe from planting to harvesting. From the moment a seed is planted through to harvest, crops need to be protected from weeds, insects and diseases as well as droughts and floods, heat and cold.
  Syngenta Crop Protection is headquartered in Switzerland.
 
 

 Job Description
  The Senior Data Engineer will be part of a team working in a collaborative DataOps environment assembled from data engineers, data scientists, visualization and analytics experts drawn from IT and R&D teams.
  Specific duties include:
 
   Support an analytical data infrastructure providing application, tool and ad-hoc access to large datasets consisting of complex data types including genomic, phenotypic, environmental, image and geospatial data.
   Process huge data using Big Data tools and technologies like Spark, Hadoop and Hive and extract, transform and load data using various ETL tools.
   Engage with business stakeholders to understand strategic roadmaps and build a technical strategy to support them.
   Deploy high value, high performance datasets in Snowflake.
   Create and support real-time data pipelines built on AWS technologies including Glue, S3, Lambda, EMR, EventBridge, Athena, Kinesis, and IoT Core.
   Create end to end database architecture and data modeling in Oracle, Microsoft SQL Server, PostgreSQL and Sybase and creating complex SQL queries, stored procedures and functions to implement business logic.
   Embed quality and intelligent reporting capabilities into data pipelines including detection of anomalies and changes in trends with meaningful alerts and statistics.
   Maintain versions of code and implement CI/CD (continuous integration and continuous deployment) pipelines using Github, Teamcity, Jenkins and SVN.
   Continually research the latest big data and visualization technologies to provide new capabilities and increase efficiency.
   Collaborate with data scientists and other tech teams to implement advanced analytics algorithms into our data pipelines that exploit our rich datasets for statistical analysis, prediction, clustering and machine learning.
   Help continually improve automation and simplifying data as a service.
   Performing work using cloud technologies including AWS, EC2, S3, Kinesis, Glue, Redshift/Spectrum, Lambda, EMR, Athena, Data Pipeline.
   Create Graphical User Interfaces by coding in advance HTML, Java and Javascript and create various utilities by programming in Python, Powershell and Unix Shell Scripting.
 
  Position based at company headquarters in Greensboro, NC; may telecommute from anywhere in the U.S.
 
 

 Qualifications
  This position requires a Bachelor’s degree or equivalent in Computer Science, Information Science, Engineering, Mathematics or related technical discipline and 5 years of related (progressive, post-baccalaureate) experience.
  Must also have 12 months of experience (which may have been gained concurrently) with each of the following:
 
   Processing huge data using Big Data tools and technologies like Spark, Hadoop and Hive and extracting, transforming and loading data using various ETL tools.
   Maintaining versions of code and implementing CI/CD (continuous integration and continuous deployment) pipelines using Github, Teamcity, Jenkins and SVN.
   Creating end to end database architecture and data modeling in Oracle, Microsoft SQL Server, PostgreSQL and Sybase and creating complex SQL queries, stored procedures and functions to implement business logic.
   Performing work using cloud technologies including AWS, EC2, S3, Kinesis, Glue, Redshift/Spectrum, Lambda, EMR, Athena, Data Pipeline.
   Creating Graphical User Interfaces by coding in advance HTML, Java and Javascript and creating various utilities by programming in Python, Powershell and Unix Shell Scripting.
 
  All experience may have been gained concurrently. Must pass a background check and drug test before beginning employment. Position based at company headquarters in Greensboro, NC; may telecommute from anywhere in the U.S.
  Additional Information
  What We Offer:
 
   A culture that celebrates diversity & inclusion, promotes professional development, and strives for a work-life balance that supports the team members. offers flexible work options to support your work and personal needs
   Full Benefit Package (Medical, Dental & Vision) that starts your first day
   401k plan with company match, Profit Sharing & Retirement Savings Contribution
   Paid Vacation, 9 Paid Holidays, Maternity and Paternity Leave, Education Assistance, Wellness Programs, Corporate Discounts, among other benefits
 
  Syngenta is an Equal Opportunity Employer and does not discriminate in recruitment, hiring, training, promotion or any other employment practices for reasons of race, color, religion, gender, national origin, age, sexual orientation, marital or veteran status, disability, or any other legally protected status.
  Family and Medical Leave Act (FMLA)
  (http://www.dol.gov/whd/regs/compliance/posters/fmla.htm)
  Equal Employment Opportunity Commission's (EEOC)
  (http://webapps.dol.gov/elaws/firststep/poster_direct.htm)
  Employee Polygraph Protection Act (EPPA)
  (http://www.dol.gov/whd/regs/compliance/posters/eppa.htm)","<div>
 Company Description
 <p><br> As a world market leader in crop protection, we help farmers to counter these threats and ensure enough safe, nutritious, affordable food for all &#x2013; while minimizing the use of land and other agricultural inputs.</p>
 <p> Syngenta Crop Protection keeps plants safe from planting to harvesting. From the moment a seed is planted through to harvest, crops need to be protected from weeds, insects and diseases as well as droughts and floods, heat and cold.</p>
 <p> Syngenta Crop Protection is headquartered in Switzerland.</p>
</div> 
<br> 
<div>
 Job Description
 <p><br> The Senior Data Engineer will be part of a team working in a collaborative DataOps environment assembled from data engineers, data scientists, visualization and analytics experts drawn from IT and R&amp;D teams.</p>
 <p> Specific duties include:</p>
 <ul>
  <li> Support an analytical data infrastructure providing application, tool and ad-hoc access to large datasets consisting of complex data types including genomic, phenotypic, environmental, image and geospatial data.</li>
  <li> Process huge data using Big Data tools and technologies like Spark, Hadoop and Hive and extract, transform and load data using various ETL tools.</li>
  <li> Engage with business stakeholders to understand strategic roadmaps and build a technical strategy to support them.</li>
  <li> Deploy high value, high performance datasets in Snowflake.</li>
  <li> Create and support real-time data pipelines built on AWS technologies including Glue, S3, Lambda, EMR, EventBridge, Athena, Kinesis, and IoT Core.</li>
  <li> Create end to end database architecture and data modeling in Oracle, Microsoft SQL Server, PostgreSQL and Sybase and creating complex SQL queries, stored procedures and functions to implement business logic.</li>
  <li> Embed quality and intelligent reporting capabilities into data pipelines including detection of anomalies and changes in trends with meaningful alerts and statistics.</li>
  <li> Maintain versions of code and implement CI/CD (continuous integration and continuous deployment) pipelines using Github, Teamcity, Jenkins and SVN.</li>
  <li> Continually research the latest big data and visualization technologies to provide new capabilities and increase efficiency.</li>
  <li> Collaborate with data scientists and other tech teams to implement advanced analytics algorithms into our data pipelines that exploit our rich datasets for statistical analysis, prediction, clustering and machine learning.</li>
  <li> Help continually improve automation and simplifying data as a service.</li>
  <li> Performing work using cloud technologies including AWS, EC2, S3, Kinesis, Glue, Redshift/Spectrum, Lambda, EMR, Athena, Data Pipeline.</li>
  <li> Create Graphical User Interfaces by coding in advance HTML, Java and Javascript and create various utilities by programming in Python, Powershell and Unix Shell Scripting.</li>
 </ul>
 <p> Position based at company headquarters in Greensboro, NC; may telecommute from anywhere in the U.S.</p>
</div> 
<br> 
<div>
 Qualifications
 <p><br> This position requires a Bachelor&#x2019;s degree or equivalent in Computer Science, Information Science, Engineering, Mathematics or related technical discipline and 5 years of related (progressive, post-baccalaureate) experience.</p>
 <p> Must also have 12 months of experience (which may have been gained concurrently) with each of the following:</p>
 <ul>
  <li> Processing huge data using Big Data tools and technologies like Spark, Hadoop and Hive and extracting, transforming and loading data using various ETL tools.</li>
  <li> Maintaining versions of code and implementing CI/CD (continuous integration and continuous deployment) pipelines using Github, Teamcity, Jenkins and SVN.</li>
  <li> Creating end to end database architecture and data modeling in Oracle, Microsoft SQL Server, PostgreSQL and Sybase and creating complex SQL queries, stored procedures and functions to implement business logic.</li>
  <li> Performing work using cloud technologies including AWS, EC2, S3, Kinesis, Glue, Redshift/Spectrum, Lambda, EMR, Athena, Data Pipeline.</li>
  <li> Creating Graphical User Interfaces by coding in advance HTML, Java and Javascript and creating various utilities by programming in Python, Powershell and Unix Shell Scripting.</li>
 </ul>
 <p> All experience may have been gained concurrently. Must pass a background check and drug test before beginning employment. Position based at company headquarters in Greensboro, NC; may telecommute from anywhere in the U.S.</p>
 <br> Additional Information
 <p><b><br> What We Offer:</b></p>
 <ul>
  <li> A culture that celebrates diversity &amp; inclusion, promotes professional development, and strives for a work-life balance that supports the team members. offers flexible work options to support your work and personal needs</li>
  <li> Full Benefit Package (Medical, Dental &amp; Vision) that starts your first day</li>
  <li> 401k plan with company match, Profit Sharing &amp; Retirement Savings Contribution</li>
  <li> Paid Vacation, 9 Paid Holidays, Maternity and Paternity Leave, Education Assistance, Wellness Programs, Corporate Discounts, among other benefits</li>
 </ul>
 <p> Syngenta is an Equal Opportunity Employer and does not discriminate in recruitment, hiring, training, promotion or any other employment practices for reasons of race, color, religion, gender, national origin, age, sexual orientation, marital or veteran status, disability, or any other legally protected status.</p>
 <p> Family and Medical Leave Act (FMLA)</p>
 <p> (http://www.dol.gov/whd/regs/compliance/posters/fmla.htm)</p>
 <p> Equal Employment Opportunity Commission&apos;s (EEOC)</p>
 <p> (http://webapps.dol.gov/elaws/firststep/poster_direct.htm)</p>
 <p> Employee Polygraph Protection Act (EPPA)</p>
 <p> (http://www.dol.gov/whd/regs/compliance/posters/eppa.htm)</p>
</div>","https://www.indeed.com/rc/clk?jk=f5f186a6f3b7168f&atk=&xpse=SoDs67I3JzdQEvQu9R0LbzkdCdPP","f5f186a6f3b7168f",,"Full-time",,"Greensboro, NC","Senior Data Engineer","6 days ago","2023-10-19T11:50:26.828Z","4.1","1503",,"2023-10-25T11:50:26.832Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=f5f186a6f3b7168f&from=jasx&tk=1hdjannibirm6800&vjs=3"
"MST Solutions","Role/Responsibilities

 Support transition of media data from RAPP (current vendor)
 Hands on technical role working with data and providing expertise on how best to transition data
 ETL, Python, AWS, SQL

Top 3-5 Skills
ETL
AWS – Apache Airflow, Kafka, Streaming Data, etc.
Python
SQL – Validate data policy checks
Modeling experience is a plus
Job Requirements:
Desired soft skills

 Hands on
 Flexible, must be able to attend meetings
 Good verbal and written communication as they will have to give their recommendations on data transfer to leadership team
 This will be a high visibility role

Job Type: Contract
Salary: $55.00 - $60.00 per hour
Experience:

 ETL: 1 year (Preferred)
 AWS: 1 year (Preferred)
 Redshift: 1 year (Preferred)

Work Location: Remote","<p>Role/Responsibilities</p>
<ul>
 <li>Support transition of media data from RAPP (current vendor)</li>
 <li>Hands on technical role working with data and providing expertise on how best to transition data</li>
 <li>ETL, Python, AWS, SQL</li>
</ul>
<p>Top 3-5 Skills</p>
<p>ETL</p>
<p>AWS &#x2013; Apache Airflow, Kafka, Streaming Data, etc.</p>
<p>Python</p>
<p>SQL &#x2013; Validate data policy checks</p>
<p>Modeling experience is a plus</p>
<p><b>Job Requirements:</b></p>
<p>Desired soft skills</p>
<ul>
 <li>Hands on</li>
 <li>Flexible, must be able to attend meetings</li>
 <li>Good verbal and written communication as they will have to give their recommendations on data transfer to leadership team</li>
 <li>This will be a high visibility role</li>
</ul>
<p>Job Type: Contract</p>
<p>Salary: &#x24;55.00 - &#x24;60.00 per hour</p>
<p>Experience:</p>
<ul>
 <li>ETL: 1 year (Preferred)</li>
 <li>AWS: 1 year (Preferred)</li>
 <li>Redshift: 1 year (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,"34d9149b4a61c486",,"Contract",,"Remote","Sr Data Engineer - Only W2","6 days ago","2023-10-19T11:50:30.357Z","4","6","$55 - $60 an hour","2023-10-25T11:50:30.358Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=34d9149b4a61c486&from=jasx&tk=1hdjannibirm6800&vjs=3"
"IBR (Imagine Believe Realize)","The Senior Data Engineer must be able to meet the key criteria below:

 Location: 100% telework
 Years' Experience: 10+ years
 Education: Bachelor’s in IT related field
 Security Clearance: IBR is a federal contractor. Applicants must be able to meet the requirements to obtain an Public Trust security clearance. NOTE: United States Citizenship is required.
 Work Authorization: Must show that applicant is legally permitted to work in the United States.
 Employment Type: Full-Time, W-2
 Key Skills:
 10+ years of IT experience focusing on enterprise data architecture and management
 Experience with Databricks required
 8+ years experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling
 Experience with ETL and ELT tools such as SSIS, Pentaho, and/or Data Migration Services
 Advanced level SQL experience (Joins, Aggregation, Windowing functions, Common Table Expressions, RDBMS schema design, Postgres performance optimization)
 Experience with AWS environment, CI/CD pipelines, and Python (Python 3) a bonus

Overview
Do you want to help build a portfolio of next-generation mobile-enabled data collection systems and enterprise portals? As a Data Engineer at IBR, you will support the Agile based engineering of a robust, secure, and scalable enterprise web portal solutions hosted in AWS. This position will work closely with the solutions delivery team to supporting the operations team performing Deployment, Systems Integration Testing, and Operations & Maintenance activities.
Responsibilities

 Plan, create, and maintain data architectures, ensuring alignment with business requirements
 Obtain data, formulate dataset processes, and store optimized data
 Identify problems and inefficiencies and apply solutions
 Determine tasks where manual participation can be eliminated with automation.
 Identify and optimize data bottlenecks, leveraging automation where possible
 Create and manage data lifecycle policies (retention, backups/restore, etc)
 Create, maintain, and manage ETL/ELT pipelines
 Create, maintain, and manage data transformations
 Maintain/update documentation
 Create, maintain, and manage data pipeline schedules
 Monitor data pipelines
 Create, maintain, and manage data quality gates (Great Expectations) to ensure high data quality
 Support AI/ML teams with optimizing feature engineering code
 Spark updates
 Create, maintain, and manage Spark Structured Steaming jobs, including using the newer Delta Live Tables and/or DBT
 Research existing data in the data lake to determine best sources for data
 Create, manage, and maintain ksqlDB and Kafka Streams queries/code
 Maintain and update Python-based data processing scripts executed on AWS Lambdas
 Unit tests for all the Spark, Python data processing and Lambda codes
 Maintain PCIS Reporting Database data lake with optimizations and maintenance (performance tuning, etc)

Qualifications

 10+ years of IT experience focusing on enterprise data architecture and management
 Experience in Conceptual/Logical/Physical Data Modeling & expertise in Relational and Dimensional Data Modeling
 Experience with Databricks, Structured Streaming, Delta Lake concepts, and Delta Live Tables required
 Additional experience with Spark, Spark SQL, Spark DataFrames and DataSets, and PySpark
 Data Lake concepts such as time travel and schema evolution and optimization
 Structured Streaming and Delta Live Tables with Databricks a bonus
 Experience leading and architecting enterprise-wide initiatives specifically system integration, data migration, transformation, data warehouse build, data mart build, and data lakes implementation / support
 Advanced level understanding of streaming data pipelines and how they differ from batch systems
 Formalize concepts of how to handle late data, defining windows, and data freshness
 Advanced understanding of ETL and ELT and ETL/ELT tools such as SSIS, Pentaho, Data Migration Service etc
 Understanding of concepts and implementation strategies for different incremental data loads such as tumbling window, sliding window, high watermark, etc.
 Familiarity and/or expertise with Great Expectations or other data quality/data validation frameworks a bonus
 Understanding of streaming data pipelines and batch systems
 Familiarity with concepts such as late data, defining windows, and how window definitions impact data freshness
 Advanced level SQL experience (Joins, Aggregation, Windowing functions, Common Table Expressions, RDBMS schema design, Postgres performance optimization)
 Indexing and partitioning strategy experience
 Debug, troubleshoot, design and implement solutions to complex technical issues
 Experience with large-scale, high-performance enterprise big data application deployment and solution
 Understanding how to create DAGs to define workflows
 Familiarity with CI/CD pipelines, containerization, and pipeline orchestration tools such as Airflow, Prefect, etc a bonus but not required
 Architecture experience in AWS environment a bonus
 Familiarity working with Kinesis and/or Lambda specifically with how to push and pull data, how to use AWS tools to view data in Kinesis streams, and for processing massive data at scale (UNICORN) a bonus
 Experience with Docker, Jenkins, and CloudWatch
 Ability to write and maintain Jenkinsfiles for supporting CI/CD pipelines
 Experience working with AWS Lambdas for configuration and optimization
 Experience working with DynamoDB to query and write data
 Experience with S3
 Knowledge of Python (Python 3 desired) for CI/CD pipelines a bonus
 Familiarity with Pytest and Unittest a bonus
 Experience working with JSON and defining JSON Schemas a bonus
 Experience setting up and management Confluent/Kafka topics and ensuring performance using Kafka a bonus
 Familiarity with Schema Registry, message formats such as Avro, ORC, etc.
 Understanding how to manage ksqlDB SQL files and migrations and Kafka Streams
 Ability to thrive in a team-based environment
 Experience briefing the benefits and constraints of technology solutions to technology partners, stakeholders, team members, and senior level of management

Physical Demands
Position consists of sitting for long periods of time, bending, stooping, crouching, and lifting up to 20 pounds. Frequently uses hands/fingers for manipulation of keyboard and mouse.
Work Environment
Work is performed primarily indoors in a well-lit office environment. The environment is normally air conditioned, but conditions may change dependent upon circumstances. Work may need to be performed in a fast-paced environment requiring quick thinking and rapid judgements. Employee will be exposed to a wide variety of clients in differing functions, personalities, and abilities.
About IBRImagine Believe Realize, LLC (IBR) is an emerging small business focused on delivering software and systems engineering solutions to government and commercial clients. Our talent acquisition strategy is tailored to career seeking candidates who embrace continuous learning and desire to grow as a professional in the software/systems engineering industry. We strive to enhance our team members ability to thrive in the workplace by creating a proper work/life balance and first-class benefits package that includes:

 Nationwide medical, dental, and vision insurance
 3 weeks of Paid Time Off and 11 Paid Federal Holidays
 401k matching
 Life Insurance, Short-Term Disability, and Long-Term Disability at no cost to our employees
 Flexible spending accounts and Dependent Care spending accounts
 Wellness incentives
 Reimbursement for professional development and certifications
 Training assistance opportunities

Upon hire and in compliance with federal law, all persons hired are required to verify identity and eligibility to work in the United States, and to complete the required employment eligibility verification and background check. IBR is a Federal Contractor.
Imagine Believe Realize, LLC is proud to be an Equal Opportunity and Affirmative Action Employer. We do not discriminate based upon race, age, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.”Learn more at http://www.teamibr.com
If alternative methods of assistance are needed with the application process, additional contact information has been provided below:
info@teamibr.com​​​​​​​407.459.1830
Job Type: Full-time
Pay: $131,243.38 - $158,056.53 per year
Benefits:

 401(k)
 401(k) matching
 Dental insurance
 Employee assistance program
 Flexible spending account
 Health insurance
 Health savings account
 Life insurance
 Paid time off
 Professional development assistance
 Referral program
 Vision insurance

Experience level:

 10 years

Schedule:

 Monday to Friday

Work Location: Remote","<p><b>The Senior Data Engineer must be able to meet the key criteria below:</b></p>
<ul>
 <li><b>Location: </b>100% telework</li>
 <li><b>Years&apos; Experience: </b>10+ years</li>
 <li><b>Education: </b>Bachelor&#x2019;s in IT related field</li>
 <li><b>Security Clearance:</b> IBR is a federal contractor. Applicants must be able to meet the requirements to obtain an Public Trust security clearance. NOTE: United States Citizenship is required.</li>
 <li><b>Work Authorization:</b> Must show that applicant is legally permitted to work in the United States.</li>
 <li><b>Employment Type:</b> Full-Time, W-2</li>
 <li><b>Key Skills:</b></li>
 <li>10+ years of IT experience focusing on enterprise data architecture and management</li>
 <li>Experience with Databricks required</li>
 <li>8+ years experience in Conceptual/Logical/Physical Data Modeling &amp; expertise in Relational and Dimensional Data Modeling</li>
 <li>Experience with ETL and ELT tools such as SSIS, Pentaho, and/or Data Migration Services</li>
 <li>Advanced level SQL experience (Joins, Aggregation, Windowing functions, Common Table Expressions, RDBMS schema design, Postgres performance optimization)</li>
 <li>Experience with AWS environment, CI/CD pipelines, and Python (Python 3) a bonus</li>
</ul>
<p><b>Overview</b></p>
<p>Do you want to help build a portfolio of next-generation mobile-enabled data collection systems and enterprise portals? As a Data Engineer at IBR, you will support the Agile based engineering of a robust, secure, and scalable enterprise web portal solutions hosted in AWS. This position will work closely with the solutions delivery team to supporting the operations team performing Deployment, Systems Integration Testing, and Operations &amp; Maintenance activities.</p>
<p><b>Responsibilities</b></p>
<ul>
 <li>Plan, create, and maintain data architectures, ensuring alignment with business requirements</li>
 <li>Obtain data, formulate dataset processes, and store optimized data</li>
 <li>Identify problems and inefficiencies and apply solutions</li>
 <li>Determine tasks where manual participation can be eliminated with automation.</li>
 <li>Identify and optimize data bottlenecks, leveraging automation where possible</li>
 <li>Create and manage data lifecycle policies (retention, backups/restore, etc)</li>
 <li>Create, maintain, and manage ETL/ELT pipelines</li>
 <li>Create, maintain, and manage data transformations</li>
 <li>Maintain/update documentation</li>
 <li>Create, maintain, and manage data pipeline schedules</li>
 <li>Monitor data pipelines</li>
 <li>Create, maintain, and manage data quality gates (Great Expectations) to ensure high data quality</li>
 <li>Support AI/ML teams with optimizing feature engineering code</li>
 <li>Spark updates</li>
 <li>Create, maintain, and manage Spark Structured Steaming jobs, including using the newer Delta Live Tables and/or DBT</li>
 <li>Research existing data in the data lake to determine best sources for data</li>
 <li>Create, manage, and maintain ksqlDB and Kafka Streams queries/code</li>
 <li>Maintain and update Python-based data processing scripts executed on AWS Lambdas</li>
 <li>Unit tests for all the Spark, Python data processing and Lambda codes</li>
 <li>Maintain PCIS Reporting Database data lake with optimizations and maintenance (performance tuning, etc)</li>
</ul>
<p><b>Qualifications</b></p>
<ul>
 <li>10+ years of IT experience focusing on enterprise data architecture and management</li>
 <li>Experience in Conceptual/Logical/Physical Data Modeling &amp; expertise in Relational and Dimensional Data Modeling</li>
 <li>Experience with Databricks, Structured Streaming, Delta Lake concepts, and Delta Live Tables required</li>
 <li>Additional experience with Spark, Spark SQL, Spark DataFrames and DataSets, and PySpark</li>
 <li>Data Lake concepts such as time travel and schema evolution and optimization</li>
 <li>Structured Streaming and Delta Live Tables with Databricks a bonus</li>
 <li>Experience leading and architecting enterprise-wide initiatives specifically system integration, data migration, transformation, data warehouse build, data mart build, and data lakes implementation / support</li>
 <li>Advanced level understanding of streaming data pipelines and how they differ from batch systems</li>
 <li>Formalize concepts of how to handle late data, defining windows, and data freshness</li>
 <li>Advanced understanding of ETL and ELT and ETL/ELT tools such as SSIS, Pentaho, Data Migration Service etc</li>
 <li>Understanding of concepts and implementation strategies for different incremental data loads such as tumbling window, sliding window, high watermark, etc.</li>
 <li>Familiarity and/or expertise with Great Expectations or other data quality/data validation frameworks a bonus</li>
 <li>Understanding of streaming data pipelines and batch systems</li>
 <li>Familiarity with concepts such as late data, defining windows, and how window definitions impact data freshness</li>
 <li>Advanced level SQL experience (Joins, Aggregation, Windowing functions, Common Table Expressions, RDBMS schema design, Postgres performance optimization)</li>
 <li>Indexing and partitioning strategy experience</li>
 <li>Debug, troubleshoot, design and implement solutions to complex technical issues</li>
 <li>Experience with large-scale, high-performance enterprise big data application deployment and solution</li>
 <li>Understanding how to create DAGs to define workflows</li>
 <li>Familiarity with CI/CD pipelines, containerization, and pipeline orchestration tools such as Airflow, Prefect, etc a bonus but not required</li>
 <li>Architecture experience in AWS environment a bonus</li>
 <li>Familiarity working with Kinesis and/or Lambda specifically with how to push and pull data, how to use AWS tools to view data in Kinesis streams, and for processing massive data at scale (UNICORN) a bonus</li>
 <li>Experience with Docker, Jenkins, and CloudWatch</li>
 <li>Ability to write and maintain Jenkinsfiles for supporting CI/CD pipelines</li>
 <li>Experience working with AWS Lambdas for configuration and optimization</li>
 <li>Experience working with DynamoDB to query and write data</li>
 <li>Experience with S3</li>
 <li>Knowledge of Python (Python 3 desired) for CI/CD pipelines a bonus</li>
 <li>Familiarity with Pytest and Unittest a bonus</li>
 <li>Experience working with JSON and defining JSON Schemas a bonus</li>
 <li>Experience setting up and management Confluent/Kafka topics and ensuring performance using Kafka a bonus</li>
 <li>Familiarity with Schema Registry, message formats such as Avro, ORC, etc.</li>
 <li>Understanding how to manage ksqlDB SQL files and migrations and Kafka Streams</li>
 <li>Ability to thrive in a team-based environment</li>
 <li>Experience briefing the benefits and constraints of technology solutions to technology partners, stakeholders, team members, and senior level of management</li>
</ul>
<p><b>Physical Demands</b></p>
<p>Position consists of sitting for long periods of time, bending, stooping, crouching, and lifting up to 20 pounds. Frequently uses hands/fingers for manipulation of keyboard and mouse.</p>
<p><b>Work Environment</b></p>
<p>Work is performed primarily indoors in a well-lit office environment. The environment is normally air conditioned, but conditions may change dependent upon circumstances. Work may need to be performed in a fast-paced environment requiring quick thinking and rapid judgements. Employee will be exposed to a wide variety of clients in differing functions, personalities, and abilities.</p>
<p><b>About IBR</b><br>Imagine Believe Realize, LLC (IBR) is an emerging small business focused on delivering software and systems engineering solutions to government and commercial clients. Our talent acquisition strategy is tailored to career seeking candidates who embrace continuous learning and desire to grow as a professional in the software/systems engineering industry. We strive to enhance our team members ability to thrive in the workplace by creating a proper work/life balance and first-class benefits package that includes:</p>
<ul>
 <li>Nationwide medical, dental, and vision insurance</li>
 <li>3 weeks of Paid Time Off and 11 Paid Federal Holidays</li>
 <li>401k matching</li>
 <li>Life Insurance, Short-Term Disability, and Long-Term Disability at no cost to our employees</li>
 <li>Flexible spending accounts and Dependent Care spending accounts</li>
 <li>Wellness incentives</li>
 <li>Reimbursement for professional development and certifications</li>
 <li>Training assistance opportunities</li>
</ul>
<p>Upon hire and in compliance with federal law, all persons hired are required to verify identity and eligibility to work in the United States, and to complete the required employment eligibility verification and background check. IBR is a Federal Contractor.</p>
<p>Imagine Believe Realize, LLC is proud to be an Equal Opportunity and Affirmative Action Employer. We do not discriminate based upon race, age, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.&#x201d;<br>Learn more at <i>http://www.teamibr.com</i></p>
<p>If alternative methods of assistance are needed with the application process, additional contact information has been provided below:</p>
<p><i>info@teamibr.com</i><br>&#x200b;&#x200b;&#x200b;&#x200b;&#x200b;&#x200b;&#x200b;407.459.1830</p>
<p>Job Type: Full-time</p>
<p>Pay: &#x24;131,243.38 - &#x24;158,056.53 per year</p>
<p>Benefits:</p>
<ul>
 <li>401(k)</li>
 <li>401(k) matching</li>
 <li>Dental insurance</li>
 <li>Employee assistance program</li>
 <li>Flexible spending account</li>
 <li>Health insurance</li>
 <li>Health savings account</li>
 <li>Life insurance</li>
 <li>Paid time off</li>
 <li>Professional development assistance</li>
 <li>Referral program</li>
 <li>Vision insurance</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>10 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>Monday to Friday</li>
</ul>
<p>Work Location: Remote</p>",,"5581f8a1bea3816b",,"Full-time",,"Remote","Senior Data Engineer","5 days ago","2023-10-20T11:50:35.075Z",,,"$131,243 - $158,057 a year","2023-10-25T11:50:35.078Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=5581f8a1bea3816b&from=jasx&tk=1hdjaluvpjc8r800&vjs=3"
"Definitive Logic","Definitive Logic is seeking a motivated senior Data Engineer IV to join our team supporting the US Air Force in merging real property and geospatial data to increase its visibility and actionability.
 
  
 
 
   The professional will be a full-time, remote employee that is part of an agile team that is responsible for analysis of existing application servers using data and tools from many sources. 
 
 
   is an exciting opportunity to be part of a dynamic team in a highly rewarding work environment that presents numerous training and professional growth opportunities.
 
 
 
   Roles and Responsibilities:
 
 
  Researches and integrates design strategies, product specifications, development schedules, for client applications
  Develops ETL jobs to bring data in from various source systems into Dataverse, both basic O365 Dataverse and expanded Enterprise Dataverse for use across Microsoft Power Platform applications
  Develops ETL jobs to federate data from DOD/Air Force Datalakes such as Advana and VAULT-IS
  Develops technical designs and specifications for complex data pipelines/data flows for customer/customers
  Identifies data quality issues and potential remediations for consideration by PM and/or customer stakeholders
  Identifies data gaps and potential remediation or integration activity for consideration by PM and/or customer
  Understands Department of Defense (DoD) data standards and management requirements
  Leads and influences team on project deliverables
  Drives quality assurance program for project deliverables
  Creates quality deliverables for customers
  Drives full life cycle of services/solution delivery for project(s)
  Provides technical leadership to lower-level engineer
 
 
   This is an exciting opportunity to be part of a dynamic team in a highly rewarding work environment that presents numerous training and professional growth opportunities.
 
  
  
  Required Qualifications
 
   8+ Years of Relevant Data Engineer Experience
   Bachelor's Degree, preferably in Engineering, Mathematics, or Business
 
  Preferred Qualifications
 
   Experience with the Dept of Defense
   Experience with Air Force Asset Management, Real Property, and Financial Data
   Data lifecycle experience using Dataverse as the storage backend
 
 
   About Definitive Logic
 
  
 
 
   Definitive Logic (DL) is a management and technology consulting firm known for delivering outcomes and ROI for agencies’ most complex business challenges. DL delivers performance-based and outcome-driven technology consulting solutions that directly support the strategic intent of our Defense, Homeland Security, Emergency Management, Federal Civilian and Commercial clients. We’re the preferred technology integration partner for Federal agencies to apply the best of data science, app dev, DevSecOps, cyber and cloud solutions to improve decision support, empower front-line employees and enhance back-office operations. We serve as trusted advisors providing objective, fact-based, vendor & technology-neutral consulting services.
 
  
 
 
   Definitive Logic is ultimately a team of problem solvers — thought leaders, domain experts, coders, data enthusiasts, and technophiles. Our exciting projects and learning and sharing culture have consistently resulted in validation as a Great Place to Work: 2023 Washington Post Top Workplaces (8-time winner) \u007C 2023 Virginia Best Places to Work (10 years running, #1 midsize in 2019).
 
 
 
   Definitive Logic is committed to hiring and retaining a diverse workforce. We are proud to be an Equal Opportunity/Affirmative Action Employer, making decisions without regard to race, color, religion, creed, sex, sexual orientation, gender identity, marital status, national origin, age, veteran status, disability, or any other protected class.
 
 
 
   If you are a qualified individual with a disability or a disabled veteran, you have the right to request a reasonable accommodation if you are unable, or limited in your ability, to use or access our Careers page: https://www.definitivelogic.com/careers/open-opportunities/ as a result of your disability. You can request a reasonable accommodation by sending an e-mail to Recruiting@DefinitiveLogic.com or via phone: 703-955-4186. In order to quickly respond to your request, please use the words ""Accommodation Request"" as your e-mail subject line.
 
 
 
   DL Benefits
 
 
   Health
 
 
   Dental
 
 
   Vision
 
 
   Life/AD&D: Company paid 
 
 
  STD/LTD:Company paid
 
 
   Supplemental Plans: TriCare Supplement, Pet Insurance through Nationwide, Legal Resources and hospital/accidental indemnity plans and Wellness initiatives.
 
  
 
 
   Compensation Benefits:
 
 
   Competitive Base Salary
 
 
   Annual performance based bonus
 
 
   401(k) & Roth option: You are fully (100%) vested on day 1 and DL matches up to 5%
 
 
   Spot Bonuses 
 
 
  Referral Bonuses
 
  
 
 
   Additional Benefits:
 
 
   Flexible Time Off (FTO): Under our FTO plan, there is no cap in the amount of leave you choose to take, with proper coordination and prior approval.
 
 
   Volunteer Hours: DL allocates up to 8 hours for you to use every year to volunteer for a 501c3 organization of your choice and DL will donate to that charity based on how many hours you volunteer.
 
 
   Cell Phone Reimbursement: $80/month
 
 
   Location Specific Metro/Parking 
 
 
  Tuition Reimbursement 
 
 
  Training & Certifications","<div>
 <div>
  Definitive Logic is seeking a motivated senior Data Engineer IV to join our team supporting the US Air Force in merging real property and geospatial data to increase its visibility and actionability.
 </div>
 <div> 
 </div>
 <div>
   The professional will be a full-time, remote employee that is part of an agile team that is responsible for analysis of existing application servers using data and tools from many sources. 
 </div>
 <div>
   is an exciting opportunity to be part of a dynamic team in a highly rewarding work environment that presents numerous training and professional growth opportunities.
 </div>
 <div></div>
 <div>
  <b><br> Roles and Responsibilities:</b>
 </div>
 <ul>
  <li>Researches and integrates design strategies, product specifications, development schedules, for client applications</li>
  <li>Develops ETL jobs to bring data in from various source systems into Dataverse, both basic O365 Dataverse and expanded Enterprise Dataverse for use across Microsoft Power Platform applications</li>
  <li>Develops ETL jobs to federate data from DOD/Air Force Datalakes such as Advana and VAULT-IS</li>
  <li>Develops technical designs and specifications for complex data pipelines/data flows for customer/customers</li>
  <li>Identifies data quality issues and potential remediations for consideration by PM and/or customer stakeholders</li>
  <li>Identifies data gaps and potential remediation or integration activity for consideration by PM and/or customer</li>
  <li>Understands Department of Defense (DoD) data standards and management requirements</li>
  <li>Leads and influences team on project deliverables</li>
  <li>Drives quality assurance program for project deliverables</li>
  <li>Creates quality deliverables for customers</li>
  <li>Drives full life cycle of services/solution delivery for project(s)</li>
  <li>Provides technical leadership to lower-level engineer</li>
 </ul>
 <div>
   This is an exciting opportunity to be part of a dynamic team in a highly rewarding work environment that presents numerous training and professional growth opportunities.
 </div>
 <br> 
 <div></div> 
 <h3 class=""jobSectionHeader""><b> Required Qualifications</b></h3>
 <ul>
  <li> 8+ Years of Relevant Data Engineer Experience</li>
  <li> Bachelor&apos;s Degree, preferably in Engineering, Mathematics, or Business</li>
 </ul>
 <h3 class=""jobSectionHeader""><b> Preferred Qualifications</b></h3>
 <ul>
  <li> Experience with the Dept of Defense</li>
  <li> Experience with Air Force Asset Management, Real Property, and Financial Data</li>
  <li> Data lifecycle experience using Dataverse as the storage backend</li>
 </ul>
 <div>
  <b> About Definitive Logic</b>
 </div>
 <div> 
 </div>
 <div>
   Definitive Logic (DL) is a management and technology consulting firm known for delivering outcomes and ROI for agencies&#x2019; most complex business challenges. DL delivers performance-based and outcome-driven technology consulting solutions that directly support the strategic intent of our Defense, Homeland Security, Emergency Management, Federal Civilian and Commercial clients. We&#x2019;re the preferred technology integration partner for Federal agencies to apply the best of data science, app dev, DevSecOps, cyber and cloud solutions to improve decision support, empower front-line employees and enhance back-office operations. We serve as trusted advisors providing objective, fact-based, vendor &amp; technology-neutral consulting services.
 </div>
 <div> 
 </div>
 <div>
   Definitive Logic is ultimately a team of problem solvers &#x2014; thought leaders, domain experts, coders, data enthusiasts, and technophiles. Our exciting projects and learning and sharing culture have consistently resulted in validation as a Great Place to Work: 2023 Washington Post Top Workplaces (8-time winner) \u007C 2023 Virginia Best Places to Work (10 years running, #1 midsize in 2019).
 </div>
 <div></div>
 <div>
  <br> Definitive Logic is committed to hiring and retaining a diverse workforce. We are proud to be an Equal Opportunity/Affirmative Action Employer, making decisions without regard to race, color, religion, creed, sex, sexual orientation, gender identity, marital status, national origin, age, veteran status, disability, or any other protected class.
 </div>
 <div></div>
 <div>
  <br> If you are a qualified individual with a disability or a disabled veteran, you have the right to request a reasonable accommodation if you are unable, or limited in your ability, to use or access our Careers page: https://www.definitivelogic.com/careers/open-opportunities/ as a result of your disability. You can request a reasonable accommodation by sending an e-mail to Recruiting@DefinitiveLogic.com or via phone: 703-955-4186. In order to quickly respond to your request, please use the words &quot;Accommodation Request&quot; as your e-mail subject line.
 </div>
 <div></div>
 <div>
  <b><br> DL Benefits</b>
 </div>
 <div>
  <b> Health</b>
 </div>
 <div>
  <b> Dental</b>
 </div>
 <div>
  <b> Vision</b>
 </div>
 <div>
  <b> Life/AD&amp;D: </b>Company paid 
 </div>
 <div>
  <b>STD/LTD:</b>Company paid
 </div>
 <div>
  <b> Supplemental Plans:</b> TriCare Supplement, Pet Insurance through Nationwide, Legal Resources and hospital/accidental indemnity plans and Wellness initiatives.
 </div>
 <div> 
 </div>
 <div>
  <b> Compensation Benefits:</b>
 </div>
 <div>
  <b> Competitive Base Salary</b>
 </div>
 <div>
  <b> Annual performance based bonus</b>
 </div>
 <div>
  <b> 401(k) &amp; Roth option: You are fully (100%) vested on day 1 and DL matches up to 5%</b>
 </div>
 <div>
  <b> Spot Bonuses </b>
 </div>
 <div>
  <b>Referral Bonuses</b>
 </div>
 <div> 
 </div>
 <div>
  <b> Additional Benefits:</b>
 </div>
 <div>
  <b> Flexible Time Off (FTO):</b> Under our FTO plan, there is no cap in the amount of leave you choose to take, with proper coordination and prior approval.
 </div>
 <div>
  <b> Volunteer Hours:</b> DL allocates up to 8 hours for you to use every year to volunteer for a 501c3 organization of your choice and DL will donate to that charity based on how many hours you volunteer.
 </div>
 <div>
  <b> Cell Phone Reimbursement:</b> &#x24;80/month
 </div>
 <div>
  <b> Location Specific Metro/Parking </b>
 </div>
 <div>
  <b>Tuition Reimbursement </b>
 </div>
 <div>
  <b>Training &amp; Certifications</b>
 </div>
</div>","https://jobs.lever.co/definitivelogic/06de73d0-53a1-4ddc-a95d-e8e89efc3e60?lever-source=Indeed","54bfae252724f784",,"Full-time",,"Arlington, VA 22202","Data Engineer IV","11 days ago","2023-10-14T11:50:36.592Z","4.4","10",,"2023-10-25T11:50:36.594Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=54bfae252724f784&from=jasx&tk=1hdjao0bljrpc800&vjs=3"
"CareFirst BlueCross BlueShield","Resp & Qualifications 
 PURPOSE:  The Lead Data Engineer is responsible for orchestrating, deploying, maintaining and scaling cloud OR on-premise infrastructure targeting big data and platform data management (Relational and NoSQL, distributed and converged) with emphasis on reliability, automation and performance. This role will focus on leading the development of solutions and helping transform the company's platforms deliver data-driven, meaningful insights and value to company.  ESSENTIAL FUNCTIONS:
 
   Lead the team to design, configure, implement, monitor, and manage all aspects of Data Integration Framework. Defines and develop the Data Integration best practices for the data management environment of optimal performance and reliability.
   Develops and maintains infrastructure systems (e.g., data warehouses, data lakes) including data access APIs. Prepares and manipulates data using Informatica, Snowflake, and Azure SQL.
   Provides detailed guidance and performs work related to Modeling Data Warehouse solutions in the cloud OR on-premise. Understands Dimensional Modeling, De-normalized Data Structures, OLAP, and Data Warehousing concepts.
   Oversees the delivery of engineering data initiatives and projects. Supports long term data initiatives as well as Ad-Hoc analysis and ELT/ETL activities. Creates data collection frameworks for structured and unstructured data. Applies data extraction, transformation and loading techniques in order to connect large data sets from a variety of sources.
   Enforces the implementation of best practices for data auditing, scalability, reliability and application performance. Develop and apply data extraction, transformation and loading techniques in order to connect large data sets from a variety of sources.
   Interprets data, analyzes results using statistical techniques, and provides ongoing reports. Executes quantitative analyses that translate data into actionable insights. Provides analytical and data-driven decision-making support for key projects. Designs, manages, and conducts quality control procedures for data sets using data from multiple systems.
   Improves data delivery engineering job knowledge by attending educational workshops; reviewing professional publications; establishing personal networks; benchmarking state-of-the-art practices; participating in professional societies.
 
  SUPERVISORY RESPONSIBILITY: Position does not have direct reports but is expected to assist in guiding and mentoring less experienced staff. May lead a team of matrixed resources.  QUALIFICATIONS:  Education Level: Bachelor's Degree in Computer Science, Information Technology or Engineering or related field OR in lieu of a Bachelor's degree, an additional 4 years of relevant work experience is required in addition to the required work experience.  Experience: 8 years Experience in leading data engineering and cross functional team to implement scalable and fine tuned ETL/ELT solutions for optimal performance. Experience developing and updating ETL/ELT scripts. Hands-on experience with application development, relational database layout, development, data modeling.  Knowledge, Skills and Abilities (KSAs)
 
   Knowledge and understanding of Informatica including Cloud version (IICS).
   Knowledge and understanding of Cloud Platforms (ie. Azure).
   Knowledge and understanding of Cloud Databases (ie. Snowflake, Azure SQL).
   Knowledge and understanding of at least one programming language (i.e., SQL, NoSQL, Python).
   Knowledge and understanding of database design and implementation concepts.
   Knowledge and understanding of data exchange formats.
   Knowledge and understanding of data movement concepts.
   Strong technical and analytical and problem solving skills to troubleshoot to solve a variety of problems.
   Requires strong organizational and communication skills, written and verbal, with the ability to handle multiple priorities.
   Able to effectively provide direction to and lead technical teams. Must be able to meet established deadlines and handle multiple customer service demands from internal and external customers, within set expectations for service excellence. Must be able to effectively communicate and provide positive customer service to every internal and external customer, including customers who may be demanding or otherwise challenging. 
 
  Salary Range: $105,408 - $209,352 
 Salary Range Disclaimer 
 The disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the work is being performed. This compensation range is specific and considers factors such as (but not limited to) the scope and responsibilites of the position, the candidate's work experience, education/training, internal peer equity, and market and business consideration. It is not typical for an individual to be hired at the top of the range, as compensation decisions depend on each case's facts and circumstances, including but not limited to experience, internal equity, and location. In addition to your compensation, CareFirst offers a comprehensive benefits package, various incentive programs/plans, and 401k contribution programs/plans (all benefits/incentives are subject to eligibility requirements).
  Department 
 Department: ODS/ETL Members
  Equal Employment Opportunity 
 CareFirst BlueCross BlueShield is an Equal Opportunity (EEO) employer. It is the policy of the Company to provide equal employment opportunities to all qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran or disabled status, or genetic information.
  Where To Apply 
 Please visit our website to apply: www.carefirst.com/careers
  Federal Disc/Physical Demand 
 Note: The incumbent is required to immediately disclose any debarment, exclusion, or other event that makes him/her ineligible to perform work directly or indirectly on Federal health care programs.
  PHYSICAL DEMANDS:
  The associate is primarily seated while performing the duties of the position. Occasional walking or standing is required. The hands are regularly used to write, type, key and handle or feel small controls and objects. The associate must frequently talk and hear. Weights up to 25 pounds are occasionally lifted.
  Sponsorship in US 
 Must be eligible to work in the U.S. without Sponsorship
  #LI-LD1","<div>
 <p><b>Resp &amp; Qualifications</b> </p>
 <p><b>PURPOSE: </b><br> The Lead Data Engineer is responsible for orchestrating, deploying, maintaining and scaling cloud OR on-premise infrastructure targeting big data and platform data management (Relational and NoSQL, distributed and converged) with emphasis on reliability, automation and performance. This role will focus on leading the development of solutions and helping transform the company&apos;s platforms deliver data-driven, meaningful insights and value to company.<br> <br> <b>ESSENTIAL FUNCTIONS:</b></p>
 <ul>
  <li> Lead the team to design, configure, implement, monitor, and manage all aspects of Data Integration Framework. Defines and develop the Data Integration best practices for the data management environment of optimal performance and reliability.</li>
  <li> Develops and maintains infrastructure systems (e.g., data warehouses, data lakes) including data access APIs. Prepares and manipulates data using Informatica, Snowflake, and Azure SQL.</li>
  <li> Provides detailed guidance and performs work related to Modeling Data Warehouse solutions in the cloud OR on-premise. Understands Dimensional Modeling, De-normalized Data Structures, OLAP, and Data Warehousing concepts.</li>
  <li> Oversees the delivery of engineering data initiatives and projects. Supports long term data initiatives as well as Ad-Hoc analysis and ELT/ETL activities. Creates data collection frameworks for structured and unstructured data. Applies data extraction, transformation and loading techniques in order to connect large data sets from a variety of sources.</li>
  <li> Enforces the implementation of best practices for data auditing, scalability, reliability and application performance. Develop and apply data extraction, transformation and loading techniques in order to connect large data sets from a variety of sources.</li>
  <li> Interprets data, analyzes results using statistical techniques, and provides ongoing reports. Executes quantitative analyses that translate data into actionable insights. Provides analytical and data-driven decision-making support for key projects. Designs, manages, and conducts quality control procedures for data sets using data from multiple systems.</li>
  <li> Improves data delivery engineering job knowledge by attending educational workshops; reviewing professional publications; establishing personal networks; benchmarking state-of-the-art practices; participating in professional societies.</li>
 </ul>
 <p><b><br> SUPERVISORY RESPONSIBILITY:</b><br> Position does not have direct reports but is expected to assist in guiding and mentoring less experienced staff. May lead a team of matrixed resources.<br> <br> <b>QUALIFICATIONS:</b><br> <br> <b>Education Level: </b>Bachelor&apos;s Degree in Computer Science, Information Technology or Engineering or related field OR in lieu of a Bachelor&apos;s degree, an additional 4 years of relevant work experience is required in addition to the required work experience.<br> <br> <b>Experience: </b>8 years Experience in leading data engineering and cross functional team to implement scalable and fine tuned ETL/ELT solutions for optimal performance. Experience developing and updating ETL/ELT scripts. Hands-on experience with application development, relational database layout, development, data modeling.<br> <br> <b>Knowledge, Skills and Abilities (KSAs)</b></p>
 <ul>
  <li> Knowledge and understanding of Informatica including Cloud version (IICS).</li>
  <li> Knowledge and understanding of Cloud Platforms (ie. Azure).</li>
  <li> Knowledge and understanding of Cloud Databases (ie. Snowflake, Azure SQL).</li>
  <li> Knowledge and understanding of at least one programming language (i.e., SQL, NoSQL, Python).</li>
  <li> Knowledge and understanding of database design and implementation concepts.</li>
  <li> Knowledge and understanding of data exchange formats.</li>
  <li> Knowledge and understanding of data movement concepts.</li>
  <li> Strong technical and analytical and problem solving skills to troubleshoot to solve a variety of problems.</li>
  <li> Requires strong organizational and communication skills, written and verbal, with the ability to handle multiple priorities.</li>
  <li> Able to effectively provide direction to and lead technical teams.</li> Must be able to meet established deadlines and handle multiple customer service demands from internal and external customers, within set expectations for service excellence. Must be able to effectively communicate and provide positive customer service to every internal and external customer, including customers who may be demanding or otherwise challenging. 
 </ul>
 <p><br> <b>Salary Range:</b> &#x24;105,408 - &#x24;209,352<br> </p>
 <p><b>Salary Range Disclaimer</b> </p>
 <p>The disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the work is being performed. This compensation range is specific and considers factors such as (but not limited to) the scope and responsibilites of the position, the candidate&apos;s work experience, education/training, internal peer equity, and market and business consideration. It is not typical for an individual to be hired at the top of the range, as compensation decisions depend on each case&apos;s facts and circumstances, including but not limited to experience, internal equity, and location. In addition to your compensation, CareFirst offers a comprehensive benefits package, various incentive programs/plans, and 401k contribution programs/plans (all benefits/incentives are subject to eligibility requirements).</p>
 <p><b> Department</b> </p>
 <p><b>Department: </b>ODS/ETL Members</p>
 <p><b> Equal Employment Opportunity</b> </p>
 <p>CareFirst BlueCross BlueShield is an Equal Opportunity (EEO) employer. It is the policy of the Company to provide equal employment opportunities to all qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran or disabled status, or genetic information.</p>
 <p><b> Where To Apply</b> </p>
 <p>Please visit our website to apply: www.carefirst.com/careers</p>
 <p><b> Federal Disc/Physical Demand</b> </p>
 <p>Note: The incumbent is required to immediately disclose any debarment, exclusion, or other event that makes him/her ineligible to perform work directly or indirectly on Federal health care programs.</p>
 <p><b> PHYSICAL DEMANDS:</b></p>
 <p> The associate is primarily seated while performing the duties of the position. Occasional walking or standing is required. The hands are regularly used to write, type, key and handle or feel small controls and objects. The associate must frequently talk and hear. Weights up to 25 pounds are occasionally lifted.</p>
 <p><b> Sponsorship in US</b> </p>
 <p>Must be eligible to work in the U.S. without Sponsorship</p>
 <p> #LI-LD1</p>
</div>","https://carefirstcareers.ttcportals.com/jobs/13495531-lead-data-engineer-remote?tm_job=18952-1A&tm_event=view&tm_company=2380","227119efc149cadc",,"Full-time",,"840 1st Street NE, Washington, DC 20002","Lead Data Engineer (Remote)","6 days ago","2023-10-19T11:50:33.385Z","3.8","732","$105,408 - $209,352 a year","2023-10-25T11:50:33.387Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=227119efc149cadc&from=jasx&tk=1hdjannibirm6800&vjs=3"
"OpenEarth Foundation","Lead Data Engineer:
Building Climate Solutions for Cities
Link to application: https://noteforms.com/forms/openearth-job-application-ctwllg
Have you created a successful career in tech and are ready to do something good with your skills and experience? If yes, then join us in our Earthshot mission to build open source digital systems and solutions to battle environmental threats.
Open Earth Foundation is a California-based research and deployment non-profit developing open innovative technology to increase planetary resilience and avoid a catastrophic climate crisis.
We are building open infrastructure in support of the UNFCCC Paris Agreement, solutions in climate finance, biodiversity tracking, community empowerment and other critical problems and solutions.
We are a diverse international team, working remotely with a network of collaborators and partners globally, from the United Nations to climate tech startups.
We have funding and a team of experts focused on Earth systems and digital innovation.
Your mission, should you choose to accept it:
As a lead data engineer, you will leverage your data and engineering expertise and analytical skills to build open digital infrastructure, with a particular focus on our newly launched CityCatalyst project.
As a Lead Data Engineer you will provide leadership and hands-on data management in our climate accounting technology programs. Your work will cover climate data pipelines, infrastructure design and implementation, working on greenhouse gas data inventories from satellite imagery and diverse data sources, being part of a software production team and even interfacing with new AI and LLM tools for data harmonization.
You will be part of the technical team and work directly with the Director of Open Technology. You will also work with a variety of team members in the organization from the Product Team and Community Manager.
The position is remote and international candidates are welcome, but prefer those willing to work with time zones compatible with PST. We're working on the planet's problems and we need the planet's best people to fix them.
The following requirements describe our ideal candidate. If you don't meet some of the requirements, you're encouraged to apply anyway. Let us know if there is something missing and how we can work together to make it up.
Essential Functions and Specific Duties:

 Design, architect, build and maintain data pipeline systems
 Write code for importing and updating large datasets to relational database and search indexes
 Define and maintain database schemas and data file formats
 Collaborate with web developers on optimizing database schemas for APIs and Web applications
 Collaborate with a team of software engineering peers
 Mentor and guide more junior data engineering staff
 Define and maintain data management processes for the organization
 Work with product managers to develop schedules, estimate tasks, and define success criteria
 Collaborate with team members from other disciplines such as web development, design, product management, and devops
 Coordinate with Open Source contributors
 Coordinate with open standards community to define interoperability standards
 Actively participate in team building and culture development activities at Open Earth Foundation
 Other duties as assigned

Required skills:

 Python programming focused on big data management
 PostgreSQL or other relational database
 Docker
 Kubernetes
 Git

Optional skills that will make a candidate stand out:

 Generative AI and large language model (LLM) APIs and data applications
 GIS tools such as ESRI
 Amazon Web Services
 ElasticSearch
 Data pipeline tools, e.g. Pachyderm
 Experience with 100Gb or larger data sets
 Climate action data such as emissions, targets, and action plans
 Physical (lat, lon, alt) and political (city, state, country) geographical data
 Remote-sensing and satellite data
 RESTful Web APIs
 Engineering leadership
 Open Source project maintainership
 Machine learning, especially for anomaly detection, pattern recognition, clustering, time series analysis

Qualifications:

 Bachelor’s degree in computer science, electrical engineering, or equivalent technical or scientific degree, or equivalent on-the-job experience
 5 years of experience in software development for data systems
 3 shipped projects

Interpersonal skills:

 Clear communicator with good verbal and written skills in English (additional languages a plus)
 Creative, flexible and efficient with a focus on details
 Capacity to work with team members and partners at all levels and across time zones in a highly collaborative and often remote environment.
 Ability to embrace new challenges, take ownership and initiative as a key team player.

Compensation and benefits

 This position is full-time with compensation of $60,000-$105,000 /year, dependent on experience and location
 Open Earth offers unlimited paid time off, paid holidays and paid sick leave
 You will work remotely within a dynamic and international environment
 We celebrate our achievements during our annual team retreat

OEF is an Equal Employment Opportunity Employer. We support diversity, equity and inclusion in teams, and believe people should align their work with their purpose. Join us if you love Earth.
Please apply by submitting your resume AND a cover letter, it is your opportunity to highlight why you feel you would be a great addition to the team.
We look forward to hearing from you!
Open Earth Foundation seeks diverse applicants from underrepresented communities. If you would like to pursue this job opportunity but don’t believe you meet all the requirements, please apply and note what’s missing in your cover letter.Lead Data Engineer:
Building Climate Solutions for Cities
Link to application: https://noteforms.com/forms/openearth-job-application-ctwllg
Have you created a successful career in tech and are ready to do something good with your skills and experience? If yes, then join us in our Earthshot mission to build open source digital systems and solutions to battle environmental threats.
Open Earth Foundation is a California-based research and deployment non-profit developing open innovative technology to increase planetary resilience and avoid a catastrophic climate crisis.
We are building open infrastructure in support of the UNFCCC Paris Agreement, solutions in climate finance, biodiversity tracking, community empowerment and other critical problems and solutions.
We are a diverse international team, working remotely with a network of collaborators and partners globally, from the United Nations to climate tech startups.
We have funding and a team of experts focused on Earth systems and digital innovation.
Your mission, should you choose to accept it:
As a lead data engineer, you will leverage your data and engineering expertise and analytical skills to build open digital infrastructure, with a particular focus on our newly launched CityCatalyst project.
As a Lead Data Engineer you will provide leadership and hands-on data management in our climate accounting technology programs. Your work will cover climate data pipelines, infrastructure design and implementation, working on greenhouse gas data inventories from satellite imagery and diverse data sources, being part of a software production team and even interfacing with new AI and LLM tools for data harmonization.
You will be part of the technical team and work directly with the Director of Open Technology. You will also work with a variety of team members in the organization from the Product Team and Community Manager.
The position is remote and international candidates are welcome, but prefer those willing to work with time zones compatible with PST. We're working on the planet's problems and we need the planet's best people to fix them.
The following requirements describe our ideal candidate. If you don't meet some of the requirements, you're encouraged to apply anyway. Let us know if there is something missing and how we can work together to make it up.
Essential Functions and Specific Duties:

 Design, architect, build and maintain data pipeline systems
 Write code for importing and updating large datasets to relational database and search indexes
 Define and maintain database schemas and data file formats
 Collaborate with web developers on optimizing database schemas for APIs and Web applications
 Collaborate with a team of software engineering peers
 Mentor and guide more junior data engineering staff
 Define and maintain data management processes for the organization
 Work with product managers to develop schedules, estimate tasks, and define success criteria
 Collaborate with team members from other disciplines such as web development, design, product management, and devops
 Coordinate with Open Source contributors
 Coordinate with open standards community to define interoperability standards
 Actively participate in team building and culture development activities at Open Earth Foundation
 Other duties as assigned

Required skills:

 Python programming focused on big data management
 PostgreSQL or other relational database
 Docker
 Kubernetes
 Git

Optional skills that will make a candidate stand out:

 Generative AI and large language model (LLM) APIs and data applications
 GIS tools such as ESRI
 Amazon Web Services
 ElasticSearch
 Data pipeline tools, e.g. Pachyderm
 Experience with 100Gb or larger data sets
 Climate action data such as emissions, targets, and action plans
 Physical (lat, lon, alt) and political (city, state, country) geographical data
 Remote-sensing and satellite data
 RESTful Web APIs
 Engineering leadership
 Open Source project maintainership
 Machine learning, especially for anomaly detection, pattern recognition, clustering, time series analysis

Qualifications:

 Bachelor’s degree in computer science, electrical engineering, or equivalent technical or scientific degree, or equivalent on-the-job experience
 5 years of experience in software development for data systems
 3 shipped projects

Interpersonal skills:

 Clear communicator with good verbal and written skills in English (additional languages a plus)
 Creative, flexible and efficient with a focus on details
 Capacity to work with team members and partners at all levels and across time zones in a highly collaborative and often remote environment.
 Ability to embrace new challenges, take ownership and initiative as a key team player.

Compensation and benefits

 This position is full-time with compensation of $60,000-$105,000 /year, dependent on experience and location
 Open Earth offers unlimited paid time off, paid holidays and paid sick leave
 You will work remotely within a dynamic and international environment
 We celebrate our achievements during our annual team retreat

OEF is an Equal Employment Opportunity Employer. We support diversity, equity and inclusion in teams, and believe people should align their work with their purpose. Join us if you love Earth.
Please apply by submitting your resume AND a cover letter, it is your opportunity to highlight why you feel you would be a great addition to the team.
We look forward to hearing from you!
Open Earth Foundation seeks diverse applicants from underrepresented communities. If you would like to pursue this job opportunity but don’t believe you meet all the requirements, please apply and note what’s missing in your cover letter.
Job Type: Full-time
Pay: $60,000.00 - $105,000.00 per year
Benefits:

 401(k)
 Dental insurance
 Flexible schedule
 Health insurance
 Paid time off
 Vision insurance

Compensation package:

 Bonus opportunities
 Yearly pay

Experience level:

 3 years
 4 years
 5 years
 6 years
 7 years
 8 years

Schedule:

 8 hour shift
 Monday to Friday

Experience:

 Informatica: 1 year (Preferred)
 SQL: 1 year (Preferred)
 Data warehouse: 1 year (Preferred)

Work Location: Remote","<p><b>Lead Data Engineer:</b></p>
<p><b>Building Climate Solutions for Cities</b></p>
<p>Link to application: https://noteforms.com/forms/openearth-job-application-ctwllg</p>
<p><i><b>Have you created a successful career in tech and are ready to do something good with your skills and experience? If yes, then join us in our Earthshot mission to build open source digital systems and solutions to battle environmental threats.</b></i></p>
<p><i>Open Earth Foundation is a California-based research and deployment non-profit developing open innovative technology to increase planetary resilience and avoid a catastrophic climate crisis.</i></p>
<p><i>We are building open infrastructure in support of the UNFCCC Paris Agreement, solutions in climate finance, biodiversity tracking, community empowerment and other critical problems and solutions.</i></p>
<p><i>We are a diverse international team, working remotely with a network of collaborators and partners globally, from the United Nations to climate tech startups.</i></p>
<p><i>We have funding and a team of experts focused on Earth systems and digital innovation.</i></p>
<p><i><b>Your mission, should you choose to accept it:</b></i></p>
<p>As a <b>lead data engineer</b>, you will leverage your data and engineering expertise and analytical skills to build open digital infrastructure, with a particular focus on our newly launched CityCatalyst project.</p>
<p>As a Lead Data Engineer you will provide leadership and hands-on data management in our climate accounting technology programs. Your work will cover climate data pipelines, infrastructure design and implementation, working on greenhouse gas data inventories from satellite imagery and diverse data sources, being part of a software production team and even interfacing with new AI and LLM tools for data harmonization.</p>
<p>You will be part of the technical team and work directly with the Director of Open Technology. You will also work with a variety of team members in the organization from the Product Team and Community Manager.</p>
<p>The position is remote and international candidates are welcome, but prefer those willing to work with time zones compatible with PST. We&apos;re working on the planet&apos;s problems and we need the planet&apos;s best people to fix them.</p>
<p><i>The following requirements describe our ideal candidate. If you don&apos;t meet some of the requirements, you&apos;re encouraged to apply anyway. Let us know if there is something missing and how we can work together to make it up.</i></p>
<p><b>Essential Functions and Specific Duties:</b></p>
<ul>
 <li>Design, architect, build and maintain data pipeline systems</li>
 <li>Write code for importing and updating large datasets to relational database and search indexes</li>
 <li>Define and maintain database schemas and data file formats</li>
 <li>Collaborate with web developers on optimizing database schemas for APIs and Web applications</li>
 <li>Collaborate with a team of software engineering peers</li>
 <li>Mentor and guide more junior data engineering staff</li>
 <li>Define and maintain data management processes for the organization</li>
 <li>Work with product managers to develop schedules, estimate tasks, and define success criteria</li>
 <li>Collaborate with team members from other disciplines such as web development, design, product management, and devops</li>
 <li>Coordinate with Open Source contributors</li>
 <li>Coordinate with open standards community to define interoperability standards</li>
 <li>Actively participate in team building and culture development activities at Open Earth Foundation</li>
 <li>Other duties as assigned</li>
</ul>
<p><b>Required skills:</b></p>
<ul>
 <li>Python programming focused on big data management</li>
 <li>PostgreSQL or other relational database</li>
 <li>Docker</li>
 <li>Kubernetes</li>
 <li>Git</li>
</ul>
<p><b>Optional skills that will make a candidate stand out:</b></p>
<ul>
 <li>Generative AI and large language model (LLM) APIs and data applications</li>
 <li>GIS tools such as ESRI</li>
 <li>Amazon Web Services</li>
 <li>ElasticSearch</li>
 <li>Data pipeline tools, e.g. Pachyderm</li>
 <li>Experience with 100Gb or larger data sets</li>
 <li>Climate action data such as emissions, targets, and action plans</li>
 <li>Physical (lat, lon, alt) and political (city, state, country) geographical data</li>
 <li>Remote-sensing and satellite data</li>
 <li>RESTful Web APIs</li>
 <li>Engineering leadership</li>
 <li>Open Source project maintainership</li>
 <li>Machine learning, especially for anomaly detection, pattern recognition, clustering, time series analysis</li>
</ul>
<p><b>Qualifications:</b></p>
<ul>
 <li>Bachelor&#x2019;s degree in computer science, electrical engineering, or equivalent technical or scientific degree, or equivalent on-the-job experience</li>
 <li>5 years of experience in software development for data systems</li>
 <li>3 shipped projects</li>
</ul>
<p><b>Interpersonal skills:</b></p>
<ul>
 <li>Clear communicator with good verbal and written skills in English (additional languages a plus)</li>
 <li>Creative, flexible and efficient with a focus on details</li>
 <li>Capacity to work with team members and partners at all levels and across time zones in a highly collaborative and often remote environment.</li>
 <li>Ability to embrace new challenges, take ownership and initiative as a key team player.</li>
</ul>
<p><i><b>Compensation and benefits</b></i></p>
<ul>
 <li>This position is full-time with compensation of &#x24;60,000-&#x24;105,000 /year, dependent on experience and location</li>
 <li>Open Earth offers unlimited paid time off, paid holidays and paid sick leave</li>
 <li>You will work remotely within a dynamic and international environment</li>
 <li>We celebrate our achievements during our annual team retreat</li>
</ul>
<p><i>OEF is an Equal Employment Opportunity Employer. We support diversity, equity and inclusion in teams, and believe people should align their work with their purpose. Join us if you love Earth.</i></p>
<p><i>Please apply by submitting your resume AND a cover letter, it is your opportunity to highlight why you feel you would be a great addition to the team.</i></p>
<p><i>We look forward to hearing from you!</i></p>
<p><i>Open Earth Foundation seeks diverse applicants from underrepresented communities. If you would like to pursue this job opportunity but don&#x2019;t believe you meet all the requirements, please apply and note what&#x2019;s missing in your cover letter.</i><b>Lead Data Engineer:</b></p>
<p><b>Building Climate Solutions for Cities</b></p>
<p>Link to application: https://noteforms.com/forms/openearth-job-application-ctwllg</p>
<p><i><b>Have you created a successful career in tech and are ready to do something good with your skills and experience? If yes, then join us in our Earthshot mission to build open source digital systems and solutions to battle environmental threats.</b></i></p>
<p><i>Open Earth Foundation is a California-based research and deployment non-profit developing open innovative technology to increase planetary resilience and avoid a catastrophic climate crisis.</i></p>
<p><i>We are building open infrastructure in support of the UNFCCC Paris Agreement, solutions in climate finance, biodiversity tracking, community empowerment and other critical problems and solutions.</i></p>
<p><i>We are a diverse international team, working remotely with a network of collaborators and partners globally, from the United Nations to climate tech startups.</i></p>
<p><i>We have funding and a team of experts focused on Earth systems and digital innovation.</i></p>
<p><i><b>Your mission, should you choose to accept it:</b></i></p>
<p>As a <b>lead data engineer</b>, you will leverage your data and engineering expertise and analytical skills to build open digital infrastructure, with a particular focus on our newly launched CityCatalyst project.</p>
<p>As a Lead Data Engineer you will provide leadership and hands-on data management in our climate accounting technology programs. Your work will cover climate data pipelines, infrastructure design and implementation, working on greenhouse gas data inventories from satellite imagery and diverse data sources, being part of a software production team and even interfacing with new AI and LLM tools for data harmonization.</p>
<p>You will be part of the technical team and work directly with the Director of Open Technology. You will also work with a variety of team members in the organization from the Product Team and Community Manager.</p>
<p>The position is remote and international candidates are welcome, but prefer those willing to work with time zones compatible with PST. We&apos;re working on the planet&apos;s problems and we need the planet&apos;s best people to fix them.</p>
<p><i>The following requirements describe our ideal candidate. If you don&apos;t meet some of the requirements, you&apos;re encouraged to apply anyway. Let us know if there is something missing and how we can work together to make it up.</i></p>
<p><b>Essential Functions and Specific Duties:</b></p>
<ul>
 <li>Design, architect, build and maintain data pipeline systems</li>
 <li>Write code for importing and updating large datasets to relational database and search indexes</li>
 <li>Define and maintain database schemas and data file formats</li>
 <li>Collaborate with web developers on optimizing database schemas for APIs and Web applications</li>
 <li>Collaborate with a team of software engineering peers</li>
 <li>Mentor and guide more junior data engineering staff</li>
 <li>Define and maintain data management processes for the organization</li>
 <li>Work with product managers to develop schedules, estimate tasks, and define success criteria</li>
 <li>Collaborate with team members from other disciplines such as web development, design, product management, and devops</li>
 <li>Coordinate with Open Source contributors</li>
 <li>Coordinate with open standards community to define interoperability standards</li>
 <li>Actively participate in team building and culture development activities at Open Earth Foundation</li>
 <li>Other duties as assigned</li>
</ul>
<p><b>Required skills:</b></p>
<ul>
 <li>Python programming focused on big data management</li>
 <li>PostgreSQL or other relational database</li>
 <li>Docker</li>
 <li>Kubernetes</li>
 <li>Git</li>
</ul>
<p><b>Optional skills that will make a candidate stand out:</b></p>
<ul>
 <li>Generative AI and large language model (LLM) APIs and data applications</li>
 <li>GIS tools such as ESRI</li>
 <li>Amazon Web Services</li>
 <li>ElasticSearch</li>
 <li>Data pipeline tools, e.g. Pachyderm</li>
 <li>Experience with 100Gb or larger data sets</li>
 <li>Climate action data such as emissions, targets, and action plans</li>
 <li>Physical (lat, lon, alt) and political (city, state, country) geographical data</li>
 <li>Remote-sensing and satellite data</li>
 <li>RESTful Web APIs</li>
 <li>Engineering leadership</li>
 <li>Open Source project maintainership</li>
 <li>Machine learning, especially for anomaly detection, pattern recognition, clustering, time series analysis</li>
</ul>
<p><b>Qualifications:</b></p>
<ul>
 <li>Bachelor&#x2019;s degree in computer science, electrical engineering, or equivalent technical or scientific degree, or equivalent on-the-job experience</li>
 <li>5 years of experience in software development for data systems</li>
 <li>3 shipped projects</li>
</ul>
<p><b>Interpersonal skills:</b></p>
<ul>
 <li>Clear communicator with good verbal and written skills in English (additional languages a plus)</li>
 <li>Creative, flexible and efficient with a focus on details</li>
 <li>Capacity to work with team members and partners at all levels and across time zones in a highly collaborative and often remote environment.</li>
 <li>Ability to embrace new challenges, take ownership and initiative as a key team player.</li>
</ul>
<p><i><b>Compensation and benefits</b></i></p>
<ul>
 <li>This position is full-time with compensation of &#x24;60,000-&#x24;105,000 /year, dependent on experience and location</li>
 <li>Open Earth offers unlimited paid time off, paid holidays and paid sick leave</li>
 <li>You will work remotely within a dynamic and international environment</li>
 <li>We celebrate our achievements during our annual team retreat</li>
</ul>
<p><i>OEF is an Equal Employment Opportunity Employer. We support diversity, equity and inclusion in teams, and believe people should align their work with their purpose. Join us if you love Earth.</i></p>
<p><i>Please apply by submitting your resume AND a cover letter, it is your opportunity to highlight why you feel you would be a great addition to the team.</i></p>
<p><i>We look forward to hearing from you!</i></p>
<p><i>Open Earth Foundation seeks diverse applicants from underrepresented communities. If you would like to pursue this job opportunity but don&#x2019;t believe you meet all the requirements, please apply and note what&#x2019;s missing in your cover letter.</i></p>
<p>Job Type: Full-time</p>
<p>Pay: &#x24;60,000.00 - &#x24;105,000.00 per year</p>
<p>Benefits:</p>
<ul>
 <li>401(k)</li>
 <li>Dental insurance</li>
 <li>Flexible schedule</li>
 <li>Health insurance</li>
 <li>Paid time off</li>
 <li>Vision insurance</li>
</ul>
<p>Compensation package:</p>
<ul>
 <li>Bonus opportunities</li>
 <li>Yearly pay</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>3 years</li>
 <li>4 years</li>
 <li>5 years</li>
 <li>6 years</li>
 <li>7 years</li>
 <li>8 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
 <li>Monday to Friday</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Informatica: 1 year (Preferred)</li>
 <li>SQL: 1 year (Preferred)</li>
 <li>Data warehouse: 1 year (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,"37b5edb36e885430",,"Full-time",,"Remote","Lead Data Engineer","7 days ago","2023-10-18T11:50:46.664Z",,,"$60,000 - $105,000 a year","2023-10-25T11:50:46.666Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=37b5edb36e885430&from=jasx&tk=1hdjao0bljrpc800&vjs=3"
"Reality Defender","** No firms - we cannot work with firms due to regulatory reasons.**
 
 
 
   Reality Defender seeks a forward deployed data engineer to join the Data Engineering team. You would work on product-oriented data infrastructure development for in-the-wild deepfake media detection, with an emphasis on engaging directly with clients and facilitating communication between technical and non-technical teams.
 
 
 
   #LI-Remote
  
 Responsibilities
 
   Building scalable robust infrastructure for data ingestion, storage, and sampling.
   Communicate complex technical concepts effectively to client executives, ensuring alignment between technical implementations and organizational objectives.
   Develop custom data tools tailored to meet specific client requirements, including adapting internally developed solutions to meet clients' needs.
   Create and maintain comprehensive technical documentation, including APIs, algorithms, and system architecture.
   Provide technical support to resolve complex issues escalated from customer support teams. Collaborate with cross-functional teams to diagnose and troubleshoot production incidents, and report results back to the customer in clear non-technical language.
 
  Requirements
 
   We encourage candidates who may not meet all the specified requirements to still apply. We value diverse perspectives and skills, and believe that unique experiences can contribute significantly to our team. If you are passionate about the role and confident in your ability to make a meaningful impact, we welcome your application. Your enthusiasm, adaptability, and potential for growth are equally important to us. Please use your cover letter to elaborate on how your background and experience make you an ideal fit for this role!
 
 
 
   Required
 
 
   3+ years of professional experience in software/data science and a bachelor's or master's degree in computer science, engineering, math, or STEM discipline.
   Strong communication skills.
   Proficiency in Python, NodeJs, Typescript, with a strong emphasis on adapting scalable software solutions to customer needs.
   Database experience, particularly NoSQL databases (MongoDB, DynamoDB, etc).
 
 
 
   Nice to have
 
 
   Interest in data exploration, visualization, cleaning, and analytics for real-world data modeling.
   Solid understanding of linear algebra, statistics and deep learning concepts.
   Experience working with audio, visual, and/or text datasets and models.
   Experience with AWS, Google Cloud, Azure, and On-Premises.
   Experience working with very large databases and deep learning APIs, including Pandas, PyTorch, PySpark, etc. 
  Highly organized, detail-oriented, and possess a proven ability to thrive under deadline pressure.
 
 
 
   Additional Requirements
 
 
   Willing to work extended hours when needed.
   Willing to occasionally work from or travel to client’s location.","<div>
 <div>
  <b>** No firms - we cannot work with firms due to regulatory reasons.**</b>
 </div>
 <div></div>
 <div>
  <br> Reality Defender seeks a forward deployed data engineer to join the Data Engineering team. You would work on product-oriented data infrastructure development for in-the-wild deepfake media detection, with an emphasis on engaging directly with clients and facilitating communication between technical and non-technical teams.
 </div>
 <div></div>
 <div>
  <br> #LI-Remote
 </div> 
 <h3 class=""jobSectionHeader""><b>Responsibilities</b></h3>
 <ul>
  <li> Building scalable robust infrastructure for data ingestion, storage, and sampling.</li>
  <li> Communicate complex technical concepts effectively to client executives, ensuring alignment between technical implementations and organizational objectives.</li>
  <li> Develop custom data tools tailored to meet specific client requirements, including adapting internally developed solutions to meet clients&apos; needs.</li>
  <li> Create and maintain comprehensive technical documentation, including APIs, algorithms, and system architecture.</li>
  <li> Provide technical support to resolve complex issues escalated from customer support teams. Collaborate with cross-functional teams to diagnose and troubleshoot production incidents, and report results back to the customer in clear non-technical language.</li>
 </ul>
 <h3 class=""jobSectionHeader""><b> Requirements</b></h3>
 <ul>
  <li> We encourage candidates who may not meet all the specified requirements to still apply. We value diverse perspectives and skills, and believe that unique experiences can contribute significantly to our team. If you are passionate about the role and confident in your ability to make a meaningful impact, we welcome your application. Your enthusiasm, adaptability, and potential for growth are equally important to us. Please use your cover letter to elaborate on how your background and experience make you an ideal fit for this role!</li>
 </ul>
 <div></div>
 <div>
  <b><br> Required</b>
 </div>
 <ul>
  <li> 3+ years of professional experience in software/data science and a bachelor&apos;s or master&apos;s degree in computer science, engineering, math, or STEM discipline.</li>
  <li> Strong communication skills.</li>
  <li> Proficiency in Python, NodeJs, Typescript, with a strong emphasis on adapting scalable software solutions to customer needs.</li>
  <li> Database experience, particularly NoSQL databases (MongoDB, DynamoDB, etc).</li>
 </ul>
 <div></div>
 <div>
  <b><br> Nice to have</b>
 </div>
 <ul>
  <li> Interest in data exploration, visualization, cleaning, and analytics for real-world data modeling.</li>
  <li> Solid understanding of linear algebra, statistics and deep learning concepts.</li>
  <li> Experience working with audio, visual, and/or text datasets and models.</li>
  <li> Experience with AWS, Google Cloud, Azure, and On-Premises.</li>
  <li> Experience working with very large databases and deep learning APIs, including Pandas, PyTorch, PySpark, etc. </li>
  <li>Highly organized, detail-oriented, and possess a proven ability to thrive under deadline pressure.</li>
 </ul>
 <div></div>
 <div>
  <b><br> Additional Requirements</b>
 </div>
 <ul>
  <li> Willing to work extended hours when needed.</li>
  <li> Willing to occasionally work from or travel to client&#x2019;s location.</li>
 </ul>
</div>","https://jobs.lever.co/realitydefender/73112d98-7a9a-4800-9921-6576fdd1f0df?lever-source=Indeed","4eb3da80458c4c7f",,"Full-time",,"New York, NY","Forward Deployed Data Engineer - Remote","7 days ago","2023-10-18T11:50:43.183Z",,,"$100,000 - $200,000 a year","2023-10-25T11:50:43.184Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=4eb3da80458c4c7f&from=jasx&tk=1hdjao0bljrpc800&vjs=3"
"Enigma","The Opportunity: 
  Join Enigma at a pivotal moment as we continue to provide valuable solutions for small businesses. We're seeking an experienced Data Product Engineer to join our team and help develop and build the iteration of small business data products . Your work will directly impact the accuracy of small business profiles, which influence decisions for companies that employ half the U.S. workforce! 
  The Role: 
  As a Data Product Engineer, you will design and develop data products that solve critical customer pain points. Your impact will be measured by your ability to deliver scalable, high-quality data products that customers love. To succeed in this role, you will bring together three distinct capabilities 
  
  Understand acute customer needs and extract common problem structures across customers 
  Analyze and extract value from data at scale 
  Build efficient, maintainable production-grade data pipelines 
  
 We are looking for someone who: 
  
  Operates with a bias for action and knows how to deliver value in the short, medium and long term 
  Loves talking to customers and works hard to solve their problems in a repeatable way 
  Adopts a principled, metrics-driven approach to difficult data problems and demonstrates excellence in their analytics and engineering craft 
  Operates transparently, collaboratively and with low ego—loves learning from others and having their ideas questioned and challenged 
  
 What Makes This Job Exciting: 
  
  Impact: Develop products that take an innovative data-first approach to solving high-value customer problems. 
  Technical Challenge: Tackle complex data and engineering problems while balancing customer impact, reliability, scalability, data quality, and an ambitious forward development plan. 
  Ownership: You'll work directly with customers. You and your teammates will design and build products based on your learnings . 
  
 Bonus Points If You: 
  
  Have experience building data products at scale. 
  Bring prior experience in Databricks or Spark 
  Have worked on data products in the marketing, kyb or credit underwriting space. 
  
 About Us: 
  At Enigma, we're building the single, most reliable source of data on businesses to power the future of financial services. By engineering better data from hundreds of public and third-party sources, we aim to tell the complete story of every business, so that companies of every size can access the financial services they need to grow and thrive. Our core values – generosity, curiosity, ingenuity, & drive – guide everything we do, from how we make our most important product decisions to how we work with and support one another on a daily basis. We're a team of curious, driven individuals with diverse backgrounds and skills, but we're all passionate about engineering deeper understanding through data—together. If this resonates, we would love to hear from you! 
  We are proud to be an equal opportunity workplace and an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. 
  Salary Range: $160,000-$210,000 
  A note on salary ranges: we pride ourselves on paying competitively for our size and industry. Salary is one piece of a total Enigma compensation package that includes additional benefits and opportunities. All of our compensation packages include equity because we believe 100% of Enigma employees should have the option to purchase ownership in the company and benefit from the value we're creating together","<div>
 <p><b>The Opportunity:</b></p> 
 <p> Join Enigma at a pivotal moment as we continue to provide valuable solutions for small businesses. We&apos;re seeking an experienced Data Product Engineer to join our team and help develop and build the iteration of small business data products . Your work will directly impact the accuracy of small business profiles, which influence decisions for companies that employ half the U.S. workforce!</p> 
 <p><b> The Role:</b></p> 
 <p> As a Data Product Engineer, you will design and develop data products that solve critical customer pain points. Your impact will be measured by your ability to deliver scalable, high-quality data products that customers love. To succeed in this role, you will bring together three distinct capabilities</p> 
 <ul> 
  <li>Understand acute customer needs and extract common problem structures across customers</li> 
  <li>Analyze and extract value from data at scale</li> 
  <li>Build efficient, maintainable production-grade data pipelines</li> 
 </ul> 
 <p><b>We are looking for someone who:</b></p> 
 <ul> 
  <li>Operates with a bias for action and knows how to deliver value in the short, medium and long term</li> 
  <li>Loves talking to customers and works hard to solve their problems in a repeatable way</li> 
  <li>Adopts a principled, metrics-driven approach to difficult data problems and demonstrates excellence in their analytics and engineering craft</li> 
  <li>Operates transparently, collaboratively and with low ego&#x2014;loves learning from others and having their ideas questioned and challenged</li> 
 </ul> 
 <p><b>What Makes This Job Exciting:</b></p> 
 <ul> 
  <li>Impact: Develop products that take an innovative data-first approach to solving high-value customer problems.</li> 
  <li>Technical Challenge: Tackle complex data and engineering problems while balancing customer impact, reliability, scalability, data quality, and an ambitious forward development plan.</li> 
  <li>Ownership: You&apos;ll work directly with customers. You and your teammates will design and build products based on your learnings .</li> 
 </ul> 
 <p><b>Bonus Points If You:</b></p> 
 <ul> 
  <li>Have experience building data products at scale.</li> 
  <li>Bring prior experience in Databricks or Spark</li> 
  <li>Have worked on data products in the marketing, kyb or credit underwriting space.</li> 
 </ul> 
 <p><b>About Us:</b></p> 
 <p> At Enigma, we&apos;re building the single, most reliable source of data on businesses to power the future of financial services. By engineering better data from hundreds of public and third-party sources, we aim to tell the complete story of every business, so that companies of every size can access the financial services they need to grow and thrive. Our core values &#x2013; generosity, curiosity, ingenuity, &amp; drive &#x2013; guide everything we do, from how we make our most important product decisions to how we work with and support one another on a daily basis. We&apos;re a team of curious, driven individuals with diverse backgrounds and skills, but we&apos;re all passionate about engineering deeper understanding through data&#x2014;together. If this resonates, we would love to hear from you!</p> 
 <p> We are proud to be an equal opportunity workplace and an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status.</p> 
 <p><i> Salary Range: &#x24;160,000-&#x24;210,000</i></p> 
 <p><i> A note on salary ranges: we pride ourselves on paying competitively for our size and industry. Salary is one piece of a total Enigma compensation package that includes additional benefits and opportunities. All of our compensation packages include equity because we believe 100% of Enigma employees should have the option to purchase ownership in the company and benefit from the value we&apos;re creating together</i></p>
</div>","https://boards.greenhouse.io/enigmaio/jobs/5432897?gh_src=5fb8781b1us","5e030fc53b9ee4f1",,,,"Remote","Senior Software Engineer, Data Product","12 days ago","2023-10-13T11:50:44.044Z",,,"$160,000 - $210,000 a year","2023-10-25T11:50:44.046Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=5e030fc53b9ee4f1&from=jasx&tk=1hdjao0bljrpc800&vjs=3"
"Rite Aid","Company 
        
        
        
         Rite Aid
         
       
      
     
     
      
       
        
         Shift 
        
        
         Day 
        
       
      
     
    
    
     
      
       
        
         Job Type 
        
        
         Full time
         
       
      
     
     
      
       
        
         Requisition 
        
        
         JR020134
         
       
      
     
     
      
       
        
         Last Updated 
        
        
         2023-10-18
         
       
      
     
     
      
       
        
         Department 
        
        
         TS Business Intelligence
         
       
      
     
    
    
     
      Job Locations 
      
       
        Valley Green - Corporate - 200 Newberry Commons, Etters, PA, 17319 
       
       
        Remote- United States 
       
      
     
    
    
     
      
       
        
         Job Summary 
        
        
         Job Summary: The Senior Google Data Engineer will define, design, and deliver solutions that leverage data to enable the company to scale and accelerate its growth within the cloud. The role will take the lead in architecting and building data pipelines, data models, data integrations, and other business systems into the Google data platform. This comprehensive approach will empower the business and analysts to make data-driven decisions. Additionally, this person will set up Google infrastructure, automation, visualization platforms, and design and manage the data platform, including the Enterprise Data Warehouse. The Google Cloud Lead Engineer will be a key member of the Riteaid Enterprise Data Services group. They will provide best practices on secure foundational cloud implementations, automated provisioning of infrastructure and applications, cloud-ready application architectures, and more. Through the cloud engineer's guidance, we will ensure our teams have an excellent experience in building, modernizing, migrating, and maintaining applications in our hybrid multi cloud environment. The cloud engineer also serves as a guide and subject matter expert elevating the overall cloud capabilities within Riteaid. 
        
       
      
     
    
    
     
      
       
        
         Job Responsibilities 
        
        
         Responsibilities
         
           Refine, evangelize, and deliver on Rite Aid’s Modernization vision and strategy
           Design, develop, and deploy scalable data pipelines and data warehousing solutions on Google Cloud Platform (GCP).
           Utilize Python and other relevant programming languages to create data processing and transformation scripts.
           Work with key GCP services such as BigQuery, Composer/Airflow, DataFlow, and DVT , Data Loss Prevention, Cloud Run, IAM, Looker, Google Cloud Storage (GCS) to optimize data operations.
           Partner in the delivery of cloud-based technical architectures, migration approaches, and application optimizations that enable business objectives
           Collaborate with Product Owners, Scrum Masters, Developers, Product Architects to implement technical solutions
           Evolve and drive our automation capabilities by fostering a culture of DevOps and SRE in the realm of data and data engineering
           Support the adoption of Cloud practices and drive the institutionalization of the practices
           Identify risk and mitigation plans associated with security, legal, data, compliance, and regulatory requirements Contribute to the development of internal best practices as well as new innovative capabilities
           Hands on-Technical lead to guide, mentor, and coach the team
         
         
          Work Experience
         
           Bringing over a decade of expertise in database technologies, spanning traditional data warehousing and cloud engineering domains.
           Led the end-to-end design and development of data pipelines, data warehousing solutions, and data integration processes on Google Cloud Platform (GCP) or AWS or Azure.
           Utilized Python extensively to create data transformation scripts, ensuring seamless data processing and efficient ETL operations.
           Successfully harnessed GCP's core services, including BigQuery, Composer/Airflow, DataFlow, Looker , DLP , DVT, Cloud Run and Google Cloud Storage (GCS) to optimize data workflows and support critical business insights.
           Spearheaded the implementation of Continuous Integration/Continuous Deployment (CI/CD) pipelines for data engineering, enhancing workflow efficiency and reliability.
           Played a key role in the architectural design and planning of data warehousing systems, with a focus on creating Snowflake and Star schema models to ensure data accuracy and performance.
           Demonstrated a profound understanding of data modeling and schema design to create robust, scalable, and efficient data solutions.
           Collaborated closely with cross-functional teams, including data scientists, data analysts, and software developers, to ensure a cohesive and data-driven approach to decision-making.
           Implemented and maintained best practices for data quality, data security, and data governance throughout the data engineering process.
           Regularly contributed to code reviews, ensuring code quality and adherence to best practices in data engineering.
           Mentored junior data engineers and collaborated with them to enhance the data engineering capabilities of the team.
           Actively kept abreast of emerging trends and innovations in the data engineering field and made recommendations for their adoption when beneficial.
         
         
           Proficient in constructing data platforms and data warehousing solutions from the foundation up.
           Skilled in using visualization platforms, including Tableau and Excel.
           Specialized in ETL (Extract, Transform, Load) design, implementation, and ongoing maintenance.
           Adept in schema design and dimensional OLTP (Online Transaction Processing) data modeling.
           Experienced in crafting data design and modeling strategies from the ground up.
          
        
       
      
     
    
    
     
      
       
        
         qualifications 
        
        
         Bonus Points :
         
           Experience in SAP HANA database operation and integration
           Leveraged knowledge of SAP HANA database for seamless data integration and ingestion processes from SAP to public cloud platforms.
           Proficiency in migrating legacy data to the GCP cloud platform, enhancing the organization's data capabilities and accessibility.
           GCP Data Engineering certification would be a valuable addition, although not mandatory.
           Develop or deploy ML/AI models on a public cloud platform.
           Practical application of basic statistical methods: Regression and other techniques to be used for statistical models
          
        
       
      
     
    
    
     
      
       
        
         Fair Chance Act
          Fair Chance Act Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.
          Pursuant to the Los Angeles Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.
          Pursuant to the California Fair Chance Act, we will consider qualified applicants with a criminal history. You do not need to disclose your criminal history or participate in a background check until a conditional job offer is made to you. After making a conditional offer and running a background check, if we identify a conviction that is directly related to the job, you will be given the chance to explain the circumstances surrounding the conviction, provide mitigating evidence, or challenge the accuracy of the background report. Find out more about the Fair Chance Act by visiting the Civil Right’s Department Fair Chance Act webpage.
          For more detailed information around city/state required notices, click here to access a list of disclosures.
          New Jersey Law Against Discrimination (LAD)
          The New Jersey Law Against Discrimination (LAD) prohibits unlawful employment discrimination based on an individual's race, creed, color, national origin, nationality, ancestry, age, sex (including pregnancy), familial status, marital/civil union status, religion, domestic partnership status, affectional or sexual orientation, gender identity and expression, atypical hereditary cellular or blood trait, genetic information, liability for military service, and mental or physical disability (including perceived disability, and AIDS and HIV status).","<p></p>
<div>
 <div>
  <div>
   <div>
    <div>
     <div>
      <div>
       <div>
        <div>
         Company 
        </div>
        <div></div>
        <div>
         Rite Aid
        </div> 
       </div>
      </div>
     </div>
     <div>
      <div>
       <div>
        <div>
         Shift 
        </div>
        <div>
         Day 
        </div>
       </div>
      </div>
     </div>
    </div>
    <div>
     <div>
      <div>
       <div>
        <div>
         Job Type 
        </div>
        <div>
         Full time
        </div> 
       </div>
      </div>
     </div>
     <div>
      <div>
       <div>
        <div>
         Requisition 
        </div>
        <div>
         JR020134
        </div> 
       </div>
      </div>
     </div>
     <div>
      <div>
       <div>
        <div>
         Last Updated 
        </div>
        <div>
         2023-10-18
        </div> 
       </div>
      </div>
     </div>
     <div>
      <div>
       <div>
        <div>
         Department 
        </div>
        <div>
         TS Business Intelligence
        </div> 
       </div>
      </div>
     </div>
    </div>
    <div>
     <div>
      <h3 class=""jobSectionHeader""><b>Job Locations </b></h3>
      <div>
       <div>
        Valley Green - Corporate - 200 Newberry Commons, Etters, PA, 17319 
       </div>
       <div>
        Remote- United States 
       </div>
      </div>
     </div>
    </div>
    <div>
     <div>
      <div>
       <div>
        <div>
         Job Summary 
        </div>
        <div>
         Job Summary: The Senior Google Data Engineer will define, design, and deliver solutions that leverage data to enable the company to scale and accelerate its growth within the cloud. The role will take the lead in architecting and building data pipelines, data models, data integrations, and other business systems into the Google data platform. This comprehensive approach will empower the business and analysts to make data-driven decisions. Additionally, this person will set up Google infrastructure, automation, visualization platforms, and design and manage the data platform, including the Enterprise Data Warehouse. The Google Cloud Lead Engineer will be a key member of the Riteaid Enterprise Data Services group. They will provide best practices on secure foundational cloud implementations, automated provisioning of infrastructure and applications, cloud-ready application architectures, and more. Through the cloud engineer&apos;s guidance, we will ensure our teams have an excellent experience in building, modernizing, migrating, and maintaining applications in our hybrid multi cloud environment. The cloud engineer also serves as a guide and subject matter expert elevating the overall cloud capabilities within Riteaid. 
        </div>
       </div>
      </div>
     </div>
    </div>
    <div>
     <div>
      <div>
       <div>
        <div>
         Job Responsibilities 
        </div>
        <div>
         <p>Responsibilities</p>
         <ul>
          <li> Refine, evangelize, and deliver on Rite Aid&#x2019;s Modernization vision and strategy</li>
          <li> Design, develop, and deploy scalable data pipelines and data warehousing solutions on Google Cloud Platform (GCP).</li>
          <li> Utilize Python and other relevant programming languages to create data processing and transformation scripts.</li>
          <li> Work with key GCP services such as BigQuery, Composer/Airflow, DataFlow, and DVT , Data Loss Prevention, Cloud Run, IAM, Looker, Google Cloud Storage (GCS) to optimize data operations.</li>
          <li> Partner in the delivery of cloud-based technical architectures, migration approaches, and application optimizations that enable business objectives</li>
          <li> Collaborate with Product Owners, Scrum Masters, Developers, Product Architects to implement technical solutions</li>
          <li> Evolve and drive our automation capabilities by fostering a culture of DevOps and SRE in the realm of data and data engineering</li>
          <li> Support the adoption of Cloud practices and drive the institutionalization of the practices</li>
          <li> Identify risk and mitigation plans associated with security, legal, data, compliance, and regulatory requirements Contribute to the development of internal best practices as well as new innovative capabilities</li>
          <li> Hands on-Technical lead to guide, mentor, and coach the team</li>
         </ul>
         <p></p>
         <p><b> Work Experience</b></p>
         <ul>
          <li> Bringing over a decade of expertise in database technologies, spanning traditional data warehousing and cloud engineering domains.</li>
          <li> Led the end-to-end design and development of data pipelines, data warehousing solutions, and data integration processes on Google Cloud Platform (GCP) or AWS or Azure.</li>
          <li> Utilized Python extensively to create data transformation scripts, ensuring seamless data processing and efficient ETL operations.</li>
          <li> Successfully harnessed GCP&apos;s core services, including BigQuery, Composer/Airflow, DataFlow, Looker , DLP , DVT, Cloud Run and Google Cloud Storage (GCS) to optimize data workflows and support critical business insights.</li>
          <li> Spearheaded the implementation of Continuous Integration/Continuous Deployment (CI/CD) pipelines for data engineering, enhancing workflow efficiency and reliability.</li>
          <li> Played a key role in the architectural design and planning of data warehousing systems, with a focus on creating Snowflake and Star schema models to ensure data accuracy and performance.</li>
          <li> Demonstrated a profound understanding of data modeling and schema design to create robust, scalable, and efficient data solutions.</li>
          <li> Collaborated closely with cross-functional teams, including data scientists, data analysts, and software developers, to ensure a cohesive and data-driven approach to decision-making.</li>
          <li> Implemented and maintained best practices for data quality, data security, and data governance throughout the data engineering process.</li>
          <li> Regularly contributed to code reviews, ensuring code quality and adherence to best practices in data engineering.</li>
          <li> Mentored junior data engineers and collaborated with them to enhance the data engineering capabilities of the team.</li>
          <li> Actively kept abreast of emerging trends and innovations in the data engineering field and made recommendations for their adoption when beneficial.</li>
         </ul>
         <ul>
          <li> Proficient in constructing data platforms and data warehousing solutions from the foundation up.</li>
          <li> Skilled in using visualization platforms, including Tableau and Excel.</li>
          <li> Specialized in ETL (Extract, Transform, Load) design, implementation, and ongoing maintenance.</li>
          <li> Adept in schema design and dimensional OLTP (Online Transaction Processing) data modeling.</li>
          <li> Experienced in crafting data design and modeling strategies from the ground up.</li>
         </ul> 
        </div>
       </div>
      </div>
     </div>
    </div>
    <div>
     <div>
      <div>
       <div>
        <div>
         qualifications 
        </div>
        <div>
         <p><b>Bonus Points :</b></p>
         <ul>
          <li> Experience in SAP HANA database operation and integration</li>
          <li> Leveraged knowledge of SAP HANA database for seamless data integration and ingestion processes from SAP to public cloud platforms.</li>
          <li> Proficiency in migrating legacy data to the GCP cloud platform, enhancing the organization&apos;s data capabilities and accessibility.</li>
          <li> GCP Data Engineering certification would be a valuable addition, although not mandatory.</li>
          <li> Develop or deploy ML/AI models on a public cloud platform.</li>
          <li> Practical application of basic statistical methods: Regression and other techniques to be used for statistical models</li>
         </ul> 
        </div>
       </div>
      </div>
     </div>
    </div>
    <div>
     <div>
      <div>
       <div>
        <div>
         <h3 class=""jobSectionHeader""><b>Fair Chance Act</b></h3>
         <p> Fair Chance Act Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.</p>
         <p> Pursuant to the Los Angeles Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.</p>
         <p> Pursuant to the California Fair Chance Act, we will consider qualified applicants with a criminal history. You do not need to disclose your criminal history or participate in a background check until a conditional job offer is made to you. After making a conditional offer and running a background check, if we identify a conviction that is directly related to the job, you will be given the chance to explain the circumstances surrounding the conviction, provide mitigating evidence, or challenge the accuracy of the background report. Find out more about the Fair Chance Act by visiting the Civil Right&#x2019;s Department Fair Chance Act webpage.</p>
         <p> For more detailed information around city/state required notices, click here to access a list of disclosures.</p>
         <h3 class=""jobSectionHeader""><b> New Jersey Law Against Discrimination (LAD)</b></h3>
         <p> The New Jersey Law Against Discrimination (LAD) prohibits unlawful employment discrimination based on an individual&apos;s race, creed, color, national origin, nationality, ancestry, age, sex (including pregnancy), familial status, marital/civil union status, religion, domestic partnership status, affectional or sexual orientation, gender identity and expression, atypical hereditary cellular or blood trait, genetic information, liability for military service, and mental or physical disability (including perceived disability, and AIDS and HIV status).</p>
        </div>
       </div>
      </div>
     </div>
    </div>
   </div>
  </div>
 </div>
</div>
<p></p>","https://careers.riteaid.com/rite_aid_jobs/69826?Source=Indeed&ittk=LVME1K6TCL","d7f1ccad64214f40",,"Full-time",,"200 Newberry Cmns, Etters, PA 17319","Sr GCP Data Engineer","13 days ago","2023-10-12T11:50:46.181Z","3.4","13609",,"2023-10-25T11:50:46.182Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=d7f1ccad64214f40&from=jasx&tk=1hdjao0bljrpc800&vjs=3"
"Vail Resorts","As a leading mountain resort operator with over 40 resorts in sixteen states and four countries. We exist to create an Experience of a Lifetime for our employees, so they can, in turn, provide and Experience of a Lifetime for our guests. We are looking for leaders, innovators, creators, and ambitious professionals to join our talented team. If you’re ready to pursue your fullest potential, we want to get to know you!
  Many of our Corporate function teams can now live and work in any of the states in which Vail Resorts currently operates* – enabling flexible remote work alongside a commitment to building and maintaining strong culture both in person and virtually. If you’re ready to pursue your fullest potential, we want to get to know you. Find your purpose with us at www.vailresortscareers.com.
 
  Job Summary:
  The Enterprise Data Engineering Team at Vail Resorts is on a journey to redefine how data is ingested, modeled, and surfaced to our key stakeholders across the enterprise. This team is at the forefront of creating a modern data estate on which the foundation of our core businesses will operate. We are looking for a passionate and driven Senior Data Engineer to become an important part of our fast-paced, high-energy, and innovative culture. The ideal candidate will have experience in Azure, Databricks, ETL processes, Python, SQL, Jira and Github. In addition to these skills, the candidate should also have experience in data lake house architecture, data modeling and migration from on-premise enterprise data warehousing to data lake.
  As a Senior Data Engineer, you will be responsible for designing and implementing data pipelines that are scalable, reliable, and efficient. You will work closely with analytics teams, data scientist and other stakeholders to understand their requirements and design solutions that meet their needs.
 
  Job Specifications:
 
   Outlet: Corporate
   Expected Pay Range: $99,900 - $135,120 + annual bonus
   Shift & Schedule Availability: Full Time / Year Round
 
 
   Other Specifics: Remote
 
 
  Job Responsibilities:
 
   Design and implement data pipelines using Azure and Databricks.
   Develop ETL/ELT processes to extract load data into Databricks Lakehouse using PySpark/Python/Scala and Delta Live Tables.
   Experience orchestrating and monitoring workflows.
   Work with data scientists/data analysts to understand their requirements and design solutions that meet their needs.
   Develop and maintain Python scripts to automate data processing tasks.
   Write complex SQL queries.
   Optimize database performance by tuning queries and indexes.
   Monitor database performance and troubleshoot issues as they arise.
   Other duties as assigned
 
 
  Job Requirements:
 
   Bachelor’s degree in Computer Science or a related field.
   5+ years of experience in data engineering.
   Experience with Azure and Databricks.
   Strong knowledge of ETL processes.
   Proficiency in Python, SQL, Jira and Github.
   Experience with big data technologies such as Hadoop, Spark, or Kafka is a plus.
   Experience with Jira and Github.
   Experience in data lake house architecture.
   Experience ingesting data from Event Hubs is a plus.
   Experience in data modeling is a plus. Experience in migration from on-premise enterprise data warehousing to data lake. 
 
 
 The expected Total Compensation for this role is $99,900 - $135,120 + annual bonus. Individual compensation decisions are based on a variety of factors.
  The perks include a free ski pass, and a set of benefits including...
 
   Medical, Dental, Vision insurance, and a 401(k) retirement plan
   Hourly employees are generally eligible for accrued Paid Time Off (PTO) and Sick Time. Salaried employees are generally eligible for Flexible Time Off (FTO)
   Paid Parental Leave for eligible mothers and fathers
   Healthcare & Dependent Care Flexible Spending Accounts
   Life, AD&D, and disability insurance
 
 
  Reach Your Peak at Vail Resorts. At Vail Resorts, our team is made whole by the brave, passionate individuals who ambitiously push boundaries and challenge the status quo. Whether you’re looking for seasonal work or the career of a lifetime, join us today to reach your peak.
 
   Remote work is currently permitted from British Columbia and the 16 U.S. states in which we currently operate. This includes: California, Colorado, Indiana, Michigan, Minnesota, Missouri, New Hampshire, New York, Nevada, Ohio, Pennsylvania, Utah, Vermont, Washington State, Wisconsin, and Wyoming. Please note that the ability to work remotely, and the particulars related to such work, are subject to change at any time; and, accordingly, the Company reserves the right to change its policies and/or require in-person/in-office work at any time in its sole discretion.
 
 
  Vail Resorts is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability, protected veteran status or any other status protected by applicable law.
 
  Requisition ID 498620 Reference Date: 10/18/2023 Job Code Function: Information Systems","<p></p>
<div>
 <p>As a leading mountain resort operator with over 40 resorts in sixteen states and four countries. We exist to create an <i>Experience of a Lifetime</i> for our employees, so they can, in turn, provide and <i>Experience of a Lifetime</i> for our guests. We are looking for leaders, innovators, creators, and ambitious professionals to join our talented team. If you&#x2019;re ready to pursue your fullest potential, we want to get to know you!</p>
 <p><br> Many of our Corporate function teams can now live and work in any of the states in which Vail Resorts currently operates* &#x2013; enabling flexible remote work alongside a commitment to building and maintaining strong culture both in person and virtually. If you&#x2019;re ready to pursue your fullest potential, we want to get to know you. Find your purpose with us at www.vailresortscareers.com.</p>
 <p></p>
 <p><b><br> Job Summary:</b></p>
 <p> The Enterprise Data Engineering Team at Vail Resorts is on a journey to redefine how data is ingested, modeled, and surfaced to our key stakeholders across the enterprise. This team is at the forefront of creating a modern data estate on which the foundation of our core businesses will operate.<br> We are looking for a passionate and driven Senior Data Engineer to become an important part of our fast-paced, high-energy, and innovative culture. The ideal candidate will have experience in Azure, Databricks, ETL processes, Python, SQL, Jira and Github. In addition to these skills, the candidate should also have experience in data lake house architecture, data modeling and migration from on-premise enterprise data warehousing to data lake.</p>
 <p><br> As a Senior Data Engineer, you will be responsible for designing and implementing data pipelines that are scalable, reliable, and efficient. You will work closely with analytics teams, data scientist and other stakeholders to understand their requirements and design solutions that meet their needs.</p>
 <p></p>
 <p><b><br> Job Specifications:</b></p>
 <ul>
  <li> Outlet: Corporate</li>
  <li> Expected Pay Range: &#x24;99,900 - &#x24;135,120 + annual bonus</li>
  <li> Shift &amp; Schedule Availability: Full Time / Year Round</li>
 </ul>
 <ul>
  <li> Other Specifics: Remote</li>
 </ul>
 <p></p>
 <p><b><br> Job Responsibilities:</b></p>
 <ul>
  <li> Design and implement data pipelines using Azure and Databricks.</li>
  <li> Develop ETL/ELT processes to extract load data into Databricks Lakehouse using PySpark/Python/Scala and Delta Live Tables.</li>
  <li> Experience orchestrating and monitoring workflows.</li>
  <li> Work with data scientists/data analysts to understand their requirements and design solutions that meet their needs.</li>
  <li> Develop and maintain Python scripts to automate data processing tasks.</li>
  <li> Write complex SQL queries.</li>
  <li> Optimize database performance by tuning queries and indexes.</li>
  <li> Monitor database performance and troubleshoot issues as they arise.</li>
  <li> Other duties as assigned</li>
 </ul>
 <p></p>
 <p><b><br> Job Requirements:</b></p>
 <ul>
  <li> Bachelor&#x2019;s degree in Computer Science or a related field.</li>
  <li> 5+ years of experience in data engineering.</li>
  <li> Experience with Azure and Databricks.</li>
  <li> Strong knowledge of ETL processes.</li>
  <li> Proficiency in Python, SQL, Jira and Github.</li>
  <li> Experience with big data technologies such as Hadoop, Spark, or Kafka is a plus.</li>
  <li> Experience with Jira and Github.</li>
  <li> Experience in data lake house architecture.</li>
  <li> Experience ingesting data from Event Hubs is a plus.</li>
  <li> Experience in data modeling is a plus.</li> Experience in migration from on-premise enterprise data warehousing to data lake. 
 </ul>
 <p></p>
 <p>The expected Total Compensation for this role is &#x24;99,900 - &#x24;135,120 + annual bonus. Individual compensation decisions are based on a variety of factors.</p>
 <p><br> The perks include a free ski pass, and a set of benefits including...</p>
 <ul>
  <li> Medical, Dental, Vision insurance, and a 401(k) retirement plan</li>
  <li> Hourly employees are generally eligible for accrued Paid Time Off (PTO) and Sick Time. Salaried employees are generally eligible for Flexible Time Off (FTO)</li>
  <li> Paid Parental Leave for eligible mothers and fathers</li>
  <li> Healthcare &amp; Dependent Care Flexible Spending Accounts</li>
  <li> Life, AD&amp;D, and disability insurance</li>
 </ul>
 <p></p>
 <p><b><br> Reach Your Peak at Vail Resorts. </b>At Vail Resorts, our team is made whole by the brave, passionate individuals who ambitiously push boundaries and challenge the status quo. Whether you&#x2019;re looking for seasonal work or the career of a lifetime, join us today to reach your peak.</p>
 <ul>
  <li><br> <b><i>Remote work is currently permitted from British Columbia and the 16 U.S. states in which we currently operate. This includes: California, Colorado, Indiana, Michigan, Minnesota, Missouri, New Hampshire, New York, Nevada, Ohio, Pennsylvania, Utah, Vermont, Washington State, Wisconsin, and Wyoming. Please note that the ability to work remotely, and the particulars related to such work, are subject to change at any time; and, accordingly, the Company reserves the right to change its policies and/or require in-person/in-office work at any time in its sole discretion.</i></b></li>
 </ul>
 <p></p>
 <p><i><br> Vail Resorts is an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability, protected veteran status or any other status protected by applicable law.</i></p>
 <p></p>
 <p><i><br> Requisition ID 498620</i><br> <i>Reference Date: 10/18/2023</i><br> <i>Job Code Function: Information Systems</i></p>
</div>","https://jobs.vailresortscareers.com/corporate/job/Remote-Senior-Analyst-Data-Engineer-%28Remote%29-Remo/1088673400/?feedId=367000&rx_campaign=indeed0&rx_ch=jobp4p&rx_group=283690&rx_job=498620&rx_medium=cpc&rx_r=none&rx_source=Indeed&rx_ts=20231025T080341Z&rx_vp=cpc&sponsored=ppsa&utm_source=Indeed&utm_medium=organic&rx_p=YBVCXD3MVL&rx_viewer=c4250624732c11ee9d147355cc47915d1c4d0206433244c8aab1f4f28d0406aa","de1f69845d4ad405",,"Full-time",,"Remote","Senior Analyst Data Engineer (Remote)","6 days ago","2023-10-19T11:50:54.053Z","3.8","2279","$99,900 - $135,120 a year","2023-10-25T11:50:54.055Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=de1f69845d4ad405&from=jasx&tk=1hdjan9ea2cc7000&vjs=3"
"The Hershey Company","Job Title: Sr Engineer Data Visualization
  
  
 Job Location: Hershey, PA
  
  
 This position is open to a 100% remote as well as hybrid.
  
  
 Summary:
  
  
 The Enterprise Data organization drives value for Hershey by providing high-quality, well governed data to the Enterprise for analytics and decision-making. 
 The Sr. Data Visualization Engineer will be part of an agile execution team, working with Hershey business partners, data scientists, technical engineers, data architects, and project managers to design, develop and grow the data visualization, reporting and analytical solutions. This role will also ensure visualization standards adhere to company best practice and help to deliver rapid impactful benefits. 
 The Sr. Data Visualization Engineer will act as a trusted advisor for Hershey business partners by ensuring that data solutions meet expectations and requirements. You will work with a diverse team of business analysts, data scientists, technical engineers, data architects, and project managers to deliver outcomes aligned with our business partner’s strategy. In addition, you will report directly to the Sr. Manager Data Visualization to ensure consistency and compliance of deliverables to frameworks and governance processes.
  
  
 Major Duties/Responsibilities:
  
  
 Data Visualization Engineering Solution Delivery: 
 Develop and deliver high-quality data visualization, reporting and analytical solutions - adhering to best practice, privacy, and governance principles.
  
  
 Data Visualization Maintenance/Optimization: 
 Work existing solutions to enhance its performance, quality and/or functionality. Resolve incidents escalated by support teams or business users.
  
  
 Data Visualization Engineering Domain: 
 Collaborate with IT and business partners to define, manage and deliver innovative Data Visualization solutions to drive growth and adoption of capabilities at Hershey.
  
  
 Data Visualization Engineering Advocacy: 
 Evangelize future Data Visualization solutions identified by Enterprise Data leadership, including innovations such as: cloud-based intelligent systems to collect, distribute, model, analyze, and visualize disparate/diverse data assets of all sizes.
  
  
 Specific Job Responsibilities:
  
  
 
  Collaborates with the agile execution team throughout the design process—from creating user flows and wireframes to building user interface mockups and prototypes for Enterprise initiatives. 
  Consults the business and project teams on the visualization and impacts on data requirements. 
  Develops complex analytical solutions, while maintaining the output performance 
  Ensures the operations of the reporting and analytical platforms meet Service Level Agreements. 
  Maintains the performance and optimization of certified reports and analytical solutions. 
  Ensures the visualization answers the questions of the business and tells the story of the data following guiding principles. 
  Models the standards and best practices established by Enterprise Data leadership, including change management, prioritization, and audit. 
  Works with data engineers to understand data solutions and data availability. 
  Leads the operational and infrastructure activities post go-live. 
  Able to articulate the holistic benefits of data visualization from a business perspective, while. 
  Uses best practice frameworks and governance, evaluate, design, and analyze solution engineering for agile execution delivery. 
  Acts upon guidance from Enterprise Data leadership, oversee the health and evolution of agile execution team visualization technologies.
 
  
  
 
 Minimum knowledge, skills and abilities required to successfully perform major duties/responsibilities:
  
 
 
  Experience designing, and developing data visualization, reporting and analytical solutions. 
  Working knowledge of agile frameworks 
  Ability to manage multiple priorities, meet deadlines and produce quality results under pressure. 
  Demonstrated leadership and managerial skills. 
  Strong problem solving and analytical skills. 
  Strong team player, change agent, and advocate. 
  Excellent customer service skills. 
  High energy self-starter. 
  Excellent verbal and written communication skills.
 
  
  
  
 Minimum Education and Experience Requirements:
  
  
 
  Education: Bachelor’s in a STEM degree 
  Master’s degree and/or related equivalent experience preferred.
 
  
  
 
 Experience: 
 5+ years of experience with reporting and visualization tools such as Power BI, Tableau, much of which has been focused on working with cross-functional teams and enterprise-wide data management programs 
 
  3+ Experience working with Alteryx or similar data wrangling tool for data modeling, data mining, forecasting, simulation. 
  3+ years of developing and maintaining the reporting ecosystem, ensuring service level agreements and data quality. 
  3+ years building analytical models. 
  3+ years of experience working with business owners to maintain and optimize key performance indicators within data and analytical solutions. 
  Ability to speak to the visualization tools and discuss tradeoff between their functionalities. 
  Knowledge and experience of working with relational/non-relational databases e.g., SQL, Teradata, Snowflake, Databricks, Azure Data solutions or Hadoop. 
  Experience working in a high performing agile delivery model, aligning with Scrum Masters, Product Owners, and other execution team members to deliver rapid and impactful solutions that align to business partner strategy. 
  
 Excellent communication and presentation skills, with the ability to articulate new ideas and concepts to technical and non-technical partners.
  
  
 Leadership Competencies:
  
  
 Strategic Thinking 
 Thinks strategically by clearly anticipating future trends, challenges, and consequences, creating breakthrough business strategies and plans to achieve a competitive advantage. Creates clear and compelling vision and strategy, translates strategy into business plan, and communicates strategic vision to employees.
  
  
 Business Acumen 
 Leverages business judgment to shape strategy, based on understanding of operational, commercial, financial, and organizational requirements and capabilities. Understands the Hershey business units, regions, and functions, and the manufacturing, commercialization and market access of its products and services.
  
  
 Drive for Results
  
  
 Pushes self and others to exceed goals and achieve breakthrough results. Recognizes the key actions necessary to achieve results, establishes and communicates the priorities to others, and maintains own and others focus on achieving the important goals. Demonstrates persistence in removing barriers to achieving results and encourages others to do the same.
  
  
 Prioritization and Judgment
  
  
 Prioritizes and focuses on the right ideas, opportunities, issues, and projects. Develops decision criteria and considers benefits, costs, and risks of each decision and its immediate and long-range implications. Makes timely, sound judgments in uncertain and changing situations.
  
  
 Influential Leadership
  
  
 Leads and persuades others, within and outside of Hershey and without direct authority or formalized structure. Understands other’s needs motivations, concerns, and positions. Establishes credibility with stakeholders and confidently influences their opinions and actions. Inspires and leads others to adopt common vision, achieve organizational change, and accomplish business strategies.
  
  
 Partnership
  
  
 Develops and maintains quality, long-term relationships and partnerships based on trust, transparency, communication, and credibility with key internal and external stakeholders to accomplish strategic objectives. Works to find common ground and mutually beneficial solutions to conflicts. Uses diplomacy and tact to diffuse high-tension situations.
  
  
 Global / M&A Mindset
  
  
 Thinks from a global / M&A perspective and understands market, regulatory, political, economic, and cultural differences across countries and regions and their interdependencies. Understands how stakeholders and teams work and communicate in other countries and regions and how to adapt behavior and strategy to ensure alignment with market and cultural differences.
  
  
 Talent Management and Development
  
  
 Defines and communicates performance standards and continuously raises the bar. Motivates employees to perform at their highest potential and achieve breakthrough results, holding individuals accountable for own actions, and recognizing individual contributions and achievements. Leads development of internal talent to meet the future needs of the organization.
  
  
 Customer Focus
  
  
 Builds a culture that strives to exceed customer needs and is creatively challenged by and responsive to customer experiences. Creates clarity within the organization of what successfully meeting customer needs looks like. Gives priority to customer needs and makes organizational adjustments based on continuously reevaluating how well customer needs are being met.
  
  
 #LI-SM2  
 #LI-Remote 
  The Hershey Company is an Equal Opportunity Employer. The policy of The Hershey Company is to extend opportunities to qualified applicants and employees on an equal basis regardless of an individual's race, color, gender, age, national origin, religion, citizenship status, marital status, sexual orientation, gender identity, transgender status, physical or mental disability, protected veteran status, genetic information, pregnancy, or any other categories protected by applicable federal, state or local laws.
  
  
 The Hershey Company is an Equal Opportunity Employer - Minority/Female/Disabled/Protected Veterans 
 If you require a reasonable accommodation as part of the application process, please contact the HR Service Center (askhr@hersheys.com).","<div>
 <p><b>Job Title: </b><b>Sr Engineer Data Visualization</b></p>
 <br> 
 <p></p> 
 <p><b>Job Location: Hershey, PA</b></p>
 <br> 
 <p></p> 
 <p><b>This position is open to a 100% remote as well as hybrid.</b></p>
 <br> 
 <p></p> 
 <p><b>Summary:</b></p>
 <br> 
 <p></p> 
 <p>The Enterprise Data organization drives value for Hershey by providing high-quality, well governed data to the Enterprise for analytics and decision-making. </p>
 <p>The Sr. Data Visualization Engineer will be part of an agile execution team, working with Hershey business partners, data scientists, technical engineers, data architects, and project managers to design, develop and grow the data visualization, reporting and analytical solutions. This role will also ensure visualization standards adhere to company best practice and help to deliver rapid impactful benefits.</p> 
 <p>The Sr. Data Visualization Engineer will act as a trusted advisor for Hershey business partners by ensuring that data solutions meet expectations and requirements. You will work with a diverse team of business analysts, data scientists, technical engineers, data architects, and project managers to deliver outcomes aligned with our business partner&#x2019;s strategy. In addition, you will report directly to the Sr. Manager Data Visualization to ensure consistency and compliance of deliverables to frameworks and governance processes.</p>
 <br> 
 <p></p> 
 <p><b>Major Duties/Responsibilities:</b></p>
 <br> 
 <p></p> 
 <p><b>Data Visualization Engineering Solution Delivery:</b></p> 
 <p>Develop and deliver high-quality data visualization, reporting and analytical solutions - adhering to best practice, privacy, and governance principles.</p>
 <br> 
 <p></p> 
 <p><b>Data Visualization Maintenance/Optimization: </b></p>
 <p>Work existing solutions to enhance its performance, quality and/or functionality. Resolve incidents escalated by support teams or business users.</p>
 <br> 
 <p></p> 
 <p><b>Data Visualization Engineering Domain: </b></p>
 <p>Collaborate with IT and business partners to define, manage and deliver innovative Data Visualization solutions to drive growth and adoption of capabilities at Hershey.</p>
 <br> 
 <p></p> 
 <p><b>Data Visualization Engineering Advocacy:</b></p> 
 <p>Evangelize future Data Visualization solutions identified by Enterprise Data leadership, including innovations such as: cloud-based intelligent systems to collect, distribute, model, analyze, and visualize disparate/diverse data assets of all sizes.</p>
 <br> 
 <p></p> 
 <p><b>Specific Job Responsibilities:</b></p>
 <br> 
 <p></p> 
 <ul>
  <li>Collaborates with the agile execution team throughout the design process&#x2014;from creating user flows and wireframes to building user interface mockups and prototypes for Enterprise initiatives.</li> 
  <li>Consults the business and project teams on the visualization and impacts on data requirements.</li> 
  <li>Develops complex analytical solutions, while maintaining the output performance</li> 
  <li>Ensures the operations of the reporting and analytical platforms meet Service Level Agreements.</li> 
  <li>Maintains the performance and optimization of certified reports and analytical solutions.</li> 
  <li>Ensures the visualization answers the questions of the business and tells the story of the data following guiding principles.</li> 
  <li>Models the standards and best practices established by Enterprise Data leadership, including change management, prioritization, and audit.</li> 
  <li>Works with data engineers to understand data solutions and data availability.</li> 
  <li>Leads the operational and infrastructure activities post go-live.</li> 
  <li>Able to articulate the holistic benefits of data visualization from a business perspective, while.</li> 
  <li>Uses best practice frameworks and governance, evaluate, design, and analyze solution engineering for agile execution delivery.</li> 
  <li>Acts upon guidance from Enterprise Data leadership, oversee the health and evolution of agile execution team visualization technologies.</li>
 </ul>
 <br> 
 <p></p> 
 <p></p>
 <p><b>Minimum knowledge, skills and abilities required to successfully perform major duties/responsibilities:</b></p>
 <br> 
 <p></p>
 <ul>
  <li>Experience designing, and developing data visualization, reporting and analytical solutions.</li> 
  <li>Working knowledge of agile frameworks </li>
  <li>Ability to manage multiple priorities, meet deadlines and produce quality results under pressure.</li> 
  <li>Demonstrated leadership and managerial skills.</li> 
  <li>Strong problem solving and analytical skills.</li> 
  <li>Strong team player, change agent, and advocate.</li> 
  <li>Excellent customer service skills.</li> 
  <li>High energy self-starter.</li> 
  <li>Excellent verbal and written communication skills.</li>
 </ul>
 <br> 
 <p></p> 
 <p></p> 
 <p><b>Minimum Education and Experience Requirements:</b></p>
 <br> 
 <p></p> 
 <ul>
  <li><b>Education: </b>Bachelor&#x2019;s in a STEM degree </li>
  <li>Master&#x2019;s degree and/or related equivalent experience preferred.</li>
 </ul>
 <br> 
 <p></p> 
 <p></p>
 <p><b>Experience: </b></p>
 <p>5+ years of experience with reporting and visualization tools such as Power BI, Tableau, much of which has been focused on working with cross-functional teams and enterprise-wide data management programs</p> 
 <ul>
  <li>3+ Experience working with Alteryx or similar data wrangling tool for data modeling, data mining, forecasting, simulation.</li> 
  <li>3+ years of developing and maintaining the reporting ecosystem, ensuring service level agreements and data quality.</li> 
  <li>3+ years building analytical models.</li> 
  <li>3+ years of experience working with business owners to maintain and optimize key performance indicators within data and analytical solutions.</li> 
  <li>Ability to speak to the visualization tools and discuss tradeoff between their functionalities.</li> 
  <li>Knowledge and experience of working with relational/non-relational databases e.g., SQL, Teradata, Snowflake, Databricks, Azure Data solutions or Hadoop.</li> 
  <li>Experience working in a high performing agile delivery model, aligning with Scrum Masters, Product Owners, and other execution team members to deliver rapid and impactful solutions that align to business partner strategy.</li> 
 </ul> 
 <p>Excellent communication and presentation skills, with the ability to articulate new ideas and concepts to technical and non-technical partners.</p>
 <br> 
 <p></p> 
 <p><b>Leadership Competencies:</b></p>
 <br> 
 <p></p> 
 <p><b>Strategic Thinking</b></p> 
 <p>Thinks strategically by clearly anticipating future trends, challenges, and consequences, creating breakthrough business strategies and plans to achieve a competitive advantage. Creates clear and compelling vision and strategy, translates strategy into business plan, and communicates strategic vision to employees.</p>
 <br> 
 <p></p> 
 <p><b>Business Acumen</b></p> 
 <p>Leverages business judgment to shape strategy, based on understanding of operational, commercial, financial, and organizational requirements and capabilities. Understands the Hershey business units, regions, and functions, and the manufacturing, commercialization and market access of its products and services.</p>
 <br> 
 <p></p> 
 <p><b>Drive for Results</b></p>
 <br> 
 <p></p> 
 <p>Pushes self and others to exceed goals and achieve breakthrough results. Recognizes the key actions necessary to achieve results, establishes and communicates the priorities to others, and maintains own and others focus on achieving the important goals. Demonstrates persistence in removing barriers to achieving results and encourages others to do the same.</p>
 <br> 
 <p></p> 
 <p><b>Prioritization and Judgment</b></p>
 <br> 
 <p></p> 
 <p>Prioritizes and focuses on the right ideas, opportunities, issues, and projects. Develops decision criteria and considers benefits, costs, and risks of each decision and its immediate and long-range implications. Makes timely, sound judgments in uncertain and changing situations.</p>
 <br> 
 <p></p> 
 <p><b>Influential Leadership</b></p>
 <br> 
 <p></p> 
 <p>Leads and persuades others, within and outside of Hershey and without direct authority or formalized structure. Understands other&#x2019;s needs motivations, concerns, and positions. Establishes credibility with stakeholders and confidently influences their opinions and actions. Inspires and leads others to adopt common vision, achieve organizational change, and accomplish business strategies.</p>
 <br> 
 <p></p> 
 <p><b>Partnership</b></p>
 <br> 
 <p></p> 
 <p>Develops and maintains quality, long-term relationships and partnerships based on trust, transparency, communication, and credibility with key internal and external stakeholders to accomplish strategic objectives. Works to find common ground and mutually beneficial solutions to conflicts. Uses diplomacy and tact to diffuse high-tension situations.</p>
 <br> 
 <p></p> 
 <p><b>Global / M&amp;A Mindset</b></p>
 <br> 
 <p></p> 
 <p>Thinks from a global / M&amp;A perspective and understands market, regulatory, political, economic, and cultural differences across countries and regions and their interdependencies. Understands how stakeholders and teams work and communicate in other countries and regions and how to adapt behavior and strategy to ensure alignment with market and cultural differences.</p>
 <br> 
 <p></p> 
 <p><b>Talent Management and Development</b></p>
 <br> 
 <p></p> 
 <p>Defines and communicates performance standards and continuously raises the bar. Motivates employees to perform at their highest potential and achieve breakthrough results, holding individuals accountable for own actions, and recognizing individual contributions and achievements. Leads development of internal talent to meet the future needs of the organization.</p>
 <br> 
 <p></p> 
 <p><b>Customer Focus</b></p>
 <br> 
 <p></p> 
 <p>Builds a culture that strives to exceed customer needs and is creatively challenged by and responsive to customer experiences. Creates clarity within the organization of what successfully meeting customer needs looks like. Gives priority to customer needs and makes organizational adjustments based on continuously reevaluating how well customer needs are being met.</p>
 <br> 
 <p></p> 
 <h1 class=""jobSectionHeader""><b>#LI-SM2 </b></h1> 
 <h1 class=""jobSectionHeader""><b>#LI-Remote</b></h1> 
 <p> The Hershey Company is an Equal Opportunity Employer. The policy of The Hershey Company is to extend opportunities to qualified applicants and employees on an equal basis regardless of an individual&apos;s race, color, gender, age, national origin, religion, citizenship status, marital status, sexual orientation, gender identity, transgender status, physical or mental disability, protected veteran status, genetic information, pregnancy, or any other categories protected by applicable federal, state or local laws.</p>
 <br> 
 <p></p> 
 <p>The Hershey Company is an Equal Opportunity Employer - Minority/Female/Disabled/Protected Veterans</p> 
 <p>If you require a reasonable accommodation as part of the application process, please contact the HR Service Center (askhr@hersheys.com).</p>
</div>","https://careers.thehersheycompany.com/job/Hershey-Sr-Engineer-Data-Visualization-PA-17033/1086613200/?applySourceOverride=Indeed","79482eec998fe60c",,,,"Hershey, PA","Sr Engineer Data Visualization","11 days ago","2023-10-14T11:50:41.873Z","3.7","1281",,"2023-10-25T11:50:41.878Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=79482eec998fe60c&from=jasx&tk=1hdjao0bljrpc800&vjs=3"
"Highway","Description:
 
  Highway is a fast-growing SaaS company operating in the mortgage lending space. The platform provides data and communication products, including market intelligence, software tools, calculators and a marketing suite to mortgage loan originators and real estate agents to facilitate higher conversion rates of conversations and prospects to actual loans. It is a proven solution to materially increase a mortgage loan origination efficiency and effectiveness coupled with improving end-customer experience.?
  Job Summary
  This is a new role for Highway and will report directly to the VP of Product Engineering, with significant interaction with other members of the data and leadership teams. You will help us envision, build, maintain, and iterate on Highway’s data strategy and components for our market-leading platform for mortgage originators.
  As a Senior Data Engineer, you'll work in a key position, where your efforts will have a noticeable impact on both the company and the quality of our product.
  You will help develop and promote our data platform across the entire Highway ecosystem. In addition to the requirements below, successful candidates and team members will share a passion for high-quality software, strong engineering principles, and methodical problem-solving skills.
 
  Responsibilities
 
   Develop a deep working knowledge of the entire Highway stack, with an eye towards developing and leveraging our data strategy and components to our best advantage.
   Analyze complex software systems and collaborate with others to improve the overall design, testability, and quality of the data architecture and components.
   Maintain our data warehouse with timely and quality data.
   Build and maintain data pipelines from internal, external databases and feeds and a variety of SaaS applications.
   Create and maintain architecture and systems documentation.
   Write maintainable, performant code.
   Improve, manage, and teach standards for code maintainability and performance in code submitted and reviewed.
   Create smaller merge requests and issues by collaborating with stakeholders to reduce scope and focus on constant iteration.
   Ship medium to large features independently but love working with a larger team.
   Provide architecture recommendations and can implement them.
   You are a great communicator and regularly achieve consensus amongst team members and teams.
   You will participate in technical interviews.
   Implement the DataOps philosophy in everything you do.
   Plan and execute system expansion as needed to support the company’s growth and analytic needs.
   Collaborate with Data Analysts to drive efficiency in their work.
   Collaborate with other functions to ensure data needs are addressed.
  Requirements: 
  General Requirements:
 
   You value collaboration, results, efficiency, iteration, and transparency
   You thrive in an environment where self-learning is encouraged and instilled as a part of our culture.
   You have a keen interest in continuously improving our product quality, security, and performance.
   You continually keep up with advancements in data engineering best practices.
   You participate in code reviews, pull requests, catch bugs and style issues in code reviews.
   You can ship small features completely independently but love to collaborate with others on the team.
   Ability to be a lead organizer of the data stack, guiding our team through suggestion of structure and documentation of how elements work together.
   Ability to work independently in creating project tasks, adhering to team deadlines, while working cross-department with the rest of the engineering team.
 
  Technical Qualifications:
 
   You have 5+ years of hands-on experience authoring, supporting, and extending data-related production code, using Python, including comfort with the following packages: pandas, boto3, sqlalchemy, pytest
   You have a demonstrably deep understanding of SQL and analytical data warehouses, specifically Snowflake, and noSQL databases (such as MongoDB, Elasticsearch) as well as other structured and unstructured data sources.
   API Framework (ex. Fastapi, Flask) experience
   You understand and implement data engineering best practices.
   You understand Containerization, Orchestration and Pipelines and have worked with Amazon ECS Fargate, Lambda, and AWS Step Functions - or similar competing technology from a major cloud vendor.
   You have hands-on experience implementing ETL (or ELT) best practices at scale.
   You have hands-on experience with data pipeline tools such as Airflow, Luigi, and Azkaban.
   You have strong data modeling skills and familiarity with the Kimball methodology.
   You use source control (Git) as part of your normal daily workflow.
 
  Nice to Have, Experience With:
 
   Data CI/CD. AWS CDK and other similar technologies
   FiveTran
   Productionalizing ML models in AWS (Sagemaker, tensorflow, pytorch, sklearn) experience
   Spark, PySpark, or SparklyR experience
   Looker or similar tools experience
   Ability to develop packages / libraries for Python and/or R
   JavaScript, HTML, CSS experience
   Salesforce experience
   Analytical tools and tag implementation experience
   Communications (email and text) platforms experience
   R: tidyverse, data.table, R Shiny, caret, etc. experience","<div>
 Description:
 <p></p>
 <p><br> Highway is a fast-growing SaaS company operating in the mortgage lending space. The platform provides data and communication products, including market intelligence, software tools, calculators and a marketing suite to mortgage loan originators and real estate agents to facilitate higher conversion rates of conversations and prospects to actual loans. It is a proven solution to materially increase a mortgage loan origination efficiency and effectiveness coupled with improving end-customer experience.?</p>
 <p><b> Job Summary</b></p>
 <p> This is a new role for Highway and will report directly to the VP of Product Engineering, with significant interaction with other members of the data and leadership teams. You will help us envision, build, maintain, and iterate on Highway&#x2019;s data strategy and components for our market-leading platform for mortgage originators.</p>
 <p> As a Senior Data Engineer, you&apos;ll work in a key position, where your efforts will have a noticeable impact on both the company and the quality of our product.</p>
 <p> You will help develop and promote our data platform across the entire Highway ecosystem. In addition to the requirements below, successful candidates and team members will share a passion for high-quality software, strong engineering principles, and methodical problem-solving skills.</p>
 <p></p>
 <p><b><br> Responsibilities</b></p>
 <ul>
  <li> Develop a deep working knowledge of the entire Highway stack, with an eye towards developing and leveraging our data strategy and components to our best advantage.</li>
  <li> Analyze complex software systems and collaborate with others to improve the overall design, testability, and quality of the data architecture and components.</li>
  <li> Maintain our data warehouse with timely and quality data.</li>
  <li> Build and maintain data pipelines from internal, external databases and feeds and a variety of SaaS applications.</li>
  <li> Create and maintain architecture and systems documentation.</li>
  <li> Write maintainable, performant code.</li>
  <li> Improve, manage, and teach standards for code maintainability and performance in code submitted and reviewed.</li>
  <li> Create smaller merge requests and issues by collaborating with stakeholders to reduce scope and focus on constant iteration.</li>
  <li> Ship medium to large features independently but love working with a larger team.</li>
  <li> Provide architecture recommendations and can implement them.</li>
  <li> You are a great communicator and regularly achieve consensus amongst team members and teams.</li>
  <li> You will participate in technical interviews.</li>
  <li> Implement the DataOps philosophy in everything you do.</li>
  <li> Plan and execute system expansion as needed to support the company&#x2019;s growth and analytic needs.</li>
  <li> Collaborate with Data Analysts to drive efficiency in their work.</li>
  <li> Collaborate with other functions to ensure data needs are addressed.</li>
 </ul> Requirements: 
 <p><b> General Requirements:</b></p>
 <ul>
  <li> You value collaboration, results, efficiency, iteration, and transparency</li>
  <li> You thrive in an environment where self-learning is encouraged and instilled as a part of our culture.</li>
  <li> You have a keen interest in continuously improving our product quality, security, and performance.</li>
  <li> You continually keep up with advancements in data engineering best practices.</li>
  <li> You participate in code reviews, pull requests, catch bugs and style issues in code reviews.</li>
  <li> You can ship small features completely independently but love to collaborate with others on the team.</li>
  <li> Ability to be a lead organizer of the data stack, guiding our team through suggestion of structure and documentation of how elements work together.</li>
  <li> Ability to work independently in creating project tasks, adhering to team deadlines, while working cross-department with the rest of the engineering team.</li>
 </ul>
 <p><b> Technical Qualifications:</b></p>
 <ul>
  <li> You have 5+ years of hands-on experience authoring, supporting, and extending data-related production code, using Python, including comfort with the following packages: pandas, boto3, sqlalchemy, pytest</li>
  <li> You have a demonstrably deep understanding of SQL and analytical data warehouses, specifically Snowflake, and noSQL databases (such as MongoDB, Elasticsearch) as well as other structured and unstructured data sources.</li>
  <li> API Framework (ex. Fastapi, Flask) experience</li>
  <li> You understand and implement data engineering best practices.</li>
  <li> You understand Containerization, Orchestration and Pipelines and have worked with Amazon ECS Fargate, Lambda, and AWS Step Functions - or similar competing technology from a major cloud vendor.</li>
  <li> You have hands-on experience implementing ETL (or ELT) best practices at scale.</li>
  <li> You have hands-on experience with data pipeline tools such as Airflow, Luigi, and Azkaban.</li>
  <li> You have strong data modeling skills and familiarity with the Kimball methodology.</li>
  <li> You use source control (Git) as part of your normal daily workflow.</li>
 </ul>
 <p><b> Nice to Have, Experience With:</b></p>
 <ul>
  <li> Data CI/CD. AWS CDK and other similar technologies</li>
  <li> FiveTran</li>
  <li> Productionalizing ML models in AWS (Sagemaker, tensorflow, pytorch, sklearn) experience</li>
  <li> Spark, PySpark, or SparklyR experience</li>
  <li> Looker or similar tools experience</li>
  <li> Ability to develop packages / libraries for Python and/or R</li>
  <li> JavaScript, HTML, CSS experience</li>
  <li> Salesforce experience</li>
  <li> Analytical tools and tag implementation experience</li>
  <li> Communications (email and text) platforms experience</li>
  <li> R: tidyverse, data.table, R Shiny, caret, etc. experience</li>
 </ul>
</div>","https://recruiting.paylocity.com/recruiting/jobs/Details/1870431/Highway/Senior-Data-Engineer?source=Indeed_Feed","10b47b3c225902e3",,"Full-time",,"Remote","Senior Data Engineer","30+ days ago","2023-09-25T11:51:08.047Z","3.9","42",,"2023-10-25T11:51:08.049Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=10b47b3c225902e3&from=jasx&tk=1hdjaoshm284l000&vjs=3"
"LookFar Labs","About the Role: We have an existing commercial SaaS platform that consists of 3 components: a web application, several 3rd party databases integrated into our backend, and a Natural Language Processing ML model based on a custom taxonomy.
We are looking to build 2.0 of our platform, with a brand new front end based on new algorithms, and scalable data science models that use a confluence of data from various data sources (e.g., patent, financial, and people). It’s a challenge and a fun opportunity for someone looking to make the next big platform that the world is going to use.
Our Data Engineer would need to create a new data pipeline, ETL process, and architecture for 2.0 of our platform. This could include multi-modal databases (including PostgreSQL and graph databases), and should consider the delineation between production, development, and staging/testing data pipelines and environments. The data pipeline should easily integrate new data sources, with both structured and unstructured data, and should enable associations between data as well. It should also enable and further enhance the strong entity resolution that we have already started building for our disparate, large data sets to be cleanly integrated.
You should also not rely solely on off the shelf tools or default pipelines. This role will require creativity and customization.
Your solutions should keep in mind scalability, to enable optimized usage of distributed computing frameworks like Spark. You should also have strong familiarity and experience with how to leverage the AWS ecosystem to bring in relevant AWS tools, services, and resources to enable substantial processing of very large datasets before runtime, entity resolution between very large datasets, and real-time processing in a scalable, distributed computing environment.
Role Responsibilities:

 Create and maintain a scalable ETL data pipeline that ingests multiple large datasets of both structured data (in the form of financial and patent data) and unstructured data (in the form of white papers, scraped websites, etc.), andenables entity resolution and other transformations for clean data integration andusage
 Create and maintain a multi-modal data storage system (including at least PostgreSQL, AWS architecture like S3, and graph databases) that enables scalable,real-time processing for production-level data
 Work with the data science team to enable ML Ops
 Have curiosity and passion for data, and the ability to efficiently query and obtain data via SQL
 Demonstrate a strong sense of ownership, of both technical and business

outcomes

 Assist dev and data science teams with processing and integrating data analysis
 Clearly document processes, methodologies, and tools usedExperience Required:
 B.S. in relevant technical degree
 Significant use and experience (at least 3-5 years) as a data engineer in the AWS ecosystem, including strong familiarity with structured and unstructured large datasets, enabling scalable and distributed compute, and ensuring real-time processingat scale
 Significant use and experience (at least 3-5 years) with writing complex SQL queries and analysis of data correlations
 At least a couple of years graph DB experience is required.
 Significant experience (at least 3-5 years) with the AWS ecosystem, including RedShift, Glue, and other tools, services, and resources that enable scalable,distributed compute
 Significant experience (at least 3-5 years) with multi-modal DB and ETL pipelines, including with graph databases
 Project management skills, ability to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform
 Excellent communication and story-telling skills (written and verbal)

Our Current Tech Stack: Please note this is not our future tech stack. AWS to host the infrastructure, including the CICD, SpringBoot, Angular, Python, PySpark, Kubernetes, EMR, Spark, Elasticsearch, RedShift, AWS (S3, Code Commit, Code Build, Code Deploy, EC2, EMR, etc.), Docker, Spacy, Scikit learn, Openpyxl, Streamlit, Watchdog, sklearn, seaborn, nltk, matplotlib, pandas, SQLAlchemy, and additional ML and python libraries.
This stack is subject to change as we build v2.0. We want to modernize and streamline our models, MLOps, code, deployment, front-end, and distributed processing capabilities.
Logistics:Geography, Work Status, Etc. The position is remote. The candidate must have the legal right to work in the United States on a W2.
Interview Process: We will conduct 3 rounds of interviews.

 First Round: Culture, fit, and background interview with the company Founders
 Second Round: Technical Interview
 Technical Project: Execute a small data engineering project, if selected for the third round of interview
 Third Round: In-Person Day in Washington D.C. (We will have the candidate fly out to D.C. to meet the founders and team.) Present the results of the data engineering project during the In-Person Day.

How to Apply: Please provide the following:

 Resume
 Cover Letter
 Any links to Git repositories or data engineering projects that we can review

About the Company: We are the source of truth for patent intelligence. Patents protect revenue and investment in the market. Given that, patent intelligence is not complete UNLESS it integrates financial and market data. We provide SaaS platforms that correlate multiple data sets (patent, financial, and people data) using scalable data science models, in order to answer fundamental questions related to patent and innovation strategy.
We provide patent intelligence to corporate IP departments and the defense sector. We are expanding to a larger commercial market, including technology transfer, venture capital, and financial institutions.
We are committed to creating a diverse environment and is proud to be an equal opportunity employer.
All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, pregnancy, disability, age, veteran status, or other characteristics.
Job Types: Permanent, Full-time
Pay: $115,000.00 - $145,000.00 per year
Benefits:

 401(k)
 Dental insurance
 Health insurance
 Paid time off

Compensation package:

 Yearly pay

Experience level:

 5 years
 6 years
 7 years
 8 years
 9 years

Schedule:

 Monday to Friday

Application Question(s):

 Are you able to work full-time in the US on a W2 without a need for work sponsorship?

Experience:

 multi-modal DB and ETL pipelines: 4 years (Required)
 AWS: 4 years (Required)
 SQL: 4 years (Required)
 working with structured & unstructured data: 4 years (Required)
 Redshift: 4 years (Required)
 ML Ops: 4 years (Required)
 Graph Databases: 3 years (Required)

Work Location: Remote","<p><b>About the Role: </b><br>We have an existing commercial SaaS platform that consists of 3 components: a web application, several 3rd party databases integrated into our backend, and a Natural Language Processing ML model based on a custom taxonomy.</p>
<p>We are looking to build 2.0 of our platform, with a brand new front end based on new algorithms, and scalable data science models that use a confluence of data from various data sources (e.g., patent, financial, and people). It&#x2019;s a challenge and a fun opportunity for someone looking to make the next big platform that the world is going to use.</p>
<p>Our Data Engineer would need to create a new data pipeline, ETL process, and architecture for 2.0 of our platform. This could include multi-modal databases (including PostgreSQL and graph databases), and should consider the delineation between production, development, and staging/testing data pipelines and environments. The data pipeline should easily integrate new data sources, with both structured and unstructured data, and should enable associations between data as well. It should also enable and further enhance the strong entity resolution that we have already started building for our disparate, large data sets to be cleanly integrated.</p>
<p>You should also not rely solely on off the shelf tools or default pipelines. This role will require creativity and customization.</p>
<p>Your solutions should keep in mind scalability, to enable optimized usage of distributed computing frameworks like Spark. You should also have strong familiarity and experience with how to leverage the AWS ecosystem to bring in relevant AWS tools, services, and resources to enable substantial processing of very large datasets before runtime, entity resolution between very large datasets, and real-time processing in a scalable, distributed computing environment.</p>
<p><b>Role Responsibilities:</b></p>
<ul>
 <li>Create and maintain a scalable ETL data pipeline that ingests multiple large datasets of both structured data (in the form of financial and patent data) and unstructured data (in the form of white papers, scraped websites, etc.), andenables entity resolution and other transformations for clean data integration andusage</li>
 <li>Create and maintain a multi-modal data storage system (including at least PostgreSQL, AWS architecture like S3, and graph databases) that enables scalable,real-time processing for production-level data</li>
 <li>Work with the data science team to enable ML Ops</li>
 <li><b>Have curiosity and passion for data</b>, and the ability to efficiently query and obtain data via SQL</li>
 <li>Demonstrate a strong sense of ownership, of both technical and business</li>
</ul>
<p>outcomes</p>
<ul>
 <li>Assist dev and data science teams with processing and integrating data analysis</li>
 <li>Clearly document processes, methodologies, and tools used<b>Experience Required:</b></li>
 <li>B.S. in relevant technical degree</li>
 <li>Significant use and experience (at least 3-5 years) as a data engineer in the AWS ecosystem, including strong familiarity with structured and unstructured large datasets, enabling scalable and distributed compute, and ensuring real-time processingat scale</li>
 <li>Significant use and experience (at least 3-5 years) with writing complex SQL queries and analysis of data correlations</li>
 <li>At least a couple of years graph DB experience is required.</li>
 <li>Significant experience (at least 3-5 years) with the AWS ecosystem, including RedShift, Glue, and other tools, services, and resources that enable scalable,distributed compute</li>
 <li>Significant experience (at least 3-5 years) with multi-modal DB and ETL pipelines, including with graph databases</li>
 <li>Project management skills, ability to scope out timeline, methodology, and deliverables for development, testing, and integration into the platform</li>
 <li>Excellent communication and story-telling skills (written and verbal)</li>
</ul>
<p>Our Current Tech Stack: <b>Please note this is not our future tech stack.</b> AWS to host the infrastructure, including the CICD, SpringBoot, Angular, Python, PySpark, Kubernetes, EMR, Spark, Elasticsearch, RedShift, AWS (S3, Code Commit, Code Build, Code Deploy, EC2, EMR, etc.), Docker, Spacy, Scikit learn, Openpyxl, Streamlit, Watchdog, sklearn, seaborn, nltk, matplotlib, pandas, SQLAlchemy, and additional ML and python libraries.</p>
<p>This stack is subject to change as we build v2.0. We want to modernize and streamline our models, MLOps, code, deployment, front-end, and distributed processing capabilities.</p>
<p><b>Logistics:</b><br>Geography, Work Status, Etc. The position is remote. The candidate must have the legal right to work in the United States on a W2.</p>
<p><b>Interview Process:</b> We will conduct 3 rounds of interviews.</p>
<ul>
 <li>First Round: Culture, fit, and background interview with the company Founders</li>
 <li>Second Round: Technical Interview</li>
 <li>Technical Project: Execute a small data engineering project, if selected for the third round of interview</li>
 <li>Third Round: In-Person Day in Washington D.C. (We will have the candidate fly out to D.C. to meet the founders and team.) Present the results of the data engineering project during the In-Person Day.</li>
</ul>
<p><b>How to Apply: </b>Please provide the following:</p>
<ul>
 <li>Resume</li>
 <li>Cover Letter</li>
 <li>Any links to Git repositories or data engineering projects that we can review</li>
</ul>
<p><b>About the Company:</b> We are the source of truth for patent intelligence. Patents protect revenue and investment in the market. Given that, patent intelligence is not complete UNLESS it integrates financial and market data. We provide SaaS platforms that correlate multiple data sets (patent, financial, and people data) using scalable data science models, in order to answer fundamental questions related to patent and innovation strategy.</p>
<p>We provide patent intelligence to corporate IP departments and the defense sector. We are expanding to a larger commercial market, including technology transfer, venture capital, and financial institutions.</p>
<p>We are committed to creating a diverse environment and is proud to be an equal opportunity employer.</p>
<p>All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, pregnancy, disability, age, veteran status, or other characteristics.</p>
<p>Job Types: Permanent, Full-time</p>
<p>Pay: &#x24;115,000.00 - &#x24;145,000.00 per year</p>
<p>Benefits:</p>
<ul>
 <li>401(k)</li>
 <li>Dental insurance</li>
 <li>Health insurance</li>
 <li>Paid time off</li>
</ul>
<p>Compensation package:</p>
<ul>
 <li>Yearly pay</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>5 years</li>
 <li>6 years</li>
 <li>7 years</li>
 <li>8 years</li>
 <li>9 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>Monday to Friday</li>
</ul>
<p>Application Question(s):</p>
<ul>
 <li>Are you able to work full-time in the US on a W2 without a need for work sponsorship?</li>
</ul>
<p>Experience:</p>
<ul>
 <li>multi-modal DB and ETL pipelines: 4 years (Required)</li>
 <li>AWS: 4 years (Required)</li>
 <li>SQL: 4 years (Required)</li>
 <li>working with structured &amp; unstructured data: 4 years (Required)</li>
 <li>Redshift: 4 years (Required)</li>
 <li>ML Ops: 4 years (Required)</li>
 <li>Graph Databases: 3 years (Required)</li>
</ul>
<p>Work Location: Remote</p>",,"c0d7732f8a02f93e",,"Full-time","Permanent","Remote","Remote Sr. Data Engineer","30+ days ago","2023-09-25T11:51:13.648Z",,,"$115,000 - $145,000 a year","2023-10-25T11:51:13.656Z","US","remote","data engineer","https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0D6GBkCh8qDOKp8Q8Je90DE9vUrmQqPUpTEHGF5kpqBlXdYvOV26Rfr3QQlsqhJNC2ByknejdY85tMHZqy12fKI9ayhdlRi_QvXMWCU0S-2wD8aryJK1JQi64fHaArw_PYN8ZT9U1TFXyL5GYHNm8xzyJuXSxjj328Ku3Hp4jtKKHfgs7AIbLZqNU-rGYDLdycRpTXrFA8h0JQx3RwQfn5-klOorXCjIec8-8Q1MDs4VEWyBTpThNn_JzbWEvi4cBavi_lgd9kvNZZFFibtTQ-aIgzes_gt5EeJ5ZZXwroPxhcThH6BKiEXl5rVXNgE1XdkYTmh3lZPmSMqW0Mz_PXkw33Ut_JHFO02mmRM6wNONa7O2-v_dJD_35AD23WqF8aqD8seaXtgiK61v0LRq2AiiIirC3y_RSfyusNpxJ6kVyOVVLZ8kCyIVioAhlfqGSEiNLoI8kJieGcGkBNLH9yodbCn0ss_aX_XfjabI4tAn5Xd7kHczsngiuhDlnFq6nosWLQ3PwwZLWtczDIwCxGLEp2N03f6_d7wf9FISyit0H_6htVhFKcWw9yvDWo2ADBpQVDuCkQ4ZUyp3iW3Iccw&xkcb=SoAA-_M3JzdsT8A2lj0GbzkdCdPP&p=13&fvj=1&vjs=3&jsa=9389&tk=1hdjaoshm284l000&from=jasx&wvign=1"
"Allata LLC","Allata is growing our Data & Analytics Practice to serve our clients nationwide. Our data architect / lead engineer practitioners will be collaborating with data engineers, machine learning engineers, analysts, data scientists, and other Allata employees and client teams on projects for companies across the United States. 
 WHAT YOU'LL BE DOING 
 
  Work with customers to build cloud-based data platforms, including integration, data storage and analytics 
  Develop innovative architectures to solve complex business problems utilizing the latest cloud technologies 
  
 WHAT YOU'LL NEED 
 
  Data Architecture Best Practices. You’ve successfully built data solutions that use industry best practices and fit with an organization’s needs. You are excited by solving problems and voraciously consume technology to do so. You have a broad and deep technical background 
  Communication. You have a natural charisma and use it to build consensus. You can have a conversation with developers, business analysts, managers of all levels, and individuals in a business function. You are comfortable presenting in front of groups and explaining architectures in a variety of levels of detail. 
  Make teams better. You're excited to be part of a team that delivers with quality and works hard on new opportunities. You work well in fast-moving environments and have no problem working with others to resolve difficult problems. You support teams as much as others supports you. 
  
 DESIRED SKILLS & EXPERIENCE 
 
  8-10 years of experience in a data related field 
  Experience building data storage and analytic solutions utilizing Snowflake 
  Expertise in building data platforms in Azure or AWS 
  Experienced with ETL tools such as Azure Data Factory, AWS Glue, WhereScape RED, Streamsets, Informatica and SAP Convergent Mediation 
  Experience in one or more Cloud Data Warehouse (Azure SQL Data Warehouse / Synapse Analytics, Snowflake, Amazon Redshift, Google BigQuery) 
  Experience in one or more Data Visualization tool (Tableau, PowerBI) 
  Expertise modeling architectures and integrations for data environments including data pipelines, data lakes, data warehouses, and data marts. 
  Experience with data backup and recovery strategies, optimization of clusters, structured/semi structured data, and changing database storage and utilization requirements 
  Experience with scripting languages such as Python / R for Business 
  Experience with Event Driven Architecture (Kafka) 
  Experience with large-scale distributed storage and database systems (e.g. SQL, NoSQL, MySQL, Cassandra) 
  
 At Allata, we value differences. 
 Allata is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees. 
 Allata makes employment determinations without regard to race, color, creed, religion, age, ancestry, national origin, veteran status, sex, sexual orientation, gender, gender identity, gender expression, marital status, disability, or any other legally protected category. 
 This policy applies to all terms and conditions of employment, including but not limited to, recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training.","<div>
 <p>Allata is growing our Data &amp; Analytics Practice to serve our clients nationwide. Our data architect / lead engineer practitioners will be collaborating with data engineers, machine learning engineers, analysts, data scientists, and other Allata employees and client teams on projects for companies across the United States. </p>
 <p>WHAT YOU&apos;LL BE DOING</p> 
 <ul>
  <li>Work with customers to build cloud-based data platforms, including integration, data storage and analytics</li> 
  <li>Develop innovative architectures to solve complex business problems utilizing the latest cloud technologies</li> 
 </ul> 
 <p>WHAT YOU&apos;LL NEED</p> 
 <ul>
  <li>Data Architecture Best Practices. You&#x2019;ve successfully built data solutions that use industry best practices and fit with an organization&#x2019;s needs. You are excited by solving problems and voraciously consume technology to do so. You have a broad and deep technical background</li> 
  <li>Communication. You have a natural charisma and use it to build consensus. You can have a conversation with developers, business analysts, managers of all levels, and individuals in a business function. You are comfortable presenting in front of groups and explaining architectures in a variety of levels of detail.</li> 
  <li>Make teams better. You&apos;re excited to be part of a team that delivers with quality and works hard on new opportunities. You work well in fast-moving environments and have no problem working with others to resolve difficult problems. You support teams as much as others supports you.</li> 
 </ul> 
 <p>DESIRED SKILLS &amp; EXPERIENCE</p> 
 <ul>
  <li>8-10 years of experience in a data related field</li> 
  <li>Experience building data storage and analytic solutions utilizing Snowflake</li> 
  <li>Expertise in building data platforms in Azure or AWS</li> 
  <li>Experienced with ETL tools such as Azure Data Factory, AWS Glue, WhereScape RED, Streamsets, Informatica and SAP Convergent Mediation</li> 
  <li>Experience in one or more Cloud Data Warehouse (Azure SQL Data Warehouse / Synapse Analytics, Snowflake, Amazon Redshift, Google BigQuery)</li> 
  <li>Experience in one or more Data Visualization tool (Tableau, PowerBI)</li> 
  <li>Expertise modeling architectures and integrations for data environments including data pipelines, data lakes, data warehouses, and data marts.</li> 
  <li>Experience with data backup and recovery strategies, optimization of clusters, structured/semi structured data, and changing database storage and utilization requirements</li> 
  <li>Experience with scripting languages such as Python / R for Business</li> 
  <li>Experience with Event Driven Architecture (Kafka)</li> 
  <li>Experience with large-scale distributed storage and database systems (e.g. SQL, NoSQL, MySQL, Cassandra)</li> 
 </ul> 
 <p>At Allata, we value differences.</p> 
 <p>Allata is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.</p> 
 <p>Allata makes employment determinations without regard to race, color, creed, religion, age, ancestry, national origin, veteran status, sex, sexual orientation, gender, gender identity, gender expression, marital status, disability, or any other legally protected category.</p> 
 <p>This policy applies to all terms and conditions of employment, including but not limited to, recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training.</p>
</div>
<p></p>","https://allata.hrmdirect.com/employment/view.php?req=1745260&jbsrc=1014","b672536b1c067560",,,,"Remote","Data Architect/Lead Engineer - Consulting (Remote Possible)","30+ days ago","2023-09-25T11:51:05.217Z",,,,"2023-10-25T11:51:05.218Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=b672536b1c067560&from=jasx&tk=1hdjaoshm284l000&vjs=3"
"Sentara Healthcare","Sentara Healthcare is seeking to hire a qualified individual to join our team as a Cyber Security Data Protection Engineer.
        
         Position Status: Full-time, Day Shift
        
         Position Location: This position is 100% remote.
        
         Standard Working Hours: 8:00AM to 5:00PM (ET).
        
         Minimum Requirements:
        
          Practical working knowledge of engineering, implementing, operating, and supporting a Data Loss Prevention Platform. 
         Understanding of end-to-end implications of DLP implementation from business requirements to implementation challenges to constituent education to continual service improvement and support. 
         Experience with data protection and cloud tooling (DLP, Data Security, Encryption and CASB).
          A working knowledge of Cloud Security and supporting Technologies (e.g., AWS, Azure, GCP, SaaS, PaaS, DBaaS), particularly Data Loss Prevention within Microsoft O365, Azure Information Protection & AWS. 
         Proven experience of information protection and data classification technologies, concepts, and techniques to classify unstructured and structured data both on-premises and in the cloud.
          Understanding of vulnerability assessment tools, methodologies, and frameworks. 
         Ability to analyze and interpret vulnerability scan results and data protection requirements, identify risks, and recommend appropriate actions.
        
        
         Diversity and Inclusion at Sentara 
        Our vision is that everyone brings the strengths that come with diversity to work with them every day. When we are achieving our vision, we have team members that feel they belong and can be their authentic selves, and our workforce is reflective of the communities we serve.
         We are realizing this vision through our Diversity and Inclusion strategy, which has three pillars: A diverse and talented workforce, an inclusive and supportive workplace, and outreach and engagement with our community. We have made remarkable strides in these areas over the past several years and, as our world continues to evolve, we know our work is never done.
         Our strategies focus on both structural inclusion, which looks at our organizational structures, processes, and practices; as well as behavioral inclusion, which evaluates our mindsets, skillsets, and relationships. Together, these strategies are moving our organization forward in an environment that fosters a culture of mutual respect and belonging for all.
         Please visit the link below to learn more about Sentara’s commitment to diversity and inclusion:
        
         https://www.sentara.com/aboutus/mission-vision-and-values/diversity.aspx
        
         Sentara Overview For more than a decade, Modern Healthcare magazine has ranked Sentara Healthcare as one of the nation's top integrated healthcare systems. That's because we are dedicated to growth, innovation, and patient safety at more than 300 sites of care in Virginia and northeastern North Carolina, including 12 acute care hospitals.  Sentara Benefits As the third-largest employer in Virginia, Sentara Healthcare was named by Forbes Magazine as one of America's best large employers. We offer a variety of amenities to our employees, including, but not limited to:
        
        
          Medical, Dental, and Vision Insurance
          Paid Annual Leave, Sick Leave
          Flexible Spending Accounts
          Retirement funds with matching contribution
          Supplemental insurance policies, including legal, Life Insurance and AD&D among others
          Work Perks program including discounted movie and theme park tickets among other great deals
          Opportunities for further advancement within our organization
        
        
         Sentara employees strive to make our communities healthier places to live. We're setting the standard for medical excellence within a vibrant, creative, and highly productive workplace. For information about our employee benefits, please visit: Benefits - Sentara (sentaracareers.com)  Join our team! We are committed to quality healthcare, improving health every day, and provide the opportunity for training, development, and growth!
        
         Please Note: The Covid Vaccination(s) and yearly Flu Vaccination are required for employment.
        
         Note: Sentara Healthcare offers employees comprehensive health care and retirement benefits designed with you and your family's well-being in mind. Our benefits packages are designed to change with you by meeting your needs now and anticipating what comes next. You have a variety of options for medical, dental and vision insurance, life insurance, disability, and voluntary benefits as well as Paid Time Off in the form of sick time, vacation time and paid parental leave. Team Members have the opportunity to earn an annual flat amount Bonus payment if established system and employee eligibility criteria is met.
        
         For applicants within Washington State, the following hiring range will be applied: $70,215.60 to $117,026.00 annually.
       
      
     
    
  
 
 
   As the Cyber Security Data Protection Engineer, you will be responsible for designing, deploying, and maintaining technology to support Sentara Health’s data protection and vulnerability management strategies. As a Cyber Security Engineer focusing on vulnerability management and data protection, you will play a critical role in ensuring the security and integrity of our organization's sensitive data and information assets. Your responsibilities will include conducting vulnerability assessments, implementing data protection controls, and collaborating with cross-functional teams to mitigate risks. An Experienced Professional applies practical knowledge of job areas typically obtained through advanced education and work experience. Responsibilities typically include: • Works independently with general supervision. • Problems faced are difficult but typically not complex. • May influence others within the job area through explanation of facts, policies, and practices. Experience in lieu of Bachelor’s Degree 3 years of relevant experience with a degree 5+ years of relevant experience without a degree
 
 
 
  
    Experience:
  
  
    Practical working knowledge of engineering, implementing, operating, and supporting a Data Loss Prevention Platform. Understanding of end-to-end implications of DLP implementation from business requirements to implementation challenges to constituent education to continual service improvement and support. Experience with data protection and cloud tooling (DLP, Data Security, Encryption and CASB). A working knowledge of Cloud Security and supporting Technologies (e.g., AWS, Azure, GCP, SaaS, PaaS, DBaaS), particularly Data Loss Prevention within Microsoft O365, Azure Information Protection & AWS. Proven experience of information protection and data classification technologies, concepts, and techniques to classify unstructured and structured data both on-premises and in the cloud. Understanding of vulnerability assessment tools, methodologies, and frameworks. Ability to analyze and interpret vulnerability scan results and data protection requirements, identify risks, and recommend appropriate actions.
  
  
    Other Minimum Qualifications:
  
  
    Experience in monitoring and supporting IDS/IPS, Firewall, SIEM, DLP, vulnerability management tools, and log aggregation hardware/software required. Understanding of networking and infrastructure and understanding of core technologies such as IP Networking, L2/L3 network protocols (OSPF, BGP), LAN/WAN, TCP/IP, OSI Model, route, switch, DNS, DHCP, Domain Controllers, LDAP, SSO, QOS, VLAN, and ACL Basic working knowledge of IAM technologies such as AD, AAD, SAML, OAUTH, LDAP, Kerberos, and OpenID Basic working knowledge of Access Control and supporting concepts such as the principle of least privilege, RBAC, and MFA Solid working knowledge of ITIL (ITIL Certification preferred) A good understanding of Industry Security standards Exceptional interpersonal skills; must build strong relationships with partners (internally and externally) Strong problem solving and troubleshooting skills with the ability to exercise mature judgment Proven execution capabilities. Willingness to creatively ensure mission success
  
 
 
 
  
    Bachelor's Level Degree
  
 
 
  
    Information Technology 3 years
  
 
 
  
    Complex Problem Solving
    Coordination
    Critical Thinking
    Installation
    Judgment and Decision Making
    Microsoft Office
    Monitoring
    Service Orientation
    Speaking
    Systems Analysis
    Systems Evaluation
    Time Management
    Troubleshooting
    Writing
    Equipment Selection
    Instructing
    Leadership
    Project Management
    Technology Design
    Active Listening
    Communication","<div>
 <div>
  <ul>
   <li>
    <div>
     <div>
      <div>
       <div>
        <p><b>Sentara Healthcare</b> is seeking to hire a qualified individual to join our team as a <b>Cyber </b><b>Security Data Protection Engineer.</b></p>
        <p></p>
        <p><b> Position Status</b>: Full-time, Day Shift</p>
        <p></p>
        <p><b> Position Location:</b> This position is 100% remote.</p>
        <p></p>
        <p><b> Standard Working Hours</b>: 8:00AM to 5:00PM (ET).</p>
        <p></p>
        <p><b> Minimum Requirements:</b></p>
        <ul>
         <li> Practical working knowledge of engineering, implementing, operating, and supporting a Data Loss Prevention Platform. </li>
         <li>Understanding of end-to-end implications of DLP implementation from business requirements to implementation challenges to constituent education to continual service improvement and support. </li>
         <li>Experience with data protection and cloud tooling (DLP, Data Security, Encryption and CASB).</li>
         <li> A working knowledge of Cloud Security and supporting Technologies (e.g., AWS, Azure, GCP, SaaS, PaaS, DBaaS), particularly Data Loss Prevention within Microsoft O365, Azure Information Protection &amp; AWS. </li>
         <li>Proven experience of information protection and data classification technologies, concepts, and techniques to classify unstructured and structured data both on-premises and in the cloud.</li>
         <li> Understanding of vulnerability assessment tools, methodologies, and frameworks. </li>
         <li>Ability to analyze and interpret vulnerability scan results and data protection requirements, identify risks, and recommend appropriate actions.</li>
        </ul>
        <p></p>
        <p><b> Diversity and Inclusion at Sentara </b></p>
        <p>Our vision is that everyone brings the strengths that come with diversity to work with them every day. When we are achieving our vision, we have team members that feel they belong and can be their authentic selves, and our workforce is reflective of the communities we serve.</p>
        <p> We are realizing this vision through our Diversity and Inclusion strategy, which has three pillars: A diverse and talented workforce, an inclusive and supportive workplace, and outreach and engagement with our community. We have made remarkable strides in these areas over the past several years and, as our world continues to evolve, we know our work is never done.</p>
        <p> Our strategies focus on both <i>structural inclusion</i>, which looks at our organizational structures, processes, and practices; as well as <i>behavioral inclusion</i>, which evaluates our mindsets, skillsets, and relationships. Together, these strategies are moving our organization forward in an environment that fosters a culture of mutual respect and belonging for all.</p>
        <p><b> Please visit the link below to learn more about Sentara&#x2019;s commitment to diversity and inclusion:</b></p>
        <p></p>
        <p> https://www.sentara.com/aboutus/mission-vision-and-values/diversity.aspx</p>
        <p></p>
        <p><b> Sentara Overview</b><br> For more than a decade, Modern Healthcare magazine has ranked Sentara Healthcare as one of the nation&apos;s top integrated healthcare systems. That&apos;s because we are dedicated to growth, innovation, and patient safety at more than 300 sites of care in Virginia and northeastern North Carolina, including 12 acute care hospitals.<br> <br> <b>Sentara Benefits</b><br> As the third-largest employer in Virginia, Sentara Healthcare was named by Forbes Magazine as one of America&apos;s best large employers. We offer a variety of amenities to our employees, including, but not limited to:</p>
        <p></p>
        <ul>
         <li> Medical, Dental, and Vision Insurance</li>
         <li> Paid Annual Leave, Sick Leave</li>
         <li> Flexible Spending Accounts</li>
         <li> Retirement funds with matching contribution</li>
         <li> Supplemental insurance policies, including legal, Life Insurance and AD&amp;D among others</li>
         <li> Work Perks program including discounted movie and theme park tickets among other great deals</li>
         <li> Opportunities for further advancement within our organization</li>
        </ul>
        <p></p>
        <p> Sentara employees strive to make our communities healthier places to live. We&apos;re setting the standard for medical excellence within a vibrant, creative, and highly productive workplace. For information about our employee benefits, please visit: Benefits - Sentara (sentaracareers.com)<br> <br> Join our team! We are committed to quality healthcare, improving health every day, and provide the opportunity for training, development, and growth!</p>
        <p></p>
        <p><b> Please Note</b><b>:</b> The Covid Vaccination(s) and yearly Flu Vaccination are required for employment.</p>
        <p></p>
        <p><b><i> Note:</i></b><i> Sentara Healthcare offers employees comprehensive health care and retirement benefits designed with you and your family&apos;s well-being in mind. Our benefits packages are designed to change with you by meeting your needs now and anticipating what comes next. You have a variety of options for medical, dental and vision insurance, life insurance, disability, and voluntary benefits as well as Paid Time Off in the form of sick time, vacation time and paid parental leave. Team Members have the opportunity to earn an annual flat amount Bonus payment if established system and employee eligibility criteria is met.</i></p>
        <p></p>
        <p> For applicants within Washington State, the following hiring range will be applied: &#x24;70,215.60 to &#x24;117,026.00 annually.</p>
       </div>
      </div>
     </div>
    </div></li>
  </ul>
 </div>
 <div>
   As the Cyber Security Data Protection Engineer, you will be responsible for designing, deploying, and maintaining technology to support Sentara Health&#x2019;s data protection and vulnerability management strategies. As a Cyber Security Engineer focusing on vulnerability management and data protection, you will play a critical role in ensuring the security and integrity of our organization&apos;s sensitive data and information assets. Your responsibilities will include conducting vulnerability assessments, implementing data protection controls, and collaborating with cross-functional teams to mitigate risks. An Experienced Professional applies practical knowledge of job areas typically obtained through advanced education and work experience. Responsibilities typically include: &#x2022; Works independently with general supervision. &#x2022; Problems faced are difficult but typically not complex. &#x2022; May influence others within the job area through explanation of facts, policies, and practices. Experience in lieu of Bachelor&#x2019;s Degree 3 years of relevant experience with a degree 5+ years of relevant experience without a degree
 </div>
 <div></div>
 <div>
  <div>
   <p><b> Experience:</b></p>
  </div>
  <div>
   <p> Practical working knowledge of engineering, implementing, operating, and supporting a Data Loss Prevention Platform. Understanding of end-to-end implications of DLP implementation from business requirements to implementation challenges to constituent education to continual service improvement and support. Experience with data protection and cloud tooling (DLP, Data Security, Encryption and CASB). A working knowledge of Cloud Security and supporting Technologies (e.g., AWS, Azure, GCP, SaaS, PaaS, DBaaS), particularly Data Loss Prevention within Microsoft O365, Azure Information Protection &amp; AWS. Proven experience of information protection and data classification technologies, concepts, and techniques to classify unstructured and structured data both on-premises and in the cloud. Understanding of vulnerability assessment tools, methodologies, and frameworks. Ability to analyze and interpret vulnerability scan results and data protection requirements, identify risks, and recommend appropriate actions.</p>
  </div>
  <div>
   <p><b> Other </b><b>Minimum Qualifications</b><b>:</b></p>
  </div>
  <div>
   <p> Experience in monitoring and supporting IDS/IPS, Firewall, SIEM, DLP, vulnerability management tools, and log aggregation hardware/software required. Understanding of networking and infrastructure and understanding of core technologies such as IP Networking, L2/L3 network protocols (OSPF, BGP), LAN/WAN, TCP/IP, OSI Model, route, switch, DNS, DHCP, Domain Controllers, LDAP, SSO, QOS, VLAN, and ACL Basic working knowledge of IAM technologies such as AD, AAD, SAML, OAUTH, LDAP, Kerberos, and OpenID Basic working knowledge of Access Control and supporting concepts such as the principle of least privilege, RBAC, and MFA Solid working knowledge of ITIL (ITIL Certification preferred) A good understanding of Industry Security standards Exceptional interpersonal skills; must build strong relationships with partners (internally and externally) Strong problem solving and troubleshooting skills with the ability to exercise mature judgment Proven execution capabilities. Willingness to creatively ensure mission success</p>
  </div>
 </div>
 <div></div>
 <div>
  <ul>
   <li> Bachelor&apos;s Level Degree</li>
  </ul>
 </div>
 <div>
  <ul>
   <li> Information Technology 3 years</li>
  </ul>
 </div>
 <div>
  <ul>
   <li> Complex Problem Solving</li>
   <li> Coordination</li>
   <li> Critical Thinking</li>
   <li> Installation</li>
   <li> Judgment and Decision Making</li>
   <li> Microsoft Office</li>
   <li> Monitoring</li>
   <li> Service Orientation</li>
   <li> Speaking</li>
   <li> Systems Analysis</li>
   <li> Systems Evaluation</li>
   <li> Time Management</li>
   <li> Troubleshooting</li>
   <li> Writing</li>
   <li> Equipment Selection</li>
   <li> Instructing</li>
   <li> Leadership</li>
   <li> Project Management</li>
   <li> Technology Design</li>
   <li> Active Listening</li>
   <li> Communication</li>
  </ul>
 </div>
</div>","https://www.sentaracareers.com/job/18839114/cyber-security-data-protection-engineer-virginia-beach-va/?utm_medium=symphonytalent-jobads&utm_campaign=Default%20Campaign&utm_content=Cyber%20Security%20Data%20Protection%20Engineer&utm_term=JR-36011&utm_source=Indeed","3d3d61d35b4b3598",,"Full-time",,"5460 Wesleyan Dr, Virginia Beach, VA 23455","Cyber Security Data Protection Engineer","30+ days ago","2023-09-25T11:51:09.945Z","3.8","2753","$70,215.60 - $117,026.00 a year","2023-10-25T11:51:09.947Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=3d3d61d35b4b3598&from=jasx&tk=1hdjaoshm284l000&vjs=3"
"Ford Motor Company","Ford is redefining how it will build a better world for the future by creating a culture and opportunities where employees will capitalize on Ford’s excellence in designing and building the most iconic products while also helping develop the future of world-class connected Battery Electric Vehicles (BEV). We call this new business Model e. The creation of Model e was informed by the success of small, passionate Ford teams that developed the Mustang Mach-E SUV and F-150 Lightning pickup, as well as Ford’s dedicated EV division in China. Similarly, dedicated teams within Model e will create the software platforms and fully networked vehicle architectures to support delightful, always-on and ever-improving vehicles and experiences that will allow us to deliver over 50% of our total sales in BEV and 33 million connected vehicles by 2030. We are building a team of the world’s best software, electrical and automotive talent and turning them loose to create shockingly great electric vehicles and digital experiences for new generations of customers. These changes in our business will allow us the greatest opportunity to bring value and serve customers since Henry Ford scaled the Model T. 
 
 At Model e we are passionate about solving hard problems and delivering extraordinary solutions to change the world. We challenge competitors, bureaucracy, obstacles and even our own assumptions. We have zero tolerance for bias or personal agendas. We are swift and decisive, trusting the experts, believing the data, seeking out and listening to bold intuition – regardless of the source. We are insurgents; averse to complexity, suspicious of habits and open to risk. We constantly learn. We focus on the mission and deliver with urgency and speed. We are an agile team, leveraging rapidly evolving technology to innovate, learn and iterate. We trust each other, surface issues openly and solve problems collaboratively. Most importantly we raise the talent bar with every hire.
 


 Minimum requirements we seek: 
 
  Bachelor’s degree in an appropriate field of study 
  3+ years of experience, with specific focus on data engineering and orchestration 
  Technical qualifications: 
   
    3+ years of DE experience with end-to-end cloud-based development (develop, test, industrialize, deploy), Google Cloud preferred 
    3+ years of Coding experience with Python, SQL, Spark 
    2+ years of experience orchestrating automated data pipelines, preferably using Airflow 
    Basic understanding of Machine Learning and Statistical Modeling 
   
 
 Our preferred qualifications: 
 
  Master’s degree, PhD, or MBA in an appropriate field of study 
  North America and global experience in supply chain and/or inventory management (demand forecasting/sensing, inventory allocation and replenishment, integrated business planning) 
  Tangible experience with the full life cycle of building digital solutions (from PoC through to Production) 
  Relevant experience linking business requirements to digital product development 
  Record of accomplishment working DevOps and agile (vs. waterfall approach) 
  Disruptor – willing to push for change 
  Challenger mindset – ability to push past status quo & obstacles to implement vision 
 
 What you'll receive in return: 
 At Model e we strive to support our employees' success professionally and personally with compensation and rewards that fit the needs of their lifestyle. 
 Our employees enjoy competitive compensation and benefits including a “Work from Anywhere” structure and the support needed to be successful in a remote environment. 
 Candidates for positions with Ford Motor Company must be legally authorized to work in the U.S. Verification of employment eligibility will be required at time of hire. Visa sponsorship is not available for this position. 
 Ford Motor Company is an equal opportunity employer committed to a culturally diverse workforce. All qualified applicants will receive consideration for employment without regard to race, religion, color, age, sex, national origin, sexual orientation, gender identity, disability status or protected veteran status. 
 
 For a detailed look at our benefits, click here 
 Visa sponsorship is not available for this position. 
 
 #LI-Hybrid
 


 What you’ll be able to do: 
 This role will provide you with a unique opportunity to build a data and analytics ecosystem to drive important inventory optimization use cases within the recently launched Model e business. The Data Engineer will drive and lead building, maintaining, and refining the data and feature engineering pipelines in google cloud that will be fundamental to Model e’s order fulfillment and inventory management capabilities, including demand sensing, allocation optimization, and overall S&OP process optimization and alignment. 
 The Data Engineer will be an integral part of the Customer Business Systems technical team for the Model e business. Reporting to the GM of Model e Customer Business Systems, this person will lead the development and maintenance of a data foundation. 
 
 The Position: 
 The Data Engineer will build and maintain Data Foundation that will be central to Model e’s Global Sales and Operations Planning (S&OP) and North American Order Fulfillment capabilities. This technical leader will interface directly with Ford’s Global Data Insights & Analytics team (GDIA) as well as with Ford IT to build and maintain data pipelines from disparate sources affecting the allocation and optimization of forward deployed inventory. They will also play a critical role in delivering the frontiers of Model e’s inventory optimization (e.g., demand sensing, inventory allocation – placement, replenishment and in stock service level maintance.) and will be critical to unlocking sustainable value creation for Ford’s Model e EV business. 
 The Person: 
 The Data Engineer will have a strong technical background, with experience building and deploying data pipelines in a cloud environment. The ideal candidate will have strong data engineering and orchestration capabilities (Python, SQL, Airflow, Spark), experience working on cloud-based systems (ideally Google Cloud) with a solid understanding of simplicity and scalability of data pipelines and interfaces. 
 They will be a natural problem solver and strategist who will enjoy working in an entrepreneurial environment and stitching together components from the traditional Ford business with transformational ideas and concepts to drive the Model e go to market strategy. They will have the ability to build critical relationships and work cross functionally with members of marketing, sales, finance, GDIA, IT, etc. As a technical leader they will mentor junior team members and help build a culture of trust and a sense of belonging, accountability, speed, and the ability to constructively challenge each other. They will have a strong bias to action, iterate quickly and have fun while doing the best work of their career.","<div>
 <p>Ford is redefining how it will build a better world for the future by creating a culture and opportunities where employees will capitalize on Ford&#x2019;s excellence in designing and building the most iconic products while also helping develop the future of world-class connected Battery Electric Vehicles (BEV). We call this new business Model e. The creation of Model e was informed by the success of small, passionate Ford teams that developed the Mustang Mach-E SUV and F-150 Lightning pickup, as well as Ford&#x2019;s dedicated EV division in China. Similarly, dedicated teams within Model e will create the software platforms and fully networked vehicle architectures to support delightful, always-on and ever-improving vehicles and experiences that will allow us to deliver over 50% of our total sales in BEV and 33 million connected vehicles by 2030. We are building a team of the world&#x2019;s best software, electrical and automotive talent and turning them loose to create shockingly great electric vehicles and digital experiences for new generations of customers. These changes in our business will allow us the greatest opportunity to bring value and serve customers since Henry Ford scaled the Model T.<br> </p>
 <p></p>
 <p>At Model e we are <b>passionate </b>about solving hard problems and delivering extraordinary solutions to change the world. We <b>challenge </b>competitors, bureaucracy, obstacles and even our own assumptions. We have zero tolerance for bias or personal agendas. We are swift and decisive, trusting the experts, believing the data, seeking out and listening to bold intuition &#x2013; regardless of the source. We are <b>insurgents; </b>averse to complexity, suspicious of habits and open to risk. We constantly learn. We focus on the mission and deliver with urgency and speed. We are an <b>agile </b>team, leveraging rapidly evolving technology to innovate, learn and iterate. We trust each other, surface issues openly and solve problems collaboratively. Most importantly we raise the talent bar with every hire.</p>
</div> 
<br>
<div>
 <p><b>Minimum requirements we seek: </b></p>
 <ul>
  <li>Bachelor&#x2019;s degree in an appropriate field of study </li>
  <li>3+ years of experience, with specific focus on data engineering and orchestration </li>
  <li>Technical qualifications: 
   <ul>
    <li>3+ years of DE experience with end-to-end cloud-based development (develop, test, industrialize, deploy), Google Cloud preferred </li>
    <li>3+ years of Coding experience with Python, SQL, Spark </li>
    <li>2+ years of experience orchestrating automated data pipelines, preferably using Airflow </li>
    <li>Basic understanding of Machine Learning and Statistical Modeling </li>
   </ul></li>
 </ul>
 <p><b>Our preferred qualifications: </b></p>
 <ul>
  <li>Master&#x2019;s degree, PhD, or MBA in an appropriate field of study </li>
  <li>North America and global experience in supply chain and/or inventory management (demand forecasting/sensing, inventory allocation and replenishment, integrated business planning) </li>
  <li>Tangible experience with the full life cycle of building digital solutions (from PoC through to Production) </li>
  <li>Relevant experience linking business requirements to digital product development </li>
  <li>Record of accomplishment working DevOps and agile (vs. waterfall approach) </li>
  <li>Disruptor &#x2013; willing to push for change </li>
  <li>Challenger mindset &#x2013; ability to push past status quo &amp; obstacles to implement vision </li>
 </ul>
 <p><b>What you&apos;ll receive in return: </b></p>
 <p>At Model e we strive to support our employees&apos; success professionally and personally with compensation and rewards that fit the needs of their lifestyle. </p>
 <p>Our employees enjoy competitive compensation and benefits including a &#x201c;Work from Anywhere&#x201d; structure and the support needed to be successful in a remote environment. </p>
 <p>Candidates for positions with Ford Motor Company must be legally authorized to work in the U.S. Verification of employment eligibility will be required at time of hire. Visa sponsorship is not available for this position. </p>
 <p>Ford Motor Company is an equal opportunity employer committed to a culturally diverse workforce. All qualified applicants will receive consideration for employment without regard to race, religion, color, age, sex, national origin, sexual orientation, gender identity, disability status or protected veteran status.<br> </p>
 <p></p>
 <p>For a detailed look at our benefits, click here </p>
 <p>Visa sponsorship is not available for this position.<br> </p>
 <p></p>
 <p>#LI-Hybrid</p>
</div> 
<br>
<div>
 <p><b>What you&#x2019;ll be able to do: </b></p>
 <p>This role will provide you with a unique opportunity to build a data and analytics ecosystem to drive important inventory optimization use cases within the recently launched Model e business. The Data Engineer will drive and lead building, maintaining, and refining the data and feature engineering pipelines in google cloud that will be fundamental to Model e&#x2019;s order fulfillment and inventory management capabilities, including demand sensing, allocation optimization, and overall S&amp;OP process optimization and alignment. </p>
 <p>The Data Engineer will be an integral part of the Customer Business Systems technical team for the Model e business. Reporting to the GM of Model e Customer Business Systems, this person will lead the development and maintenance of a data foundation.<br> </p>
 <p></p>
 <p><b>The Position: </b></p>
 <p>The Data Engineer will build and maintain Data Foundation that will be central to Model e&#x2019;s Global Sales and Operations Planning (S&amp;OP) and North American Order Fulfillment capabilities. This technical leader will interface directly with Ford&#x2019;s Global Data Insights &amp; Analytics team (GDIA) as well as with Ford IT to build and maintain data pipelines from disparate sources affecting the allocation and optimization of forward deployed inventory. They will also play a critical role in delivering the frontiers of Model e&#x2019;s inventory optimization (e.g., demand sensing, inventory allocation &#x2013; placement, replenishment and in stock service level maintance.) and will be critical to unlocking sustainable value creation for Ford&#x2019;s Model e EV business. </p>
 <p><b>The Person: </b></p>
 <p>The Data Engineer will have a strong technical background, with experience building and deploying data pipelines in a cloud environment. The ideal candidate will have strong data engineering and orchestration capabilities (Python, SQL, Airflow, Spark), experience working on cloud-based systems (ideally Google Cloud) with a solid understanding of simplicity and scalability of data pipelines and interfaces. </p>
 <p>They will be a natural problem solver and strategist who will enjoy working in an entrepreneurial environment and stitching together components from the traditional Ford business with transformational ideas and concepts to drive the Model e go to market strategy. They will have the ability to build critical relationships and work cross functionally with members of marketing, sales, finance, GDIA, IT, etc. As a technical leader they will mentor junior team members and help build a culture of trust and a sense of belonging, accountability, speed, and the ability to constructively challenge each other. They will have a strong bias to action, iterate quickly and have fun while doing the best work of their career.</p>
</div>","https://efds.fa.em5.oraclecloud.com/hcmUI/CandidateExperience/en/sites/CX_1/requisitions/preview/16569?utm_medium=jobboard&utm_source=indeed","38d9d96a49f7e3ba",,"Full-time",,"1907 Michigan Ave, Detroit, MI 48216","Data Engineer, Model e Customer Business Systems","30+ days ago","2023-09-25T11:51:20.676Z","4.1","8825",,"2023-10-25T11:51:20.683Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=38d9d96a49f7e3ba&from=jasx&tk=1hdjaoshm284l000&vjs=3"
"KeyLogic Systems","Position: Cyber Security Data Integration Engineer/Developer - 3101 Location: Remote/Dulles, VA Salary Range: $140-150K Clearance: Secret (Refer to Required Skills) 
  
    KeyLogic is supporting a U.S. Government customer on a large mission critical development and sustainment program to design, build, deliver, and operate a network operations environment; including introducing new cyber capabilities to address emerging threats. KeyLogic is seeking a 
   Cyber Security Data Integration Engineer/Developer to support the design, development, and deployment of advanced cybersecurity capabilities.
    
    
   Job Responsibilities:
    KeyLogic is seeking a Security Engineer to play a key role in supporting a statewide program providing cyber assessment services and management that will protect 20+ affiliates from growing and evolving cyber threats. The engineering effort will focus on cloud security, SIEM and log management, and endpoint detection/response protecting customers from the ever growing and evolving cyber threats. This person will also work with customers to ensure the organization’s compliance standards are met and maintained while also driving solid customer relationships to the next level.
    
    This position requires a thorough understanding of network architecture fundamentals, protocols, routing, firewalls, cloud, and DevOps. This position is part of a larger team; however, the candidate is expected to work well on his or her own under general supervision, be self-directed, able to multi-task, and prioritize work.
    
    
   Required Skills:
    
   
    U.S. Citizenship required
    Active Secret clearance and must be able to obtain a TS/SCI clearance
    Must be able to obtain DHS Suitability
    6+ years of directly relevant experience
    4+ years of experience with administration of enterprise SIEM technologies (Splunk primarily)
    Splunk Cloud experience: Architect, design, engineer, support, configure, administer content and maintain infrastructure for a highly available and disaster recovery configuration
    Splunk experience: Administer Splunk and Splunk Application for Enterprise Security log or event management
    Expertise with EDR toolsets – administration, analysis, and integrations preferably CrowdStrike
    Familiarity with SOAR Products include Phantom and ThreatConnect
    Experience with scripting (e.g., PowerShell, bash/ksh/sh,python)
    Ability to assist team with Incident response and handling
    Excellent demonstrated experience in communicating technical information to non-technical and technical audiences.
    Experience working directly with senior leadership and management.
   
    
    Desired Skills:
    
   
    Automation: Experience related to Ansible for performing administration using code and Git/Gitlab for workflow management
    Familiarity with Windows and Linux integration, SQL database technologies, troubleshooting, deployment, patching, and administration
    Experience with Logstash and ability to collect, parse, and transform logs
    Experience with the standards compliance process (e.g., NIST) and writing network security documentation
   
    
    Required Education:
    
   
    Bachelor’s degree in Systems Engineering, Computer Science or related degree. Two years of related work experience may be substituted for each year of degree level education.
   
    
    Desired Certifications:
    
   
    Splunk IT Service Intelligence Certified Admin, Splunk Enterprise Security Certified Admin, Splunk Cloud Certified Admin, CCNA, CCNP)
   
   
 
 
 
  
   
    
     At KeyLogic we recognize that our employees are our most valuable resources. We hire talented, qualified professionals and provide each of our employees with every resource and opportunity to excel in their day-to-day activities as well as advance their career. 
      KeyLogic is a highly successful provider of professional and engineering services. We specialize in solutions that enable our customers to make better decisions for their organization. KeyLogic’s performance has earned the company a solid reputation for high standards, proactive solutions, and an outstanding commitment to the customer, best exemplified by the fact we have never had a one-time federal customer — all of our customers have provided repeat business. This has led us to achieve significant growth every year since our founding in 1999. 
      At KeyLogic, we're known for our extraordinary commitment to the success of the organizations we serve. Our client list includes the Department of Defense (DoD), Environmental Protection Agency (EPA), Energy (DOE), Transportation (DOT) and Treasury (including the Internal Revenue Service (IRS)), General Services Administration (GSA), and the National Aeronautics and Space Administration (NASA). 
      All qualified applicants will receive consideration for employment at KeyLogic without regard to race; creed; color; religion; national origin; sex; age; disability; sexual orientation; gender identity or expression; genetic predisposition or carrier status; veteran, marital; or any other status protected by law. KeyLogic is proud to be an affirmative action and equal opportunity employer. 
      NOTE: KeyLogic is an Equal Employment/Affirmative Action employer. We do not discriminate in hiring on the basis of sex, gender identity, sexual orientation, race, color, religious creed, national origin, physical or mental disability, protected Veteran status, or any other characteristic protected by federal, state, or local law. 
      If you need a reasonable accommodation for any part of the employment process, please contact us by email at Recruiting@KeyLogic.com and let us know the nature of your request and your contact information. Request for accommodation will be considered on a case-by-case basis. 
     
   
  
 
 
  
   
    
     Job Code:
     
    
     1907","<div>
 <div>
  <p>Position: Cyber Security Data Integration Engineer/Developer - 3101<br> Location: Remote/Dulles, VA<br> Salary Range: &#x24;140-150K<br> Clearance: Secret (Refer to Required Skills)</p> 
  <div>
    KeyLogic is supporting a U.S. Government customer on a large mission critical development and sustainment program to design, build, deliver, and operate a network operations environment; including introducing new cyber capabilities to address emerging threats. KeyLogic is seeking a 
   <i>Cyber Security Data Integration Engineer/Developer</i> to support the design, development, and deployment of advanced cybersecurity capabilities.
   <br> 
   <br> 
   <b>Job Responsibilities:</b>
   <br> KeyLogic is seeking a Security Engineer to play a key role in supporting a statewide program providing cyber assessment services and management that will protect 20+ affiliates from growing and evolving cyber threats. The engineering effort will focus on cloud security, SIEM and log management, and endpoint detection/response protecting customers from the ever growing and evolving cyber threats. This person will also work with customers to ensure the organization&#x2019;s compliance standards are met and maintained while also driving solid customer relationships to the next level.
   <br> 
   <br> This position requires a thorough understanding of network architecture fundamentals, protocols, routing, firewalls, cloud, and DevOps. This position is part of a larger team; however, the candidate is expected to work well on his or her own under general supervision, be self-directed, able to multi-task, and prioritize work.
   <br> 
   <br> 
   <b>Required Skills:</b>
   <br> 
   <ul>
    <li>U.S. Citizenship required</li>
    <li>Active Secret clearance and must be able to obtain a TS/SCI clearance</li>
    <li>Must be able to obtain DHS Suitability</li>
    <li>6+ years of directly relevant experience</li>
    <li>4+ years of experience with administration of enterprise SIEM technologies (Splunk primarily)</li>
    <li>Splunk Cloud experience: Architect, design, engineer, support, configure, administer content and maintain infrastructure for a highly available and disaster recovery configuration</li>
    <li>Splunk experience: Administer Splunk and Splunk Application for Enterprise Security log or event management</li>
    <li>Expertise with EDR toolsets &#x2013; administration, analysis, and integrations preferably CrowdStrike</li>
    <li>Familiarity with SOAR Products include Phantom and ThreatConnect</li>
    <li>Experience with scripting (e.g., PowerShell, bash/ksh/sh,python)</li>
    <li>Ability to assist team with Incident response and handling</li>
    <li>Excellent demonstrated experience in communicating technical information to non-technical and technical audiences.</li>
    <li>Experience working directly with senior leadership and management.</li>
   </ul>
   <br> 
   <b> Desired Skills:</b>
   <br> 
   <ul>
    <li>Automation: Experience related to Ansible for performing administration using code and Git/Gitlab for workflow management</li>
    <li>Familiarity with Windows and Linux integration, SQL database technologies, troubleshooting, deployment, patching, and administration</li>
    <li>Experience with Logstash and ability to collect, parse, and transform logs</li>
    <li>Experience with the standards compliance process (e.g., NIST) and writing network security documentation</li>
   </ul>
   <br> 
   <b> Required Education:</b>
   <br> 
   <ul>
    <li>Bachelor&#x2019;s degree in Systems Engineering, Computer Science or related degree. Two years of related work experience may be substituted for each year of degree level education.</li>
   </ul>
   <br> 
   <b> Desired Certifications:</b>
   <br> 
   <ul>
    <li>Splunk IT Service Intelligence Certified Admin, Splunk Enterprise Security Certified Admin, Splunk Cloud Certified Admin, CCNA, CCNP)</li>
   </ul>
  </div> 
 </div>
 <p></p>
 <div>
  <div>
   <div>
    <div>
     <p><b>At KeyLogic we recognize that our employees are our most valuable resources. We hire talented, qualified professionals and provide each of our employees with every resource and opportunity to excel in their day-to-day activities as well as advance their career.</b></p> 
     <p><i> KeyLogic is a highly successful provider of professional and engineering services. We specialize in solutions that enable our customers to make better decisions for their organization. KeyLogic&#x2019;s performance has earned the company a solid reputation for high standards, proactive solutions, and an outstanding commitment to the customer, best exemplified by the fact we have never had a one-time federal customer &#x2014; all of our customers have provided repeat business. This has led us to achieve significant growth every year since our founding in 1999.</i></p> 
     <p><i> At KeyLogic, we&apos;re known for our extraordinary commitment to the success of the organizations we serve. Our client list includes the Department of Defense (DoD), Environmental Protection Agency (EPA), Energy (DOE), Transportation (DOT) and Treasury (including the Internal Revenue Service (IRS)), General Services Administration (GSA), and the National Aeronautics and Space Administration (NASA).</i></p> 
     <p><i> All qualified applicants will receive consideration for employment at KeyLogic without regard to race; creed; color; religion; national origin; sex; age; disability; sexual orientation; gender identity or expression; genetic predisposition or carrier status; veteran, marital; or any other status protected by law. KeyLogic is proud to be an affirmative action and equal opportunity employer.</i></p> 
     <p><i> NOTE: KeyLogic is an Equal Employment/Affirmative Action employer. We do not discriminate in hiring on the basis of sex, gender identity, sexual orientation, race, color, religious creed, national origin, physical or mental disability, protected Veteran status, or any other characteristic protected by federal, state, or local law.</i></p> 
     <p><i> If you need a reasonable accommodation for any part of the employment process, please contact us by email at </i>Recruiting@KeyLogic.com<i> and let us know the nature of your request and your contact information. Request for accommodation will be considered on a case-by-case basis.</i></p> 
    </div> 
   </div>
  </div>
 </div>
 <div>
  <div>
   <div>
    <div>
     Job Code:
    </div> 
    <div>
     <b>1907</b>
    </div>
   </div>
  </div>
 </div>
</div>
<p></p>","https://phg.tbe.taleo.net/phg02/ats/careers/v2/viewRequisition?org=KEYLOGIC&cws=37&rid=1907&source=Indeed","9eb823fbea56740f",,,,"Dulles, VA","Cyber Security Data Integration Engineer/Developer - 3101","30+ days ago","2023-09-25T11:51:21.158Z","3.9","15","$140,000 - $150,000 a year","2023-10-25T11:51:21.160Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=9eb823fbea56740f&from=jasx&tk=1hdjaoshm284l000&vjs=3"
"CDW","The Senior Software Engineer II – Data will play a pivotal role in building and operationalizing the minimally inclusive data necessary for the enterprise data and analytics initiatives following best practices The bulk of the data engineer’s work would be in building, managing and optimizing data pipelines and then moving these data pipelines effectively into production for key data and analytics consumers like business/data analysts, data scientists or any persona that needs curated data for data and analytics use cases across the enterprise.
  
  
 Key Areas of Responsibility 
 
  Develops and maintains scalable data pipelines to support continuing increases in data volume and complexity. 
  Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using big data technologies and SQL 
  Collaborates with business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization. 
  Collaborate with other technology teams to help engineer data sets that data science teams uses to implement advanced analytics algorithms that exploit our rich datasets for statistical analysis, prediction, clustering and machine learning. 
  Responsible for using innovative and modern tools, techniques and architectures to partially or completely automate the most-common, repeatable and tedious data preparation and integration tasks in order to minimize manual and error-prone processes and improve productivity. 
  Train counterparts such as data scientists, data analysts, data consumers in data pipelines and preparation techniques, which make it easier for them to integrate and consume the data they need for their own use cases. 
  Helps ensure compliance and governance during use of data. 
  Be curious and knowledgeable about new data management techniques and how to apply them to solve business problems. 
  
 
 Education and/or Experience Qualifications 
 
  BS or MS degree in Computer Science or a related technical field. 
  7+ years data application development experience 
  
 Required Qualifications 
 
  Strong experience with various Data Management architectures like Data Warehouse, Data Lake, Data Hub and the supporting processes like Data Integration, Governance, Metadata Management. 
  Extensive experience with popular data processing languages including SQL, PL/SQL, Python, others for relational databases and on NoSQL/Hadoop oriented databases like MongoDB, Cassandra, others for nonrelational databases. 
  Demonstrated ability to build rapport and maintain productive working relationships cross-departmentally and cross-functionally. 
  Demonstrated ability to coach and mentor others. 
  Excellent written and verbal communication skills with the ability to effectively interact with and present to all stakeholders including senior leadership. 
  Strong organizational, planning and creative problem solving-skills with critical attention to detail. 
  Demonstrated success of facilitation and solutions implementation. 
  History of balancing competing priorities with the ability to adapt to the changing needs of the business while meeting deadlines. 
  
 Preferred Qualifications 
 
  Extensive experience with Azure Data Factory 
  Extensive experience working with cloud platform (at least one of Azure, AWS, GCP) 
  Experience with ETL Tools (SSIS, Informatica, Ab Initio, Talend), Python, Databricks, Microsoft SQL Server Platform (version 2012 or later), and/or working in an Agile environment
 
  
  
 COVID-19 Update: CDW is committed to maintaining a workplace that is free of known hazards and to ensuring the safety, health, and well-being of coworkers and candidates for employment and their families, as well as the community. 
 CDW requires all coworkers be fully vaccinated against COVID-19, with the only exceptions being a documented, legally required medical or religious accommodation. Prior to starting with CDW, successful candidates will be required to: (i) be fully vaccinated against COVID-19 and provide CDW with proof of full vaccination; or (ii) apply for and receive a medical or religious-based accommodation to be exempt from the mandatory vaccination policy.","<p></p>
<div>
 <p>The Senior Software Engineer II &#x2013; Data will play a pivotal role in building and operationalizing the minimally inclusive data necessary for the enterprise data and analytics initiatives following best practices The bulk of the data engineer&#x2019;s work would be in building, managing and optimizing data pipelines and then moving these data pipelines effectively into production for key data and analytics consumers like business/data analysts, data scientists or any persona that needs curated data for data and analytics use cases across the enterprise.</p>
 <br> 
 <p></p> 
 <p>Key Areas of Responsibility</p> 
 <ul>
  <li>Develops and maintains scalable data pipelines to support continuing increases in data volume and complexity.</li> 
  <li>Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using big data technologies and SQL</li> 
  <li>Collaborates with business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.</li> 
  <li>Collaborate with other technology teams to help engineer data sets that data science teams uses to implement advanced analytics algorithms that exploit our rich datasets for statistical analysis, prediction, clustering and machine learning.</li> 
  <li>Responsible for using innovative and modern tools, techniques and architectures to partially or completely automate the most-common, repeatable and tedious data preparation and integration tasks in order to minimize manual and error-prone processes and improve productivity.</li> 
  <li>Train counterparts such as data scientists, data analysts, data consumers in data pipelines and preparation techniques, which make it easier for them to integrate and consume the data they need for their own use cases.</li> 
  <li>Helps ensure compliance and governance during use of data.</li> 
  <li>Be curious and knowledgeable about new data management techniques and how to apply them to solve business problems.</li> 
 </ul> 
 <p></p>
 <p>Education and/or Experience Qualifications</p> 
 <ul>
  <li>BS or MS degree in Computer Science or a related technical field.</li> 
  <li>7+ years data application development experience</li> 
 </ul> 
 <p>Required Qualifications</p> 
 <ul>
  <li>Strong experience with various Data Management architectures like Data Warehouse, Data Lake, Data Hub and the supporting processes like Data Integration, Governance, Metadata Management.</li> 
  <li>Extensive experience with popular data processing languages including SQL, PL/SQL, Python, others for relational databases and on NoSQL/Hadoop oriented databases like MongoDB, Cassandra, others for nonrelational databases.</li> 
  <li>Demonstrated ability to build rapport and maintain productive working relationships cross-departmentally and cross-functionally.</li> 
  <li>Demonstrated ability to coach and mentor others.</li> 
  <li>Excellent written and verbal communication skills with the ability to effectively interact with and present to all stakeholders including senior leadership.</li> 
  <li>Strong organizational, planning and creative problem solving-skills with critical attention to detail.</li> 
  <li>Demonstrated success of facilitation and solutions implementation.</li> 
  <li>History of balancing competing priorities with the ability to adapt to the changing needs of the business while meeting deadlines.</li> 
 </ul> 
 <p>Preferred Qualifications</p> 
 <ul>
  <li>Extensive experience with Azure Data Factory</li> 
  <li>Extensive experience working with cloud platform (at least one of Azure, AWS, GCP)</li> 
  <li>Experience with ETL Tools (SSIS, Informatica, Ab Initio, Talend), Python, Databricks, Microsoft SQL Server Platform (version 2012 or later), and/or working in an Agile environment</li>
 </ul>
 <br> 
 <p> </p>
 <p>COVID-19 Update:<br> CDW is committed to maintaining a workplace that is free of known hazards and to ensuring the safety, health, and well-being of coworkers and candidates for employment and their families, as well as the community.</p> 
 <p>CDW requires all coworkers be fully vaccinated against COVID-19, with the only exceptions being a documented, legally required medical or religious accommodation. Prior to starting with CDW, successful candidates will be required to: (i) be fully vaccinated against COVID-19 and provide CDW with proof of full vaccination; or (ii) apply for and receive a medical or religious-based accommodation to be exempt from the mandatory vaccination policy.</p>
</div>","https://www.cdwjobs.com/jobs/11855463-sr-software-engineer-ii-data?rx_campaign=indeed0&rx_ch=jobp4p&rx_group=109109&rx_job=23000239&rx_r=none&rx_source=Indeed&rx_ts=20231025T080339Z&rx_vp=cpc&tm_company=2376&tm_event=view&tm_job=23000239&bid=326&src=JB-10640&rx_p=JHJ8ICEZBG&rx_viewer=d2021152732c11eebd3cad5ba72df949018fedff64d34dc9931d45303535c9ce","e88a350480debfa8",,"Full-time",,"Remote","SR Software Engineer II, Cloud (Azure) Data","30+ days ago","2023-09-25T11:51:18.668Z","3.7","957",,"2023-10-25T11:51:18.670Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=e88a350480debfa8&from=jasx&tk=1hdjaoshm284l000&vjs=3"
"Corebridge Financial","Who we are
 
 
   Corebridge Financial helps people make some of the most meaningful decisions they’re ever going to make. We help them plan and take action to protect the future they envision, and respond to some of life’s most difficult moments through the solutions and services we provide. We do this through our broad portfolio of life insurance, retirement and institutional products, offered through an extensive, multichannel distribution network. We provide solutions for a brighter future through our client centered service, breadth of product expertise, deep distribution relationships, and outstanding team of hardworking and passionate employees.
 
 
 
   About the role
 
 
   As a Sr. Data & ETL Platform Engineer you will be responsible for leading, influencing AIG’s Datahub platform to address needs of all enterprise datasets at AIG. You will engineer for reliability, scalability, performance, observability and supportability of enterprise Datahub environment.
 
 
   In this role, you will be responsible for administering, supporting, monitoring the entire technology stacks of Datahub environment. This involves the management and administration of enterprise Datahubs, including IT Data Lake and Worker Datahub. You will be responsible for maturing implementation of Datahub services, and to maintain a highly secure and reliable operational environment. You will be current in Datahub best practices and apply them to the environment to continuously improve performance, scalability, and proactively manage capacity of the environment.
 
 
 
   This role provides an opportunity to make a significant impact across L&R and to define Datahub technology roadmaps. This senior leadership role requires extensive experience and is hands-on along with requiring the candidate to lead and influence a small team. The candidate will act as Datahub thought leader and interface with Architecture and Engineering teams, IT Security, and Production Operations to design and implement transformational improvements. The technologies you will manage include but are not limited to the following: Data Stage, Talend, AWS EMR, Snowflake, Oracle, SQL Server and AWS database services.
 
 
   Please note: The job can only be performed in the State locations listed: Houston, TX and Remote-TX.
 
 
 
   The Senior Datahub Engineer is expected to perform the following duties:
 
 
   Leading the engineering, design, development, and launch of high available, low latency, flexible and scalable Datahub services.
   Significantly influence overall strategy by helping define features, drive the system architecture, and spearhead the best practices that enable quality product and services.
   Design and implement service data models, ETL process, caching models and APIs.
   Promoting infrastructure as code and helping teams to shift to an automated deployment process
   Responsible for improving the performance, reliability, and observability of the Datahub platform
   Design, build, and maintain highly scalable and reliable data integration layer
   Contribute to the design and documentation of the Datahub system architecture
   Troubleshoot and resolve production issues across services and multiple layers of the stack
   Maintain a high level of code quality and testing
   Extending service monitoring capabilities and publicly available data such as uptime and performance metrics
   Expanding the Datahub application and services for availability and redundancy
   Maintaining back end services and data stores to power the Datahub application
   Develop and oversee monitoring systems to measure usage and ensure operational stability.
   Monitor the process during the entire lifecycle for adherence and updating or creating new process for improvement and cost effectiveness.
   Excellent communication, conceptual, critical thinking, analytical, problem-solving abilities, and organizational skills.
   Complex Problem Solving – Identifying complex problems and reviewing related information to develop and evaluate options and implement solutions.
 
 
   Primary Technical Skills:
 
 
   ETL Tools: Talend, IBM DataStage, AWS EMR
   Cloud: AWS
 
 
 
   Secondary Technical Skills:
 
 
   Data: SQL (Snowflake, Oracle)
 
 
 
   POSITION REQUIREMENTS:
   10+ years
 
 
   Hands-on Administration experience with ETL tools like Talend, DataStage and AWS EMR or Hadoop
   Technical experience in troubleshooting and ITIL process and practices
 
 
   7+ years
 
 
   Administration expertise in Talend, IBM DataStage and AWS EMR or Hadoop
   Experience supporting infrastructure and applications hosted in AWS or Azure
   Experience establishing and maturing ETL and High availability best practices
   Strong proficiency in AWS services, particularly EMR and related big data technologies (Hadoop, Hive, Spark, etc.)
   Sound knowledge of database management, SQL querying, data modeling, data warehousing, business intelligence, and OLAP (Online Analytical Processing)
   Experience managing/working with Windows / Linux infrastructure teams
   Experience in creating disaster recovery plans for both on-premise and cloud infrastructures
   Excellent communication, conceptual, critical thinking, analytical, problem-solving abilities, and organizational skills
   Complex Problem Solving – Identifying complex problems and reviewing related information to develop and evaluate options and implement solutions.
   Comfortable leading discussions with leadership and have experience tailoring the level of technical details to suit the audience
   Experience with leading a team
 
 
 
   Preferred/Plus Experience:
 
 
   Batchelor’s degree in Computer Science, Information Technology, or related
   5 years Proven experience as a Platform Engineer or similar role with a focus on ETL tools and big data technologies.
   Familiarity with Administering reporting tools is a plus.
   Strong background with Public Cloud Infrastructure management
   Excellent problem-solving skills and the ability to work in a dynamic, collaborative environment
   Experience with Cloud Cost Management, Demand Forecasting, Budget forecasting, Capacity management, Chargeback mechanism
 
 
   What our employees like most about working for Corebridge Financial
 
 
   We care about your professional development. Our career progression program will provide you with the opportunity to develop your skills, strengthen your productivity and be eligible to progressively advance to positions with an increased responsibility and increased compensation.
   Our “Giving Back” policy is at the core of our daily operations and guides our future progress. Don’t believe us? We put our money where our mouth is! Corebridge Financial will give you up to 16 hours a year paid time off to volunteer in the community.
   Our people are our most important asset therefore we provide a generous benefits plan and competitive pay. Benefit package includes:
 
 
   Paid Time Off (Corebridge Financial recognizes the importance of work life balance). We offer 24 PTO days to start. YES, 24! 17 paid holidays per calendar year.
   A 401(k) Retirement Plan which will be HARD TO BEAT. Our 401K - $1 for $1 match up to 6% with immediate vesting, plus Corebridge Financial automatically contributes an additional 3% into your 401K regardless of if you enroll or not.
 
 
   #LI-LR1
 
 
   #LI-SAFG
 
 
   #LI-Remote
 
 
 
   We are an Equal Opportunity Employer
 
 
 
   Corebridge Financial, Inc., its subsidiaries and affiliates are committed to be an Equal Opportunity Employer and its policies and procedures reflect this commitment. We provide equal opportunity to all qualified individuals regardless of race, color, religion, age, gender, gender expression, national origin, veteran status, disability or any other legally protected categories such as sexual orientation. At Corebridge Financial, we believe that diversity and inclusion are critical to our future and our mission – creating a foundation for a creative workplace that leads to innovation, growth, and profitability. Through a wide variety of programs and initiatives, we invest in each employee, seeking to ensure that our people are not only respected as individuals, but also truly valued for their unique perspectives.
 
 
   To learn more please visit: 
  
   www.corebridgefinancial.com
  
 
 
 
   Corebridge Financial is committed to working with and providing reasonable accommodations to job applicants and employees with physical or mental disabilities. If you believe you need a reasonable accommodation in order to search for a job opening or to complete any part of the application or hiring process, please send an email to 
  
   TalentandInclusion@corebridgefinancial.com
  . Reasonable accommodations will be determined on a case-by-case basis.
 
 
 
   Functional Area:
  IT - Information Technology
  Estimated Travel Percentage (%): No Travel
  Relocation Provided: No
  American General Life Insurance Company","<div>
 <div>
  Who we are
 </div>
 <div>
   Corebridge Financial helps people make some of the most meaningful decisions they&#x2019;re ever going to make. We help them plan and take action to protect the future they envision, and respond to some of life&#x2019;s most difficult moments through the solutions and services we provide. We do this through our broad portfolio of life insurance, retirement and institutional products, offered through an extensive, multichannel distribution network. We provide solutions for a brighter future through our client centered service, breadth of product expertise, deep distribution relationships, and outstanding team of hardworking and passionate employees.
 </div>
 <div></div>
 <div>
   About the role
 </div>
 <div>
   As a Sr. Data &amp; ETL Platform Engineer you will be responsible for leading, influencing AIG&#x2019;s Datahub platform to address needs of all enterprise datasets at AIG. You will engineer for reliability, scalability, performance, observability and supportability of enterprise Datahub environment.
 </div>
 <div>
   In this role, you will be responsible for administering, supporting, monitoring the entire technology stacks of Datahub environment. This involves the management and administration of enterprise Datahubs, including IT Data Lake and Worker Datahub. You will be responsible for maturing implementation of Datahub services, and to maintain a highly secure and reliable operational environment. You will be current in Datahub best practices and apply them to the environment to continuously improve performance, scalability, and proactively manage capacity of the environment.
 </div>
 <div></div>
 <div>
   This role provides an opportunity to make a significant impact across L&amp;R and to define Datahub technology roadmaps. This senior leadership role requires extensive experience and is hands-on along with requiring the candidate to lead and influence a small team. The candidate will act as Datahub thought leader and interface with Architecture and Engineering teams, IT Security, and Production Operations to design and implement transformational improvements. The technologies you will manage include but are not limited to the following: Data Stage, Talend, AWS EMR, Snowflake, Oracle, SQL Server and AWS database services.
 </div>
 <div>
  <br> Please note: The job can only be performed in the State locations listed: Houston, TX and Remote-TX.
 </div>
 <div></div>
 <div>
   The Senior Datahub Engineer is expected to perform the following duties:
 </div>
 <ul>
  <li> Leading the engineering, design, development, and launch of high available, low latency, flexible and scalable Datahub services.</li>
  <li> Significantly influence overall strategy by helping define features, drive the system architecture, and spearhead the best practices that enable quality product and services.</li>
  <li> Design and implement service data models, ETL process, caching models and APIs.</li>
  <li> Promoting infrastructure as code and helping teams to shift to an automated deployment process</li>
  <li> Responsible for improving the performance, reliability, and observability of the Datahub platform</li>
  <li> Design, build, and maintain highly scalable and reliable data integration layer</li>
  <li> Contribute to the design and documentation of the Datahub system architecture</li>
  <li> Troubleshoot and resolve production issues across services and multiple layers of the stack</li>
  <li> Maintain a high level of code quality and testing</li>
  <li> Extending service monitoring capabilities and publicly available data such as uptime and performance metrics</li>
  <li> Expanding the Datahub application and services for availability and redundancy</li>
  <li> Maintaining back end services and data stores to power the Datahub application</li>
  <li> Develop and oversee monitoring systems to measure usage and ensure operational stability.</li>
  <li> Monitor the process during the entire lifecycle for adherence and updating or creating new process for improvement and cost effectiveness.</li>
  <li> Excellent communication, conceptual, critical thinking, analytical, problem-solving abilities, and organizational skills.</li>
  <li> Complex Problem Solving &#x2013; Identifying complex problems and reviewing related information to develop and evaluate options and implement solutions.</li>
 </ul>
 <div>
   Primary Technical Skills:
 </div>
 <ul>
  <li> ETL Tools: Talend, IBM DataStage, AWS EMR</li>
  <li> Cloud: AWS</li>
 </ul>
 <div></div>
 <div>
   Secondary Technical Skills:
 </div>
 <ul>
  <li> Data: SQL (Snowflake, Oracle)</li>
 </ul>
 <div></div>
 <div>
  <br> POSITION REQUIREMENTS:
  <br> 10+ years
 </div>
 <ul>
  <li> Hands-on Administration experience with ETL tools like Talend, DataStage and AWS EMR or Hadoop</li>
  <li> Technical experience in troubleshooting and ITIL process and practices</li>
 </ul>
 <div>
   7+ years
 </div>
 <ul>
  <li> Administration expertise in Talend, IBM DataStage and AWS EMR or Hadoop</li>
  <li> Experience supporting infrastructure and applications hosted in AWS or Azure</li>
  <li> Experience establishing and maturing ETL and High availability best practices</li>
  <li> Strong proficiency in AWS services, particularly EMR and related big data technologies (Hadoop, Hive, Spark, etc.)</li>
  <li> Sound knowledge of database management, SQL querying, data modeling, data warehousing, business intelligence, and OLAP (Online Analytical Processing)</li>
  <li> Experience managing/working with Windows / Linux infrastructure teams</li>
  <li> Experience in creating disaster recovery plans for both on-premise and cloud infrastructures</li>
  <li> Excellent communication, conceptual, critical thinking, analytical, problem-solving abilities, and organizational skills</li>
  <li> Complex Problem Solving &#x2013; Identifying complex problems and reviewing related information to develop and evaluate options and implement solutions.</li>
  <li> Comfortable leading discussions with leadership and have experience tailoring the level of technical details to suit the audience</li>
  <li> Experience with leading a team</li>
 </ul>
 <div></div>
 <div>
   Preferred/Plus Experience:
 </div>
 <ul>
  <li> Batchelor&#x2019;s degree in Computer Science, Information Technology, or related</li>
  <li> 5 years Proven experience as a Platform Engineer or similar role with a focus on ETL tools and big data technologies.</li>
  <li> Familiarity with Administering reporting tools is a plus.</li>
  <li> Strong background with Public Cloud Infrastructure management</li>
  <li> Excellent problem-solving skills and the ability to work in a dynamic, collaborative environment</li>
  <li> Experience with Cloud Cost Management, Demand Forecasting, Budget forecasting, Capacity management, Chargeback mechanism</li>
 </ul>
 <div>
   What our employees like most about working for Corebridge Financial
 </div>
 <ul>
  <li> We care about your professional development. Our career progression program will provide you with the opportunity to develop your skills, strengthen your productivity and be eligible to progressively advance to positions with an increased responsibility and increased compensation.</li>
  <li> Our &#x201c;Giving Back&#x201d; policy is at the core of our daily operations and guides our future progress. Don&#x2019;t believe us? We put our money where our mouth is! Corebridge Financial will give you up to 16 hours a year paid time off to volunteer in the community.</li>
  <li> Our people are our most important asset therefore we provide a generous benefits plan and competitive pay. Benefit package includes:</li>
 </ul>
 <ul>
  <li> Paid Time Off (Corebridge Financial recognizes the importance of work life balance). We offer 24 PTO days to start. YES, 24! 17 paid holidays per calendar year.</li>
  <li> A 401(k) Retirement Plan which will be HARD TO BEAT. Our 401K - &#x24;1 for &#x24;1 match up to 6% with immediate vesting, plus Corebridge Financial automatically contributes an additional 3% into your 401K regardless of if you enroll or not.</li>
 </ul>
 <div>
   #LI-LR1
 </div>
 <div>
   #LI-SAFG
 </div>
 <div>
   #LI-Remote
 </div>
 <div></div>
 <div>
   We are an Equal Opportunity Employer
 </div>
 <div></div>
 <div>
   Corebridge Financial, Inc., its subsidiaries and affiliates are committed to be an Equal Opportunity Employer and its policies and procedures reflect this commitment. We provide equal opportunity to all qualified individuals regardless of race, color, religion, age, gender, gender expression, national origin, veteran status, disability or any other legally protected categories such as sexual orientation. At Corebridge Financial, we believe that diversity and inclusion are critical to our future and our mission &#x2013; creating a foundation for a creative workplace that leads to innovation, growth, and profitability. Through a wide variety of programs and initiatives, we invest in each employee, seeking to ensure that our people are not only respected as individuals, but also truly valued for their unique perspectives.
 </div>
 <div>
   To learn more please visit: 
  <div>
   www.corebridgefinancial.com
  </div>
 </div>
 <div></div>
 <div>
   Corebridge Financial is committed to working with and providing reasonable accommodations to job applicants and employees with physical or mental disabilities. If you believe you need a reasonable accommodation in order to search for a job opening or to complete any part of the application or hiring process, please send an email to 
  <div>
   TalentandInclusion@corebridgefinancial.com
  </div>. Reasonable accommodations will be determined on a case-by-case basis.
 </div>
 <div></div>
 <div>
   Functional Area:
 </div> IT - Information Technology
 <div></div> Estimated Travel Percentage (%): No Travel
 <div></div> Relocation Provided: No
 <div></div> American General Life Insurance Company
</div>
<div></div>","https://www.indeed.com/rc/clk?jk=505411b3fa61e4b6&atk=&xpse=SoCn67I3Jzdo1MgFSb0LbzkdCdPP","505411b3fa61e4b6",,"Full-time",,"Houston, TX","Sr. Data & ETL Platform Engineer","30+ days ago","2023-09-25T11:51:25.482Z","2.8","25",,"2023-10-25T11:51:25.483Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=505411b3fa61e4b6&from=jasx&tk=1hdjaoshm284l000&vjs=3"
"Enterra Solutions","LOCATION: U.S. Eastern Time Zone 
   Must reside in the US – preferably in the Eastern Time Zone. Remote working permitted. Must be eligible to work in the US without sponsorship now or in the future. This is a full-time position with benefits. Contractors will not be considered for this position.
   
   
   Who we are: 
   Enterra provides solutions that leverage sophisticated machine learning, artificial intelligence (ontologies, inference engines and rules) and natural language processing to provide highly actionable insights and recommendations to business users. Today, our solutions impact just about every aspect of the products you buy at your local store – from what is available to how it is priced and even where it is placed on the shelf. Our SolaaS (Solution as a Service) solutions are deployed within private clouds – principally on Azure. We help transform market-leading companies into true data-driven digital enterprises.
 
 
  What you will do: 
  The ideal candidate must be collaborative, and deadline driven. Because of the nature of our work and our technology, successful candidates must take a growth mindset and be comfortable with ambiguity, with the ability to take a proactive, structured approach to achieve results. Results-orientation and deadline driven are critical in our fast-paced environment. 
  The successful candidate will join a diverse team to: 
 
  Build unique high-impact business solutions utilizing advanced technologies for use by world class clients. 
  Create and maintain the underlying data pipeline architecture for the solution offerings from raw client data to final solution output. 
  Create, populate, and maintain data structures for machine learning and other analytics. 
  Use quantitative and statistical methods to derive insights from data. 
  Guide the data technology stack used to build Enterra's solution offerings. 
  Combine machine learning, artificial intelligence (ontologies, inference engines and rules) and natural language processing under a holistic vision to scale and transform businesses — across multiple functions and processes. 
  
 Responsibilities Include: 
  
  Work with other Enterra personnel to develop and enhance commercial quality solution offerings 
   
    Design, create and maintain optimal data pipeline architecture, incorporating data wrangling and Extract-Transform-Load (ETL) flows. 
    Assemble large, complex data sets to meet analytical requirements – analytics tables, feature-engineering etc. 
    Design and build the infrastructure required for optimal, automated extraction, transformation, and loading of data from a wide variety of data sources using SQL and other 'big data' technologies such as Databricks. 
    Design and build automated analytics tools that utilize the data pipeline to derive actionable insights. 
    Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. 
    Design and develop data integrations and data quality framework 
    Develop appropriate testing strategies and reports for the solution as well as data from external sources. 
    Evaluate new technology for use within Enterra. 
   
  Work with other Enterra and client personnel to administer and operate client-specific instances of the Enterra solution offerings 
 
 
  
   Configure the data pipelines to accommodate client-specific requirements to onboard new clients. 
  
 
 
  Perform regular operations tasks to ingest new and changing data – implement automation where possible. 
  Implement processes and tools to monitor data quality - investigate and remedy any data-related issues in daily solution operations. 
 
 
  May provide guidance and oversight to fellow data engineers 
  
 Requirements: 
  
  Bachelor's degree in Computer Science or a STEM (Science, Technology, Engineering or Math) field required 
  Minimum of 7 years hands on experience as a data engineer or similar position. 
  Minimum of 7 years commercial experience with Python or Scala Programming Language 
  Minimum of 7 years SQL and experience working with relational databases (Postgres preferred). 
  Experience with at least one of the following – Databricks, Spark, Hadoop or Kafka 
  Demonstratable knowledge and experience developing data pipelines to automate data processing workflows 
  Demonstratable experience in data modeling 
  Demonstratable knowledge of data warehousing, business intelligence, and application data integration solutions 
  Demonstratable experience in developing applications and services that run on a cloud infrastructure Azure preferred 
  Excellent problem-solving and communication skills 
  Ability to thrive in a fast-paced, remote environment. 
  Comfortable with ambiguity with the ability to build structure and take a proactive approach to drive results. 
  Attention to detail – quality and accuracy in work is essential.
 
  
  
  The following additional skills would be beneficial: 
 
  Knowledge of one or more of the following technologies: Data Science, Machine Learning, Natural Language Processing, Business Intelligence, and Data Visualization. 
  Knowledge of statistics and experience using statistical or BI packages for analyzing large datasets (Excel, R, Python, Power BI, Tableau etc.). 
  Experience with container management and deployment, e.g., Docker and Kubernetes","<div>
 <div>
  <p><b>LOCATION: </b><b>U.S. Eastern Time Zone</b></p> 
  <p> Must reside in the US &#x2013; preferably in the Eastern Time Zone. Remote working permitted. Must be eligible to work in the US without sponsorship now or in the future. This is a full-time position with benefits. Contractors will not be considered for this position.</p>
  <br> 
  <p></p> 
  <p><b> Who we are:</b></p> 
  <p> Enterra provides solutions that leverage sophisticated machine learning, artificial intelligence (ontologies, inference engines and rules) and natural language processing to provide highly actionable insights and recommendations to business users. Today, our solutions impact just about every aspect of the products you buy at your local store &#x2013; from what is available to how it is priced and even where it is placed on the shelf. Our SolaaS (Solution as a Service) solutions are deployed within private clouds &#x2013; principally on Azure.<b> </b><i>We help transform market-leading companies into true data-driven digital enterprises.</i></p>
 </div>
 <p></p>
 <p><b><br> What you will do:</b></p> 
 <p> The ideal candidate must be collaborative, and deadline driven. Because of the nature of our work and our technology, successful candidates must take a growth mindset and be comfortable with ambiguity, with the ability to take a proactive, structured approach to achieve results. Results-orientation and deadline driven are critical in our fast-paced environment.</p> 
 <p><b> The successful candidate will join a diverse team to: </b></p>
 <ul>
  <li>Build unique high-impact business solutions utilizing advanced technologies for use by world class clients.</li> 
  <li>Create and maintain the underlying data pipeline architecture for the solution offerings from raw client data to final solution output.</li> 
  <li>Create, populate, and maintain data structures for machine learning and other analytics.</li> 
  <li>Use quantitative and statistical methods to derive insights from data.</li> 
  <li>Guide the data technology stack used to build Enterra&apos;s solution offerings.</li> 
  <li>Combine machine learning, artificial intelligence (ontologies, inference engines and rules) and natural language processing under a holistic vision to scale and transform businesses &#x2014; across multiple functions and processes.</li> 
 </ul> 
 <p><b>Responsibilities Include:</b></p> 
 <ul> 
  <li>Work with other Enterra personnel to develop and enhance commercial quality solution offerings 
   <ul>
    <li>Design, create and maintain optimal data pipeline architecture, incorporating data wrangling and Extract-Transform-Load (ETL) flows.</li> 
    <li>Assemble large, complex data sets to meet analytical requirements &#x2013; analytics tables, feature-engineering etc.</li> 
    <li>Design and build the infrastructure required for optimal, automated extraction, transformation, and loading of data from a wide variety of data sources using SQL and other &apos;big data&apos; technologies such as Databricks.</li> 
    <li>Design and build automated analytics tools that utilize the data pipeline to derive actionable insights.</li> 
    <li>Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.</li> 
    <li>Design and develop data integrations and data quality framework</li> 
    <li>Develop appropriate testing strategies and reports for the solution as well as data from external sources.</li> 
    <li>Evaluate new technology for use within Enterra.</li> 
   </ul></li>
  <li>Work with other Enterra and client personnel to administer and operate client-specific instances of the Enterra solution offerings</li> 
 </ul>
 <ul>
  <ul>
   <li>Configure the data pipelines to accommodate client-specific requirements to onboard new clients.</li> 
  </ul>
 </ul>
 <ul>
  <li>Perform regular operations tasks to ingest new and changing data &#x2013; implement automation where possible.</li> 
  <li>Implement processes and tools to monitor data quality - investigate and remedy any data-related issues in daily solution operations.</li> 
 </ul>
 <ul>
  <li>May provide guidance and oversight to fellow data engineers</li> 
 </ul> 
 <p><b>Requirements:</b></p> 
 <ul> 
  <li>Bachelor&apos;s degree in Computer Science or a STEM (Science, Technology, Engineering or Math) field required</li> 
  <li>Minimum of 7 years hands on experience as a data engineer or similar position.</li> 
  <li>Minimum of 7 years commercial experience with Python or Scala Programming Language</li> 
  <li>Minimum of 7 years SQL and experience working with relational databases (Postgres preferred).</li> 
  <li>Experience with at least one of the following &#x2013; Databricks, Spark, Hadoop or Kafka</li> 
  <li>Demonstratable knowledge and experience developing data pipelines to automate data processing workflows</li> 
  <li>Demonstratable experience in data modeling</li> 
  <li>Demonstratable knowledge of data warehousing, business intelligence, and application data integration solutions</li> 
  <li>Demonstratable experience in developing applications and services that run on a cloud infrastructure Azure preferred</li> 
  <li>Excellent problem-solving and communication skills</li> 
  <li>Ability to thrive in a fast-paced, remote environment.</li> 
  <li>Comfortable with ambiguity with the ability to build structure and take a proactive approach to drive results.</li> 
  <li>Attention to detail &#x2013; quality and accuracy in work is essential.</li>
 </ul>
 <br> 
 <p></p> 
 <h3 class=""jobSectionHeader""><b> The following additional skills would be beneficial:</b></h3> 
 <ul>
  <li>Knowledge of one or more of the following technologies: Data Science, Machine Learning, Natural Language Processing, Business Intelligence, and Data Visualization.</li> 
  <li>Knowledge of statistics and experience using statistical or BI packages for analyzing large datasets (Excel, R, Python, Power BI, Tableau etc.).</li> 
  <li>Experience with container management and deployment, e.g., Docker and Kubernetes</li>
 </ul>
</div>","https://www.indeed.com/rc/clk?jk=2ba5e02f648d0cda&atk=&xpse=SoDv67I3JzdobAQSK50LbzkdCdPP","2ba5e02f648d0cda",,"Full-time",,"Remote","Senior Data Engineer","30+ days ago","2023-09-25T11:51:32.458Z",,,,"2023-10-25T11:51:32.460Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=2ba5e02f648d0cda&from=jasx&tk=1hdjaoshm284l000&vjs=3"
"Nagarro","Full-time
   Service Region: South Asia
  
 
 
  
   
     Company Description
   
   
     We are a Digital Product Engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale — across all devices and digital mediums, and our people exist everywhere in the world (15000+ experts across 26 countries, to be exact). Our work culture is dynamic and non-hierarchical. We are looking for great new colleagues. That is where you come in!
   
  
  
  
   
     Job Description
   
   
     Must have Skills : Snowflake, SQL Server, Python (Strong), SQL (Strong),
     Total Experience : 10+ years of experience
     Qualifications:
    
      Bachelor's degree in Computer Science, Engineering, or related field;
      Master's degree is a plus.
      Minimum of 10 years of hands-on experience in data engineering.
      5+ years commercial experience with MSSQL
      2+ years commercial experience with major cloud provide, preferably AWS
      Experience in embedded DBA /DBM role working directly with developers
      Proven experience in designing and implementing complex data pipelines, ETL processes, and data integrations.
      In-depth knowledge of Snowflake architecture, features, and best practices for performance optimization.
      Proficiency in SQL, Python, or other relevant programming languages for data manipulation and transformation.
      Excellent problem-solving skills and the ability to troubleshoot and resolve complex data engineering challenges.
      Strong communication skills and the ability to work collaboratively in a team environment.
      Data engineering certifications, are a plus.","<div>
 <ul>
  <li>Full-time</li>
  <li> Service Region: South Asia</li>
 </ul> 
 <p></p>
 <div>
  <div>
   <div>
    <h2 class=""jobSectionHeader""><b> Company Description</b></h2>
   </div>
   <div>
    <p> We are a Digital Product Engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale &#x2014; across all devices and digital mediums, and our people exist everywhere in the world (15000+ experts across 26 countries, to be exact). Our work culture is dynamic and non-hierarchical. We are looking for great new colleagues. That is where you come in!</p>
   </div>
  </div>
  <p></p>
  <div>
   <div>
    <h2 class=""jobSectionHeader""><b> Job Description</b></h2>
   </div>
   <div>
    <p> Must have Skills : Snowflake, SQL Server, Python (Strong), SQL (Strong),</p>
    <p> Total Experience : 10+ years of experience</p>
    <p> Qualifications:</p>
    <ul>
     <li> Bachelor&apos;s degree in Computer Science, Engineering, or related field;</li>
     <li> Master&apos;s degree is a plus.</li>
     <li> Minimum of 10 years of hands-on experience in data engineering.</li>
     <li> 5+ years commercial experience with MSSQL</li>
     <li> 2+ years commercial experience with major cloud provide, preferably AWS</li>
     <li> Experience in embedded DBA /DBM role working directly with developers</li>
     <li> Proven experience in designing and implementing complex data pipelines, ETL processes, and data integrations.</li>
     <li> In-depth knowledge of Snowflake architecture, features, and best practices for performance optimization.</li>
     <li> Proficiency in SQL, Python, or other relevant programming languages for data manipulation and transformation.</li>
     <li> Excellent problem-solving skills and the ability to troubleshoot and resolve complex data engineering challenges.</li>
     <li> Strong communication skills and the ability to work collaboratively in a team environment.</li>
     <li> Data engineering certifications, are a plus.</li>
    </ul>
   </div>
  </div>
 </div>
</div>","https://jobs.smartrecruiters.com/Nagarro1/743999927374543-associate-principal-engineer-big-data","b2b0115826e2d0f7",,"Full-time",,"Remote","Associate Principal Engineer, Big Data","30+ days ago","2023-09-25T11:51:30.840Z","3.9","67",,"2023-10-25T11:51:30.841Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=b2b0115826e2d0f7&from=jasx&tk=1hdjaoshm284l000&vjs=3"
"MGIC","Why work at MGIC?
   
   Are you someone who wants to play a critical role in our company’s success? Do you enjoy solving puzzles and finding a better way to get things done? Are you someone who likes to Take The Lead and make an impact? If so, then imagine yourself at MGIC. At MGIC we are a team of dedicated professionals on a fearless mission. A team that fosters a culture of career development and continuous learning opportunities to help you rise to new heights. We are passionate about providing outstanding customer service and making a difference in our community. #WeAreMGIC
 
 
 
   Preferred location: Milwaukee based – hybrid (3 days office, 2 days remote)
 
 
   Other locations: Madison, Minnesota, Illinois, and Michigan (occasional travel into the office to support team engagements).
 
 
 
   How will you make an impact?
 
 
   We are currently looking for a Senior Data Engineer to deliver high quality, maintainable and scalable data solutions. The Senior Data Engineer will partner with solution architects and other data engineers to develop our enterprise analytics data platform through new and updated data pipelines leveraging shared components and aligning to standards and best practices. This role is focused on collecting data from internal and external sources and transforming it into usable information for the business including data scientists and data analysts.
 
 
   Deliver high quality data assets to be used by the business to transform business processes and to enable leaders to make data-driven decisions
   Continuously improve data solutions to increase quality, speed of delivery and trust of data engineering team’s deliverables to enable business outcomes
   Reduce total cost of ownership of solutions by developing shared components and implementing best practices and coding standards
 
 
 
   Do you have what it takes?
 
 
   Ability to translate data engineering designs into working code
   Data analysis and data engineering pipeline experience including design, development, and support
   Experience with AWS services or cloud data offerings including S3, Lambda, EMR, Dynamo DB, Glue, Snowflake and/or other data technologies.
   Experience with coding in Python, PySpark, and Terraform
   Experience with DevOps practices including Continuous Integration, Continuous Delivery, and Infrastructure as Code to deliver end-to-end automation of data delivery.
   Ability to train and mentor junior data engineers.
   Experience with Agile engineering practices including the scrum framework.
 
 
 
   #LI-NV1 #LI-Hybrid
 
 
 
   Enjoy these benefits from day one:
   
   
  
   Competitive Salary & pay-for-performance bonus
   Financial Benefits (401k with company match, profit sharing, HSA, wellness rewards program)
   On-site Fitness Center and classes (corporate office)
   Paid-time off and paid company holidays
   Business casual dress
  
 
 
 
   For additional information about MGIC and to apply, please visit our website at www.mgic.com/careers.","<div>
 <div>
  Why work at MGIC?
  <br> 
  <br> Are you someone who wants to play a critical role in our company&#x2019;s success? Do you enjoy solving puzzles and finding a better way to get things done? Are you someone who likes to Take The Lead and make an impact? If so, then imagine yourself at MGIC. At MGIC we are a team of dedicated professionals on a fearless mission. A team that fosters a culture of career development and continuous learning opportunities to help you rise to new heights. We are passionate about providing outstanding customer service and making a difference in our community. #WeAreMGIC
 </div>
 <div></div>
 <div>
   Preferred location: Milwaukee based &#x2013; hybrid (3 days office, 2 days remote)
 </div>
 <div>
   Other locations: Madison, Minnesota, Illinois, and Michigan (occasional travel into the office to support team engagements).
 </div>
 <div></div>
 <div>
   How will you make an impact?
 </div>
 <div>
   We are currently looking for a Senior Data Engineer to deliver high quality, maintainable and scalable data solutions. The Senior Data Engineer will partner with solution architects and other data engineers to develop our enterprise analytics data platform through new and updated data pipelines leveraging shared components and aligning to standards and best practices. This role is focused on collecting data from internal and external sources and transforming it into usable information for the business including data scientists and data analysts.
 </div>
 <ul>
  <li> Deliver high quality data assets to be used by the business to transform business processes and to enable leaders to make data-driven decisions</li>
  <li> Continuously improve data solutions to increase quality, speed of delivery and trust of data engineering team&#x2019;s deliverables to enable business outcomes</li>
  <li> Reduce total cost of ownership of solutions by developing shared components and implementing best practices and coding standards</li>
 </ul>
 <div></div>
 <div>
   Do you have what it takes?
 </div>
 <ul>
  <li> Ability to translate data engineering designs into working code</li>
  <li> Data analysis and data engineering pipeline experience including design, development, and support</li>
  <li> Experience with AWS services or cloud data offerings including S3, Lambda, EMR, Dynamo DB, Glue, Snowflake and/or other data technologies.</li>
  <li> Experience with coding in Python, PySpark, and Terraform</li>
  <li> Experience with DevOps practices including Continuous Integration, Continuous Delivery, and Infrastructure as Code to deliver end-to-end automation of data delivery.</li>
  <li> Ability to train and mentor junior data engineers.</li>
  <li> Experience with Agile engineering practices including the scrum framework.</li>
 </ul>
 <div></div>
 <div>
   #LI-NV1 #LI-Hybrid
 </div>
 <div></div>
 <div>
   Enjoy these benefits from day one:
  <br> 
  <br> 
  <ul>
   <li>Competitive Salary &amp; pay-for-performance bonus</li>
   <li>Financial Benefits (401k with company match, profit sharing, HSA, wellness rewards program)</li>
   <li>On-site Fitness Center and classes (corporate office)</li>
   <li>Paid-time off and paid company holidays</li>
   <li>Business casual dress</li>
  </ul>
 </div>
 <div></div>
 <div>
   For additional information about MGIC and to apply, please visit our website at www.mgic.com/careers.
 </div>
</div>","https://mgic.wd5.myworkdayjobs.com/en-US/MGIC/job/Remote/Senior-Data-Engineer_R1918","791622278da406a5",,"Full-time",,"Remote","Senior Data Engineer","18 days ago","2023-10-07T11:51:33.235Z","4","39",,"2023-10-25T11:51:33.238Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=791622278da406a5&from=jasx&tk=1hdjaos832ci6000&vjs=3"
"LiveRamp","LiveRamp is the data collaboration platform of choice for the world’s most innovative companies. A groundbreaking leader in consumer privacy, data ethics, and foundational identity, LiveRamp is setting the new standard for building a connected customer view with unmatched clarity and context while protecting precious brand and consumer trust. LiveRamp offers complete flexibility to collaborate wherever data lives to support the widest range of data collaboration use cases—within organizations, between brands, and across its premier global network of top-quality partners.
 
 
 
   Hundreds of global innovators, from iconic consumer brands and tech giants to banks, retailers, and healthcare leaders turn to LiveRamp to build enduring brand and business value by deepening customer engagement and loyalty, activating new partnerships, and maximizing the value of their first-party data while staying on the forefront of rapidly evolving compliance and privacy requirements.
 
 
 
   LiveRamp is the leading data connectivity platform. We believe connected data has the power to change the world. Our platform powers insights and experiences centered around the needs of real people, and in ways that keep the Internet open for all. LiveRampers thrive on building together with curiosity and humility—and have a good bit of fun along the way. We’re always looking for smart, kind, and creative people to grow our team and impact.
   
   Mission: LiveRamp makes it safe and easy for businesses to use data effectively.
 
 
   The Activations Back End team is responsible for the bulk of big data processing that powers LiveRamp’s primary product (a product that delivers several hundred million in annual recurring revenue). Our systems process tens of thousands of batch requests per day, ranging in size from GBs to tens of TBs, and provide detailed monitoring, statistics, error recovery, and resiliency to power this core product.
   
   At LiveRamp big data processing is not just for analytics to uncover business insights. Our product fundamentally is a big data product, as we transform, make useful, and transport massive datasets to hundreds of integrations. In doing so, we enable many of the most successful companies on the planet.
   
   You will:
 
 
   Work collaboratively with a small team of engineers to reinvent a system already processing petabytes of data every day.
   Play a key role in transforming Activations Back End systems to being cloud-agnostic, multi-regional, and fundamentally more extensible
   Building new pipelines and products with SQL on cloud data warehouses
   Help shape the future of our big data technology platform.
   Infuse industry best practices into our team processes and development practices.
   Assist in architectural design and implementation of our systems and interfaces.
   Develop our data processing pipelines using technology such as Spark, Dataproc, and Kubernetes.
   Research and experiment strategies to optimize the performance and efficiency of petabytes of data processing.
   Foster a positive environment of integrity, empowerment, initiative, and teamwork.
   Provide operational support for our team’s production systems.
   Share the remarkable work of our Activations team with the world through tech talks, blog posts, whitepapers, and conference participation.
   Learn and grow in your role and career.
 
 
   Your team will:
 
 
   Rearchitect its platform to power our products in multiple clouds and regions, as well as to take advantage of cutting-edge cloud data warehouses, like Snowflake and SingleStore, which unlocks many new exciting possibilities.
   Deliver 1-> N Global deployments, Novel Uses of Cloud Data Warehouses, 100x Processing Speedups, and other Paradigm shifts.
   Play an important role in improving LiveRamp’s foundational products
 
 
   About you: :
 
 
   3+ years of experience writing and deploying high-quality production code.
   Have made significant and meaningful development contributions to team projects involving distributed systems operating at scale.
   Excitement to learn and improve, comfort with ambiguity
   Communication skills to share highly technical information with technical and non-technical teammates.
   Deeply inquisitive. Always asking why we do things and how we can do them better.
   Thrive in a collaborative environment.
   Demonstrated ability to build sustainable technical solutions that verifiably meet specific, well-defined business requirements.
   Are you the type of person that likes running 200TB spark shuffles, 100x-ing job times, and or having an insight that saves $1 million in recurring cloud infrastructure cost? Call us
   Experience architecting and improving system performance on cloud data warehouses
   Experience writing and managing complex SQL queries
 
 
   Benefits:
 
 
   People: Work with talented, collaborative, and friendly people who love what they do.
   Fun: We host in-person and virtual events such as game nights, happy hours, camping trips, and sports leagues.
   Work/Life Harmony: Flexible paid time off, paid holidays, options for working from home, and paid parental leave.
   Comprehensive Benefits Package: Medical, dental, vision, life, and disability. Plus, mental health support (via Talkspace), flexible time off, parental leave, family forming benefits, and a flexible lifestyle and wellbeing reimbursement program (up to $375 per quarter, U.S. LiveRampers)
   Savings: Our 401K matching plan—1:1 match up to 6% of salary—helps you plan ahead. Also Employee Stock Purchase Plan - 15% discount off purchase price of LiveRamp stock (U.S. LiveRampers)
 
 
 
   The approximate annual base compensation range is $130,000 to $190,000 The actual offer, reflecting the total compensation package and benefits, will be determined by a number of factors including the applicant's experience, knowledge, skills, and abilities, geography, as well as internal equity among our team.
 
 
 
   More about us:
 
 
   LiveRamp’s mission is to connect data in ways that matter, and doing so starts with our people. We know that inspired teams enlist people from a blend of backgrounds and experiences. And we know that individuals do their best when they not only bring their full selves to work but feel like they truly belong. Connecting LiveRampers to new ideas and one another is one of our guiding principles—one that informs how we hire, train, and grow our global team across nine countries and four continents. Click 
  
   here 
  
  to learn more about Diversity, Inclusion, & Belonging (DIB) at LiveRamp.","<div>
 <div>
  LiveRamp is the data collaboration platform of choice for the world&#x2019;s most innovative companies. A groundbreaking leader in consumer privacy, data ethics, and foundational identity, LiveRamp is setting the new standard for building a connected customer view with unmatched clarity and context while protecting precious brand and consumer trust. LiveRamp offers complete flexibility to collaborate wherever data lives to support the widest range of data collaboration use cases&#x2014;within organizations, between brands, and across its premier global network of top-quality partners.
 </div>
 <div></div>
 <div>
   Hundreds of global innovators, from iconic consumer brands and tech giants to banks, retailers, and healthcare leaders turn to LiveRamp to build enduring brand and business value by deepening customer engagement and loyalty, activating new partnerships, and maximizing the value of their first-party data while staying on the forefront of rapidly evolving compliance and privacy requirements.
 </div>
 <div></div>
 <div>
  <br> LiveRamp is the leading data connectivity platform. We believe connected data has the power to change the world. Our platform powers insights and experiences centered around the needs of real people, and in ways that keep the Internet open for all. LiveRampers thrive on building together with curiosity and humility&#x2014;and have a good bit of fun along the way. We&#x2019;re always looking for smart, kind, and creative people to grow our team and impact.
  <br> 
  <br> Mission: LiveRamp makes it safe and easy for businesses to use data effectively.
 </div>
 <div>
  <br> The Activations Back End team is responsible for the bulk of big data processing that powers LiveRamp&#x2019;s primary product (a product that delivers several hundred million in annual recurring revenue). Our systems process tens of thousands of batch requests per day, ranging in size from GBs to tens of TBs, and provide detailed monitoring, statistics, error recovery, and resiliency to power this core product.
  <br> 
  <br> At LiveRamp big data processing is not just for analytics to uncover business insights. Our product fundamentally is a big data product, as we transform, make useful, and transport massive datasets to hundreds of integrations. In doing so, we enable many of the most successful companies on the planet.
  <br> 
  <br> You will:
 </div>
 <ul>
  <li> Work collaboratively with a small team of engineers to reinvent a system already processing petabytes of data every day.</li>
  <li> Play a key role in transforming Activations Back End systems to being cloud-agnostic, multi-regional, and fundamentally more extensible</li>
  <li> Building new pipelines and products with SQL on cloud data warehouses</li>
  <li> Help shape the future of our big data technology platform.</li>
  <li> Infuse industry best practices into our team processes and development practices.</li>
  <li> Assist in architectural design and implementation of our systems and interfaces.</li>
  <li> Develop our data processing pipelines using technology such as Spark, Dataproc, and Kubernetes.</li>
  <li> Research and experiment strategies to optimize the performance and efficiency of petabytes of data processing.</li>
  <li> Foster a positive environment of integrity, empowerment, initiative, and teamwork.</li>
  <li> Provide operational support for our team&#x2019;s production systems.</li>
  <li> Share the remarkable work of our Activations team with the world through tech talks, blog posts, whitepapers, and conference participation.</li>
  <li> Learn and grow in your role and career.</li>
 </ul>
 <div>
  <br> Your team will:
 </div>
 <ul>
  <li> Rearchitect its platform to power our products in multiple clouds and regions, as well as to take advantage of cutting-edge cloud data warehouses, like Snowflake and SingleStore, which unlocks many new exciting possibilities.</li>
  <li> Deliver 1-&gt; N Global deployments, Novel Uses of Cloud Data Warehouses, 100x Processing Speedups, and other Paradigm shifts.</li>
  <li> Play an important role in improving LiveRamp&#x2019;s foundational products</li>
 </ul>
 <div>
  <br> About you: :
 </div>
 <ul>
  <li> 3+ years of experience writing and deploying high-quality production code.</li>
  <li> Have made significant and meaningful development contributions to team projects involving distributed systems operating at scale.</li>
  <li> Excitement to learn and improve, comfort with ambiguity</li>
  <li> Communication skills to share highly technical information with technical and non-technical teammates.</li>
  <li> Deeply inquisitive. Always asking why we do things and how we can do them better.</li>
  <li> Thrive in a collaborative environment.</li>
  <li> Demonstrated ability to build sustainable technical solutions that verifiably meet specific, well-defined business requirements.</li>
  <li> Are you the type of person that likes running 200TB spark shuffles, 100x-ing job times, and or having an insight that saves &#x24;1 million in recurring cloud infrastructure cost? Call us</li>
  <li> Experience architecting and improving system performance on cloud data warehouses</li>
  <li> Experience writing and managing complex SQL queries</li>
 </ul>
 <div>
   Benefits:
 </div>
 <ul>
  <li> People: Work with talented, collaborative, and friendly people who love what they do.</li>
  <li> Fun: We host in-person and virtual events such as game nights, happy hours, camping trips, and sports leagues.</li>
  <li> Work/Life Harmony: Flexible paid time off, paid holidays, options for working from home, and paid parental leave.</li>
  <li> Comprehensive Benefits Package: Medical, dental, vision, life, and disability. Plus, mental health support (via Talkspace), flexible time off, parental leave, family forming benefits, and a flexible lifestyle and wellbeing reimbursement program (up to &#x24;375 per quarter, U.S. LiveRampers)</li>
  <li> Savings: Our 401K matching plan&#x2014;1:1 match up to 6% of salary&#x2014;helps you plan ahead. Also Employee Stock Purchase Plan - 15% discount off purchase price of LiveRamp stock (U.S. LiveRampers)</li>
 </ul>
 <div></div>
 <div>
  <i> The approximate annual base compensation range is &#x24;130,000 to &#x24;190,000 The actual offer, reflecting the total compensation package and benefits, will be determined by a number of factors including the applicant&apos;s experience, knowledge, skills, and abilities, geography, as well as internal equity among our team.</i>
 </div>
 <div></div>
 <div>
  <i><br> More about us:</i>
 </div>
 <div>
  <i> LiveRamp&#x2019;s mission is to connect data in ways that matter, and doing so starts with our people. We know that inspired teams enlist people from a blend of backgrounds and experiences. And we know that individuals do their best when they not only bring their full selves to work but feel like they truly belong. Connecting LiveRampers to new ideas and one another is one of our guiding principles&#x2014;one that informs how we hire, train, and grow our global team across nine countries and four continents. Click </i>
  <div>
   <i>here </i>
  </div>
  <i>to learn more about Diversity, Inclusion, &amp; Belonging (DIB) at LiveRamp.</i>
 </div>
</div>
<div></div>","https://liveramp.wd5.myworkdayjobs.com/en-US/LiveRampCareers/job/Remote---US/Senior-Software-Engineer--Big-Data-Activation_JR010020","97f5548d6f257ea6",,"Full-time",,"Remote","Senior Software Engineer, Big Data Activation","30+ days ago","2023-09-25T11:51:31.818Z","3.3","4","$130,000 - $190,000 a year","2023-10-25T11:51:31.819Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=97f5548d6f257ea6&from=jasx&tk=1hdjaoshm284l000&vjs=3"
"Novanta","Build a career powered by innovations that matter! At Novanta, our innovations power technology products that are transforming healthcare and advanced manufacturing—improving productivity, enhancing people’s lives and redefining what’s possible. We create for our global customers engineered components and sub-systems that deliver extreme precision and performance for a range of mission-critical applications—from minimally invasive surgery to robotics to 3D metal printing.
 
 
 
   Novanta is one global team with over 26 offices located in The Americas, Europe and Asia-Pacific. Looking for a great place to work? You have found it with a culture that embraces teamwork, collaboration and empowerment. Come explore Novanta.
 
 
 
   This position is part of Novanta’s Corporate and Shared Services global teams. Novanta’s Corporate and Shared Services teams play an important role in executing the company’s strategic mission and operations. Included in Corporate and Shared Services are the business functions including Finance, Accounting, Human Resources, Information Technology, Legal, Compliance, Corporate Development and Corporate Marketing. The Corporate and Shared Services teams work closely with all Novanta business units to support operating initiatives contributing to the organization’s financial success.
 
 
 
   Job Summary
 
 
 
   As a Data Analytics Engineer, you will be responsible for building and maintaining the data layer for our analytics stack, top to bottom. Your role will span multiple disciplines from data engineering to data analytics and visualization across all stages of data maturity for the purpose of delivering robust Business Intelligence solutions. You will consider software engineering best practices including version control, automated testing, documentation, code review and continuous integration, as essential to any data stack.
 
 
 
   Primary Responsibilities
 
 
   Design, develop and maintain scaled, automated, user-friendly systems, reports, dashboards, etc.
   Write complex, production-quality (i.e., accurate, performant, and maintainable) data transformation code to solve the needs of analysts, and business stakeholders (ex. MS SQL Server, Oracle, and Snowflake)
   Analyze assigned projects for data quality issues. Troubleshoot and resolve issues as they arise.
   Automate standard report creation and sharing using tools or scripts
   Convert raw data into consumable information applying business logic and utilizing clean engineering workflows
   Ensure that data, systems, architecture, business logic, and metrics are well-documented
   Support the acquisition of external data sets, interpreting data layouts, structures, fields, and values to incorporate new data into the core analytics database
   Serve as a catalyst for sharing knowledge, information, and ideas throughout the company as it relates to business intelligence
   Interface with business customers to gather data and metrics requirements, then driving analytic projects to solve complex challenges
 
 
   Draw insights from data and clearly communicate findings to stakeholders and external customers
   Provide exceptional customer service to stakeholders through project execution and timely delivery of solutions
 
 
 
   Required Experience, Education, Skills and Competencies
 
 
   Bachelor’s degree in Information Technology preferred or relevant experience.
   3+ years - Experience with MS SQL Server and Snowflake
   3+ years - Experience with ETL/ELT Tools (ex. Mulesoft, API, Informatica)
   3+ years - Experience using Power BI, Tableau, or similar data visualization tool
   Expert SQL Fluency (Well versed in CTEs and window functions)
   Demonstrated ability in data modeling, ETL/ELT, data pipelines, EDW
   Experienced building data warehouse infrastructure and BI tables
   Motivated individual with strong analytic, problem solving, and troubleshooting skills
 
 
 
   Travel Requirements
 
 
   Less than 20%
 
 
 
   Compensation and Benefits
 
 
   The base pay for this position ranges from $90,000 to $120,000 depending on the geographic market
   Dependent on the position offered, annual bonuses and other forms of compensation may be provided as part of the compensation package.
   Novanta supports all aspects of your life. This position provides a full range of benefits including paid parental and family leave.
 
 
 
   Novanta is proud to be an equal employment opportunity and affirmative action workplace. We consider all qualified applicants without regard to race, color, religion, sex (including pregnancy), sexual orientation, gender identity or expression, national origin, military and veteran status, disability, genetics, or any other category protected by federal law or Novanta policy.
 
 
 
   Please call +1 781-266-5700 if you need a disability accommodation for any part of the employment process.","<div>
 <div>
  Build a career powered by innovations that matter! At Novanta, our innovations power technology products that are transforming healthcare and advanced manufacturing&#x2014;improving productivity, enhancing people&#x2019;s lives and redefining what&#x2019;s possible. We create for our global customers engineered components and sub-systems that deliver extreme precision and performance for a range of mission-critical applications&#x2014;from minimally invasive surgery to robotics to 3D metal printing.
 </div>
 <div></div>
 <div>
   Novanta is one global team with over 26 offices located in The Americas, Europe and Asia-Pacific. Looking for a great place to work? You have found it with a culture that embraces teamwork, collaboration and empowerment. Come explore Novanta.
 </div>
 <div></div>
 <div>
   This position is part of Novanta&#x2019;s Corporate and Shared Services global teams. Novanta&#x2019;s Corporate and Shared Services teams play an important role in executing the company&#x2019;s strategic mission and operations. Included in Corporate and Shared Services are the business functions including Finance, Accounting, Human Resources, Information Technology, Legal, Compliance, Corporate Development and Corporate Marketing. The Corporate and Shared Services teams work closely with all Novanta business units to support operating initiatives contributing to the organization&#x2019;s financial success.
 </div>
 <div></div>
 <div>
   Job Summary
 </div>
 <div></div>
 <div>
   As a Data Analytics Engineer, you will be responsible for building and maintaining the data layer for our analytics stack, top to bottom. Your role will span multiple disciplines from data engineering to data analytics and visualization across all stages of data maturity for the purpose of delivering robust Business Intelligence solutions. You will consider software engineering best practices including version control, automated testing, documentation, code review and continuous integration, as essential to any data stack.
 </div>
 <div></div>
 <div>
   Primary Responsibilities
 </div>
 <ul>
  <li> Design, develop and maintain scaled, automated, user-friendly systems, reports, dashboards, etc.</li>
  <li> Write complex, production-quality (i.e., accurate, performant, and maintainable) data transformation code to solve the needs of analysts, and business stakeholders (ex. MS SQL Server, Oracle, and Snowflake)</li>
  <li> Analyze assigned projects for data quality issues. Troubleshoot and resolve issues as they arise.</li>
  <li> Automate standard report creation and sharing using tools or scripts</li>
  <li> Convert raw data into consumable information applying business logic and utilizing clean engineering workflows</li>
  <li> Ensure that data, systems, architecture, business logic, and metrics are well-documented</li>
  <li> Support the acquisition of external data sets, interpreting data layouts, structures, fields, and values to incorporate new data into the core analytics database</li>
  <li> Serve as a catalyst for sharing knowledge, information, and ideas throughout the company as it relates to business intelligence</li>
  <li> Interface with business customers to gather data and metrics requirements, then driving analytic projects to solve complex challenges</li>
 </ul>
 <ul>
  <li> Draw insights from data and clearly communicate findings to stakeholders and external customers</li>
  <li> Provide exceptional customer service to stakeholders through project execution and timely delivery of solutions</li>
 </ul>
 <div></div>
 <div>
   Required Experience, Education, Skills and Competencies
 </div>
 <ul>
  <li> Bachelor&#x2019;s degree in Information Technology preferred or relevant experience.</li>
  <li> 3+ years - Experience with MS SQL Server and Snowflake</li>
  <li> 3+ years - Experience with ETL/ELT Tools (ex. Mulesoft, API, Informatica)</li>
  <li> 3+ years - Experience using Power BI, Tableau, or similar data visualization tool</li>
  <li> Expert SQL Fluency (Well versed in CTEs and window functions)</li>
  <li> Demonstrated ability in data modeling, ETL/ELT, data pipelines, EDW</li>
  <li> Experienced building data warehouse infrastructure and BI tables</li>
  <li> Motivated individual with strong analytic, problem solving, and troubleshooting skills</li>
 </ul>
 <div></div>
 <div>
   Travel Requirements
 </div>
 <ul>
  <li> Less than 20%</li>
 </ul>
 <div></div>
 <div>
   Compensation and Benefits
 </div>
 <ul>
  <li> The base pay for this position ranges from &#x24;90,000 to &#x24;120,000 depending on the geographic market</li>
  <li> Dependent on the position offered, annual bonuses and other forms of compensation may be provided as part of the compensation package.</li>
  <li> Novanta supports all aspects of your life. This position provides a full range of benefits including paid parental and family leave.</li>
 </ul>
 <div></div>
 <div>
   Novanta is proud to be an equal employment opportunity and affirmative action workplace. We consider all qualified applicants without regard to race, color, religion, sex (including pregnancy), sexual orientation, gender identity or expression, national origin, military and veteran status, disability, genetics, or any other category protected by federal law or Novanta policy.
 </div>
 <div></div>
 <div>
   Please call +1 781-266-5700 if you need a disability accommodation for any part of the employment process.
 </div>
</div>","https://novanta.wd5.myworkdayjobs.com/en-US/Novanta-Careers/job/Remote---US/Data-Analytics-Engineer_R006258","7a36e7ca655affdd",,"Full-time",,"Remote","Data Analytics Engineer","18 days ago","2023-10-07T11:51:35.216Z","2.9","31","$90,000 - $120,000 a year","2023-10-25T11:51:35.253Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=7a36e7ca655affdd&from=jasx&tk=1hdjaos832ci6000&vjs=3"
"Blackhawk Network","About Blackhawk Network: 
 
   Blackhawk Network (BHN) is the leader in global branded payment technologies. We strengthen relationships between brands and their customers, employees, and partners by transforming transactions into connections. BHN’s portfolio includes: Gift Card & eGift products, promotions and distribution that grow revenue faster; Rewards & Incentives that build loyalty and acquisition and are integrated into today’s leading platforms; and Payments that enable businesses and customers to access and disburse funds in convenient and innovative ways. BHN’s network spans across the globe with over 400,000 consumer touchpoints. Learn more at BHN.com.
 
 
 
   This position may be performed remotely anywhere within the United States except for the State of Alaska, North Dakota, or South Dakota.
  Overview: 
 
   We are looking for an experienced data scientist to lead the initiatives we’re taking, to improve the product offerings of Blackhawk Network, from the perspective of risk modelling and business forecasting (prescriptive & predictive).
 
 
 
   As a team member in core data science team, you will own the research charter for data and decision science to enable the business stakeholders to be data-driven and deterministic, by providing insights into the decisions at-hand and also roadmap planning.
 
 
 
   You will be working in a team of Data Scientists to enable a culture of 360-degree analysis of business, with ownership of the modelling environments and ML models. You will get the support to evangelize sound practices for prototyping of concepts, to fail-fast and/or maintain continuum of persistent research.
 
 
 
   You will collaborate with multi-disciplinary teams of engineers, product owners & business stakeholders to solve complex & ambiguous problems, in the domains of:
 
 
   Gift Cards E-Commerce (B2B & B2C) 
  Forecasting of Inventory, Traffic & Breakage 
  Risk Modelling (Scorecards) 
  Fraud Detection & Prevention 
  Loyalty & Rewards Programs Modelling 
 Responsibilities: 
 
   You will:
 
 
   Partner closely with Product, Engineering, Marketing, Sales, Finance and Data Science teams to shape product strategy using rigorous scientific solutions
   Apply statistical, machine learning and econometric models on large datasets to: i) measure results and outcomes of our current models and product strategies, ii) optimize user experience while minimizing fraud/risks
   Design, conduct, and analyze experiments to quantify the impact of product and operation changes
   Develop metrics to guide product development, and create dashboards for key performance indicators and deep dive to understand the drivers
   Drive the collection of new data and the refinement of existing data sources
  Qualifications: 
 
  Masters (or equivalent) degree in a quantitative discipline (Statistics, Operations Research, Data Science, Mathematics, Physics, Engineering etc.). 
  A strong background in advanced mathematics, in particular statistics & probability theory, data mining, and machine learning. 
  5+ years of overall professional experience with 3+ years in data science, doing exploratory data analysis, testing hypotheses, and building prescriptive & predictive models.
   Demonstrated experience in creation, deployment and performance evaluation of supervised and unsupervised machine learning models.
   Proficiency in Python. 
  Experience handling terabyte size datasets, diving into data to discover hidden patterns, using data visualization tools, writing SQL.
   Strong Communication: ability to articulate clearly, navigate & adapt across different seniority levels. 
  Ability to use statistical, algorithmic, data mining, and visualization techniques to model complex problems, find opportunities, discover solutions, and deliver actionable business insights. 
  Be passionate about collaborating daily with your team and other groups while working via a distributed global operating model. 
  Be eager to help your teammates, share your knowledge with them, and learn from them. 
 
 
  Preferred Qualifications
 
 
   Experience with MLOPs frameworks
   Experience with creating explainable ML model
   Experience with source code control systems like Git
   Experience with data visualization and business intelligence tools like PowerBI
   Knowledge of experimental design and A/B testing
   Experience in an Agile development environment
   Experience with AWS cloud environment
   Proven ability to quickly learn and apply new techniques
   Demonstrable track record of dealing well with ambiguity, prioritizing needs, and delivering results in a dynamic environment 
  Must be a team-player and capable of handling multi-tasks in a dynamic environment
   Excellent business writing, verbal communication, and presentation skills
  Benefits: 
 
   Salary Range for all U.S. Residents (excluding Alaska, California, North Dakota, South Dakota): $125,430.00 to $149,000.00
 
 
   Salary Range for California Residents Only: $99,530.00 to $149,000.00
 
 
 
   Pay is based on several factors including but not limited to education, work experience, certifications, etc. In addition to your salary, Blackhawk Network offers benefits including 401k with employer match, medical, dental, vision, 12 paid holidays in the year 2023, sick pay accrual according to state law, parental leave, life insurance, disability insurance, accident and illness insurance, health and dependent care flexible spending accounts, wellness benefits, and flexible time off for all full-time employees. 
 EEO Statement: 
 
   Blackhawk Network provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. Blackhawk Network believes that diversity leads to strength. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training.
 
 
 
   Blackhawk Network encourages applicants with previous criminal records to apply to all positions and, pursuant to the San Francisco and Los Angeles Fair Chance Acts (and other “Fair Chance” laws), Blackhawk Network will consider for employment qualified applicants with arrest and conviction records. For Philadelphia applicants or jobs, please see a copy of Philadelphia’s ordinance on this topic by clicking this link: https://codelibrary.amlegal.com/codes/philadelphia/latest/philadelphia_pa/0-0-0-280104.","<div>
 About Blackhawk Network: 
 <div>
   Blackhawk Network (BHN) is the leader in global branded payment technologies. We strengthen relationships between brands and their customers, employees, and partners by transforming transactions into connections. BHN&#x2019;s portfolio includes: Gift Card &amp; eGift products, promotions and distribution that grow revenue faster; Rewards &amp; Incentives that build loyalty and acquisition and are integrated into today&#x2019;s leading platforms; and Payments that enable businesses and customers to access and disburse funds in convenient and innovative ways. BHN&#x2019;s network spans across the globe with over 400,000 consumer touchpoints. Learn more at BHN.com.
 </div>
 <div></div>
 <div>
  <br> This position may be performed remotely anywhere within the United States except for the State of Alaska, North Dakota, or South Dakota.
 </div> Overview: 
 <div>
   We are looking for an experienced data scientist to lead the initiatives we&#x2019;re taking, to improve the product offerings of Blackhawk Network, from the perspective of risk modelling and business forecasting (prescriptive &amp; predictive).
 </div>
 <div></div>
 <div>
  <br> As a team member in core data science team, you will own the research charter for data and decision science to enable the business stakeholders to be data-driven and deterministic, by providing insights into the decisions at-hand and also roadmap planning.
 </div>
 <div></div>
 <div>
  <br> You will be working in a team of Data Scientists to enable a culture of 360-degree analysis of business, with ownership of the modelling environments and ML models. You will get the support to evangelize sound practices for prototyping of concepts, to fail-fast and/or maintain continuum of persistent research.
 </div>
 <div></div>
 <div>
  <br> You will collaborate with multi-disciplinary teams of engineers, product owners &amp; business stakeholders to solve complex &amp; ambiguous problems, in the domains of:
 </div>
 <ul>
  <li> Gift Cards E-Commerce (B2B &amp; B2C) </li>
  <li>Forecasting of Inventory, Traffic &amp; Breakage </li>
  <li>Risk Modelling (Scorecards) </li>
  <li>Fraud Detection &amp; Prevention </li>
  <li>Loyalty &amp; Rewards Programs Modelling </li>
 </ul>Responsibilities: 
 <div>
   You will:
 </div>
 <ul>
  <li> Partner closely with Product, Engineering, Marketing, Sales, Finance and Data Science teams to shape product strategy using rigorous scientific solutions</li>
  <li> Apply statistical, machine learning and econometric models on large datasets to: i) measure results and outcomes of our current models and product strategies, ii) optimize user experience while minimizing fraud/risks</li>
  <li> Design, conduct, and analyze experiments to quantify the impact of product and operation changes</li>
  <li> Develop metrics to guide product development, and create dashboards for key performance indicators and deep dive to understand the drivers</li>
  <li> Drive the collection of new data and the refinement of existing data sources</li>
 </ul> Qualifications: 
 <ul>
  <li>Masters (or equivalent) degree in a quantitative discipline (Statistics, Operations Research, Data Science, Mathematics, Physics, Engineering etc.). </li>
  <li>A strong background in advanced mathematics, in particular statistics &amp; probability theory, data mining, and machine learning. </li>
  <li>5+ years of overall professional experience with 3+ years in data science, doing exploratory data analysis, testing hypotheses, and building prescriptive &amp; predictive models.</li>
  <li> Demonstrated experience in creation, deployment and performance evaluation of supervised and unsupervised machine learning models.</li>
  <li> Proficiency in Python. </li>
  <li>Experience handling terabyte size datasets, diving into data to discover hidden patterns, using data visualization tools, writing SQL.</li>
  <li> Strong Communication: ability to articulate clearly, navigate &amp; adapt across different seniority levels. </li>
  <li>Ability to use statistical, algorithmic, data mining, and visualization techniques to model complex problems, find opportunities, discover solutions, and deliver actionable business insights. </li>
  <li>Be passionate about collaborating daily with your team and other groups while working via a distributed global operating model. </li>
  <li>Be eager to help your teammates, share your knowledge with them, and learn from them. </li>
 </ul>
 <div>
  <b>Preferred Qualifications</b>
 </div>
 <ul>
  <li> Experience with MLOPs frameworks</li>
  <li> Experience with creating explainable ML model</li>
  <li> Experience with source code control systems like Git</li>
  <li> Experience with data visualization and business intelligence tools like PowerBI</li>
  <li> Knowledge of experimental design and A/B testing</li>
  <li> Experience in an Agile development environment</li>
  <li> Experience with AWS cloud environment</li>
  <li> Proven ability to quickly learn and apply new techniques</li>
  <li> Demonstrable track record of dealing well with ambiguity, prioritizing needs, and delivering results in a dynamic environment </li>
  <li>Must be a team-player and capable of handling multi-tasks in a dynamic environment</li>
  <li> Excellent business writing, verbal communication, and presentation skills</li>
 </ul> Benefits: 
 <div>
   Salary Range for all U.S. Residents (excluding Alaska, California, North Dakota, South Dakota): &#x24;125,430.00 to &#x24;149,000.00
 </div>
 <div>
   Salary Range for California Residents Only: &#x24;99,530.00 to &#x24;149,000.00
 </div>
 <div></div>
 <div>
  <br> Pay is based on several factors including but not limited to education, work experience, certifications, etc. In addition to your salary, Blackhawk Network offers benefits including 401k with employer match, medical, dental, vision, 12 paid holidays in the year 2023, sick pay accrual according to state law, parental leave, life insurance, disability insurance, accident and illness insurance, health and dependent care flexible spending accounts, wellness benefits, and flexible time off for all full-time employees. 
 </div>EEO Statement: 
 <div>
   Blackhawk Network provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. Blackhawk Network believes that diversity leads to strength. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training.
 </div>
 <div></div>
 <div>
  <br> Blackhawk Network encourages applicants with previous criminal records to apply to all positions and, pursuant to the San Francisco and Los Angeles Fair Chance Acts (and other &#x201c;Fair Chance&#x201d; laws), Blackhawk Network will consider for employment qualified applicants with arrest and conviction records. For Philadelphia applicants or jobs, please see a copy of Philadelphia&#x2019;s ordinance on this topic by clicking this link: https://codelibrary.amlegal.com/codes/philadelphia/latest/philadelphia_pa/0-0-0-280104.
 </div>
</div>","https://careers-blackhawknetwork.icims.com/jobs/20300/job?utm_source=indeed_integration&iis=Job%20Board&iisn=Indeed&indeed-apply-token=73a2d2b2a8d6d5c0a62696875eaebd669103652d3f0c2cd5445d3e66b1592b0f","8205e7f76522b355",,"Full-time",,"Remote","Staff Software Engineer - Data science and Machine Learning (Remote)","19 days ago","2023-10-06T11:51:39.852Z","3.5","206","$99,530 - $149,000 a year","2023-10-25T11:51:39.853Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=8205e7f76522b355&from=jasx&tk=1hdjaos832ci6000&vjs=3"
"Providencia Group","TITLE: Data Engineer
 
 
   LOCATION: Remote
 
 
   TRAVEL: Minimal
 
 
 
   About Us
 
 
   The Providencia Group is led by a purpose: to address global challenges and make an impact that matters through delivering transformative solutions. This purpose defines who we are and extends to relationships with our clients, our people, and our communities. We combine purpose, innovation, and experience to deliver impactful results.
 
 
 
   About The Team
 
 
   We are problem solvers working with leading agencies and organizations to help them address many of today’s most complex challenges. Our world-class team of technologists, program managers, and subject matter experts is uniquely qualified to address ever-evolving, large-scale challenges. In an imperfect world, The Providencia Group puts capability and purpose into action.
 
 
 
   What you’ll be part of – TPG Culture
 
 
   At TPG, we expect incredible tangible results. TPG professionals play a unique role in delivering these results. We reach across disciplines and borders to serve our global organization. We provide a roadmap focusing on people, our work, and continuous improvement. We see people as people, take care of each other, commit to the mission, move quickly and bravely, get better every day, and seek truth. We are the backbone of TPG.
 
 
 
   What You’ll Do
 
 
   The Data Engineer plays a critical role in managing and processing data to support our organization's analytical and operational needs. This position involves working with data pipelines, databases, and architectures to ensure data quality and accessibility.
 
 
 
   Responsibilities include, but are not limited to:
 
 
   Analyze raw data from various sources
   Develop and maintain datasets, schemas and models
   Improve data quality and efficiency
   Document data flows and mappings
   Collaborate with data analysts, scientists and other stakeholders
   Duties are performed via a government approved computer system
   Employees are required to possess strong computer skills in MS Word and Excel
   Perform related duties as assigned, within your scope of practice – management reserves the right to revise these duties as necessary
   Develop internal/external reports for information that is coming from ORR
   Assist with buildout of data dictionary through SharePoint
   Analyze data sets to ensure alignment to overall organizational focus
   Advise on updated reporting to capture data through mapping and format it in a useable format that can be easily interpreted by other departments
   Bring innovation and creativity to legacy technologies and programs to drive/shift changes
 
 
 
   Minimum Qualifications & Skills
 
 
   Bachelor’s degree in computer science, engineering or related field
   Preferred programming languages include SQL, Python, R, Tableau, and PowerBI
   Experience in data engineering, development or analysis
   Knowledge of data structures, algorithms and programming languages
   Proficiency in database systems, tools and frameworks
   Data visualization, communication and presentation skills
   Must possess strong computer skills in MS Office, including Excel, Word, Teams
   Ability to type 45 wpm
 
 
 
   Work Environment
 
 
   This is a remote opportunity where occasional travel could be required. Since this is a remote role, a dedicated workspace conducive to full videoconferencing (camera and audio) for facilitating webinars and online discussions.
 
 
 
   Work Schedule
 
 
   This is a full-time position, but hours could vary depending on needs. May include travel, evenings, and weekends to meet different time zones and projects.
 
 
 
   Condition of employment
 
 
   Complete a rigorous culture and competency testing process
   Complete a Drug Test
   Must be at least 21 years of age
   A valid US Driver’s license
   Have the ability to obtain a Public Trust Clearance
 
 
 
   Security Clearance Requirements
 
 
   Applicants selected will be subject to a government background investigation and may need to meet eligibility requirements for access to classified information.
   Must be a U.S Citizen or Permanent Resident.
   Residency requirement - 3 consecutive years in the last 5 years.
 
 
 
   Physical Demands
 
 
   Standing/Walking/Mobility: Must have mobility to attend meeting with other managers and employees.
   Climbing/Stooping/Kneeling: 10% of the time.
   Lifting/Pulling/Pushing: 10% of the time.
   Fingering/Grasping/Feeling: Must be able to write, type and use a telephone system 100% of the time.
   Sitting: Sitting for prolonged and extended periods of time.
 
 
   For more information about the company please visit our website at 
  
   https://www.theprovidenciagroup.com
  
 
 
 
   Providencia is an Equal Opportunity Employer and does not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, disability or any other federal, state or local protected class.","<div>
 <div>
  TITLE: Data Engineer
 </div>
 <div>
   LOCATION: Remote
 </div>
 <div>
   TRAVEL: Minimal
 </div>
 <div></div>
 <div>
   About Us
 </div>
 <div>
   The Providencia Group is led by a purpose: to address global challenges and make an impact that matters through delivering transformative solutions. This purpose defines who we are and extends to relationships with our clients, our people, and our communities. We combine purpose, innovation, and experience to deliver impactful results.
 </div>
 <div></div>
 <div>
   About The Team
 </div>
 <div>
   We are problem solvers working with leading agencies and organizations to help them address many of today&#x2019;s most complex challenges. Our world-class team of technologists, program managers, and subject matter experts is uniquely qualified to address ever-evolving, large-scale challenges. In an imperfect world, The Providencia Group puts capability and purpose into action.
 </div>
 <div></div>
 <div>
   What you&#x2019;ll be part of &#x2013; TPG Culture
 </div>
 <div>
   At TPG, we expect incredible tangible results. TPG professionals play a unique role in delivering these results. We reach across disciplines and borders to serve our global organization. We provide a roadmap focusing on people, our work, and continuous improvement. We see people as people, take care of each other, commit to the mission, move quickly and bravely, get better every day, and seek truth. We are the backbone of TPG.
 </div>
 <div></div>
 <div>
   What You&#x2019;ll Do
 </div>
 <div>
   The Data Engineer plays a critical role in managing and processing data to support our organization&apos;s analytical and operational needs. This position involves working with data pipelines, databases, and architectures to ensure data quality and accessibility.
 </div>
 <div></div>
 <div>
   Responsibilities include, but are not limited to:
 </div>
 <ul>
  <li> Analyze raw data from various sources</li>
  <li> Develop and maintain datasets, schemas and models</li>
  <li> Improve data quality and efficiency</li>
  <li> Document data flows and mappings</li>
  <li> Collaborate with data analysts, scientists and other stakeholders</li>
  <li> Duties are performed via a government approved computer system</li>
  <li> Employees are required to possess strong computer skills in MS Word and Excel</li>
  <li> Perform related duties as assigned, within your scope of practice &#x2013; management reserves the right to revise these duties as necessary</li>
  <li> Develop internal/external reports for information that is coming from ORR</li>
  <li> Assist with buildout of data dictionary through SharePoint</li>
  <li> Analyze data sets to ensure alignment to overall organizational focus</li>
  <li> Advise on updated reporting to capture data through mapping and format it in a useable format that can be easily interpreted by other departments</li>
  <li> Bring innovation and creativity to legacy technologies and programs to drive/shift changes</li>
 </ul>
 <div></div>
 <div>
   Minimum Qualifications &amp; Skills
 </div>
 <ul>
  <li> Bachelor&#x2019;s degree in computer science, engineering or related field</li>
  <li> Preferred programming languages include SQL, Python, R, Tableau, and PowerBI</li>
  <li> Experience in data engineering, development or analysis</li>
  <li> Knowledge of data structures, algorithms and programming languages</li>
  <li> Proficiency in database systems, tools and frameworks</li>
  <li> Data visualization, communication and presentation skills</li>
  <li> Must possess strong computer skills in MS Office, including Excel, Word, Teams</li>
  <li> Ability to type 45 wpm</li>
 </ul>
 <div></div>
 <div>
   Work Environment
 </div>
 <div>
   This is a remote opportunity where occasional travel could be required. Since this is a remote role, a dedicated workspace conducive to full videoconferencing (camera and audio) for facilitating webinars and online discussions.
 </div>
 <div></div>
 <div>
   Work Schedule
 </div>
 <div>
   This is a full-time position, but hours could vary depending on needs. May include travel, evenings, and weekends to meet different time zones and projects.
 </div>
 <div></div>
 <div>
   Condition of employment
 </div>
 <ul>
  <li> Complete a rigorous culture and competency testing process</li>
  <li> Complete a Drug Test</li>
  <li> Must be at least 21 years of age</li>
  <li> A valid US Driver&#x2019;s license</li>
  <li> Have the ability to obtain a Public Trust Clearance</li>
 </ul>
 <div></div>
 <div>
   Security Clearance Requirements
 </div>
 <ul>
  <li> Applicants selected will be subject to a government background investigation and may need to meet eligibility requirements for access to classified information.</li>
  <li> Must be a U.S Citizen or Permanent Resident.</li>
  <li> Residency requirement - 3 consecutive years in the last 5 years.</li>
 </ul>
 <div></div>
 <div>
   Physical Demands
 </div>
 <ul>
  <li> Standing/Walking/Mobility: Must have mobility to attend meeting with other managers and employees.</li>
  <li> Climbing/Stooping/Kneeling: 10% of the time.</li>
  <li> Lifting/Pulling/Pushing: 10% of the time.</li>
  <li> Fingering/Grasping/Feeling: Must be able to write, type and use a telephone system 100% of the time.</li>
  <li> Sitting: Sitting for prolonged and extended periods of time.</li>
 </ul>
 <div>
   For more information about the company please visit our website at 
  <div>
   https://www.theprovidenciagroup.com
  </div>
 </div>
 <div></div>
 <div>
   Providencia is an Equal Opportunity Employer and does not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, disability or any other federal, state or local protected class.
 </div>
</div>","https://kmkp.wd1.myworkdayjobs.com/en-US/TPGCareers/job/Remote/Data-Engineer_R-2300947-1","45d5e9c894c62713",,"Full-time",,"Remote","Data Engineer","19 days ago","2023-10-06T11:51:42.573Z","2.8","16",,"2023-10-25T11:51:42.574Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=45d5e9c894c62713&from=jasx&tk=1hdjaos832ci6000&vjs=3"
"Argano","REQUIRED QUALIFICATIONS, EXPERIENCE & SKILLS 
 
  
   5 years' experience consulting in Data or Dynamics business application domain
   
  
   Expert experience with T-SQL language
   
  
   Experience in creating Data Factory pipelines to orchestrate ingestion and transformation data for use in analytics and system integration
   
  
   Experience with the Dynamics 365 data model
   
  
   Experience using modern Azure data services such as Azure Synapse, Azure SQL Database, Azure Data Lake Storage Gen 2, Microsoft Fabric
   
  
   Familiarity with security configuration and security policies, and best practices within Azure
   
  
   Curious and tenacious when it comes to leveraging new Azure technology to deliver novel solutions for clients
   
  
 
 ADDITIONAL QUALIFICATIONS & SKILLS 
 
  
   Azure Data Engineer Associate Certification preferred (DP-203)
   
  
   Familiar with data lakehouse patterns and practices preferred
   
  
   Bachelor’s Degree in Information Systems Management, Computer Science, or related field
   
  
   Demonstrated ability to lead project workstream including interfacing in a client facing role
   
  
   Familiar with Agile implementation methodology
   
  
   Experience working with a multicultural and/or multilingual team across several time zones remotely","<p></p>
<div>
 <p>REQUIRED QUALIFICATIONS, EXPERIENCE &amp; SKILLS</p> 
 <div>
  <ul>
   <li>5 years&apos; experience consulting in Data or Dynamics business application domain</li>
  </ul> 
  <ul>
   <li>Expert experience with T-SQL language</li>
  </ul> 
  <ul>
   <li>Experience in creating Data Factory pipelines to orchestrate ingestion and transformation data for use in analytics and system integration</li>
  </ul> 
  <ul>
   <li>Experience with the Dynamics 365 data model</li>
  </ul> 
  <ul>
   <li>Experience using modern Azure data services such as Azure Synapse, Azure SQL Database, Azure Data Lake Storage Gen 2, Microsoft Fabric</li>
  </ul> 
  <ul>
   <li>Familiarity with security configuration and security policies, and best practices within Azure</li>
  </ul> 
  <ul>
   <li>Curious and tenacious when it comes to leveraging new Azure technology to deliver novel solutions for clients</li>
  </ul> 
 </div> 
 <p></p>
 <p>ADDITIONAL QUALIFICATIONS &amp; SKILLS</p> 
 <div>
  <ul>
   <li>Azure Data Engineer Associate Certification preferred (DP-203)</li>
  </ul> 
  <ul>
   <li>Familiar with data lakehouse patterns and practices preferred</li>
  </ul> 
  <ul>
   <li>Bachelor&#x2019;s Degree in Information Systems Management, Computer Science, or related field</li>
  </ul> 
  <ul>
   <li>Demonstrated ability to lead project workstream including interfacing in a client facing role</li>
  </ul> 
  <ul>
   <li>Familiar with Agile implementation methodology</li>
  </ul> 
  <ul>
   <li>Experience working with a multicultural and/or multilingual team across several time zones remotely</li>
  </ul>
 </div>
</div>","https://argano.hrmdirect.com/employment/view.php?req=2778994&jbsrc=1014","502637fdf8fd4c35",,,,"Remote","Senior Data Engineer / Remote Mexico Position","14 days ago","2023-10-11T11:51:42.374Z",,,,"2023-10-25T11:51:42.375Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=502637fdf8fd4c35&from=jasx&tk=1hdjaos832ci6000&vjs=3"
"Murmuration","Who We Are
  Murmuration is a nonprofit organization focused on leveraging civic engagement to drive greater equity. We provide sophisticated tools, data, strategic guidance, and programmatic support to help our partner organizations increase civic engagement and marshal support to drive change at the community level. Our best-in-class data and easy-to-use tools have been used by hundreds of organizations to make informed decisions about who they need to reach and how to achieve and sustain impact – and to put those decisions into action. 
 Note: At Murmuration, we are committed to becoming an even more diverse, equitable, and inclusive workplace. To this end, all staff members are expected to actively participate in DEI (diversity, equity, inclusion) programming. 
 About the Position
  We are looking for an innovative Data Engineer who will build and support key components within our data infrastructure with a specific focus on the data pipelines that power our products. This individual will work within our Data Engineering team, partnering with Data Managers and Data Scientists to manage the ongoing delivery of our key data sets for our analytical and product use cases. This individual must be able to understand data requirements and will also be responsible for providing continuous refinement and improvements to our data pipelines. The Data Team is a highly collaborative, friendly, and hard-working group, and we are looking for team members who embody those values. 
 The Data Engineer will report to our Senior Data Engineer. 
 What You’ll Do:
  
  Design, develop, and maintain data pipelines using tools and technologies, such as Dagster and Airflow for orchestration, and Snowflake, AWS, and MongoDB for datastores; 
  Ensure pipelines are scalable, reliable, and fault-tolerant; 
  Be responsible for managing data from various sources, such as third party data providers, data collected, or data created internally; 
  Ensure data is ingested in a timely and efficient manner, with processes to manage data quality and integrity; 
  Transform and cleanse raw data into a structured and usable format; 
  Implement monitoring and alerting processes to detect, communicate, and address issues in data pipelines; 
  Implement data quality checks and validation processes to ensure data accuracy, completeness, and consistency; 
  Continuously optimize data pipelines for better performance and cost efficiency; 
  Maintain comprehensive and up-to-date documentation for data pipelines, including data lineage, dependencies, and configurations; 
  Ensure documentation is up-to-date and accessible to team members; 
  Provide support for data-related issues, including investigating and resolving pipeline failures; 
  Respond to ad-hoc data requests and troubleshoot data-related problems; 
  Collaborate with data scientists, analysts, and other stakeholders to understand their data requirements and deliver data in a usable format; and 
  Work closely with other data engineers to align data pipelines with overall data architecture strategies. 
 
 Requirements
  What You Should Have:
  
  Education and/or experience in Computer Science, Computer Engineering, or relevant field; 
  A minimum of 3 years’ experience working with large scale databases/cloud databases using SQL and Python; 
  Strong organizational and analytical abilities; 
  Strong problem-solving skills; 
  Strong written and verbal communication skills; 
  Familiarity with Data Orchestration Tools (Dagster, Airflow); 
  Familiarity with Snowflake and AWS (primarily S3, EC2, ECS); 
  Experience working flexibly within smaller teams; and 
  Practical knowledge of software development lifecycle (SDLC). 
 
 What You Could Have:
  
  Familiarity with Voter File Data; 
  Experience with or interest in political data; and 
  Experience within a support team providing technical support to other data functions (e.g., Data Scientists, Data Managers, etc.) 
 
 Talented Data Engineers come from all walks of life and careers. If you are passionate about civic engagement and technology, please apply, even if you do not check every box!
  Benefits
  Location and Compensation
  The Data Engineer is a full-time, salaried position with a comprehensive benefits package. It is based anywhere in the U.S. The salary range for this position is $100,000 - $130,000 and is commensurate with experience. 
 Our Culture of Care
  We work hard to create a culture of care to ensure that our staff are best equipped to lead happy, healthy, and balanced lives. To that end, we offer a comprehensive benefits package which includes:
  
  Health, vision, and dental insurance with 100% of premiums covered for you and qualifying family members; 
  Retirement benefits with a 4% employer match; 
  A flexible unlimited PTO plan; 
  Generous paid parental leave; 
  Pre-tax commuter benefits; 
  A company laptop; 
  A flexible remote work environment; 
  A home office setup stipend for all new employees; 
  Monthly reimbursement for remote work expenses; 
  A yearly professional development fund; 
  Mental health and wellness benefits through Calm and Better Help; and 
  Yearly in-person staff retreats; and 
  A welcoming culture that celebrates diversity, equity, and inclusion. 
 
 An Equal-Opportunity Employer with a Commitment to Diversity
  Murmuration is proud to be an equal opportunity employer, and as an organization committed to diversity and the perspective of all voices, we consider applicants equally of race, gender, color, sexual orientation, religion, marital status, disability, political affiliation and national origin. We reasonably accommodate staff members and/or applicants with disabilities, provided they are otherwise able to perform the essential functions of the job.","<div>
 <p><b>Who We Are</b></p>
 <p> Murmuration is a nonprofit organization focused on leveraging civic engagement to drive greater equity. We provide sophisticated tools, data, strategic guidance, and programmatic support to help our partner organizations increase civic engagement and marshal support to drive change at the community level. Our best-in-class data and easy-to-use tools have been used by hundreds of organizations to make informed decisions about who they need to reach and how to achieve and sustain impact &#x2013; and to put those decisions into action.</p> 
 <p>Note: At Murmuration, we are committed to becoming an even more diverse, equitable, and inclusive workplace. To this end, all staff members are expected to actively participate in DEI (diversity, equity, inclusion) programming.</p> 
 <p><b>About the Position</b></p>
 <p> We are looking for an innovative Data Engineer who will build and support key components within our data infrastructure with a specific focus on the data pipelines that power our products. This individual will work within our Data Engineering team, partnering with Data Managers and Data Scientists to manage the ongoing delivery of our key data sets for our analytical and product use cases. This individual must be able to understand data requirements and will also be responsible for providing continuous refinement and improvements to our data pipelines. The Data Team is a highly collaborative, friendly, and hard-working group, and we are looking for team members who embody those values. </p>
 <p>The Data Engineer will report to our Senior Data Engineer.</p> 
 <p><b>What You&#x2019;ll Do:</b></p>
 <ul> 
  <li>Design, develop, and maintain data pipelines using tools and technologies, such as Dagster and Airflow for orchestration, and Snowflake, AWS, and MongoDB for datastores;</li> 
  <li>Ensure pipelines are scalable, reliable, and fault-tolerant;</li> 
  <li>Be responsible for managing data from various sources, such as third party data providers, data collected, or data created internally;</li> 
  <li>Ensure data is ingested in a timely and efficient manner, with processes to manage data quality and integrity;</li> 
  <li>Transform and cleanse raw data into a structured and usable format;</li> 
  <li>Implement monitoring and alerting processes to detect, communicate, and address issues in data pipelines;</li> 
  <li>Implement data quality checks and validation processes to ensure data accuracy, completeness, and consistency;</li> 
  <li>Continuously optimize data pipelines for better performance and cost efficiency;</li> 
  <li>Maintain comprehensive and up-to-date documentation for data pipelines, including data lineage, dependencies, and configurations;</li> 
  <li>Ensure documentation is up-to-date and accessible to team members;</li> 
  <li>Provide support for data-related issues, including investigating and resolving pipeline failures;</li> 
  <li>Respond to ad-hoc data requests and troubleshoot data-related problems;</li> 
  <li>Collaborate with data scientists, analysts, and other stakeholders to understand their data requirements and deliver data in a usable format; and</li> 
  <li>Work closely with other data engineers to align data pipelines with overall data architecture strategies.</li> 
 </ul>
 <p><b>Requirements</b></p>
 <p><b> What You </b><b><i>Should</i></b><b> Have:</b></p>
 <ul> 
  <li>Education and/or experience in Computer Science, Computer Engineering, or relevant field;</li> 
  <li>A minimum of 3 years&#x2019; experience working with large scale databases/cloud databases using SQL and Python;</li> 
  <li>Strong organizational and analytical abilities;</li> 
  <li>Strong problem-solving skills;</li> 
  <li>Strong written and verbal communication skills;</li> 
  <li>Familiarity with Data Orchestration Tools (Dagster, Airflow);</li> 
  <li>Familiarity with Snowflake and AWS (primarily S3, EC2, ECS);</li> 
  <li>Experience working flexibly within smaller teams; and</li> 
  <li>Practical knowledge of software development lifecycle (SDLC).</li> 
 </ul>
 <p><b>What You </b><b><i>Could</i></b><b> Have:</b></p>
 <ul> 
  <li>Familiarity with Voter File Data;</li> 
  <li>Experience with or interest in political data; and</li> 
  <li>Experience within a support team providing technical support to other data functions (e.g., Data Scientists, Data Managers, etc.)</li> 
 </ul>
 <p>Talented Data Engineers come from all walks of life and careers. If you are passionate about civic engagement and technology, please apply, even if you do not check every box!</p>
 <p><b> Benefits</b></p>
 <p><b> Location and Compensation</b></p>
 <p> The Data Engineer is a full-time, salaried position with a comprehensive benefits package. It is based anywhere in the U.S. The salary range for this position is &#x24;100,000 - &#x24;130,000 and is commensurate with experience.</p> 
 <p><b>Our Culture of Care</b></p>
 <p> We work hard to create a culture of care to ensure that our staff are best equipped to lead happy, healthy, and balanced lives. To that end, we offer a comprehensive benefits package which includes:</p>
 <ul> 
  <li>Health, vision, and dental insurance with 100% of premiums covered for you and qualifying family members;</li> 
  <li>Retirement benefits with a 4% employer match;</li> 
  <li>A flexible unlimited PTO plan;</li> 
  <li>Generous paid parental leave;</li> 
  <li>Pre-tax commuter benefits;</li> 
  <li>A company laptop;</li> 
  <li>A flexible remote work environment;</li> 
  <li>A home office setup stipend for all new employees;</li> 
  <li>Monthly reimbursement for remote work expenses;</li> 
  <li>A yearly professional development fund;</li> 
  <li>Mental health and wellness benefits through Calm and Better Help; and</li> 
  <li>Yearly in-person staff retreats; and</li> 
  <li>A welcoming culture that celebrates diversity, equity, and inclusion.</li> 
 </ul>
 <p><b>An Equal-Opportunity Employer with a Commitment to Diversity</b></p>
 <p> Murmuration is proud to be an equal opportunity employer, and as an organization committed to diversity and the perspective of all voices, we consider applicants equally of race, gender, color, sexual orientation, religion, marital status, disability, political affiliation and national origin. We reasonably accommodate staff members and/or applicants with disabilities, provided they are otherwise able to perform the essential functions of the job.</p>
</div>","https://apply.workable.com/murmuration/j/1CEFDBC383","1f74a01d7a395286",,"Full-time",,"Remote","Data Engineer","18 days ago","2023-10-07T11:51:40.433Z",,,"$100,000 - $130,000 a year","2023-10-25T11:51:40.435Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=1f74a01d7a395286&from=jasx&tk=1hdjaos832ci6000&vjs=3"
"Baylor Scott & White Health","The Lead Data Protection Engineer will be responsible for overseeing and implementing CIS Control 3 (Data Protection) strategies and solutions to safeguard our organization’s critical data assets. This role will play a pivotal role in enhancing data protection measures across the organization, developing information security policies and standards, introducing security best practices, and supporting the implementation and refinement of selected technologies to support a continuous growth model. This role will also be responsible for building and maintaining strong relationships with service line leaders, vendors, and other departments to help collectively further BSWH’s strategic initiatives. They will assist respective IS Directors and/or leadership with understanding the role the data protection applications play in relation to the IS Application environment and managing architecture of the technology as needed to support the data protection controls. 
  SALARY 
  The pay range for this position is $116,521 (entry-level qualifications) - $209,560 (highly experienced) The specific rate will depend upon the successful candidate’s specific qualifications and prior experience. 
  ESSENTIAL FUNCTIONS OF THE ROLE 
  Responsible for the day-to-day operational activities of the Data Loss Prevention team that includes implementation, support and maintenance of the technology and management of the managed service provider. 
  Ensures that all system platforms are functional and secure. 
  Ensures fulfillment of legal and contractual information security and privacy mandates. 
  Creates and drives long term planning and strategic vision creation for continuous improvement to the information management program, with oversight from Director as needed. 
  Establishes budget and tracks to ensure budget is on track and aligned with strategic goals. 
  Understanding of interdependencies of healthcare landscape and its influence on portfolio. 
  Owns assorted departmental responsibilities as assigned (ie. contract management staffing) 
  Partners with and leads team towards the identification of problems opportunities from a digital innovation perspective, oversee and perform the development and documentation of business requirements, objectives, deliverables, and specifications, and collaboration with customers employees, and support staff. 
  Actively maintains a pulse on digital disruption, innovation, and healthcare. 
  Provides recommendations for improvements that meet team objectives. 
  Presents and explains findings to leadership team and individuals. 
  KEY SUCCESS FACTORS 
  
  Superior leadership, problem solving, team building, and decision-making skills. 
  Skilled project manager with ability to articulate business needs. 
  Excellent written, verbal, and interpersonal communication skills. 
  Proficient computer software and database skills, including Microsoft Products (Excel, SharePoint, Teams, Forms, etc.). 
  Ability to focus and prioritize strategic objectives and work in a growing and challenging environment. 
  Maintains a broad knowledge of state-of-the-art technology equipment and systems. 
  
 LOCATION: Remote 
  SCHEDULE: Full Time 
  BENEFITS 
  Our competitive benefits package includes the following 
  
  Immediate eligibility for health and welfare benefits 
  401(k) savings plan with dollar-for-dollar match up to 5% 
  Tuition Reimbursement 
  PTO accrual beginning Day 1 
  
 Note: Benefits may vary based upon position type and/or level
  QUALIFICATIONS 
 
  EDUCATION - Bachelor's or 4 years of work experience above the minimum qualification 
  EXPERIENCE - 7 Years of Experience","<div>
 <p>The <b>Lead Data Protection Engineer</b> will be responsible for overseeing and implementing CIS Control 3 (Data Protection) strategies and solutions to safeguard our organization&#x2019;s critical data assets. This role will play a pivotal role in enhancing data protection measures across the organization, developing information security policies and standards, introducing security best practices, and supporting the implementation and refinement of selected technologies to support a continuous growth model. This role will also be responsible for building and maintaining strong relationships with service line leaders, vendors, and other departments to help collectively further BSWH&#x2019;s strategic initiatives. They will assist respective IS Directors and/or leadership with understanding the role the data protection applications play in relation to the IS Application environment and managing architecture of the technology as needed to support the data protection controls.</p> 
 <p><b> SALARY</b></p> 
 <p> The pay range for this position is &#x24;116,521 (entry-level qualifications) - &#x24;209,560 (highly experienced) The specific rate will depend upon the successful candidate&#x2019;s specific qualifications and prior experience.</p> 
 <p><b> ESSENTIAL FUNCTIONS OF THE ROLE</b></p> 
 <p> Responsible for the day-to-day operational activities of the Data Loss Prevention team that includes implementation, support and maintenance of the technology and management of the managed service provider.</p> 
 <p> Ensures that all system platforms are functional and secure.</p> 
 <p> Ensures fulfillment of legal and contractual information security and privacy mandates.</p> 
 <p> Creates and drives long term planning and strategic vision creation for continuous improvement to the information management program, with oversight from Director as needed.</p> 
 <p> Establishes budget and tracks to ensure budget is on track and aligned with strategic goals.</p> 
 <p> Understanding of interdependencies of healthcare landscape and its influence on portfolio.</p> 
 <p> Owns assorted departmental responsibilities as assigned (ie. contract management staffing)</p> 
 <p> Partners with and leads team towards the identification of problems opportunities from a digital innovation perspective, oversee and perform the development and documentation of business requirements, objectives, deliverables, and specifications, and collaboration with customers employees, and support staff.</p> 
 <p> Actively maintains a pulse on digital disruption, innovation, and healthcare.</p> 
 <p> Provides recommendations for improvements that meet team objectives.</p> 
 <p> Presents and explains findings to leadership team and individuals.</p> 
 <p><b> KEY SUCCESS FACTORS</b></p> 
 <ul> 
  <li>Superior leadership, problem solving, team building, and decision-making skills.</li> 
  <li>Skilled project manager with ability to articulate business needs.</li> 
  <li>Excellent written, verbal, and interpersonal communication skills.</li> 
  <li>Proficient computer software and database skills, including Microsoft Products (Excel, SharePoint, Teams, Forms, etc.).</li> 
  <li>Ability to focus and prioritize strategic objectives and work in a growing and challenging environment.</li> 
  <li>Maintains a broad knowledge of state-of-the-art technology equipment and systems.</li> 
 </ul> 
 <p><b>LOCATION: Remote</b></p> 
 <p><b> SCHEDULE: Full Time</b></p> 
 <p><b> BENEFITS</b></p> 
 <p> Our competitive benefits package includes the following</p> 
 <ul> 
  <li>Immediate eligibility for health and welfare benefits</li> 
  <li>401(k) savings plan with dollar-for-dollar match up to 5%</li> 
  <li>Tuition Reimbursement</li> 
  <li>PTO accrual beginning Day 1</li> 
 </ul> 
 <p>Note: Benefits may vary based upon position type and/or level</p>
 <p><b> QUALIFICATIONS</b></p> 
 <ul>
  <li>EDUCATION - Bachelor&apos;s or 4 years of work experience above the minimum qualification</li> 
  <li>EXPERIENCE - 7 Years of Experience</li>
 </ul>
</div>","https://jobs.bswhealth.com/us/en/job/23018360/Lead-Data-Protection-Engineer?rx_campaign=indeed0&rx_ch=jobp4p&rx_group=119123&rx_job=23018360&rx_r=none&rx_source=Indeed&rx_ts=20231025T080340Z&rx_vp=cpc&utm_source=indeedorganic&rx_p=CGKRAT6ZO6&rx_viewer=e1acfd9c732c11ee8d0f35ee692dbc171ae413578d9141b88419d262d0e5832b","780bbe8f6d47e19d",,"Full-time",,"Remote","Lead Data Protection Engineer","18 days ago","2023-10-07T11:51:45.199Z","3.8","4051","$116,521 a year","2023-10-25T11:51:45.201Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=780bbe8f6d47e19d&from=jasx&tk=1hdjaos832ci6000&vjs=3"
"Ferguson Enterprises, LLC","Job Posting: 
 Ferguson is North America’s leading value-added distributor across residential, non-residential, new construction and repair, maintenance, and improvement (RMI) end markets. Spanning 34,000 suppliers and more than one million customers, we deliver local expertise, value-added solutions, and the industry’s most extensive portfolio of products. From infrastructure, plumbing, and appliances, to HVAC, fire protection, fabrication, and more, we make our customers’ complex projects simple, successful, and sustainable.
 
  Ferguson is seeking a Senior Master Data Software Engineer (with a strong foundation in C# and Python) to join our Enterprise Data & Analytics, Data Application Technology team to support transformational projects in the product data domain! Product Data is at the heart of Ferguson and having innovative technology to support the strategy and operations of the data is crucial. The Senior Master Data Software Engineer will both support and drive this innovation joining a team that is enthused about shaping the future. Ferguson is seeking minds with a passion and eye for technical and automation advancement.
 
  **This position may be remote or hybrid in accordance with company policy.**
  Duties and Responsibilities:
 
   Translates complex enterprise data business, functional, and technical requirements into technology solutions that enable product data collection, ingestion, enrichment and syndication.
   Conducts proof of concepts, technical design, coding, code reviews, bug id and resolution.
   Works with architects, BAs and QAs to develop vital implementation plans and designs.
   Work with vendors to evaluate 3rd party applications and/or resolve 3rd level issues.
   Conduct reviews, demos and presentations to peers, collaborators and vendors.
 
  Qualifications and Requirements:
 
   5-10 years of experience in Data Engineering / Software Development ideally implementing technical enterprise data solutions.
   Bachelor’s degree in computer science, information systems or equivalent work experience.
   High proficiency in C# and/or Python is required and knowledge in data management, data integration, data quality, project life cycle phases and industry best practices is expected.
   Prior experience in a master data implementation for wholesale / distributor industry is a plus.
   Hands-on configuration experience of data hierarchies, entity types, attributes, relationships, and crosswalks.
   Proficient in query tuning, performance tuning, troubleshooting, and debugging big data solutions.
   Experience implementing solutions in a cloud platform (Azure, AWS, and/or GCP) with an emphasis on data orchestration and enrichment. Experience on Azure is preferred.
   Experience with RDBMS like SQL Server, Oracle, Postgres, etc is required.
   Experience with big data platforms like Azure Synapse, Databricks, Amazon Redshift, and/or Google BigQuery is desirable.
   Experience with NoSQL databases like Azure CosmosDB, Amazon DynamoDB or MongoDB is desirable.
   Experience with data science and machine learning models is desirable.
   Experience with Terraform, ARM Templates or similar.
   Experience in implementing integrations using standard connectors, publish-subscribe, and other mechanisms (Azure, Oracle, Salesforce CRM, PIM/MDM solutions, etc).
   Familiarity with platform authentication patterns (SAML, SSO, OAuth).
   Solid understanding of environment management, release management, code versioning, and deployment methodologies.
 
 
  Ferguson is dedicated to providing meaningful benefits programs and products to our associates and their families—geared toward benefits, wellness, financial protection, and retirement savings. Ferguson offers a competitive benefits package that includes medical, dental, vision, retirement savings with company match, paid leave (vacation, sick, personal, holiday, and parental), employee assistance programs, associate discounts, community involvement opportunities, and much more!
 
  #LI-REMOTE
 
  Pay Range:
 
  Actual pay rate may vary depending upon location. The estimated pay range for this position is below. The specific rate will depend on a candidate’s qualifications and prior experience.
  $6,805.95 - $11,921.25
 
  Estimated Ranges displayed are Monthly for Salaried roles OR Hourly for all other roles.
 
  This role is Bonus or Incentive Plan eligible.
 
  The Company is an equal opportunity employer as well as a government contractor that shall abide by the requirements of 41 CFR 60-300.5(a), which prohibits discrimination against qualified protected Veterans and the requirements of 41 CFR 60-741.5(A), which prohibits discrimination against qualified individuals on the basis of disability.
 
  Ferguson Enterprises, LLC. is an equal employment employer F/M/Disability/Vet/Sexual Orientation/Gender Identity.
 
  Equal Employment Opportunity and Reasonable Accommodation Information","<div>
 <p><b>Job Posting: </b></p>
 <p>Ferguson is North America&#x2019;s leading value-added distributor across residential, non-residential, new construction and repair, maintenance, and improvement (RMI) end markets. Spanning 34,000 suppliers and more than one million customers, we deliver local expertise, value-added solutions, and the industry&#x2019;s most extensive portfolio of products. From infrastructure, plumbing, and appliances, to HVAC, fire protection, fabrication, and more, we make our customers&#x2019; complex projects simple, successful, and sustainable.</p>
 <p></p>
 <p> Ferguson is seeking a Senior Master Data Software Engineer (with a strong foundation in C# and Python) to join our Enterprise Data &amp; Analytics, Data Application Technology team to support transformational projects in the product data domain! Product Data is at the heart of Ferguson and having innovative technology to support the strategy and operations of the data is crucial. The Senior Master Data Software Engineer will both support and drive this innovation joining a team that is enthused about shaping the future. Ferguson is seeking minds with a passion and eye for technical and automation advancement.</p>
 <p></p>
 <p> **This position may be remote or hybrid in accordance with company policy.**</p>
 <h2 class=""jobSectionHeader""><b> Duties and Responsibilities:</b></h2>
 <ul>
  <li> Translates complex enterprise data business, functional, and technical requirements into technology solutions that enable product data collection, ingestion, enrichment and syndication.</li>
  <li> Conducts proof of concepts, technical design, coding, code reviews, bug id and resolution.</li>
  <li> Works with architects, BAs and QAs to develop vital implementation plans and designs.</li>
  <li> Work with vendors to evaluate 3rd party applications and/or resolve 3rd level issues.</li>
  <li> Conduct reviews, demos and presentations to peers, collaborators and vendors.</li>
 </ul>
 <h2 class=""jobSectionHeader""><b> Qualifications and Requirements:</b></h2>
 <ul>
  <li> 5-10 years of experience in Data Engineering / Software Development ideally implementing technical enterprise data solutions.</li>
  <li> Bachelor&#x2019;s degree in computer science, information systems or equivalent work experience.</li>
  <li> High proficiency in C# and/or Python is required and knowledge in data management, data integration, data quality, project life cycle phases and industry best practices is expected.</li>
  <li> Prior experience in a master data implementation for wholesale / distributor industry is a plus.</li>
  <li> Hands-on configuration experience of data hierarchies, entity types, attributes, relationships, and crosswalks.</li>
  <li> Proficient in query tuning, performance tuning, troubleshooting, and debugging big data solutions.</li>
  <li> Experience implementing solutions in a cloud platform (Azure, AWS, and/or GCP) with an emphasis on data orchestration and enrichment. Experience on Azure is preferred.</li>
  <li> Experience with RDBMS like SQL Server, Oracle, Postgres, etc is required.</li>
  <li> Experience with big data platforms like Azure Synapse, Databricks, Amazon Redshift, and/or Google BigQuery is desirable.</li>
  <li> Experience with NoSQL databases like Azure CosmosDB, Amazon DynamoDB or MongoDB is desirable.</li>
  <li> Experience with data science and machine learning models is desirable.</li>
  <li> Experience with Terraform, ARM Templates or similar.</li>
  <li> Experience in implementing integrations using standard connectors, publish-subscribe, and other mechanisms (Azure, Oracle, Salesforce CRM, PIM/MDM solutions, etc).</li>
  <li> Familiarity with platform authentication patterns (SAML, SSO, OAuth).</li>
  <li> Solid understanding of environment management, release management, code versioning, and deployment methodologies.</li>
 </ul>
 <p></p>
 <p> Ferguson is dedicated to providing meaningful benefits programs and products to our associates and their families&#x2014;geared toward benefits, wellness, financial protection, and retirement savings. Ferguson offers a competitive benefits package that includes medical, dental, vision, retirement savings with company match, paid leave (vacation, sick, personal, holiday, and parental), employee assistance programs, associate discounts, community involvement opportunities, and much more!</p>
 <p></p>
 <p> #LI-REMOTE</p>
 <ul></ul>
 <p><b> Pay Range:</b></p>
 <ul></ul>
 <p><i> Actual pay rate may vary depending upon location. The estimated pay range for this position is below. The specific rate will depend on a candidate&#x2019;s qualifications and prior experience.</i></p>
 <ul></ul> &#x24;6,805.95 - &#x24;11,921.25
 <ul></ul>
 <p><b><i> Estimated Ranges displayed are Monthly for Salaried roles </i></b><b>OR</b><b><i> Hourly for all other roles.</i></b></p>
 <ul></ul>
 <p> This role is Bonus or Incentive Plan eligible.</p>
 <ul></ul>
 <p><i> The Company is an equal opportunity employer as well as a government contractor that shall abide by the requirements of 41 CFR 60-300.5(a), which prohibits discrimination against qualified protected Veterans and the requirements of 41 CFR 60-741.5(A), which prohibits discrimination against qualified individuals on the basis of disability.</i></p>
 <p></p>
 <p><i> Ferguson Enterprises, LLC. is an equal employment employer </i><i>F/M/Disability/Vet/Sexual</i><i> Orientation/Gender</i><i> Identity.</i></p>
 <p></p>
 <p> Equal Employment Opportunity and Reasonable Accommodation Information</p>
</div>
<p></p>","https://ferguson.wd1.myworkdayjobs.com/Ferguson_Experienced/job/Remote/Sr-Master-Data-Software-Engineer---C----Python_R-106443?codes=JB-IND","57bd32600a16490a",,"Full-time",,"Richmond, VA 23219","Sr. Master Data Software Engineer - C# / Python","15 days ago","2023-10-10T11:51:54.204Z","3.3","1984",,"2023-10-25T11:51:54.207Z","US","remote","data engineer","https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0CE_hYSUASMK1H_jZaTCYczPzPzRspZHXdJ0kOAoFvUnbrGP-m66Hs79sy4tfvvujLdNH-zzeGBKtmJ8iZ4_WYzHBJVta2ijQ0cqMu5DMIEl_plVa8s1Gl4ZlgZzaWGT5_ygAPN4Uq-jHy2PNCxBZXsL1ZQeHs84AgXqKvXgS1BbteBG3vvggsYWntUWcXs41FKZfF_lw6T3z4w28J3S75XjysuLuWZSf5Hxi9VSXFmwA6W-XySwiYMhN_q5oAnWbJn1VsHdjZcfD8Gx5k9NLVX89U5CTmlYx5Ip7pmxYID4mtp_8IoD-bxsGLwsLu7_TzIY6xxrmQ06GlVMg_9G-DcMztjxvoXaKbdW0524eWZrsZUFWM9OuPU1IXYgM_qrlzRbnm_aH4bUbE0PZsDVrIPhdnKrcdMP3xR8YFSke8kLY85UOCshs1t0kT-jbjQp8Bgqt0pIGruQuaYGsPDYv1LPZmg43Xp2ymjAd0DXJahuDx77DVQDHKhZd_TWdgE5QvoXRF5Ukux2_1A-RZsN1NS1vfjsJMjj0mQS9ery6uVNx7jdcQ7rEw4kjrF9Sfid7IFMsiaRPrpk0lnzesIrwqkuIM7cbXWyXJWRYzCDtOFJWH_XZkKVtr4sw8uCURCT0MqSXkGpq5vGgeWEybcqljg00Phsa6kEPwy6Pbg5oNq8_5hjzL3j8rkQ08rf_P2vTlxFRAhLvekoQ7fdvARP4X-9OzGZiJXvu6uS-2IUMiddF4tl2qSZWAxXkGhspQP4ZQ%3D&xkcb=SoB5-_M3JzdsUZQ27J0NbzkdCdPP&p=6&fvj=0&vjs=3&jsa=9080&tk=1hdjaos832ci6000&from=jasx&wvign=1"
"GliaCell Technologies","Are you a Senior Data Scientist who is ready for a new challenge that will launch your career to the next level?
 
   Tired of being treated like a company drone?
   Tired of promised adventures during the hiring phase, then dropped off on a remote contract and never seen or heard from the mothership again?
   Our engineers were certainly tired of the same.
 
  At GliaCell our slogan is “We make It happen”.
 
   We will immerse you in the latest technologies.
   We will develop and support your own personalized training program to continue your individual growth.
   We will provide you with work that matters with our mission focused customers, and surround you with a family of brilliant engineers. 
 
 Culture isn’t something you need to talk about…if it just exists.
  If this sounds interesting to you, then we’d like to have a discussion regarding your next adventure! If you want to be a drone, this isn’t the place for you.
  We Make It Happen!
  GliaCell Technologies focuses on Software & System Engineering in Enterprise and Cyber Security solution spaces. We excel at delivering stable and reliable software solutions using Agile Software Development principles. These provide us the capability to deliver a quick turn-around using interactive applications and the integration of industry standard software stacks. 
 GliaCell’s Enterprise capabilities include Full-Stack Application Development, Big Data, Cloud Technologies, Analytics, Machine Learning, AI, and DevOps Containerization. We also provide customer solutions in the areas of CND, CNE, and CNO by providing our customers with assessments and solutions in Threat Mitigation, Vulnerability Exposure, Penetration Testing, Threat Hunting, and Preventing Advanced Persistent Threat.
  We Offer:
 
   Long term job security
   Competitive salaries & bonus opportunities
   Challenging work you are passionate about
   Ability to work with some amazingly talented people
 
  Job Description:
  GliaCell is seeking a Senior Data Scientist on one of our subcontracts. This is a full-time position offering the opportunity to support a U.S. Government customer. The mission is to provide technical expertise that assists in sustaining critical mission-related software and systems to a large government contract.
  Key Requirements:
  To be considered for this position you must have the following:
 
   Possess an active or rein-statable TS/SCI with Polygraph security clearance.
   Be a U.S. Citizen.
   Bachelor of Science Degree in Computer Science (or a related subject) (4 years of work experience can be substituted for the degree).
   14+ years of experience.
   Demonstrated experience with Python and/or Java programming languages.
   Bash scripting.
   Familiarity with Machine Learning/Artificial Intelligence (professional or personal research).
   Cloud development experience.
   Enterprise development experience.
   Works well independently as well as on a team.
   Strong communication skills.
   Flexible and motivated.
 
  Desired:
 
   Experience responding to user issues
   Cloud developer certification (Hadoop, Cloudera, other)
   Jupyter Notebook experience
   Data science techniques
   Natural language processing experience
   Experience with at-scale development and cloud programming (Spark, Map Reduce, etc)
   Docker experience
   Kubernetes experience
   GPU application experience
   Strong interpersonal and organizational skills
   Ability to work in a dynamically changing environment
 
  Location: Annapolis Junction, Maryland / Partial Telework
  Salary: Based on Education, Years of Experience, Skill, and Abilities
  Check Out Our Benefits:
 
   Paid Time Off
   Medical, Dental & Vision Benefits
   Life & Disability Insurance
   Tuition, Training & Certification Reimbursement
   401K Contribution
   Employee Referral Bonus Program
   Equipment Reimbursement
   Team Engagement & Outings
   Swag
 
  …And more!
  Learn more about GliaCell Technologies: https://gliacelltechnologies.applytojob.com/apply/
  To apply for this position, respond to this job posting and attach an updated resume for us to review.
  GliaCell Technologies, LLC is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.
  
 6J9dAEv78l","<div>
 <p><b>Are you a Senior Data Scientist who is ready for a new challenge that will launch your career to the next level?</b></p>
 <ul>
  <li> Tired of being treated like a company drone?</li>
  <li> Tired of promised adventures during the hiring phase, then dropped off on a remote contract and never seen or heard from the mothership again?</li>
  <li> Our engineers were certainly tired of the same.</li>
 </ul>
 <p><b> At GliaCell our slogan is &#x201c;We make It happen&#x201d;.</b></p>
 <ul>
  <li> We will immerse you in the latest technologies.</li>
  <li> We will develop and support your own personalized training program to continue your individual growth.</li>
  <li> We will provide you with work that matters with our mission focused customers, and surround you with a family of brilliant engineers.<b> </b></li>
 </ul>
 <p><b>Culture isn&#x2019;t something you need to talk about&#x2026;if it just exists.</b></p>
 <p><b> If this sounds interesting to you, then we&#x2019;d like to have a discussion regarding your next adventure! If you want to be a drone, this isn&#x2019;t the place for you.</b></p>
 <p><b> We Make It Happen!</b></p>
 <p><i> GliaCell Technologies focuses on Software &amp; System Engineering in Enterprise and Cyber Security solution spaces. We excel at delivering stable and reliable software solutions using Agile Software Development principles. These provide us the capability to deliver a quick turn-around using interactive applications and the integration of industry standard software stacks. </i></p>
 <p><i>GliaCell&#x2019;s Enterprise capabilities include Full-Stack Application Development, Big Data, Cloud Technologies, Analytics, Machine Learning, AI, and DevOps Containerization. We also provide customer solutions in the areas of CND, CNE, and CNO by providing our customers with assessments and solutions in Threat Mitigation, Vulnerability Exposure, Penetration Testing, Threat Hunting, and Preventing Advanced Persistent Threat.</i></p>
 <p><b> We Offer:</b></p>
 <ul>
  <li> Long term job security</li>
  <li> Competitive salaries &amp; bonus opportunities</li>
  <li> Challenging work you are passionate about</li>
  <li> Ability to work with some amazingly talented people</li>
 </ul>
 <p><b> Job Description:</b></p>
 <p> GliaCell is seeking a <b>Senior Data Scientist </b>on one of our subcontracts. This is a full-time position offering the opportunity to support a U.S. Government customer. The mission is to provide technical expertise that assists in sustaining critical mission-related software and systems to a large government contract.</p>
 <p><b> Key Requirements:</b></p>
 <p> To be considered for this position you must have the following:</p>
 <ul>
  <li><b> Possess an active or rein-statable TS/SCI with Polygraph security clearance.</b></li>
  <li> Be a U.S. Citizen.</li>
  <li> Bachelor of Science Degree in Computer Science (or a related subject) (4 years of work experience can be substituted for the degree).</li>
  <li> 14+ years of experience.</li>
  <li> Demonstrated experience with Python and/or Java programming languages.</li>
  <li> Bash scripting.</li>
  <li> Familiarity with Machine Learning/Artificial Intelligence (professional or personal research).</li>
  <li> Cloud development experience.</li>
  <li> Enterprise development experience.</li>
  <li> Works well independently as well as on a team.</li>
  <li> Strong communication skills.</li>
  <li> Flexible and motivated.</li>
 </ul>
 <p><b> Desired:</b></p>
 <ul>
  <li> Experience responding to user issues</li>
  <li> Cloud developer certification (Hadoop, Cloudera, other)</li>
  <li> Jupyter Notebook experience</li>
  <li> Data science techniques</li>
  <li> Natural language processing experience</li>
  <li> Experience with at-scale development and cloud programming (Spark, Map Reduce, etc)</li>
  <li> Docker experience</li>
  <li> Kubernetes experience</li>
  <li> GPU application experience</li>
  <li> Strong interpersonal and organizational skills</li>
  <li> Ability to work in a dynamically changing environment</li>
 </ul>
 <p><b> Location:</b> Annapolis Junction, Maryland / Partial Telework</p>
 <p><b> Salary:</b> Based on Education, Years of Experience, Skill, and Abilities</p>
 <p><b> Check Out Our Benefits:</b></p>
 <ul>
  <li> Paid Time Off</li>
  <li> Medical, Dental &amp; Vision Benefits</li>
  <li> Life &amp; Disability Insurance</li>
  <li> Tuition, Training &amp; Certification Reimbursement</li>
  <li> 401K Contribution</li>
  <li> Employee Referral Bonus Program</li>
  <li> Equipment Reimbursement</li>
  <li> Team Engagement &amp; Outings</li>
  <li> Swag</li>
 </ul>
 <p> &#x2026;And more!</p>
 <p> Learn more about GliaCell Technologies: https://gliacelltechnologies.applytojob.com/apply/</p>
 <p><b> To apply for this position, respond to this job posting and attach an updated resume for us to review.</b></p>
 <p> GliaCell Technologies, LLC is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status.</p>
 <p> </p>
 <p>6J9dAEv78l</p>
</div>","https://gliacelltechnologies.applytojob.com/apply/6J9dAEv78l/Senior-Data-Scientist-Software-Engineer-Partial-Telework?source=INDE","010379e5014e90b3",,"Full-time",,"Annapolis Junction, MD","Senior Data Scientist / Software Engineer - Partial Telework","11 days ago","2023-10-14T11:51:54.648Z",,,,"2023-10-25T11:51:54.651Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=010379e5014e90b3&from=jasx&tk=1hdjaos832ci6000&vjs=3"
"State of North Carolina","JOB 
This is a repost -applicants will need to reapply to be considered. Pay Grade: DT09 Recruitment Range: $75,814.00 to $91,908.00.This position has flexibility and can be full-time telework, hybrid or on-site at the headquarters building located in Raleigh. The North Carolina Department of Revenue is seeking a Data Engineer Administrator. This position will work with a team of IT professionals to support customer business intelligence and data analysis initiatives through the design, development, and implementation of software applications, packages, and components focused on data warehousing and SAS. The work to be performed is identified and prioritized by the Department’s business divisions and is implemented using structured project and software development lifecycle methodologies within the ITIL framework. This position will serve as a data warehouse engineer and will support and perform application development in a diverse environment of applications that include SAS, SQL, PL/SQL, JAVA, and UNIX/LINUX scripting. The position will participate in end-to-end customer requirements gathering, development and implementation, including identifying data analysis needs as well as processing and exploring system data. The position will translate business requirements into analytic methods, build analytics models, develop reports, and present reports to customers. The position will be responsible for automation and conducting code reviews. The position will create and execute test scripts and document findings. The position will provide daily operational support to customers and other IT initiatives as needed. The individual selected for this position must have the ability to maintain ongoing interaction with internal and external audiences using strong written and verbal communication skills and serve as a resource for others. The Data Engineer/Administrator must be able to recognize the limitations of business information systems in relation to business processes; exercise creative and critical thinking in evaluating situations and developing solutions; and maintain a high level of technical skill or knowledge in the area of application development and system integration.COMPENSATION & BENEFITS: The state of North Carolina offers excellent comprehensive benefits. Employees can participate in health insurance options, standard and supplemental retirement plans, and the NCFlex program (numerous high-quality, low-cost benefits on a pre-tax basis). Employees also receive paid vacation, sick, and community service leave. In addition, paid parental leave is available to eligible employees. Some highlights include: The best funded pension plan/retirement system in the nation according to Moody’s Investor’s Service Twelve (12) holidays/year Fourteen (14) vacation days/year which increase as length of service increases and accumulate year-to-year Twelve (12) sick days/year which are cumulative indefinitelyLongevity pay lump sum payout yearly based on length of service 401K, 457, and 403(b) plans Learn more about employee perks/benefits: Why Work For NC NC OSHR: Benefits NC OSHR: Total Compensation Calculator 

 EXAMPLE OF DUTIES 
Experience programming languages in SAS and SQLDemonstrated administration skills on SAS environmentsExperience extracting, transforming, and loading (ETL) dataExperience and expertise with Linux/Unix scriptingDemonstrated experience in application design, software development, testing, and working with systems processing event-based logs, user logs, and debuggingThorough knowledge and understanding of business systems theories, processes, rules and regulations and how they apply to technology in the applicable area(s)Management Preferences: Experience with SAS and performing quantitative analysis using SAS business intelligence toolsDevelopment and support experience in Oracle, PL/SQL, and JAVAApplication server support experience in Unix/LinuxAbilities to write queries and extract data from a database 

 SUPPLEMENTAL INFORMATION 
The Department of Revenue is interested in every qualified candidate who is eligible to work in the United States. However, we are not able to sponsor visas at this time. Selected candidates will be required to complete a USCIS Form I-9 and will be processed through E-Verify. The Department of Revenue seeks to fill positions with the most qualified individuals in its effort to provide taxpayers with the most efficient and effective services possible. This position is subject to federal and state criminal background checks that may include fingerprinting and verification of tax compliance. ""Tax compliance"" is defined as having filed and paid all North Carolina State taxes owed each year leading up to the current calendar year or currently in a non-delinquent payment status with the State of North Carolina on taxes that are currently owed to the state. If selected for an interview, you may have the option to interview via telephone and/or other method as technology allows. To be considered within the most qualified pool of applicants and receive credit for your work history and credentials, you must document all related education and experience on the State of North Carolina application in the appropriate sections of the application form. Any information omitted from the application cannot be considered for qualifying credit. NC DOR welcomes attached resumes, cover letters and reference information, but these items will not be used for screening for qualifying credit. Please make sure the application is completed in full. ""See Resume"" or ""See Attachment"" will NOT be accepted. Applicants eligible for veteran's preference should attach a copy of form DD-214. If you are having technical issues logging into your account or applying for a position please review the Get Help/FAQ's information on the website. If you are still experiencing technical issues with your application, please call the NeoGov Help Line at 855-524-5627. If you have general questions about the application process, you may contact Human Resources at 919-814-1200 or HumanResources@ncdor.gov. Individuals with disabilities requiring disability-related accommodations in the interview process, please call the agency ADA Administrator at 919-814-1172. NC Department of Revenue Human Resources Division 919-814-1200","JOB 
<br>This is a repost -applicants will need to reapply to be considered. Pay Grade: DT09 Recruitment Range: &#x24;75,814.00 to &#x24;91,908.00.This position has flexibility and can be full-time telework, hybrid or on-site at the headquarters building located in Raleigh. The North Carolina Department of Revenue is seeking a Data Engineer Administrator. This position will work with a team of IT professionals to support customer business intelligence and data analysis initiatives through the design, development, and implementation of software applications, packages, and components focused on data warehousing and SAS. The work to be performed is identified and prioritized by the Department&#x2019;s business divisions and is implemented using structured project and software development lifecycle methodologies within the ITIL framework. This position will serve as a data warehouse engineer and will support and perform application development in a diverse environment of applications that include SAS, SQL, PL/SQL, JAVA, and UNIX/LINUX scripting. The position will participate in end-to-end customer requirements gathering, development and implementation, including identifying data analysis needs as well as processing and exploring system data. The position will translate business requirements into analytic methods, build analytics models, develop reports, and present reports to customers. The position will be responsible for automation and conducting code reviews. The position will create and execute test scripts and document findings. The position will provide daily operational support to customers and other IT initiatives as needed. The individual selected for this position must have the ability to maintain ongoing interaction with internal and external audiences using strong written and verbal communication skills and serve as a resource for others. The Data Engineer/Administrator must be able to recognize the limitations of business information systems in relation to business processes; exercise creative and critical thinking in evaluating situations and developing solutions; and maintain a high level of technical skill or knowledge in the area of application development and system integration.COMPENSATION &amp; BENEFITS: The state of North Carolina offers excellent comprehensive benefits. Employees can participate in health insurance options, standard and supplemental retirement plans, and the NCFlex program (numerous high-quality, low-cost benefits on a pre-tax basis). Employees also receive paid vacation, sick, and community service leave. In addition, paid parental leave is available to eligible employees. Some highlights include: The best funded pension plan/retirement system in the nation according to Moody&#x2019;s Investor&#x2019;s Service Twelve (12) holidays/year Fourteen (14) vacation days/year which increase as length of service increases and accumulate year-to-year Twelve (12) sick days/year which are cumulative indefinitelyLongevity pay lump sum payout yearly based on length of service 401K, 457, and 403(b) plans Learn more about employee perks/benefits: Why Work For NC NC OSHR: Benefits NC OSHR: Total Compensation Calculator 
<br>
<br> EXAMPLE OF DUTIES 
<br>Experience programming languages in SAS and SQLDemonstrated administration skills on SAS environmentsExperience extracting, transforming, and loading (ETL) dataExperience and expertise with Linux/Unix scriptingDemonstrated experience in application design, software development, testing, and working with systems processing event-based logs, user logs, and debuggingThorough knowledge and understanding of business systems theories, processes, rules and regulations and how they apply to technology in the applicable area(s)Management Preferences: Experience with SAS and performing quantitative analysis using SAS business intelligence toolsDevelopment and support experience in Oracle, PL/SQL, and JAVAApplication server support experience in Unix/LinuxAbilities to write queries and extract data from a database 
<br>
<br> SUPPLEMENTAL INFORMATION 
<br>The Department of Revenue is interested in every qualified candidate who is eligible to work in the United States. However, we are not able to sponsor visas at this time. Selected candidates will be required to complete a USCIS Form I-9 and will be processed through E-Verify. The Department of Revenue seeks to fill positions with the most qualified individuals in its effort to provide taxpayers with the most efficient and effective services possible. This position is subject to federal and state criminal background checks that may include fingerprinting and verification of tax compliance. &quot;Tax compliance&quot; is defined as having filed and paid all North Carolina State taxes owed each year leading up to the current calendar year or currently in a non-delinquent payment status with the State of North Carolina on taxes that are currently owed to the state. If selected for an interview, you may have the option to interview via telephone and/or other method as technology allows. To be considered within the most qualified pool of applicants and receive credit for your work history and credentials, you must document all related education and experience on the State of North Carolina application in the appropriate sections of the application form. Any information omitted from the application cannot be considered for qualifying credit. NC DOR welcomes attached resumes, cover letters and reference information, but these items will not be used for screening for qualifying credit. Please make sure the application is completed in full. &quot;See Resume&quot; or &quot;See Attachment&quot; will NOT be accepted. Applicants eligible for veteran&apos;s preference should attach a copy of form DD-214. If you are having technical issues logging into your account or applying for a position please review the Get Help/FAQ&apos;s information on the website. If you are still experiencing technical issues with your application, please call the NeoGov Help Line at 855-524-5627. If you have general questions about the application process, you may contact Human Resources at 919-814-1200 or HumanResources@ncdor.gov. Individuals with disabilities requiring disability-related accommodations in the interview process, please call the agency ADA Administrator at 919-814-1172. NC Department of Revenue Human Resources Division 919-814-1200","https://www.governmentjobs.com/jobs/4088287/data-engineer-administrator?utm_source=Indeed&utm_medium=organic&utm_campaign=Indeed","61db1f6117c78171",,"Full-time",,"Salisbury, NC 28145","Data Engineer Administrator","8 days ago","2023-10-17T11:51:57.735Z","3.4","363",,"2023-10-25T11:51:57.736Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=61db1f6117c78171&from=jasx&tk=1hdjaos832ci6000&vjs=3"
"Cognizant Technology Solutions","We are Cognizant Artificial Intelligence 
  Digital technologies, including analytics and AI, give companies a once-in-a-generation opportunity to perform orders of magnitude better than ever before. But clients need new business models built from analyzing customers and business operations at every angle to really understand them. 
  With the power to apply artificial intelligence and data science to business decisions via enterprise data management solutions, we help leading companies prototype, refine, validate and scale the most desirable products and delivery models to enterprise scale within weeks
  
  
  Role and Responsibilities: 
  
  5+ years of industry experience in software development data engineering or a related field with a solid track record of building services for manipulating processing datasets 
  Hands-on experience and advanced knowledge of AWS DataOps (i.e. IAM Lambda Step Functions EMR/Glue and DynamoDB) 
  Hands-on experience and advanced knowledge of SQL/Non-relational Data Modeling 
  Experience working with data streaming technologies (Kafka Spark Streaming etc.) 
 
 
  Designing and implementing complex ingestion and processing pipelines through orchestration 
  Design and implement API interfaces for engineering teams to interact with ingestion/processing pipelines 
  Design implement and support scalable multi-tenant service and data infrastructure solutions to integrate with multi heterogeneous data sources aggregate and retrieve data in a fast and secure mode curate data that can be used in reporting analysis machine learning models and ad-hoc data requests 
  Interface with other engineering and ML teams to extract transform and load data from a wide variety of data sources 
  Work with business product owners to understand gather and analyze their processing and extraction needs to solve problems
 
  
  
  Salary and Other Compensation 
  The annual salary for this position is between USD ($110kp/a – $120kp/a) depending on experience and other qualifications of the successful candidate. 
  This position is also eligible for Cognizant’s discretionary annual incentive program, based on performance and subject to the terms of Cognizant’s applicable plans. 
  Benefits: Cognizant offers the following benefits for this position, subject to applicable eligibility requirements: 
  
  Medical/Dental/Vision/Life Insurance 
  Paid holidays plus Paid Time Off 
  401(k) plan and contributions 
  Long-term/Short-term Disability 
  Paid Parental Leave 
  Employee Stock Purchase Plan 
  
 Disclaimer: The salary, other compensation, and benefits information is accurate as of the date of this posting. Cognizant reserves the right to modify this information at any time, subject to applicable law.
  
  
  #LI-JL1 
  #CB 
  #IND123
 
  Employee Status : Full Time Employee
  Shift : Day Job
  Travel : No
  Job Posting : Oct 17 2023
 
 
   About Cognizant
  Cognizant (Nasdaq-100: CTSH) is one of the world's leading professional services companies, transforming clients' business, operating and technology models for the digital era. Our unique industry-based, consultative approach helps clients envision, build and run more innovative and efficient businesses. Headquartered in the U.S., Cognizant is ranked 185 on the Fortune 500 and is consistently listed among the most admired companies in the world. Learn how Cognizant helps clients lead with digital at www.cognizant.com or follow us @USJobsCognizant.
 
  Applicants may be required to attend interviews in person or by video conference. In addition, candidates may be required to present their current state or government issued ID during each interview.
 
  Cognizant is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected Veteran status, age, or any other characteristic protected by law.
  If you have a disability that requires a reasonable accommodation to search for a job opening or submit an application, please email CareersNA2@cognizant.com with your request and contact information.","<div>
 <p><b>We are Cognizant Artificial Intelligence</b></p> 
 <p> Digital technologies, including analytics and AI, give companies a once-in-a-generation opportunity to perform orders of magnitude better than ever before. But clients need new business models built from analyzing customers and business operations at every angle to really understand them.</p> 
 <p> With the power to apply artificial intelligence and data science to business decisions via enterprise data management solutions, we help leading companies prototype, refine, validate and scale the most desirable products and delivery models to enterprise scale within weeks</p>
 <br> 
 <p></p> 
 <p><b> Role and Responsibilities:</b></p> 
 <ul> 
  <li>5+ years of industry experience in software development data engineering or a related field with a solid track record of building services for manipulating processing datasets</li> 
  <li>Hands-on experience and advanced knowledge of AWS DataOps (i.e. IAM Lambda Step Functions EMR/Glue and DynamoDB)</li> 
  <li>Hands-on experience and advanced knowledge of SQL/Non-relational Data Modeling</li> 
  <li>Experience working with data streaming technologies (Kafka Spark Streaming etc.)</li> 
 </ul>
 <ul>
  <li>Designing and implementing complex ingestion and processing pipelines through orchestration</li> 
  <li>Design and implement API interfaces for engineering teams to interact with ingestion/processing pipelines</li> 
  <li>Design implement and support scalable multi-tenant service and data infrastructure solutions to integrate with multi heterogeneous data sources aggregate and retrieve data in a fast and secure mode curate data that can be used in reporting analysis machine learning models and ad-hoc data requests</li> 
  <li>Interface with other engineering and ML teams to extract transform and load data from a wide variety of data sources</li> 
  <li>Work with business product owners to understand gather and analyze their processing and extraction needs to solve problems</li>
 </ul>
 <br> 
 <p></p> 
 <p><b> Salary and Other Compensation</b></p> 
 <p> The annual salary for this position is between USD (&#x24;110kp/a &#x2013; &#x24;120kp/a) depending on experience and other qualifications of the successful candidate.</p> 
 <p> This position is also eligible for Cognizant&#x2019;s discretionary annual incentive program, based on performance and subject to the terms of Cognizant&#x2019;s applicable plans.</p> 
 <p> Benefits: Cognizant offers the following benefits for this position, subject to applicable eligibility requirements:</p> 
 <ul> 
  <li>Medical/Dental/Vision/Life Insurance</li> 
  <li>Paid holidays plus Paid Time Off</li> 
  <li>401(k) plan and contributions</li> 
  <li>Long-term/Short-term Disability</li> 
  <li>Paid Parental Leave</li> 
  <li>Employee Stock Purchase Plan</li> 
 </ul> 
 <p>Disclaimer: The salary, other compensation, and benefits information is accurate as of the date of this posting. Cognizant reserves the right to modify this information at any time, subject to applicable law.</p>
 <br> 
 <p></p> 
 <p> #LI-JL1</p> 
 <p> #CB</p> 
 <p> #IND123</p>
 <p></p>
 <p><b><br> Employee Status : </b>Full Time Employee</p>
 <p><b> Shift : </b>Day Job</p>
 <p><b> Travel : </b>No</p>
 <p><b> Job Posting : </b>Oct 17 2023</p>
 <p></p>
 <div>
  <b> About Cognizant</b>
 </div> Cognizant (Nasdaq-100: CTSH) is one of the world&apos;s leading professional services companies, transforming clients&apos; business, operating and technology models for the digital era. Our unique industry-based, consultative approach helps clients envision, build and run more innovative and efficient businesses. Headquartered in the U.S., Cognizant is ranked 185 on the Fortune 500 and is consistently listed among the most admired companies in the world. Learn how Cognizant helps clients lead with digital at www.cognizant.com or follow us @USJobsCognizant.
 <p></p>
 <p> Applicants may be required to attend interviews in person or by video conference. In addition, candidates may be required to present their current state or government issued ID during each interview.</p>
 <p></p>
 <p> Cognizant is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected Veteran status, age, or any other characteristic protected by law.</p>
 <p> If you have a disability that requires a reasonable accommodation to search for a job opening or submit an application, please email CareersNA2@cognizant.com with your request and contact information.</p>
</div>
<p></p>","https://click.appcast.io/track/hra6v3l-org?cs=hqw&jg=6mnw&ittk=UYLGOWHZML","8c2d8bd464c9e1f8",,"Full-time",,"Chicago, IL 60290","Sr. AWS Data Engineer (Remote)","7 days ago","2023-10-18T11:52:00.047Z","3.9","15982","$110,000 - $120,000 a year","2023-10-25T11:52:00.050Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=8c2d8bd464c9e1f8&from=jasx&tk=1hdjaos832ci6000&vjs=3"
"Tiger Analytics","Tiger Analytics is a global AI and analytics consulting firm. With data and technology at the core of our solutions, we are solving problems that eventually impact the lives of millions globally. Our culture is modeled around expertise and respect with a team-first mindset. Headquartered in Silicon Valley, you’ll find our delivery centers across the globe and offices in multiple cities across India, the US, UK, Canada, and Singapore, including a substantial remote global workforce.
  We’re Great Place to Work-Certified™. Working at Tiger Analytics, you’ll be at the heart of an AI revolution. You’ll work with teams that push the boundaries of what is possible and build solutions that energize and inspire.
 
  Requirements
  Curious about the role? What your typical day would look like?
  As a Principal Data Engineer (Azure), you would have hands on experience working on Azure as cloud, Databricks and some exposure/experience on Data Modelling. You will build and learn about a variety of analytics solutions & platforms, data lakes, modern data platforms, data fabric solutions, etc. using different Open Source, Big Data, and Cloud technologies on Microsoft Azure. 
 
  Design and build scalable & metadata-driven data ingestion pipelines (For Batch and Streaming Datasets)
   Conceptualize and execute high-performance data processing for structured and unstructured data, and data 
 harmonization
 
  Schedule, orchestrate, and validate pipelines
  Design exception handling and log monitoring for debugging
  Ideate with your peers to make tech stack and tools-related decisions
  Interact and collaborate with multiple teams (Consulting/Data Science & App Dev) and various stakeholders to meet deadlines, to bring Analytical Solutions to life.
 
  What do we expect?
 
  Experience in implementing Data Lake with technologies like Azure Data Factory (ADF), PySpark, Databricks, ADLS,
 
  Azure SQL Database
 
  A comprehensive foundation with working knowledge of Azure Synapse Analytics, Event Hub & Streaming 
 Analytics, Cosmos DB, and Purview
 
  A passion for writing high-quality code and the code should be modular, scalable, and free of bugs (debugging 
 skills in SQL, Python, or Scala/Java).
 
  Enthuse to collaborate with various stakeholders across the organization and take complete ownership of 
 deliverables.
 
  Experience in using big data technologies like Hadoop, Spark, Airflow, NiFi, Kafka, Hive, Neo4J, Elastic Search
  Adept understanding of different file formats like Delta Lake, Avro, Parquet, JSON, and CSV
  Good knowledge of building and designing REST APIs with real-time experience working on Data Lake or 
 Lakehouse projects.
 
  Experience in supporting BI and Data Science teams in consuming the data in a secure and governed manner
  Certifications like Data Engineering on Microsoft Azure (DP-203) or Databricks Certified Developer (DE) are 
 valuable addition.
  Note: The designation will be commensurate with expertise and experience. Compensation packages are among the best in the industry.
  Job Requirement
  
  Mandatory: Azure Data Factory (ADF), PySpark, Databricks, ADLS, Azure SQL Database 
  Optional: Azure Synapse Analytics, Event Hub & Streaming Analytics, Cosmos DB and Purview. 
  Strong programming, unit testing & debugging skills in SQL, Python or Scala/Java. 
  Some experience of using big data technologies like Hadoop, Spark, Airflow, NiFi, Kafka, Hive, Neo4J, Elastic  Search. 
  Good Understanding of different file formats like Delta Lake, Avro, Parquet, JSON and CSV. 
  Experience of working in Agile projects and following DevOps processes with technologies like Git, Jenkins & Azure DevOps. 
  Good to have: 
  Experience of working on Data Lake & Lakehouse projects 
  Experience of building REST services and implementing service-oriented architectures. 
  Experience of supporting BI and Data Science teams in consuming the data in a secure and governed manner. 
  Certifications like Data Engineering on Microsoft Azure (DP-203) or Databricks Certified Developer (DE) 
 
 Benefits
  This position offers an excellent opportunity for significant career development in a fast-growing and challenging entrepreneurial environment with a high degree of individual responsibility.","<div>
 <p>Tiger Analytics is a global AI and analytics consulting firm. With data and technology at the core of our solutions, we are solving problems that eventually impact the lives of millions globally. Our culture is modeled around expertise and respect with a team-first mindset. Headquartered in Silicon Valley, you&#x2019;ll find our delivery centers across the globe and offices in multiple cities across India, the US, UK, Canada, and Singapore, including a<br> substantial remote global workforce.</p>
 <p> We&#x2019;re Great Place to Work-Certified&#x2122;. Working at Tiger Analytics, you&#x2019;ll be at the heart of an AI revolution. You&#x2019;ll work with teams that push the boundaries of what is possible and build solutions that energize and inspire.</p>
 <p></p>
 <p><b><br> Requirements</b></p>
 <p> Curious about the role? What your typical day would look like?</p>
 <p><br> As a Principal Data Engineer (Azure), you would have hands on experience working on Azure as cloud, Databricks and some exposure/experience on Data Modelling. You will build and learn about a variety of analytics solutions &amp; platforms, data lakes, modern data platforms, data fabric solutions, etc. using different Open Source, Big Data, and Cloud technologies on Microsoft Azure. </p>
 <ul>
  <li>Design and build scalable &amp; metadata-driven data ingestion pipelines (For Batch and Streaming Datasets)</li>
  <li><br> Conceptualize and execute high-performance data processing for structured and unstructured data, and data </li>
 </ul>harmonization
 <ul>
  <li>Schedule, orchestrate, and validate pipelines</li>
  <li>Design exception handling and log monitoring for debugging</li>
  <li>Ideate with your peers to make tech stack and tools-related decisions</li>
  <li>Interact and collaborate with multiple teams (Consulting/Data Science &amp; App Dev) and various stakeholders to meet deadlines, to bring Analytical Solutions to life.</li>
 </ul>
 <p> What do we expect?</p>
 <ul>
  <li>Experience in implementing Data Lake with technologies like Azure Data Factory (ADF), PySpark, Databricks, ADLS,</li>
 </ul>
 <p><br> Azure SQL Database</p>
 <ul>
  <li>A comprehensive foundation with working knowledge of Azure Synapse Analytics, Event Hub &amp; Streaming </li>
 </ul>Analytics, Cosmos DB, and Purview
 <ul>
  <li>A passion for writing high-quality code and the code should be modular, scalable, and free of bugs (debugging </li>
 </ul>skills in SQL, Python, or Scala/Java).
 <ul>
  <li>Enthuse to collaborate with various stakeholders across the organization and take complete ownership of </li>
 </ul>deliverables.
 <ul>
  <li>Experience in using big data technologies like Hadoop, Spark, Airflow, NiFi, Kafka, Hive, Neo4J, Elastic Search</li>
  <li>Adept understanding of different file formats like Delta Lake, Avro, Parquet, JSON, and CSV</li>
  <li>Good knowledge of building and designing REST APIs with real-time experience working on Data Lake or </li>
 </ul>Lakehouse projects.
 <ul>
  <li>Experience in supporting BI and Data Science teams in consuming the data in a secure and governed manner</li>
  <li>Certifications like Data Engineering on Microsoft Azure (DP-203) or Databricks Certified Developer (DE) are </li>
 </ul>valuable addition.
 <p> Note: The designation will be commensurate with expertise and experience. Compensation packages are among the best in the industry.</p>
 <p><br> Job Requirement</p>
 <ul> 
  <li>Mandatory: Azure Data Factory (ADF), PySpark, Databricks, ADLS, Azure SQL Database</li> 
  <li>Optional: Azure Synapse Analytics, Event Hub &amp; Streaming Analytics, Cosmos DB and Purview.</li> 
  <li>Strong programming, unit testing &amp; debugging skills in SQL, Python or Scala/Java.</li> 
  <li>Some experience of using big data technologies like Hadoop, Spark, Airflow, NiFi, Kafka, Hive, Neo4J, Elastic <br> Search.</li> 
  <li>Good Understanding of different file formats like Delta Lake, Avro, Parquet, JSON and CSV.</li> 
  <li>Experience of working in Agile projects and following DevOps processes with technologies like Git, Jenkins &amp; Azure DevOps.</li> 
  <li>Good to have:</li> 
  <li>Experience of working on Data Lake &amp; Lakehouse projects</li> 
  <li>Experience of building REST services and implementing service-oriented architectures.</li> 
  <li>Experience of supporting BI and Data Science teams in consuming the data in a secure and governed manner.</li> 
  <li>Certifications like Data Engineering on Microsoft Azure (DP-203) or Databricks Certified Developer (DE)</li> 
 </ul>
 <p><b>Benefits</b></p>
 <p> This position offers an excellent opportunity for significant career development in a fast-growing and challenging entrepreneurial environment with a high degree of individual responsibility.</p>
</div>","https://apply.workable.com/tiger-analytics/j/E49C630B4F","e70f4c56bc14b1be",,"Full-time",,"Jersey City, NJ","Principal Data Engineer (Azure)","11 days ago","2023-10-14T11:52:03.789Z",,,,"2023-10-25T11:52:03.793Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=e70f4c56bc14b1be&from=jasx&tk=1hdjaos832ci6000&vjs=3"
"Bamboo Health","Bamboo Health is a leader in cloud-based care coordination software and analytics solutions focused on patients with complex needs, including those suffering from physical health and mental health issues and substance use disorders. We deliver on a mission of enabling better care for patients across the continuum, and our software solutions help healthcare professionals collaborate on shared patients across the spectrum of care. 
  Summary: 
  This position (Senior Data Engineer) is on the Bamboo Health Engineering Team. You will be working specifically within Data Warehousing and reporting areas. You will be hands-on with PostgreSQL, Redshift, Kubernetes, and other AWS technologies and you will be working within a team that builds, designs, and supports cloud based ETL, containerized solutions and data warehouses that enable data reporting, analytics, and data science. Our Data Engineering team is continuously looking for opportunities to automate, apply new AWS products and features, while striving for continuous technological modernization. You will partner with other Engineering team members, Architects, Data Scientists, Analysts, and other Bamboo Health professionals. 
  What You’ll Do: 
  
  Develop, debug and support ETL processes utilizing AWS services 
  Partner in ideation and development of data models used for data science and analytics 
  Meet the delivery expectations of the Agile Project Management methodology (1-week Data Engineering Sprint cycles) 
  Maintain reports and extracts that serve as lifesaving information sources to customers 
  Write clear and concise documentation regarding technical solutions, while sharing knowledge and documentation with teammates via “Lunch and Learns” 
  Work with internal and external customers to prove requirements are met 
  
 What Success Looks Like… 
  In 3 months… 
  
  Become familiar with the basic architecture and processes of Bamboo Health services, especially Data Warehouses and reporting products. 
  Understand the roadmap of Data Engineering projects and how your efforts contribute. 
  Start to contribute fixes and small improvements (EX: Query tuning). 
  Participate in troubleshooting and resolution of production ETL and reporting issues/outages. 
  
 In 6 months… 
  
  Independently audit and repair ETL and rerun reporting as needed. 
  Troubleshoot Kubernetes deployments and other operational processes with emphasis on ETL and reporting. 
  Develop complex queries, scripts that utilize AWS infrastructure and automation for reporting. 
  Understand the securities strategies, grant, and remove access to Data Engineering systems and reports, and follow AWS Infrastructure access patterns needed for new deployments. 
  
 In 12 months… 
  
  Represent Data Engineering in new ETL and data reporting discussions providing technical solution recommendations within project roadmaps. 
  Contribute novel coding patterns and capabilities from emerging API’s and SDK’s. 
  Work with architects to identify and correct missing product components, deploy new ETL or reporting products and help deploy dependencies and processes. 
  
 What You Need: 
  
  Bachelor’s degree in Computer Science, Analytics, a related field, or equivalent experience 
  5+ years total software and relational database development experience. 
  3 years with a strong demonstrated ability to develop and maintain ETL solutions, ideally using Python and various Application Programming Interfaces (API) 
  3+ years’ experience with AWS Cloud Solutions/Services 
  Ability to work in an Agile environment include using ticketing software such as JIRA 
  Strong technical problem-solving abilities 
  Hands on experience maintaining databases on Redshift, PostgreSQL, MySQL or Oracle relational database systems. 
  Experience with software development using Python, Ruby, or other modern scripting languages, ideally in container solutions such as Docker or Kubernetes. 
  Experience with Apache Airflow, dbt Core, or similar workflow tool is a plus. 
  
 What You Get: 
  
  Join one of the fastest growing health IT companies in the country 
  Have the autonomy to build something with an enthusiastically supportive team 
  Learn from working at the highest levels and on the most strategic priorities of the company, including from world class investors and advisors 
  Receive competitive compensation, including equity, with health, dental, vision and other benefits 
  
 Belonging at Bamboo 
  We Care. #BambooHealthValuesCare 
  Every human being has the right to the best possible healthcare. Our solutions enable healthcare professionals to see and treat every individual as a whole person by providing the right information, at the right time – regardless of physical, behavioral, or social barriers. 
  We’re a great place to work because we care. We continually seek to learn about our differences and ensure the unique identities and contributions of all employees are welcome, valued and celebrated. 
  Our commitment to making a positive impact starts by recognizing and leveraging our differences, building inclusive teams, cultivating a sense of belonging, combating biases, and actively removing barriers to equity. 
  Bamboo Health is proud to be an Equal Employment Opportunity and affirmative action employer.","<div>
 <p>Bamboo Health is a leader in cloud-based care coordination software and analytics solutions focused on patients with complex needs, including those suffering from physical health and mental health issues and substance use disorders. We deliver on a mission of enabling better care for patients across the continuum, and our software solutions help healthcare professionals collaborate on shared patients across the spectrum of care.</p> 
 <p><b> Summary:</b></p> 
 <p> This position (Senior Data Engineer) is on the Bamboo Health Engineering Team. You will be working specifically within Data Warehousing and reporting areas. You will be hands-on with PostgreSQL, Redshift, Kubernetes, and other AWS technologies and you will be working within a team that builds, designs, and supports cloud based ETL, containerized solutions and data warehouses that enable data reporting, analytics, and data science. Our Data Engineering team is continuously looking for opportunities to automate, apply new AWS products and features, while striving for continuous technological modernization. You will partner with other Engineering team members, Architects, Data Scientists, Analysts, and other Bamboo Health professionals.</p> 
 <p><b> What You&#x2019;ll Do:</b></p> 
 <ul> 
  <li>Develop, debug and support ETL processes utilizing AWS services</li> 
  <li>Partner in ideation and development of data models used for data science and analytics</li> 
  <li>Meet the delivery expectations of the Agile Project Management methodology (1-week Data Engineering Sprint cycles)</li> 
  <li>Maintain reports and extracts that serve as lifesaving information sources to customers</li> 
  <li>Write clear and concise documentation regarding technical solutions, while sharing knowledge and documentation with teammates via &#x201c;Lunch and Learns&#x201d;</li> 
  <li>Work with internal and external customers to prove requirements are met</li> 
 </ul> 
 <p><b>What Success Looks Like&#x2026;</b></p> 
 <p><b> In 3 months&#x2026;</b></p> 
 <ul> 
  <li>Become familiar with the basic architecture and processes of Bamboo Health services, especially Data Warehouses and reporting products.</li> 
  <li>Understand the roadmap of Data Engineering projects and how your efforts contribute.</li> 
  <li>Start to contribute fixes and small improvements (EX: Query tuning).</li> 
  <li>Participate in troubleshooting and resolution of production ETL and reporting issues/outages.</li> 
 </ul> 
 <p><b>In 6 months&#x2026;</b></p> 
 <ul> 
  <li>Independently audit and repair ETL and rerun reporting as needed.</li> 
  <li>Troubleshoot Kubernetes deployments and other operational processes with emphasis on ETL and reporting.</li> 
  <li>Develop complex queries, scripts that utilize AWS infrastructure and automation for reporting.</li> 
  <li>Understand the securities strategies, grant, and remove access to Data Engineering systems and reports, and follow AWS Infrastructure access patterns needed for new deployments.</li> 
 </ul> 
 <p><b>In 12 months&#x2026;</b></p> 
 <ul> 
  <li>Represent Data Engineering in new ETL and data reporting discussions providing technical solution recommendations within project roadmaps.</li> 
  <li>Contribute novel coding patterns and capabilities from emerging API&#x2019;s and SDK&#x2019;s.</li> 
  <li>Work with architects to identify and correct missing product components, deploy new ETL or reporting products and help deploy dependencies and processes.</li> 
 </ul> 
 <p><b>What You Need:</b></p> 
 <ul> 
  <li>Bachelor&#x2019;s degree in Computer Science, Analytics, a related field, or equivalent experience</li> 
  <li>5+ years total software and relational database development experience.</li> 
  <li>3 years with a strong demonstrated ability to develop and maintain ETL solutions, ideally using Python and various Application Programming Interfaces (API)</li> 
  <li>3+ years&#x2019; experience with AWS Cloud Solutions/Services</li> 
  <li>Ability to work in an Agile environment include using ticketing software such as JIRA</li> 
  <li>Strong technical problem-solving abilities</li> 
  <li>Hands on experience maintaining databases on Redshift, PostgreSQL, MySQL or Oracle relational database systems.</li> 
  <li>Experience with software development using Python, Ruby, or other modern scripting languages, ideally in container solutions such as Docker or Kubernetes.</li> 
  <li>Experience with Apache Airflow, dbt Core, or similar workflow tool is a plus.</li> 
 </ul> 
 <p><b>What You Get:</b></p> 
 <ul> 
  <li>Join one of the fastest growing health IT companies in the country</li> 
  <li>Have the autonomy to build something with an enthusiastically supportive team</li> 
  <li>Learn from working at the highest levels and on the most strategic priorities of the company, including from world class investors and advisors</li> 
  <li>Receive competitive compensation, including equity, with health, dental, vision and other benefits</li> 
 </ul> 
 <p><b>Belonging at Bamboo</b></p> 
 <p><b> We Care. #BambooHealthValuesCare</b></p> 
 <p> Every human being has the right to the best possible healthcare. Our solutions enable healthcare professionals to see and treat every individual as a whole person by providing the right information, at the right time &#x2013; regardless of physical, behavioral, or social barriers.</p> 
 <p> We&#x2019;re a great place to work because we care. We continually seek to learn about our differences and ensure the unique identities and contributions of all employees are welcome, valued and celebrated.</p> 
 <p> Our commitment to making a positive impact starts by recognizing and leveraging our differences, building inclusive teams, cultivating a sense of belonging, combating biases, and actively removing barriers to equity.</p> 
 <p><b><i> Bamboo Health is proud to be an Equal Employment Opportunity and affirmative action employer.</i></b></p>
</div>","https://bamboo-health.rippling-ats.com/job/690192/sr-data-engineer?s=in","e3e95b61427316bf",,,,"Remote","Sr. Data Engineer","19 days ago","2023-10-06T11:51:52.395Z","3.8","4",,"2023-10-25T11:51:52.397Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=e3e95b61427316bf&from=jasx&tk=1hdjaos832ci6000&vjs=3"
"FreeWheel","Comcast brings together the best in media and technology. We drive innovation to create the world's best entertainment and online experiences. As a Fortune 50 leader, we set the pace in a variety of innovative and fascinating businesses and create career opportunities across a wide range of locations and disciplines. We are at the forefront of change and move at an amazing pace, thanks to our remarkable people, who bring cutting-edge products and services to life for millions of customers every day. If you share in our passion for teamwork, our vision to revolutionize industries and our goal to lead the future in media and technology, we want you to fast-forward your career at Comcast.
  Job Summary
  Job Description
  DUTIES: Contribute to the Data and BI Team, which is responsible for designing and implementing centralized reporting platforms for the various software applications and databases used within the Freewheel division; copy, transform, validate, and automate processes from various databases into a centralized process; perform relational database design and programming using SQL; develop and implement simple, efficient, and accurate ETL (extract, transform, load) processes using Powershell and Python; add and improve data within the Reporting Databases; perform customization using C#.NET, Python, and Amazon Web Services; load data into reporting applications that can be accessed by clients; and apply automation and testing processes to enhancements as data is added to the Reporting Database. Position is eligible for 100% remote work.
 
  REQUIREMENTS: Bachelor’s degree, or foreign equivalent, in Computer Science, Engineering, or related technical field, and two (2) years of experience designing and programming relational databases utilizing SQL; developing ETL processes; utilizing Python and AWS; and applying automation processes.
 
  Disclaimer:
 
 
   This information has been designed to indicate the general nature and level of work performed by employees in this role. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications.
 
 
  Comcast is proud to be an equal opportunity workplace. We will consider all qualified applicants for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran status, genetic information, or any other basis protected by applicable law.
  Base pay is one part of the Total Rewards that Comcast provides to compensate and recognize employees for their work. Most sales positions are eligible for a Commission under the terms of an applicable plan, while most non-sales positions are eligible for a Bonus. Additionally, Comcast provides best-in-class Benefits. We believe that benefits should connect you to the support you need when it matters most, and should help you care for those who matter most. That’s why we provide an array of options, expert guidance and always-on tools, that are personalized to meet the needs of your reality – to help support you physically, financially and emotionally through the big milestones and in your everyday life. Please visit the compensation and benefits summary on our careers site for more details.","<div>
 Comcast brings together the best in media and technology. We drive innovation to create the world&apos;s best entertainment and online experiences. As a Fortune 50 leader, we set the pace in a variety of innovative and fascinating businesses and create career opportunities across a wide range of locations and disciplines. We are at the forefront of change and move at an amazing pace, thanks to our remarkable people, who bring cutting-edge products and services to life for millions of customers every day. If you share in our passion for teamwork, our vision to revolutionize industries and our goal to lead the future in media and technology, we want you to fast-forward your career at Comcast.
 <p><b><br> Job Summary</b></p>
 <p><b> Job Description</b></p>
 <p> DUTIES: Contribute to the Data and BI Team, which is responsible for designing and implementing centralized reporting platforms for the various software applications and databases used within the Freewheel division; copy, transform, validate, and automate processes from various databases into a centralized process; perform relational database design and programming using SQL; develop and implement simple, efficient, and accurate ETL (extract, transform, load) processes using Powershell and Python; add and improve data within the Reporting Databases; perform customization using C#.NET, Python, and Amazon Web Services; load data into reporting applications that can be accessed by clients; and apply automation and testing processes to enhancements as data is added to the Reporting Database. Position is eligible for 100% remote work.</p>
 <p></p>
 <p> REQUIREMENTS: Bachelor&#x2019;s degree, or foreign equivalent, in Computer Science, Engineering, or related technical field, and two (2) years of experience designing and programming relational databases utilizing SQL; developing ETL processes; utilizing Python and AWS; and applying automation processes.</p>
 <p></p>
 <p><b> Disclaimer:</b></p>
 <p></p>
 <ul>
  <li> This information has been designed to indicate the general nature and level of work performed by employees in this role. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications.</li>
 </ul>
 <p></p>
 <p> Comcast is proud to be an equal opportunity workplace. We will consider all qualified applicants for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran status, genetic information, or any other basis protected by applicable law.</p>
 <p><br> Base pay is one part of the Total Rewards that Comcast provides to compensate and recognize employees for their work. Most sales positions are eligible for a Commission under the terms of an applicable plan, while most non-sales positions are eligible for a Bonus. Additionally, Comcast provides best-in-class Benefits. We believe that benefits should connect you to the support you need when it matters most, and should help you care for those who matter most. That&#x2019;s why we provide an array of options, expert guidance and always-on tools, that are personalized to meet the needs of your reality &#x2013; to help support you physically, financially and emotionally through the big milestones and in your everyday life. Please visit the compensation and benefits summary on our careers site for more details.</p>
</div>","https://jobs.comcast.com/jobs/description?external_or_internal=External&job_id=R374387&source=ind_orga_at&jobPipeline=Indeed","8d1592b6c85ff539",,,,"Philadelphia, PA 19103","8530- Software Engineer 3 (Data)","13 days ago","2023-10-12T11:52:04.964Z","3.8","4",,"2023-10-25T11:52:04.966Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=8d1592b6c85ff539&from=jasx&tk=1hdjaos832ci6000&vjs=3"
"Stratagen","Stratagen is currently seeking a highly skilled Data Engineer 2 to join our team. In this role, you will play a key part in implementing essential changes to the legacy National Crime Information Center (NCIC) system in the FBI’s Criminal Justice Information Services (CJIS) Division . These changes are critical for enabling the agile development of the NCIC 3rd Generation (N3G) system. As a Data Engineer, you will have diverse responsibilities encompassing data profiling, data design, data management, and the generation of test data.
Required:

 B.A. or B.S. from an accredited institution
 A minimum of twelve (12) years of data management, database administration or equivalent experience
 A minimum of eight (8) years of data engineering, data analysis, or equivalent experience
 Active Top Secret Clearance

Preferred; however, not required:

 Prior work experience with the Federal Government
 Prior work experience with CJIS systems and associated persistent and/or non- persistent data
 Prior work experience with either Amazon web Services (AwS) or Azure
 Familiarity with Agile development
 Prior work experience with the Scaled Agile Framework (SAFe)
 Experience building data visualizations and reports
 Experience working with IT and business stakeholders in designing the data architecture for a system
 Experience synthesizing and analyzing extremely large data sets
 Familiarity with optimization models

Job Type: Full-time
Pay: $100,000.00 - $140,000.00 per year
Benefits:

 Dental insurance
 Health insurance
 Health savings account
 Life insurance
 Paid time off
 Retirement plan
 Vision insurance

Experience level:

 11+ years

Schedule:

 Monday to Friday

Security clearance:

 Top Secret (Required)

Work Location: Remote","<p>Stratagen is currently seeking a highly skilled Data Engineer 2 to join our team. In this role, you will play a key part in implementing essential changes to the legacy National Crime Information Center (NCIC) system in the FBI&#x2019;s Criminal Justice Information Services (CJIS) Division . These changes are critical for enabling the agile development of the NCIC 3rd Generation (N3G) system. As a Data Engineer, you will have diverse responsibilities encompassing data profiling, data design, data management, and the generation of test data.</p>
<p>Required:</p>
<ul>
 <li>B.A. or B.S. from an accredited institution</li>
 <li>A minimum of twelve (12) years of data management, database administration or equivalent experience</li>
 <li>A minimum of eight (8) years of data engineering, data analysis, or equivalent experience</li>
 <li><i>Active Top Secret Clearance</i></li>
</ul>
<p>Preferred; however, not required:</p>
<ul>
 <li>Prior work experience with the Federal Government</li>
 <li>Prior work experience with CJIS systems and associated persistent and/or non- persistent data</li>
 <li>Prior work experience with either Amazon web Services (AwS) or Azure</li>
 <li>Familiarity with Agile development</li>
 <li>Prior work experience with the Scaled Agile Framework (SAFe)</li>
 <li>Experience building data visualizations and reports</li>
 <li>Experience working with IT and business stakeholders in designing the data architecture for a system</li>
 <li>Experience synthesizing and analyzing extremely large data sets</li>
 <li>Familiarity with optimization models</li>
</ul>
<p>Job Type: Full-time</p>
<p>Pay: &#x24;100,000.00 - &#x24;140,000.00 per year</p>
<p>Benefits:</p>
<ul>
 <li>Dental insurance</li>
 <li>Health insurance</li>
 <li>Health savings account</li>
 <li>Life insurance</li>
 <li>Paid time off</li>
 <li>Retirement plan</li>
 <li>Vision insurance</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>11+ years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>Monday to Friday</li>
</ul>
<p>Security clearance:</p>
<ul>
 <li>Top Secret (Required)</li>
</ul>
<p>Work Location: Remote</p>",,"e65490aaaff7f716",,"Full-time",,"Remote","Data Engineer 2","8 days ago","2023-10-17T11:52:14.563Z",,,"$100,000 - $140,000 a year","2023-10-25T11:52:14.565Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=e65490aaaff7f716&from=jasx&tk=1hdjaqvhcirm6800&vjs=3"
"Discord","The central Data Platform seeks to build a self-service tooling platform to make the petabytes of data at Discord easily accessible for everyone at the company. We build full-stack applications, tooling, and frameworks to improve the productivity of teams at Discord, in particular our product, analytics, and machine learning teams. Our tooling covers the end-to-end lifecycle of data from acquisition to consumption. Reporting to the Engineering Manager of Data Products, you will work on strategy that is foundational to the company and product. To learn more about Discord Engineering, read our engineering blog here — including ""How We Create Insights From Trillion Data Points"" that this team is behind! 
  What you'll be doing 
  
   Lead end-to-end development of data tooling and frameworks, using modern technologies such as BigQuery, Apache Beam, Airflow, Dagster, dbt, Kubernetes, and Rust. 
   Ensure tight-knit collaboration with leadership, cross-functional stake-holders and senior engineers across the organizationCollaborate with leadership and senior engineers across the team to define the technical vision and build on the technical roadmap for Data Platform. 
   Work with Data Platform to ensure we have a platform with strong governance that respects our users' privacy throughout. 
   Care deeply about business outcomes and constraints and keep them in mind as you solve hard, unbounded problems. 
   Work with other Staff Engineers to make decisions for the organization and engineering function as a whole. 
   Coach and mentor the next generation of technical leaders at Discord. 
  
 
 
  What you should have 
  
   7+ years of experience as a Software Engineer. 
   Empathy for both your internal and external users and seek feedback on your work. 
   Ability to approach problems with first principles thinking, embrace ambiguity, and enjoy collaborative work on complex solutions. 
   Experience defining architecture, tooling, and strategy for a large-scale data processing system. 
   Proactive in staying up-to-date with industry trends and assessing new technologies to enhanse problem solving capabilities. 
   
 
 
 
  Bonus Points 
  
   
    
     Experience working with very high-scale data infrastructure and tooling
     
   
    
     Experience with data products on Google Cloud Platform, Kubernetes, or Airflow
     
   
    
     Full-stack development or product engineering experience
      
  
  
  The US base salary range for this full-time position is $214,000 to $233,000 + equity + benefits. Our salary ranges are determined by role and level. Within the range, individual pay is determined by additional factors, including job-related skills, experience, and relevant education or training. Please note that the compensation details listed in US role postings reflect the base salary only, and do not include equity, or benefits.  #buildbelonging #LI-Remote #LI-Hybrid #LI-HY1 
 
 
   Benefits and Perks 
  
   Comprehensive medical insurance including Health, Dental and Vision (plus up to $20,000 for gender affirmation procedures) 
   Mental health resources and quarterly wellness stipends 
   14+ paid holidays, 4 weeks of PTO + use-what-you-need sick days 
   Paid parental leave (plus fertility, adoption and other family planning benefits) 
   Flexible long-term work options (remote and hybrid) 
   Volunteer time off 
   A diverse slate of Employee Resource Groups 
   Plus commuter contributions and other perks for office-based employees 
  
  About Us 
   Discord is a voice, video and text app that helps friends and communities come together to hang out and explore their interests — from artists and activists, to study groups, sneakerheads, plant parents, and more. With 150 million monthly users across 19 million active communities, called servers, Discord has grown to become one of the most popular communications services in the world. Discord was built without selling ads or user data and instead, offers a premium subscription called Nitro that gives users special perks like higher quality streams and fun customizations. 
   We're working toward an inclusive world where no one feels like an outsider, where genuine human connection is a click, text chat, or voice call away. A place where everyone can find belonging. Challenging? Heck yes. Rewarding? Double heck yes. It's a mission that gives us the chance to positively impact millions of people all over the world. So if this strikes a chord with you, come build belonging with us!","<div>
 <div>
  <h2 class=""jobSectionHeader""><b>The central Data Platform seeks to build a self-service tooling platform to make the petabytes of data at Discord easily accessible for everyone at the company. We build full-stack applications, tooling, and frameworks to improve the productivity of teams at Discord, in particular our product, analytics, and machine learning teams. Our tooling covers the end-to-end lifecycle of data from acquisition to consumption. Reporting to the Engineering Manager of Data Products, you will work on strategy that is foundational to the company and product. To learn more about Discord Engineering, read our </b><b>engineering blog here</b><b> &#x2014; including </b><b>&quot;How We Create Insights From Trillion Data Points&quot;</b><b> that this team is behind!</b></h2> 
  <h4 class=""jobSectionHeader""><b>What you&apos;ll be doing</b></h4> 
  <ul>
   <li>Lead end-to-end development of data tooling and frameworks, using modern technologies such as BigQuery, Apache Beam, Airflow, Dagster, dbt, Kubernetes, and Rust.</li> 
   <li>Ensure tight-knit collaboration with leadership, cross-functional stake-holders and senior engineers across the organizationCollaborate with leadership and senior engineers across the team to define the technical vision and build on the technical roadmap for Data Platform.</li> 
   <li>Work with Data Platform to ensure we have a platform with strong governance that respects our users&apos; privacy throughout.</li> 
   <li>Care deeply about business outcomes and constraints and keep them in mind as you solve hard, unbounded problems.</li> 
   <li>Work with other Staff Engineers to make decisions for the organization and engineering function as a whole.</li> 
   <li>Coach and mentor the next generation of technical leaders at Discord.</li> 
  </ul>
 </div>
 <div>
  <h4 class=""jobSectionHeader""><b>What you should have</b></h4> 
  <ul>
   <li>7+ years of experience as a Software Engineer.</li> 
   <li>Empathy for both your internal and external users and seek feedback on your work.</li> 
   <li>Ability to approach problems with first principles thinking, embrace ambiguity, and enjoy collaborative work on complex solutions.</li> 
   <li>Experience defining architecture, tooling, and strategy for a large-scale data processing system.</li> 
   <li>Proactive in staying up-to-date with industry trends and assessing new technologies to enhanse problem solving capabilities.</li> 
  </ul> 
 </div>
 <p></p>
 <div>
  <h4 class=""jobSectionHeader""><b>Bonus Points</b></h4> 
  <ul>
   <li>
    <div>
     Experience working with very high-scale data infrastructure and tooling
    </div> </li>
   <li>
    <div>
     Experience with data products on Google Cloud Platform, Kubernetes, or Airflow
    </div> </li>
   <li>
    <div>
     Full-stack development or product engineering experience
    </div> </li> 
  </ul>
  <p></p>
  <p><i>The US base salary range for this full-time position is &#x24;214,000 to &#x24;233,000 + equity + benefits. Our salary ranges are determined by role and level. Within the range, individual pay is determined by additional factors, including job-related skills, experience, and relevant education or training. Please note that the compensation details listed in US role postings reflect the base salary only, and do not include equity, or benefits.</i><br> <br> #buildbelonging #LI-Remote #LI-Hybrid #LI-HY1</p> 
 </div>
 <div>
  <h4 class=""jobSectionHeader""><b> Benefits and Perks</b></h4> 
  <ul>
   <li>Comprehensive medical insurance including Health, Dental and Vision (plus up to &#x24;20,000 for gender affirmation procedures)</li> 
   <li>Mental health resources and quarterly wellness stipends</li> 
   <li>14+ paid holidays, 4 weeks of PTO + use-what-you-need sick days</li> 
   <li>Paid parental leave (plus fertility, adoption and other family planning benefits)</li> 
   <li>Flexible long-term work options (remote and hybrid)</li> 
   <li>Volunteer time off</li> 
   <li>A diverse slate of Employee Resource Groups</li> 
   <li>Plus commuter contributions and other perks for office-based employees</li> 
  </ul>
  <h4 class=""jobSectionHeader""><b>About Us</b></h4> 
  <p> Discord is a voice, video and text app that helps friends and communities come together to hang out and explore their interests &#x2014; from artists and activists, to study groups, sneakerheads, plant parents, and more. With 150 million monthly users across 19 million active communities, called servers, Discord has grown to become one of the most popular communications services in the world. Discord was built without selling ads or user data and instead, offers a premium subscription called Nitro that gives users special perks like higher quality streams and fun customizations.</p> 
  <p> We&apos;re working toward an inclusive world where no one feels like an outsider, where genuine human connection is a click, text chat, or voice call away. A place where everyone can find belonging. Challenging? Heck yes. Rewarding? Double heck yes. It&apos;s a mission that gives us the chance to positively impact millions of people all over the world. <b>So if this strikes a chord with you, come build belonging with us!</b></p>
 </div>
</div>","https://boards.greenhouse.io/discord/jobs/6977135002?gh_src=753f74f22us","d204b938f054b0b9",,"Full-time",,"444 De Haro St, San Francisco, CA 94107","Staff Software Engineer - Data Platform","7 days ago","2023-10-18T11:52:09.093Z","4.4","7",,"2023-10-25T11:52:09.094Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=d204b938f054b0b9&from=jasx&tk=1hdjaos832ci6000&vjs=3"
"Verizon","When you join Verizon
  Verizon is one of the world’s leading providers of technology and communications services, transforming the way we connect around the world. We’re a human network that reaches across the globe and works behind the scenes. We anticipate, lead, and believe that listening is where learning begins. In crisis and in celebration, we come together—lifting up our communities and striving to make an impact to move the world forward. If you’re fueled by purpose, and powered by persistence, explore a career with us. Here, you’ll discover the rigor it takes to make a difference and the fulfillment that comes with living the #NetworkLife.
 
  What you’ll be doing...
  This role is part of the Artificial and Intelligence (AI&D) organization which focuses on delivering the power of data and AI to drive our marketplace leadership through innovative solutions that create value for our customers, shareholders, V teamers and society, responsibly. At Verizon, we are on a multi-year journey to industrialize our data practices and AI capabilities. Very simply, this means that AI and data will fuel all decisions and business processes across the company.
 
   Understanding code development that is compliant and meets high standards/quality, delivers desired functionality using cutting edge technology.
   Programming a component, developing feature or frameworks
   Working independently and contributing to the immediate team and cross functional teams.
   Contributing to design discussions.
   Taking ownership of delegated tasks and helping other team members.
   Demonstrating the initiative to explore alternate technology and approaches to solving problems.
   Following the agile based approach to deliver the data products.
 
  What we’re looking for...
  We're looking for a Junior Data Engineer with a good fundamental understanding of Google Cloud Platform, data ingesting and data curation concept. Depending upon the need of the product, we're looking for someone to help us build either in GCP platform, On-prem Hadoop or Teradata.
 
  You’ll need to have:
 
   Bachelor's degree or one or more years of work experience.
   Good understanding of data warehousing, data lakes and big data platforms.
   Strong leadership, communication, persuasion and teamwork skills.
   Empathy and a positive attitude.
 
 
  Even better if you have one or more of the following:
 
   Bachelors/Masters degree in Computer Science, Information Science, Engineering or other related field with 1 or more years of relevant work experience.
   One or more years of programming experience in GCP Data Proc, Cloud Shell SDK, Cloud Composer, GCS, Cloud Functions & Big Query.
   One or more years of experience in designing and deployment of Hadoop cluster and different big data analytical tools including HDFS, PIG, Hive, Sqoop, Spark, Oozie.
   Hands on experience in designing and building data pipelines in Nifi/Airflow/Apache Beam in GCP (Data Proc and Big Query) for ETL related jobs.
   Knowledge in Google data catalog and other Google Cloud APIs for monitoring, querying and billing related analysis for Big Query usage.
   Experience working with at least 1 NoSQL Databases (HBase, Cassandra, Couchbase) and 1 relational database (Oracle, MySql, Teradata).
 
 
  If Verizon and this role sound like a fit for you, we encourage you to apply even if you don’t meet every “even better” qualification listed above.
 
  This role is eligible to be considered for the Department of Defense SkillBridge Program.
 
 
  
   
    
     
      
       
         Where you’ll be working
       
      
     
    
   
  
  In this hybrid role, you'll have a defined work location that includes work from home and assigned office days set by your manager.
 
  Scheduled Weekly Hours 40
 
  Equal Employment Opportunity
  We’re proud to be an equal opportunity employer - and celebrate our employees’ differences, including race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, and Veteran status. At Verizon, we know that diversity makes us stronger. We are committed to a collaborative, inclusive environment that encourages authenticity and fosters a sense of belonging. We strive for everyone to feel valued, connected, and empowered to reach their potential and contribute their best. Check out our diversity and inclusion page to learn more.","<div>
 <h3 class=""jobSectionHeader""><b>When you join Verizon</b></h3>
 <p> Verizon is one of the world&#x2019;s leading providers of technology and communications services, transforming the way we connect around the world. We&#x2019;re a human network that reaches across the globe and works behind the scenes. We anticipate, lead, and believe that listening is where learning begins. In crisis and in celebration, we come together&#x2014;lifting up our communities and striving to make an impact to move the world forward. If you&#x2019;re fueled by purpose, and powered by persistence, explore a career with us. Here, you&#x2019;ll discover the rigor it takes to make a difference and the fulfillment that comes with living the #NetworkLife.</p>
 <p></p>
 <h3 class=""jobSectionHeader""><b> What you&#x2019;ll be doing...</b></h3>
 <p> This role is part of the Artificial and Intelligence (AI&amp;D) organization which focuses on delivering the power of data and AI to drive our marketplace leadership through innovative solutions that create value for our customers, shareholders, V teamers and society, responsibly. At Verizon, we are on a multi-year journey to industrialize our data practices and AI capabilities. Very simply, this means that AI and data will fuel all decisions and business processes across the company.</p>
 <ul>
  <li><p> Understanding code development that is compliant and meets high standards/quality, delivers desired functionality using cutting edge technology.</p></li>
  <li><p> Programming a component, developing feature or frameworks</p></li>
  <li><p> Working independently and contributing to the immediate team and cross functional teams.</p></li>
  <li><p> Contributing to design discussions.</p></li>
  <li><p> Taking ownership of delegated tasks and helping other team members.</p></li>
  <li><p> Demonstrating the initiative to explore alternate technology and approaches to solving problems.</p></li>
  <li><p> Following the agile based approach to deliver the data products.</p></li>
 </ul>
 <h3 class=""jobSectionHeader""><b> What we&#x2019;re looking for...</b></h3>
 <p> We&apos;re looking for a Junior Data Engineer with a good fundamental understanding of Google Cloud Platform, data ingesting and data curation concept. Depending upon the need of the product, we&apos;re looking for someone to help us build either in GCP platform, On-prem Hadoop or Teradata.</p>
 <p></p>
 <h3 class=""jobSectionHeader""><b> You&#x2019;ll need to have:</b></h3>
 <ul>
  <li><p> Bachelor&apos;s degree or one or more years of work experience.</p></li>
  <li><p> Good understanding of data warehousing, data lakes and big data platforms.</p></li>
  <li><p> Strong leadership, communication, persuasion and teamwork skills.</p></li>
  <li><p> Empathy and a positive attitude.</p></li>
 </ul>
 <p></p>
 <h3 class=""jobSectionHeader""><b> Even better if you have one or more of the following:</b></h3>
 <ul>
  <li><p> Bachelors/Masters degree in Computer Science, Information Science, Engineering or other related field with 1 or more years of relevant work experience.</p></li>
  <li><p> One or more years of programming experience in GCP Data Proc, Cloud Shell SDK, Cloud Composer, GCS, Cloud Functions &amp; Big Query.</p></li>
  <li><p> One or more years of experience in designing and deployment of Hadoop cluster and different big data analytical tools including HDFS, PIG, Hive, Sqoop, Spark, Oozie.</p></li>
  <li><p> Hands on experience in designing and building data pipelines in Nifi/Airflow/Apache Beam in GCP (Data Proc and Big Query) for ETL related jobs.</p></li>
  <li><p> Knowledge in Google data catalog and other Google Cloud APIs for monitoring, querying and billing related analysis for Big Query usage.</p></li>
  <li><p> Experience working with at least 1 NoSQL Databases (HBase, Cassandra, Couchbase) and 1 relational database (Oracle, MySql, Teradata).</p></li>
 </ul>
 <p></p>
 <p> If Verizon and this role sound like a fit for you, we encourage you to apply even if you don&#x2019;t meet every &#x201c;even better&#x201d; qualification listed above.</p>
 <p></p>
 <p> This role is eligible to be considered for the Department of Defense SkillBridge Program.</p>
 <p></p>
 <div>
  <div>
   <div>
    <div>
     <div>
      <div>
       <div>
        <h3 class=""jobSectionHeader""><b> Where you&#x2019;ll be working</b></h3>
       </div>
      </div>
     </div>
    </div>
   </div>
  </div>
 </div> In this hybrid role, you&apos;ll have a defined work location that includes work from home and assigned office days set by your manager.
 <p></p>
 <h3 class=""jobSectionHeader""><b> Scheduled Weekly Hours</b></h3> 40
 <p></p>
 <h3 class=""jobSectionHeader""><b> Equal Employment Opportunity</b></h3>
 <p> We&#x2019;re proud to be an equal opportunity employer - and celebrate our employees&#x2019; differences, including race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, and Veteran status. At Verizon, we know that diversity makes us stronger. We are committed to a collaborative, inclusive environment that encourages authenticity and fosters a sense of belonging. We strive for everyone to feel valued, connected, and empowered to reach their potential and contribute their best. Check out our diversity and inclusion page to learn more.</p>
</div>
<p></p>","https://www.indeed.com/rc/clk?jk=2b906b6aac7448f3&atk=&xpse=SoBY67I3JzdikpxTop0LbzkdCdPP","2b906b6aac7448f3",,"Full-time",,"7301 N Point Pkwy, Alpharetta, GA 30022","Junior Data Engineer","13 days ago","2023-10-12T11:52:17.719Z","3.8","32098",,"2023-10-25T11:52:17.722Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=2b906b6aac7448f3&from=jasx&tk=1hdjaqvhcirm6800&vjs=3"
"Callibus Inc","Job Title: Senior Data EngineerLocation: RemoteDuration: Full Time
Must Have:

 Recent PySpark exp
 Recent Scala exp
 GCP
 NOSQL database

Role:

 Designs, develops, and implements Hadoop eco-system-based applications to support business requirements.
 Follows approved life cycle methodologies, creates design documents, and performs program coding and testing.
 Resolves technical issues through debugging, research, and investigation.

Additional Experience/Skills Required:

 3-5 years of experience in design, implementation, and support of solutions big data solution in Hadoop using Hive, Spark, Drill, Impala, HBase
 Hands on experience with Unix, Teradata and other relational databases.
 Experience with Scale a plus
 Strong communication and problem-solving skills

Job Type: Full-time
Work Location: Remote","<p><b>Job Title: </b>Senior Data Engineer<br><b>Location:</b> Remote<br><b>Duration:</b> Full Time</p>
<p><b>Must Have:</b></p>
<ul>
 <li>Recent PySpark exp</li>
 <li>Recent Scala exp</li>
 <li>GCP</li>
 <li>NOSQL database</li>
</ul>
<p><b>Role:</b></p>
<ul>
 <li>Designs, develops, and implements Hadoop eco-system-based applications to support business requirements.</li>
 <li>Follows approved life cycle methodologies, creates design documents, and performs program coding and testing.</li>
 <li>Resolves technical issues through debugging, research, and investigation.</li>
</ul>
<p><b>Additional Experience/Skills Required</b>:</p>
<ul>
 <li>3-5 years of experience in design, implementation, and support of solutions big data solution in Hadoop using Hive, Spark, Drill, Impala, HBase</li>
 <li>Hands on experience with Unix, Teradata and other relational databases.</li>
 <li>Experience with Scale a plus</li>
 <li>Strong communication and problem-solving skills</li>
</ul>
<p>Job Type: Full-time</p>
<p>Work Location: Remote</p>",,"c649c13730fce3c1",,"Full-time",,"Remote","Senior Data Engineer","13 days ago","2023-10-12T11:52:20.582Z",,,,"2023-10-25T11:52:20.583Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=c649c13730fce3c1&from=jasx&tk=1hdjaqvhcirm6800&vjs=3"
"HHS Technology Group","Data Engineer DyD
  WHO WE ARE: 
 At HHS Tech Group (HTG), our work matters, and each of us makes a difference in the lives of people every day. 
 HTG is a leader in developing and delivering innovative, purpose-built modular software and technology solutions to clients in the commercial and government sectors. 
 WHAT WE DO: 
 HHS Tech Group creates innovative, purpose-built technology products and solutions, resulting in value and positive, quantifiable impact for our clients and those they serve. 
 Our people bring our software to life through collaborative relationships with our clients, working as a team, and helping to solve complex problems that create positive personal and community impact for the people our clients serve. 
 Each day, our software products and our people are making a difference. 
 OUR PEOPLE MATTER MOST: 
 Improving the lives of others and making an impact daily is no simple task. We are dedicated to our team’s professional and personal growth and well-being. Some key rewards and benefits include: 
 
  Generously sponsored Medical Insurance 
  Fully paid premiums on dental, vision, life, and disability insurance. 
  Generous 401k matching program (100% match up to 6%) 
  Tuition and Certification reimbursement 
  Open PTO policy 
 
 Join us!
  WHO WE ARE HIRING: Data Engineer DyD
  
 WHAT YOU WILL DO: 
 HHS Technology Group is expanding. We create software products relevant to the healthcare insurance domain. Our growth trajectory is fantastic. We are leveraging data science to create healthcare insurance software to identify data trends which would indicate fraud waste and abuse scenarios as well as third-party liability, thusly allowing the insurance provider to recover their funds, which may have been paid erroneously. This is a fully remote role.
 
  
 MINIMUM QUALIFICATIONS:
  
 
 
   2+ years of work experience with ETL
   Expert-level skills in writing SQL
   Python and SQL/MongoDB skills are must requirements.
   Experience with Big Data technologies such as Spark
   Experience with AWS Big Data technologies such as AWS Glue/EMR, S3, AWS Athena
   Experience with Data warehouses such as Snowflake/Redshift
   Knowledge of processing Claims/EMR/EHR/Provider / Member datasets.
   Experience with NodeJS, ReactJS
 
 
  DESIRED SKILLS:
  
 
 
   Ability to multi-task, take initiative, assume accountability, work independently, and self-manage duties and responsibilities, as well as intuitively having the ability to pivot focus between tasks according to priorities to achieve maximum efficiency.
   Experience with Apache AirFlow
   Working with HIPAA and Secure Data transfers (TLS 1.2+ standards)
   Understand data s3 Data encryption standards for HIPAA data (AES 256 +)
   Data pipeline building and preparing documentation and flow charting.
   Working with a client's technical team to ensure efficient processing of data.
 
 
  HHS Technology Group’s flagship product, Discover your Provider (DyP) is a provider relations/provider enrollment solution. Components of this modular software can be re-purposed or built upon to deliver cloud based technical web services solutions.
 
  Applicants for employment in the US must have work authorization that does not, now or in the future, require sponsorship of a visa for employment authorization in the United States. 
 
 EEO Statement 
 HHS Technology Group is an equal opportunity employer. All aspects of employment including the decision to hire, promote, discipline, or discharge, will be based on merit, competence, performance, and business needs. We do not discriminate on the basis of race, color, religion, marital status, age, national origin, ancestry, physical or mental disability, medical condition, pregnancy, genetic information, gender, sexual orientation, gender identity or expression, veteran status, or any other status protected under federal, state, or local law.
  
 NI3tKHsl6f","<p></p>
<div>
 <h1 class=""jobSectionHeader""><b>Data Engineer DyD</b></h1>
 <p><b><br> WHO WE ARE: </b></p>
 <p>At HHS Tech Group (HTG), our work matters, and each of us makes a difference in the lives of people every day. </p>
 <p>HTG is a leader in developing and delivering innovative, purpose-built modular software and technology solutions to clients in the commercial and government sectors. </p>
 <p><b>WHAT WE DO: </b></p>
 <p>HHS Tech Group creates innovative, purpose-built technology products and solutions, resulting in value and positive, quantifiable impact for our clients and those they serve. </p>
 <p>Our people bring our software to life through collaborative relationships with our clients, working as a team, and helping to solve complex problems that create positive personal and community impact for the people our clients serve. </p>
 <p>Each day, our software products and our people are making a difference. </p>
 <p><b>OUR PEOPLE MATTER MOST: </b></p>
 <p>Improving the lives of others and making an impact daily is no simple task. We are dedicated to our team&#x2019;s professional and personal growth and well-being. Some key rewards and benefits include: </p>
 <ul>
  <li>Generously sponsored Medical Insurance </li>
  <li>Fully paid premiums on dental, vision, life, and disability insurance. </li>
  <li>Generous 401k matching program (100% match up to 6%) </li>
  <li>Tuition and Certification reimbursement </li>
  <li>Open PTO policy </li>
 </ul>
 <p><b>Join us!</b></p>
 <p><b> WHO WE ARE HIRING:</b><b> </b><b>Data Engineer DyD</b></p>
 <p><br> </p>
 <p><b>WHAT YOU WILL DO:</b> </p>
 <p>HHS Technology Group is expanding. We create software products relevant to the healthcare insurance domain. Our growth trajectory is fantastic. We are leveraging data science to create healthcare insurance software to identify data trends which would indicate fraud waste and abuse scenarios as well as third-party liability, thusly allowing the insurance provider to recover their funds, which may have been paid erroneously. This is a fully remote role.</p>
 <p></p>
 <p><br> </p>
 <p><b>MINIMUM QUALIFICATIONS:</b></p>
 <br> 
 <p></p>
 <ul>
  <li> 2+ years of work experience with ETL</li>
  <li> Expert-level skills in writing SQL</li>
  <li> Python and SQL/MongoDB skills are must requirements.</li>
  <li> Experience with Big Data technologies such as Spark</li>
  <li> Experience with AWS Big Data technologies such as AWS Glue/EMR, S3, AWS Athena</li>
  <li> Experience with Data warehouses such as Snowflake/Redshift</li>
  <li> Knowledge of processing Claims/EMR/EHR/Provider / Member datasets.</li>
  <li> Experience with NodeJS, ReactJS</li>
 </ul>
 <p></p>
 <p><b><br> DESIRED SKILLS:</b></p>
 <br> 
 <p></p>
 <ul>
  <li> Ability to multi-task, take initiative, assume accountability, work independently, and self-manage duties and responsibilities, as well as intuitively having the ability to pivot focus between tasks according to priorities to achieve maximum efficiency.</li>
  <li> Experience with Apache AirFlow</li>
  <li> Working with HIPAA and Secure Data transfers (TLS 1.2+ standards)</li>
  <li> Understand data s3 Data encryption standards for HIPAA data (AES 256 +)</li>
  <li> Data pipeline building and preparing documentation and flow charting.</li>
  <li> Working with a client&apos;s technical team to ensure efficient processing of data.</li>
 </ul>
 <p></p>
 <p><br> HHS Technology Group&#x2019;s flagship product, Discover your Provider (DyP) is a provider relations/provider enrollment solution. Components of this modular software can be re-purposed or built upon to deliver cloud based technical web services solutions.</p>
 <p></p>
 <p><br> Applicants for employment in the US must have work authorization that does not, now or in the future, require sponsorship of a visa for employment authorization in the United States.</p> 
 <p></p>
 <p>EEO Statement</p> 
 <p>HHS Technology Group is an equal opportunity employer. All aspects of employment including the decision to hire, promote, discipline, or discharge, will be based on merit, competence, performance, and business needs. We do not discriminate on the basis of race, color, religion, marital status, age, national origin, ancestry, physical or mental disability, medical condition, pregnancy, genetic information, gender, sexual orientation, gender identity or expression, veteran status, or any other status protected under federal, state, or local law.</p>
 <p> </p>
 <p>NI3tKHsl6f</p>
</div>","https://hhstechnologygroup.applytojob.com/apply/NI3tKHsl6f/Data-Engineer?source=INDE","c2da6d9ac38d9eed",,"Full-time",,"Remote","Data Engineer","12 days ago","2023-10-13T11:52:14.171Z","5","2",,"2023-10-25T11:52:14.217Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=c2da6d9ac38d9eed&from=jasx&tk=1hdjaqvhcirm6800&vjs=3"
"Gen4 Dental","Company Description
At Gen4, we pride ourselves on our commitment to providing an incredible patient experience. We are passionate about our craft and the impact we can have on our patients. We aim to foster a doctor-centric organization that allows our doctors to do more of what they love. Culture, high performance, growth, and development are deeply embedded in our business and we have our sights set on finding individuals who are excited to be a part of a growing, flourishing company like ours. We offer competitive pay and a comprehensive benefits package available on the 1st of the month after 30 days of employment for full-time employees.
To learn more about us, check out our website here: https://gen4dental.com/
Job Description
The Data Engineer is a detail-oriented and innovative engineer with a deep understanding of Microsoft data platforms and proficiency in Python. In this role, you will spearhead the development and maintenance of our data infrastructure to facilitate efficient data processing and analytics, enhancing our dental services through informed, data-driven strategies.
Duties & Responsibilities

 Develop, construct, test, and maintain robust ETL processes primarily using Microsoft SQL Server Integration Services (SSIS) and\or Python.
 Design and implement relational and non-relational database systems utilizing Microsoft SQL Server, ensuring data integrity, availability, and confidentiality.
 Leverage Microsoft Azure data services (Azure Data Factory, Azure SQL Data Warehouse, Azure Data Lake, etc.) for scalable and cost-effective solutions.
 Develop analytical tools and solutions using Microsoft Power BI to empower teams with actionable insights for strategic planning and operational efficiency.
 Optimize and automate data delivery processes and establish routines for database tasks to improve system performance and stability.
 Uphold high standards for data quality by devising and implementing effective data cleaning and validation procedures.
 Collaborate closely with cross-functional teams to align data management and analytics solutions with business objectives.

Qualifications
Required:

 Minimum of 5+ years’ experience as a Data Engineer, ETL Developer, or similar role with a focus on Microsoft data platforms.
 Proficient in SQL with a solid understanding of Microsoft SQL Server and SSIS.
 Experience with Azure data services and Microsoft Power BI is essential.
 Knowledge of data modeling principles, including experience with data warehousing and big data technologies.
 Python programming experience.
 Strong problem-solving skills with an analytical mindset.
 Excellent communication skills, capable of explaining complex technical concepts to non-technical stakeholders.
 Multi-site healthcare experience.

Preferred:

 Bachelor's degree in a technology or engineering field, or equivalent combination of experience
 Dental industry experience
 Google Bigquery experience
 Postgresql experience.

Physical Requirements:

 Prolonged periods of sitting at a desk and working on a computer.
 Ability to lift up to 15 pounds.
 Excellent written, speaking and listening skills, requiring the perception of speech.
 Must have high finger dexterity to perform duties involving work on the computer.
 Able to travel as needed.

Equipment Used:
General office equipment (e.g. computer).
Additional information
Working conditions include those typically seen in an office environment. Prolonged periods of sitting at a desk and working on a computer.
Equal Opportunity Employer
Gen4 Dental Partners provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.
Job Type: Full-time
Pay: $115,000.00 - $125,000.00 per year
Benefits:

 401(k)
 401(k) matching
 Dental insurance
 Health insurance
 Paid time off
 Vision insurance

Schedule:

 Monday to Friday

Application Question(s):

 Are you at least 18 years of age?
 How many years of Microsoft SQL Server Experience do you have?
 How many years of Transact SQL experience do you have?
 How many years of Python Experience, specifically with libraries such as Pandas and NumPy?

Work Location: Remote","<p><b>Company Description</b></p>
<p>At Gen4, we pride ourselves on our commitment to providing an incredible patient experience. We are passionate about our craft and the impact we can have on our patients. We aim to foster a doctor-centric organization that allows our doctors to do more of what they love. Culture, high performance, growth, and development are deeply embedded in our business and we have our sights set on finding individuals who are excited to be a part of a growing, flourishing company like ours. <b>We offer competitive pay and a comprehensive benefits package available on the 1st of the month after 30 days of employment for full-time employees</b>.</p>
<p>To learn more about us, check out our website here: https://gen4dental.com/</p>
<p><b>Job Description</b></p>
<p>The Data Engineer is a detail-oriented and innovative engineer with a deep understanding of Microsoft data platforms and proficiency in Python. In this role, you will spearhead the development and maintenance of our data infrastructure to facilitate efficient data processing and analytics, enhancing our dental services through informed, data-driven strategies.</p>
<p><b>Duties &amp; Responsibilities</b></p>
<ul>
 <li>Develop, construct, test, and maintain robust ETL processes primarily using Microsoft SQL Server Integration Services (SSIS) and\or Python.</li>
 <li>Design and implement relational and non-relational database systems utilizing Microsoft SQL Server, ensuring data integrity, availability, and confidentiality.</li>
 <li>Leverage Microsoft Azure data services (Azure Data Factory, Azure SQL Data Warehouse, Azure Data Lake, etc.) for scalable and cost-effective solutions.</li>
 <li>Develop analytical tools and solutions using Microsoft Power BI to empower teams with actionable insights for strategic planning and operational efficiency.</li>
 <li>Optimize and automate data delivery processes and establish routines for database tasks to improve system performance and stability.</li>
 <li>Uphold high standards for data quality by devising and implementing effective data cleaning and validation procedures.</li>
 <li>Collaborate closely with cross-functional teams to align data management and analytics solutions with business objectives.</li>
</ul>
<p><b>Qualifications</b></p>
<p><b>Required:</b></p>
<ul>
 <li>Minimum of 5+ years&#x2019; experience as a Data Engineer, ETL Developer, or similar role with a focus on Microsoft data platforms.</li>
 <li>Proficient in SQL with a solid understanding of Microsoft SQL Server and SSIS.</li>
 <li>Experience with Azure data services and Microsoft Power BI is essential.</li>
 <li>Knowledge of data modeling principles, including experience with data warehousing and big data technologies.</li>
 <li>Python programming experience.</li>
 <li>Strong problem-solving skills with an analytical mindset.</li>
 <li>Excellent communication skills, capable of explaining complex technical concepts to non-technical stakeholders.</li>
 <li>Multi-site healthcare experience.</li>
</ul>
<p><b>Preferred:</b></p>
<ul>
 <li>Bachelor&apos;s degree in a technology or engineering field, or equivalent combination of experience</li>
 <li>Dental industry experience</li>
 <li>Google Bigquery experience</li>
 <li>Postgresql experience.</li>
</ul>
<p><b>Physical Requirements:</b></p>
<ul>
 <li>Prolonged periods of sitting at a desk and working on a computer.</li>
 <li>Ability to lift up to 15 pounds.</li>
 <li>Excellent written, speaking and listening skills, requiring the perception of speech.</li>
 <li>Must have high finger dexterity to perform duties involving work on the computer.</li>
 <li>Able to travel as needed.</li>
</ul>
<p><b>Equipment Used:</b></p>
<p>General office equipment (e.g. computer).</p>
<p><b>Additional information</b></p>
<p>Working conditions include those typically seen in an office environment. Prolonged periods of sitting at a desk and working on a computer.</p>
<p><b>Equal Opportunity Employer</b></p>
<p>Gen4 Dental Partners provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.</p>
<p>Job Type: Full-time</p>
<p>Pay: &#x24;115,000.00 - &#x24;125,000.00 per year</p>
<p>Benefits:</p>
<ul>
 <li>401(k)</li>
 <li>401(k) matching</li>
 <li>Dental insurance</li>
 <li>Health insurance</li>
 <li>Paid time off</li>
 <li>Vision insurance</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>Monday to Friday</li>
</ul>
<p>Application Question(s):</p>
<ul>
 <li>Are you at least 18 years of age?</li>
 <li>How many years of Microsoft SQL Server Experience do you have?</li>
 <li>How many years of Transact SQL experience do you have?</li>
 <li>How many years of Python Experience, specifically with libraries such as Pandas and NumPy?</li>
</ul>
<p>Work Location: Remote</p>",,"e1d633e960423d61",,"Full-time",,"Remote","Data Engineer","7 days ago","2023-10-18T11:52:21.570Z","2","4","$115,000 - $125,000 a year","2023-10-25T11:52:21.571Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=e1d633e960423d61&from=jasx&tk=1hdjaqvhcirm6800&vjs=3"
"FACEBOOK APP","AWS data engineers have a wide range of responsibilities, which can include:
Creating data models that can be used to extract information from various sources and store it in a usable formatMaintaining the integrity of data by designing backup and recovery proceduresIdentifying opportunities to improve performance by improving database structure or indexing methodsConducting research to identify new technologies that can be applied to current projectsAnalyzing data to find patterns or insights that can be used to develop strategies or make business decisionsDeveloping new applications using existing data sets to create new products or improve existing servicesMaintaining existing applications by updating existing code or adding new features to meet new requirementsDesigning and implementing security measures to protect data from unauthorized access or misuseRecommending infrastructure changes to improve storage capacity or performance
Job Types: Contract, Full-time
Salary: $40.34 - $86.70 per hour
Experience level:

 10 years
 11+ years
 7 years
 8 years
 9 years

Experience:

 AWS Data Engineer: 10 years (Preferred)

Work Location: Remote","<p>AWS data engineers have a wide range of responsibilities, which can include:</p>
<p>Creating data models that can be used to extract information from various sources and store it in a usable format<br>Maintaining the integrity of data by designing backup and recovery procedures<br>Identifying opportunities to improve performance by improving database structure or indexing methods<br>Conducting research to identify new technologies that can be applied to current projects<br>Analyzing data to find patterns or insights that can be used to develop strategies or make business decisions<br>Developing new applications using existing data sets to create new products or improve existing services<br>Maintaining existing applications by updating existing code or adding new features to meet new requirements<br>Designing and implementing security measures to protect data from unauthorized access or misuse<br>Recommending infrastructure changes to improve storage capacity or performance</p>
<p>Job Types: Contract, Full-time</p>
<p>Salary: &#x24;40.34 - &#x24;86.70 per hour</p>
<p>Experience level:</p>
<ul>
 <li>10 years</li>
 <li>11+ years</li>
 <li>7 years</li>
 <li>8 years</li>
 <li>9 years</li>
</ul>
<p>Experience:</p>
<ul>
 <li>AWS Data Engineer: 10 years (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,"77f1038f4ded6818",,"Full-time","Contract","Remote","senior AWS Data Engineer","8 days ago","2023-10-17T11:52:23.808Z",,,"$40.34 - $86.70 an hour","2023-10-25T11:52:23.809Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=77f1038f4ded6818&from=jasx&tk=1hdjaqvhcirm6800&vjs=3"
"OrangePeople","We are seeking a highly experienced Principal Data Engineer with a deep understanding of PySpark using Databricks or AWS Glue or AWS EMR and cloud-based databases such as Snowflake. Proficiency in workflow management tools like Airflow is essential. Healthcare industry experience is a significant advantage. The ideal candidate will be responsible for designing, implementing, and maintaining data pipelines while ensuring the highest performance, security, and data quality.
Responsibilities:

 Design, develop, and maintain scalable, reliable, and secure data pipelines to process large volumes of structured and unstructured healthcare data using PySpark and cloud-based databases.
 Collaborate with data architects, data scientists, and analysts to understand data requirements and implement solutions that meet business and technical objectives.
 Leverage AWS or Azure cloud services for data storage, processing, and analytics, optimizing cost and performance.
 Utilize tools like Airflow for workflow management and Kubernetes for container orchestration to ensure seamless deployment, scaling, and management of data processing applications.
 Develop and implement data ingestion, transformation, and validation processes to ensure data quality, consistency, and reliability across various healthcare datasets.
 Monitor and troubleshoot data pipelines, proactively identifying and resolving issues to minimize downtime and ensure optimal performance.
 Establish and enforce data engineering best practices, ensuring compliance with data privacy and security regulations specific to the healthcare industry.
 Continuously evaluate and adopt new tools, technologies, and frameworks to improve the data infrastructure and drive innovation.
 Mentor and guide junior data engineers, fostering a culture of collaboration, learning, and growth within the team.
 Collaborate with cross-functional teams to align data engineering efforts with broader organizational goals and strategies.
 Is familiar with SOC 2 compliance and its impact on company policies and processes.
 Understands the importance of adhering to SOC 2 requirements and maintains an effort to do so.
 Reviews and understands the Employee Handbook, and internal policies that define individual security responsibilities, and maintains segregation of duties in accordance with their role requirements.

Requirements:

 Bachelor's or Master’s degree in Computer Science, Engineering, or a related field.
 8+ years of experience in data engineering, with a strong background in Apache Spark and cloud-based databases such as Snowflake.
 Strong Knowledge in Big Data Technologies, Databricks, AWS Services, and PySpark, thorough in one or more programming languages like Python.
 Proven experience with AWS or Azure cloud services for data storage, processing, and analytics.
 Expertise in workflow management tools like Airflow and container orchestration systems such as Kubernetes.
 Strong knowledge of SQL and NoSQL databases, as well as data modeling and schema design principles.
 Familiarity with healthcare data standards, terminologies, and regulations, such as HIPAA and GDPR, is highly desirable.
 Excellent problem-solving, communication, and collaboration skills, with the ability to work effectively in cross-functional teams.
 Demonstrated ability to manage multiple projects, prioritize tasks, and meet deadlines in a fast-paced environment.
 A strong desire to learn, adapt, and contribute to a rapidly evolving data landscape.

Job Type: Contract
Pay: From $60.00 per hour
Benefits:

 401(k)
 401(k) matching
 Dental insurance
 Health insurance
 Vision insurance

Schedule:

 8 hour shift
 Monday to Friday

Experience:

 Python: 5 years (Preferred)
 data storage, processing, and analytics: 6 years (Preferred)
 SQL and NoSQL: 6 years (Preferred)
 healthcare: 5 years (Preferred)
 Airflow: 5 years (Preferred)
 Kubernetes: 6 years (Preferred)
 Docker: 6 years (Preferred)
 ETL: 5 years (Preferred)
 data engineering: 8 years (Preferred)
 Apache Spark & Snowflake: 6 years (Preferred)
 Big data: 6 years (Preferred)
 Databricks: 5 years (Preferred)
 AWS Services: 5 years (Preferred)
 PySpark: 5 years (Preferred)

Work Location: Remote","<p>We are seeking a highly experienced Principal Data Engineer with a deep understanding of PySpark using Databricks or AWS Glue or AWS EMR and cloud-based databases such as Snowflake. Proficiency in workflow management tools like Airflow is essential. Healthcare industry experience is a significant advantage. The ideal candidate will be responsible for designing, implementing, and maintaining data pipelines while ensuring the highest performance, security, and data quality.</p>
<p><b>Responsibilities:</b></p>
<ul>
 <li>Design, develop, and maintain scalable, reliable, and secure data pipelines to process large volumes of structured and unstructured healthcare data using PySpark and cloud-based databases.</li>
 <li>Collaborate with data architects, data scientists, and analysts to understand data requirements and implement solutions that meet business and technical objectives.</li>
 <li>Leverage AWS or Azure cloud services for data storage, processing, and analytics, optimizing cost and performance.</li>
 <li>Utilize tools like Airflow for workflow management and Kubernetes for container orchestration to ensure seamless deployment, scaling, and management of data processing applications.</li>
 <li>Develop and implement data ingestion, transformation, and validation processes to ensure data quality, consistency, and reliability across various healthcare datasets.</li>
 <li>Monitor and troubleshoot data pipelines, proactively identifying and resolving issues to minimize downtime and ensure optimal performance.</li>
 <li>Establish and enforce data engineering best practices, ensuring compliance with data privacy and security regulations specific to the healthcare industry.</li>
 <li>Continuously evaluate and adopt new tools, technologies, and frameworks to improve the data infrastructure and drive innovation.</li>
 <li>Mentor and guide junior data engineers, fostering a culture of collaboration, learning, and growth within the team.</li>
 <li>Collaborate with cross-functional teams to align data engineering efforts with broader organizational goals and strategies.</li>
 <li>Is familiar with SOC 2 compliance and its impact on company policies and processes.</li>
 <li>Understands the importance of adhering to SOC 2 requirements and maintains an effort to do so.</li>
 <li>Reviews and understands the Employee Handbook, and internal policies that define individual security responsibilities, and maintains segregation of duties in accordance with their role requirements.</li>
</ul>
<p><b>Requirements:</b></p>
<ul>
 <li>Bachelor&apos;s or Master&#x2019;s degree in Computer Science, Engineering, or a related field.</li>
 <li>8+ years of experience in data engineering, with a strong background in Apache Spark and cloud-based databases such as Snowflake.</li>
 <li>Strong Knowledge in Big Data Technologies, Databricks, AWS Services, and PySpark, thorough in one or more programming languages like Python.</li>
 <li>Proven experience with AWS or Azure cloud services for data storage, processing, and analytics.</li>
 <li>Expertise in workflow management tools like Airflow and container orchestration systems such as Kubernetes.</li>
 <li>Strong knowledge of SQL and NoSQL databases, as well as data modeling and schema design principles.</li>
 <li>Familiarity with healthcare data standards, terminologies, and regulations, such as HIPAA and GDPR, is highly desirable.</li>
 <li>Excellent problem-solving, communication, and collaboration skills, with the ability to work effectively in cross-functional teams.</li>
 <li>Demonstrated ability to manage multiple projects, prioritize tasks, and meet deadlines in a fast-paced environment.</li>
 <li>A strong desire to learn, adapt, and contribute to a rapidly evolving data landscape.</li>
</ul>
<p>Job Type: Contract</p>
<p>Pay: From &#x24;60.00 per hour</p>
<p>Benefits:</p>
<ul>
 <li>401(k)</li>
 <li>401(k) matching</li>
 <li>Dental insurance</li>
 <li>Health insurance</li>
 <li>Vision insurance</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
 <li>Monday to Friday</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Python: 5 years (Preferred)</li>
 <li>data storage, processing, and analytics: 6 years (Preferred)</li>
 <li>SQL and NoSQL: 6 years (Preferred)</li>
 <li>healthcare: 5 years (Preferred)</li>
 <li>Airflow: 5 years (Preferred)</li>
 <li>Kubernetes: 6 years (Preferred)</li>
 <li>Docker: 6 years (Preferred)</li>
 <li>ETL: 5 years (Preferred)</li>
 <li>data engineering: 8 years (Preferred)</li>
 <li>Apache Spark &amp; Snowflake: 6 years (Preferred)</li>
 <li>Big data: 6 years (Preferred)</li>
 <li>Databricks: 5 years (Preferred)</li>
 <li>AWS Services: 5 years (Preferred)</li>
 <li>PySpark: 5 years (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,"2865c3b2b6437bd7",,"Contract",,"Remote","Cloud Data Engineer","7 days ago","2023-10-18T11:52:25.562Z",,,"From $60 an hour","2023-10-25T11:52:25.564Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=2865c3b2b6437bd7&from=jasx&tk=1hdjaqvhcirm6800&vjs=3"
"LaTronic Solutions LLC","DATA ENGINEER
***Candidate Must have an Active DoD Secret clearance***
Responsible for the engineering of big data solutions and multi-tiered data environments. Experience with large scale big data deployments in Government and large commercial environments preferred.
Key requirements include:

 Leads initiatives utilizing big data solutions to provide actionable insights for addressing strategic and tactical mission objectives.
 Experience with Hadoop-based technologies (Cloudera, Hortonworks, MapReduce, Hive, HDFS).
 Experience with NoSQL technologies (e.g., MongoDB, Cassandra) (d) Ability to design data models using Enterprise Data modeling tools (ERWin, Visio).
 Builds high-performance algorithms, prototypes, and data models using required programing languages (e.g., Python, C/C++, Java, Perl, Scala).
 Analyzes and develops data set processes for data ingestion, modeling, mining. Experience integrating Big Data solutions with SAP technologies.

Minimum Experience:
Senior Level: 5+ years of relevant experience.
Must be experienced in the following or equivalent technologies:

 Python
 Flask, FastAPI, or other microweb framework equivalent
 An Object Relational Mapping toolkit like SQLAlchemy or equivalent
 Github
 Data analysis libraries such as Pandas.
 AWS S3 - Used as code triggers and intermediate result storage
 CloudTrail - Used to analyze logs
 Lambda - Used as serverless compute for small functions
 ECS Fargate - Used as serverless compute for large functions. Infrastructure:
 Terraform - Used to manage AWS infrastructure.

Minimum Education:
Undergraduate degree required.
Required:
Must have DoD Secret clearance
Must possess IT-II security clearance or have a current National Agency Check with Local Agency Check and Credit Check (NACLC
Job Types: Full-time, Contract
Pay: From $130,000.00 per year
Compensation package:

 1099 contract

Experience level:

 5 years

Schedule:

 8 hour shift

Experience:

 ETL: 4 years (Preferred)
 Big data: 4 years (Preferred)
 Data science: 4 years (Preferred)

Security clearance:

 Secret (Required)

Ability to Commute:

 Remote (Preferred)

Work Location: Remote","<p><b>DATA ENGINEER</b></p>
<p><b>***Candidate Must have an Active DoD Secret clearance***</b></p>
<p>Responsible for the engineering of big data solutions and multi-tiered data environments. Experience with large scale big data deployments in Government and large commercial environments preferred.</p>
<p>Key requirements include:</p>
<ul>
 <li>Leads initiatives utilizing big data solutions to provide actionable insights for addressing strategic and tactical mission objectives.</li>
 <li>Experience with Hadoop-based technologies (Cloudera, Hortonworks, MapReduce, Hive, HDFS).</li>
 <li>Experience with NoSQL technologies (e.g., MongoDB, Cassandra) (d) Ability to design data models using Enterprise Data modeling tools (ERWin, Visio).</li>
 <li>Builds high-performance algorithms, prototypes, and data models using required programing languages (e.g., Python, C/C++, Java, Perl, Scala).</li>
 <li>Analyzes and develops data set processes for data ingestion, modeling, mining. Experience integrating Big Data solutions with SAP technologies.</li>
</ul>
<p>Minimum Experience:</p>
<p>Senior Level: 5+ years of relevant experience.</p>
<p>Must be experienced in the following or equivalent technologies:</p>
<ul>
 <li>Python</li>
 <li>Flask, FastAPI, or other microweb framework equivalent</li>
 <li>An Object Relational Mapping toolkit like SQLAlchemy or equivalent</li>
 <li>Github</li>
 <li>Data analysis libraries such as Pandas.</li>
 <li>AWS S3 - Used as code triggers and intermediate result storage</li>
 <li>CloudTrail - Used to analyze logs</li>
 <li>Lambda - Used as serverless compute for small functions</li>
 <li>ECS Fargate - Used as serverless compute for large functions. Infrastructure:</li>
 <li>Terraform - Used to manage AWS infrastructure.</li>
</ul>
<p>Minimum Education:</p>
<p>Undergraduate degree required.</p>
<p>Required:</p>
<p>Must have DoD Secret clearance</p>
<p>Must possess IT-II security clearance or have a current National Agency Check with Local Agency Check and Credit Check (NACLC</p>
<p>Job Types: Full-time, Contract</p>
<p>Pay: From &#x24;130,000.00 per year</p>
<p>Compensation package:</p>
<ul>
 <li>1099 contract</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>5 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
</ul>
<p>Experience:</p>
<ul>
 <li>ETL: 4 years (Preferred)</li>
 <li>Big data: 4 years (Preferred)</li>
 <li>Data science: 4 years (Preferred)</li>
</ul>
<p>Security clearance:</p>
<ul>
 <li>Secret (Required)</li>
</ul>
<p>Ability to Commute:</p>
<ul>
 <li>Remote (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,"d9235c7fdf0bf5f3",,"Full-time","Contract","Remote","Data Engineer","11 days ago","2023-10-14T11:52:24.910Z",,,"From $130,000 a year","2023-10-25T11:52:24.911Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=d9235c7fdf0bf5f3&from=jasx&tk=1hdjaqvhcirm6800&vjs=3"
"Info Origin, Inc.","Requirements:

 Bachelors or equivalent degree Computer Science, Data Science, Statistics or another relevant quantitative field
 5+ years as a data engineer
 Sound Python and SQL skills with ability to query and analyze data, understand complexity and data structures Experience with data and analytics technology, including but not limited to Hadoop, Spark, Java, Python, R, ElasticSearch, and others
 Familiarity with relational database concepts
 Detail-oriented, analytical, and inquisitive Good communication skills
 Highly organized with strong time-management skills
 Ability to work independently and collaborate well with others
 Ability to affect smooth organizational transformations

Job Type: Contract
Experience level:

 5 years

Work Location: Remote","<p><b>Requirements:</b></p>
<ul>
 <li>Bachelors or equivalent degree Computer Science, Data Science, Statistics or another relevant quantitative field</li>
 <li>5+ years as a data engineer</li>
 <li>Sound Python and SQL skills with ability to query and analyze data, understand complexity and data structures Experience with data and analytics technology, including but not limited to Hadoop, Spark, Java, Python, R, ElasticSearch, and others</li>
 <li>Familiarity with relational database concepts</li>
 <li>Detail-oriented, analytical, and inquisitive Good communication skills</li>
 <li>Highly organized with strong time-management skills</li>
 <li>Ability to work independently and collaborate well with others</li>
 <li>Ability to affect smooth organizational transformations</li>
</ul>
<p>Job Type: Contract</p>
<p>Experience level:</p>
<ul>
 <li>5 years</li>
</ul>
<p>Work Location: Remote</p>",,"b75faf0b732502e9",,"Contract",,"Remote","Data Engineer","11 days ago","2023-10-14T11:52:25.844Z",,,,"2023-10-25T11:52:25.846Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=b75faf0b732502e9&from=jasx&tk=1hdjaqvhcirm6800&vjs=3"
"Manrco Inc","Data Engineer
Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative,inclusive, and iterative delivery environment?
We are seeking Data Engineers who are passionate about marrying data with emerging technologies to join our team. A
What You’ll Do:

 Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies


 Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems


 Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Snowflake


 Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community


 Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment


 Perform unit tests and conducting reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance

Basic Qualifications:

 Bachelor’s Degree


 At least 2 years of experience in application development


 At least 1 years of experience in big data technologies (Cassandra, Accumulo, HBase, Spark, Hadoop, HDFS, AVRO, MongoDB, or Zookeeper)

Preferred Qualifications:

 Master's Degree


 3+ years of experience in application development


 1+ year experience working on streaming data applications (Spark Streaming, Kafka, Kinesis, and Flink


 1+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)


 1+ years of experience with Ansible / Terraform


 2+ years of experience with Agile engineering practices


 2+ years in-depth experience with the Hadoop stack (MapReduce, Pig, Hive, Hbase)


 2+ years of experience with NoSQL implementation (Mongo, Cassandra)


 2+ years of experience developing Java based software solutions


 2+ years of experience in at least one scripting language (Python, Perl, JavaScript, Shell)


 2+ years of experience developing software solutions to solve complex business problems


 2+ years of experience with UNIX/Linux including basic commands and shell scripting

Job Type: Contract
Pay: $80.00 per hour
Expected hours: 40 per week
Benefits:

 Health insurance

Compensation package:

 Hourly pay

Experience level:

 5 years
 6 years
 7 years
 8 years

Schedule:

 12 hour shift
 Monday to Friday

Experience:

 Data warehouse: 1 year (Preferred)
 Informatica: 4 years (Preferred)
 Snowflake: 3 years (Preferred)

Work Location: Remote","<p>Data Engineer</p>
<p>Do you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative,inclusive, and iterative delivery environment?</p>
<p>We are seeking Data Engineers who are passionate about marrying data with emerging technologies to join our team. A</p>
<p>What You&#x2019;ll Do:</p>
<ul>
 <li>Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies</li>
</ul>
<ul>
 <li>Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems</li>
</ul>
<ul>
 <li>Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Snowflake</li>
</ul>
<ul>
 <li>Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal &amp; external technology communities, and mentoring other members of the engineering community</li>
</ul>
<ul>
 <li>Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment</li>
</ul>
<ul>
 <li>Perform unit tests and conducting reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance</li>
</ul>
<p>Basic Qualifications:</p>
<ul>
 <li>Bachelor&#x2019;s Degree</li>
</ul>
<ul>
 <li>At least 2 years of experience in application development</li>
</ul>
<ul>
 <li>At least 1 years of experience in big data technologies (Cassandra, Accumulo, HBase, Spark, Hadoop, HDFS, AVRO, MongoDB, or Zookeeper)</li>
</ul>
<p>Preferred Qualifications:</p>
<ul>
 <li>Master&apos;s Degree</li>
</ul>
<ul>
 <li>3+ years of experience in application development</li>
</ul>
<ul>
 <li>1+ year experience working on streaming data applications (Spark Streaming, Kafka, Kinesis, and Flink</li>
</ul>
<ul>
 <li>1+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)</li>
</ul>
<ul>
 <li>1+ years of experience with Ansible / Terraform</li>
</ul>
<ul>
 <li>2+ years of experience with Agile engineering practices</li>
</ul>
<ul>
 <li>2+ years in-depth experience with the Hadoop stack (MapReduce, Pig, Hive, Hbase)</li>
</ul>
<ul>
 <li>2+ years of experience with NoSQL implementation (Mongo, Cassandra)</li>
</ul>
<ul>
 <li>2+ years of experience developing Java based software solutions</li>
</ul>
<ul>
 <li>2+ years of experience in at least one scripting language (Python, Perl, JavaScript, Shell)</li>
</ul>
<ul>
 <li>2+ years of experience developing software solutions to solve complex business problems</li>
</ul>
<ul>
 <li>2+ years of experience with UNIX/Linux including basic commands and shell scripting</li>
</ul>
<p>Job Type: Contract</p>
<p>Pay: &#x24;80.00 per hour</p>
<p>Expected hours: 40 per week</p>
<p>Benefits:</p>
<ul>
 <li>Health insurance</li>
</ul>
<p>Compensation package:</p>
<ul>
 <li>Hourly pay</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>5 years</li>
 <li>6 years</li>
 <li>7 years</li>
 <li>8 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>12 hour shift</li>
 <li>Monday to Friday</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Data warehouse: 1 year (Preferred)</li>
 <li>Informatica: 4 years (Preferred)</li>
 <li>Snowflake: 3 years (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,"7df0841eea60802f",,"Contract",,"Remote","Data Engineer","12 days ago","2023-10-13T11:52:26.457Z",,,"$80 an hour","2023-10-25T11:52:26.460Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=7df0841eea60802f&from=jasx&tk=1hdjaqvhcirm6800&vjs=3"
"Resource Consulting Services","Opening for Data EngineerLocation: RemoteDuration: 12+MonthNeed Minimum 10 Years experienceMust have Linked idMust have Healthcare experience in recent projectSkills:String data engineering experienceMust have ETL- Matillion tool experienceMust have Snowflake experienceMust have Strong SQL ExperienceBODS is good to have
Ofc :: +1 (201)-809-9207 Ext-424Email: sajjad@rconsultinginc.com
Job Type: Contract
Salary: $65.00 - $70.00 per hour
People with a criminal record are encouraged to apply
Experience:

 Informatica: 1 year (Preferred)
 SQL: 1 year (Preferred)
 Data warehouse: 1 year (Preferred)

Work Location: Remote","<p>Opening for Data Engineer<br>Location: Remote<br>Duration: 12+Month<br>Need Minimum 10 Years experience<br>Must have Linked id<br>Must have Healthcare experience in recent project<br>Skills:<br>String data engineering experience<br>Must have ETL- Matillion tool experience<br>Must have Snowflake experience<br>Must have Strong SQL Experience<br>BODS is good to have</p>
<p>Ofc :: +1 (201)-809-9207 Ext-424<br>Email: sajjad@rconsultinginc.com</p>
<p>Job Type: Contract</p>
<p>Salary: &#x24;65.00 - &#x24;70.00 per hour</p>
<p>People with a criminal record are encouraged to apply</p>
<p>Experience:</p>
<ul>
 <li>Informatica: 1 year (Preferred)</li>
 <li>SQL: 1 year (Preferred)</li>
 <li>Data warehouse: 1 year (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,"efbd2611031a4f0d",,"Contract",,"Remote","Data Engineer","12 days ago","2023-10-13T11:52:27.549Z",,,"$65 - $70 an hour","2023-10-25T11:52:27.551Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=efbd2611031a4f0d&from=jasx&tk=1hdjaqvhcirm6800&vjs=3"
"Hire IT People Inc","**Position:** Azure Date Engineer
**Location:** [Remote] / Customer is located in Washington DC.
**Responsibilities:**
1. Provide support for Azure cloud-based applications and infrastructure.
2. Diagnose and troubleshoot technical issues related to Azure cloud services.
3. Collaborate with cross-functional teams to implement best practices and ensure service optimization.
4. Continuously monitor and manage cloud environment to prevent and mitigate risks.
5. Assist client in designing and implementing Azure cloud solutions based on their unique needs by moving forward siting applications into Cloud environment.
6. Keep up to date with the latest Azure updates and features, ensuring optimal utilization and implementation.
7. Offer guidance and recommendations on cost-management strategies within Azure.
8. Contribute to the development of internal tools and processes to enhance Azure cloud management and support.
**Requirements:**
1. Proven experience as a Cloud Support Engineer, preferably with a focus on Azure.
2. Strong knowledge of Azure services, including but not limited to Azure Active Directory, Azure DevOps, Azure Kubernetes Service, and more.
3. Excellent problem-solving skills and the ability to diagnose complex cloud-based issues.
4. Strong verbal and written communication skills.
5. Must be a US citizen.
6. Must successfully pass Level 4 Public Trust verification.
7. Relevant certifications in Azure, such as AZ-104 or AZ-303/304, are a plus.
Job Type: Full-time
Salary: From $140,000.00 per year
Benefits:

 401(k)
 Dental insurance
 Health insurance
 Vision insurance

Work Location: Remote","<p>**Position:** Azure Date Engineer</p>
<p>**Location:** [Remote] / Customer is located in Washington DC.</p>
<p>**Responsibilities:**</p>
<p>1. Provide support for Azure cloud-based applications and infrastructure.</p>
<p>2. Diagnose and troubleshoot technical issues related to Azure cloud services.</p>
<p>3. Collaborate with cross-functional teams to implement best practices and ensure service optimization.</p>
<p>4. Continuously monitor and manage cloud environment to prevent and mitigate risks.</p>
<p>5. Assist client in designing and implementing Azure cloud solutions based on their unique needs by moving forward siting applications into Cloud environment.</p>
<p>6. Keep up to date with the latest Azure updates and features, ensuring optimal utilization and implementation.</p>
<p>7. Offer guidance and recommendations on cost-management strategies within Azure.</p>
<p>8. Contribute to the development of internal tools and processes to enhance Azure cloud management and support.</p>
<p>**Requirements:**</p>
<p>1. Proven experience as a Cloud Support Engineer, preferably with a focus on Azure.</p>
<p>2. Strong knowledge of Azure services, including but not limited to Azure Active Directory, Azure DevOps, Azure Kubernetes Service, and more.</p>
<p>3. Excellent problem-solving skills and the ability to diagnose complex cloud-based issues.</p>
<p>4. Strong verbal and written communication skills.</p>
<p>5. Must be a US citizen.</p>
<p>6. Must successfully pass Level 4 Public Trust verification.</p>
<p>7. Relevant certifications in Azure, such as AZ-104 or AZ-303/304, are a plus.</p>
<p>Job Type: Full-time</p>
<p>Salary: From &#x24;140,000.00 per year</p>
<p>Benefits:</p>
<ul>
 <li>401(k)</li>
 <li>Dental insurance</li>
 <li>Health insurance</li>
 <li>Vision insurance</li>
</ul>
<p>Work Location: Remote</p>",,"649f0d12515402f8",,"Full-time",,"Remote","Azure Data Engineer","12 days ago","2023-10-13T11:52:29.633Z",,,"From $140,000 a year","2023-10-25T11:52:29.634Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=649f0d12515402f8&from=jasx&tk=1hdjaqvhcirm6800&vjs=3"
"Amaze systems","Location – Alpharetta, GA/Armonk, NY/Boston, MA/Fort Wayne, IN/Miami, FL/NYC, NY/Windsor, CT/ Washington, DC 
Job Title - Data Engineer
6 months C2h with client
Role Description:
Looking for a data engineer to design, develop and maintain pipelines and workflows and create analytics to digitally transform the current NA Accident and Health reporting and business decision processes.
Responsibilities
Development and support of data pipelines that produce data assets for various A&H workstreams including UW, UA, Actuarial, Claims
Handle data pipelines while testing for data curation, parsing, cleaning, transformation and enrichment of data
Work with fundamentals of data processing, data pipeline, data lineage and ETL (Extract-Transform-Load) methodologies
Implement the project according to the Software Development Life Cycle (SDLC) and programming by using fast paced agile methodology, involving task completion, user stories
Utilize knowledge of database management system software, object oriented programming development, system architecture and components and various programming languages
Review and analyze business workflows and user data needs
Design and implement business performance dashboards
Write customized queries/programs to generate automatic periodical reports highlighting all the Key Performance Indicators (KPIs)
Build applications using SQL and/or Python scripts to manipulate data, monitor and help to improve data quality
Design, build and maintain end-to-end data solutions supporting our processes with the right data architecture
Have working knowledge of Apache Spark, big data processing and building products on distributed cluster-computing framework
Construct workflow charts and diagrams and writing specifications.
Documentation of end-to-end data pipeline process.
Documentation of data assets for information management purposes.
Ad hoc team / business support as needed.
Exploration and evaluation of new technologies and platforms.
Requirements
Bachelors or equivalent degree Computer Science, Data Science, Statistics or another relevant quantitative field
5+ years as a data engineer
Sound Python and SQL skills with ability to query and analyze data, understand complexity and data structuresExperience with data and analytics technology, including but not limited to Hadoop, Spark, Java, Python, R, ElasticSearch, and others
Familiarity with relational database concepts
Detail-oriented, analytical, and inquisitiveGood communication skills
Highly organized with strong time-management skills
Ability to work independently and collaborate well with others
Ability to affect smooth organizational transformations
Job Type: Contract
Salary: From $60.00 per hour
Experience level:

 11+ years

Experience:

 Data Engineer: 10 years (Required)
 Python: 7 years (Required)
 Insurance Domain: 5 years (Required)

Work Location: Remote","<p><b>Location &#x2013; Alpharetta, GA/Armonk, NY/Boston, MA/Fort Wayne, IN/Miami, FL/NYC, NY/Windsor, CT/ Washington, DC </b></p>
<p><b>Job Title - Data Engineer</b></p>
<p><b>6 months C2h with client</b></p>
<p><b>Role Description:</b></p>
<p>Looking for a data engineer to design, develop and maintain pipelines and workflows and create analytics to digitally transform the current NA Accident and Health reporting and business decision processes.</p>
<p>Responsibilities</p>
<p>Development and support of data pipelines that produce data assets for various A&amp;H workstreams including UW, UA, Actuarial, Claims</p>
<p>Handle data pipelines while testing for data curation, parsing, cleaning, transformation and enrichment of data</p>
<p>Work with fundamentals of data processing, data pipeline, data lineage and ETL (Extract-Transform-Load) methodologies</p>
<p>Implement the project according to the Software Development Life Cycle (SDLC) and programming by using fast paced agile methodology, involving task completion, user stories</p>
<p>Utilize knowledge of database management system software, object oriented programming development, system architecture and components and various programming languages</p>
<p>Review and analyze business workflows and user data needs</p>
<p>Design and implement business performance dashboards</p>
<p>Write customized queries/programs to generate automatic periodical reports highlighting all the Key Performance Indicators (KPIs)</p>
<p>Build applications using SQL and/or Python scripts to manipulate data, monitor and help to improve data quality</p>
<p>Design, build and maintain end-to-end data solutions supporting our processes with the right data architecture</p>
<p>Have working knowledge of Apache Spark, big data processing and building products on distributed cluster-computing framework</p>
<p>Construct workflow charts and diagrams and writing specifications.</p>
<p>Documentation of end-to-end data pipeline process.</p>
<p>Documentation of data assets for information management purposes.</p>
<p>Ad hoc team / business support as needed.</p>
<p>Exploration and evaluation of new technologies and platforms.</p>
<p>Requirements</p>
<p>Bachelors or equivalent degree Computer Science, Data Science, Statistics or another relevant quantitative field</p>
<p>5+ years as a data engineer</p>
<p>Sound Python and SQL skills with ability to query and analyze data, understand complexity and data structures<br>Experience with data and analytics technology, including but not limited to Hadoop, Spark, Java, Python, R, ElasticSearch, and others</p>
<p>Familiarity with relational database concepts</p>
<p>Detail-oriented, analytical, and inquisitive<br>Good communication skills</p>
<p>Highly organized with strong time-management skills</p>
<p>Ability to work independently and collaborate well with others</p>
<p>Ability to affect smooth organizational transformations</p>
<p>Job Type: Contract</p>
<p>Salary: From &#x24;60.00 per hour</p>
<p>Experience level:</p>
<ul>
 <li>11+ years</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Data Engineer: 10 years (Required)</li>
 <li>Python: 7 years (Required)</li>
 <li>Insurance Domain: 5 years (Required)</li>
</ul>
<p>Work Location: Remote</p>",,"22cd2b66172d9b52",,"Contract",,"Remote","Data Engineer","10 days ago","2023-10-15T11:52:30.211Z",,,"From $60 an hour","2023-10-25T11:52:30.212Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=22cd2b66172d9b52&from=jasx&tk=1hdjaqvhcirm6800&vjs=3"
"Stand Up Wireless","Senior Data Engineer / Analyst
Who We Are
StandUp Wireless is a wireless service provider focused on leveraging technical insights and innovative solutions to keep our communities connected. The company offers an energetic, entrepreneurial atmosphere where employees work in teams to help achieve goals. StandUp is a company built on trust that values individuals with personal initiative and creative solutions. We are a small company that makes a huge impact.
What You Will Be Doing
This role will report to the Director of Data Services. We are seeking an experienced and highly skilled Sr. AWS Data Engineer/Analyst to join our growing team. The ideal candidate will have a strong background in AWS services, data engineering, data warehousing, and data analysis. This person will play a critical role in transforming raw data into actionable insights to drive business growth.
Key responsibilities include:

 Design, develop, and maintain scalable, secure, and cost-effective data solutions on the AWS platform.
 Collaborate with cross-functional teams to gather and understand business requirements, translating them into technical solutions and analytical insights.
 Architect and implement data pipelines using services such as Glue, Matillion, Kinesis, Lambda, and Step Functions.
 Work with various AWS data storage solutions such as S3, RDS, Redshift, and Neptune, ensuring optimal performance and data integrity.
 Utilize AWS data analytics services like Athena, QuickSight, and EMR to analyze and visualize large datasets, providing insights to stakeholders.
 Develop and maintain advanced data models, statistical analyses, and machine learning algorithms to uncover trends, patterns, and correlations in the data warehouse and other reporting environments.
 Ensure data security, privacy, and compliance by implementing best practices and industry standards, including data encryption, access controls, and auditing.
 Monitor, troubleshoot, and optimize data solutions to maintain high availability, performance, and cost-efficiency.
 Stay current on the latest AWS services, data engineering, and data analysis technologies, advocating for their adoption where appropriate.
 Mentor and guide junior data engineers and analysts, fostering a culture of continuous learning and improvement.
 Document and communicate technical designs, best practices, and analytical findings to both technical and non-technical stakeholders. Collaborate with cross-functional teams to ensure that solutions align with objectives and requirements.

How You Qualify
Education and Experience:

 Bachelor’s degree in computer science, Engineering, Data Science, or a related field.
 5+ years of experience in data engineering, data warehousing, and data analysis with a focus on AWS services.
 Strong knowledge of SQL, Python, and/or Java.
 Experience with big data technologies like Hadoop, Spark, and Kafka.
 Deep understanding of data modeling, ETL processes, data warehousing concepts, and statistical analysis techniques.
 Familiarity with AWS security best practices, including IAM, KMS, and Cognito.
 Excellent problem-solving skills and the ability to work independently or as part of a team.
 Strong communication skills, both written and verbal, with the ability to present complex technical concepts and analytical findings to non-technical audiences.

Preferred Qualifications:

 AWS Certified Data Analytics - Specialty or AWS Certified Big Data - Specialty certification.
 Experience with machine learning and AI technologies on AWS, such as SageMaker and Forecast.
 Familiarity with DevOps principles and tools, such as Git.

What We Offer
A culture that thrives by empowering our people and respecting everyone’s work-life balance. We work hard but have fun! Members of the StandUp Wireless family engage in a highly collaborative and fast paced environment that values their opinions in every step of the creative process.
In addition to the inviting culture and collaborative environment, StandUp Wireless offers great benefits:

 Competitive Compensation
 Bonus Incentive Programs
 Medical, Dental and Vision Benefits Packages
 401K Plan
 Company paid Life and Long-term Disability Benefits
 Voluntary Benefits and Wellness Programs
 Home Office Monthly Stipends
 Paid vacation, sick leave and holidays.

How to Apply
PLEASE SUBMIT RESUME AND DESIRED SALARY TO BE CONSIDERED FOR THIS ROLE
Candidates must be legally eligible to work in the US for any employer.
StandUp has moved all operations to a remote environment; this position will work remotely
We are an Equal Opportunity Employer that values diversity in the workplace.
Job Type: Full-time
Pay: $95,000.00 - $115,000.00 per year
Benefits:

 401(k)
 Dental insurance
 Health insurance
 Paid time off
 Parental leave
 Vision insurance
 Work from home

Experience level:

 3 years
 4 years
 5 years
 6 years

Schedule:

 Monday to Friday

Application Question(s):

 PLEASE NOTE THAT IF YOU DO NOT RESPOND TO EMPLOYER-WRITTEN QUESTIONS, YOUR APPLICATION IS NOT COMPLETE AND MAY NOT BE CONSIDERED.
 1. How do you approach optimizing the performance and cost-efficiency of data storage and processing on AWS? Please provide specific examples of the techniques and tools you have used to achieve this in your previous projects.
 2. In your experience as a data analyst, can you provide an example of a complex analysis or data-driven insight that significantly impacted a business decision or outcome? What analytical techniques and tools did you use to arrive at your conclusion?
 3. Security and compliance are critical aspects of data engineering and analysis. Can you share an experience where you had to implement or improve data security, privacy, and compliance measures in an AWS environment? Please detail the specific AWS services, features, or best practices you employed.
 4. In the context of data engineering and analysis, how do you stay up to date with the latest advancements in AWS services and technologies, as well as industry best practices? Can you share a recent example of a new AWS service, feature, or technique you have adopted in a project to improve its efficiency or effectiveness?
 5. Can you describe a project where you designed and implemented an end-to-end data pipeline using AWS services? Please outline the key AWS services used, data sources, and the overall architecture.

Work Location: Remote","<p><b>Senior Data Engineer / Analyst</b></p>
<p><b>Who We Are</b></p>
<p>StandUp Wireless is a wireless service provider focused on leveraging technical insights and innovative solutions to keep our communities connected. The company offers an energetic, entrepreneurial atmosphere where employees work in teams to help achieve goals. StandUp is a company built on trust that values individuals with personal initiative and creative solutions. We are a small company that makes a huge impact.</p>
<p><b>What You Will Be Doing</b></p>
<p>This role will report to the Director of Data Services. We are seeking an experienced and highly skilled Sr. AWS Data Engineer/Analyst to join our growing team. The ideal candidate will have a strong background in AWS services, data engineering, data warehousing, and data analysis. This person will play a critical role in transforming raw data into actionable insights to drive business growth.</p>
<p><b>Key responsibilities include:</b></p>
<ul>
 <li>Design, develop, and maintain scalable, secure, and cost-effective data solutions on the AWS platform.</li>
 <li>Collaborate with cross-functional teams to gather and understand business requirements, translating them into technical solutions and analytical insights.</li>
 <li>Architect and implement data pipelines using services such as Glue, Matillion, Kinesis, Lambda, and Step Functions.</li>
 <li>Work with various AWS data storage solutions such as S3, RDS, Redshift, and Neptune, ensuring optimal performance and data integrity.</li>
 <li>Utilize AWS data analytics services like Athena, QuickSight, and EMR to analyze and visualize large datasets, providing insights to stakeholders.</li>
 <li>Develop and maintain advanced data models, statistical analyses, and machine learning algorithms to uncover trends, patterns, and correlations in the data warehouse and other reporting environments.</li>
 <li>Ensure data security, privacy, and compliance by implementing best practices and industry standards, including data encryption, access controls, and auditing.</li>
 <li>Monitor, troubleshoot, and optimize data solutions to maintain high availability, performance, and cost-efficiency.</li>
 <li>Stay current on the latest AWS services, data engineering, and data analysis technologies, advocating for their adoption where appropriate.</li>
 <li>Mentor and guide junior data engineers and analysts, fostering a culture of continuous learning and improvement.</li>
 <li>Document and communicate technical designs, best practices, and analytical findings to both technical and non-technical stakeholders. Collaborate with cross-functional teams to ensure that solutions align with objectives and requirements.</li>
</ul>
<p><b>How You Qualify</b></p>
<p>Education and Experience:</p>
<ul>
 <li>Bachelor&#x2019;s degree in computer science, Engineering, Data Science, or a related field.</li>
 <li>5+ years of experience in data engineering, data warehousing, and data analysis with a focus on AWS services.</li>
 <li>Strong knowledge of SQL, Python, and/or Java.</li>
 <li>Experience with big data technologies like Hadoop, Spark, and Kafka.</li>
 <li>Deep understanding of data modeling, ETL processes, data warehousing concepts, and statistical analysis techniques.</li>
 <li>Familiarity with AWS security best practices, including IAM, KMS, and Cognito.</li>
 <li>Excellent problem-solving skills and the ability to work independently or as part of a team.</li>
 <li>Strong communication skills, both written and verbal, with the ability to present complex technical concepts and analytical findings to non-technical audiences.</li>
</ul>
<p>Preferred Qualifications:</p>
<ul>
 <li>AWS Certified Data Analytics - Specialty or AWS Certified Big Data - Specialty certification.</li>
 <li>Experience with machine learning and AI technologies on AWS, such as SageMaker and Forecast.</li>
 <li>Familiarity with DevOps principles and tools, such as Git.</li>
</ul>
<p><b>What We Offer</b></p>
<p>A culture that thrives by empowering our people and respecting everyone&#x2019;s work-life balance. We work hard but have fun! Members of the StandUp Wireless family engage in a highly collaborative and fast paced environment that values their opinions in every step of the creative process.</p>
<p>In addition to the inviting culture and collaborative environment, StandUp Wireless offers great benefits:</p>
<ul>
 <li>Competitive Compensation</li>
 <li>Bonus Incentive Programs</li>
 <li>Medical, Dental and Vision Benefits Packages</li>
 <li>401K Plan</li>
 <li>Company paid Life and Long-term Disability Benefits</li>
 <li>Voluntary Benefits and Wellness Programs</li>
 <li>Home Office Monthly Stipends</li>
 <li>Paid vacation, sick leave and holidays.</li>
</ul>
<p><b>How to Apply</b></p>
<p><i><b>PLEASE SUBMIT RESUME AND DESIRED SALARY TO BE CONSIDERED FOR THIS ROLE</b></i></p>
<p>Candidates must be legally eligible to work in the US for any employer.</p>
<p>StandUp has moved all operations to a remote environment; this position will work remotely</p>
<p><b>We are an Equal Opportunity Employer that values diversity in the workplace.</b></p>
<p>Job Type: Full-time</p>
<p>Pay: &#x24;95,000.00 - &#x24;115,000.00 per year</p>
<p>Benefits:</p>
<ul>
 <li>401(k)</li>
 <li>Dental insurance</li>
 <li>Health insurance</li>
 <li>Paid time off</li>
 <li>Parental leave</li>
 <li>Vision insurance</li>
 <li>Work from home</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>3 years</li>
 <li>4 years</li>
 <li>5 years</li>
 <li>6 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>Monday to Friday</li>
</ul>
<p>Application Question(s):</p>
<ul>
 <li>PLEASE NOTE THAT IF YOU DO NOT RESPOND TO EMPLOYER-WRITTEN QUESTIONS, YOUR APPLICATION IS NOT COMPLETE AND MAY NOT BE CONSIDERED.</li>
 <li>1. How do you approach optimizing the performance and cost-efficiency of data storage and processing on AWS? Please provide specific examples of the techniques and tools you have used to achieve this in your previous projects.</li>
 <li>2. In your experience as a data analyst, can you provide an example of a complex analysis or data-driven insight that significantly impacted a business decision or outcome? What analytical techniques and tools did you use to arrive at your conclusion?</li>
 <li>3. Security and compliance are critical aspects of data engineering and analysis. Can you share an experience where you had to implement or improve data security, privacy, and compliance measures in an AWS environment? Please detail the specific AWS services, features, or best practices you employed.</li>
 <li>4. In the context of data engineering and analysis, how do you stay up to date with the latest advancements in AWS services and technologies, as well as industry best practices? Can you share a recent example of a new AWS service, feature, or technique you have adopted in a project to improve its efficiency or effectiveness?</li>
 <li>5. Can you describe a project where you designed and implemented an end-to-end data pipeline using AWS services? Please outline the key AWS services used, data sources, and the overall architecture.</li>
</ul>
<p>Work Location: Remote</p>",,"148f0aaf86afa309",,"Full-time",,"Remote","Senior Data Engineer / Analyst","30+ days ago","2023-09-25T11:52:33.313Z","3.9","53","$95,000 - $115,000 a year","2023-10-25T11:52:33.315Z","US","remote","data engineer","https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0BjCBRdhP65IZiSCQvFiT0PZro7cNEJWrEbXymqjdX66e9d72a1Jy_HOjQ52dpIL8AkgL1yEfv-9JIZeBJ9C14brnAEGLkO6K6sWaedQUVCxnbMIJaVDUIPh8mfT27FARVBODpbAVwSfmy_zrADg15OsjE8M-IxUfpX_GdsWbCFj2i6C6Gpz9S0bMQmkU7w4RjUCCQp1MqPA86CVgH6wITZ3LRx5y2hohzytIdTLzym3nburhq0kVMPFCrwJwIN7WIeTm56kEHGhkZyL4Y021zCmgJi8G0sIz4U4czm1nMtOAbNcYU4oWKHmvYOm-DnA256d24euN8CK3EJdO4P_Xsc-1kRiup4cPcyxF5kCsKL-qO0-ylsrozwg_1FZ9yfP_7bxov8Kzr--8cboQNkNUBej8uVzuAXNslbHp_tLuuZETZCXhiIwCbdYJx36wZ7tl2CfZ4Yq79RKQeEMi1LIA-zR01qSoQa8RIL74YQmtrmyoyVGEGIZcWCMWdHbYeE8zwtAjhE3l6UgyCxUxz8-8tZmVgBQ-Kg6M3MMZB8Qt4oH4V8KDp2XnTPYPOroF47bG9WQ7kQCggiIwbqjbZT8kXDjqk8NIQt7Gf8fGAabCQ26DHLTKH8Zok2&xkcb=SoDC-_M3JzdhoWwdyR0CbzkdCdPP&p=9&fvj=1&vjs=3&jsa=8644&tk=1hdjarjmuje3s800&from=jasx&wvign=1"
"Density","About Density 
      Density’s mission is to measure and improve our footprint on the world. 
      We help companies understand how their workplaces are used. At Density, we build one of the most advanced people sensing systems in the world. Density can tell you how many people are in any room in near real-time, with very high degrees of accuracy and without invading privacy. 
      We translate that data into actionable, opinionated insights that help companies increase the financial and experiential performance of any workplace. Today, we work with companies ranging from Fortune 1000 to high growth such as Uber, Pinterest, Shopify and Okta, occupying more than a billion square feet worldwide. 
      The result: lower emissions, less waste, better access, safer buildings and better designed cities. It is a long term pursuit and one we could use your help achieving. That’s where you come in. 
      The Opportunity: 
      We are seeking a highly skilled and experienced Senior Data Pipeline Engineer to join our platform software team. The team's mission revolves around processing, storing, and presenting real-time analytics derived from thousands of sensors, offering insights into workplace utilization. As a Senior Data Pipeline Engineer at Density, you will be instrumental in designing, implementing, and optimizing our data pipelines. Your expertise will be crucial in transforming raw sensor data into actionable insights that drive informed decision-making across the workplace organization. 
      In this role you will: 
      
      Design, develop, and maintain scalable and robust data pipelines, ensuring efficient data extraction, transformation, and loading (ETL) processes. 
      Optimize and fine-tune existing data pipelines for performance, reliability, and scalability. 
      Work with large data-streams to extract meaningful analytics and insights. 
      Implement best practices for data governance, data security, and data quality assurance. 
      Collaborate with data scientists and analysts to provide them with clean, structured data for analysis and reporting. 
      Articulate your contributions with local French and US overseas platform team members 
      Find satisfaction in working remotely from your home. 
      
     The ideal candidate will have: 
      
      Bachelor's or Master's degree in Computer Science, Engineering, or a related field. 
      5+ years of experience as a Data Engineer, with a proven track record of designing and implementing complex data solutions. 
      Proficiency in Python. 
      In-depth knowledge of SQL relational language 
      Excellent problem-solving skills and the ability to troubleshoot complex data issues. 
      Strong communication skills and the ability to work effectively in a collaborative team environment. 
      
     Nice-To-Have: 
      
      Knowledge of Rust and Go is a plus. 
      Strong experience with ETL processes, data modeling, and data integration techniques. 
      Hands-on experience with big data technologies such as Spark, and Kafka. 
      Familiarity with AWS cloud platform and their data services. 
      Experience with data warehousing solutions (e.g Clickhouse, BigQuery) is a plus. 
      
     We offer: 
      
      A company full of fun, smart, talented and legitimately kind teammates. Our culture powers everything we do and we work hard to nurture it by bringing on the right humans. 
      A team hailing from innovators like Apple, LinkedIn, Stripe, Meraki, Flexport, WeWork, NASA & beyond. 
      $227M raised from investors including Kleiner Perkins, Founders Fund and Upfront Ventures. 
      The chance to change the built world as we know it. 
      
     Density provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. 
      This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. 
      Job Compensation Range: 
      Salary Range: €60,000.00 - €90,000.00 
      Preferred Primary Location: Paris, France
      
      
      An important note on salary: 
      The annual pay range for this position is based on the preferred primary location of the role which is listed above. If you are applying to this role at a location that is not the preferred primary location, please keep in mind the salary range will vary and may fall outside of what is listed. Only in truly rare and exceptional circumstances, where an external candidate has experience, credentials or expertise that far exceed those required or expected for the position, would the Density consider paying a salary or rate near the higher end of the range. Equity may be provided as part of the compensation package, in addition to a full range of medical, financial, and/or other benefits, depending on the position offered.","<div>
 <div>
  <div>
   <div>
    <div>
     <p><b>About Density</b></p> 
     <p> Density&#x2019;s mission is to measure and improve our footprint on the world.</p> 
     <p> We help companies understand how their workplaces are used. At Density, we build one of the most advanced people sensing systems in the world. Density can tell you how many people are in any room in near real-time, with very high degrees of accuracy and without invading privacy.</p> 
     <p> We translate that data into actionable, opinionated insights that help companies increase the financial and experiential performance of any workplace. Today, we work with companies ranging from Fortune 1000 to high growth such as Uber, Pinterest, Shopify and Okta, occupying more than a billion square feet worldwide.</p> 
     <p> The result: lower emissions, less waste, better access, safer buildings and better designed cities. It is a long term pursuit and one we could use your help achieving. That&#x2019;s where you come in.</p> 
     <p><b> The Opportunity:</b></p> 
     <p> We are seeking a highly skilled and experienced Senior Data Pipeline Engineer to join our platform software team. The team&apos;s mission revolves around processing, storing, and presenting real-time analytics derived from thousands of sensors, offering insights into workplace utilization. As a Senior Data Pipeline Engineer at Density, you will be instrumental in designing, implementing, and optimizing our data pipelines. Your expertise will be crucial in transforming raw sensor data into actionable insights that drive informed decision-making across the workplace organization.</p> 
     <p><b> In this role you will:</b></p> 
     <ul> 
      <li>Design, develop, and maintain scalable and robust data pipelines, ensuring efficient data extraction, transformation, and loading (ETL) processes.</li> 
      <li>Optimize and fine-tune existing data pipelines for performance, reliability, and scalability.</li> 
      <li>Work with large data-streams to extract meaningful analytics and insights.</li> 
      <li>Implement best practices for data governance, data security, and data quality assurance.</li> 
      <li>Collaborate with data scientists and analysts to provide them with clean, structured data for analysis and reporting.</li> 
      <li>Articulate your contributions with local French and US overseas platform team members</li> 
      <li>Find satisfaction in working remotely from your home.</li> 
     </ul> 
     <p><b>The ideal candidate will have:</b></p> 
     <ul> 
      <li>Bachelor&apos;s or Master&apos;s degree in Computer Science, Engineering, or a related field.</li> 
      <li>5+ years of experience as a Data Engineer, with a proven track record of designing and implementing complex data solutions.</li> 
      <li>Proficiency in Python.</li> 
      <li>In-depth knowledge of SQL relational language</li> 
      <li>Excellent problem-solving skills and the ability to troubleshoot complex data issues.</li> 
      <li>Strong communication skills and the ability to work effectively in a collaborative team environment.</li> 
     </ul> 
     <p><b>Nice-To-Have:</b></p> 
     <ul> 
      <li>Knowledge of Rust and Go is a plus.</li> 
      <li>Strong experience with ETL processes, data modeling, and data integration techniques.</li> 
      <li>Hands-on experience with big data technologies such as Spark, and Kafka.</li> 
      <li>Familiarity with AWS cloud platform and their data services.</li> 
      <li>Experience with data warehousing solutions (e.g Clickhouse, BigQuery) is a plus.</li> 
     </ul> 
     <p><b>We offer:</b></p> 
     <ul> 
      <li>A company full of fun, smart, talented and legitimately kind teammates. Our culture powers everything we do and we work hard to nurture it by bringing on the right humans.</li> 
      <li>A team hailing from innovators like Apple, LinkedIn, Stripe, Meraki, Flexport, WeWork, NASA &amp; beyond.</li> 
      <li>&#x24;227M raised from investors including Kleiner Perkins, Founders Fund and Upfront Ventures.</li> 
      <li>The chance to change the built world as we know it.</li> 
     </ul> 
     <p>Density provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.</p> 
     <p> This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.</p> 
     <p><b> Job Compensation Range:</b></p> 
     <p> Salary Range: &#x20ac;60,000.00 - &#x20ac;90,000.00</p> 
     <p> Preferred Primary Location: Paris, France</p>
     <br> 
     <p></p> 
     <p><i> An important note on salary:</i></p> 
     <p> The annual pay range for this position is based on the preferred primary location of the role which is listed above. If you are applying to this role at a location that is not the preferred primary location, please keep in mind the salary range will vary and may fall outside of what is listed. Only in truly rare and exceptional circumstances, where an external candidate has experience, credentials or expertise that far exceed those required or expected for the position, would the Density consider paying a salary or rate near the higher end of the range. Equity may be provided as part of the compensation package, in addition to a full range of medical, financial, and/or other benefits, depending on the position offered.</p>
    </div>
   </div>
  </div>
 </div>
</div>",,"dac26e24aca48718",,,,"Remote","Senior Data Engineer","30+ days ago","2023-09-25T11:52:35.941Z","3.3","3",,"2023-10-25T11:52:35.946Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=dac26e24aca48718&from=jasx&tk=1hdjarjmuje3s800&vjs=3"
"Nascent","About Nascent… 
 Nascent is a team of builders who back early-stage web3 founders creating products and primitives for an open financial world. Founded in 2020, we've invested in 50+ early-stage teams (https://www.nascent.xyz/portfolio) that we believe have the potential to create substantive change, expand boundaries and find new horizons. Building from a base of permanent capital, we also deploy a sizable liquid portfolio utilizing a range of strategies that ensure we are among the most active users of the open financial system we are helping to build. The fluid structure that enables our team to build, use, and invest in the future of crypto makes Nascent both an ideal early-stage partner and long-term ally. 
 The Opportunity 
 As a Data Engineer at Nascent, you'll play a critical role in building and maintaining our data infrastructure and pipelines across our liquid and investing activities. This includes designing and building ETLs, optimizing research and transaction infrastructure, and implementing APIs for data querying and access. Reporting directly to the Data Engineering Lead you'll support the strategy and execution of data acquisition to enable internal research and externally facing data projects. You'll serve as a key partner across the organization, delivering internal data projects such as dashboarding and visualizations to improve investing performance. 
 The ideal candidate will have experience in designing and building data pipelines and APIs, with a strong foundation in data modeling, database design, and query optimization. Think data engineer with a software engineering lens, you can design and build software beyond data pipelines (you know the difference). You'll have a passion for working with large datasets and a desire to leverage data to drive business insights and support decision-making. As a fast-moving crypto-native firm, we have a lot of activities and both ingest and create a lot of data—working closely with our Data Engineering Lead you will execute high impact work across data-related activities for the firm. This is a full time fully remote position with preference for EST working hours. 
 About you 
 
  You thrive in less structured environments and are at your best when driving and delivering results with the freedom to build and execute your own plan.
   You are as excited by starting a project as completing, maintaining, and continually optimizing it.
   You find the plethora of opportunities to leverage data to drive value for the firm exciting versus overwhelming
  
 Required Experience 
 
  Building and managing data pipelines on bare metal outside of pure cloud infrastructure (AWS, Azure, GCP, etc.)
   Hands-on experience from beginning to end of a shipped working product (i.e., you're an experienced builder)
   Substantive experience building and maintaining enterprise data systems, ETL frameworks, & pipelines
   Excellent Python and SQL skills
   Familiarity with APIs (REST, Websockets, etc)
   Experience beyond testing (e.g. quality processes, verification & validation)
   Experience in configuring and managing cloud infrastructure (preferred AWS)
  
 Our Team & Culture 
 At Nascent, we are an interdisciplinary team of investors, builders & creators, capable of achieving more together than we can as individuals. We offer the opportunity to contribute to building the future global economic system with a world-class team and culture that pairs the freedom to explore, experiment & play with a competitive drive to win. We invest in our people by providing the autonomy to build, coupled with accountability & honest feedback to help learn, grow, perform & win. We're a fully distributed team that understands the value of in-person time—we host two team retreats per year and encourage team members to come together for more frequent in-person work. 
 Principles that drive our team & work 
 
  Build for the long term
   Align incentives
   Be nimble
   Compete to win
   Explore, experiment, play
   Always be building
   Give and embrace real feedback
  
 What We Offer 
 At Nascent, we offer a competitive total compensation package heavily weighted toward bonus, ensuring that when we perform at our best and the firm wins we all win. 
 
  The opportunity to learn, experiment and build in an entrepreneurial environment
   Remote and distributed working environment
   Comprehensive health benefits package including dental, vision, and life
   Generous paid parental leave & supported return to work
   Home Office, coworking space and wellness stipend
   Retirement plan matching contributions
   Open vacation policy as well as flexible work hours and location
   Access to our internal performance coaching, technical experts and support for continuing your skill development and growth
   Team activities and bi-annual in-person team retreats
  
 We are an equal opportunity employer and celebrate diversity and differences of perspectives. We do not discriminate on the basis of any status, inclusive of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.","<div>
 <p><b><i>About Nascent&#x2026;</i></b></p> 
 <p>Nascent is a team of builders who back early-stage web3 founders creating products and primitives for an open financial world. Founded in 2020, we&apos;ve invested in 50+ early-stage teams (https://www.nascent.xyz/portfolio) that we believe have the potential to create substantive change, expand boundaries and find new horizons. Building from a base of permanent capital, we also deploy a sizable liquid portfolio utilizing a range of strategies that ensure we are among the most active users of the open financial system we are helping to build. The fluid structure that enables our team to build, use, and invest in the future of crypto makes Nascent both an ideal early-stage partner and long-term ally.</p> 
 <p><b><i>The</i></b> <b><i>Opportunity</i></b></p> 
 <p>As a Data Engineer at Nascent, you&apos;ll play a critical role in building and maintaining our data infrastructure and pipelines across our liquid and investing activities. This includes designing and building ETLs, optimizing research and transaction infrastructure, and implementing APIs for data querying and access. Reporting directly to the Data Engineering Lead you&apos;ll support the strategy and execution of data acquisition to enable internal research and externally facing data projects. You&apos;ll serve as a key partner across the organization, delivering internal data projects such as dashboarding and visualizations to improve investing performance. </p>
 <p>The ideal candidate will have experience in designing and building data pipelines and APIs, with a strong foundation in data modeling, database design, and query optimization. Think data engineer with a software engineering lens, you can design and build software beyond data pipelines (you know the difference). You&apos;ll have a passion for working with large datasets and a desire to leverage data to drive business insights and support decision-making. As a fast-moving crypto-native firm, we have a lot of activities and both ingest and create a lot of data&#x2014;working closely with our Data Engineering Lead you will execute high impact work across data-related activities for the firm. This is a full time fully remote position with preference for EST working hours. </p>
 <p><b><i>About you</i></b></p> 
 <ul>
  <li>You thrive in less structured environments and are at your best when driving and delivering results with the freedom to build and execute your own plan.</li>
  <li> You are as excited by starting a project as completing, maintaining, and continually optimizing it.</li>
  <li> You find the plethora of opportunities to leverage data to drive value for the firm exciting versus overwhelming</li>
 </ul> 
 <p><b><i>Required Experience</i></b></p> 
 <ul>
  <li>Building and managing data pipelines on bare metal outside of pure cloud infrastructure (AWS, Azure, GCP, etc.)</li>
  <li> Hands-on experience from beginning to end of a shipped working product (i.e., you&apos;re an experienced builder)</li>
  <li> Substantive experience building and maintaining enterprise data systems, ETL frameworks, &amp; pipelines</li>
  <li> Excellent Python and SQL skills</li>
  <li> Familiarity with APIs (REST, Websockets, etc)</li>
  <li> Experience beyond testing (e.g. quality processes, verification &amp; validation)</li>
  <li> Experience in configuring and managing cloud infrastructure (preferred AWS)</li>
 </ul> 
 <p><b><i>Our Team &amp; Culture</i></b></p> 
 <p>At Nascent, we are an interdisciplinary team of investors, builders &amp; creators, capable of achieving more together than we can as individuals. We offer the opportunity to contribute to building the future global economic system with a world-class team and culture that pairs the freedom to explore, experiment &amp; play with a competitive drive to win. We invest in our people by providing the autonomy to build, coupled with accountability &amp; honest feedback to help learn, grow, perform &amp; win. We&apos;re a fully distributed team that understands the value of in-person time&#x2014;we host two team retreats per year and encourage team members to come together for more frequent in-person work.</p> 
 <p><b><i>Principles that drive our team &amp; work</i></b></p> 
 <ul>
  <li>Build for the long term</li>
  <li> Align incentives</li>
  <li> Be nimble</li>
  <li> Compete to win</li>
  <li> Explore, experiment, play</li>
  <li> Always be building</li>
  <li> Give and embrace real feedback</li>
 </ul> 
 <p><b><i>What We Offer</i></b></p> 
 <p>At Nascent, we offer a competitive total compensation package heavily weighted toward bonus, ensuring that when we perform at our best and the firm wins we all win.</p> 
 <ul>
  <li>The opportunity to learn, experiment and build in an entrepreneurial environment</li>
  <li> Remote and distributed working environment</li>
  <li> Comprehensive health benefits package including dental, vision, and life</li>
  <li> Generous paid parental leave &amp; supported return to work</li>
  <li> Home Office, coworking space and wellness stipend</li>
  <li> Retirement plan matching contributions</li>
  <li> Open vacation policy as well as flexible work hours and location</li>
  <li> Access to our internal performance coaching, technical experts and support for continuing your skill development and growth</li>
  <li> Team activities and bi-annual in-person team retreats</li>
 </ul> 
 <p><i>We are an equal opportunity employer and celebrate diversity and differences of perspectives. We do not discriminate on the basis of any status, inclusive of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.</i></p>
</div>
<p></p>","https://nascent-xyz.breezy.hr/p/cfd0b01e7274-data-engineer?source=indeed&ittk=RKKFY4QQPN","713aaea093a693fd",,"Full-time",,"Remote","Data Engineer","8 days ago","2023-10-17T11:52:30.193Z","4.3","4","$130,000 - $160,000 a year","2023-10-25T11:52:30.195Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=713aaea093a693fd&from=jasx&tk=1hdjaqvhcirm6800&vjs=3"
"84.51°","84.51° Overview: 
   84.51° is a retail data science, insights and media company. We help the Kroger company, consumer packaged goods companies, agencies, publishers and affiliated partners create more personalized and valuable experiences for shoppers across the path to purchase. 
   Powered by cutting edge science, we leverage 1st party retail data from nearly 1 of 2 US households and 2BN+ transactions to fuel a more customer-centric journey utilizing 84.51° Insights, 84.51° Loyalty Marketing and our retail media advertising solution, Kroger Precision Marketing. 
   Join us at 84.51°! 
   __________________________________________________________
 
 
  As a member of our engineering team, you will use various cutting-edge technologies to develop applications that turn our data into actionable insights used to personalize the customer experience for shoppers at Kroger. We also work with Kroger's supply chain related data assets including but not limited to: Orders and Shipments, Inventory, Planogram and Pricing information to help Kroger, CPG and broker clients make tactical and strategic business decisions. We use agile development methodology bringing everyone into the planning process to build scalable enterprise applications and solutions. 
  What you'll do 
  
  Work within a team that owns the Semantic Layer of a large commercial reporting application. 
  Create and maintain complex retail customer loyalty measures and commercial data security rules. 
  Use strong business analysis skills to translate commercial requirements into technical requirements. 
  Use strong Data Warehouse and BI Tool background and understanding of every layer of the technical stack in order to convert requirements into platform wide implementation solutions. 
  Work with the team to implement the semantic layer portion of those solutions using a BI Tool framework. 
  Develop custom engineering tools to assist with the implementation, automation, and testing of the solution to reduce time to market for new Insights capabilities. 
  
 Responsibilities 
  
  Participate in design and development of highly visible data solutions 
  Support Commercial Facing data pipelines 
  Perform unit and integration testing 
  Collaborate with architecture and lead engineers to ensure consistent development practices 
  Participate in retrospective reviews 
  Participate in the estimation process for new work and releases 
  Collaborate with other engineers to solve and bring new perspectives to complex problems 
  Drive improvements in people, practices, and procedures 
  Embrace new technologies and an ever-changing environment 
  
 Requirements 
  
  3+ years proven ability of professional Data Development experience 
  Experience with a Business Intelligence Reporting Tool 
  Full understanding of ETL concepts and Data Warehousing concepts 
  Proficient with Relational Data Modeling 
  Thorough understanding of CI/CD concepts and best practices 
  Comprehensive Understanding of ANSI SQL 
  Foundational Understanding of Cloud Processing Concepts 
  Foundational Understanding of Agile Principles 
  Exposure to Retail Business Intelligence 
  Exposure to interacting with , enhancing and creating Cloud based services 
  Passion for Problem Solving 
  Passion for creating supportable technical solutions 
  Experience communicating to and with functional colleagues 
  
 Preferred Skills 
  
  dbt 
  Python 
  Python FAST API framework 
  Power BI 
  Snowflake 
  Alation 
  Microsoft Azure 
  MongoDB 
  Oracle 
  
 #LI-Remote #LI-DOLF","<div>
 <div>
  <p><b>84.51&#xb0; Overview:</b></p> 
  <p> 84.51&#xb0; is a retail data science, insights and media company. We help the Kroger company, consumer packaged goods companies, agencies, publishers and affiliated partners create more personalized and valuable experiences for shoppers across the path to purchase.</p> 
  <p> Powered by cutting edge science, we leverage 1st party retail data from nearly 1 of 2 US households and 2BN+ transactions to fuel a more customer-centric journey utilizing 84.51&#xb0; Insights, 84.51&#xb0; Loyalty Marketing and our retail media advertising solution, Kroger Precision Marketing.</p> 
  <p> Join us at 84.51&#xb0;!</p> 
  <p> __________________________________________________________</p>
 </div>
 <p></p>
 <p><br> As a member of our engineering team, you will use various cutting-edge technologies to develop applications that turn our data into actionable insights used to personalize the customer experience for shoppers at Kroger. We also work with Kroger&apos;s supply chain related data assets including but not limited to: Orders and Shipments, Inventory, Planogram and Pricing information to help Kroger, CPG and broker clients make tactical and strategic business decisions. We use agile development methodology bringing everyone into the planning process to build scalable enterprise applications and solutions.</p> 
 <p><b> What you&apos;ll do</b></p> 
 <ul> 
  <li>Work within a team that owns the Semantic Layer of a large commercial reporting application.</li> 
  <li>Create and maintain complex retail customer loyalty measures and commercial data security rules.</li> 
  <li>Use strong business analysis skills to translate commercial requirements into technical requirements.</li> 
  <li>Use strong Data Warehouse and BI Tool background and understanding of every layer of the technical stack in order to convert requirements into platform wide implementation solutions.</li> 
  <li>Work with the team to implement the semantic layer portion of those solutions using a BI Tool framework.</li> 
  <li>Develop custom engineering tools to assist with the implementation, automation, and testing of the solution to reduce time to market for new Insights capabilities.</li> 
 </ul> 
 <p><b>Responsibilities</b></p> 
 <ul> 
  <li>Participate in design and development of highly visible data solutions</li> 
  <li>Support Commercial Facing data pipelines</li> 
  <li>Perform unit and integration testing</li> 
  <li>Collaborate with architecture and lead engineers to ensure consistent development practices</li> 
  <li>Participate in retrospective reviews</li> 
  <li>Participate in the estimation process for new work and releases</li> 
  <li>Collaborate with other engineers to solve and bring new perspectives to complex problems</li> 
  <li>Drive improvements in people, practices, and procedures</li> 
  <li>Embrace new technologies and an ever-changing environment</li> 
 </ul> 
 <p><b>Requirements</b></p> 
 <ul> 
  <li>3+ years proven ability of professional Data Development experience</li> 
  <li>Experience with a Business Intelligence Reporting Tool</li> 
  <li>Full understanding of ETL concepts and Data Warehousing concepts</li> 
  <li>Proficient with Relational Data Modeling</li> 
  <li>Thorough understanding of CI/CD concepts and best practices</li> 
  <li>Comprehensive Understanding of ANSI SQL</li> 
  <li>Foundational Understanding of Cloud Processing Concepts</li> 
  <li>Foundational Understanding of Agile Principles</li> 
  <li>Exposure to Retail Business Intelligence</li> 
  <li>Exposure to interacting with , enhancing and creating Cloud based services</li> 
  <li>Passion for Problem Solving</li> 
  <li>Passion for creating supportable technical solutions</li> 
  <li>Experience communicating to and with functional colleagues</li> 
 </ul> 
 <p><b>Preferred Skills</b></p> 
 <ul> 
  <li>dbt</li> 
  <li>Python</li> 
  <li>Python FAST API framework</li> 
  <li>Power BI</li> 
  <li>Snowflake</li> 
  <li>Alation</li> 
  <li>Microsoft Azure</li> 
  <li>MongoDB</li> 
  <li>Oracle</li> 
 </ul> 
 <p>#LI-Remote #LI-DOLF</p>
</div>","https://boards.greenhouse.io/8451/jobs/6913858002?gh_src=c6449e542us","7412673961592fbf",,,,"100 W 5th St, Cincinnati, OH 45202","Senior Data Engineer (P996)","30+ days ago","2023-09-25T11:52:33.481Z","4.1","11",,"2023-10-25T11:52:33.482Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=7412673961592fbf&from=jasx&tk=1hdjarjmuje3s800&vjs=3"
"Comfort Keepers","Job Summary:
   Comfort Keepers is seeking a skilled and experienced Data Engineer to join our data team. As a Data Engineer, you will play a crucial role in designing, developing, and maintaining our data infrastructure and pipelines. You will work closely with cross-functional teams to ensure the availability and accessibility of high-quality data for analysis and reporting purposes. The ideal candidate is passionate about data, governance, has strong programming skills, is a creative problem solver and is proficient in building efficient and scalable data solutions.
   Be part of a transformational experience as we meet the challenges of today’s business landscape and lay the foundation for future growth.
   Location: Remote-U.S. or Irvine, CA Expected Salary Range: $100k
   Responsibilities:
  
    Build Azure data factory pipeline, Azure Data Lake, Datawarehouse, Power BI Reports and dashboards.
    Design, develop, and maintain data pipelines and ETL processes to extract, transform, and load data from various sources into our data warehouse.
    Optimize and fine-tune data processes to ensure high performance and reliability.
    Implement data validation, testing, and quality assurance processes to maintain data accuracy and consistency.
    Work with large and complex datasets, both structured and unstructured, and employ appropriate data storage solutions.
    Develop and maintain documentation for data pipelines, processes, and data dictionaries.
    Monitor and troubleshoot data pipeline issues, resolving them in a timely manner to minimize downtime.
    Collaborate with data scientists, analysts, agency partners and other stakeholders to understand data requirements and ensure data integrity, accuracy, and availability.
    Additional responsibilities will be supporting applications and projects as appropriate.
  
  
   Qualifications:
  
    Microsoft SQL Server Database 2019 and above, including Azure SQL
    Azure Data Factory, Azure Data Lake, Azure Devops , Azure Data warehouse
    Experience with Repository and version control, GitHub
    Microsoft Power BI and Power Query with DAX and M Language
  
 
  
 
 
   Azure Analysis service—Tabular Cube
   Amazon Athena and Redshift
   Development tools (Visual Studio)
   Designing and implementing data models, and data lake solutions
   Working with data scientists and digital teams to meet strategic data needs through project management tools like Microsoft Teams, JIRA, are desired
   Strong problem-solving skills and attention to detail.
   Excellent communication and teamwork skills.
 
  Preferred:
 
   C# Experience
   Python, and/or R applications and languages while managing work using software version control like GitHub and/or Dev Ops
 
 
  Work Environment: Remote
  An Equal Opportunity and Affirmative Action employer, Comfort Keepers considers applicants for all positions without regard to race, color, religion, creed, gender, national origin, age, disability, marital or veteran status, or any legally protected status. We will make reasonable accommodations for qualified individuals with known disabilities unless doing so would result in an undue hardship.
  
 Xwc2axfoet","<div>
 <div>
  <h1 class=""jobSectionHeader""><b>Job </b><b>Summary:</b></h1>
  <p> Comfort Keepers is seeking a skilled and experienced Data Engineer to join our data team. As a Data Engineer, you will play a crucial role in designing, developing, and maintaining our data infrastructure and pipelines. You will work closely with cross-functional teams to ensure the availability and accessibility of high-quality data for analysis and reporting purposes. The ideal candidate is passionate about data, governance, has strong programming skills, is a creative problem solver and is proficient in building efficient and scalable data solutions.</p>
  <p> Be part of a transformational experience as we meet the challenges of today&#x2019;s business landscape and lay the foundation for future growth.</p>
  <p> Location: Remote-U.S. or Irvine, CA<br> Expected Salary Range: &#x24;100k</p>
  <h1 class=""jobSectionHeader""><b> Responsibilities:</b></h1>
  <ul>
   <li> Build Azure data factory pipeline, Azure Data Lake, Datawarehouse, Power BI Reports and dashboards.</li>
   <li> Design, develop, and maintain data pipelines and ETL processes to extract, transform, and load data from various sources into our data warehouse.</li>
   <li> Optimize and fine-tune data processes to ensure high performance and reliability.</li>
   <li> Implement data validation, testing, and quality assurance processes to maintain data accuracy and consistency.</li>
   <li> Work with large and complex datasets, both structured and unstructured, and employ appropriate data storage solutions.</li>
   <li> Develop and maintain documentation for data pipelines, processes, and data dictionaries.</li>
   <li> Monitor and troubleshoot data pipeline issues, resolving them in a timely manner to minimize downtime.</li>
   <li> Collaborate with data scientists, analysts, agency partners and other stakeholders to understand data requirements and ensure data integrity, accuracy, and availability.</li>
   <li> Additional responsibilities will be supporting applications and projects as appropriate.</li>
  </ul>
  <p></p>
  <h1 class=""jobSectionHeader""><b><br> Qualifications:</b></h1>
  <ul>
   <li> Microsoft SQL Server Database 2019 and above, including Azure SQL</li>
   <li> Azure Data Factory, Azure Data Lake, Azure Devops , Azure Data warehouse</li>
   <li> Experience with Repository and version control, GitHub</li>
   <li> Microsoft Power BI and Power Query with DAX and M Language</li>
  </ul>
 </div>
 <br> 
 <p></p>
 <ul>
  <li> Azure Analysis service&#x2014;Tabular Cube</li>
  <li> Amazon Athena and Redshift</li>
  <li> Development tools (Visual Studio)</li>
  <li> Designing and implementing data models, and data lake solutions</li>
  <li> Working with data scientists and digital teams to meet strategic data needs through project management tools like Microsoft Teams, JIRA, are desired</li>
  <li> Strong problem-solving skills and attention to detail.</li>
  <li> Excellent communication and teamwork skills.</li>
 </ul>
 <p> Preferred:</p>
 <ul>
  <li> C# Experience</li>
  <li> Python, and/or R applications and languages while managing work using software version control like GitHub and/or Dev Ops</li>
 </ul>
 <p></p>
 <p><b><br> Work Environment: Remote</b></p>
 <p> An Equal Opportunity and Affirmative Action employer, Comfort Keepers considers applicants for all positions without regard to race, color, religion, creed, gender, national origin, age, disability, marital or veteran status, or any legally protected status. We will make reasonable accommodations for qualified individuals with known disabilities unless doing so would result in an undue hardship.</p>
 <p> </p>
 <p>Xwc2axfoet</p>
</div>","https://sdxhomecareoperationsllc.applytojob.com/apply/Xwc2axfoet/Data-Engineer?source=INDE&utm_source=Indeed&utm_medium=organic&utm_campaign=Indeed","daada383998fad4a",,"Full-time",,"Remote","Data Engineer","8 days ago","2023-10-17T11:52:31.012Z","3.6","4812","$100,000 a year","2023-10-25T11:52:31.014Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=daada383998fad4a&from=jasx&tk=1hdjaqvhcirm6800&vjs=3"
"Clari","Clari’s Revenue platform gives forecasting accuracy and visibility from the sales rep to the board room on revenue performance - helping them spot revenue leak to answer if they will meet, beat, or miss their sales goals. With insights like this, no wonder leading companies worldwide, including Okta, Adobe, Workday, and Zoom use Clari to drive revenue accuracy and precision. We never get tired of our customers singing our praises because it fuels us to help them continue to achieve remarkable. The next generation of revenue excellence is here…are you ready to achieve remarkable with us?
 
 
 
   About the Team
 
 
   The Engineering team at Clari is an Agile shop that practices Scrum across all of our teams. We layer in coordination practices such as Big Room Planning to stay aligned to Clari’s KPIs quarterly across sites and teams. If you love working in an Agile environment that values collaboration and continuous improvement then we can’t wait to meet you.
 
 
 
   About the Role
 
 
   We are looking for a talented Principal Software Engineer to join our Query Manager team. Query Manager is a part of Clari’s Data Platform team, and is the interface that allows application and API developers to easily and efficiently retrieve data across hundreds of databases and billions of rows of data that comprise our ever-evolving Data Platform.
 
 
 
   You will work with remarkable colleagues to architect, build and optimize the query layer to derive exceptional performance from our data warehouse built on top of AWS Aurora Postgres. You will collaborate closely with the product management, architecture, application and infrastructure teams to build the data services that power our best-in-class enterprise product suite. Most of Clari’s application and API queries are processed through the query manager layer. The products you build are used and loved by many of the most well-known companies in the world. Don’t believe us? Hear what our customers have to say
   
 
 
  
 
  Come join this fluid, dynamic, and growing team to learn, teach, and make a big, measurable impact every day. We work in an open, collaborative environment and seek exceptional developers who enjoy problem-solving and straying outside their routine.
   
 
 
  
 
  This is a fully remote opportunity and can be worked from any location in the United States.
  
 Responsibilities
 
   Design and evolve the architecture for the query layer that powers Clari’s product suite and platform
   Learn and contribute to all aspects of the data platform, from extracting and ingesting data from external systems to modeling, transforming, and managing large volumes of data at rest and in motion
   Mentor junior engineers to set and maintain high standards of engineering excellence while helping to grow their careers
   Write scalable, robust, and fully tested software for deployment in mission-critical production environments
   Create and improve tooling and processes to help reduce development friction and enable greater productivity across the development organization
   Contribute to the growth of Clari by being a brand ambassador and assisting in the hiring of great talent
 
  Qualifications
 
   10+ years of software development experience using Java or similar object-oriented languages for backend development
   Deep expertise with relational database skills and concepts
   Experience having led multiple projects from inception through deployment, maintenance, and support
   Experience with Postgres and non-relational databases like MongoDB
   Experience with AWS
   Experience building scalable systems and architectures
   Experience with database performance tuning
 
  Perks and Benefits @ Clari 
 
  Remote-first with opportunities to work and celebrate in person
   Medical, dental, vision, short & long-term disability, Life insurance, and EAP
   Mental health support provided by Modern Health
   Pre-IPO stock options
   Well-being and professional development funds
   Retirement 401(k) plan
   100% paid parental leave, plus fertility and family planning support provided by Maven
   Discretionary paid time off, monthly ‘take a break’ days, and Focus Fridays
   Focus on culture: Charitable giving match, plus in-person and virtual events
 
 
 
   It is Clari’s intent to pay all Clarians competitive wages and salaries that are motivational, fair, and equitable. The goal of Clari’s compensation program is to be transparent, attract potential employees, meet the needs of all current employees and encourage employees to stay and grow at Clari.
 
 
 
   Actual compensation packages are based on several factors that are unique to each candidate, including but not limited to specific work location, skill set, depth of experience, education and certifications.
 
 
 
   The salary range for this position is $191,300 to $286,900. The compensation package for this position also includes stock options and company-paid benefits, including well-being and professional development stipends.
 
 
   #BI-Remote #LI-Remote
 
 
 
   You’ll often hear our CEO talk about being remarkable. To Clari, remarkable means many things. We believe in providing interesting and meaningful work in a nurturing and inclusive environment. One that is free from discrimination for everyone without regard to race, color, religion, sex, sexual orientation, national origin, age, disability, gender identity, or veteran status. Efforts have to be recognized. Voices have to be heard. And work-life balance has to be baked into the very fiber of the company. We are honored to be recognized by Inc. Magazine and Bay Area News Group as a best place to work for several years running. We’d love to have you join us on our journey to remarkable!
 
 
 
   If you feel you don’t meet 100% of the qualifications outlined above, we want you to apply! Clari believes in hiring people, not just skills. If you are passionate about learning and excited about what we are doing, then we want to hear from you.
 
 
 
   Clari focuses on culture add, not culture fit. One of our values is One with Customers, and we know we can serve them better when we involve as many different perspectives as possible. Our team is made stronger by what makes you unique, so we hope you’ll bring your whole self to the job.","<div>
 <div>
  Clari&#x2019;s Revenue platform gives forecasting accuracy and visibility from the sales rep to the board room on revenue performance - helping them spot revenue leak to answer if they will meet, beat, or miss their sales goals. With insights like this, no wonder leading companies worldwide, including Okta, Adobe, Workday, and Zoom use Clari to drive revenue accuracy and precision. We never get tired of our customers singing our praises because it fuels us to help them continue to achieve remarkable. The next generation of revenue excellence is here&#x2026;are you ready to achieve remarkable with us?
 </div>
 <div></div>
 <div>
  <b><br> About the Team</b>
 </div>
 <div>
   The Engineering team at Clari is an Agile shop that practices Scrum across all of our teams. We layer in coordination practices such as Big Room Planning to stay aligned to Clari&#x2019;s KPIs quarterly across sites and teams. If you love working in an Agile environment that values collaboration and continuous improvement then we can&#x2019;t wait to meet you.
 </div>
 <div></div>
 <div>
  <b><br> About the Role</b>
 </div>
 <div>
   We are looking for a talented Principal Software Engineer to join our Query Manager team. Query Manager is a part of Clari&#x2019;s Data Platform team, and is the interface that allows application and API developers to easily and efficiently retrieve data across hundreds of databases and billions of rows of data that comprise our ever-evolving Data Platform.
 </div>
 <div></div>
 <div>
  <br> You will work with remarkable colleagues to architect, build and optimize the query layer to derive exceptional performance from our data warehouse built on top of AWS Aurora Postgres. You will collaborate closely with the product management, architecture, application and infrastructure teams to build the data services that power our best-in-class enterprise product suite. Most of Clari&#x2019;s application and API queries are processed through the query manager layer. The products you build are used and loved by many of the most well-known companies in the world. Don&#x2019;t believe us? Hear what our customers have to say
  <br> 
 </div>
 <div></div>
 <br> 
 <div>
  Come join this fluid, dynamic, and growing team to learn, teach, and make a big, measurable impact every day. We work in an open, collaborative environment and seek exceptional developers who enjoy problem-solving and straying outside their routine.
  <br> 
 </div>
 <div></div>
 <br> 
 <div>
  <i>This is a fully remote opportunity and can be worked from any location in the United States.</i>
 </div> 
 <h3 class=""jobSectionHeader""><b>Responsibilities</b></h3>
 <ul>
  <li> Design and evolve the architecture for the query layer that powers Clari&#x2019;s product suite and platform</li>
  <li> Learn and contribute to all aspects of the data platform, from extracting and ingesting data from external systems to modeling, transforming, and managing large volumes of data at rest and in motion</li>
  <li> Mentor junior engineers to set and maintain high standards of engineering excellence while helping to grow their careers</li>
  <li> Write scalable, robust, and fully tested software for deployment in mission-critical production environments</li>
  <li> Create and improve tooling and processes to help reduce development friction and enable greater productivity across the development organization</li>
  <li> Contribute to the growth of Clari by being a brand ambassador and assisting in the hiring of great talent</li>
 </ul>
 <h3 class=""jobSectionHeader""><b> Qualifications</b></h3>
 <ul>
  <li> 10+ years of software development experience using Java or similar object-oriented languages for backend development</li>
  <li> Deep expertise with relational database skills and concepts</li>
  <li> Experience having led multiple projects from inception through deployment, maintenance, and support</li>
  <li> Experience with Postgres and non-relational databases like MongoDB</li>
  <li> Experience with AWS</li>
  <li> Experience building scalable systems and architectures</li>
  <li> Experience with database performance tuning</li>
 </ul>
 <h3 class=""jobSectionHeader""><b> Perks and Benefits @ Clari </b></h3>
 <ul>
  <li>Remote-first with opportunities to work and celebrate in person</li>
  <li> Medical, dental, vision, short &amp; long-term disability, Life insurance, and EAP</li>
  <li> Mental health support provided by Modern Health</li>
  <li> Pre-IPO stock options</li>
  <li> Well-being and professional development funds</li>
  <li> Retirement 401(k) plan</li>
  <li> 100% paid parental leave, plus fertility and family planning support provided by Maven</li>
  <li> Discretionary paid time off, monthly &#x2018;take a break&#x2019; days, and Focus Fridays</li>
  <li> Focus on culture: Charitable giving match, plus in-person and virtual events</li>
 </ul>
 <div></div>
 <div>
  <br> It is Clari&#x2019;s intent to pay all Clarians competitive wages and salaries that are motivational, fair, and equitable. The goal of Clari&#x2019;s compensation program is to be transparent, attract potential employees, meet the needs of all current employees and encourage employees to stay and grow at Clari.
 </div>
 <div></div>
 <div>
  <br> Actual compensation packages are based on several factors that are unique to each candidate, including but not limited to specific work location, skill set, depth of experience, education and certifications.
 </div>
 <div></div>
 <div>
  <br> The salary range for this position is &#x24;191,300 to &#x24;286,900. The compensation package for this position also includes stock options and company-paid benefits, including well-being and professional development stipends.
 </div>
 <div>
   #BI-Remote #LI-Remote
 </div>
 <div></div>
 <div>
  <br> You&#x2019;ll often hear our CEO talk about being remarkable. To Clari, remarkable means many things. We believe in providing interesting and meaningful work in a nurturing and inclusive environment. One that is free from discrimination for everyone without regard to race, color, religion, sex, sexual orientation, national origin, age, disability, gender identity, or veteran status. Efforts have to be recognized. Voices have to be heard. And work-life balance has to be baked into the very fiber of the company. We are honored to be recognized by Inc. Magazine and Bay Area News Group as a best place to work for several years running. We&#x2019;d love to have you join us on our journey to remarkable!
 </div>
 <div></div>
 <div>
  <b><br> If you feel you don&#x2019;t meet 100% of the qualifications outlined above, we want you to apply! Clari believes in hiring people, not just skills. If you are passionate about learning and excited about what we are doing, then we want to hear from you.</b>
 </div>
 <div></div>
 <div>
  <br> Clari focuses on culture add, not culture fit. One of our values is One with Customers, and we know we can serve them better when we involve as many different perspectives as possible. Our team is made stronger by what makes you unique, so we hope you&#x2019;ll bring your whole self to the job.
 </div>
</div>","https://jobs.lever.co/clari/3b2a2738-2090-42de-b748-acfc28b25609?lever-source=Indeed","416b51eb49b3f599",,"Full-time",,"Seattle, WA","Principal Software Engineer, Data Platform - Remote","8 days ago","2023-10-17T11:52:44.279Z","4","3","$191,300 - $286,900 a year","2023-10-25T11:52:44.282Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=416b51eb49b3f599&from=jasx&tk=1hdjao0bljrpc800&vjs=3"
"SourceMantra","Hello,

 Job Title: Senior AWS Data Engineer with Strong Python Development(10+)
 Location: Reston, VA – Once in a month
 Duration: 12+ Months Contract position
 Experience : 10+ years

Need to be strong in AWS SNS, SQS, Lambda functions.
Job Description :

 • Strong knowledge on AWS services such as (S3, RDS, EC2, Lambda, SQS, SNS, Redshift)
 • Having prior working experience in Fannie Mae will be added advantage.
 • Good Knowledge on Java and Database (Oracle Postgres)

Thanks & RegardsSwarna - swarna@sourcemantra.com|Source Mantra IncTechnical Recruiter Desk No: (908-381-0321)LinkedIn: linkedin.com/in/swarna-yenumula-6512a1167295 Durham Ave, Suite # 201, South Plainfield, NJ 07080Source Mantra Inc | Certified Minority Business Enterprise (MBE)
Job Type: Contract
Salary: $60.00 - $70.00 per hour
Benefits:

 Health insurance

Compensation package:

 Hourly pay

Experience level:

 10 years

Schedule:

 8 hour shift

Experience:

 Java: 4 years (Preferred)
 Python: 3 years (Preferred)
 Redshift: 3 years (Preferred)

Work Location: Remote","<p>Hello,</p>
<ul>
 <li><b>Job Title:</b> Senior AWS Data Engineer with Strong Python Development(10+)</li>
 <li><b>Location:</b> Reston, VA &#x2013; Once in a month</li>
 <li><b>Duration:</b> 12+ Months Contract position</li>
 <li><b>Experience :</b> 10+ years</li>
</ul>
<p>Need to be strong in AWS SNS, SQS, Lambda functions.</p>
<p><b>Job Description :</b></p>
<ul>
 <li>&#x2022; Strong knowledge on AWS services such as (S3, RDS, EC2, Lambda, SQS, SNS, Redshift)</li>
 <li>&#x2022; Having prior working experience in Fannie Mae will be added advantage.</li>
 <li>&#x2022; Good Knowledge on Java and Database (Oracle Postgres)</li>
</ul>
<p><b>Thanks &amp; Regards</b><br><b>Swarna - </b>swarna@sourcemantra.com|<b>Source Mantra Inc</b><br><b>Technical Recruiter </b><br><b>Desk No</b>: (908-381-0321)<br><b>LinkedIn</b>: linkedin.com/in/swarna-yenumula-6512a1167<br><b>295 Durham Ave, Suite # 201, South Plainfield, NJ 07080</b><br><b>Source Mantra Inc | Certified Minority Business Enterprise (MBE)</b></p>
<p>Job Type: Contract</p>
<p>Salary: &#x24;60.00 - &#x24;70.00 per hour</p>
<p>Benefits:</p>
<ul>
 <li>Health insurance</li>
</ul>
<p>Compensation package:</p>
<ul>
 <li>Hourly pay</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>10 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Java: 4 years (Preferred)</li>
 <li>Python: 3 years (Preferred)</li>
 <li>Redshift: 3 years (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,"6154aa92888e85ea",,"Contract",,"Remote","AWS Data Engineer with Python 10+Years","12 days ago","2023-10-13T11:53:00.848Z",,,"$60 - $70 an hour","2023-10-25T11:53:00.851Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=6154aa92888e85ea&from=jasx&tk=1hdjasd0pk60q801&vjs=3"
"Arista Networks","Company Description
  Arista Networks is an industry leader in Cognitive Cloud Networking for mission critical data center and campus environments. Our award winning open source platforms deliver ultra low latency, high availability, automated analytics and secure network solutions.
  Our culture is one that is founded on our core key values which resonate across all of our employee and include respect, integrity, teamwork, innovation, passion, trust and quality.
 
 

 Job Description
  The Data Center Site Operations Engineer will help customers successfully plan and optimize their Data Center physical space, mechanical, cabling and other site infrastructure to receive Arista equipment and operate it in the highest performing and most efficient manner.
  Arista Professional Services team helps customers throughout the entire technology lifecycle from design to implementation, migration and ongoing operation, to ensure a seamless transition from initial exposure to being able to operate and realize tangible business outcomes from their network infrastructure.
  In this role, you would function as the subject matter expert in this area of the Arista PS team, interfacing with mechanical engineers in the Arista product team to understand detailed requirements, translate these into best practices for field installation, then take a customer facing role in training and advisory.
  Responsibilities will be to provide expert consulting to customers including site design assistance, proactive site prep. for installation, demonstration of best practices, creating training material and delivering training to customers and partiers, site surveys and oversight of physical installation and commissioning. Areas of expertise should include:
  Site requirements for all Arista Data Center products including:
 
   Equipment installation logistics including floorspace and rack preparation, moving and lifting
   Proper handling of all components from receiving, storage, unboxing and assembly
   Size, Weight and Rack mounting considerations for safe operation and serviceability
   Airflow and cooling requirements
   Power requirements including power resiliency
   Cable management considerations
   Copper and Fiber cable types, connector types, pluggable modules
 
  Understand overall DC design and optimization, including:
 
   Floorplan and rack layouts
   Power distribution and resiliency
   Cooling strategies including liquid cooling
   Cable layout strategies for different DC network topologies
   Equipment density and cable length considerations
 
 
 

 Qualifications
  
 
   An engineering, IT or technology focused degree.
   Mechanical Engineering degree a plus.
   10+ years of industry experience designing, constructing and overseeing operations in complex datacenter environments.
   Experience with the handling, installation and servicing of large scale IT equipment in the DC.
   Experience creating technical drawings, documenting methodologies and training DC staff to execute operations.
   Excellent written and verbal skills and ability to effectively communicate with all Arista, partner and customer levels.
   Prior experience with DC site operations as well as with customer facing services or support will be a plus
   This position is remote with a potential for 50% travel globally.
 
  Compensation Information:
  The new hire base pay for this role has a pay range of $103,000 to $158,000 across the US, within California, the base pay range for this role is $124,000 to 158,000.
  Arista offers different pay ranges based on work location, so that we can offer consistent and competitive pay appropriate to the market. The actual base pay offered will be based on a wide range of factors, including skills, qualifications, relevant experience, and work location. The pay range provided reflects base pay only and in addition certain roles may also be eligible for discretionary Arista bonuses and equity. Employees in Sales roles are eligible to participate in Arista’s Sales Incentive Plan, which pays commissions calculated as a percentage of eligible sales. US-based employees are also entitled to benefits including medical, dental, vision, wellbeing, tax savings and income protection. The recruiting team can share more details during the hiring process specific to the role and location.
  Additional Information
  All your information will be kept confidential according to EEO guidelines.","<div>
 Company Description
 <p><br> Arista Networks is an industry leader in Cognitive Cloud Networking for mission critical data center and campus environments. Our award winning open source platforms deliver ultra low latency, high availability, automated analytics and secure network solutions.</p>
 <p> Our culture is one that is founded on our core key values which resonate across all of our employee and include respect, integrity, teamwork, innovation, passion, trust and quality.</p>
</div> 
<br> 
<div>
 Job Description
 <p><br> The Data Center Site Operations Engineer will help customers successfully plan and optimize their Data Center physical space, mechanical, cabling and other site infrastructure to receive Arista equipment and operate it in the highest performing and most efficient manner.</p>
 <p> Arista Professional Services team helps customers throughout the entire technology lifecycle from design to implementation, migration and ongoing operation, to ensure a seamless transition from initial exposure to being able to operate and realize tangible business outcomes from their network infrastructure.</p>
 <p> In this role, you would function as the subject matter expert in this area of the Arista PS team, interfacing with mechanical engineers in the Arista product team to understand detailed requirements, translate these into best practices for field installation, then take a customer facing role in training and advisory.</p>
 <p> Responsibilities will be to provide expert consulting to customers including site design assistance, proactive site prep. for installation, demonstration of best practices, creating training material and delivering training to customers and partiers, site surveys and oversight of physical installation and commissioning. Areas of expertise should include:</p>
 <p><b> Site requirements for all Arista Data Center products including:</b></p>
 <ul>
  <li> Equipment installation logistics including floorspace and rack preparation, moving and lifting</li>
  <li> Proper handling of all components from receiving, storage, unboxing and assembly</li>
  <li> Size, Weight and Rack mounting considerations for safe operation and serviceability</li>
  <li> Airflow and cooling requirements</li>
  <li> Power requirements including power resiliency</li>
  <li> Cable management considerations</li>
  <li> Copper and Fiber cable types, connector types, pluggable modules</li>
 </ul>
 <p><b> Understand overall DC design and optimization, including:</b></p>
 <ul>
  <li> Floorplan and rack layouts</li>
  <li> Power distribution and resiliency</li>
  <li> Cooling strategies including liquid cooling</li>
  <li> Cable layout strategies for different DC network topologies</li>
  <li> Equipment density and cable length considerations</li>
 </ul>
</div> 
<br> 
<div>
 Qualifications
 <br> 
 <ul>
  <li> An engineering, IT or technology focused degree.</li>
  <li> Mechanical Engineering degree a plus.</li>
  <li> 10+ years of industry experience designing, constructing and overseeing operations in complex datacenter environments.</li>
  <li> Experience with the handling, installation and servicing of large scale IT equipment in the DC.</li>
  <li> Experience creating technical drawings, documenting methodologies and training DC staff to execute operations.</li>
  <li> Excellent written and verbal skills and ability to effectively communicate with all Arista, partner and customer levels.</li>
  <li> Prior experience with DC site operations as well as with customer facing services or support will be a plus</li>
  <li> This position is remote with a potential for 50% travel globally.</li>
 </ul>
 <p><b> Compensation Information:</b></p>
 <p> The new hire base pay for this role has a pay range of &#x24;103,000 to &#x24;158,000 across the US, within California, the base pay range for this role is &#x24;124,000 to 158,000.</p>
 <p> Arista offers different pay ranges based on work location, so that we can offer consistent and<br> competitive pay appropriate to the market. The actual base pay offered will be based on a wide range of factors, including skills, qualifications, relevant experience, and work location.<br> The pay range provided reflects base pay only and in addition certain roles may also be eligible for discretionary Arista bonuses and equity. Employees in Sales roles are eligible to participate in Arista&#x2019;s Sales Incentive Plan, which pays commissions calculated as a percentage of eligible sales. US-based employees are also entitled to benefits including medical, dental, vision, wellbeing, tax savings and income protection. The recruiting team can share more details during the hiring process specific to the role and location.</p>
 <br> Additional Information
 <p><br> All your information will be kept confidential according to EEO guidelines.</p>
</div>","https://jobs.smartrecruiters.com/AristaNetworks/743999937616147-data-center-site-operations-engineer-professional-services","6492b9f12e5769ae",,"Full-time",,"5453 Great America Pkwy, Santa Clara, CA 95054","Data Center Site Operations Engineer - Professional Services","8 days ago","2023-10-17T11:52:52.631Z","4","40","$103,000 - $158,000 a year","2023-10-25T11:52:52.637Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=6492b9f12e5769ae&from=jasx&tk=1hdjaruujjrqo801&vjs=3"
"Index Analytics LLC","Company Overview
Index Analytics, LLC, is a rapidly growing Baltimore-based small business providing health-related consulting services to the federal government. At the center of our company culture is a commitment to instilling a dynamic and employee-friendly place to work. We place a priority on promoting a supportive and collegial team environment and enhancing staff’s experience through career development and educational opportunities.
Job Overview
The Senior Data Engineer will assist in moving the existing system solution into a more modernized platform while providing direct guidance to other members of the team. This includes acting as a go-to resource when technical challenges arise and providing leadership and coordination as part of a project team of data engineers, Cloud Architects, analysts, and BI developers to implement a Snowflake environment to ingest a wide variety of data sources to allow for data analytics.
The Senior Data Engineer determines structural, interface, and business requirements for developing and installing solutions. This includes the design of relational databases, other types of databases, and associated interfaces, used for data storage and processing. The Senior Data Engineer develops warehouse and data mart implementation strategies, data acquisition, and archive recovery. In pursuit of system and service optimization, the Architect may perform other duties, such as investigating new data products and technology, evaluating new data sources, and reviewing existing products and products under development for adherence to the organization's quality standards and ease of integration.
Responsibilities

 Streamline migration processes from the legacy SSIS and SQL Server ETL into Snowflake
 Re-architect and incrementally improve the performance of ETL solutions without impacting the delivery of new features requested by the client
 Assist with development efforts to modernize data ingestion patterns
 Completing development tasks such as building custom reports, developing complex queries, ETL and data warehousing, and disseminating data to stakeholders.
 Cloud engineering, CRM development and support, testing, DevOps, app support, data migrations, data loads, and scheduling jobs
 Data Modeling to support ingestion of a wide variety of CMS data sources and requirements from data analysts and other stakeholders
 Implementing business rules using SQL Server stored procedures, views, triggers, and other database objects to reduce the number of cases requiring manual analysis
 Using Python to perform file management tasks
 Optimizing existing processes to improve performance
 Work closely with product owners, the SDET team, and DevOps to ensure compliance with SDLC processes
 Collaborate with business analysts to gather requirements, develop and document business rules, create test scenarios to ensure properly working code, and communicate technical concepts for transparency

Qualifications

 US citizen, or lived in the US for 3 of the last 5 years
 Experience analyzing data and presenting information to stakeholders.
 Experience with programming SQL or BI Platform.
 Experience working within an Agile development environment or development and testing activities.
 8+ years of overall work experience
 Bachelor’s Degree or equivalent OR 4 years’ relevant experience in lieu of degree
 7 additional years of relevant experience
 Data modeling and database administration experience with a variety of databases
 Knowledge of Snowflake cloud database platform
 2+ years of hands-on experience with Python or similar
 Hands-on experience with database performance tuning, clustering key analysis, sizing, and cost optimization
 Working knowledge of database security, audit, and RBAC controls
 Experience with Cloud platforms such as Azure (preferred), AWS
 Database deployment automation experience via CI/CD tools such as CloudBees Jenkins, Azure Pipelines, Aws Code Pipeline, etc. is a plus
 Experience working with un/semi-structured (XML, JSON, PARQUET, video, audio, images, pdf) data a plus
 SnowPro certification or Azure certifications are a plus
 Ability to obtain Public Trust level clearance.
 Must demonstrate excellent communication skills and passion for continuous learning

Index Analytics provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.
Job Type: Full-time
Benefits:

 401(k)
 Dental insurance
 Flexible spending account
 Health insurance
 Health savings account
 Paid time off
 Retirement plan
 Vision insurance

Experience level:

 7 years

Schedule:

 8 hour shift

Work Location: Remote","<p><b>Company Overview</b></p>
<p>Index Analytics, LLC, is a rapidly growing Baltimore-based small business providing health-related consulting services to the federal government. At the center of our company culture is a commitment to instilling a dynamic and employee-friendly place to work. We place a priority on promoting a supportive and collegial team environment and enhancing staff&#x2019;s experience through career development and educational opportunities.</p>
<p><b>Job Overview</b></p>
<p>The Senior Data Engineer will assist in moving the existing system solution into a more modernized platform while providing direct guidance to other members of the team. This includes acting as a go-to resource when technical challenges arise and providing leadership and coordination as part of a project team of data engineers, Cloud Architects, analysts, and BI developers to implement a Snowflake environment to ingest a wide variety of data sources to allow for data analytics.</p>
<p>The Senior Data Engineer determines structural, interface, and business requirements for developing and installing solutions. This includes the design of relational databases, other types of databases, and associated interfaces, used for data storage and processing. The Senior Data Engineer develops warehouse and data mart implementation strategies, data acquisition, and archive recovery. In pursuit of system and service optimization, the Architect may perform other duties, such as investigating new data products and technology, evaluating new data sources, and reviewing existing products and products under development for adherence to the organization&apos;s quality standards and ease of integration.</p>
<p><b>Responsibilities</b></p>
<ul>
 <li>Streamline migration processes from the legacy SSIS and SQL Server ETL into Snowflake</li>
 <li>Re-architect and incrementally improve the performance of ETL solutions without impacting the delivery of new features requested by the client</li>
 <li>Assist with development efforts to modernize data ingestion patterns</li>
 <li>Completing development tasks such as building custom reports, developing complex queries, ETL and data warehousing, and disseminating data to stakeholders.</li>
 <li>Cloud engineering, CRM development and support, testing, DevOps, app support, data migrations, data loads, and scheduling jobs</li>
 <li>Data Modeling to support ingestion of a wide variety of CMS data sources and requirements from data analysts and other stakeholders</li>
 <li>Implementing business rules using SQL Server stored procedures, views, triggers, and other database objects to reduce the number of cases requiring manual analysis</li>
 <li>Using Python to perform file management tasks</li>
 <li>Optimizing existing processes to improve performance</li>
 <li>Work closely with product owners, the SDET team, and DevOps to ensure compliance with SDLC processes</li>
 <li>Collaborate with business analysts to gather requirements, develop and document business rules, create test scenarios to ensure properly working code, and communicate technical concepts for transparency</li>
</ul>
<p><b>Qualifications</b></p>
<ul>
 <li><b>US citizen, or lived in the US for 3 of the last 5 years</b></li>
 <li>Experience analyzing data and presenting information to stakeholders.</li>
 <li>Experience with programming SQL or BI Platform.</li>
 <li>Experience working within an Agile development environment or development and testing activities.</li>
 <li><b>8+ years of overall work experience</b></li>
 <li><b>Bachelor&#x2019;s Degree or equivalent OR 4 years&#x2019; relevant experience in lieu of degree</b></li>
 <li><b>7 additional years of relevant experience</b></li>
 <li>Data modeling and database administration experience with a variety of databases</li>
 <li>Knowledge of <b>Snowflake </b>cloud database platform</li>
 <li><b>2+ years of hands-on experience with Python or similar</b></li>
 <li>Hands-on experience with database performance tuning, clustering key analysis, sizing, and cost optimization</li>
 <li>Working knowledge of database security, audit, and RBAC controls</li>
 <li>Experience with Cloud platforms such as Azure (preferred), AWS</li>
 <li>Database deployment automation experience via CI/CD tools such as CloudBees Jenkins, Azure Pipelines, Aws Code Pipeline, etc. is a plus</li>
 <li>Experience working with un/semi-structured (XML, JSON, PARQUET, video, audio, images, pdf) data a plus</li>
 <li>SnowPro certification or Azure certifications are a plus</li>
 <li>Ability to obtain Public Trust level clearance.</li>
 <li>Must demonstrate excellent communication skills and passion for continuous learning</li>
</ul>
<p><i>Index Analytics provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.</i></p>
<p>Job Type: Full-time</p>
<p>Benefits:</p>
<ul>
 <li>401(k)</li>
 <li>Dental insurance</li>
 <li>Flexible spending account</li>
 <li>Health insurance</li>
 <li>Health savings account</li>
 <li>Paid time off</li>
 <li>Retirement plan</li>
 <li>Vision insurance</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>7 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
</ul>
<p>Work Location: Remote</p>",,"c38667c417507a51",,"Full-time",,"Remote","Sr. Data Engineer/Migration Specialist","11 days ago","2023-10-14T11:53:07.834Z","3.7","3",,"2023-10-25T11:53:07.835Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=c38667c417507a51&from=jasx&tk=1hdjasd0pk60q801&vjs=3"
"ICF","Senior Data Engineer - Scala
 
 
 
  
   
    
     
       Working at ICF means applying a passion for meaningful work with intellectual rigor to help solve the leading issues of our day. Smart, compassionate, innovative, committed, ICF employees tackle unprecedented challenges to benefit people, businesses, and governments around the globe. We believe in collaboration, mutual respect, open communication, and opportunity for growth.
     
     
     
       **This role can be completely remote, sitting anywhere within the US**
     
     
     
       We seek a talented Data Engineer who is eager to apply computer science, software engineering, databases, and distributed/parallel processing frameworks to prepare big data for the use of data analysts and data scientists. If you have experience with Scala and Spark and want your work to contribute to systems that collect healthcare data used by hundreds of thousands of daily users, we want to (virtually) meet you!
     
     
     
       You will work on projects that support the Centers for Medicare and Medicaid Services (CMS) as we develop a next-generation analytics and reporting system that directly impacts healthcare quality. You will use Spark to build data processing pipelines that derive information from large sets of government data. You will be the go-to on your team for Spark, the Spark Engine, and the Spark Dataframe API. This program allows for the continued quality of clinicians’ work according to CMS standards. We are a collaborative company, so we want you to use your knowledge of Spark to teach others, inform design decisions, and debug runtime problems.
     
     
     
       Our mission is to help the government improve healthcare for patients and reduce costs. We value bringing individuals that are experts in their disciplines, highly communicative, and self-motivated to own their work. Technology and domain experts work side-by-side in highly dynamic teams with all the roles necessary to deliver high-quality digital services. In addition, critical to our success is forming teams of highly diverse individuals passionate about making a difference.
     
     
     
       Tools & Technology:
     
     
       Spark, Hadoop, Scala, Python, and AWS EMR
       Airflow, Jenkins
       AWS Redshift and Teradata
       Git and Github
       Confluence
     
     
     
       Key Responsibilities:
     
     
       Write complex unit and integration tests for all data processing code
       Work with DevOps engineers on CI, CD, and IaC
       Read specs and translate them into test designs and test automation
       Perform code reviews and develop processes for improving code quality
     
     
     
       Basic Qualifications:
     
     
       Bachelor’s Degree
       5+ years of high volume experience with Scala, Spark, the Spark Engine, and the Spark Dataset API
       2+ years of experience with Agile methodology
       2+ years of experience performing data pipeline and data validation
       Must live in the United States, have lived in the US for 3 of the last 5 years, and be able to obtain and maintain a Public Trust Clearance.
     
     
     
       Preferred Qualifications:
     
     
       MS and 3+ years of technical experience
       Experience working in the healthcare industry with PHI/PII
       Federal Government contracting work experience
       Expertise working as part of a dynamic, interactive Agile team
       Strong written and verbal communication skills
       Demonstrated time management skills.
       Strong organizational skills with attention to detail
       Curiosity about how things work, ability to look out for potential risks
     
     
     
       #Indeed
     
     
       #LI-CC1
     
     
       #DMX
     
    
   
  
 
 
 
   Working at ICF
 
  ICF is a global advisory and technology services provider, but we’re not your typical consultants. We combine unmatched expertise with cutting-edge technology to help clients solve their most complex challenges, navigate change, and shape the future.
 
 
   We can only solve the world's toughest challenges by building an inclusive workplace that allows everyone to thrive. We are an equal opportunity employer, committed to hiring regardless of any protected characteristic, such as race, ethnicity, national origin, color, sex, gender identity/expression, sexual orientation, religion, age, disability status, or military/veteran status. Together, our employees are empowered to share their expertise and collaborate with others to achieve personal and professional goals. For more information, please read our 
  
   EEO & AA policy
  .
 
 
 
   Reasonable Accommodations are available, including, but not limited to, for disabled veterans, individuals with disabilities, and individuals with sincerely held religious beliefs, in all phases of the application and employment process. To request an accommodation please email 
  
   icfcareercenter@icf.com
   and we will be happy to assist. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. Read more about non-discrimination: 
  
   Know Your Rights
   and 
  
   Pay Transparency Statement.
  
 
 
 
   Pay Range - There are multiple factors that are considered in determining final pay for a position, including, but not limited to, relevant work experience, skills, certifications and competencies that align to the specified role, geographic location, education and certifications as well as contract provisions regarding labor categories that are specific to the position. The pay range for this position is:
  $82,673.00 - $140,544.00
  Nationwide Remote Office (US99)","<div>
 <div>
  Senior Data Engineer - Scala
 </div>
 <div></div>
 <div>
  <div>
   <div>
    <div>
     <div>
       Working at ICF means applying a passion for meaningful work with intellectual rigor to help solve the leading issues of our day. Smart, compassionate, innovative, committed, ICF employees tackle unprecedented challenges to benefit people, businesses, and governments around the globe. We believe in collaboration, mutual respect, open communication, and opportunity for growth.
     </div>
     <div></div>
     <div>
       **This role can be completely remote, sitting anywhere within the US**
     </div>
     <div></div>
     <div>
       We seek a talented Data Engineer who is eager to apply computer science, software engineering, databases, and distributed/parallel processing frameworks to prepare big data for the use of data analysts and data scientists. If you have experience with Scala and Spark and want your work to contribute to systems that collect healthcare data used by hundreds of thousands of daily users, we want to (virtually) meet you!
     </div>
     <div></div>
     <div>
       You will work on projects that support the Centers for Medicare and Medicaid Services (CMS) as we develop a next-generation analytics and reporting system that directly impacts healthcare quality. You will use Spark to build data processing pipelines that derive information from large sets of government data. You will be the go-to on your team for Spark, the Spark Engine, and the Spark Dataframe API. This program allows for the continued quality of clinicians&#x2019; work according to CMS standards. We are a collaborative company, so we want you to use your knowledge of Spark to teach others, inform design decisions, and debug runtime problems.
     </div>
     <div></div>
     <div>
       Our mission is to help the government improve healthcare for patients and reduce costs. We value bringing individuals that are experts in their disciplines, highly communicative, and self-motivated to own their work. Technology and domain experts work side-by-side in highly dynamic teams with all the roles necessary to deliver high-quality digital services. In addition, critical to our success is forming teams of highly diverse individuals passionate about making a difference.
     </div>
     <div></div>
     <div>
       Tools &amp; Technology:
     </div>
     <ul>
      <li> Spark, Hadoop, Scala, Python, and AWS EMR</li>
      <li> Airflow, Jenkins</li>
      <li> AWS Redshift and Teradata</li>
      <li> Git and Github</li>
      <li> Confluence</li>
     </ul>
     <div></div>
     <div>
       Key Responsibilities:
     </div>
     <ul>
      <li> Write complex unit and integration tests for all data processing code</li>
      <li> Work with DevOps engineers on CI, CD, and IaC</li>
      <li> Read specs and translate them into test designs and test automation</li>
      <li> Perform code reviews and develop processes for improving code quality</li>
     </ul>
     <div></div>
     <div>
       Basic Qualifications:
     </div>
     <ul>
      <li> Bachelor&#x2019;s Degree</li>
      <li> 5+ years of high volume experience with Scala, Spark, the Spark Engine, and the Spark Dataset API</li>
      <li> 2+ years of experience with Agile methodology</li>
      <li> 2+ years of experience performing data pipeline and data validation</li>
      <li> Must live in the United States, have lived in the US for 3 of the last 5 years, and be able to obtain and maintain a Public Trust Clearance.</li>
     </ul>
     <div></div>
     <div>
       Preferred Qualifications:
     </div>
     <ul>
      <li> MS and 3+ years of technical experience</li>
      <li> Experience working in the healthcare industry with PHI/PII</li>
      <li> Federal Government contracting work experience</li>
      <li> Expertise working as part of a dynamic, interactive Agile team</li>
      <li> Strong written and verbal communication skills</li>
      <li> Demonstrated time management skills.</li>
      <li> Strong organizational skills with attention to detail</li>
      <li> Curiosity about how things work, ability to look out for potential risks</li>
     </ul>
     <div></div>
     <div>
       #Indeed
     </div>
     <div>
       #LI-CC1
     </div>
     <div>
       #DMX
     </div>
    </div>
   </div>
  </div>
 </div>
 <div></div>
 <div>
   Working at ICF
 </div>
 <div></div> ICF is a global advisory and technology services provider, but we&#x2019;re not your typical consultants. We combine unmatched expertise with cutting-edge technology to help clients solve their most complex challenges, navigate change, and shape the future.
 <div></div>
 <div>
   We can only solve the world&apos;s toughest challenges by building an inclusive workplace that allows everyone to thrive. We are an equal opportunity employer, committed to hiring regardless of any protected characteristic, such as race, ethnicity, national origin, color, sex, gender identity/expression, sexual orientation, religion, age, disability status, or military/veteran status. Together, our employees are empowered to share their expertise and collaborate with others to achieve personal and professional goals. For more information, please read our 
  <div>
   EEO &amp; AA policy
  </div>.
 </div>
 <div></div>
 <div>
   Reasonable Accommodations are available, including, but not limited to, for disabled veterans, individuals with disabilities, and individuals with sincerely held religious beliefs, in all phases of the application and employment process. To request an accommodation please email 
  <div>
   icfcareercenter@icf.com
  </div> and we will be happy to assist. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. Read more about non-discrimination: 
  <div>
   Know Your Rights
  </div> and 
  <div>
   Pay Transparency Statement.
  </div>
 </div>
 <div></div>
 <div>
  <br> Pay Range - There are multiple factors that are considered in determining final pay for a position, including, but not limited to, relevant work experience, skills, certifications and competencies that align to the specified role, geographic location, education and certifications as well as contract provisions regarding labor categories that are specific to the position. The pay range for this position is:
 </div> &#x24;82,673.00 - &#x24;140,544.00
 <div></div> Nationwide Remote Office (US99)
</div>","https://icf.wd5.myworkdayjobs.com/en-US/ICFExternal_Career_Site/job/Reston-VA/Senior-Data-Engineer---Scala--Remote-_R2304300?source=indeed&source=Indeed_PPC","dc6d198d707948d7",,"Full-time",,"Reston, VA","Senior Data Engineer - Scala (Remote)","11 days ago","2023-10-14T11:53:01.703Z","3.4","666","$82,673 - $140,544 a year","2023-10-25T11:53:01.705Z","US","remote","data engineer","https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0DSYylACSg1DQGEyO4cxxwKRDBdzsQD8Ezqb3xaeFD8w2bUcpHUj6MJLSCuqCkYknUsAdB5pQBMJJsXP1G28aAaZVZhWZqPAfUj9c-NBh3siXFLb8X3HTZYVtfpNoSYEg8D-4oae1QDsS0lXyFL04QW7GQHfQhhppQS2c-0kYBkXz_icy0KH8zun7_vM6V8g21YjOWtEgXtOufwb8IjZ7du4T22rzuNcQwptY9aQS-LvJLRW_uLMM9XN7NFU-VT2MH5nmyyUGr2CRhUu9RJZybNUzT1r1y8hpIg42_xj78OkK-uwmAaesDzH6xDcfPL0UXY7zlPU9ElHHsn1ZVThwelwJ9_u8FTgvB5J8b5EADQnqNFdZkW1v4Pg3ry4VdQ0RwuvmknJsApI6ZrWAbah4znHIEkIZ4oW38OYpxmxzs8JiRuq_fGFALY0C34gueON-0fLJT-ElVCDiUWFahTFYGO8uy3efGB7q38PEhhiOSFpWgtSbuNrw0cRkth0JzrGbRy1OUn4Tod7lKQei73G-3ekGdhM-NM2OELs05qs8lw668cZo5cqmiV72Upp3zVQFgm3cxU1YYYXRxg07ApJV58JGDTaqqZZYiyfTVdYKe9WHEH0-Cm8222gDlGO83B6lRCq0-MLNDo4SIwZz1mqEBlPHsIjPrur4raAEq6tThJfVzAtwVtNM0dEK19-DnYx3yJAi9v-Go743SNKj7h2hNnCSoYcY5r1aYhIDVF8iIXAGx5WsAZzbiozD5i0ldIjZ0DrEBQjPnv5MzMAjchUafa1NDx0cxG5FAXZpQ2jMzuCg%3D%3D&xkcb=SoBv-_M3Jzd-d5TdcB0CbzkdCdPP&p=9&fvj=0&vjs=3&jsa=4844&tk=1hdjasd9nk26u800&from=jasx&wvign=1"
"Gevo, Inc.","About the role: 
  The team at Gevo works with experts in software engineering and data science. sustainability, agronomy, carbon accounting, and data science to help farmers create new value for sustainable practices. We are looking for a team member with excellent geospatial programming, analytics, and data engineering skills for the position of Geospatial Agronomist, Data Engineer. To be successful in this role you will: 
 
  Lead the overall geospatial data management program 
  Design scalable processes and programs to help acquire, organize, analyze, and display layered and temporal geospatial data sets. 
  Be the interface with both growers and external partners to lead collection and transfer of field and crop management boundaries files and other shapefiles from the farm into Verity Tracking. 
  Conduct manual (and developing automated processes for the) clean-up, corrections, and adjustments of geometries as well as attributional data, to fit into the platforms overall data flow. 
  Review, compare, validate, and normalize naming and acreage discrepancies between in-platform maps, USDA shapefiles, and third-party shapefiles. 
  Ensure maps are of acceptable quality and format with the appropriate tract, field, and farm boundaries demarcated. 
  Manage geospatial data associated with projects related to remote sensing, including data acquisition, processing, analysis, and storytelling with data. 
  Integrating remote sensing data with GIS databases and other geospatial information to provide an integrated and comprehensive understanding of the area(s) of interest. 
  Work with the data science and sustainability teams to analyze and interpret remote sensing data. This includes identifying features, objects, characteristics, and patterns within the imagery, such as land cover classification, vegetation indices, change detection, and anomaly detection. 
  Resolve problems by assisting, coaching, and training dealers/distributors on technical problem investigation and solution implementation. 
  Present and promote data management best practices to retailers, agronomy partners, and farmers. 
  Collaborate with the data engineering and data science teams to build automated geospatial workflows. 
  Identification of geospatial trends and insights to solve key tactical and strategic business problems 
  Write clean, testable, and modularized code 
  Work with key stakeholders to identify opportunities to enhance the flow, analysis, and presentation of multiple different geospatial data sets into and across the company 
  Transparently communicate priorities, obstacles, and progress on a regular basis to the Product, Sustainability, and Marketing teams. 
  Comfortably lead technical geospatial operations while following agile principles 
  
 Who you are:
  
  
  Degree or equivalent experience in a quantitative field such as data science, data analytics, geospatial engineering, or computer science; with applied experience in technical geospatial data implementation (programming and visualization) 
  Expert in common geospatial platforms and programming languages such as Python and proficient with data management systems (e.g. ArcGis, GEOJSON etc.) 
  Experience with databases / SQL dialects is preferred 
  Experience in agriculture sales and/or business development and/or crop production preferred. 
  Comfortable in data validation, verification, and quality assurance methods for geospatial data - strong analytical skills and an eye for detail are crucial for fulfilling your duties. 
  Logic-driven critical thinker committed to accuracy, precision, and increasing understanding though geospatial data visualization. 
  Strong ability to manage projects, ask the right questions, and propose product solutions. 
  Passionate about environmental sustainability with domain knowledge in agricultural practices, industrial processes, and data. 
  Experienced in building trusted business relationships. 
  Customer-oriented, with a demonstrated ability to respect and earn the respect of farmers and retail distribution partners 
  Strong written and verbal communicator 
  Enjoy attending farm shows to promote the use of products and services 
  Proficient at managing time, priorities, and expenses. 
  Self-starter, reliable, and able to work independently. 
  Computer Skills: To perform this job successfully, an individual should be proficient in Google Suite applications (and similar office suites). 
 
 Who We Are 
  
  We are People First 
  We are Mission-Focused 
  We are Agile 
  We are Innovators 
  
 What Gevo Offers You 
 
  Free health, dental, vision, life and disability insurance for employee and family 
  21 days of paid time off plus 10 paid holidays 
  401k contribution plan 
  Annual incentive plan, based on Company performance 
  Paid community service time 
  Dog friendly office 
  Be part of a smart, high performing, passionate team 
  Work-from-home stipend, if remote 
  
 Commitment to Diversity and Inclusion
  
  Gevo, Inc. is an equal opportunity employer that is committed to diversity and inclusion in the workplace. We prohibit discrimination and harassment of any kind based on race, color, sex, religion, sexual orientation, national origin, disability, genetic information, pregnancy, or any other protected characteristic as outlined by federal, state, or local laws.","<p></p>
<div>
 <p><b>About the role:</b></p> 
 <p> The team at Gevo works with experts in software engineering and data science. sustainability, agronomy, carbon accounting, and data science to help farmers create new value for sustainable practices. We are looking for a team member with excellent geospatial programming, analytics, and data engineering skills for the position of Geospatial Agronomist, Data Engineer. To be successful in this role you will: </p>
 <ul>
  <li>Lead the overall geospatial data management program </li>
  <li>Design scalable processes and programs to help acquire, organize, analyze, and display layered and temporal geospatial data sets. </li>
  <li>Be the interface with both growers and external partners to lead collection and transfer of field and crop management boundaries files and other shapefiles from the farm into Verity Tracking. </li>
  <li>Conduct manual (and developing automated processes for the) clean-up, corrections, and adjustments of geometries as well as attributional data, to fit into the platforms overall data flow. </li>
  <li>Review, compare, validate, and normalize naming and acreage discrepancies between in-platform maps, USDA shapefiles, and third-party shapefiles. </li>
  <li>Ensure maps are of acceptable quality and format with the appropriate tract, field, and farm boundaries demarcated. </li>
  <li>Manage geospatial data associated with projects related to remote sensing, including data acquisition, processing, analysis, and storytelling with data. </li>
  <li>Integrating remote sensing data with GIS databases and other geospatial information to provide an integrated and comprehensive understanding of the area(s) of interest. </li>
  <li>Work with the data science and sustainability teams to analyze and interpret remote sensing data. This includes identifying features, objects, characteristics, and patterns within the imagery, such as land cover classification, vegetation indices, change detection, and anomaly detection. </li>
  <li>Resolve problems by assisting, coaching, and training dealers/distributors on technical problem investigation and solution implementation. </li>
  <li>Present and promote data management best practices to retailers, agronomy partners, and farmers. </li>
  <li>Collaborate with the data engineering and data science teams to build automated geospatial workflows. </li>
  <li>Identification of geospatial trends and insights to solve key tactical and strategic business problems </li>
  <li>Write clean, testable, and modularized code </li>
  <li>Work with key stakeholders to identify opportunities to enhance the flow, analysis, and presentation of multiple different geospatial data sets into and across the company </li>
  <li>Transparently communicate priorities, obstacles, and progress on a regular basis to the Product, Sustainability, and Marketing teams. </li>
  <li>Comfortably lead technical geospatial operations while following agile principles</li> 
 </ul> 
 <p><b>Who you are:</b></p>
 <br> 
 <ul> 
  <li>Degree or equivalent experience in a quantitative field such as data science, data analytics, geospatial engineering, or computer science; with applied experience in technical geospatial data implementation (programming and visualization) </li>
  <li>Expert in common geospatial platforms and programming languages such as Python and proficient with data management systems (e.g. ArcGis, GEOJSON etc.) </li>
  <li>Experience with databases / SQL dialects is preferred </li>
  <li>Experience in agriculture sales and/or business development and/or crop production preferred. </li>
  <li>Comfortable in data validation, verification, and quality assurance methods for geospatial data - strong analytical skills and an eye for detail are crucial for fulfilling your duties. </li>
  <li>Logic-driven critical thinker committed to accuracy, precision, and increasing understanding though geospatial data visualization. </li>
  <li>Strong ability to manage projects, ask the right questions, and propose product solutions. </li>
  <li>Passionate about environmental sustainability with domain knowledge in agricultural practices, industrial processes, and data. </li>
  <li>Experienced in building trusted business relationships. </li>
  <li>Customer-oriented, with a demonstrated ability to respect and earn the respect of farmers and retail distribution partners </li>
  <li>Strong written and verbal communicator </li>
  <li>Enjoy attending farm shows to promote the use of products and services </li>
  <li>Proficient at managing time, priorities, and expenses. </li>
  <li>Self-starter, reliable, and able to work independently. </li>
  <li>Computer Skills: To perform this job successfully, an individual should be proficient in Google Suite applications (and similar office suites). </li>
 </ul>
 <p><b>Who We Are</b></p> 
 <ul> 
  <li>We are People First </li>
  <li>We are Mission-Focused </li>
  <li>We are Agile </li>
  <li>We are Innovators</li> 
 </ul> 
 <p><b>What Gevo Offers You</b> </p>
 <ul>
  <li>Free health, dental, vision, life and disability insurance for employee and family </li>
  <li>21 days of paid time off plus 10 paid holidays </li>
  <li>401k contribution plan </li>
  <li>Annual incentive plan, based on Company performance </li>
  <li>Paid community service time </li>
  <li>Dog friendly office </li>
  <li>Be part of a smart, high performing, passionate team </li>
  <li>Work-from-home stipend, if remote</li> 
 </ul> 
 <p><b>Commitment to Diversity and Inclusion</b></p>
 <br> 
 <p> Gevo, Inc. is an equal opportunity employer that is committed to diversity and inclusion in the workplace. We prohibit discrimination and harassment of any kind based on race, color, sex, religion, sexual orientation, national origin, disability, genetic information, pregnancy, or any other protected characteristic as outlined by federal, state, or local laws.</p>
</div>","https://gevo-inc-careers.rippling-ats.com/job/679776/geospatial-data-engineer?s=in","96205e6a1d6acd47",,,,"Remote","Geospatial Data Engineer","13 days ago","2023-10-12T11:53:01.195Z",,,"$75,000 - $114,000 a year","2023-10-25T11:53:01.198Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=96205e6a1d6acd47&from=jasx&tk=1hdjasd0pk60q801&vjs=3"
"Analytica","Analytica is seeking a remote Data Engineer to support one or more dynamic, long-term federal government enterprise big-data programs. The company works as a trusted advisor to U.S. federal government clients in health, civilian, and national security missions. The ideal candidate will be comfortable as a key member of a multi-disciplinary team working in an Agile environment to produce big data analytics solutions.  Analytica has been recognized by Inc. Magazine as one of the 250 fastest-growing US small businesses for three consecutive years. We work with U.S. government clients to build data-driven products and cultures that make an impact on our daily lives. Analytica offers competitive compensation with opportunities for bonuses, employer paid health care, unlimited training funds, and a 401k match.  Requirements (may include, but not limited to):
 
   Collaborate with client stakeholders and technical employees to optimize data collection, storage, and usage to maximize the value of information within the organization.
   Research, design, build, optimize and maintain efficient and reliable data systems, data pipelines, and models.
   Align closely with operating user requirements on data science, architecture, governance, infrastructure, and security to apply standards and optimize production environments and practices.
   Translate business needs into data architecture solutions, designing and implementing in production environments within supported data systems.
   Implement data orchestration pipelines, data sourcing, cleansing, augmentation, and quality control processes within supported data systems.
   Deploy applications to production in partnership with business units.
   Develop, test, and integrate new data features and functionality as defined by the product owners and business teams
   Work with a multi-disciplinary team of analysts, data engineers, data scientists, developers, and data consumers in a fast-paced Agile environment
 
  Qualifications:
 
   Bachelor’s degree in Computer Science, Engineering, Science, or related field.
   5+ years of data engineering and data warehousing experience
   3+ years of experience building cloud-data pipeline solutions for data ingestion, data storage, real-time processing, and analytics
   Experience working within Agile development environment, DevOps, and using version control platforms, e.g., GitHub)
   Have domain knowledge of standard data methodologies (DMBOK, NIST, etc.). 
  Team player and the ability to work effectively within a group as well as self-motivated with minimal supervision
   Excellent problem solving, collaboration, and communication skills
   AWS verifiable certification is strongly preferred 
  US Citizen with the ability to secure a US Federal Clearance
 
  About Analytica: Analytica is a leading consulting and information technology solutions provider to public sector organizations supporting health, civilian, and national security missions. Founded in 2009 and headquartered in Bethesda, MD, the company is an established 8(a) small business that has been recognized by Inc. Magazine each of the past three years as one of the 250 fastest-growing companies in the U.S. Analytica specializes in providing software and systems engineering, information management, analytics & visualization, agile project management, and management consulting services. The Software Engineering Institute (SEI) appraises the company at CMMI® Maturity Level 3 and is an ISO 9001:2008 certified provider.
  As a federal contractor, Analytica is required to verify that all employees are fully vaccinated against COVID-19. If you receive an offer and are unable to get vaccinated for religious or medical reasons, you may request a reasonable accommodation. 
  
 k1MLhVKFNs","<div>
 <p>Analytica is seeking a remote <b>Data Engineer </b>to support one or more dynamic, long-term federal government enterprise big-data programs. The company works as a trusted advisor to U.S. federal government clients in health, civilian, and national security missions. The ideal candidate will be comfortable as a key member of a multi-disciplinary team working in an Agile environment to produce big data analytics solutions.<br> <br> Analytica has been recognized by <i>Inc. Magazine</i> as one of the 250 fastest-growing US small businesses for three consecutive years. We work with U.S. government clients to build data-driven products and cultures that make an impact on our daily lives. Analytica offers competitive compensation with opportunities for bonuses, employer paid health care, unlimited training funds, and a 401k match.<br> <br> <b>Requirements </b>(may include, but not limited to)<b>:</b></p>
 <ul>
  <li> Collaborate with client stakeholders and technical employees to optimize data collection, storage, and usage to maximize the value of information within the organization.</li>
  <li> Research, design, build, optimize and maintain efficient and reliable data systems, data pipelines, and models.</li>
  <li> Align closely with operating user requirements on data science, architecture, governance, infrastructure, and security to apply standards and optimize production environments and practices.</li>
  <li> Translate business needs into data architecture solutions, designing and implementing in production environments within supported data systems.</li>
  <li> Implement data orchestration pipelines, data sourcing, cleansing, augmentation, and quality control processes within supported data systems.</li>
  <li> Deploy applications to production in partnership with business units.</li>
  <li> Develop, test, and integrate new data features and functionality as defined by the product owners and business teams</li>
  <li> Work with a multi-disciplinary team of analysts, data engineers, data scientists, developers, and data consumers in a fast-paced Agile environment</li>
 </ul>
 <p><b> Qualifications:</b></p>
 <ul>
  <li> Bachelor&#x2019;s degree in Computer Science, Engineering, Science, or related field.</li>
  <li> 5+ years of data engineering and data warehousing experience</li>
  <li> 3+ years of experience building cloud-data pipeline solutions for data ingestion, data storage, real-time processing, and analytics</li>
  <li> Experience working within Agile development environment, DevOps, and using version control platforms, e.g., GitHub)</li>
  <li> Have domain knowledge of standard data methodologies (DMBOK, NIST, etc.). </li>
  <li>Team player and the ability to work effectively within a group as well as self-motivated with minimal supervision</li>
  <li> Excellent problem solving, collaboration, and communication skills</li>
  <li> AWS verifiable certification is strongly preferred </li>
  <li>US Citizen with the ability to secure a US Federal Clearance</li>
 </ul>
 <p><b> About</b> <b>Analytica</b>: Analytica is a leading consulting and information technology solutions provider to public sector organizations supporting health, civilian, and national security missions. Founded in 2009 and headquartered in Bethesda, MD, the company is an established 8(a) small business that has been recognized by <i>Inc. Magazine</i> each of the past three years as one of the 250 fastest-growing companies in the U.S. Analytica specializes in providing software and systems engineering, information management, analytics &amp; visualization, agile project management, and management consulting services. The Software Engineering Institute (SEI) appraises the company at CMMI&#xae; Maturity Level 3 and is an ISO 9001:2008 certified provider.</p>
 <p> As a federal contractor, Analytica is required to verify that all employees are fully vaccinated against COVID-19. If you receive an offer and are unable to get vaccinated for religious or medical reasons, you may request a reasonable accommodation.<br> </p>
 <p> </p>
 <p>k1MLhVKFNs</p>
</div>","https://analyticallc.applytojob.com/apply/520e3a11547b6a70634c775859445f0e5603666d331a760f252b200b2b0f6b600f7d71/Data-Engineer?sid=8aXNfKXCSyGjhrm9c49ZbWCNdxu3nE3QLO8&source=INDE","450f2696a75feafd",,"Full-time",,"Remote","Data Engineer","8 days ago","2023-10-17T11:53:10.445Z","3.5","10",,"2023-10-25T11:53:10.447Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=450f2696a75feafd&from=jasx&tk=1hdjasd0pk60q801&vjs=3"
"Dama Technology Inc","About Dama Financial
   At Dama Financial, we use technology to solve problems that critically impact the growth and reputation of the cannabis industry. We offer innovative, compliant, sustainable financial and traceability products, removing the barriers that exclude cannabis businesses from accessing the fundamental solutions required to support a rapidly growing industry. We have a diverse team of professionals with deep expertise in financial services, payments technology, cannabis regulations, and successfully building and growing companies. Throughout the organization, you’ll find people who solve problems, deliver solutions, and deal with uncertainties while building best in class products for the industry.
 
  
  
 
   The Role
   The Data & Analytics team within Dama is growing to help clients scale their retail operations with a world-class point-of-sale system and flexible payment options. The team is also responsible for helping internal teams become more data-driven by helping them to understand, utilize and extract value from the data generated by our systems and available in our warehouse.
 
  
  
  Although we’re a growth-stage company, our environment is typical of a start-up 
 
   We work in small, high-performing teams, are fast-paced, and we all get a lot done by everyone wearing many hats.
  
 
   We are serious about optimizing our time and staying focused on the most important goals and outcomes.
  
 
   We are a 100% remote team meaning we focus on communication to ensure we can stay in sync despite our physical distance.
 
  
  
 
   What you'll do
  
 
   Report to Director of Data & Analytics
  
 
   Build and maintain pipelines to move data from source systems into our cloud data warehouse
  
 
   Transform and model data using SQL & Python
  
 
   Monitor data pipelines for errors/data quality issues and work with Dev Ops Engineering to improve observability and alerting
  
 
   Improve data quality, reliability, efficiency, and performance while optimizing cost
  
 
   Document data models, schemas, business logic, pipelines, and other metadata
  
 
   Collaborate with engineers and product managers to understand data requirements
 
  
  
 
   What we’re looking for
  
 
   You’re a great creative problem solver with an analytical mind who loves to dig in and solve hard problems
  
 
   You see data flowing and you can’t stop yourself from classifying, categorizing, organizing and directing those flows to create efficient/performant, useful and usable datasets for both operations and decision support
  
 
   You’re not just a “data person”, you’re an Engineer who specializes in data and metadata.
  
 
   You love learning new things and have a passion for building, monitoring and improving a well-oiled “data machine”
  
 
   You have the ability to work both independently and collaboratively as part of a remote team
 
  
  
  Required Experience 
 
   1-3 years of experience in a Data Engineer role working with the “Modern Data Stack”
  
 
   SQL. You know SQL. SQL is a friend of yours. You two probably share a secret handshake
  
 
   Ideal candidate will have experience with both BigQuery & dbt (including Python)
 
  
  
  Nice-to-have Experience 
 
   Integration tools (Airbyte, Stitch, Fivetran)
  
 
   Orchestration tools (Dagster, Airflow, dbt Cloud)
  
 
   Metadata tools (DataHub, OpenMetadata)
  
 
   BI/Dashboard tools (Looker, Superset, PowerBI)
 
  
  
 
   Benefits
  
 
   Healthcare
  
 
   401K
  
 
   Generous PTO
  
 
   Collaborative Environment
 
  
  
 
   What we offer
  
 
   A low ego environment where you can give and receive direct feedback.
  
 
   Managers who care about your career development.
 
  
  
 
   Due to the nature of financial systems, you will be required to pass a background check.
 
  
  
 
   Send resumes to 
  jobs@damafinancial.com
 
  
  
  CHR: Jr./Mid-Level Data Engineer
  
  
 
   LI: Jr./Mid Level Data Engineer
 
  
  
  Salary commensurate upon experience
  
  
  Bonus goals based on company goals","<div>
 <div>
  <b>About Dama Financial</b>
  <br> At Dama Financial, we use technology to solve problems that critically impact the growth and reputation of the cannabis industry. We offer innovative, compliant, sustainable financial and traceability products, removing the barriers that exclude cannabis businesses from accessing the fundamental solutions required to support a rapidly growing industry. We have a diverse team of professionals with deep expertise in financial services, payments technology, cannabis regulations, and successfully building and growing companies. Throughout the organization, you&#x2019;ll find people who solve problems, deliver solutions, and deal with uncertainties while building best in class products for the industry.
 </div>
 <br> 
 <div></div> 
 <div>
  <b> The Role</b>
  <br> The Data &amp; Analytics team within Dama is growing to help clients scale their retail operations with a world-class point-of-sale system and flexible payment options. The team is also responsible for helping internal teams become more data-driven by helping them to understand, utilize and extract value from the data generated by our systems and available in our warehouse.
 </div>
 <br> 
 <div></div> 
 <p> Although we&#x2019;re a growth-stage company, our environment is typical of a start-up</p> 
 <ul>
  <li> We work in small, high-performing teams, are fast-paced, and we all get a lot done by everyone wearing many hats.</li>
 </ul> 
 <ul>
  <li> We are serious about optimizing our time and staying focused on the most important goals and outcomes.</li>
 </ul> 
 <ul>
  <li> We are a 100% remote team meaning we focus on communication to ensure we can stay in sync despite our physical distance.</li>
 </ul>
 <br> 
 <p></p> 
 <div>
  <b> What you&apos;ll do</b>
 </div> 
 <ul>
  <li> Report to Director of Data &amp; Analytics</li>
 </ul> 
 <ul>
  <li> Build and maintain pipelines to move data from source systems into our cloud data warehouse</li>
 </ul> 
 <ul>
  <li> Transform and model data using SQL &amp; Python</li>
 </ul> 
 <ul>
  <li> Monitor data pipelines for errors/data quality issues and work with Dev Ops Engineering to improve observability and alerting</li>
 </ul> 
 <ul>
  <li> Improve data quality, reliability, efficiency, and performance while optimizing cost</li>
 </ul> 
 <ul>
  <li> Document data models, schemas, business logic, pipelines, and other metadata</li>
 </ul> 
 <ul>
  <li> Collaborate with engineers and product managers to understand data requirements</li>
 </ul>
 <br> 
 <p></p> 
 <div>
  <b> What we&#x2019;re looking for</b>
 </div> 
 <ul>
  <li> You&#x2019;re a great creative problem solver with an analytical mind who loves to dig in and solve hard problems</li>
 </ul> 
 <ul>
  <li> You see data flowing and you can&#x2019;t stop yourself from classifying, categorizing, organizing and directing those flows to create efficient/performant, useful and usable datasets for both operations and decision support</li>
 </ul> 
 <ul>
  <li> You&#x2019;re not just a &#x201c;data person&#x201d;, you&#x2019;re an Engineer who specializes in data and metadata.</li>
 </ul> 
 <ul>
  <li> You love learning new things and have a passion for building, monitoring and improving a well-oiled &#x201c;data machine&#x201d;</li>
 </ul> 
 <ul>
  <li> You have the ability to work both independently and collaboratively as part of a remote team</li>
 </ul>
 <br> 
 <p></p> 
 <p><b> Required Experience</b></p> 
 <ul>
  <li> 1-3 years of experience in a Data Engineer role working with the &#x201c;Modern Data Stack&#x201d;</li>
 </ul> 
 <ul>
  <li> SQL. You know SQL. SQL is a friend of yours. You two probably share a secret handshake</li>
 </ul> 
 <ul>
  <li> Ideal candidate will have experience with both BigQuery &amp; dbt (including Python)</li>
 </ul>
 <br> 
 <div></div> 
 <p><b> Nice-to-have Experience</b></p> 
 <ul>
  <li> Integration tools (Airbyte, Stitch, Fivetran)</li>
 </ul> 
 <ul>
  <li> Orchestration tools (Dagster, Airflow, dbt Cloud)</li>
 </ul> 
 <ul>
  <li> Metadata tools (DataHub, OpenMetadata)</li>
 </ul> 
 <ul>
  <li> BI/Dashboard tools (Looker, Superset, PowerBI)</li>
 </ul>
 <br> 
 <p></p> 
 <div>
  <b> Benefits</b>
 </div> 
 <ul>
  <li> Healthcare</li>
 </ul> 
 <ul>
  <li> 401K</li>
 </ul> 
 <ul>
  <li> Generous PTO</li>
 </ul> 
 <ul>
  <li> Collaborative Environment</li>
 </ul>
 <br> 
 <div></div> 
 <div>
  <b> What we offer</b>
 </div> 
 <ul>
  <li> A low ego environment where you can give and receive direct feedback.</li>
 </ul> 
 <ul>
  <li> Managers who care about your career development.</li>
 </ul>
 <br> 
 <div></div> 
 <div>
  <b> Due to the nature of financial systems, you will be required to pass a background check.</b>
 </div>
 <br> 
 <div></div> 
 <div>
  <b> Send resumes to </b>
  <b>jobs@damafinancial.com</b>
 </div>
 <br> 
 <div></div> 
 <p> <b>CHR: Jr./Mid-Level Data Engineer</b></p>
 <br> 
 <div></div> 
 <div>
  <b> LI: Jr./Mid Level Data Engineer</b>
 </div>
 <br> 
 <p></p> 
 <p> Salary commensurate upon experience</p>
 <br> 
 <p></p> 
 <p> Bonus goals based on company goals</p>
</div>","https://secure.entertimeonline.com/ta/CBIZ20462.careers?ShowJob=352679041","28162dd96d9b7a0d",,"Full-time",,"South San Francisco, CA 94083","Jr./Mid-Level Data Engineer","7 days ago","2023-10-18T11:53:06.004Z",,,"$70,000 - $125,000 a year","2023-10-25T11:53:06.005Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=28162dd96d9b7a0d&from=jasx&tk=1hdjasd0pk60q801&vjs=3"
"Bamboo Health","Bamboo Health is a leader in cloud-based care coordination software and analytics solutions focused on patients with complex needs, including those suffering from physical health and mental health issues and substance use disorders. We are driven by our mission of enabling better care for patients across the continuum. Our software solutions help healthcare professionals collaborate on shared patients across the spectrum of care. Join us in improving healthcare for all! 
  Summary: 
  We are actively hiring a full-time Sr. Data Integration Engineer to focus on supporting and extending our data platform. Bamboo Health receives HL7 data from hospitals, EHRs and HIEs around the country and this role will be responsible for integrating new HL7 EHR senders to the data pipeline using in-house tools, scripts, and custom applications. The ideal candidate will work well in a team, have a data-first mentality, and thrive in customer-facing projects. 
  What You Will Do: 
 
  Partner with Operations to ensure on-boarding HL7 integrations meet target deadlines through task resolution in a timely and organized manner
  
 
  Partner with broader Platform Engineering team on cross-functional initiatives focused on infrastructure scalability and stability.
  
 
  Partner with Software Engineers focused on improving our data pipeline
  
 
  Design and execute HL7 test plans for on-boarding new integrations
  
 
  Build a standard integration process to receive data and post events to new EHR systems
  
 
  Work with ADT senders to resolve customer issues and maintain high quality interfaces
  
 
  On-board and track standard HL7 integrations
  
 
  Triage customer issues related to HL7 integrations
 
  
  
  What Success Looks Like… 
  In 3 months… 
  Execute: 
 
  Develop solid understanding of Bamboo Health onboarding process for Technical Implementation Services
  
 
  Contribute to HL7 data validation, mapping, and testing processes
  
 
  Develop an understanding of our HL7 data pipeline
  
 
  Build relationships across the broader Product Platform organization
 
  
  
  In 6 months…Manage: 
 
  Work with our Product, Operations and Network Operations Center teams to drive streamlined data processing and continuous improvement initiatives.
  
 
  Contribute to the development and reporting of data quality metrics
 
  
  
  In 12 months…Scale: 
 
  Develop a comprehensive knowledge of our data ingestion architecture
  
 
  Manage complex customer integrations with a heavy focus on service and quality outcomes
 
  
  
  What You Need: 
 
  5+ years professional experience in or around software development
  
 
  Experience in or around the Healthcare domain
  
 
  Experience in at least one modern language such as Java, Python, JavaScript
  
 
  Proficient in SQL
  
 
  Willingness to learn healthcare data exchange formats
  
 
  Ability to self-start project tasks and communicate progress clearly
  
 
  Ability to work autonomously on multiple concurrent projects and prioritize appropriately
  
 
  Experience organizing and delivering on several lines of work with clear communication on progress
  
 
  Desire to work in a fast-paced collaborative environment
  
 
  A work environment that is conducive to high quality virtual interactions. This includes but is not limited to being able to work from a quiet space with minimal interruptions or distractions, and a strong internet connection.
 
  
  
  Helpful/Preferred Experience: 
  
  Healthcare data integration tools (Mirth preferred) 
  Cloud-native AWS solutions 
  SDLC – Git, pull requests 
  Docker & Kubernetes 
  Atlassian product suite 
  SumoLogic, Prometheus/Grafana, or equivalent
 
  
  
  What You Get: 
 
  Join one of the most innovative healthcare technology companies in the country.
  
 
  Have the autonomy to build something with an enthusiastically supportive team.
  
 
  Learn from working at the highest levels and on the most strategic priorities of the company, including from world class investors and advisors.
  
 
  Receive competitive compensation, including equity, with health, dental, vision and other benefits.
 
  
  
  Bamboo Health is proud to be an Equal Employment Opportunity and affirmative action employer. We celebrate diversity and are committed to creating an inclusive environment for all employees. 
  #LI-Remote","<div>
 <p>Bamboo Health is a leader in cloud-based care coordination software and analytics solutions focused on patients with complex needs, including those suffering from physical health and mental health issues and substance use disorders. We are driven by our mission of enabling better care for patients across the continuum. Our software solutions help healthcare professionals collaborate on shared patients across the spectrum of care. Join us in improving healthcare for all!</p> 
 <p><b> Summary:</b></p> 
 <p> We are actively hiring a full-time Sr. Data Integration Engineer to focus on supporting and extending our data platform. Bamboo Health receives HL7 data from hospitals, EHRs and HIEs around the country and this role will be responsible for integrating new HL7 EHR senders to the data pipeline using in-house tools, scripts, and custom applications. The ideal candidate will work well in a team, have a data-first mentality, and thrive in customer-facing projects.</p> 
 <p><b> What You Will Do:</b></p> 
 <ul>
  <li>Partner with Operations to ensure on-boarding HL7 integrations meet target deadlines through task resolution in a timely and organized manner</li>
 </ul> 
 <ul>
  <li>Partner with broader Platform Engineering team on cross-functional initiatives focused on infrastructure scalability and stability.</li>
 </ul> 
 <ul>
  <li>Partner with Software Engineers focused on improving our data pipeline</li>
 </ul> 
 <ul>
  <li>Design and execute HL7 test plans for on-boarding new integrations</li>
 </ul> 
 <ul>
  <li>Build a standard integration process to receive data and post events to new EHR systems</li>
 </ul> 
 <ul>
  <li>Work with ADT senders to resolve customer issues and maintain high quality interfaces</li>
 </ul> 
 <ul>
  <li>On-board and track standard HL7 integrations</li>
 </ul> 
 <ul>
  <li>Triage customer issues related to HL7 integrations</li>
 </ul>
 <br> 
 <p></p> 
 <p><b> What Success Looks Like&#x2026;</b></p> 
 <p><b> In 3 months&#x2026;</b></p> 
 <p> Execute:</p> 
 <ul>
  <li>Develop solid understanding of Bamboo Health onboarding process for Technical Implementation Services</li>
 </ul> 
 <ul>
  <li>Contribute to HL7 data validation, mapping, and testing processes</li>
 </ul> 
 <ul>
  <li>Develop an understanding of our HL7 data pipeline</li>
 </ul> 
 <ul>
  <li>Build relationships across the broader Product Platform organization</li>
 </ul>
 <br> 
 <p></p> 
 <p><b> In 6 months&#x2026;</b>Manage:</p> 
 <ul>
  <li>Work with our Product, Operations and Network Operations Center teams to drive streamlined data processing and continuous improvement initiatives.</li>
 </ul> 
 <ul>
  <li>Contribute to the development and reporting of data quality metrics</li>
 </ul>
 <br> 
 <p></p> 
 <p><b> In 12 months&#x2026;</b>Scale:</p> 
 <ul>
  <li>Develop a comprehensive knowledge of our data ingestion architecture</li>
 </ul> 
 <ul>
  <li>Manage complex customer integrations with a heavy focus on service and quality outcomes</li>
 </ul>
 <br> 
 <p></p> 
 <p><b> What You Need:</b></p> 
 <ul>
  <li>5+ years professional experience in or around software development</li>
 </ul> 
 <ul>
  <li>Experience in or around the Healthcare domain</li>
 </ul> 
 <ul>
  <li>Experience in at least one modern language such as Java, Python, JavaScript</li>
 </ul> 
 <ul>
  <li>Proficient in SQL</li>
 </ul> 
 <ul>
  <li>Willingness to learn healthcare data exchange formats</li>
 </ul> 
 <ul>
  <li>Ability to self-start project tasks and communicate progress clearly</li>
 </ul> 
 <ul>
  <li>Ability to work autonomously on multiple concurrent projects and prioritize appropriately</li>
 </ul> 
 <ul>
  <li>Experience organizing and delivering on several lines of work with clear communication on progress</li>
 </ul> 
 <ul>
  <li>Desire to work in a fast-paced collaborative environment</li>
 </ul> 
 <ul>
  <li>A work environment that is conducive to high quality virtual interactions. This includes but is not limited to being able to work from a quiet space with minimal interruptions or distractions, and a strong internet connection.</li>
 </ul>
 <br> 
 <p></p> 
 <p><b> Helpful/Preferred Experience:</b></p> 
 <ul> 
  <li>Healthcare data integration tools (Mirth preferred)</li> 
  <li>Cloud-native AWS solutions</li> 
  <li>SDLC &#x2013; Git, pull requests</li> 
  <li>Docker &amp; Kubernetes</li> 
  <li>Atlassian product suite</li> 
  <li>SumoLogic, Prometheus/Grafana, or equivalent</li>
 </ul>
 <br> 
 <p></p> 
 <p><b> What You Get:</b></p> 
 <ul>
  <li>Join one of the most innovative healthcare technology companies in the country.</li>
 </ul> 
 <ul>
  <li>Have the autonomy to build something with an enthusiastically supportive team.</li>
 </ul> 
 <ul>
  <li>Learn from working at the highest levels and on the most strategic priorities of the company, including from world class investors and advisors.</li>
 </ul> 
 <ul>
  <li>Receive competitive compensation, including equity, with health, dental, vision and other benefits.</li>
 </ul>
 <br> 
 <p></p> 
 <p><b><i> Bamboo Health is proud to be an Equal Employment Opportunity and affirmative action employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.</i></b></p> 
 <p> <i>#LI-Remote</i></p>
</div>","https://www.indeed.com/rc/clk?jk=e527cd7df50362d9&atk=&xpse=SoAe67I3Jzd8BOyRcJ0LbzkdCdPP","e527cd7df50362d9",,"Full-time",,"Remote","Sr. Data Integration Engineer","12 days ago","2023-10-13T11:53:11.463Z","3.8","4",,"2023-10-25T11:53:11.464Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=e527cd7df50362d9&from=jasx&tk=1hdjasd0pk60q801&vjs=3"
"The Dedham Group","About Us:
 Our mission at Pulse Analytics is to help decision-makers in oncology and other specialty therapeutic areas, identify and reduce access barriers across the healthcare industry, ensuring patients have access to the treatments they need. Our web-based decision support application connects healthcare industry organizations and key influencers to deliver targeted quality insights to support our client’s customer engagement strategies. We are currently growing and are looking to bring on talented and driven individuals to help build the future of B2B healthcare data analytics products 

 This role is for someone who is excited about architecting data infrastructure and building out data platforms from the ground up. As a Data Engineer, you will have the opportunity to work on key data delivery initiatives – automating data extraction processes and enhancing data sources for our clients. If you are passionate about leveraging data to drive business decisions and thrive in a dynamic environment, we want to hear from you. 

 
In this role you will:
 Design, build, and maintain efficient data pipelines from various sources to support key business initiatives. 
Perform data cleansing and validation processes to ensure the integrity and quality of data used for analysis. 
Act as a data evangelist within the company, promoting the value and impact of data science and engineering initiatives. 
Collaborate closely with leadership, software engineers, and product managers to understand data requirements and align data solutions with business objectives. 
Work alongside Business Analysts to identify opportunities for automated data acquisition and deliver high-quality data that drives actionable insights. 
Develop and implement data governance policies and procedures to ensure the security, privacy, and quality of data. 
Requirements 

 
Minimum Qualifications:
 3+ years of experience as a Data Engineer or in a similar role 
Proficiency in Python and SQL, with the ability to write complex scripts and queries 
Strong knowledge of data modeling, data warehousing, and ETL pipeline development 
Understanding of data management fundamentals and data storage principles 
Exceptional problem-solving and communication skills 
Experience with cloud platforms such as AWS, Azure, or Google Cloud 
Proficiency in Cloud Orchestration tools such as Airflow, Dagster, or Prefect 

 
Preferred Qualifications:
 Bachelors degree in Computer Science or related field 
Experience with Big Data Technologies (e.g. Hadoop, Hive, Spark) 
Familiarity with the AWS Ecosystem (e.g. AWS S3, AWS Athena, AWS Glue) 
Knowledge of Distributed Systems 
Benefits 

 
Company Culture and Values:
 At Pulse Analytics, we foster a collaborative and innovative environment where your ideas are heard and valued. Our core values: 

 
Service: We adopt a client-first approach to solutions 

Agile: Being flexible in our approach allows us to adapt and iterate quickly to client demands or needs 

Innovation: We strive for excellence, embrace risk-taking, and take bold actions to innovate and improve 

Transparency: When everyone is on the same page, we produce our best work 

Ownership: Everyone is a product owner and operates with integrity and self-accountability 

 
Perks and Benefits:
 401k match 
Medical, Vision, and Dental Insurance Coverage 
Casual dress code 
Remote work flexibility 
Generous PTO/Vacation 
Stipend for conferences 

 The expected base salary for this position ranges from $115,000 - $150,000. It is not typical for offers to be made at or near the top of the range. Salary offers are based on a wide range of factors including relevant skills, training, experience, education, and, where applicable, licensure or certifications obtained. Market and organizational factors are also considered. In addition to base salary and a competitive benefits package, successful candidates are eligible to receive a discretionary bonus. 

 The Dedham Group is an equal opportunities employer and does not discriminate on the grounds of gender, sexual orientation, marital or civil partner status, pregnancy or maternity, gender reassignment, race, color, nationality, ethnic or national origin, religion or belief, disability or age. Our ethos is to respect and value people’s differences, to help everyone achieve more at work as well as in their personal lives so that they feel proud of the part they play in our success. We believe that all decisions about people at work should be based on the individual’s abilities, skills, performance and behavior and our business requirements. The Dedham Group operates a zero tolerance policy to any form of discrimination, abuse or harassment. 

 #LI-REMOTE 

 #LI-YK1","<b>About Us:</b>
<br> Our mission at Pulse Analytics is to help decision-makers in oncology and other specialty therapeutic areas, identify and reduce access barriers across the healthcare industry, ensuring patients have access to the treatments they need. Our web-based decision support application connects healthcare industry organizations and key influencers to deliver targeted quality insights to support our client&#x2019;s customer engagement strategies. We are currently growing and are looking to bring on talented and driven individuals to help build the future of B2B healthcare data analytics products 
<br>
<br> This role is for someone who is excited about architecting data infrastructure and building out data platforms from the ground up. As a Data Engineer, you will have the opportunity to work on key data delivery initiatives &#x2013; automating data extraction processes and enhancing data sources for our clients. If you are passionate about leveraging data to drive business decisions and thrive in a dynamic environment, we want to hear from you. 
<br>
<br> 
<b>In this role you will:</b>
<br> Design, build, and maintain efficient data pipelines from various sources to support key business initiatives. 
<br>Perform data cleansing and validation processes to ensure the integrity and quality of data used for analysis. 
<br>Act as a data evangelist within the company, promoting the value and impact of data science and engineering initiatives. 
<br>Collaborate closely with leadership, software engineers, and product managers to understand data requirements and align data solutions with business objectives. 
<br>Work alongside Business Analysts to identify opportunities for automated data acquisition and deliver high-quality data that drives actionable insights. 
<br>Develop and implement data governance policies and procedures to ensure the security, privacy, and quality of data. 
<br>Requirements 
<br>
<br> 
<b>Minimum Qualifications:</b>
<br> 3+ years of experience as a Data Engineer or in a similar role 
<br>Proficiency in Python and SQL, with the ability to write complex scripts and queries 
<br>Strong knowledge of data modeling, data warehousing, and ETL pipeline development 
<br>Understanding of data management fundamentals and data storage principles 
<br>Exceptional problem-solving and communication skills 
<br>Experience with cloud platforms such as AWS, Azure, or Google Cloud 
<br>Proficiency in Cloud Orchestration tools such as Airflow, Dagster, or Prefect 
<br>
<br> 
<b>Preferred Qualifications:</b>
<br> Bachelors degree in Computer Science or related field 
<br>Experience with Big Data Technologies (e.g. Hadoop, Hive, Spark) 
<br>Familiarity with the AWS Ecosystem (e.g. AWS S3, AWS Athena, AWS Glue) 
<br>Knowledge of Distributed Systems 
<br>Benefits 
<br>
<br> 
<b>Company Culture and Values:</b>
<br> At Pulse Analytics, we foster a collaborative and innovative environment where your ideas are heard and valued. Our core values: 
<br>
<br> 
<b>Service:</b> We adopt a client-first approach to solutions 
<br>
<b>Agile:</b> Being flexible in our approach allows us to adapt and iterate quickly to client demands or needs 
<br>
<b>Innovation:</b> We strive for excellence, embrace risk-taking, and take bold actions to innovate and improve 
<br>
<b>Transparency:</b> When everyone is on the same page, we produce our best work 
<br>
<b>Ownership:</b> Everyone is a product owner and operates with integrity and self-accountability 
<br>
<br> 
<b>Perks and Benefits:</b>
<br> 401k match 
<br>Medical, Vision, and Dental Insurance Coverage 
<br>Casual dress code 
<br>Remote work flexibility 
<br>Generous PTO/Vacation 
<br>Stipend for conferences 
<br>
<br> The expected base salary for this position ranges from &#x24;115,000 - &#x24;150,000. It is not typical for offers to be made at or near the top of the range. Salary offers are based on a wide range of factors including relevant skills, training, experience, education, and, where applicable, licensure or certifications obtained. Market and organizational factors are also considered. In addition to base salary and a competitive benefits package, successful candidates are eligible to receive a discretionary bonus. 
<br>
<br> The Dedham Group is an equal opportunities employer and does not discriminate on the grounds of gender, sexual orientation, marital or civil partner status, pregnancy or maternity, gender reassignment, race, color, nationality, ethnic or national origin, religion or belief, disability or age. Our ethos is to respect and value people&#x2019;s differences, to help everyone achieve more at work as well as in their personal lives so that they feel proud of the part they play in our success. We believe that all decisions about people at work should be based on the individual&#x2019;s abilities, skills, performance and behavior and our business requirements. The Dedham Group operates a zero tolerance policy to any form of discrimination, abuse or harassment. 
<br>
<br> #LI-REMOTE 
<br>
<br> #LI-YK1","https://apply.workable.com/the-dedham-group/j/D3C968FDA4/","38d10858e6384637",,"Full-time",,"New York, NY","Data Engineer","13 days ago","2023-10-12T11:53:13.834Z",,,"$115,000 - $150,000 a year","2023-10-25T11:53:13.835Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=38d10858e6384637&from=jasx&tk=1hdjasd0pk60q801&vjs=3"
"PrismHR","Do you have a passion for building data architectures that enable smooth and seamless product experiences? Are you an all-around data enthusiast with a knack for ETL? We're hiring Data Engineers to help build and optimize the foundational architecture of our product's data.
  We’ve built a strong data engineering team to date, but have a lot of work ahead of us, including:
 
   Migrating from relational databases to a streaming and big data architecture, including a complete overhaul of our data feeds
   Defining streaming event data feeds required for real-time analytics and reporting
   Leveling up our platform, including enhancing our automation, test coverage, observability, alerting, and performance
 
  As a Senior Data Engineer, you will work with the development team to construct a data streaming platform and data warehouse that serves as the data foundations for our product. 
 Help us scale our business to meet the needs of our growing customer base and develop new products on our platform. You'll be a critical part of our growing company, working on a cross-functional team to implement best practices in technology, architecture, and process. You’ll have the chance to work in an open and collaborative environment, receive hands-on mentorship and have ample opportunities to grow and accelerate your career!
  Responsibilities:
 
   Build our next generation data warehouse
   Build our event stream platform
   Translate user requirements for reporting and analysis into actionable deliverables
   Enhance automation, operation, and expansion of real-time and batch data environment
   Manage numerous projects in an ever-changing work environment
   Extract, transform, and load complex data into the data warehouse using cutting-edge technologies
   Build processes for topnotch security, performance, reliability, and accuracy
   Provide mentorship and collaborate with fellow team members
 
 
  Requirements:
 
   Bachelor’s or Master’s degree in Computer Science, Information Systems, Operations Research, or related field required
   3+ years of experience building data pipelines
   3+ years of experience building data frameworks for unit testing, data lineage tracking, and automation 
  Fluency in Scala is required
   Working knowledge of Apache Spark
   Familiarity with streaming technologies (e.g., Kafka, Kinesis, Flink)
 
 
  Nice to Have:
 
   Experience with Machine Learning 
  Familiarity with Looker a plus
   Knowledge of additional server-side programming languages (e.g. Golang, C#, Ruby)
 
 
 
   Please note: This position can be remote/telecommute. Notice for candidates located in the following states: CA, CO, NJ, NY, WA: The base salary range for this position is between $100,000 - $145,000 (salary is dependent on location, experience, knowledge, and skills based on the responsibilities outlined in the job description).
 
  #LI-REMOTE
  PrismHR is a fast-paced SaaS company which provides customers with a cloud-based payroll process software application. PrismHR also provides professional services including system implementation consulting, custom configurations, and training. Lastly, via the Company’s Marketplace platform customers and end users access other human resources and employee benefits applications from PrismHR’s Marketplace Partners. 
 Diversity, Equity and Inclusion Program/Affirmative Action Plan: We have transformed our company into an inclusive environment where individuals are valued for their talents and empowered to reach their fullest potential. At PrismHR, we strive to continually lead with our values and beliefs that enable our employees to develop their potential, bring their full self to work, and engage in a world of inclusion.  Ensuring an inclusive environment for our employees is an integral part of the PrismHR culture. We aren't just checking a box, we are truly committed to creating a workplace that celebrates the diversity of our employees and fosters a sense of belonging for everyone. This is essential to our success. We are dedicated to building a diverse, inclusive, and authentic workplace, so if you’re excited about our roles but your past experience doesn’t align perfectly with every qualification in the job description, we encourage you to apply anyway. You may be just the right candidate for these open roles or other open roles. We particularly encourage applicants from traditionally under-represented groups as we seek to increase the diversity of our workforce and provide fair opportunities for all.  As a proud Equal Opportunity and Affirmative Action Employer, PrismHR encourages talent from all backgrounds to join our team. Employment decisions are based on an individual’s qualifications as they relate to the job under consideration. The Company’s policy prohibits unlawful discrimination based on sex (which includes pregnancy, childbirth, breastfeeding, or related medical conditions, the actual sex of the individual, or the gender identity or gender expression), race, color, religion, including religious dress practices and religious grooming practices, sexual orientation, national origin, ancestry, citizenship, marital status, familial status, age, physical disability, mental disability, medical condition, genetic information, protected veteran or military status, or any other consideration made unlawful by federal, state or local laws, ordinances, or regulations.  The Company is committed to complying with all applicable laws providing equal employment opportunities. This commitment applies to all persons involved in the operations of the Company and prohibits unlawful discrimination by any employee of the Company, including supervisors and co-workers. 
 Privacy Policy: For information about how we collect and use your personal information, please see our privacy statement available at https://www.prismhr.com/about/privacy-policy. 
 PrismHR provides reasonable accommodation for qualified individuals with disabilities and disabled veterans in job application procedures. If you have any difficulty using our online system and you need a reasonable accommodation due to a disability, you may use the following alternative email address to contact us about your interest in employment at PrismHR: taglobal@prismhr.com. Please indicate in the subject line of your email that you are requesting accommodation. Only candidates being considered for a position who require an accommodation will receive a follow-up response. 
 #LI-ML1
  
 vlsNrOjLD3","<div>
 <p>Do you have a passion for building data architectures that enable smooth and seamless product experiences? Are you an all-around data enthusiast with a knack for ETL? We&apos;re hiring Data Engineers to help build and optimize the foundational architecture of our product&apos;s data.</p>
 <p> We&#x2019;ve built a strong data engineering team to date, but have a lot of work ahead of us, including:</p>
 <ul>
  <li> Migrating from relational databases to a streaming and big data architecture, including a complete overhaul of our data feeds</li>
  <li> Defining streaming event data feeds required for real-time analytics and reporting</li>
  <li> Leveling up our platform, including enhancing our automation, test coverage, observability, alerting, and performance</li>
 </ul>
 <p> As a Senior Data Engineer, you will work with the development team to construct a data streaming platform and data warehouse that serves as the data foundations for our product. </p>
 <p>Help us scale our business to meet the needs of our growing customer base and develop new products on our platform. You&apos;ll be a critical part of our growing company, working on a cross-functional team to implement best practices in technology, architecture, and process. You&#x2019;ll have the chance to work in an open and collaborative environment, receive hands-on mentorship and have ample opportunities to grow and accelerate your career!</p>
 <p><b> Responsibilities:</b></p>
 <ul>
  <li> Build our next generation data warehouse</li>
  <li> Build our event stream platform</li>
  <li> Translate user requirements for reporting and analysis into actionable deliverables</li>
  <li> Enhance automation, operation, and expansion of real-time and batch data environment</li>
  <li> Manage numerous projects in an ever-changing work environment</li>
  <li> Extract, transform, and load complex data into the data warehouse using cutting-edge technologies</li>
  <li> Build processes for topnotch security, performance, reliability, and accuracy</li>
  <li> Provide mentorship and collaborate with fellow team members</li>
 </ul>
 <p></p>
 <p><b><br> Requirements:</b></p>
 <ul>
  <li> Bachelor&#x2019;s or Master&#x2019;s degree in Computer Science, Information Systems, Operations Research, or related field required</li>
  <li> 3+ years of experience building data pipelines</li>
  <li> 3+ years of experience building data frameworks for unit testing, data lineage tracking, and automation </li>
  <li>Fluency in Scala is required</li>
  <li> Working knowledge of Apache Spark</li>
  <li> Familiarity with streaming technologies (e.g., Kafka, Kinesis, Flink)</li>
 </ul>
 <p></p>
 <p><b><br> Nice to Have:</b></p>
 <ul>
  <li> Experience with Machine Learning </li>
  <li>Familiarity with Looker a plus</li>
  <li> Knowledge of additional server-side programming languages (e.g. Golang, C#, Ruby)</li>
 </ul>
 <p></p>
 <ul>
  <li><br> Please note: This position can be remote/telecommute. Notice for candidates located in the following states: CA, CO, NJ, NY, WA: The base salary range for this position is between &#x24;100,000 - &#x24;145,000 (salary is dependent on location, experience, knowledge, and skills based on the responsibilities outlined in the job description).</li>
 </ul>
 <p> #LI-REMOTE</p>
 <p> PrismHR is a fast-paced SaaS company which provides customers with a cloud-based payroll process software application. PrismHR also provides professional services including system implementation consulting, custom configurations, and training. Lastly, via the Company&#x2019;s Marketplace platform customers and end users access other human resources and employee benefits applications from PrismHR&#x2019;s Marketplace Partners.</p> 
 <p><b>Diversity, Equity and Inclusion Program/Affirmative Action Plan:</b><br> We have transformed our company into an inclusive environment where individuals are valued for their talents and empowered to reach their fullest potential. At PrismHR, we strive to continually lead with our values and beliefs that enable our employees to develop their potential, bring their full self to work, and engage in a world of inclusion.<br> <br> Ensuring an inclusive environment for our employees is an integral part of the PrismHR culture. We aren&apos;t just checking a box, we are truly committed to creating a workplace that celebrates the diversity of our employees and fosters a sense of belonging for everyone. This is essential to our success. We are dedicated to building a diverse, inclusive, and authentic workplace, so if you&#x2019;re excited about our roles but your past experience doesn&#x2019;t align perfectly with every qualification in the job description, we encourage you to apply anyway. You may be just the right candidate for these open roles or other open roles. We particularly encourage applicants from traditionally under-represented groups as we seek to increase the diversity of our workforce and provide fair opportunities for all.<br> <br> As a proud Equal Opportunity and Affirmative Action Employer, PrismHR encourages talent from all backgrounds to join our team. Employment decisions are based on an individual&#x2019;s qualifications as they relate to the job under consideration. The Company&#x2019;s policy prohibits unlawful discrimination based on sex (which includes pregnancy, childbirth, breastfeeding, or related medical conditions, the actual sex of the individual, or the gender identity or gender expression), race, color, religion, including religious dress practices and religious grooming practices, sexual orientation, national origin, ancestry, citizenship, marital status, familial status, age, physical disability, mental disability, medical condition, genetic information, protected veteran or military status, or any other consideration made unlawful by federal, state or local laws, ordinances, or regulations.<br> <br> The Company is committed to complying with all applicable laws providing equal employment opportunities. This commitment applies to all persons involved in the operations of the Company and prohibits unlawful discrimination by any employee of the Company, including supervisors and co-workers. </p>
 <p>Privacy Policy: For information about how we collect and use your personal information, please see our privacy statement available at https://www.prismhr.com/about/privacy-policy.</p> 
 <p><i>PrismHR provides reasonable accommodation for qualified individuals with disabilities and disabled veterans in job application procedures. If you have any difficulty using our online system and you need a reasonable accommodation due to a disability, you may use the following alternative email address to contact us about your interest in employment at PrismHR: taglobal@prismhr.com. Please indicate in the subject line of your email that you are requesting accommodation. Only candidates being considered for a position who require an accommodation will receive a follow-up response.</i></p> 
 <p>#LI-ML1</p>
 <p> </p>
 <p>vlsNrOjLD3</p>
</div>","https://prismhr.applytojob.com/apply/vlsNrOjLD3/Senior-Data-Engineer?source=INDE","3cf3060bcd436793",,"Full-time",,"Remote","Senior Data Engineer","12 days ago","2023-10-13T11:53:18.995Z","3.2","38","$100,000 - $145,000 a year","2023-10-25T11:53:18.997Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=3cf3060bcd436793&from=jasx&tk=1hdjasd0pk60q801&vjs=3"
"eTeam Inc.","Job Title :- Data engineer (10+ years candidate required) Job Location :- Remote   Job Description :- 
  LinkedIn is Mandatory 
  Description:- 
  
  
   Bachelors degree in the areas of Computer Science, Engineering, Information Systems, Business, or equivalent field of study required 
   7+ years of experience in working with data solutions. 
   3+ years of experience coding in Python, or Scala or similar scripting language. 
   3+ years of experience in developing data pipelines in AWS Cloud Platform (preferred), Azure, or Snowflake at scale. 
   2+ years Experience in designing and implementing data ingestion with real-time data streaming tools like Kafka, Kinesis or any similar tools.SAP/Client or other cloud integrations are preferred. 
   3+ years experience working with MPP databases such as Snowflake (Preferred) , Redshift or similar MPP databases. 
   2+ years experience working with Serverless ETL processes (Lambda, AWS Glue, Matillion or similar) 
   1+ years experience with big data technologies like EMR, Hadoop, Spark, Cassandra, MongoDB or other open source big data tools. 
   Knowledge of professional software engineering best practices for the full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations. 
   Experience designing, documenting, and defending designs for key components in large distributed computing systems 
   Demonstrated ability to learn new technologies quickly and independently 
   Demonstrated ability to achieve stretch goals in a very innovative and fast paced environment 
   Ability to handle multiple competing priorities in a fast-paced environment 
   Excellent verbal and written communication skills, especially in technical communications 
   Strong interpersonal skills and a desire to work collaboratively 
  
  Experience participating in an Agile software development team, e.g. SCRUM 
  
  Job Responsibilities : 
  
   Responsible for the building, deployment, and maintenance of critical scalable Data Pipelines to assemble large, complex sets of data that meet non-functional and functional business requirements 
   Work closely with SMEs, Data Modeler, Architects, Analysts and other team members on requirements to build scalable real time/near real time/batch data solutions. 
   Contributes design, code, configurations, and documentation for components that manage data ingestion, real time streaming, batch processing, data extraction, transformation, and loading into Data Lake/Cloud Data Warehouse/MPP (Snowflake/Redshift/similar Technologies ) . 
   Owns one or more key components of the infrastructure and works to continually improve it, identifying gaps and improving the platforms quality, robustness, maintainability, and speed. 
   Cross-trains other team members on technologies being developed, while also continuously learning new technologies from other team members. 
   Interacts with technical teams across and ensures that solutions meet customer requirements in terms of functionality, performance, availability, scalability, and reliability. 
   Performs development, QA, and dev-ops roles as needed to ensure total end to end responsibility of solutions. 
   Keep up with current trends in big data and Analytics , evaluate tools and pace yourself for innovation. 
  
  Mentor Junior engineers ,create necessary documentation and Run-books while still being able to deliver on goals","<div>
 <div>
  <p><b>Job Title :- Data engineer (10+ years candidate required)</b><br> <b>Job Location :-</b><b> </b><b>Remote</b><b> </b><br> <br> <b>Job Description :-</b><b> </b></p>
  <p><b>LinkedIn is Mandatory</b><b> </b></p>
  <p><b>Description:-</b> </p>
  <p></p>
  <ul>
   <li>Bachelors degree in the areas of Computer Science, Engineering, Information Systems, Business, or equivalent field of study required</li> 
   <li>7+ years of experience in working with data solutions.</li> 
   <li>3+ years of experience coding in Python, or Scala or similar scripting language. </li>
   <li>3+ years of experience in developing data pipelines in AWS Cloud Platform (preferred), Azure, or Snowflake at scale.</li> 
   <li>2+ years Experience in designing and implementing data ingestion with real-time data streaming tools like Kafka, Kinesis or any similar tools.SAP/Client or other cloud integrations are preferred.</li> 
   <li>3+ years experience working with MPP databases such as Snowflake (Preferred) , Redshift or similar MPP databases.</li> 
   <li>2+ years experience working with Serverless ETL processes (Lambda, AWS Glue, Matillion or similar)</li> 
   <li>1+ years experience with big data technologies like EMR, Hadoop, Spark, Cassandra, MongoDB or other open source big data tools. </li>
   <li>Knowledge of professional software engineering best practices for the full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations.</li> 
   <li>Experience designing, documenting, and defending designs for key components in large distributed computing systems</li> 
   <li>Demonstrated ability to learn new technologies quickly and independently</li> 
   <li>Demonstrated ability to achieve stretch goals in a very innovative and fast paced environment</li> 
   <li>Ability to handle multiple competing priorities in a fast-paced environment</li> 
   <li>Excellent verbal and written communication skills, especially in technical communications</li> 
   <li>Strong interpersonal skills and a desire to work collaboratively</li> 
  </ul>
  <p>Experience participating in an Agile software development team, e.g. SCRUM</p> 
  <p></p>
  <p><b>Job Responsibilities</b><b> </b>: </p>
  <ul>
   <li>Responsible for the building, deployment, and maintenance of critical scalable Data Pipelines to assemble large, complex sets of data that meet non-functional and functional business requirements</li> 
   <li>Work closely with SMEs, Data Modeler, Architects, Analysts and other team members on requirements to build scalable real time/near real time/batch data solutions.</li> 
   <li>Contributes design, code, configurations, and documentation for components that manage data ingestion, real time streaming, batch processing, data extraction, transformation, and loading into Data Lake/Cloud Data Warehouse/MPP (Snowflake/Redshift/similar Technologies ) .</li> 
   <li>Owns one or more key components of the infrastructure and works to continually improve it, identifying gaps and improving the platforms quality, robustness, maintainability, and speed.</li> 
   <li>Cross-trains other team members on technologies being developed, while also continuously learning new technologies from other team members.</li> 
   <li>Interacts with technical teams across and ensures that solutions meet customer requirements in terms of functionality, performance, availability, scalability, and reliability.</li> 
   <li>Performs development, QA, and dev-ops roles as needed to ensure total end to end responsibility of solutions.</li> 
   <li>Keep up with current trends in big data and Analytics , evaluate tools and pace yourself for innovation.</li> 
  </ul>
  <p>Mentor Junior engineers ,create necessary documentation and Run-books while still being able to deliver on goals</p>
 </div>
</div>","https://www.indeed.com/rc/clk?jk=a03e6162d11ae269&atk=&xpse=SoCs67I3Jzd6g0xj0J0LbzkdCdPP","a03e6162d11ae269",,"Contract",,"Cary, NC","Data engineer","13 days ago","2023-10-12T11:53:22.509Z",,,"$45 - $55 an hour","2023-10-25T11:53:22.510Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=a03e6162d11ae269&from=jasx&tk=1hdjasd0pk60q801&vjs=3"
"Atlassian","Overview: 
  Atlassian is looking for a Data Engineer to join our Data Engineering Team. You will build top-notch data solutions and applications that inspire important decisions across the organization. You will be reporting to the Senior Data Engineering Manager.
  You'll have flexibility in where you work – whether in an office, from home (remote), or a combination of the two. 
 Compensation
  At Atlassian, we strive to design equitable, explainable, and competitive compensation programs. To support this goal, the baseline of our range is higher than that of the typical market range, but in turn we expect to hire most candidates near this baseline. Base pay within the range is ultimately determined by a candidate's skills, expertise, or experience. In the United States, we have three geographic pay zones. For this role, our current base pay ranges for new hires in each zone are: 
 Zone A: $147,500 - $196,600
  Zone B: $132,700 - $177,000
  Zone C: $122,400 - $163,200
  This role may also be eligible for benefits, bonuses, commissions, and equity. 
 Please visit go.atlassian.com/payzones for more information on which locations are included in each of our geographic pay zones. However, please confirm the zone for your specific location with your recruiter. Responsibilities: 
  A typical day may involve collaborating with partners, you will design data models, acquisition processes, and applications to address needs. With experience in large-scale data processing systems (batch and streaming), you will lead business growth and enhance product experiences. And will collaborate with Technology Teams, Global Analytical Teams, and Data Scientists across programs.
  You'll take ownership of problems from end-to-end: extracting/cleaning data, and understanding generating systems. Improving the quality of data by adding sources, coding rules, and producing metrics is crucial as requirements evolve. Agility and smart risk-taking are important qualities in this industry where digital innovation meets partner/customer needs over time. Qualifications: 
  On your first day, we'll expect you to have:
 
   BS in Computer Science or equivalent experience with 3+ years as a Data Engineer or a similar role
   Programming skills in Python & Java (good to have)
   Design data models for storage and retrieval to meet product and requirements
   Build scalable data pipelines using Spark, Airflow, AWS data services (Redshift, Athena, EMR), Apache projects (Spark, Flink, Hive, and Kafka)
   Familiar with modern software development practices (Agile, TDD, CICD) applied to data engineering
   Enhance data quality through internal tools/frameworks detecting DQ issues. Working knowledge of relational databases and SQL query authoring
 
  We’d be super excited if you have:
 
   Followed a Kappa architecture with any of your previous deployments and domain knowledge of Financial and People System","<div>
 Overview: 
 <p> Atlassian is looking for a Data Engineer to join our Data Engineering Team. You will build top-notch data solutions and applications that inspire important decisions across the organization. You will be reporting to the Senior Data Engineering Manager.</p>
 <p> You&apos;ll have flexibility in where you work &#x2013; whether in an office, from home (remote), or a combination of the two.</p> 
 <p><b>Compensation</b></p>
 <p> At Atlassian, we strive to design equitable, explainable, and competitive compensation programs. To support this goal, the baseline of our range is higher than that of the typical market range, but in turn we expect to hire most candidates near this baseline. Base pay within the range is ultimately determined by a candidate&apos;s skills, expertise, or experience. In the United States, we have three geographic pay zones. For this role, our current base pay ranges for new hires in each zone are:</p> 
 <p>Zone A: &#x24;147,500 - &#x24;196,600</p>
 <p> Zone B: &#x24;132,700 - &#x24;177,000</p>
 <p> Zone C: &#x24;122,400 - &#x24;163,200</p>
 <p> This role may also be eligible for benefits, bonuses, commissions, and equity.</p> 
 <p>Please visit go.atlassian.com/payzones for more information on which locations are included in each of our geographic pay zones. However, please confirm the zone for your specific location with your recruiter.</p> Responsibilities: 
 <p> A typical day may involve collaborating with partners, you will design data models, acquisition processes, and applications to address needs. With experience in large-scale data processing systems (batch and streaming), you will lead business growth and enhance product experiences. And will collaborate with Technology Teams, Global Analytical Teams, and Data Scientists across programs.</p>
 <p> You&apos;ll take ownership of problems from end-to-end: extracting/cleaning data, and understanding generating systems. Improving the quality of data by adding sources, coding rules, and producing metrics is crucial as requirements evolve. Agility and smart risk-taking are important qualities in this industry where digital innovation meets partner/customer needs over time.</p> Qualifications: 
 <p><b> On your first day, we&apos;ll expect you to have:</b></p>
 <ul>
  <li><p> BS in Computer Science or equivalent experience with 3+ years as a Data Engineer or a similar role</p></li>
  <li><p> Programming skills in Python &amp; Java (good to have)</p></li>
  <li><p> Design data models for storage and retrieval to meet product and requirements</p></li>
  <li><p> Build scalable data pipelines using Spark, Airflow, AWS data services (Redshift, Athena, EMR), Apache projects (Spark, Flink, Hive, and Kafka)</p></li>
  <li><p> Familiar with modern software development practices (Agile, TDD, CICD) applied to data engineering</p></li>
  <li><p> Enhance data quality through internal tools/frameworks detecting DQ issues. Working knowledge of relational databases and SQL query authoring</p></li>
 </ul>
 <p><b> We&#x2019;d be super excited if you have:</b></p>
 <ul>
  <li><p> Followed a Kappa architecture with any of your previous deployments and domain knowledge of Financial and People System</p></li>
 </ul>
</div>
<p></p>","https://careers-americas.icims.com/jobs/11427/job?utm_source=indeed_integration&iis=Job%20Board&iisn=Indeed&indeed-apply-token=73a2d2b2a8d6d5c0a62696875eaebd669103652d3f0c2cd5445d3e66b1592b0f&lever-source=Indeed","b3bf485f8a0f7808",,"Full-time",,"San Francisco, CA 94104","Data Engineer","7 days ago","2023-10-18T11:53:22.164Z","4.1","34",,"2023-10-25T11:53:22.165Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=b3bf485f8a0f7808&from=jasx&tk=1hdjasd0pk60q801&vjs=3"
"AutoAlert","Data Engineer
  Location – Remote
  Travel Required – Minimal
  Classification – Exempt
  Salary Range - $90,000 to $113,000
  Leadership Level – GS 5 to GS 7
  
  
 
  Core Competencies associated with this role: Analytical Skills, Continuous Learning, Cooperation, Job Knowledge, Problem Solving, Quality, Teamwork, Use of Technology
 
  AutoAlert is the original disruptor in the automotive software space. Our Customer Experience Management (CXM) platform is revolutionizing the dealership-customer relationship, creating direct opportunities for meaningful connections and seamless experiences both online and offline.
  
  
 About AutoAlert
  AutoAlert offers a portfolio of innovative solutions that maximize dealership profitability by focusing on customer relationships and creating direct opportunities for meaningful data-driven connections. Founded in 2002, AutoAlert is the leading automotive software and data mining provider, enhancing customer relationships that lead to repeat sales, service, and increased loyalty. AutoAlert’s mission is to empower innovative automotive partnerships to improve data-driven customer experiences. AutoAlert is proud to lead the industry in data security, with independently audited high-level security in place via Soc 2 Type 2 and ISO/IEC 27001:2013 certifications.
  
  
 Role Description
  At AutoAlert, the Data Engineer will manage the intake, export, and preservation of data as it moves throughout the company. The Data Engineer is expected to have technical skills working with many different relational and non-relational database management systems and have knowledge of how to move data between those systems. ETL is a top-line priority which can span Databases, Big Data architectures, Data Lakes and Datamarts. This role involves execution on data distribution for downstream analysis by Data Scientists and Data Analysts. Data Engineers must balance requirements and current/future use within Data Governance procedures and guidelines. The role provides solutions for errors, data anomalies, system events, reliability, efficiency and quality. 
  
  You are a team player with strong problem-solving skills and an innovative mindset. You are part of a creative, diverse, collaborative team of hardworking individuals with the goal of increasing brand favorability, customer engagement and advocacy, and revenue growth. 
  
 
  Collaborate with the Data Science team, using Scrum to implement the Software Development Life Cycle (SDLC) and Data Science Life Cycle (DSL)
   Deliver data related objectives within the team
   Design Python programs geared towards ETL/ELT
   Interact with the team by performing code reviews and QA
   Help drive product vision/enhancement requests
   Collaborate with other team members and product stakeholders
   Assemble and model industry best-practices in order to foster best-in-class software development within and across departments
   Quickly understand functional requirements and work with the team to provide solutions
   Understand the need for time-to-market
   Other duties as assigned
 
  
  Your Experience and Impact
 
   Bachelor’s degree in Data Science, Computer Science, Software Development, Data or other technical discipline preferred
   2-5 years of Data Engineering work
   Strong proficiency in Python is required
   Strong proficiency in Linux, command line tools, and scripting
   Strong proficiency with ETL/data transformations
   Exposure to Amazon Web Services (AWS)
   Working knowledge of MPPs, ideally AWS Redshift
   Experience with RDBMS, ideally PostgreSQL and MSSQL
   Experience with back-end application support
   Demonstrated proficiency with Data Warehousing design and implementation
   Experience working with very large data sets
   Experience working with horizontally scalable, distributed architectures
 
  
  Supervisory Responsibilities
  This position has no direct supervisory responsibilities but works with other team members inside and outside of the department
  
  
 Across all teams, we look for the following Values:
 
   Be a Role Model
   Be Passionate About our Partners’ Success
   Own Working Together
   Deliver Results
 
  Living the AutoAlert values is core to all team members’ success. We welcome and encourage all people of diverse backgrounds, experiences, abilities, and perspectives. We are an equal opportunity employer, focused on providing a positive place for you to grow your career.
  
  
 Accommodations
  If you require assistance applying for open positions, please reach out to Human Resources at hr@autoalert.com.
  
  
 Benefits
  AutoAlert provides a robust benefits package to eligible employees. Eligibility requirements apply to all plans in the United States and Canada. AutoAlert reserves the right to alter benefits offerings at will.
  
  
 Posting Statement
  AutoAlert is an Equal Opportunity Employer. Qualified applicants will receive consideration for employment regardless of race, color, religion, sex, sexual orientation, gender perception or identity, national origin, age, marital status, protected veteran status, or disability status. 
  
  AutoAlert does not accept unsolicited headhunter and agency resumes. AutoAlert will not pay any third-party agency or company that does not have a signed agreement with AutoAlert. 
  
  We are unable to sponsor H1-B Visas at this time.","<div>
 <b>Data Engineer</b>
 <br> Location &#x2013; Remote
 <br> Travel Required &#x2013; Minimal
 <br> Classification &#x2013; Exempt
 <br> Salary Range - &#x24;90,000 to &#x24;113,000
 <br> Leadership Level &#x2013; GS 5 to GS 7
 <br> 
 <br> 
 <ul>
  <li><b>Core Competencies associated with this role: Analytical Skills, Continuous Learning, Cooperation, Job Knowledge, Problem Solving, Quality, Teamwork, Use of Technology</b></li>
 </ul>
 <br> AutoAlert is the original disruptor in the automotive software space. Our Customer Experience Management (CXM) platform is revolutionizing the dealership-customer relationship, creating direct opportunities for meaningful connections and seamless experiences both online and offline.
 <br> 
 <br> 
 <b>About AutoAlert</b>
 <br> AutoAlert offers a portfolio of innovative solutions that maximize dealership profitability by focusing on customer relationships and creating direct opportunities for meaningful data-driven connections. Founded in 2002, AutoAlert is the leading automotive software and data mining provider, enhancing customer relationships that lead to repeat sales, service, and increased loyalty. AutoAlert&#x2019;s mission is to empower innovative automotive partnerships to improve data-driven customer experiences. AutoAlert is proud to lead the industry in data security, with independently audited high-level security in place via Soc 2 Type 2 and ISO/IEC 27001:2013 certifications.
 <br> 
 <br> 
 <b>Role Description</b>
 <br> At AutoAlert, the Data Engineer will manage the intake, export, and preservation of data as it moves throughout the company. The Data Engineer is expected to have technical skills working with many different relational and non-relational database management systems and have knowledge of how to move data between those systems. ETL is a top-line priority which can span Databases, Big Data architectures, Data Lakes and Datamarts. This role involves execution on data distribution for downstream analysis by Data Scientists and Data Analysts. Data Engineers must balance requirements and current/future use within Data Governance procedures and guidelines. The role provides solutions for errors, data anomalies, system events, reliability, efficiency and quality. 
 <br> 
 <br> You are a team player with strong problem-solving skills and an innovative mindset. You are part of a creative, diverse, collaborative team of hardworking individuals with the goal of increasing brand favorability, customer engagement and advocacy, and revenue growth. 
 <br> 
 <ul>
  <li>Collaborate with the Data Science team, using Scrum to implement the Software Development Life Cycle (SDLC) and Data Science Life Cycle (DSL)</li>
  <li> Deliver data related objectives within the team</li>
  <li> Design Python programs geared towards ETL/ELT</li>
  <li> Interact with the team by performing code reviews and QA</li>
  <li> Help drive product vision/enhancement requests</li>
  <li> Collaborate with other team members and product stakeholders</li>
  <li> Assemble and model industry best-practices in order to foster best-in-class software development within and across departments</li>
  <li> Quickly understand functional requirements and work with the team to provide solutions</li>
  <li> Understand the need for time-to-market</li>
  <li> Other duties as assigned</li>
 </ul>
 <br> 
 <b> Your Experience and Impact</b>
 <ul>
  <li> Bachelor&#x2019;s degree in Data Science, Computer Science, Software Development, Data or other technical discipline preferred</li>
  <li> 2-5 years of Data Engineering work</li>
  <li> Strong proficiency in Python is required</li>
  <li> Strong proficiency in Linux, command line tools, and scripting</li>
  <li> Strong proficiency with ETL/data transformations</li>
  <li> Exposure to Amazon Web Services (AWS)</li>
  <li> Working knowledge of MPPs, ideally AWS Redshift</li>
  <li> Experience with RDBMS, ideally PostgreSQL and MSSQL</li>
  <li> Experience with back-end application support</li>
  <li> Demonstrated proficiency with Data Warehousing design and implementation</li>
  <li> Experience working with very large data sets</li>
  <li> Experience working with horizontally scalable, distributed architectures</li>
 </ul>
 <br> 
 <b> Supervisory Responsibilities</b>
 <br> This position has no direct supervisory responsibilities but works with other team members inside and outside of the department
 <br> 
 <br> 
 <b>Across all teams, we look for the following Values:</b>
 <ul>
  <li> Be a Role Model</li>
  <li> Be Passionate About our Partners&#x2019; Success</li>
  <li> Own Working Together</li>
  <li> Deliver Results</li>
 </ul>
 <br> Living the AutoAlert values is core to all team members&#x2019; success. We welcome and encourage all people of diverse backgrounds, experiences, abilities, and perspectives. We are an equal opportunity employer, focused on providing a positive place for you to grow your career.
 <br> 
 <br> 
 <b>Accommodations</b>
 <br> If you require assistance applying for open positions, please reach out to Human Resources at hr@autoalert.com.
 <br> 
 <br> 
 <b>Benefits</b>
 <br> AutoAlert provides a robust benefits package to eligible employees. Eligibility requirements apply to all plans in the United States and Canada. AutoAlert reserves the right to alter benefits offerings at will.
 <br> 
 <br> 
 <b>Posting Statement</b>
 <br> AutoAlert is an Equal Opportunity Employer. Qualified applicants will receive consideration for employment regardless of race, color, religion, sex, sexual orientation, gender perception or identity, national origin, age, marital status, protected veteran status, or disability status. 
 <br> 
 <br> AutoAlert does not accept unsolicited headhunter and agency resumes. AutoAlert will not pay any third-party agency or company that does not have a signed agreement with AutoAlert. 
 <br> 
 <br> We are unable to sponsor H1-B Visas at this time.
</div>","http://j.brt.mv/PortalViewRequirement.do?source=Indeed&reqGK=27714986","2766e6efdad0e143",,"Full-time",,"Kansas City, MO 64105","Data Engineer","8 days ago","2023-10-17T11:53:22.226Z","3.4","48",,"2023-10-25T11:53:22.227Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=2766e6efdad0e143&from=jasx&tk=1hdjasd0pk60q801&vjs=3"
"Blueprint Technologies","Senior Data Engineer, Azure 
  Senior Data Engineer 
  Remote-US Only 
  Who is Blueprint? 
  We are a technology solutions firm headquartered in Bellevue, Washington, with a strong presence across the United States. Unified by a shared passion for solving complicated problems, our people are our greatest asset. We use technology as a tool to bridge the gap between strategy and execution, powered by the knowledge, skills, and the expertise of our teams, who all have unique perspectives and years of experience across multiple industries. We're bold, smart, agile, and fun. 
  What does Blueprint do? 
  Blueprint helps organizations unlock value from existing assets by leveraging cutting-edge technology to create additional revenue streams and new lines of business. We connect strategy, business solutions, products, and services to transform and grow companies. 
  Why Blueprint? 
  At Blueprint, we believe in the power of possibility and are passionate about bringing it to life. Whether you join our bustling product division, our multifaceted services team or you want to grow your career in human resources, your ability to make an impact is amplified when you join one of our teams. You'll focus on solving unique business problems while gaining hands-on experience with the world's best technology. We believe in unique perspectives and build teams of people with diverse skillsets and backgrounds. At Blueprint, you'll have the opportunity to work with multiple clients and teams, such as data science and product development, all while learning, growing, and developing new solutions. We guarantee you won't find a better place to work and thrive than at Blueprint. 
  What will I be doing? 
  Blueprint is looking for an Azure, Senior Data Engineer to join us as we build cutting-edge technology solutions! This is a fast-paced role that needs a dedicated and passionate individual focused on team and client satisfaction! The ideal candidate will have a solid background in consulting, with demonstrating experience in leading clients through the process of building modern data estates. You will also be responsible for mentoring any junior developers on the engagement. 
  Responsibilities: 
  
  Develop and implement effective data architecture solutions using Databricks and Lakehouse 
  Optimize and tune data pipelines for performance and scalability 
  Monitor and troubleshoot data pipelines to ensure data availability and reliability 
  Collaborate with data scientists, analysts, and other stakeholders to understand their data needs and build solutions that enable them to extract insights from data 
  Implement best practices for data governance, data security, and data quality to ensure data integrity across all data sources 
  Create and maintain documentation related to data architecture, data pipelines, and data models 
  Stay up to date with emerging technologies and best practices in data engineering and big data processing 
  Mentor and train other data engineers on best practices for data engineering and Databricks usage 
  Provide thought leadership in the Databricks and Lakehouse space, both within the organization and externally 
  
 Qualifications: 
  
  Bachelor's degree in computer science or equivalent experience 
  At least 5+ -years of experience as a data engineer 
  At least 5+ years of experience with SQL Development (ETL transformations, stored procedure) 
  Data Ingestion experience from inception to Gold Medallion 
 
 
  Strong understanding of data engineering, data warehousing, data modeling, data governance, and data security best practices 
 
 
  At least 3-5 -years of experience programming with PySpark performing various transformations 
  Design, build, implement and maintain our data infrastructure to power analytics and ML 
  Partner with engineers, producers and designers to deliver data insights that impact our players 
  Contribute to our investments into various open-source and 3rd party tools to build a system that scales with the company 
  Collaborate with our Data Scientists and Analytics Engineers to make pipeline implementation faster, more straightforward, and more trustworthy driven decisions that will shape our business 
  2-5+ years building large scale data infrastructure on Spark/Databricks or similar 
  3+ years experience working with real-time data ingestion/processing 
  Working knowledge of Databricks DLT(Delta Live Table) and Unity Catalog a plus 
  Experience with relational and non-relational database technologies (i.e., NoSQL, blob storage, etc.) 
  Experience with data wrangling skills with csv, tsv, parquet, and json 
  Experience designing, building and optimizing Big Data platforms that are robust, scalable and reliable 
  Excellent problem-solving and troubleshooting skills 
  
 Salary Range 
  Pay ranges vary based on multiple factors including, without limitation, skill sets, education, responsibilities, experience, and geographical market. The pay range for this position reflects geographic based ranges for Washington state: $146,400 to $175,100 USD/annually. The salary/wage and job title for this opening will be based on the selected candidate's qualifications and experience and may be outside this range. 
  Equal Opportunity Employer 
  Blueprint Technologies, LLC is an equal employment opportunity employer. Qualified applicants are considered without regard to race, color, age, disability, sex, gender identity or expression, orientation, veteran/military status, religion, national origin, ancestry, marital, or familial status, genetic information, citizenship, or any other status protected by law. 
  If you need assistance or a reasonable accommodation to complete the application process, please reach out to: recruiting@bpcs.com 
  Blueprint believe in the importance of a healthy and happy team, which is why our comprehensive benefits package includes: 
  
  Medical, dental, and vision coverage 
  Flexible Spending Account 
  401k program 
  Competitive PTO offerings 
  Parental Leave 
  Opportunities for professional growth and development","<div>
 <p><b>Senior Data Engineer, Azure</b></p> 
 <p><b> Senior Data Engineer</b></p> 
 <p><b> Remote-US Only</b></p> 
 <p><b> Who is Blueprint?</b></p> 
 <p> We are a technology solutions firm headquartered in Bellevue, Washington, with a strong presence across the United States. Unified by a shared passion for solving complicated problems, our people are our greatest asset. We use technology as a tool to bridge the gap between strategy and execution, powered by the knowledge, skills, and the expertise of our teams, who all have unique perspectives and years of experience across multiple industries. We&apos;re bold, smart, agile, and fun.</p> 
 <p><b> What does Blueprint do?</b></p> 
 <p> Blueprint helps organizations unlock value from existing assets by leveraging cutting-edge technology to create additional revenue streams and new lines of business. We connect strategy, business solutions, products, and services to transform and grow companies.</p> 
 <p><b> Why Blueprint?</b></p> 
 <p> At Blueprint, we believe in the power of possibility and are passionate about bringing it to life. Whether you join our bustling product division, our multifaceted services team or you want to grow your career in human resources, your ability to make an impact is amplified when you join one of our teams. You&apos;ll focus on solving unique business problems while gaining hands-on experience with the world&apos;s best technology. We believe in unique perspectives and build teams of people with diverse skillsets and backgrounds. At Blueprint, you&apos;ll have the opportunity to work with multiple clients and teams, such as data science and product development, all while learning, growing, and developing new solutions. We guarantee you won&apos;t find a better place to work and thrive than at Blueprint.</p> 
 <p><b> What will I be doing?</b></p> 
 <p> Blueprint is looking for an <b>Azure, Senior Data Engineer </b>to join us as we build cutting-edge technology solutions! This is a fast-paced role that needs a dedicated and passionate individual focused on team and client satisfaction! The ideal candidate will have a solid background in consulting, with demonstrating experience in leading clients through the process of building modern data estates. You will also be responsible for mentoring any junior developers on the engagement.</p> 
 <p><b> Responsibilities:</b></p> 
 <ul> 
  <li>Develop and implement effective data architecture solutions using Databricks and Lakehouse</li> 
  <li>Optimize and tune data pipelines for performance and scalability</li> 
  <li>Monitor and troubleshoot data pipelines to ensure data availability and reliability</li> 
  <li>Collaborate with data scientists, analysts, and other stakeholders to understand their data needs and build solutions that enable them to extract insights from data</li> 
  <li>Implement best practices for data governance, data security, and data quality to ensure data integrity across all data sources</li> 
  <li>Create and maintain documentation related to data architecture, data pipelines, and data models</li> 
  <li>Stay up to date with emerging technologies and best practices in data engineering and big data processing</li> 
  <li>Mentor and train other data engineers on best practices for data engineering and Databricks usage</li> 
  <li>Provide thought leadership in the Databricks and Lakehouse space, both within the organization and externally</li> 
 </ul> 
 <p><b>Qualifications:</b></p> 
 <ul> 
  <li>Bachelor&apos;s degree in computer science or equivalent experience</li> 
  <li>At least 5+ -years of experience as a data engineer</li> 
  <li>At least 5+ years of experience with SQL Development (ETL transformations, stored procedure)</li> 
  <li>Data Ingestion experience from inception to Gold Medallion</li> 
 </ul>
 <ul>
  <li>Strong understanding of data engineering, data warehousing, data modeling, data governance, and data security best practices</li> 
 </ul>
 <ul>
  <li>At least 3-5 -years of experience programming with PySpark performing various transformations</li> 
  <li>Design, build, implement and maintain our data infrastructure to power analytics and ML</li> 
  <li>Partner with engineers, producers and designers to deliver data insights that impact our players</li> 
  <li>Contribute to our investments into various open-source and 3rd party tools to build a system that scales with the company</li> 
  <li>Collaborate with our Data Scientists and Analytics Engineers to make pipeline implementation faster, more straightforward, and more trustworthy driven decisions that will shape our business</li> 
  <li>2-5+ years building large scale data infrastructure on Spark/Databricks or similar</li> 
  <li>3+ years experience working with real-time data ingestion/processing</li> 
  <li>Working knowledge of Databricks DLT(Delta Live Table) and Unity Catalog a plus</li> 
  <li>Experience with relational and non-relational database technologies (i.e., NoSQL, blob storage, etc.)</li> 
  <li>Experience with data wrangling skills with csv, tsv, parquet, and json</li> 
  <li>Experience designing, building and optimizing Big Data platforms that are robust, scalable and reliable</li> 
  <li>Excellent problem-solving and troubleshooting skills</li> 
 </ul> 
 <p><b>Salary Range</b></p> 
 <p> Pay ranges vary based on multiple factors including, without limitation, skill sets, education, responsibilities, experience, and geographical market. The pay range for this position reflects geographic based ranges for Washington state: &#x24;146,400 to &#x24;175,100 USD/annually. The salary/wage and job title for this opening will be based on the selected candidate&apos;s qualifications and experience and may be outside this range.</p> 
 <p><b> Equal Opportunity Employer</b></p> 
 <p> Blueprint Technologies, LLC is an equal employment opportunity employer. Qualified applicants are considered without regard to race, color, age, disability, sex, gender identity or expression, orientation, veteran/military status, religion, national origin, ancestry, marital, or familial status, genetic information, citizenship, or any other status protected by law.</p> 
 <p> If you need assistance or a reasonable accommodation to complete the application process, please reach out to: recruiting@bpcs.com</p> 
 <p> Blueprint believe in the importance of a healthy and happy team, which is why our comprehensive benefits package includes:</p> 
 <ul> 
  <li>Medical, dental, and vision coverage</li> 
  <li>Flexible Spending Account</li> 
  <li>401k program</li> 
  <li>Competitive PTO offerings</li> 
  <li>Parental Leave</li> 
  <li>Opportunities for professional growth and development</li>
 </ul>
</div>
<p></p>","https://www.indeed.com/rc/clk?jk=75fadf29bd2cd624&atk=&xpse=SoB167I3Jzd5XOw8FZ0LbzkdCdPP","75fadf29bd2cd624",,,,"Remote","Sr. Data Engineer","12 days ago","2023-10-13T11:53:33.168Z","3.2","57","$146,400 - $175,100 a year","2023-10-25T11:53:33.169Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=75fadf29bd2cd624&from=jasx&tk=1hdjasd9nk26u800&vjs=3"
"E Source","Are you an experienced data engineer with a passion for data and innovation? Do you thrive on a cross-functional team of professionals?
 
  E Source is a leading provider of software solutions and consulting services for utilities and their customers. We use data science and machine learning to help utilities optimize their operations and achieve their sustainability goals. We process large amounts of data from our clients and uncover insights and patterns that they didn’t see before.
 
  Data engineering is a key function enabling a robust data science environment for our consulting services and as middleware in preparing our client’s data for processing within our software as a service (SaaS) solutions.
 
  We are seeking a Senior Data engineer to join our Machine Learning Engineering team and help us design and build scalable, reliable, and secure data pipelines, infrastructure, and systems for our consulting services and software as a service solutions. The Senior Data Engineer will work with a cross-functional team of Machine Learning engineers, software engineers, data scientists, and data analysts to deliver data products and solutions for our clients while also contributing to our internal data practices.
 
  Responsibilities:
  
 
 
   Design, develop and maintain data pipelines, infrastructure, and systems to support data products and solutions using technologies such as AWS, Python, Databricks, Spark, and SQL databases like PostgreSQL.
   Work with cross-functional teams to translate business problems into technical solutions and provide technical guidance and mentorship to junior data engineers.
   Develop and implement data engineering strategies and best practices that align with business objectives and customer needs.
   Monitor and troubleshoot data pipelines and systems to ensure data quality, integrity, and availability.
   Conduct research on industry trends and best practices to improve data engineering capabilities and evaluate new data technologies and tools.
   Build and maintain data lakes to support business intelligence needs.
   Manage the software development lifecycle and DevOps aspects of the code.
   Collaborate with data scientists and analysts to understand their data requirements and provide them with optimal data solutions.
   Create new data validation methods and data analysis tools.
 
 
  Requirements:
  
 
 
   Bachelor's degree in computer science, information technology, or a related field.
   4-7 years of experience in data engineering or a similar role.
   Expert-level skills in Python, SQL databases such as PostgreSQL, and big data technologies such as Databricks and Spark.
   Hands-on experience building cloud resident data pipelines in AWS.
   Strong understanding of data governance, security, privacy, and retention policies and procedures.
   Strong communication, collaboration, and problem-solving skills.
   High proficiency using agile software tools like Jira and following mature DevOps practices using GIT, Docker, and CI servers like Jenkins.
   Passion for data and innovation.
 
 
  Preferred Qualifications:
 
   Demonstrated capacity to work autonomously and proactively, with a proven track record of achieving results without constant supervision.
   Experience with ETL optimization, designing, coding, and tuning big data processes in Databricks.
   Sound knowledge of data lineage and data quality techniques.
   Experience in working with data science and machine learning models and frameworks
   Previous experience in the Energy or Utility industry in an analytic role.
   MS Degree in management information systems, computer programming, software engineering, data science, or an equivalent STEM field.
 
 
  A little about E Source
 
  Since 1986, E Source has focused on partnering with utilities to help their customers save electricity. That novel approach defined the art of electric end-use efficiency—better known as energy efficiency. We’ve expanded that concept to include a broader perspective of sustainability for utilities that deliver electricity, natural gas, and water.
 
  We work to enhance relationships with the people utilities serve, achieve the next generation of savings, and lead the carbon-reduction effort. We help utilities think differently, make data useful, and learn from the best strategies across the industry. Our people, our insights, and our network help our clients and give them the assurance that the programs they implement are the most effective.
 
  Benefits
  
 
 
   We offer excellent insurance packages, including medical, dental, and vision plans; company-paid life insurance; company-paid long- and short-term disability insurance; and medical and dependent-care flexible spending plans.
   We provide a flexible time off (FTO) program; E Source employees can take as many paid days off per year as they need, with manager approval, while fulfilling their work obligations and ensuring proper coverage of their responsibilities.
   We offer flexible schedules, flexible work locations, and a paid parental leave benefit.
   We provide a 401(k) plan with a 3% employer match.
 
 
  The budgeted salary for this position is $99,750 to $135,450 (includes base + annual bonus). Actual pay will be adjusted based on experience.
 
  This person can work remote, a hybrid schedule, or from one of our physical office locations.
 
  Applicants must be authorized to work for any employer in the US. We’re unable to sponsor or take over sponsorship of employment visas at this time.
 
  All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.
 
  #LI-Remote
 
  #LI-AP1","<div>
 <p>Are you an experienced data engineer with a passion for data and innovation? Do you thrive on a cross-functional team of professionals?</p>
 <p></p>
 <p><br> E Source is a leading provider of software solutions and consulting services for utilities and their customers. We use data science and machine learning to help utilities optimize their operations and achieve their sustainability goals. We process large amounts of data from our clients and uncover insights and patterns that they didn&#x2019;t see before.</p>
 <p></p>
 <p><br> Data engineering is a key function enabling a robust data science environment for our consulting services and as middleware in preparing our client&#x2019;s data for processing within our software as a service (SaaS) solutions.</p>
 <p></p>
 <p><br> We are seeking a Senior Data engineer to join our Machine Learning Engineering team and help us design and build scalable, reliable, and secure data pipelines, infrastructure, and systems for our consulting services and software as a service solutions. The Senior Data Engineer will work with a cross-functional team of Machine Learning engineers, software engineers, data scientists, and data analysts to deliver data products and solutions for our clients while also contributing to our internal data practices.</p>
 <p></p>
 <p><b><br> Responsibilities:</b></p>
 <br> 
 <p></p>
 <ul>
  <li> Design, develop and maintain data pipelines, infrastructure, and systems to support data products and solutions using technologies such as AWS, Python, Databricks, Spark, and SQL databases like PostgreSQL.</li>
  <li> Work with cross-functional teams to translate business problems into technical solutions and provide technical guidance and mentorship to junior data engineers.</li>
  <li> Develop and implement data engineering strategies and best practices that align with business objectives and customer needs.</li>
  <li> Monitor and troubleshoot data pipelines and systems to ensure data quality, integrity, and availability.</li>
  <li> Conduct research on industry trends and best practices to improve data engineering capabilities and evaluate new data technologies and tools.</li>
  <li> Build and maintain data lakes to support business intelligence needs.</li>
  <li> Manage the software development lifecycle and DevOps aspects of the code.</li>
  <li> Collaborate with data scientists and analysts to understand their data requirements and provide them with optimal data solutions.</li>
  <li> Create new data validation methods and data analysis tools.</li>
 </ul>
 <p></p>
 <p><b><br> Requirements:</b></p>
 <br> 
 <p></p>
 <ul>
  <li> Bachelor&apos;s degree in computer science, information technology, or a related field.</li>
  <li> 4-7 years of experience in data engineering or a similar role.</li>
  <li> Expert-level skills in Python, SQL databases such as PostgreSQL, and big data technologies such as Databricks and Spark.</li>
  <li> Hands-on experience building cloud resident data pipelines in AWS.</li>
  <li> Strong understanding of data governance, security, privacy, and retention policies and procedures.</li>
  <li> Strong communication, collaboration, and problem-solving skills.</li>
  <li> High proficiency using agile software tools like Jira and following mature DevOps practices using GIT, Docker, and CI servers like Jenkins.</li>
  <li> Passion for data and innovation.</li>
 </ul>
 <p></p>
 <p><b><br> Preferred Qualifications:</b></p>
 <ul>
  <li> Demonstrated capacity to work autonomously and proactively, with a proven track record of achieving results without constant supervision.</li>
  <li> Experience with ETL optimization, designing, coding, and tuning big data processes in Databricks.</li>
  <li> Sound knowledge of data lineage and data quality techniques.</li>
  <li> Experience in working with data science and machine learning models and frameworks</li>
  <li> Previous experience in the Energy or Utility industry in an analytic role.</li>
  <li> MS Degree in management information systems, computer programming, software engineering, data science, or an equivalent STEM field.</li>
 </ul>
 <p></p>
 <p><b><br> A little about E Source</b></p>
 <p></p>
 <p><br> Since 1986, E Source has focused on partnering with utilities to help their customers save electricity. That novel approach defined the art of electric end-use efficiency&#x2014;better known as energy efficiency. We&#x2019;ve expanded that concept to include a broader perspective of sustainability for utilities that deliver electricity, natural gas, and water.</p>
 <p></p>
 <p><br> We work to enhance relationships with the people utilities serve, achieve the next generation of savings, and lead the carbon-reduction effort. We help utilities think differently, make data useful, and learn from the best strategies across the industry. Our people, our insights, and our network help our clients and give them the assurance that the programs they implement are the most effective.</p>
 <p></p>
 <p><b><br> Benefits</b></p>
 <br> 
 <p></p>
 <ul>
  <li> We offer excellent insurance packages, including medical, dental, and vision plans; company-paid life insurance; company-paid long- and short-term disability insurance; and medical and dependent-care flexible spending plans.</li>
  <li> We provide a flexible time off (FTO) program; E Source employees can take as many paid days off per year as they need, with manager approval, while fulfilling their work obligations and ensuring proper coverage of their responsibilities.</li>
  <li> We offer flexible schedules, flexible work locations, and a paid parental leave benefit.</li>
  <li> We provide a 401(k) plan with a 3% employer match.</li>
 </ul>
 <p></p>
 <p><br> The budgeted salary for this position is <b>&#x24;99,750 to &#x24;135,450 (includes base + annual bonus)</b>. Actual pay will be adjusted based on experience.</p>
 <p></p>
 <p><br> This person can work remote, a hybrid schedule, or from one of our physical office locations.</p>
 <p></p>
 <p><br> Applicants must be authorized to work for any employer in the US. We&#x2019;re unable to sponsor or take over sponsorship of employment visas at this time.</p>
 <p></p>
 <p><br> All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.</p>
 <p></p>
 <p><br> #LI-Remote</p>
 <p></p>
 <p><br> #LI-AP1</p>
</div>","https://www.paycomonline.net/v4/ats/web.php/jobs/ViewJobDetails?job=174420&clientkey=D091157AE56F3B54E83C2790C96E526E&source=Indeed&source=Indeed.com","cf276d4ea3c5fdcb",,"Full-time",,"Hybrid remote","Data Engineer III - REMOTE","12 days ago","2023-10-13T11:53:32.702Z","4","10","$99,750 - $135,450 a year","2023-10-25T11:53:32.704Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=cf276d4ea3c5fdcb&from=jasx&tk=1hdjasd9nk26u800&vjs=3"
"For People","For People is a team of skilled technologists improving government digital services for disadvantaged and vulnerable populations. We embed directly in government agencies to modernize software, systems, and platforms so that they better serve people.
  Your Impact We are a dedicated team focused on creating and managing an extensive Medicare data warehouse at the Centers for Medicare & Medicaid Services (CMS) to serve Medicare beneficiaries' demographic, enrollment, and claims data in a FHIR (Fast Healthcare Interoperability Resources) format. We are responsible for providing data to several Medicare APIs so that those systems can seamlessly exchange data between various healthcare providers, insurers, and patients. You will directly impact the quality of healthcare that over 65 million Medicare enrollees nationwide receive.
  Our Culture For People is a team of humans. We place a significant amount of emphasis on positive work-life balance, setting healthy expectations, and making sure our loved ones are taken care of first. That means picking a child up from school during the day or going for a mid-day walk is okay!
  This position is 100% remote. Our entire team is remote across the United States, from the West Coast to the East Coast. There will never be a return-to-office, as we have none!
  This position's published base salary range is between $125,000 and $160,000 annually, plus generous benefits (e.g., For People pays 100% of Gold-tier employee health insurance premiums) and annual company profit sharing.
  Your Opportunities
 
   Lead the implementation of FHIR (Fast Healthcare Interoperability Resources) standards within the data warehouse, ensuring accuracy and efficient data exchange across the ecosystem of partner APIs.
   Ingest healthcare data from source systems into FHIR resources and profiles through developing ETL (Extract, Transform, Load) processes, checking for data quality and integrity.
   Create and maintain data mapping specifications to transform non-FHIR data formats into FHIR-compliant data.
   Design and maintain the data warehouse's FHIR-based data model to meet the needs of downstream API systems.
   Implement security measures and access controls to protect sensitive healthcare data and comply with healthcare data privacy regulations, such as HIPAA.
   Maintain comprehensive documentation of FHIR implementations, data transformation processes, and data flows.
   Stay informed about industry best practices and evolving FHIR standards.
 
  You Bring
 
   A humble and caring attitude
   In-depth knowledge and experience with FHIR standards and resource types.
   Expert-level Java programming abilities, alongside some familiarity with Python and Bash scripts.
   Proficiency in designing and implementing data ingestion and transformation processes.
   Strong database design and data modeling skills, with experience creating and maintaining data models in a healthcare context.
   A systematic approach to identifying and resolving issues related to FHIR data integration, data quality, and performance.
   Demonstrated commitment to staying updated on industry best practices, evolving FHIR standards, and opportunities for process improvement.
 
  If you're passionate about healthcare technology and ready to positively impact the quality of healthcare for millions of Medicare enrollees nationwide, we encourage you to apply. Join us in revolutionizing healthcare data accessibility.
  Some fine print. You will be working on a United States government platform, and they have a few basic requirements for contractors like ourselves. You must perform all work physically within the United States at all times. In addition, you must be a United States citizen and be able to pass a government-performed public trust background check.
  For People is an Equal Employment Opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, disability, genetics, and/or veteran status. 
  
 1D2WJ7jBEl","<div>
 <p>For People is a team of skilled technologists improving government digital services for disadvantaged and vulnerable populations. We embed directly in government agencies to modernize software, systems, and platforms so that they better serve people.</p>
 <p><b> Your Impact</b><br> We are a dedicated team focused on creating and managing an extensive Medicare data warehouse at the Centers for Medicare &amp; Medicaid Services (CMS) to serve Medicare beneficiaries&apos; demographic, enrollment, and claims data in a FHIR (Fast Healthcare Interoperability Resources) format. We are responsible for providing data to several Medicare APIs so that those systems can seamlessly exchange data between various healthcare providers, insurers, and patients. You will <i>directly impact the quality of healthcare that over 65 million Medicare enrollees nationwide receive</i>.</p>
 <p><b> Our Culture</b><br> For People is a team of humans. We place a significant amount of emphasis on positive work-life balance, setting healthy expectations, and making sure our loved ones are taken care of first. That means picking a child up from school during the day or going for a mid-day walk is okay!</p>
 <p> This position is <b><i>100% remote</i></b>. Our entire team is remote across the United States, from the West Coast to the East Coast. There will never be a return-to-office, as we have none!</p>
 <p> This position&apos;s published base salary range is between <i>&#x24;125,000 and &#x24;160,000</i> annually, plus generous benefits (e.g., For People pays 100% of Gold-tier employee health insurance premiums) and annual company profit sharing.</p>
 <p><b> Your Opportunities</b></p>
 <ul>
  <li> Lead the implementation of FHIR (Fast Healthcare Interoperability Resources) standards within the data warehouse, ensuring accuracy and efficient data exchange across the ecosystem of partner APIs.</li>
  <li> Ingest healthcare data from source systems into FHIR resources and profiles through developing ETL (Extract, Transform, Load) processes, checking for data quality and integrity.</li>
  <li> Create and maintain data mapping specifications to transform non-FHIR data formats into FHIR-compliant data.</li>
  <li> Design and maintain the data warehouse&apos;s FHIR-based data model to meet the needs of downstream API systems.</li>
  <li> Implement security measures and access controls to protect sensitive healthcare data and comply with healthcare data privacy regulations, such as HIPAA.</li>
  <li> Maintain comprehensive documentation of FHIR implementations, data transformation processes, and data flows.</li>
  <li> Stay informed about industry best practices and evolving FHIR standards.</li>
 </ul>
 <p><b> You Bring</b></p>
 <ul>
  <li> A humble and caring attitude</li>
  <li> In-depth knowledge and experience with <b>FHIR</b> <b>standards</b> and resource types.</li>
  <li> Expert-level <b>Java</b> programming abilities, alongside some familiarity with Python and Bash scripts.</li>
  <li> Proficiency in designing and implementing data ingestion and transformation processes.</li>
  <li> Strong database design and data modeling skills, with experience creating and maintaining data models in a healthcare context.</li>
  <li> A systematic approach to identifying and resolving issues related to FHIR data integration, data quality, and performance.</li>
  <li> Demonstrated commitment to staying updated on industry best practices, evolving FHIR standards, and opportunities for process improvement.</li>
 </ul>
 <p> If you&apos;re passionate about healthcare technology and ready to positively impact the quality of healthcare for millions of Medicare enrollees nationwide, we encourage you to apply. Join us in revolutionizing healthcare data accessibility.</p>
 <p><b><i> Some fine print.</i></b> You will be working on a United States government platform, and they have a few basic requirements for contractors like ourselves. You must perform all work physically within the United States at all times. In addition, you must be a United States citizen and be able to pass a government-performed public trust background check.</p>
 <p> For People is an Equal Employment Opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, disability, genetics, and/or veteran status.<br> </p>
 <p> </p>
 <p>1D2WJ7jBEl</p>
</div>","https://forpeople.applytojob.com/apply/1D2WJ7jBEl/Senior-Backend-Engineer-FHIR-Data?source=INDE","b151e97f7474d1c8",,"Full-time",,"Remote","Senior Backend Engineer, FHIR Data","13 days ago","2023-10-12T11:53:31.554Z",,,"$125,000 - $160,000 a year","2023-10-25T11:53:31.557Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=b151e97f7474d1c8&from=jasx&tk=1hdjasd9nk26u800&vjs=3"
"UNCOMN","Here at 
  UN
  COMN, our mission is to empower systems thinkers to create elegant solutions to complex problems – to improve the systems that improve our communities. Our team members apply their natural curiosity and grit to discover elegant solutions for our clients’ most complex organizational, logistics, process, data, and technical challenges, with the overall goal of building great businesses that contribute to great communities.
 
  
  
 
   We’re an award-winning firm, one of the country’s fastest-growing and—more importantly—a consistent ‘
  Top Workplace’ as evaluated by our own employees. We are a values-driven organization (see the 
  Core Values section of our website) and we’re looking for new Uncommon Geniuses to join our growing team, so if you’re a person who likes to solve problems, fix things, build things, tweak things, or otherwise show creative flair, you might just be an ""
  UN
  COMN Genius"" and we encourage you to check out the specifics of this position below!
 
  
  
 
   UN
  COMN is seeking a 
  Data Engineer 3 to:
  
 
   Meet with client stakeholders to gather requirements for data engineering tasks
   Collaborate with cross-functional team to design loading and transformation processes within a big data environment
   Analyze raw source data to determine contents and meaning and apply to client requirements
   Design and build database objects for a large supply chain ecosystem
   Design and build processes for loading and transforming data
   Analyze and optimize processes for performance
   Identify, troubleshoot, and fix bugs in data environment
 
 
   5+ years of data engineering in AWS with a focus on ETL 
   
    5+ years PL/pgSQL
     5+ years Python
     3+ years AWS Glue
    
  5+ years data analysis
   3+ years data management in relational databases
   3+ years gathering requirements for data engineering projects
   Ability to build processes that support data transformation, workload management, data structures, dependency, and metadata
   Ability to build and optimize data sets, big data pipelines, and architectures
   Fluency with AWS CLI
   Ability to perform root cause analysis on external and internal processes and data to identify opportunities for improvement and answer questions
   Excellent analytic skills associated with working on unstructured datasets
   Proactive approach to problem-solving
   Natural curiosity and desire to learn customer data and business processes
   Must be eligible to obtain a Secret clearance granted by the US Government as needed
 
  
  
  Preferred 
 
   3+ years of data management using AWS Postgres RDS
   Experience working with supply chain data
   Experience working with EDI
 
 
   Flexible PTO effective on day 1*
   7 Paid Holidays & up to 3 Floating Holidays*
   Eligible for Health Benefits on day 1*
   401K Safe Harbor Match Program*
   Training and Education Assistance*
  
   Must be a full-time employee
  
 
  
  
 
   Don’t meet every single requirement? We’re dedicated to building an uncommon, inclusive, and authentic workplace, so if you’re excited about this role but your experience doesn’t align perfectly with every qualification in the job description, we encourage you to apply anyways. You may be just the right candidate for this or other roles.
 
  
  
  All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, or national origin.","<div>
 <div>
  Here at 
  <b>UN</b>
  <b>COMN</b>, our mission is to empower systems thinkers to create elegant solutions to complex problems &#x2013; to improve the systems that improve our communities. Our team members apply their natural curiosity and grit to discover elegant solutions for our clients&#x2019; most complex organizational, logistics, process, data, and technical challenges, with the overall goal of building great businesses that contribute to great communities.
 </div>
 <br> 
 <div></div> 
 <div>
   We&#x2019;re an award-winning firm, one of the country&#x2019;s fastest-growing and&#x2014;more importantly&#x2014;a consistent &#x2018;
  <b>Top Workplace</b>&#x2019; as evaluated by our own employees. We are a values-driven organization (see the 
  <b>Core Values</b> section of our website) and we&#x2019;re looking for new Uncommon Geniuses to join our growing team, so if you&#x2019;re a person who likes to solve problems, fix things, build things, tweak things, or otherwise show creative flair, you might just be an &quot;
  <b>UN</b>
  <b>COMN </b>Genius&quot; and we encourage you to check out the specifics of this position below!
 </div>
 <br> 
 <div></div> 
 <div>
  <b> UN</b>
  <b>COMN </b>is seeking a 
  <b>Data Engineer 3 </b>to:
 </div> 
 <ul>
  <li> Meet with client stakeholders to gather requirements for data engineering tasks</li>
  <li> Collaborate with cross-functional team to design loading and transformation processes within a big data environment</li>
  <li> Analyze raw source data to determine contents and meaning and apply to client requirements</li>
  <li> Design and build database objects for a large supply chain ecosystem</li>
  <li> Design and build processes for loading and transforming data</li>
  <li> Analyze and optimize processes for performance</li>
  <li> Identify, troubleshoot, and fix bugs in data environment</li>
 </ul>
 <ul>
  <li> 5+ years of data engineering in AWS with a focus on ETL 
   <ul>
    <li>5+ years PL/pgSQL</li>
    <li> 5+ years Python</li>
    <li> 3+ years AWS Glue</li>
   </ul> </li>
  <li>5+ years data analysis</li>
  <li> 3+ years data management in relational databases</li>
  <li> 3+ years gathering requirements for data engineering projects</li>
  <li> Ability to build processes that support data transformation, workload management, data structures, dependency, and metadata</li>
  <li> Ability to build and optimize data sets, big data pipelines, and architectures</li>
  <li> Fluency with AWS CLI</li>
  <li> Ability to perform root cause analysis on external and internal processes and data to identify opportunities for improvement and answer questions</li>
  <li> Excellent analytic skills associated with working on unstructured datasets</li>
  <li> Proactive approach to problem-solving</li>
  <li> Natural curiosity and desire to learn customer data and business processes</li>
  <li> Must be eligible to obtain a Secret clearance granted by the US Government as needed</li>
 </ul>
 <br> 
 <p></p> 
 <p> Preferred</p> 
 <ul>
  <li> 3+ years of data management using AWS Postgres RDS</li>
  <li> Experience working with supply chain data</li>
  <li> Experience working with EDI</li>
 </ul>
 <ul>
  <li> Flexible PTO effective on day 1*</li>
  <li> 7 Paid Holidays &amp; up to 3 Floating Holidays*</li>
  <li> Eligible for Health Benefits on day 1*</li>
  <li> 401K Safe Harbor Match Program*</li>
  <li> Training and Education Assistance*</li>
  <ul>
   <li><i>Must be a full-time employee</i></li>
  </ul>
 </ul>
 <br> 
 <div></div> 
 <div>
  <b> Don&#x2019;t meet every single requirement? We&#x2019;re dedicated to building an uncommon, inclusive, and authentic workplace, so if you&#x2019;re excited about this role but your experience doesn&#x2019;t align perfectly with every qualification in the job description, we encourage you to apply anyways. You may be just the right candidate for this or other roles.</b>
 </div>
 <br> 
 <p></p> 
 <p><i> All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, or national origin.</i></p>
</div>","https://www.indeed.com/applystart?jk=17557d019e22347b&from=vj&pos=top&mvj=0&spon=0&sjdu=YmZE5d5THV8u75cuc0H6Y26AwfY51UOGmh3Z9h4OvXgrjvbiKhAL49VaIgrWcKPWXAwdulcUk0atwlDdDDqlBQ&vjfrom=serp&astse=1899e069110c11b7&assa=4505","17557d019e22347b",,"Full-time",,"St. Louis, MO 63101","Data Engineer 3 (Remote)","8 days ago","2023-10-17T11:52:44.794Z","4.5","2",,"2023-10-25T11:52:44.796Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=17557d019e22347b&from=jasx&tk=1hdjaruujjrqo801&vjs=3"
"InvoiceCloud","About InvoiceCloud: 
   InvoiceCloud, an EngageSmart solution, is a leading provider of online bill payment services. Founded in 2009, the company has grown to be one of the leading disruptors in the cloud-based electronic bill presentment and payment (EBPP) space, helping institutions put customer experience first. By switching to InvoiceCloud, clients can improve customer engagement, loyalty, and efficiency while reducing churn and missed payments in the process. With over 50 million payments processed annually, InvoiceCloud is one of the most secure, innovative, and inclusive fintech solutions in the market. To learn more, visit www.InvoiceCloud.com.
 
  As a Data Conversion Engineer on our integrations team, you will work to convert data for InvoiceCloud clients as they transition or upgrade their core processing system, ensuring their customers' online bill paying experience remains consistent and uninterrupted. You will be responsible for analyzing the source data, designing the target data format, extracting existing data, and performing the transform & load. You will be encouraged to develop automated scripts and tools to facilitate repeatable conversions where possible. 
  As a Data Conversion Engineer You Will: 
  
  Understand how to write and troubleshoot complex SQL queries 
  Understand ETL (Extract, Transform, and Load) processes and tools 
  Create wikis for data conversion processes and tools 
  Able to handle a fast-paced environment 
  Able to multitask efficiently 
  
 What We Seek: 
  
  Bachelor's Degree preferred or equivalent combination of education and experience required 
  5 years of Experience using Microsoft Technologies including VB.NET, ASP.NET, C#, Visual Studio 
  Proficiency with SQL Server and Microsoft ETL tools 
  Machine learning and automation experience a plus 
  Self-led, capable of working with little direction 
  Skilled communicator with a collaborative spirit 
  
 
 Base Compensation Range: ($70,000.00 to $90,000.00) annually 
  Base salary is one component of total compensation. Employees may also be eligible for an annual bonus or commission and equity. Some roles may also be eligible for overtime pay. 
  The above represents the expected base compensation range for this job requisition. Ultimately, in determining your pay, we'll consider many factors including, but not limited to, skills, experience, qualifications, geographic location, and other job-related factors. 
 
 
   Benefits 
   We offer a competitive benefits program including: 
   
   Medical, dental, vision, life & disability insurance 
   401(k) plan with company match & employee stock purchase plan (ESPP) 
   Flexible Time Off (FTO), wellbeing days, paid holidays, and summer Fridays 
   Mental health resources 
   Paid parental leave & Backup Care 
   Tuition reimbursement 
   Employee Resource Groups (ERGs) 
   
  Invoice Cloud is an Equal Opportunity Employer. 
   Invoice Cloud provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. 
   This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training. 
   If you have a disability under the Americans with Disabilities Act or similar law, or you require a religious accommodation, and you wish to discuss potential accommodations related to applying for employment at our company, please contact jobs@engagesmart.com. 
   Click here to review EngageSmart's Job Applicant Privacy Policy. 
   To all recruitment agencies: Invoice Cloud does not accept agency resumes. Please do not forward resumes to our job's alias, employees, or any other organization location. Invoice Cloud is not responsible for any fees related to unsolicited resumes.","<div>
 <div>
  <p><b>About InvoiceCloud</b>:</p> 
  <p> InvoiceCloud, an EngageSmart solution, is a leading provider of online bill payment services. Founded in 2009, the company has grown to be one of the leading disruptors in the cloud-based electronic bill presentment and payment (EBPP) space, helping institutions put customer experience first. By switching to InvoiceCloud, clients can improve customer engagement, loyalty, and efficiency while reducing churn and missed payments in the process. With over 50 million payments processed annually, InvoiceCloud is one of the most secure, innovative, and inclusive fintech solutions in the market. To learn more, visit www.InvoiceCloud.com.</p>
 </div>
 <p> As a Data Conversion Engineer on our integrations team, you will work to convert data for InvoiceCloud clients as they transition or upgrade their core processing system, ensuring their customers&apos; online bill paying experience remains consistent and uninterrupted. You will be responsible for analyzing the source data, designing the target data format, extracting existing data, and performing the transform &amp; load. You will be encouraged to develop automated scripts and tools to facilitate repeatable conversions where possible.</p> 
 <p><b> As a Data Conversion Engineer You Will:</b></p> 
 <ul> 
  <li>Understand how to write and troubleshoot complex SQL queries</li> 
  <li>Understand ETL (Extract, Transform, and Load) processes and tools</li> 
  <li>Create wikis for data conversion processes and tools</li> 
  <li>Able to handle a fast-paced environment</li> 
  <li>Able to multitask efficiently</li> 
 </ul> 
 <p><b>What We Seek:</b></p> 
 <ul> 
  <li>Bachelor&apos;s Degree preferred or equivalent combination of education and experience required</li> 
  <li>5 years of Experience using Microsoft Technologies including VB.NET, ASP.NET, C#, Visual Studio</li> 
  <li>Proficiency with SQL Server and Microsoft ETL tools</li> 
  <li>Machine learning and automation experience a plus</li> 
  <li>Self-led, capable of working with little direction</li> 
  <li>Skilled communicator with a collaborative spirit</li> 
 </ul> 
 <p></p>
 <p>Base Compensation Range: (&#x24;70,000.00 to &#x24;90,000.00) annually</p> 
 <p> Base salary is one component of total compensation. Employees may also be eligible for an annual bonus or commission and equity. Some roles may also be eligible for overtime pay.</p> 
 <p> The above represents the expected base compensation range for this job requisition. Ultimately, in determining your pay, we&apos;ll consider many factors including, but not limited to, skills, experience, qualifications, geographic location, and other job-related factors.</p> 
 <p></p>
 <div>
  <p><b> Benefits</b></p> 
  <p> We offer a competitive benefits program including:</p> 
  <ul> 
   <li>Medical, dental, vision, life &amp; disability insurance</li> 
   <li>401(k) plan with company match &amp; employee stock purchase plan (ESPP)</li> 
   <li>Flexible Time Off (FTO), wellbeing days, paid holidays, and summer Fridays</li> 
   <li>Mental health resources</li> 
   <li>Paid parental leave &amp; Backup Care</li> 
   <li>Tuition reimbursement</li> 
   <li>Employee Resource Groups (ERGs)</li> 
  </ul> 
  <p><b>Invoice Cloud is an Equal Opportunity Employer.</b></p> 
  <p> Invoice Cloud provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.</p> 
  <p> This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training.</p> 
  <p> If you have a disability under the Americans with Disabilities Act or similar law, or you require a religious accommodation, and you wish to discuss potential accommodations related to applying for employment at our company, please contact jobs@engagesmart.com.</p> 
  <p> Click here to review EngageSmart&apos;s Job Applicant Privacy Policy.</p> 
  <p><b> To all recruitment agencies:</b> Invoice Cloud does not accept agency resumes. Please do not forward resumes to our job&apos;s alias, employees, or any other organization location. Invoice Cloud is not responsible for any fees related to unsolicited resumes.</p>
 </div>
</div>","https://engagesmart.com/careers-invoicecloud/?gh_jid=5778848003&gh_src=791f54f23us","0a1b5ba4666e2bed",,,,"Remote","Data Conversion Engineer","7 days ago","2023-10-18T11:53:33.953Z","3.5","10","$70,000 - $90,000 a year","2023-10-25T11:53:33.955Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=0a1b5ba4666e2bed&from=jasx&tk=1hdjasd9nk26u800&vjs=3"
"Duke University","If you are passionate about addressing challenging issues in information security through the use of data engineering practices and tools, we would like to talk to you about an engineering role in the Duke University IT Security Office.
  
 
 POSITION SUMMARY: 
 The IT Security Office (ITSO) is responsible for the overall coordination, implementation, and assessment of information security at Duke University. The overall goal of ITSO is to achieve the optimal level of confidentiality, integrity and availability of Duke's information assets and systems while providing a safe and secure computing environment for research, teaching, learning, and the everyday conduct of university business.
  
  
 The incumbent will be embedded in the IT Security Office and work with the team to analyze log data and assist with managing the log analytics infrastructure. The position will be responsible for working with Duke University Office of Information Technology (OIT) and Duke Health Technology Services (DHTS) teams to support this shared environment and to work with both teams to ensure the performance and maintenance of the systems. In addition, the successful candidate will work with the support teams to establish or support performance reporting via dashboards within the enterprise tools.
  
 
 Duties include: 
 
  Manage log analysis platforms, including but not limited to performance tuning and application maintenance. 
  Work hands-on with data to collect, summarize, and visualize operational metrics based on detailed log data in support of IT activities. 
  Build, maintain, and improve dashboards to describe past, present, and future trends. 
  Work with OIT and DHTS teams to establish, maintain or improve processes for adding resources and groups to the shared log analysis environment. 
  Identify opportunities to improve institutional systems, process, and data. 
  Support log ingestion from a variety of sources into the enterprise log analysis infrastructure. 
  Design and implement local and cloud batch and streaming data pipelines. 
  Gather data requirements from stakeholder analysts and data scientists and deliver prepared data for analytics and modeling. 
  Integrate data derived from a variety of data sources including logs, external APIs, and internal databases.
 
  
  
 Skills/Qualifications
  
  
 
  Deep experience with log aggregation and analytics platforms such as Splunk, Apache Spark or ELK required. 
  Demonstrated technical expertise with application performance tuning, log ingestion, and general administration with the aforementioned applications. 
  Demonstrated experience with Kafka or other pub/sub technologies. 
  Knowledge of or experience with creation of dashboards or other data visualizations within analytics platforms. 
  Experience in maintaining log analytics platforms on virtual or cloud infrastructures. 
  Demonstrated experience with analyzing and building complex datasets via Python. 
  Experience working within one or more cloud providers (Azure, AWS, GCP) 
  Ability to understand new technology concepts quickly and apply them. accurately through an evolving, dynamic environment. 
  Ability to take ownership of activities and work independently. 
  Excellent verbal and written skills. 
  Experience with Data Modeling (Normalization, star schema) a plus. 
  Experience with CI/CD flows a plus.
 
  
  
 
 EDUCATION: 
 Required: B.S. in Computer Science, Engineering, or related field, or commensurate experience.
  
  
 QUALIFICATIONS: 
 3 years of experience in security engineering, log analytics, systems administration, etc.
  
 
 Certifications inclusive of log analysis products or approaches, etc. are optional, but will be considered favorably.
  
  
 This position may have an opportunity to work remotely. All Duke University and Duke Health remote workers must reside in one of the following states or districts: Arizona; California; Florida; Georgia; Hawaii; Illinois; Maryland; Massachusetts; Montana; New Jersey; New York; North Carolina; Pennsylvania; South Carolina; Tennessee; Texas; Virginia or Washington, DC.
  
  
 Duke is an Affirmative Action/Equal Opportunity Employer committed to providing employment opportunity without regard to an individual's age, color, disability, gender, gender expression, gender identity, genetic information, national origin, race, religion, sex, sexual orientation, or veteran status.
  
 Duke aspires to create a community built on collaboration, innovation, creativity, and belonging. Our collective success depends on the robust exchange of ideas—an exchange that is best when the rich diversity of our perspectives, backgrounds, and experiences flourishes. To achieve this exchange, it is essential that all members of the community feel secure and welcome, that the contributions of all individuals are respected, and that all voices are heard. All members of our community have a responsibility to uphold these values.
  
 Essential Physical Job Functions: Certain jobs at Duke University and Duke University Health System may include essentialjob functions that require specific physical and/or mental abilities. Additional information and provision for requests for reasonable accommodation will be provided by each hiring department.","<div>
 <p>If you are passionate about addressing challenging issues in information security through the use of data engineering practices and tools, we would like to talk to you about an engineering role in the Duke University IT Security Office.</p>
 <br> 
 <p></p>
 <p><b>POSITION SUMMARY</b>:</p> 
 <p>The IT Security Office (ITSO) is responsible for the overall coordination, implementation, and assessment of information security at Duke University. The overall goal of ITSO is to achieve the optimal level of confidentiality, integrity and availability of Duke&apos;s information assets and systems while providing a safe and secure computing environment for research, teaching, learning, and the everyday conduct of university business.</p>
 <br> 
 <p></p> 
 <p>The incumbent will be embedded in the IT Security Office and work with the team to analyze log data and assist with managing the log analytics infrastructure. The position will be responsible for working with Duke University Office of Information Technology (OIT) and Duke Health Technology Services (DHTS) teams to support this shared environment and to work with both teams to ensure the performance and maintenance of the systems. In addition, the successful candidate will work with the support teams to establish or support performance reporting via dashboards within the enterprise tools.</p>
 <br> 
 <p></p>
 <p><b>Duties include:</b></p> 
 <ul>
  <li>Manage log analysis platforms, including but not limited to performance tuning and application maintenance.</li> 
  <li>Work hands-on with data to collect, summarize, and visualize operational metrics based on detailed log data in support of IT activities.</li> 
  <li>Build, maintain, and improve dashboards to describe past, present, and future trends.</li> 
  <li>Work with OIT and DHTS teams to establish, maintain or improve processes for adding resources and groups to the shared log analysis environment.</li> 
  <li>Identify opportunities to improve institutional systems, process, and data.</li> 
  <li>Support log ingestion from a variety of sources into the enterprise log analysis infrastructure.</li> 
  <li>Design and implement local and cloud batch and streaming data pipelines.</li> 
  <li>Gather data requirements from stakeholder analysts and data scientists and deliver prepared data for analytics and modeling.</li> 
  <li>Integrate data derived from a variety of data sources including logs, external APIs, and internal databases.</li>
 </ul>
 <br> 
 <p> </p>
 <p><b>Skills/Qualifications</b></p>
 <br> 
 <p></p> 
 <ul>
  <li>Deep experience with log aggregation and analytics platforms such as Splunk, Apache Spark or ELK required.</li> 
  <li>Demonstrated technical expertise with application performance tuning, log ingestion, and general administration with the aforementioned applications.</li> 
  <li>Demonstrated experience with Kafka or other pub/sub technologies. </li>
  <li>Knowledge of or experience with creation of dashboards or other data visualizations within analytics platforms.</li> 
  <li>Experience in maintaining log analytics platforms on virtual or cloud infrastructures.</li> 
  <li>Demonstrated experience with analyzing and building complex datasets via Python.</li> 
  <li>Experience working within one or more cloud providers (Azure, AWS, GCP)</li> 
  <li>Ability to understand new technology concepts quickly and apply them. accurately through an evolving, dynamic environment.</li> 
  <li>Ability to take ownership of activities and work independently.</li> 
  <li>Excellent verbal and written skills.</li> 
  <li>Experience with Data Modeling (Normalization, star schema) a plus.</li> 
  <li>Experience with CI/CD flows a plus.</li>
 </ul>
 <br> 
 <p></p> 
 <p></p>
 <p><b>EDUCATION:</b></p> 
 <p>Required: B.S. in Computer Science, Engineering, or related field, or commensurate experience.</p>
 <br> 
 <p></p> 
 <p><b>QUALIFICATIONS:</b></p> 
 <p>3 years of experience in security engineering, log analytics, systems administration, etc.</p>
 <br> 
 <p></p>
 <p>Certifications inclusive of log analysis products or approaches, etc. are optional, but will be considered favorably.</p>
 <br> 
 <p></p> 
 <p><b><i>This position may have an opportunity to work remotely. All Duke University and Duke Health remote workers must reside in one of the following states or districts: Arizona; California; Florida; Georgia; Hawaii; Illinois; Maryland; Massachusetts; Montana; New Jersey; New York; North Carolina; Pennsylvania; South Carolina; Tennessee; Texas; Virginia or Washington, DC.</i></b></p>
 <br> 
 <p></p> 
 <p>Duke is an Affirmative Action/Equal Opportunity Employer committed to providing employment opportunity without regard to an individual&apos;s age, color, disability, gender, gender expression, gender identity, genetic information, national origin, race, religion, sex, sexual orientation, or veteran status.</p>
 <p><br> </p>
 <p>Duke aspires to create a community built on collaboration, innovation, creativity, and belonging. Our collective success depends on the robust exchange of ideas&#x2014;an exchange that is best when the rich diversity of our perspectives, backgrounds, and experiences flourishes. To achieve this exchange, it is essential that all members of the community feel secure and welcome, that the contributions of all individuals are respected, and that all voices are heard. All members of our community have a responsibility to uphold these values.</p>
 <p><br> </p>
 <p>Essential Physical Job Functions: Certain jobs at Duke University and Duke University Health System may include essentialjob functions that require specific physical and/or mental abilities. Additional information and provision for requests for reasonable accommodation will be provided by each hiring department.</p>
</div>
<p></p>","https://www.indeed.com/rc/clk?jk=df3eadb9e1b40cd6&atk=&xpse=SoAw67I3Jzd31pwBq50LbzkdCdPP","df3eadb9e1b40cd6",,,,"905 W Main St Ste 19, Durham, NC 27701","Security Data Engineer (Analyst, IT)","30+ days ago","2023-09-25T11:53:45.986Z","4","1479",,"2023-10-25T11:53:45.988Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=df3eadb9e1b40cd6&from=jasx&tk=1hdjaoshm284l000&vjs=3"
"ESRI, Inc.","Overview: 
 
   In this position, you will bring locations to life with ArcGIS Velocity, Workflow Manager, and Geotrigger. You will work to identify and define customer requirements and build and release high quality software. In addition, you'll have the opportunity to run automation tests, monitor tests, and perform scripting using Python.
 
 
   The Professional Services division is the consulting and implementation arm of Esri. We break ground in new markets, push the technology envelope, and ultimately deliver transformational solutions to high-profile clients worldwide. The Professional Services organization is comprised of nearly 1,000 talented business and technical professionals who strive every day to help our users be successful.
  Responsibilities: 
 
  Work with software developers to design, build, test, and release high quality software
   Communicate with product users to identify product requirements and advocate for their needs throughout the software development lifecycle
   Research throughout industry standards and specifications to translate requirements to software design
   Develop support systems to ensure quality software through robust automated functional and performance testing
   Provide best practices, user documentation, demonstrations, and technical assistance for the product
   Collaborate in focused team efforts the design lifecycle
   Leverage the knowledge of the target audience to better understand business trends, customer communities, and go-to-market strategies
  Requirements: 
 
  5+ years of professional experience in a similar position supporting similar responsibilities
   Experience with bringing teams together to perform a task (for example: project coordination or project management)
   Enthusiasm for improving software capabilities for end users in real-world application
   Bachelor`s in Geographic Information Systems (GIS), Geography, Computer Science, or STEM related field
  Recommended Qualifications: 
 
  Experience using ArcGIS GeoEvent Server and/or ArcGIS Velocity, ArcGIS Enterprise, ArcGIS Online, and ArcGIS Pro
   Experience with complex enterprise systems
   Master`s in Geographic Information Systems (GIS), Geography, Computer Science or STEM related field
  The Company: 
 
   Our passion for improving quality of life through geography is at the heart of everything we do. Esri’s geographic information system (GIS) technology inspires and enables governments, universities, and businesses worldwide to save money, lives, and our environment through a deeper understanding of the changing world around them.
   
   Esri is an equal opportunity employer (EOE) and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law.
   
   If you need a reasonable accommodation for any part of the employment process, please email askcareers@esri.com and let us know the nature of your request and your contact information. Please note that only those inquiries concerning a request for reasonable accommodation will be responded to from this e-mail address.
   
   Esri’s competitive total rewards strategy includes industry-leading health and welfare benefits: medical, dental, vision, basic and supplemental life insurance for employees (and their families), 401(k) and profit-sharing programs, minimum accrual of 80 hours of vacation leave, twelve paid holidays throughout the calendar year, and opportunities for personal and professional growth. Base salary is one component of our total rewards strategy. Compensation decisions and the base range for this role take into account many factors including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.
 
 
 
   A reasonable estimate of the base salary range is $89,440.00 - $147,680.00.
 
 
 
   #LI-MN1
 
 
   #LI-REMOTE","<div>
 Overview: 
 <div>
   In this position, you will bring locations to life with ArcGIS Velocity, Workflow Manager, and Geotrigger. You will work to identify and define customer requirements and build and release high quality software. In addition, you&apos;ll have the opportunity to run automation tests, monitor tests, and perform scripting using Python.
 </div>
 <div>
   The Professional Services division is the consulting and implementation arm of Esri. We break ground in new markets, push the technology envelope, and ultimately deliver transformational solutions to high-profile clients worldwide. The Professional Services organization is comprised of nearly 1,000 talented business and technical professionals who strive every day to help our users be successful.
 </div> Responsibilities: 
 <ul>
  <li>Work with software developers to design, build, test, and release high quality software</li>
  <li> Communicate with product users to identify product requirements and advocate for their needs throughout the software development lifecycle</li>
  <li> Research throughout industry standards and specifications to translate requirements to software design</li>
  <li> Develop support systems to ensure quality software through robust automated functional and performance testing</li>
  <li> Provide best practices, user documentation, demonstrations, and technical assistance for the product</li>
  <li> Collaborate in focused team efforts the design lifecycle</li>
  <li> Leverage the knowledge of the target audience to better understand business trends, customer communities, and go-to-market strategies</li>
 </ul> Requirements: 
 <ul>
  <li>5+ years of professional experience in a similar position supporting similar responsibilities</li>
  <li> Experience with bringing teams together to perform a task (for example: project coordination or project management)</li>
  <li> Enthusiasm for improving software capabilities for end users in real-world application</li>
  <li> Bachelor`s in Geographic Information Systems (GIS), Geography, Computer Science, or STEM related field</li>
 </ul> Recommended Qualifications: 
 <ul>
  <li>Experience using ArcGIS GeoEvent Server and/or ArcGIS Velocity, ArcGIS Enterprise, ArcGIS Online, and ArcGIS Pro</li>
  <li> Experience with complex enterprise systems</li>
  <li> Master`s in Geographic Information Systems (GIS), Geography, Computer Science or STEM related field</li>
 </ul> The Company: 
 <div>
   Our passion for improving quality of life through geography is at the heart of everything we do. Esri&#x2019;s geographic information system (GIS) technology inspires and enables governments, universities, and businesses worldwide to save money, lives, and our environment through a deeper understanding of the changing world around them.
  <br> 
  <br> Esri is an equal opportunity employer (EOE) and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status, or any other characteristic protected by law.
  <br> 
  <br> If you need a reasonable accommodation for any part of the employment process, please email askcareers@esri.com and let us know the nature of your request and your contact information. Please note that only those inquiries concerning a request for reasonable accommodation will be responded to from this e-mail address.
  <br> 
  <br> Esri&#x2019;s competitive total rewards strategy includes industry-leading health and welfare benefits: medical, dental, vision, basic and supplemental life insurance for employees (and their families), 401(k) and profit-sharing programs, minimum accrual of 80 hours of vacation leave, twelve paid holidays throughout the calendar year, and opportunities for personal and professional growth. Base salary is one component of our total rewards strategy. Compensation decisions and the base range for this role take into account many factors including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs.
 </div>
 <div></div>
 <div>
  <br> A reasonable estimate of the base salary range is &#x24;89,440.00 - &#x24;147,680.00.
 </div>
 <div></div>
 <div>
  <br> #LI-MN1
 </div>
 <div>
   #LI-REMOTE
 </div>
</div>","https://external-esri.icims.com/jobs/18648/job?utm_source=indeed_integration&iis=Job%20Board&iisn=Indeed&indeed-apply-token=73a2d2b2a8d6d5c0a62696875eaebd669103652d3f0c2cd5445d3e66b1592b0f","ad5d4ac0639f9242",,"Full-time",,"Remote","Sr. Product Engineer - Real-Time & Big Data","30+ days ago","2023-09-25T11:53:44.657Z","3.8","184","$89,440 - $147,680 a year","2023-10-25T11:53:44.660Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=ad5d4ac0639f9242&from=jasx&tk=1hdjaoshm284l000&vjs=3"
"BTI Solutions","Why Work for Us?
  
  Established in 2006, continues to grow dramatically within the IT, telecommunications, Automotive and SCM industry. We encourage our employees in personal development with a passion to succeed and we offer an excellent benefit package. Every employee has access to Medical, Vision, Dental, Life and 401K plus many more.
  
  401K with Employer Match
  Company Paid Dental, Vision, Life and Medical up to 100%
  Paid Sick Leave
  Chance for VISA sponsoring
  
 
  SUMMARY OF ESSENTIAL JOB FUNCTIONS 
   
    Design and develop analytical models and be the face to the data consumers 
    Perform data curation to meet the business requirements 
    Build batch and streaming data pipelines 
    Develop processes for automating, testing, and deploying your work 
    Identify risks and opportunities of potential logic and data issues within the data environment 
    Collaborate effectively with the global team and ensure day to day deliverables are met 
   
  MINIMUN REUIREMENTS 
   
    Bachelor’s degree and 5+ years of industry experience in software development, data engineering, business intelligence, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets 
    3+ years of experience as Data Engineer or in a similar role 
    Proven experiences with AWS and/or GCP, Hadoop, Vertica, Talend, Tableau, and other modern technology platforms is required 
    Cloud to Cloud migration experience preferred 
    Strong PySpark skill is a must have 
    Have knowledge of data management fundamentals and data storage principles 
    Have knowledge of systems as it pertains to data storage and computing 
    Strong source to target mapping experience and ETL principles/knowledge 
    Excellent verbal and written communication skills. 
    Strong quantitative and analytical skills with accuracy and attention to detail 
    Ability to work well independently with minimal supervision and can manage multiple priorities 
    
 
  BTI Solutions, Inc. is an equal opportunity employer m/f/d/v.","<div>
 Why Work for Us?
 <br> 
 <br> Established in 2006, continues to grow dramatically within the IT, telecommunications, Automotive and SCM industry. We encourage our employees in personal development with a passion to succeed and we offer an excellent benefit package. Every employee has access to Medical, Vision, Dental, Life and 401K plus many more.
 <br> 
 <br> 401K with Employer Match
 <br> Company Paid Dental, Vision, Life and Medical up to 100%
 <br> Paid Sick Leave
 <br> Chance for VISA sponsoring
 <br> 
 <ul>
  <li>SUMMARY OF ESSENTIAL JOB FUNCTIONS 
   <ul>
    <li>Design and develop analytical models and be the face to the data consumers</li> 
    <li>Perform data curation to meet the business requirements</li> 
    <li>Build batch and streaming data pipelines</li> 
    <li>Develop processes for automating, testing, and deploying your work</li> 
    <li>Identify risks and opportunities of potential logic and data issues within the data environment</li> 
    <li>Collaborate effectively with the global team and ensure day to day deliverables are met</li> 
   </ul></li>
  <li>MINIMUN REUIREMENTS 
   <ul>
    <li>Bachelor&#x2019;s degree and 5+ years of industry experience in software development, data engineering, business intelligence, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets</li> 
    <li>3+ years of experience as Data Engineer or in a similar role</li> 
    <li>Proven experiences with AWS and/or GCP, Hadoop, Vertica, Talend, Tableau, and other modern technology platforms is required</li> 
    <li>Cloud to Cloud migration experience preferred</li> 
    <li>Strong PySpark skill is a must have</li> 
    <li>Have knowledge of data management fundamentals and data storage principles</li> 
    <li>Have knowledge of systems as it pertains to data storage and computing</li> 
    <li>Strong source to target mapping experience and ETL principles/knowledge</li> 
    <li>Excellent verbal and written communication skills.</li> 
    <li>Strong quantitative and analytical skills with accuracy and attention to detail</li> 
    <li>Ability to work well independently with minimal supervision and can manage multiple priorities</li> 
   </ul> </li>
 </ul>
 <br> BTI Solutions, Inc. is an equal opportunity employer m/f/d/v.
</div>","https://btisolutions.catsone.com/careers/6245-General/jobs/16268177-Data-Engineer--Remote--Logistics--SC702786/?ref=Indeed","129ad46624074ddf",,,,"Remote","Data Engineer / Remote / Logistics / SC702786","30+ days ago","2023-09-25T11:53:50.685Z","4","66",,"2023-10-25T11:53:50.686Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=129ad46624074ddf&from=jasx&tk=1hdjaoshm284l000&vjs=3"
"Agilon Health","Agilon health is transforming healthcare by empowering community-based physicians with the resources and expertise they need to innovate the payment and delivery of care for seniors.
The agilon health Total Care Model is powered by our purpose-built platform and frees physicians from the constraints of the traditional fee-for-service reimbursement model, all enabled through a growing national network of like-minded physician partners. With agilon health, physicians are able to practice team-based, coordinated care to serve the individual needs of their senior patients and to transition to a sustainable and predictable, long-term business model.
As you might imagine, analytics and insights are the heart of how we support our physician partners and is our special sauce. We have a lot of analytics programs in place already, but we need more! From generating insights related to our risk adjustment programs to helping analyze health plan attribution, health plan and membership trends, building our next generation Cube or analytics surrounding our brand-new data lake, there's a lot of data to process and actionable insights to present, as well as analytics infrastructure to build. You will be executing some of this work individually, and others in tight partnership with other critical teams such as Finance, Clinical Analytics, and Medical Economics.
There’s much more data we can be leveraging and analyzing to generate and test our hypotheses to help improve patient outcomes and reduce medical waste. Come join the team and help make a direct impact on our senior members’ lives!
More about this role:
- Be part of an agile team working collaboratively with Agilon leadership and many different cross-functional teams, including UX, Product, Technology, Operations, and Clinician teams- Leverage your analytical thinking to explore our ever-growing datasets to test hypotheses and bring life to your own insights- Generate insights that help improve patient outcomes and/or reduce medical waste- Continuously learn and share your new-found knowledge with others
Desired Traits:
1. Experience: A minimum of 4 years of experience performing data analysis and generating intuitive dashboards.
2. Healthcare domain expertise:
a. Exposure to the healthcare industry & domain is preferred, with specific knowledge in either risk adjustment, clinical analytics, clinical quality, or health economics.b. Familiarity with healthcare data models.
3. Education: A bachelor's degree or equivalent education is required.
4. Technical skills:
a. should have strong proficiency in SQL, and expertise in Snowflake would be a plus.b. Data modeling skills are also important.c. Familiarity with data visualization tools such as Sigma, Tableau, Power BI, or Excel is necessary to create intuitive dashboards.d. Some experience with coding in Python or other programming languages is nice to have.
5. Analytical and problem-solving skills: Strong analytical and problem-solving abilities are essential for effectively analyzing data and providing meaningful insights.6. Business Acumen: should be able to translate business needs into data analysis requirements, understanding the goals and objectives of the organization.7. Curiosity and exploration: Should have an intellectual curiosity about data and the ability to go beyond immediate problems. Balancing deadlines and exploring new opportunities for analysis and insights is important.8. Ownership and user focus: Takes ownership over the insights the team generates, and the processes used to develop those analyses, keeping our users' needs in clear focus9. Startup environment readiness: Comfortable in a startup environment and ready to “roll up your sleeves!”10. Communication skills: Excellent communication skills are necessary to effectively convey complex findings to both technical and non-technical stakeholders.11. Nimble learner: Should be eager to learn and adapt quickly to new technologies, methodologies, and industry trends.12. Enthusiasm and drive: A proactive and enthusiastic approach to getting things done is valued in a candidate.
Job Type: Full-time
Pay: $100,000.00 - $115,000.00 per year
Benefits:

 401(k)
 401(k) matching
 Dental insurance
 Employee assistance program
 Flexible spending account
 Health insurance
 Health savings account
 Life insurance
 Paid time off
 Parental leave
 Professional development assistance
 Vision insurance

Compensation package:

 Yearly pay

Experience level:

 4 years

Schedule:

 Monday to Friday

Work Location: Remote","<p>Agilon health is transforming healthcare by empowering community-based physicians with the resources and expertise they need to innovate the payment and delivery of care for seniors.</p>
<p>The agilon health Total Care Model is powered by our purpose-built platform and frees physicians from the constraints of the traditional fee-for-service reimbursement model, all enabled through a growing national network of like-minded physician partners. With agilon health, physicians are able to practice team-based, coordinated care to serve the individual needs of their senior patients and to transition to a sustainable and predictable, long-term business model.</p>
<p>As you might imagine, analytics and insights are the heart of how we support our physician partners and is our special sauce. We have a lot of analytics programs in place already, but we need more! From generating insights related to our risk adjustment programs to helping analyze health plan attribution, health plan and membership trends, building our next generation Cube or analytics surrounding our brand-new data lake, there&apos;s a lot of data to process and actionable insights to present, as well as analytics infrastructure to build. You will be executing some of this work individually, and others in tight partnership with other critical teams such as Finance, Clinical Analytics, and Medical Economics.</p>
<p>There&#x2019;s much more data we can be leveraging and analyzing to generate and test our hypotheses to help improve patient outcomes and reduce medical waste. Come join the team and help make a direct impact on our senior members&#x2019; lives!</p>
<p><b>More about this role:</b></p>
<p>- Be part of an agile team working collaboratively with Agilon leadership and many different cross-functional teams, including UX, Product, Technology, Operations, and Clinician teams<br>- Leverage your analytical thinking to explore our ever-growing datasets to test hypotheses and bring life to your own insights<br>- Generate insights that help improve patient outcomes and/or reduce medical waste<br>- Continuously learn and share your new-found knowledge with others</p>
<p><b>Desired Traits:</b></p>
<p>1. Experience: A minimum of 4 years of experience performing data analysis and generating intuitive dashboards.</p>
<p>2. Healthcare domain expertise:</p>
<p>a. Exposure to the healthcare industry &amp; domain is preferred, with specific knowledge in either risk adjustment, clinical analytics, clinical quality, or health economics.<br>b. Familiarity with healthcare data models.</p>
<p>3. Education: A bachelor&apos;s degree or equivalent education is required.</p>
<p>4. Technical skills:</p>
<p>a. should have strong proficiency in SQL, and expertise in Snowflake would be a plus.<br>b. Data modeling skills are also important.<br>c. Familiarity with data visualization tools such as Sigma, Tableau, Power BI, or Excel is necessary to create intuitive dashboards.<br>d. Some experience with coding in Python or other programming languages is nice to have.</p>
<p>5. Analytical and problem-solving skills: Strong analytical and problem-solving abilities are essential for effectively analyzing data and providing meaningful insights.<br>6. Business Acumen: should be able to translate business needs into data analysis requirements, understanding the goals and objectives of the organization.<br>7. Curiosity and exploration: Should have an intellectual curiosity about data and the ability to go beyond immediate problems. Balancing deadlines and exploring new opportunities for analysis and insights is important.<br>8. Ownership and user focus: Takes ownership over the insights the team generates, and the processes used to develop those analyses, keeping our users&apos; needs in clear focus<br>9. Startup environment readiness: Comfortable in a startup environment and ready to &#x201c;roll up your sleeves!&#x201d;<br>10. Communication skills: Excellent communication skills are necessary to effectively convey complex findings to both technical and non-technical stakeholders.<br>11. Nimble learner: Should be eager to learn and adapt quickly to new technologies, methodologies, and industry trends.<br>12. Enthusiasm and drive: A proactive and enthusiastic approach to getting things done is valued in a candidate.</p>
<p>Job Type: Full-time</p>
<p>Pay: &#x24;100,000.00 - &#x24;115,000.00 per year</p>
<p>Benefits:</p>
<ul>
 <li>401(k)</li>
 <li>401(k) matching</li>
 <li>Dental insurance</li>
 <li>Employee assistance program</li>
 <li>Flexible spending account</li>
 <li>Health insurance</li>
 <li>Health savings account</li>
 <li>Life insurance</li>
 <li>Paid time off</li>
 <li>Parental leave</li>
 <li>Professional development assistance</li>
 <li>Vision insurance</li>
</ul>
<p>Compensation package:</p>
<ul>
 <li>Yearly pay</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>4 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>Monday to Friday</li>
</ul>
<p>Work Location: Remote</p>",,"f32a0eb4fc36bab8",,"Full-time",,"Remote","Senior Engineer BOI Insights (Healthcare Data Analytics)","8 days ago","2023-10-17T11:54:01.446Z","2.8","62","$100,000 - $115,000 a year","2023-10-25T11:54:01.448Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=f32a0eb4fc36bab8&from=jasx&tk=1hdjau6m4jgap800&vjs=3"
"Sleeper","Position Summary
  The Senior/First Data Engineer will play a crucial role in building, maintaining, and enhancing ETL processes that drive our analytics and machine learning platforms. This individual will be responsible for developing actionable insights from complex data sets, and work closely with various business units to inform strategy and decision-making.
  You will be the first hire in this function.
 
  Location
 
   SF Bay Area, NYC, or Remote
 
 
  Key Responsibilities
 
   ETL & Backend Development:
 
 
   Design and optimize ETL pipelines.
   Develop robust backend systems for large-scale data processing using Elixir and database solutions like Cassandra/ScyllaDB.
 
 
   Data Architecture:
 
 
   Design scalable and efficient data models for Cassandra and ScyllaDB.
   Ensure data integrity, quality, and security.
 
 
   Data Science Support:
 
 
   Collaborate with data scientists, providing them with clean and reliable datasets.
   Assist in implementing and scaling data science models.
 
 
   Innovation & Research:
 
 
   Stay abreast of latest technologies.
   Recommend technical improvements for data processing and storage.
 
 
  Qualifications
  Required
 
   Bachelor’s or Master’s degree in Computer Science, Engineering, or a related technical field.
   5+ years of experience in backend development, with a strong focus on data engineering.
   
     Technical skills: Expertise in Python, Java, Scala, and Elixer for backend and ETL processes.
     Mastery of ETL tools/frameworks (e.g. Apache Kafka, Apache Airflow).
     Deep knowledge of SQL/NoSQL databases, including Cassandra and ScyllaDB, and data warehousing solutions (e.g., Redshift, BigQueary, Snowflake).
     Proficiency in cloud platforms (AWS, GCP, Azure) and distributed systems.
     Familiarity with data science concepts, tools, and libraries (e.g. Pandas, Scikit-learn).
     Soft Skills: Exceptional problem-solving skills.
     Strong communication for technical and non-technical discussions.
   
 
  Nice-to-have
 
   Experience with cloud platforms like AWS, GCP, or Azure.
   Exceptional communication skills, both verbal and written.
   Expertise in machine learning algorithms and frameworks (e.g., TensorFlow, PyTorch, scikit-learn).
 
 
  Benefits
 
   Competitive salary ($150,000-$225,000/year) and stock options
   Comprehensive health, dental, and vision plans
   401(k)
   Flexible working hours and remote work options
   Regular team building events and activities","<div>
 <h2 class=""jobSectionHeader""><b>Position Summary</b></h2>
 <p> The Senior/First Data Engineer will play a crucial role in building, maintaining, and enhancing ETL processes that drive our analytics and machine learning platforms. This individual will be responsible for developing actionable insights from complex data sets, and work closely with various business units to inform strategy and decision-making.</p>
 <p> You will be the first hire in this function.</p>
 <p></p>
 <h2 class=""jobSectionHeader""><b> Location</b></h2>
 <ul>
  <li><p> SF Bay Area, NYC, or Remote</p></li>
 </ul>
 <p></p>
 <h2 class=""jobSectionHeader""><b> Key Responsibilities</b></h2>
 <ul>
  <li><p> ETL &amp; Backend Development:</p></li>
 </ul>
 <ul>
  <li><p> Design and optimize ETL pipelines.</p></li>
  <li><p> Develop robust backend systems for large-scale data processing using Elixir and database solutions like Cassandra/ScyllaDB.</p></li>
 </ul>
 <ul>
  <li><p> Data Architecture:</p></li>
 </ul>
 <ul>
  <li><p> Design scalable and efficient data models for Cassandra and ScyllaDB.</p></li>
  <li><p> Ensure data integrity, quality, and security.</p></li>
 </ul>
 <ul>
  <li><p> Data Science Support:</p></li>
 </ul>
 <ul>
  <li><p> Collaborate with data scientists, providing them with clean and reliable datasets.</p></li>
  <li><p> Assist in implementing and scaling data science models.</p></li>
 </ul>
 <ul>
  <li><p> Innovation &amp; Research:</p></li>
 </ul>
 <ul>
  <li><p> Stay abreast of latest technologies.</p></li>
  <li><p> Recommend technical improvements for data processing and storage.</p></li>
 </ul>
 <p></p>
 <h2 class=""jobSectionHeader""><b> Qualifications</b></h2>
 <h3 class=""jobSectionHeader""><b> Required</b></h3>
 <ul>
  <li><p> Bachelor&#x2019;s or Master&#x2019;s degree in Computer Science, Engineering, or a related technical field.</p></li>
  <li><p> 5+ years of experience in backend development, with a strong focus on data engineering.</p>
   <ul>
    <li><p> Technical skills: Expertise in Python, Java, Scala, and Elixer for backend and ETL processes.</p></li>
    <li><p> Mastery of ETL tools/frameworks (e.g. Apache Kafka, Apache Airflow).</p></li>
    <li><p> Deep knowledge of SQL/NoSQL databases, including Cassandra and ScyllaDB, and data warehousing solutions (e.g., Redshift, BigQueary, Snowflake).</p></li>
    <li><p> Proficiency in cloud platforms (AWS, GCP, Azure) and distributed systems.</p></li>
    <li><p> Familiarity with data science concepts, tools, and libraries (e.g. Pandas, Scikit-learn).</p></li>
    <li><p> Soft Skills: Exceptional problem-solving skills.</p></li>
    <li><p> Strong communication for technical and non-technical discussions.</p></li>
   </ul></li>
 </ul>
 <h3 class=""jobSectionHeader""><b> Nice-to-have</b></h3>
 <ul>
  <li><p> Experience with cloud platforms like AWS, GCP, or Azure.</p></li>
  <li><p> Exceptional communication skills, both verbal and written.</p></li>
  <li><p> Expertise in machine learning algorithms and frameworks (e.g., TensorFlow, PyTorch, scikit-learn).</p></li>
 </ul>
 <p></p>
 <h2 class=""jobSectionHeader""><b> Benefits</b></h2>
 <ul>
  <li><p> Competitive salary (&#x24;150,000-&#x24;225,000/year) and stock options</p></li>
  <li><p> Comprehensive health, dental, and vision plans</p></li>
  <li><p> 401(k)</p></li>
  <li><p> Flexible working hours and remote work options</p></li>
  <li><p> Regular team building events and activities</p></li>
 </ul>
</div>
<p></p>","https://www.indeed.com/rc/clk?jk=8baee8cb684bbc8a&atk=&xpse=SoA467I3Jzd2dcRJrR0LbzkdCdPP","8baee8cb684bbc8a",,"Full-time",,"Washington, DC","Senior/First Data Engineer","7 days ago","2023-10-18T11:53:59.580Z",,,"$150,000 - $225,000 a year","2023-10-25T11:53:59.582Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=8baee8cb684bbc8a&from=jasx&tk=1hdjau6m4jgap800&vjs=3"
"ICF","Senior Data Engineer at ICF
**This role can be completely remote, sitting anywhere within the US**
We seek a talented Data Engineer who is eager to apply computer science, software engineering, databases, and distributed/parallel processing frameworks to prepare big data for the use of data analysts and data scientists. If you have experience with Scala and Spark and want your work to contribute to systems that collect healthcare data used by hundreds of thousands of daily users, we want to (virtually) meet you!
You will work on projects that support the Centers for Medicare and Medicaid Services (CMS) as we develop a next-generation analytics and reporting system that directly impacts healthcare quality. You will use Spark to build data processing pipelines that derive information from large sets of government data. You will be the go-to on your team for Spark, the Spark Engine, and the Spark Dataframe API. This program allows for the continued quality of clinicians’ work according to CMS standards. We are a collaborative company, so we want you to use your knowledge of Spark to teach others, inform design decisions, and debug runtime problems.
Our mission is to help the government improve healthcare for patients and reduce costs. We value bringing individuals that are experts in their disciplines, highly communicative, and self-motivated to own their work. Technology and domain experts work side-by-side in highly dynamic teams with all the roles necessary to deliver high-quality digital services. In addition, critical to our success is forming teams of highly diverse individuals passionate about making a difference.
Tools & Technology:
o Spark, Hadoop, Scala, Python, and AWS EMR
o Airflow, Jenkins
o AWS Redshift and Teradata
o Git and Github
o Confluence
Key Responsibilities:
o Write complex unit and integration tests for all data processing code
o Work with DevOps engineers on CI, CD, and IaC
o Read specs and translate them into test designs and test automation
o Perform code reviews and develop processes for improving code quality
Basic Qualifications:
o Bachelor’s degree
o 5+ years of high-volume experience with Scala, Spark, the Spark Engine, and the Spark Dataset API
o 2+ years of experience with Agile methodology
o 2+ years of experience performing data pipeline and data validation
o Must live in the United States, have lived in the US for 3 of the last 5 years, and be able to obtain a Public Trust Clearance.
Preferred Qualifications:
o MS and 3+ years of Data Engineering experience
o Experience working in the healthcare industry with PHI/PII
o Federal Government contracting work experience
o Expertise working as part of a dynamic, interactive Agile team
o Strong written and verbal communication skills
o Demonstrated time management skills.
o Strong organizational skills with attention to detail
o Curiosity about how things work, ability to look out for potential risks
Job Type: Full-time
Pay: $100,000.00 - $160,000.00 per year
Benefits:

 401(k)
 401(k) matching
 Dental insurance
 Flexible schedule
 Health insurance
 Health savings account
 Life insurance
 Paid time off
 Professional development assistance
 Tuition reimbursement
 Vision insurance

Compensation package:

 Yearly pay

Experience level:

 10 years
 8 years
 9 years

Schedule:

 8 hour shift

Education:

 Bachelor's (Required)

Experience:

 Informatica: 1 year (Preferred)
 Spark: 2 years (Required)
 SQL: 3 years (Preferred)
 Cloud: 1 year (Required)
 Scala: 3 years (Required)

Work Location: Remote","<p><b>Senior Data Engineer at ICF</b></p>
<p>**This role can be completely remote, sitting anywhere within the US**</p>
<p>We seek a talented Data Engineer who is eager to apply computer science, software engineering, databases, and distributed/parallel processing frameworks to prepare big data for the use of data analysts and data scientists. If you have experience with Scala and Spark and want your work to contribute to systems that collect healthcare data used by hundreds of thousands of daily users, we want to (virtually) meet you!</p>
<p>You will work on projects that support the Centers for Medicare and Medicaid Services (CMS) as we develop a next-generation analytics and reporting system that directly impacts healthcare quality. You will use Spark to build data processing pipelines that derive information from large sets of government data. You will be the go-to on your team for Spark, the Spark Engine, and the Spark Dataframe API. This program allows for the continued quality of clinicians&#x2019; work according to CMS standards. We are a collaborative company, so we want you to use your knowledge of Spark to teach others, inform design decisions, and debug runtime problems.</p>
<p>Our mission is to help the government improve healthcare for patients and reduce costs. We value bringing individuals that are experts in their disciplines, highly communicative, and self-motivated to own their work. Technology and domain experts work side-by-side in highly dynamic teams with all the roles necessary to deliver high-quality digital services. In addition, critical to our success is forming teams of highly diverse individuals passionate about making a difference.</p>
<p><b>Tools &amp; Technology:</b></p>
<p>o Spark, Hadoop, Scala, Python, and AWS EMR</p>
<p>o Airflow, Jenkins</p>
<p>o AWS Redshift and Teradata</p>
<p>o Git and Github</p>
<p>o Confluence</p>
<p><b>Key Responsibilities:</b></p>
<p>o Write complex unit and integration tests for all data processing code</p>
<p>o Work with DevOps engineers on CI, CD, and IaC</p>
<p>o Read specs and translate them into test designs and test automation</p>
<p>o Perform code reviews and develop processes for improving code quality</p>
<p><b>Basic Qualifications:</b></p>
<p>o Bachelor&#x2019;s degree</p>
<p>o <b>5+ years of high-volume experience with Scala, Spark, the Spark Engine, and the Spark Dataset API</b></p>
<p>o 2+ years of experience with Agile methodology</p>
<p>o 2+ years of experience performing data pipeline and data validation</p>
<p>o Must live in the United States, have lived in the US for 3 of the last 5 years, and be able to obtain a Public Trust Clearance.</p>
<p><b>Preferred Qualifications:</b></p>
<p>o MS and 3+ years of Data Engineering experience</p>
<p>o Experience working in the healthcare industry with PHI/PII</p>
<p>o Federal Government contracting work experience</p>
<p>o Expertise working as part of a dynamic, interactive Agile team</p>
<p>o Strong written and verbal communication skills</p>
<p>o Demonstrated time management skills.</p>
<p>o Strong organizational skills with attention to detail</p>
<p>o Curiosity about how things work, ability to look out for potential risks</p>
<p>Job Type: Full-time</p>
<p>Pay: &#x24;100,000.00 - &#x24;160,000.00 per year</p>
<p>Benefits:</p>
<ul>
 <li>401(k)</li>
 <li>401(k) matching</li>
 <li>Dental insurance</li>
 <li>Flexible schedule</li>
 <li>Health insurance</li>
 <li>Health savings account</li>
 <li>Life insurance</li>
 <li>Paid time off</li>
 <li>Professional development assistance</li>
 <li>Tuition reimbursement</li>
 <li>Vision insurance</li>
</ul>
<p>Compensation package:</p>
<ul>
 <li>Yearly pay</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>10 years</li>
 <li>8 years</li>
 <li>9 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
</ul>
<p>Education:</p>
<ul>
 <li>Bachelor&apos;s (Required)</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Informatica: 1 year (Preferred)</li>
 <li>Spark: 2 years (Required)</li>
 <li>SQL: 3 years (Preferred)</li>
 <li>Cloud: 1 year (Required)</li>
 <li>Scala: 3 years (Required)</li>
</ul>
<p>Work Location: Remote</p>",,"032e33bd776de85e",,"Full-time",,"Remote","Senior Data Engineer - Scala/Spark","11 days ago","2023-10-14T11:54:05.248Z","3.4","666","$100,000 - $160,000 a year","2023-10-25T11:54:05.250Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=032e33bd776de85e&from=jasx&tk=1hdjau6m4jgap800&vjs=3"
"SmithRx","Who We Are: 
  SmithRx is a rapidly growing, venture-backed Health-Tech company. Our mission is to disrupt the expensive and inefficient Pharmacy Benefit Management (PBM) sector by building a next-generation drug acquisition platform driven by cutting edge technology, innovative cost saving tools, and best-in-class customer service. With hundreds of thousands of members onboarded since 2016, SmithRx has a solution that is resonating with clients all across the country. 
  We pride ourselves for our mission-driven and collaborative culture that inspires our employees to do their best work. We believe that the U.S healthcare system is in need of transformation, and we come to work each day dedicated to making that change a reality. At our core, we are guided by our company values: 
  
  Integrity: Do the right thing. Especially when it's hard. 
  Courage: Embrace the challenge. 
  Together: Build bridges and lift up your colleagues. 
  
 Job Summary: 
  SmithRx is developing the next-generation pharmacy benefits management (PBM) platform, revolutionizing how businesses administer and oversee pharmacy benefits. Our cutting-edge technology platform provides real-time actionable insights that drive cost efficiencies, enhance clinical services, and elevate the customer experience. Operating within SmithRx's production and engineering division, the data engineering team is dedicated to establishing a scalable and dependable single source of truth data ecosystem. This ecosystem plays a critical role in enabling our organization to provide unparalleled service excellence and operational superiority to our valued customers. 
  We are currently seeking a highly motivated Senior Data Quality Engineer to join our fast-paced data team. The primary focus of this role will be delivering high data availability and quality throughout the entire data life cycle from ingestion to end products: data pipelines, and data warehouse production datasets. This role will gather data quality requirements from stakeholders, these data consumers could include business users, product owners, data analysts, software developers, or even other data developers. 
  This role is critical because data quality is a continuous process as data environments are complex, interdependent, and constantly changing. He/She will advocate and bring best practices/methodologies, coding standards, and large-scale data warehouse design perspectives to our team. 
  What you will do: 
  
  Gather requirements, develop and execute manual and automated test cases for data warehouse solutions to ensure the reliability of data pipelines, ETL processes, and data transformations. 
  Conduct in-depth data profiling and analysis of data sources & sets to identify inconsistencies, inaccuracies, and anomalies. 
  Develop, maintain, and enhance a comprehensive data quality framework that defines data standards, quality rules, and validation processes. 
  Set up data monitoring systems to continuously assess data warehouse quality and report on deviations to stakeholders. 
  Contribute to data governance efforts by participating in data stewardship, data quality & PII/PHI compliance 
  Collaborate closely with data engineers, data scientists, engineers, and data analysts to address data quality concerns and drive improvements. 
  Drive data engineering excellence and lead craftsmanship in building, testing, and optimizing ETL/feature/metric pipelines 
  Maintain thorough documentation of data quality standards, policies, and procedures. 
  
 What you will bring to SmithRx: 
  
  BS or Master's degree preferred in Computer Science, Information Technology or Systems Engineering. Bachelor's degree required. 
  A minimum of 6+ years of related experience in data engineering, software engineering, DevOps, and technical leadership. With a Bachelor's degree, 8+ years of experience are required. 
  Start-up experience is highly desirable 
  Prior experience in cloud services (AWS preferred), and columnar databases (Redshift, Snowflake). 
  Proficiency in SQL, Python, Spark (Batch/Streaming), SparkSQL, PySpark, AWS Glue, Airflow, Terraform, Kubernetes, CI/CD, and automated testing capabilities 
  Build end-to-end integration testing between multiple independent systems and interfaces (flat files, APIs, ETL), etc. 
  
 What SmithRx Offers You: 
  
  Highly competitive wellness benefits including Medical, Pharmacy, Dental, Vision, and Life Insurance 
  Flexible Spending Benefits 
  401(k) Retirement Savings Program 
  Short-term and long-term disability 
  Discretionary Time Off 
  13 Paid Holidays 
  Wellness benefits- Spring Health, Gympass, and Headspace 
  Commuter Benefits 
  Paid Parental Leave benefits 
  Employee Assistance Program (EAP) 
  Well stocked kitchen in office locations 
  Professional development and training opportunities","<div>
 <p><b>Who We Are:</b></p> 
 <p> SmithRx is a rapidly growing, venture-backed Health-Tech company. Our mission is to disrupt the expensive and inefficient Pharmacy Benefit Management (PBM) sector by building a next-generation drug acquisition platform driven by cutting edge technology, innovative cost saving tools, and best-in-class customer service. With hundreds of thousands of members onboarded since 2016, SmithRx has a solution that is resonating with clients all across the country.</p> 
 <p> We pride ourselves for our mission-driven and collaborative culture that inspires our employees to do their best work. We believe that the U.S healthcare system is in need of transformation, and we come to work each day dedicated to making that change a reality. At our core, we are guided by our company values:</p> 
 <ul> 
  <li><b>Integrity:</b> Do the right thing. Especially when it&apos;s hard.</li> 
  <li><b>Courage:</b> Embrace the challenge.</li> 
  <li><b>Together: </b>Build bridges and lift up your colleagues.</li> 
 </ul> 
 <p><b>Job Summary:</b></p> 
 <p> SmithRx is developing the next-generation pharmacy benefits management (PBM) platform, revolutionizing how businesses administer and oversee pharmacy benefits. Our cutting-edge technology platform provides real-time actionable insights that drive cost efficiencies, enhance clinical services, and elevate the customer experience. Operating within SmithRx&apos;s production and engineering division, the data engineering team is dedicated to establishing a scalable and dependable single source of truth data ecosystem. This ecosystem plays a critical role in enabling our organization to provide unparalleled service excellence and operational superiority to our valued customers.</p> 
 <p> We are currently seeking a highly motivated Senior Data Quality Engineer to join our fast-paced data team. The primary focus of this role will be delivering high data availability and quality throughout the entire data life cycle from ingestion to end products: data pipelines, and data warehouse production datasets. This role will gather data quality requirements from stakeholders, these data consumers could include business users, product owners, data analysts, software developers, or even other data developers.</p> 
 <p> This role is critical because data quality is a continuous process as data environments are complex, interdependent, and constantly changing. He/She will advocate and bring best practices/methodologies, coding standards, and large-scale data warehouse design perspectives to our team.</p> 
 <p><b> What you will do:</b></p> 
 <ul> 
  <li>Gather requirements, develop and execute manual and automated test cases for data warehouse solutions to ensure the reliability of data pipelines, ETL processes, and data transformations.</li> 
  <li>Conduct in-depth data profiling and analysis of data sources &amp; sets to identify inconsistencies, inaccuracies, and anomalies.</li> 
  <li>Develop, maintain, and enhance a comprehensive data quality framework that defines data standards, quality rules, and validation processes.</li> 
  <li>Set up data monitoring systems to continuously assess data warehouse quality and report on deviations to stakeholders.</li> 
  <li>Contribute to data governance efforts by participating in data stewardship, data quality &amp; PII/PHI compliance</li> 
  <li>Collaborate closely with data engineers, data scientists, engineers, and data analysts to address data quality concerns and drive improvements.</li> 
  <li>Drive data engineering excellence and lead craftsmanship in building, testing, and optimizing ETL/feature/metric pipelines</li> 
  <li>Maintain thorough documentation of data quality standards, policies, and procedures.</li> 
 </ul> 
 <p><b>What you will bring to SmithRx:</b></p> 
 <ul> 
  <li>BS or Master&apos;s degree preferred in Computer Science, Information Technology or Systems Engineering. Bachelor&apos;s degree required.</li> 
  <li>A minimum of 6+ years of related experience in data engineering, software engineering, DevOps, and technical leadership. With a Bachelor&apos;s degree, 8+ years of experience are required.</li> 
  <li>Start-up experience is highly desirable</li> 
  <li>Prior experience in cloud services (AWS preferred), and columnar databases (Redshift, Snowflake).</li> 
  <li>Proficiency in SQL, Python, Spark (Batch/Streaming), SparkSQL, PySpark, AWS Glue, Airflow, Terraform, Kubernetes, CI/CD, and automated testing capabilities</li> 
  <li>Build end-to-end integration testing between multiple independent systems and interfaces (flat files, APIs, ETL), etc.</li> 
 </ul> 
 <p><b>What SmithRx Offers You:</b></p> 
 <ul> 
  <li>Highly competitive wellness benefits including Medical, Pharmacy, Dental, Vision, and Life Insurance</li> 
  <li>Flexible Spending Benefits</li> 
  <li>401(k) Retirement Savings Program</li> 
  <li>Short-term and long-term disability</li> 
  <li>Discretionary Time Off</li> 
  <li>13 Paid Holidays</li> 
  <li>Wellness benefits- Spring Health, Gympass, and Headspace</li> 
  <li>Commuter Benefits</li> 
  <li>Paid Parental Leave benefits</li> 
  <li>Employee Assistance Program (EAP)</li> 
  <li>Well stocked kitchen in office locations</li> 
  <li>Professional development and training opportunities</li>
 </ul>
</div>","https://boards.greenhouse.io/smithrx/jobs/6976811002?gh_src=027693ea2us&lever-source=Indeed","a1ec6d7ad1511944",,,,"Remote","Senior Data Quality Engineer","7 days ago","2023-10-18T11:53:59.312Z","3","7",,"2023-10-25T11:53:59.313Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=a1ec6d7ad1511944&from=jasx&tk=1hdjau6m4jgap800&vjs=3"
"FutureFit AI","FutureFit AI is looking for a contract Senior Data Engineer to join our team. We have a culture of high trust, high impact, high velocity and a gritty determination to win. If you are passionate about development, like to have fun, do it right, and get things done we would love to hear from you!
  An important note: Data shows that men on average apply for a role if they meet 3/10 requirements while women often only do so if it’s 10/10. We work hard to be clear and specific about what our roles include and demand and encourage you to apply if you see a strong (but not necessarily perfect) fit between you and the opportunity.
 
  About You:
 
   Technology - You are a passionate data engineer, eager to learn, solve problems and share your knowledge with others
   Data - Strong architecture and design around various data paradigms (document stores, data lakes/warehouses and relational) and databases (Mongo, Elastic, Postgres, Redshift, etc.)
   Data Query - Experience writing highly performant data queries for large/complex data sets across relational and NoSQL databases
   Data Pipelines - Experience in architecture and implementation of ETL processes, streaming data, data process orchestration (Airflow)
   BI - Adept at connecting BI platforms (Looker) to data warehouses to provide critical insights via dashboards
   MLOps - Experience with model versioning, deployment, monitoring and serving models via REST APIs
   Understanding - You are eager to understand the business problem and collaborate with product to build a solution that meets both technology and business requirements
   Communication - You can share your perspective clearly and constructively, you are comfortable challenging ideas and collaborating to find the best approach
   Grit - You like to solve challenging problems, are not afraid of the unknown, eager to learn and take a systematic approach to solving problems
   AI - You are interested in integrating advances in AI into FutureFit AI products and the development process to drive improvements in velocity, quality and engineer experience
   Values - You are a good person who shares our values of excellence, transparency, and humanity
 
 
  The Product’s Technology Stack:
 
   Front End - React, TypeScript, Tailwind, Storybook
   Back End - Node.js, Nest.js, Python, Flask, GraphQL, Postgres, MongoDB, RedShift, Airflow
   AWS - CloudFront, Cognito, ECS, RDS, S3, SQS, Elasticsearch, VPC, Lambda
 
 
  Location:
  Most days we work from home but everyone comes to the office at least once a week for face to face collaboration and team building. The office is located at 325 Front St West (a short walk from Union Station).
 
  Our Company:
  In looking at a job posting, it’s often hard to get a basic picture of the company profile (size, stage, structure, etc.) which is why we are sharing it with you upfront. This helps you quickly decide and helps us focus any time we spend together on going beyond the basics:
  Funding: We have raised one round of funding led by JP Morgan which fueled a significant growth trajectory for us and we have a safe financial runway to execute against.
  Problem Domain: Future of Work / Workforce Development - important that the problem domain interests you even if you haven’t worked in the space before.
  Customers: We partner with workforce development agencies, government agencies, and employers/enterprises.
  Structure: We are organized around the following key departments: Growth, Customer Success, Product, Engineering, Data, People & Culture, Finance & Operations.
  Team: We are a team of 30-50 across US and Canada with main hubs in New York and Toronto. This role will be based in Toronto.  Core Principles: Be Curious, Drive to Outcomes, Raise the Bar, Speed Matters, Own It, Put We Over Me
 
  About FutureFit AI
  At FutureFit AI, we’re on a mission to unlock pathways between talent and opportunity using the power of AI. We focus on personalized, AI-powered career guidance for job seekers, emphasizing skills over extensive resumes, and partner with workforce development partners, governments, and employers to level access to opportunity.
 
  FutureFit AI is a growing, venture-backed company focused on using technology to improve the lives and outcomes for people going through career transitions. We’re a small, driven team, united by our commitment to the job seekers and workforce ecosystems we serve. We're not just building a company; we're shaping the future of work.
 
  We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, perform essential job functions, and receive other benefits and privileges of employment. Please contact us to request an accommodation.
  FutureFit AI All rights reserved, we are proud to be an equal opportunity workplace. We celebrate diversity and are committed to creating an inclusive environment for all employees. We do not discriminate on the basis of race, religion, color, gender identity, sexual orientation, age, disability, veteran status, or other applicable legally protected characteristics. We encourage people of different backgrounds, experiences, abilities, and perspectives to apply.","<div>
 <p>FutureFit AI is looking for a contract Senior Data Engineer to join our team. We have a culture of high trust, high impact, high velocity and a gritty determination to win. If you are passionate about development, like to have fun, do it right, and get things done we would love to hear from you!</p>
 <p><i> An important note: Data shows that men on average apply for a role if they meet 3/10 requirements while women often only do so if it&#x2019;s 10/10. We work hard to be clear and specific about what our roles include and demand and encourage you to apply if you see a strong (but not necessarily perfect) fit between you and the opportunity.</i></p>
 <p></p>
 <p><b> About You:</b></p>
 <ul>
  <li><p> Technology - You are a passionate data engineer, eager to learn, solve problems and share your knowledge with others</p></li>
  <li><p> Data - Strong architecture and design around various data paradigms (document stores, data lakes/warehouses and relational) and databases (Mongo, Elastic, Postgres, Redshift, etc.)</p></li>
  <li><p> Data Query - Experience writing highly performant data queries for large/complex data sets across relational and NoSQL databases</p></li>
  <li><p> Data Pipelines - Experience in architecture and implementation of ETL processes, streaming data, data process orchestration (Airflow)</p></li>
  <li><p> BI - Adept at connecting BI platforms (Looker) to data warehouses to provide critical insights via dashboards</p></li>
  <li><p> MLOps - Experience with model versioning, deployment, monitoring and serving models via REST APIs</p></li>
  <li><p> Understanding - You are eager to understand the business problem and collaborate with product to build a solution that meets both technology and business requirements</p></li>
  <li><p> Communication - You can share your perspective clearly and constructively, you are comfortable challenging ideas and collaborating to find the best approach</p></li>
  <li><p> Grit - You like to solve challenging problems, are not afraid of the unknown, eager to learn and take a systematic approach to solving problems</p></li>
  <li><p> AI - You are interested in integrating advances in AI into FutureFit AI products and the development process to drive improvements in velocity, quality and engineer experience</p></li>
  <li><p> Values - You are a good person who shares our values of excellence, transparency, and humanity</p></li>
 </ul>
 <p></p>
 <p><b><br> The Product&#x2019;s Technology Stack:</b></p>
 <ul>
  <li><p> Front End - React, TypeScript, Tailwind, Storybook</p></li>
  <li><p> Back End - Node.js, Nest.js, Python, Flask, GraphQL, Postgres, MongoDB, RedShift, Airflow</p></li>
  <li><p> AWS - CloudFront, Cognito, ECS, RDS, S3, SQS, Elasticsearch, VPC, Lambda</p></li>
 </ul>
 <p></p>
 <p><b> Location:</b></p>
 <p> Most days we work from home but everyone comes to the office at least once a week for face to face collaboration and team building. The office is located at 325 Front St West (a short walk from Union Station).</p>
 <p></p>
 <p><b> Our Company:</b></p>
 <p> In looking at a job posting, it&#x2019;s often hard to get a basic picture of the company profile (size, stage, structure, etc.) which is why we are sharing it with you upfront. This helps you quickly decide and helps us focus any time we spend together on going beyond the basics:</p>
 <p><i><br> Funding</i>: We have raised one round of funding led by JP Morgan which fueled a significant growth trajectory for us and we have a safe financial runway to execute against.</p>
 <p><i><br> Problem Domain:</i> Future of Work / Workforce Development - important that the problem domain interests you even if you haven&#x2019;t worked in the space before.</p>
 <p><i><br> Customers</i>: We partner with workforce development agencies, government agencies, and employers/enterprises.</p>
 <p><i><br> Structure</i>: We are organized around the following key departments: Growth, Customer Success, Product, Engineering, Data, People &amp; Culture, Finance &amp; Operations.</p>
 <p><i><br> Team</i>: We are a team of 30-50 across US and Canada with main hubs in New York and Toronto. This role will be based in Toronto.<br> <br> <i>Core Principles</i>: Be Curious, Drive to Outcomes, Raise the Bar, Speed Matters, Own It, Put We Over Me</p>
 <p></p>
 <h2 class=""jobSectionHeader""><b> About FutureFit AI</b></h2>
 <p> At FutureFit AI, we&#x2019;re on a mission to unlock pathways between talent and opportunity using the power of AI. We focus on personalized, AI-powered career guidance for job seekers, emphasizing skills over extensive resumes, and partner with workforce development partners, governments, and employers to level access to opportunity.</p>
 <p></p>
 <p> FutureFit AI is a growing, venture-backed company focused on using technology to improve the lives and outcomes for people going through career transitions. We&#x2019;re a small, driven team, united by our commitment to the job seekers and workforce ecosystems we serve. We&apos;re not just building a company; we&apos;re shaping the future of work.</p>
 <p></p>
 <p><i> We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, perform essential job functions, and receive other benefits and privileges of employment. Please contact us to request an accommodation.</i></p>
 <p><b><i><br> </i></b><i>FutureFit AI All rights reserved, we are proud to be an equal opportunity workplace. We celebrate diversity and are committed to creating an inclusive environment for all employees. We do not discriminate on the basis of race, religion, color, gender identity, sexual orientation, age, disability, veteran status, or other applicable legally protected characteristics. We encourage people of different backgrounds, experiences, abilities, and perspectives to apply.</i></p>
</div>","https://jobs.ashbyhq.com/futurefitai/d7718fd0-070e-4e87-9785-6eb79113e39e?utm_source=j5R86mOA1K","ebf3b0433986571a",,"Full-time",,"Ontario, CA","Senior Data Engineer","8 days ago","2023-10-17T11:54:00.831Z",,,,"2023-10-25T11:54:00.919Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=ebf3b0433986571a&from=jasx&tk=1hdjau6m4jgap800&vjs=3"
"Blackwell Security, Inc.","Blackwell Security Inc. is a start-up backed by venture capital, focused on bridging the technology gap in healthcare. Our purpose-built ecosystem provides comprehensive cybersecurity managed services for life sciences and healthcare. We are building a customizable product that ensures health systems have access to a suite of security solutions, with built-in visualization and optimization to ensure the safety of patient information. 
  As we continue to build out our core team, we are adding a Data Engineer to our Engineering Team. You will be working cross-functionally with business domain experts, analytics, and engineering teams to design and implement our Data models. You will design, implement, and scale data pipelines that transform billions of records into action and insight. 
  This is a unique opportunity to jump into an early-stage start-up at a pivotal time and make a meaningful impact. If you thrive in a small, growing environment and love the energy of start-ups, this is the role for you! 
  While our headquarters are in Detroit, Michigan, this is a remote role but ideally a candidate would live in Detroit or Minneapolis, location of our core Engineering Team. This role is not eligible for visa sponsorship. 
  What you will do in the Data Engineer role: 
  
  Collaborate with engineering to build and maintain an enterprise data ecosystem including ingestion, storage, organization and interface. 
  Analyze the business and technical requirements for data systems and applications; Coordinate the integration of IT policies, procedures and development practices. 
  Translate business requirements into data models that are easy to understand and used by different disciplines across the company. 
  Design, implement, build/enhance pipelines that deliver data with measurable quality under the SLA. 
  Partner with business domain experts, data analysts and engineering teams to build foundational data sets that are trusted, well understood, aligned with business strategy and enable self-service. 
  Champion the overall strategy for data governance, security, privacy, quality and retention that will satisfy business policies and requirements. 
  Own and document foundational company metrics and benchmarks with clear definition and data lineage. 
  Identify, document and promote best practices. 
  Design and architect data systems, focusing not only on performance and scalability, but also on crafting a beautiful user experience. 
  Define/Implement data visualizations & UX for external/internal customers. 
  Taking a thoughtful approach to decision making; balance speed and quality, with a focus on tangible results. 
  Explore Blackwell’s data to discover trends and opportunities, identify what questions we should be asking of our data. 
  Analyze & evaluate transactional system data for transformation and use in reporting, analytics, and AI/ML. 
  Evaluate and establish early strategies and usage of AI (machine learning, generative AI, etc.). 
  
 Qualities and skills for success in the Data Engineer role: 
  
  Bachelor's degree in Computer Science, Engineering, or related technical or business field. 
  Attention to detail, and Agile development experience. 
  Experience with Python and AWS services. 
  Experience with various data storage systems, RDBMS, Document/NoSQL DBs, etc. 
  Experience implementing data pipelines via methods such as ETL, ELT, EL/TL, DaaS, Data Lake or ODS. 
  Experience experimenting with and applying AI (machine learning, generative AI, etc.) in an enterprise environment. 
  Experience working with multi-customer multi-tenant environments preferred. Experience with cybersecurity data is not required but is a plus. 
  Adaptable and focused on solutions. 
  
 Equal Employment Opportunity 
  We’re proud to be an equal opportunity employer and welcome our employee’s differences, regardless of race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, or Veteran status. Difference makes us better. Join us.","<div>
 <p>Blackwell Security Inc. is a start-up backed by venture capital, focused on bridging the technology gap in healthcare. Our purpose-built ecosystem provides comprehensive cybersecurity managed services for life sciences and healthcare. We are building a customizable product that ensures health systems have access to a suite of security solutions, with built-in visualization and optimization to ensure the safety of patient information.</p> 
 <p> As we continue to build out our core team, we are adding a Data Engineer to our Engineering Team. You will be working cross-functionally with business domain experts, analytics, and engineering teams to design and implement our Data models. You will design, implement, and scale data pipelines that transform billions of records into action and insight.</p> 
 <p> This is a unique opportunity to jump into an early-stage start-up at a pivotal time and make a meaningful impact. If you thrive in a small, growing environment and love the energy of start-ups, this is the role for you!</p> 
 <p> While our headquarters are in Detroit, Michigan, this is a remote role but ideally a candidate would live in Detroit or Minneapolis, location of our core Engineering Team. This role is not eligible for visa sponsorship.</p> 
 <p> <b>What you will do in the Data Engineer role:</b></p> 
 <ul> 
  <li>Collaborate with engineering to build and maintain an enterprise data ecosystem including ingestion, storage, organization and interface.</li> 
  <li>Analyze the business and technical requirements for data systems and applications; Coordinate the integration of IT policies, procedures and development practices.</li> 
  <li>Translate business requirements into data models that are easy to understand and used by different disciplines across the company.</li> 
  <li>Design, implement, build/enhance pipelines that deliver data with measurable quality under the SLA.</li> 
  <li>Partner with business domain experts, data analysts and engineering teams to build foundational data sets that are trusted, well understood, aligned with business strategy and enable self-service.</li> 
  <li>Champion the overall strategy for data governance, security, privacy, quality and retention that will satisfy business policies and requirements.</li> 
  <li>Own and document foundational company metrics and benchmarks with clear definition and data lineage.</li> 
  <li>Identify, document and promote best practices.</li> 
  <li>Design and architect data systems, focusing not only on performance and scalability, but also on crafting a beautiful user experience.</li> 
  <li>Define/Implement data visualizations &amp; UX for external/internal customers.</li> 
  <li>Taking a thoughtful approach to decision making; balance speed and quality, with a focus on tangible results.</li> 
  <li>Explore Blackwell&#x2019;s data to discover trends and opportunities, identify what questions we should be asking of our data.</li> 
  <li>Analyze &amp; evaluate transactional system data for transformation and use in reporting, analytics, and AI/ML.</li> 
  <li>Evaluate and establish early strategies and usage of AI (machine learning, generative AI, etc.).</li> 
 </ul> 
 <p><b>Qualities and skills for success in the Data Engineer role:</b></p> 
 <ul> 
  <li>Bachelor&apos;s degree in Computer Science, Engineering, or related technical or business field.</li> 
  <li>Attention to detail, and Agile development experience.</li> 
  <li>Experience with Python and AWS services.</li> 
  <li>Experience with various data storage systems, RDBMS, Document/NoSQL DBs, etc.</li> 
  <li>Experience implementing data pipelines via methods such as ETL, ELT, EL/TL, DaaS, Data Lake or ODS.</li> 
  <li>Experience experimenting with and applying AI (machine learning, generative AI, etc.) in an enterprise environment.</li> 
  <li>Experience working with multi-customer multi-tenant environments preferred. Experience with cybersecurity data is not required but is a plus.</li> 
  <li>Adaptable and focused on solutions.</li> 
 </ul> 
 <p><b>Equal Employment Opportunity</b></p> 
 <p> We&#x2019;re proud to be an equal opportunity employer and welcome our employee&#x2019;s differences, regardless of race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, or Veteran status. Difference makes us better. Join us.</p>
</div>
<p></p>","https://blank-metal-inc.rippling-ats.com/job/642107/data-engineer?s=in","90181ff14d6a60ba",,,,"Remote","Data Engineer","12 days ago","2023-10-13T11:54:04.354Z","2.5","6",,"2023-10-25T11:54:04.356Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=90181ff14d6a60ba&from=jasx&tk=1hdjau6m4jgap800&vjs=3"
"Buyers Edge Platform, LLC","Who are we? 
  Buyers Edge Platform (""BEP"") was born out of the 20-year growth and success of its founding companies Dining Alliance, Buyers Edge and Consolidated Concepts. BEP is a technology enabled group purchasing network, which provides group purchasing services, SaaS based technology solutions, and supply chain consulting and procurement related services to foodservice operators across many verticals, including restaurants (independents as well as multiunit, chains and franchise concepts), hospitality (hotels, casinos, resorts), healthcare (LTC based operators), colleges & universities and array of other foodservice based operators (food trucks, caterers, amusement parks and other leisure based operators). BEP represents over $35 Billion in Network Transactions. We are committed and passionate about our mission to keep foodservice operators thriving by saving them money and increasing the quality of their products. 
  This position is remote and based around East Coast working hours. We are unable to offer work sponsorship for this role. 
  As a Data Integration Engineer, you will help us bring in data from external sources to support the various development teams. You will also support those teams in various ways including as a development resource. 
  Your impact: 
  
  Collaborate with stakeholders to understand data needs. Learn about various internal software products to support development teams with data integration projects. 
  Work to align various new or existing data systems to meet business goals. Combine multiple data sources for business use. 
  Source and evaluate the quality, reliability, and security of various data sources relevant to assigned projects. 
  Develop code to work with internal data or retrieve data from external sources including APIs, databases, and/or other third-party applications as needed. This may include cleaning, processing, or transforming raw data to make it suitable for use in our systems. 
  Coordinate projects with multiple cross-functional teams including developers, data scientists, analysts, and business stakeholders. 
  
 About you: 
  
  Proficient in programming languages such as Python, Java, or Node. 
  Strong understanding of database technologies such as MySQL, Aurora, and Redshift. 
  Experience performing research/discovery for new data from external sources. 
  Experience leading projects and strong understanding of project management principles. 
  Experience integrating with REST APIs to pull data and convert/save for business use. 
  Experience with API tools such as Postman. 
  Experience developing software in an agile environment and strong understanding of agile development methodologies/practices. 
  Ability to work independently on complex systems and algorithms. Strong teamwork and collaboration skills with the ability to work effectively with cross-functional teams and stakeholders. 
  Strong communication and interpersonal skills. 
  Strong analytical and problem-solving skills. 
  Strong Microsoft Office skills including Word & Excel. 
  BA/BS in a technical discipline, or equivalent professional experience. 
  5-7+ years experience in programming & the key abilities outlined above. 
  
 What's in this for you? 
 Amazing coverages to start. Medical, dental, and vision coverages are just the beginning! We also offer ancillary plans, such as flexible spending accounts for both health and dependent care, critical illness, accident, and voluntary life as well as company paid life and long-term-disability plans! On top of this, we also offer a 401(k) plan with company match. 
  Invest in your success. We will provide you with a thorough training and development program; and offer competitive compensation. 
  Live well = Work well. Relax with our Personal Responsibility Paid Time Off policy where you don't have to accrue time off in order to take it! We also offer half-day Summer Fridays!
 
   We welcome all. 
   We are committed to creating a diverse environment and are proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to actual or perceived race, color, creed, religion, national origin, ancestry, citizenship status, age, sex or gender (including pregnancy, childbirth and pregnancy-related conditions), gender identity or expression (including transgender status), sexual orientation, marital status, military service and veteran status, physical or mental disability, genetic information, or any other characteristic protected by applicable federal, state or local laws and ordinances.","<div>
 <p><b>Who are we?</b></p> 
 <p> Buyers Edge Platform (&quot;BEP&quot;) was born out of the 20-year growth and success of its founding companies Dining Alliance, Buyers Edge and Consolidated Concepts. BEP is a technology enabled group purchasing network, which provides group purchasing services, SaaS based technology solutions, and supply chain consulting and procurement related services to foodservice operators across many verticals, including restaurants (independents as well as multiunit, chains and franchise concepts), hospitality (hotels, casinos, resorts), healthcare (LTC based operators), colleges &amp; universities and array of other foodservice based operators (food trucks, caterers, amusement parks and other leisure based operators). BEP represents over &#x24;35 Billion in Network Transactions. We are committed and passionate about our mission to keep foodservice operators thriving by saving them money and increasing the quality of their products.</p> 
 <p><b> This position is remote and based around East Coast working hours. We are unable to offer work sponsorship for this role.</b></p> 
 <p> As a Data Integration Engineer, you will help us bring in data from external sources to support the various development teams. You will also support those teams in various ways including as a development resource.</p> 
 <p><b> Your impact:</b></p> 
 <ul> 
  <li>Collaborate with stakeholders to understand data needs. Learn about various internal software products to support development teams with data integration projects.</li> 
  <li>Work to align various new or existing data systems to meet business goals. Combine multiple data sources for business use.</li> 
  <li>Source and evaluate the quality, reliability, and security of various data sources relevant to assigned projects.</li> 
  <li>Develop code to work with internal data or retrieve data from external sources including APIs, databases, and/or other third-party applications as needed. This may include cleaning, processing, or transforming raw data to make it suitable for use in our systems.</li> 
  <li>Coordinate projects with multiple cross-functional teams including developers, data scientists, analysts, and business stakeholders.</li> 
 </ul> 
 <p><b>About you:</b></p> 
 <ul> 
  <li>Proficient in programming languages such as Python, Java, or Node.</li> 
  <li>Strong understanding of database technologies such as MySQL, Aurora, and Redshift.</li> 
  <li>Experience performing research/discovery for new data from external sources.</li> 
  <li>Experience leading projects and strong understanding of project management principles.</li> 
  <li>Experience integrating with REST APIs to pull data and convert/save for business use.</li> 
  <li>Experience with API tools such as Postman.</li> 
  <li>Experience developing software in an agile environment and strong understanding of agile development methodologies/practices.</li> 
  <li>Ability to work independently on complex systems and algorithms. Strong teamwork and collaboration skills with the ability to work effectively with cross-functional teams and stakeholders.</li> 
  <li>Strong communication and interpersonal skills.</li> 
  <li>Strong analytical and problem-solving skills.</li> 
  <li>Strong Microsoft Office skills including Word &amp; Excel.</li> 
  <li>BA/BS in a technical discipline, or equivalent professional experience.</li> 
  <li>5-7+ years experience in programming &amp; the key abilities outlined above.</li> 
 </ul> 
 <p><b>What&apos;s in this for you? </b></p>
 <p><b>Amazing coverages to start.</b> Medical, dental, and vision coverages are just the beginning! We also offer ancillary plans, such as flexible spending accounts for both health and dependent care, critical illness, accident, and voluntary life as well as company paid life and long-term-disability plans! On top of this, we also offer a 401(k) plan with company match.</p> 
 <p><b> Invest in your success.</b> We will provide you with a thorough training and development program; and offer competitive compensation.</p> 
 <p><b> Live well = Work well.</b> Relax with our Personal Responsibility Paid Time Off policy where you don&apos;t have to accrue time off in order to take it! We also offer half-day Summer Fridays!</p>
 <div>
  <p> We welcome all.</p> 
  <p> We are committed to creating a diverse environment and are proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to actual or perceived race, color, creed, religion, national origin, ancestry, citizenship status, age, sex or gender (including pregnancy, childbirth and pregnancy-related conditions), gender identity or expression (including transgender status), sexual orientation, marital status, military service and veteran status, physical or mental disability, genetic information, or any other characteristic protected by applicable federal, state or local laws and ordinances.</p>
 </div>
</div>","https://www.indeed.com/rc/clk?jk=0b0dbb24f9a073a7&atk=&xpse=SoDy67I3Jzd0wKQ9R50LbzkdCdPP","0b0dbb24f9a073a7",,,,"Remote","Data Integration Engineer","7 days ago","2023-10-18T11:54:09.574Z","4.2","11",,"2023-10-25T11:54:09.576Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=0b0dbb24f9a073a7&from=jasx&tk=1hdjau6m4jgap800&vjs=3"
"INADEV","Description:
 
  Formed in 2011, INADEV is focused on its founding principle to build innovative customer-centric solutions incredibly fast, secure, and at scale. We deliver world-class digital experiences to some of the largest federal agencies and commercial companies. Our technical expertise and innovations are comprised of codeless automation, identity intelligence, immersive technology, artificial intelligence/machine learning (AI/ML), virtualization, and digital transformation.
  POSITION DESCRIPTION:
 
   Perform migration and testing of static data and transaction data from one core system to another, audit, reconciliation and exception reporting.
   Provide support for data migration, data engineering, and integration of existing systems.
   Developing and integrating multiple data types across a range of data sets and sources.
   Performing day-to-day operations of systems that depend on data, ensuring data is properly processed and securely transferred to its appropriate location, in a timely manner.
   Evaluate current system designs and identify areas for improvement to create a system that is highly available and has low data latency.
   Plan and design the integration of various source systems and the migration of data between systems.
   Build and implement the source system integration and data migration plan.
   Developing, managing, manipulating, storing and parsing data across a data pipeline for variety of target sources and data consumers
   Writing code to ensure the performance and reliability of data extraction and processing
   Supporting continuous process automation for data ingestion
   Assisting with the maintenance of applications and tools that reside on the data driven systems (upgrades, patches, configuration changes, etc.)
   Working with program management and engineers to implement and document complex and evolving requirements
   Actively and collaboratively participating as a member of a cross-functional Agile/Scrum team while following all Agile/Scrum best practices
   Advocating for and adhering to lean-agile engineering principles and practices such as API-first design, simple design, continuous integration, version control, and automated testing
   Helping cultivate an environment that promotes customer service excellence, innovation, collaboration, and teamwork
   Demonstrating significant technical competence and ownership to broad audiences while driving progress on company strategic objectives at multiple levels.
   Generating and articulating technical strategy to diverse audiences, both technical and non-technical.
 
  PHYSICAL DEMANDS:
 
   Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions
 
  INADEV Corporation does not discriminate against qualified individuals based on their status as protected veterans or individuals with disabilities and prohibits discrimination against all individuals based on their race, color, religion, sex, sexual orientation/gender identity, or national origin. Requirements: 
  NON-TECHNICAL REQUIREMENTS:
 
   Ability to pass a 7 year background check and have the ability to obtain a U.S. Government clearance.
   Must have been a resident of the continental U.S. for at least the last 3 years.
   Must be willing to work in Eastern Standard Business hours.
   Must possess good communication (written/verbal) skills.
 
  MANDATORY REQUIREMENTS:
 
   Must have a Bachelor's Degree in a technical discipline and 5+ years pertinent experience as a Data Engineer.
   Must have proven experience with data migration project from on-prem to cloud.
   Must have working knowledge of ETL tools like AWS Glue
   Experience with Data Reconciliation post migration
   Must have working experience with a commercial database like SQL Server, Oracle, MySQL
 
  DESIRED SKILLS:
 
   Experience with Microsoft SSIS or equivalent, AWS DMS/MGN preferred","<div>
 Description:
 <p></p>
 <p><br> Formed in 2011, INADEV is focused on its founding principle to build innovative customer-centric solutions incredibly fast, secure, and at scale. We deliver world-class digital experiences to some of the largest federal agencies and commercial companies. Our technical expertise and innovations are comprised of codeless automation, identity intelligence, immersive technology, artificial intelligence/machine learning (AI/ML), virtualization, and digital transformation.</p>
 <p><b> POSITION DESCRIPTION:</b></p>
 <ul>
  <li> Perform migration and testing of static data and transaction data from one core system to another, audit, reconciliation and exception reporting.</li>
  <li> Provide support for data migration, data engineering, and integration of existing systems.</li>
  <li> Developing and integrating multiple data types across a range of data sets and sources.</li>
  <li> Performing day-to-day operations of systems that depend on data, ensuring data is properly processed and securely transferred to its appropriate location, in a timely manner.</li>
  <li> Evaluate current system designs and identify areas for improvement to create a system that is highly available and has low data latency.</li>
  <li> Plan and design the integration of various source systems and the migration of data between systems.</li>
  <li> Build and implement the source system integration and data migration plan.</li>
  <li> Developing, managing, manipulating, storing and parsing data across a data pipeline for variety of target sources and data consumers</li>
  <li> Writing code to ensure the performance and reliability of data extraction and processing</li>
  <li> Supporting continuous process automation for data ingestion</li>
  <li> Assisting with the maintenance of applications and tools that reside on the data driven systems (upgrades, patches, configuration changes, etc.)</li>
  <li> Working with program management and engineers to implement and document complex and evolving requirements</li>
  <li> Actively and collaboratively participating as a member of a cross-functional Agile/Scrum team while following all Agile/Scrum best practices</li>
  <li> Advocating for and adhering to lean-agile engineering principles and practices such as API-first design, simple design, continuous integration, version control, and automated testing</li>
  <li> Helping cultivate an environment that promotes customer service excellence, innovation, collaboration, and teamwork</li>
  <li> Demonstrating significant technical competence and ownership to broad audiences while driving progress on company strategic objectives at multiple levels.</li>
  <li> Generating and articulating technical strategy to diverse audiences, both technical and non-technical.</li>
 </ul>
 <p><b> PHYSICAL DEMANDS:</b></p>
 <ul>
  <li> Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions</li>
 </ul>
 <p> INADEV Corporation does not discriminate against qualified individuals based on their status as protected veterans or individuals with disabilities and prohibits discrimination against all individuals based on their race, color, religion, sex, sexual orientation/gender identity, or national origin.</p> Requirements: 
 <p><b> NON-TECHNICAL REQUIREMENTS:</b></p>
 <ul>
  <li> Ability to pass a 7 year background check and have the ability to obtain a U.S. Government clearance.</li>
  <li> Must have been a resident of the continental U.S. for at least the last 3 years.</li>
  <li> Must be willing to work in Eastern Standard Business hours.</li>
  <li> Must possess good communication (written/verbal) skills.</li>
 </ul>
 <p><b> MANDATORY REQUIREMENTS:</b></p>
 <ul>
  <li> Must have a Bachelor&apos;s Degree in a technical discipline and 5+ years pertinent experience as a Data Engineer.</li>
  <li> Must have proven experience with data migration project from on-prem to cloud.</li>
  <li> Must have working knowledge of ETL tools like AWS Glue</li>
  <li> Experience with Data Reconciliation post migration</li>
  <li> Must have working experience with a commercial database like SQL Server, Oracle, MySQL</li>
 </ul>
 <p><b> DESIRED SKILLS:</b></p>
 <ul>
  <li> Experience with Microsoft SSIS or equivalent, AWS DMS/MGN preferred</li>
 </ul>
</div>","https://recruiting.paylocity.com/recruiting/jobs/Details/2011376/INADEV/Data-Engineer?source=Indeed_Feed","cab4f56240621034",,"Full-time",,"Remote","Data Engineer","11 days ago","2023-10-14T11:54:09.364Z","3","6","$90,000 - $110,000 a year","2023-10-25T11:54:09.371Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=cab4f56240621034&from=jasx&tk=1hdjau6m4jgap800&vjs=3"
"CorEvitas","POSITION SUMMARY: 
 The Data Engineer will be responsible for designing and developing our data and data pipeline architecture, as well as optimizing data flow and collection for cross-functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure that optimal data delivery architecture is consistent throughout projects. The successful candidate will be self-motivated and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives. 
 This is an important, visible, roll-up-your-sleeves position that enables our biostatisticians, pharmacovigilance experts and epidemiologists to gain insights and critical knowledge, enabling them to make better decisions faster. 
 PRINCIPLE DUTIES AND RESPONSIBILITIES: 
 
 
  
   Work together with the Director, Data Engineering and the software development team to realize the data architecture and data processing pipelines on AWS.
   
  
   Create and maintain optimal data pipeline architecture.
   
  
   Assemble large, complex data sets that meet functional / non-functional business requirements.
   
  
   Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
   
  
   Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
   
  
   Build analytics tools that utilize the data pipeline to provide actionable insights.
   
  
   Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
   
  
   Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.
   
  
   Create data tools for analytics and data scientist team members that assist them in building and optimizing our life sciences offerings.
   
  
   Promote data best-practices across the organization and help build a “culture of data”.
   
  
 
 MINIMUM QUALIFICATIONS: 
 Skills/Knowledge: 
 
 Must have Strong SQL Server Database development, maintenance experience. 
 Must have working knowledge SSRS reporting. 
 Must have proficiency with SQL Server functions and stored procedures. 
 Must be able to debug, profile, tune SQL queries, functions and stored procedures. 
 NoSQL Database experience is a big plus. 
 Proficiency in at least one high-level programming language, e.g., Python or Java. 
 Demonstrated experience developing data pipelines/ETL/ELT processes. 
 Excellent communication and organizational skills. 
 Also, Cloud platform experience that include: 
 
  AWS data storage and retrieval experience highly preferred 
  AWS data services, e.g., AWS Glue 
  AWS data processing and analytics services 
  
 Big Plus: 
 
  Familiarity working with sensitive data and with CFR Part 11, HIPAA, GDPR compliance. 
  Working knowledge of clinical and pharma data 
  Working knowledge of Clinical Data Standards such as CDISC 
  
 Experience: 
 5+ yeas overall experience, at least 3 in data management and/or data-centric software development. Life sciences background preferred. 
 Experience in writing production-level SQL and working with various databases and reporting systems. 
 
 Education/training: 
 BS/MS degree in Computer Science, Computer Engineering, or other technical discipline. 
 
 SPECIAL REQUIREMENTS: 
 Fully Remote, though some Travel may be required for this role 
 
 This description is not intended to be a complete statement of the job, but rather to act as a guide 
 
 About CorEvitas 
 CorEvitas, now part of Thermo Fisher Scientific, is a science-led, real-world data intelligence company. Using syndicated registry data and analytic services to understand the post-approval comparative effectiveness and safety of approved therapies, CorEvitas provides biopharmaceutical companies with objective data and clinical insights to demonstrate the value of their products to clinicians, patients, payers, and regulators. The company operates nine major autoimmune and inflammatory registries across the U.S., Canada, and Japan, collecting data from over 400 participating investigator sites, including collection of biosamples linked to the deep clinical data. CorEvitas recently expanded its services to include Pregnancy Registries, through the acquisition of Pregistry. CorEvitas also conducts client-sponsored registries through its Patient Powered Registries business, employing a transformative patient-focused registry model to support research needs for patient-centered outcomes across all therapeutic areas. The company’s regulatory-grade registry data is complemented by its Patient Experience business, supporting evidence-based patient engagement initiatives across the product lifecycle, as well as its Specialty EMR Data business and retinal data set. CorEvitas is headquartered in Waltham, MA and is a portfolio company of Audax Private Equity. www.corevitas.com 
 
 CorEvitas is proud to provide equal employment opportunities to all qualified individuals without regard to race, color, religion, sex, gender identity, sexual orientation, pregnancy, age, national origin, physical or mental disability, military or veteran status, genetic information, or any other protected classification. Minorities, women, LGBTQ candidates, Veterans, and individuals with disabilities are encouraged to apply. 
 CorEvitas participate with E-Verify","<div>
 <p><b>POSITION SUMMARY: </b></p>
 <p>The Data Engineer will be responsible for designing and developing our data and data pipeline architecture, as well as optimizing data flow and collection for cross-functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure that optimal data delivery architecture is consistent throughout projects. The successful candidate will be self-motivated and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company&#x2019;s data architecture to support our next generation of products and data initiatives.</p> 
 <p>This is an important, visible, roll-up-your-sleeves position that enables our biostatisticians, pharmacovigilance experts and epidemiologists to gain insights and critical knowledge, enabling them to make better decisions faster.</p> 
 <p><b>PRINCIPLE DUTIES AND RESPONSIBILITIES: </b></p>
 <p></p>
 <div>
  <ul>
   <li>Work together with the Director, Data Engineering and the software development team to realize the data architecture and data processing pipelines on AWS.</li>
  </ul> 
  <ul>
   <li>Create and maintain optimal data pipeline architecture.</li>
  </ul> 
  <ul>
   <li>Assemble large, complex data sets that meet functional / non-functional business requirements.</li>
  </ul> 
  <ul>
   <li>Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.</li>
  </ul> 
  <ul>
   <li>Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS &#x2018;big data&#x2019; technologies.</li>
  </ul> 
  <ul>
   <li>Build analytics tools that utilize the data pipeline to provide actionable insights.</li>
  </ul> 
  <ul>
   <li>Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.</li>
  </ul> 
  <ul>
   <li>Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.</li>
  </ul> 
  <ul>
   <li>Create data tools for analytics and data scientist team members that assist them in building and optimizing our life sciences offerings.</li>
  </ul> 
  <ul>
   <li>Promote data best-practices across the organization and help build a &#x201c;culture of data&#x201d;.</li>
  </ul> 
 </div> 
 <p></p>
 <p><b>MINIMUM QUALIFICATIONS:</b><b> </b></p>
 <p>Skills/Knowledge:</p> 
 <p></p>
 <p>Must have Strong SQL Server Database development, maintenance experience.</p> 
 <p>Must have working knowledge SSRS reporting.</p> 
 <p>Must have proficiency with SQL Server functions and stored procedures.</p> 
 <p>Must be able to debug, profile, tune SQL queries, functions and stored procedures.</p> 
 <p>NoSQL Database experience is a big plus.</p> 
 <p>Proficiency in at least one high-level programming language, e.g., Python or Java.</p> 
 <p>Demonstrated experience developing data pipelines/ETL/ELT processes.</p> 
 <p>Excellent communication and organizational skills.</p> 
 <p>Also, Cloud platform experience that include:</p> 
 <ul>
  <li>AWS data storage and retrieval experience highly preferred</li> 
  <li>AWS data services, e.g., AWS Glue</li> 
  <li>AWS data processing and analytics services</li> 
 </ul> 
 <p>Big Plus:</p> 
 <ul>
  <li>Familiarity working with sensitive data and with CFR Part 11, HIPAA, GDPR compliance.</li> 
  <li>Working knowledge of clinical and pharma data</li> 
  <li>Working knowledge of Clinical Data Standards such as CDISC</li> 
 </ul> 
 <p>Experience:</p> 
 <p>5+ yeas overall experience, at least 3 in data management and/or data-centric software development. Life sciences background preferred.</p> 
 <p>Experience in writing production-level SQL and working with various databases and reporting systems.</p> 
 <p></p>
 <p>Education/training:</p> 
 <p>BS/MS degree in Computer Science, Computer Engineering, or other technical discipline.</p> 
 <p></p>
 <p><b>SPECIAL REQUIREMENTS: </b></p>
 <p>Fully Remote, though some Travel may be required for this role</p> 
 <p></p>
 <p>This description is not intended to be a complete statement of the job, but rather to act as a guide</p> 
 <p></p>
 <p><b>About CorEvitas</b></p> 
 <p>CorEvitas, now part of Thermo Fisher Scientific, is a science-led, real-world data intelligence company. Using syndicated registry data and analytic services to understand the post-approval comparative effectiveness and safety of approved therapies, CorEvitas provides biopharmaceutical companies with objective data and clinical insights to demonstrate the value of their products to clinicians, patients, payers, and regulators. The company operates nine major autoimmune and inflammatory registries across the U.S., Canada, and Japan, collecting data from over 400 participating investigator sites, including collection of biosamples linked to the deep clinical data. CorEvitas recently expanded its services to include Pregnancy Registries, through the acquisition of Pregistry. CorEvitas also conducts client-sponsored registries through its Patient Powered Registries business, employing a transformative patient-focused registry model to support research needs for patient-centered outcomes across all therapeutic areas. The company&#x2019;s regulatory-grade registry data is complemented by its Patient Experience business, supporting evidence-based patient engagement initiatives across the product lifecycle, as well as its Specialty EMR Data business and retinal data set. CorEvitas is headquartered in Waltham, MA and is a portfolio company of Audax Private Equity. www.corevitas.com</p> 
 <p></p>
 <p><i>CorEvitas is proud to provide equal employment opportunities to all qualified individuals without regard to race, color, religion, sex, gender identity, sexual orientation, pregnancy, age, national origin, physical or mental disability, military or veteran status, genetic information, or any other protected classification. Minorities, women, LGBTQ candidates, Veterans, and individuals with disabilities are encouraged to apply.</i></p> 
 <p>CorEvitas participate with E-Verify</p>
</div>","https://corevitas.hrmdirect.com/employment/view.php?req=2775802&jbsrc=1014","7fc589735d63bd6c",,,,"Waltham, MA 02451","Data Engineer, Remote","12 days ago","2023-10-13T11:54:06.205Z","2.5","6",,"2023-10-25T11:54:06.206Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=7fc589735d63bd6c&from=jasx&tk=1hdjau6m4jgap800&vjs=3"
"Samsara","Who we are 
   Samsara (NYSE: IOT) is the pioneer of the Connected Operations™ Cloud, which is a platform that enables organizations that depend on physical operations to harness Internet of Things (IoT) data to develop actionable insights and improve their operations. At Samsara, we are helping improve the safety, efficiency and sustainability of the physical operations that power our global economy. Representing more than 40% of global GDP, these industries are the infrastructure of our planet, including agriculture, construction, field services, transportation, and manufacturing — and we are excited to help digitally transform their operations at scale. 
   Working at Samsara means you'll help define the future of physical operations and be on a team that's shaping an exciting array of product solutions, including Video-Based Safety, Vehicle Telematics, Apps and Driver Workflows, Equipment Monitoring, and Site Visibility. As part of a recently public company, you'll have the autonomy and support to make an impact as we build for the long term. 
   Recent awards we've won include: 
   Glassdoor's Highest-Rated Tech Companies for Culture and Values 2023 
   Great Place To Work Certified™ 2023 
   Best Place to Work by Built In 2023 
   Financial Times The Americas' Fastest Growing Companies 2023 
   Deloitte Fast 500 Companies 
   We see a profound opportunity for data to improve the safety, efficiency, and sustainability of operations, and hope you consider joining us on this exciting journey.
 
  About the team: 
  Data and Analytics is a critical team within Marketing. Our mission is to enable revenue performance by providing marketing and sales teams with the insights, tools, infrastructure and consultation to make data driven judgements. We are a scrappy and growing team that loves all things data! The team will be composed of data engineers, analytics managers and data scientists. We are passionate about leveraging world class data and analytics to deliver a great customer experience. 
  Our team promotes an agile, collaborative, supportive environment where diverse thinking, innovative design, and experimentation is welcomed and encouraged. 
  You should apply if: 
  
  You want to impact the industries that run our world: Your efforts will result in real-world impact—helping to keep the lights on, get food into grocery stores, reduce emissions, and most importantly, ensure workers return home safely. 
  You want to build for scale: With over 2.3 million IoT devices deployed to our global customers, you will work on a range of new and mature technologies driving scalable innovation for customers across industries driving the world's physical operations. 
  You are a team player: Working on our partners requires a mix of independent effort and collaboration. Motivated by our mission, we're all racing toward our connected operations vision, and we intend to win—together. 
  You are a life-long learner: We have ambitious goals. Every Samsarian has a growth mindset as we work with a wide range of technologies, challenges, and customers that push us to learn on the go. 
  
 Click here to learn about what we value at Samsara. 
  In this role, you will: 
  
  Develop and maintain marketing databases, datasets, pipelines and Samsara's Customer Data Platform (CDP) to enable advanced segmentation, targeting, automation and analytics. 
  Manage critical data pipelines to enable our growth initiatives and advanced analytics. Manage the SLAs for those data pipelines and constantly improve efficiency and data quality. 
  Facilitate data integration and transformation requirements for moving data between applications; ensuring interoperability of applications with data mart and CDP environments. 
  Develop and improve the current data architecture, data quality, monitoring and data availability. 
  Identify data needs from broad stakeholders, understand requirements for metrics and analysis, and build efficient and scalable data products to enable a data-driven marketing approach. 
  Write sophisticated yet optimized data transformations in SQL/Python to generate data products consumed by customer systems and Analytics, Marketing Operations, Sales Operations teams. 
  Champion, role model, and embed Samsara's cultural principles (Obsess Over the Customer, Build for the Long Term, Growth Mindset) as we scale globally and across new offices 
  
 Minimum requirements for the role: 
  
  5+ years of working experience in a growth, software or data engineering role 
  Excellent SQL and Python knowledge with strong hands-on data modeling 
  Experience with data warehouse technical architectures, infrastructure components, ETL/ ELT and reporting/analytic tools. 
  Hands-on experience working with modern data technologies stack such as Databricks, Google Big Query, Redshift, RDS, Snowflake or similar solutions 
  Experience with development lifecycle tools such as Github, TFS, etc. 
  Comfort in working with business customers to gather requirements and gain a deep understanding of varied datasets. 
  Familiarity with customer, marketing and/or web data 
  Experience integrating data from core Sales and Marketing platforms (e.g. Marketing Automation, CRM, and web analytics) 
  Self-starter, motivated, responsible, innovative and technology-driven candidate who performs well both unsupervised and as a team member 
  A proactive problem solver and have good communication as well as project management skills to relay your findings and solutions across technical and non technical audiences 
  
 An ideal candidate also has: 
  
  Knowledge of Marketo, Salesforce.com and Google Analytics 
  Experience working with CDPs such as Segment, Blueshift, Lytics or Adobe Real-time CDP 
  Experience with data visualization tools and packages (e.g. Looker, Domo, Tableau, MixPanel) 
  Familiarity with Marketing Technologies (MarTech stacks) 
  Experience coding with Scala, R or Pandas 
  Data Science, machine learning or predictive analytics experience 
 
 
  
   Samsara’s Compensation Philosophy: Samsara’s compensation program is designed to deliver total compensation (based on role, level, and geography) that is above market. We do this through our base salary + bonus/variable + restricted stock unit awards (RSUs). A new hire RSU award is awarded at the time of hire, and additional RSU refresh grants may be awarded annually. 
    We pay for performance, and top performers are eligible to receive above target equity refresh awards which allow employees to achieve higher market positioning.
  
   The range of annual base salary for full-time employees for this position is below. Please note that base pay offered may vary depending on factors including your city of residence, job-related knowledge, skills, and experience.
  
    $122,400—$180,000 USD
  
 
 
   At Samsara, we welcome everyone regardless of their background, race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, etc. We depend on the unique approaches of our team members to help us solve complex problems. We are committed to increasing diversity across our team and ensuring that Samsara is a place where people from all backgrounds can make an impact. 
   Accommodations 
   Samsara is an inclusive work environment, and we are committed to ensuring equal opportunity in employment for qualified persons with disabilities. Please email accessibleinterviewing@samsara.com or click here if you require any reasonable accommodations throughout the recruiting process. 
   Benefits 
   Full time employees receive an above market total compensation package along with employee-led remote and flexible working, health benefits, Samsara for Good charity fund, and much, much more. Take a look at our Benefits site to learn more. 
   Flexible Working 
   At Samsara, we have adopted a flexible way of working, enabling teams and individuals to do their best work, regardless of where they're based. We value in-person collaboration and know a change of scenery and quiet space to work is welcomed from time to time, but also appreciate that the world of work has changed. Our offices remain open for those who prefer to collaborate or work in-office, but we also encourage fully remote applicants. As most roles are not required to be in the office, we are able to hire remotely where Samsara has an established presence. If a role is required to be in a certain location and candidates do not have work authorization for that location, Samsara will conduct an immigration assessment. If the role is not required to be in a specific location, Samsara will move forward with the remote location that works best for the business. All offers of employment are contingent upon an individual's ability to secure and maintain the legal right to work at the company.
   
   
   Please note: Samsara does not accept agency resumes and is not responsible for any fees related to unsolicited resumes. Please do not forward resumes to Samsara employees.","<div>
 <div>
  <p><b>Who we are</b></p> 
  <p> Samsara (NYSE: IOT) is the pioneer of the Connected Operations&#x2122; Cloud, which is a platform that enables organizations that depend on physical operations to harness Internet of Things (IoT) data to develop actionable insights and improve their operations. At Samsara, we are helping improve the safety, efficiency and sustainability of the physical operations that power our global economy. Representing more than 40% of global GDP, these industries are the infrastructure of our planet, including agriculture, construction, field services, transportation, and manufacturing &#x2014; and we are excited to help digitally transform their operations at scale.</p> 
  <p> Working at Samsara means you&apos;ll help define the future of physical operations and be on a team that&apos;s shaping an exciting array of product solutions, including Video-Based Safety, Vehicle Telematics, Apps and Driver Workflows, Equipment Monitoring, and Site Visibility. As part of a recently public company, you&apos;ll have the autonomy and support to make an impact as we build for the long term.</p> 
  <p><b> Recent awards we&apos;ve won include:</b></p> 
  <p> Glassdoor&apos;s Highest-Rated Tech Companies for Culture and Values 2023</p> 
  <p> Great Place To Work Certified&#x2122; 2023</p> 
  <p> Best Place to Work by Built In 2023</p> 
  <p> Financial Times The Americas&apos; Fastest Growing Companies 2023</p> 
  <p> Deloitte Fast 500 Companies</p> 
  <p> We see a profound opportunity for data to improve the safety, efficiency, and sustainability of operations, and hope you consider joining us on this exciting journey.</p>
 </div>
 <p><b> About the team:</b></p> 
 <p> Data and Analytics is a critical team within Marketing. Our mission is to enable revenue performance by providing marketing and sales teams with the insights, tools, infrastructure and consultation to make data driven judgements. We are a scrappy and growing team that loves all things data! The team will be composed of data engineers, analytics managers and data scientists. We are passionate about leveraging world class data and analytics to deliver a great customer experience.</p> 
 <p> Our team promotes an agile, collaborative, supportive environment where diverse thinking, innovative design, and experimentation is welcomed and encouraged.</p> 
 <p><b> You should apply if:</b></p> 
 <ul> 
  <li><b>You want to impact the industries that run our world:</b> Your efforts will result in real-world impact&#x2014;helping to keep the lights on, get food into grocery stores, reduce emissions, and most importantly, ensure workers return home safely.</li> 
  <li><b>You want to build for scale:</b> With over 2.3 million IoT devices deployed to our global customers, you will work on a range of new and mature technologies driving scalable innovation for customers across industries driving the world&apos;s physical operations.</li> 
  <li><b>You are a team player:</b> Working on our partners requires a mix of independent effort and collaboration. Motivated by our mission, we&apos;re all racing toward our connected operations vision, and we intend to win&#x2014;together.</li> 
  <li><b>You are a life-long learner:</b> We have ambitious goals. Every Samsarian has a growth mindset as we work with a wide range of technologies, challenges, and customers that push us to learn on the go.</li> 
 </ul> 
 <p><i>Click here</i><i> to learn about what we value at Samsara.</i></p> 
 <p><b> In this role, you will:</b></p> 
 <ul> 
  <li>Develop and maintain marketing databases, datasets, pipelines and Samsara&apos;s Customer Data Platform (CDP) to enable advanced segmentation, targeting, automation and analytics.</li> 
  <li>Manage critical data pipelines to enable our growth initiatives and advanced analytics. Manage the SLAs for those data pipelines and constantly improve efficiency and data quality.</li> 
  <li>Facilitate data integration and transformation requirements for moving data between applications; ensuring interoperability of applications with data mart and CDP environments.</li> 
  <li>Develop and improve the current data architecture, data quality, monitoring and data availability.</li> 
  <li>Identify data needs from broad stakeholders, understand requirements for metrics and analysis, and build efficient and scalable data products to enable a data-driven marketing approach.</li> 
  <li>Write sophisticated yet optimized data transformations in SQL/Python to generate data products consumed by customer systems and Analytics, Marketing Operations, Sales Operations teams.</li> 
  <li>Champion, role model, and embed Samsara&apos;s cultural principles (Obsess Over the Customer, Build for the Long Term, Growth Mindset) as we scale globally and across new offices</li> 
 </ul> 
 <p><b>Minimum requirements for the role:</b></p> 
 <ul> 
  <li>5+ years of working experience in a growth, software or data engineering role</li> 
  <li>Excellent SQL and Python knowledge with strong hands-on data modeling</li> 
  <li>Experience with data warehouse technical architectures, infrastructure components, ETL/ ELT and reporting/analytic tools.</li> 
  <li>Hands-on experience working with modern data technologies stack such as Databricks, Google Big Query, Redshift, RDS, Snowflake or similar solutions</li> 
  <li>Experience with development lifecycle tools such as Github, TFS, etc.</li> 
  <li>Comfort in working with business customers to gather requirements and gain a deep understanding of varied datasets.</li> 
  <li>Familiarity with customer, marketing and/or web data</li> 
  <li>Experience integrating data from core Sales and Marketing platforms (e.g. Marketing Automation, CRM, and web analytics)</li> 
  <li>Self-starter, motivated, responsible, innovative and technology-driven candidate who performs well both unsupervised and as a team member</li> 
  <li>A proactive problem solver and have good communication as well as project management skills to relay your findings and solutions across technical and non technical audiences</li> 
 </ul> 
 <p><b>An ideal candidate also has:</b></p> 
 <ul> 
  <li>Knowledge of Marketo, Salesforce.com and Google Analytics</li> 
  <li>Experience working with CDPs such as Segment, Blueshift, Lytics or Adobe Real-time CDP</li> 
  <li>Experience with data visualization tools and packages (e.g. Looker, Domo, Tableau, MixPanel)</li> 
  <li>Familiarity with Marketing Technologies (MarTech stacks)</li> 
  <li>Experience coding with Scala, R or Pandas</li> 
  <li>Data Science, machine learning or predictive analytics experience</li> 
 </ul>
 <div>
  <div>
   <b>Samsara&#x2019;s Compensation Philosophy</b>: Samsara&#x2019;s compensation program is designed to deliver total compensation (based on role, level, and geography) that is above market. We do this through our base salary + bonus/variable + restricted stock unit awards (RSUs). A new hire RSU award is awarded at the time of hire, and additional RSU refresh grants may be awarded annually. 
   <p> We pay for performance, and top performers are eligible to receive above target equity refresh awards which allow employees to achieve higher market positioning.</p>
  </div>
  <p><b> The range of annual base salary for full-time employees for this position is below. Please note that base pay offered may vary depending on factors including your city of residence, job-related knowledge, skills, and experience.</b></p>
  <div>
    &#x24;122,400&#x2014;&#x24;180,000 USD
  </div>
 </div>
 <div>
  <p> At Samsara, we welcome everyone regardless of their background, race, national origin, gender, gender identity, sexual orientation, protected veteran status, disability, age, etc. We depend on the unique approaches of our team members to help us solve complex problems. We are committed to increasing diversity across our team and ensuring that Samsara is a place where people from all backgrounds can make an impact.</p> 
  <p><b> Accommodations</b></p> 
  <p> Samsara is an inclusive work environment, and we are committed to ensuring equal opportunity in employment for qualified persons with disabilities. Please email accessibleinterviewing@samsara.com or click here if you require any reasonable accommodations throughout the recruiting process.</p> 
  <p><b> Benefits</b></p> 
  <p> Full time employees receive an above market total compensation package along with employee-led remote and flexible working, health benefits, Samsara for Good charity fund, and much, much more. Take a look at our Benefits site to learn more.</p> 
  <p><b> Flexible Working</b></p> 
  <p> At Samsara, we have adopted a flexible way of working, enabling teams and individuals to do their best work, regardless of where they&apos;re based. We value in-person collaboration and know a change of scenery and quiet space to work is welcomed from time to time, but also appreciate that the world of work has changed. Our offices remain open for those who prefer to collaborate or work in-office, but we also encourage fully remote applicants. As most roles are not required to be in the office, we are able to hire remotely where Samsara has an established presence. If a role is required to be in a certain location and candidates do not have work authorization for that location, Samsara will conduct an immigration assessment. If the role is not required to be in a specific location, Samsara will move forward with the remote location that works best for the business. All offers of employment are contingent upon an individual&apos;s ability to secure and maintain the legal right to work at the company.</p>
  <br> 
  <p></p> 
  <p> Please note: Samsara does not accept agency resumes and is not responsible for any fees related to unsolicited resumes. Please do not forward resumes to Samsara employees.</p>
 </div>
</div>","https://boards.greenhouse.io/samsara/jobs/5423928?gh_jid=5423928&gh_src=2f2e775d1us","ea5fa4de8d1e5a10",,"Full-time",,"Remote","Senior Marketing Data Engineer","7 days ago","2023-10-18T11:54:14.721Z","3.5","20",,"2023-10-25T11:54:14.722Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=ea5fa4de8d1e5a10&from=jasx&tk=1hdjau6m4jgap800&vjs=3"
"Equitable","Data Engineer, Marketing Analytics (230000SU)
 
 
   Primary Location 
  : UNITED STATES-NC-Charlotte
 
 
   Organization 
  : Equitable
 
 
   Schedule 
  : Full-time
 
 
   Description
  
   At Equitable, our power is in our people.  We're individuals from different cultures and backgrounds. Those differences make us stronger as a team and a force for good in our communities. Here, you'll work with dynamic individuals, build your skills, and unleash new ways of working and thinking. Are you ready to join an organization that will help unlock your potential?
   
   
   We are seeking a highly motivated and collaborative Data Engineer to join our Marketing Analytics team. If you have a passion for building and optimizing data pipelines in the cloud, possess deep expertise in Databricks, SQL, Python, and PySpark, and excel at solving complex problems, we want to hear from you.
   
   
   As Data Engineer, you will play a pivotal role in designing, implementing, and optimizing our data infrastructure, pipelines, and integration. Your work will be central to our ability to analyze and leverage data for marketing insights. You will collaborate closely with cross-functional teams, including marketing, IT, and business units, to ensure our data solutions align with business needs. The ideal candidate has a strong track record of managing data pipelines, extensive experience in working with large datasets, and the ability to bridge the gap between technical and business requirements.
   
   
   If you are a collaborative problem solver with a proven background in data engineering, a deep understanding of data pipelines, and expertise in Databricks, SQL, Python, and PySpark, we encourage you to apply for this exciting opportunity. Join our team at Equitable and play a pivotal role in shaping our data-driven marketing initiatives.
   
   
   Key Job Responsibilities
   
   
  
   Design, build, and optimize data pipelines, architectures, and datasets to support marketing analytics and decision-making. 
   Develop and maintain code using Databricks, SQL, Python, and PySpark to ensure efficient data processing. 
   Manage marketing and sales data workflows using tools such as Marketing Cloud Intelligence (Datorama) and Salesforce Marketing Cloud and others. 
   Collaborate with cross-functional teams in a dynamic environment to understand and address their data needs. 
   Support and partner with the IT organization to implement data solutions in an enterprise fashion. 
   Utilize Cloudera and Azure DevOps to enhance data infrastructure and manage data workflows. 
   Proactively identify opportunities to improve data quality, reliability, and accessibility. 
   Troubleshoot and resolve data-related issues to minimize disruptions to analytics processes. 
   Employ strong project management and organizational skills to deliver on time and within scope. 
   Mentor junior team members and share best practices in data engineering. 
   
  The base salary range for this position is $67,000-$90,000. Actual base salaries vary based on skills, experience, and geographical location. In addition to base pay, Equitable provides compensation to reward performance with base salary increases, spot bonuses, and short-term incentive compensation opportunities. Eligibility for these programs depends on level and functional area of responsibility. 
   For eligible employees, Equitable provides a full range of benefits. This includes medical, dental, vision, a 401(k) plan, and paid time off. For detailed descriptions of these benefits, please reference the link below. 
   Equitable Pay and Benefits: Equitable Total Rewards Program 
   
  
 
 
  Qualifications
   
   
   
  Required Qualifications
   
   
   
   Bachelor's degree in Computer Science, Data Engineering, or a STEM related field; advanced degree is a plus. 
   3+ years of of working in a Data Engineer role, with experience in building and optimizing data models, data pipelines, and data integration. 
   2+ years expertise in SQL and Python for data processing and analysis. 
   2+ years expertise in Databricks and PySpark for developing cloud data pipelines. 
   1+ years experience working with Azure Cloud and Azure DevOps.
  
   
   
   Preferred Qualifications
   
   
   
   Hands on work experience with Datorama and Salesforce Marketing Cloud. 
   Experience working with Rest API’s and SFTP to access, ingest, transform first party data and third party data. 
   Strong project management and organizational skills to manage complex data projects. 
   Experience working with large datasets and a deep understanding of enterprise data management principles. 
   Ability to collaborate effectively with cross-functional teams in a fast-paced environment. 
   Experience working with Change Management and CI/CD 
   Exceptional problem-solving skills and attention to detail.
  
   
   
   Skills
   
   
   Business Data Analysis: Knowledge of business data analysis; ability to collect, identify, analyze and interpret business data using various kinds of techniques to meet business needs and requirements.
   
   
   Cross-functional Collaboration: Knowledge of collaborative techniques and approaches; ability to promote a culture of continuous improvement and working together across functions to solve business problems and meet business goals.
   
   
   Data Analysis Tools: Knowledge of key uses and benefits of data analysis tools; ability to utilize data analysis tools to identify factors influencing business performance and to gain greater insight into trends within a business, industry and customer base.
   
   
   Data Gathering and Analysis: Knowledge of data gathering and analysis tools, techniques and processes; ability to gather and analyze data on the learning needs of a target population.
   
   
   Industry Knowledge: Knowledge of the organization's industry group, trends, directions, major issues, regulatory considerations, and trendsetters; ability to apply industry knowledge appropriately to diverse situations.
   
   
   Market Research: Knowledge of market research; ability to collect, collate and analyze information about existing or potential markets and market needs.
   
   
   Storytelling: Knowledge of concepts and ability to plan, create and present business proposals, initiatives and ideas by storytelling actual business scenarios that are situation-specific, engaging, memorable and persuasive as compared to one-way, fact-based presentations.
   
   
   Diversity, Equity and Inclusion: Demonstrates a commitment to Diversity, Equity and Inclusion by treating everyone with respect and dignity, ensuring all voices are heard and advocating for change.
   
   
   ABOUT EQUITABLE  At Equitable, we’re a team of over ten thousand strong; committed to helping our clients secure their financial well-being so that they can pursue long and fulfilling lives.  We turn challenges into opportunities by thinking, working, and leading differently – where everyone is a leader. We encourage every employee to leverage their unique talents to become a force for good at Equitable and in their local communities.  We are continuously investing in our people by offering growth, internal mobility, comprehensive compensation and benefits to support overall well-being, flexibility, and a culture of collaboration and teamwork.  We are looking for talented, dedicated, purposeful people who want to make an impact. Join Equitable and pursue a career with purpose.  **********  Equitable is committed to providing equal employment opportunities to our employees, applicants and candidates based on individual qualifications, without regard to race, color, religion, gender, gender identity and expression, age, national origin, mental or physical disabilities, sexual orientation, veteran status, genetic information or any other class protected by federal, state and local laws.  NOTE: Equitable participates in the E-Verify program.  If reasonable accommodation is needed to participate in the job application or interview process or to perform the essential job functions of this position, please contact Human Resources at (212) 314-2211 or email us at TalentAcquisition@equitable.com. 
   #LI-Remote","<div>
 <div>
  <p><b>Data Engineer, Marketing Analytics</b></p> (230000SU)
 </div>
 <div>
  <b> Primary Location</b> 
  <b>:</b> UNITED STATES-NC-Charlotte
 </div>
 <div>
  <b> Organization</b> 
  <b>:</b> Equitable
 </div>
 <div>
  <b> Schedule</b> 
  <b>:</b> Full-time
 </div>
 <div>
  <b> Description</b>
  <div></div>
  <p><br> At Equitable, our power is in our people.<br> <br> We&apos;re individuals from different cultures and backgrounds. Those differences make us stronger as a team and a force for good in our communities. Here, you&apos;ll work with dynamic individuals, build your skills, and unleash new ways of working and thinking. Are you ready to join an organization that will help unlock your potential?</p>
  <br> 
  <p></p> 
  <p> We are seeking a highly motivated and collaborative Data Engineer to join our Marketing Analytics team. If you have a passion for building and optimizing data pipelines in the cloud, possess deep expertise in Databricks, SQL, Python, and PySpark, and excel at solving complex problems, we want to hear from you.</p>
  <br> 
  <p></p> 
  <p> As Data Engineer, you will play a pivotal role in designing, implementing, and optimizing our data infrastructure, pipelines, and integration. Your work will be central to our ability to analyze and leverage data for marketing insights. You will collaborate closely with cross-functional teams, including marketing, IT, and business units, to ensure our data solutions align with business needs. The ideal candidate has a strong track record of managing data pipelines, extensive experience in working with large datasets, and the ability to bridge the gap between technical and business requirements.</p>
  <br> 
  <p></p> 
  <p> If you are a collaborative problem solver with a proven background in data engineering, a deep understanding of data pipelines, and expertise in Databricks, SQL, Python, and PySpark, we encourage you to apply for this exciting opportunity. Join our team at Equitable and play a pivotal role in shaping our data-driven marketing initiatives.</p>
  <br> 
  <p></p> 
  <p><b> Key Job Responsibilities</b></p>
  <br> 
  <p><b> </b></p>
  <ul>
   <li>Design, build, and optimize data pipelines, architectures, and datasets to support marketing analytics and decision-making.</li> 
   <li>Develop and maintain code using Databricks, SQL, Python, and PySpark to ensure efficient data processing.</li> 
   <li>Manage marketing and sales data workflows using tools such as Marketing Cloud Intelligence (Datorama) and Salesforce Marketing Cloud and others.</li> 
   <li>Collaborate with cross-functional teams in a dynamic environment to understand and address their data needs.</li> 
   <li>Support and partner with the IT organization to implement data solutions in an enterprise fashion.</li> 
   <li>Utilize Cloudera and Azure DevOps to enhance data infrastructure and manage data workflows.</li> 
   <li>Proactively identify opportunities to improve data quality, reliability, and accessibility.</li> 
   <li>Troubleshoot and resolve data-related issues to minimize disruptions to analytics processes.</li> 
   <li>Employ strong project management and organizational skills to deliver on time and within scope.</li> 
   <li>Mentor junior team members and share best practices in data engineering.</li> 
  </ul> 
  <p>The base salary range for this position is &#x24;67,000-&#x24;90,000. Actual base salaries vary based on skills, experience, and geographical location. In addition to base pay, Equitable provides compensation to reward performance with base salary increases, spot bonuses, and short-term incentive compensation opportunities. Eligibility for these programs depends on level and functional area of responsibility.</p> 
  <p> For eligible employees, Equitable provides a full range of benefits. This includes medical, dental, vision, a 401(k) plan, and paid time off. For detailed descriptions of these benefits, please reference the link below.</p> 
  <p><b> Equitable Pay and Benefits</b>: Equitable Total Rewards Program<br> </p>
  <div> 
  </div>
 </div>
 <div>
  <b>Qualifications</b>
  <br> 
  <p></p> 
  <p></p> 
  <p><b>Required Qualifications</b></p>
  <br> 
  <p></p> 
  <ul> 
   <li>Bachelor&apos;s degree in Computer Science, Data Engineering, or a STEM related field; advanced degree is a plus.</li> 
   <li>3+ years of of working in a Data Engineer role, with experience in building and optimizing data models, data pipelines, and data integration.</li> 
   <li>2+ years expertise in SQL and Python for data processing and analysis.</li> 
   <li>2+ years expertise in Databricks and PySpark for developing cloud data pipelines.</li> 
   <li>1+ years experience working with Azure Cloud and Azure DevOps.</li>
  </ul>
  <br> 
  <p></p> 
  <p><b> Preferred Qualifications</b></p>
  <br> 
  <p></p> 
  <ul> 
   <li>Hands on work experience with Datorama and Salesforce Marketing Cloud.</li> 
   <li>Experience working with Rest API&#x2019;s and SFTP to access, ingest, transform first party data and third party data.</li> 
   <li>Strong project management and organizational skills to manage complex data projects.</li> 
   <li>Experience working with large datasets and a deep understanding of enterprise data management principles.</li> 
   <li>Ability to collaborate effectively with cross-functional teams in a fast-paced environment.</li> 
   <li>Experience working with Change Management and CI/CD</li> 
   <li>Exceptional problem-solving skills and attention to detail.</li>
  </ul>
  <br> 
  <p></p> 
  <p><b> Skills</b></p>
  <br> 
  <p></p> 
  <p><b> Business Data Analysis:</b> Knowledge of business data analysis; ability to collect, identify, analyze and interpret business data using various kinds of techniques to meet business needs and requirements.</p>
  <br> 
  <p></p> 
  <p><b> Cross-functional Collaboration:</b> Knowledge of collaborative techniques and approaches; ability to promote a culture of continuous improvement and working together across functions to solve business problems and meet business goals.</p>
  <br> 
  <p></p> 
  <p><b> Data Analysis Tools:</b> Knowledge of key uses and benefits of data analysis tools; ability to utilize data analysis tools to identify factors influencing business performance and to gain greater insight into trends within a business, industry and customer base.</p>
  <br> 
  <p></p> 
  <p><b> Data Gathering and Analysis:</b> Knowledge of data gathering and analysis tools, techniques and processes; ability to gather and analyze data on the learning needs of a target population.</p>
  <br> 
  <p></p> 
  <p><b> Industry Knowledge:</b> Knowledge of the organization&apos;s industry group, trends, directions, major issues, regulatory considerations, and trendsetters; ability to apply industry knowledge appropriately to diverse situations.</p>
  <br> 
  <p></p> 
  <p><b> Market Research:</b> Knowledge of market research; ability to collect, collate and analyze information about existing or potential markets and market needs.</p>
  <br> 
  <p></p> 
  <p><b> Storytelling:</b> Knowledge of concepts and ability to plan, create and present business proposals, initiatives and ideas by storytelling actual business scenarios that are situation-specific, engaging, memorable and persuasive as compared to one-way, fact-based presentations.</p>
  <br> 
  <p></p> 
  <p><b> Diversity, Equity and Inclusion:</b> Demonstrates a commitment to Diversity, Equity and Inclusion by treating everyone with respect and dignity, ensuring all voices are heard and advocating for change.</p>
  <br> 
  <p></p> 
  <p><b> ABOUT EQUITABLE</b><br> <br> At Equitable, we&#x2019;re a team of over ten thousand strong; committed to helping our clients secure their financial well-being so that they can pursue long and fulfilling lives.<br> <br> We turn challenges into opportunities by thinking, working, and leading differently &#x2013; where everyone is a leader. We encourage every employee to leverage their unique talents to become a force for good at Equitable and in their local communities.<br> <br> We are continuously investing in our people by offering growth, internal mobility, comprehensive compensation and benefits to support overall well-being, flexibility, and a culture of collaboration and teamwork.<br> <br> We are looking for talented, dedicated, purposeful people who want to make an impact. Join Equitable and pursue a career with purpose.<br> <br> **********<br> <br> Equitable is committed to providing equal employment opportunities to our employees, applicants and candidates based on individual qualifications, without regard to race, color, religion, gender, gender identity and expression, age, national origin, mental or physical disabilities, sexual orientation, veteran status, genetic information or any other class protected by federal, state and local laws.<br> <br> NOTE: Equitable participates in the E-Verify program.<br> <br> If reasonable accommodation is needed to participate in the job application or interview process or to perform the essential job functions of this position, please contact Human Resources at (212) 314-2211 or email us at TalentAcquisition@equitable.com.</p> 
  <p> #LI-Remote</p>
 </div>
</div>
<div></div>","https://www.indeed.com/rc/clk?jk=2a6492ae7ded8c78&atk=&xpse=SoAh67I3JzdzdwwSK50LbzkdCdPP","2a6492ae7ded8c78",,"Full-time",,"Charlotte, NC","Data Engineer, Marketing Analytics","21 days ago","2023-10-04T11:54:23.425Z","3.4","562","$67,000 - $90,000 a year","2023-10-25T11:54:23.519Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=2a6492ae7ded8c78&from=jasx&tk=1hdjautnrj4ij800&vjs=3"
"Mission Cloud","As a Data Engineer, Planning & Analysis, you will report to the Vice President, Planning & Analysis. You will build and maintain Mission Cloud’s internal data lake and its data connections to Mission Cloud’s Business Intelligence Platform. You will collaborate across multiple departments to understand data sources and reporting needs, and business problems. You will use your technical expertise to automate and simplify reporting and data analysis requirements for teams at Mission Cloud.
 
 
 
   This position is 100% remote.
  
 Responsibilities 
 
  Build and maintain Mission Cloud’s internal data lake
   Establish connections from the data lake to business intelligence tools, including Amazon Quicksight
   Configure AWS services to support ETL and data warehousing
   Build and maintain the process for syncing the data lake with Amazon Quicksight
   Create operational efficiencies and improve data governance by establishing a singular data access point and source of truth for Mission Cloud
   Build repeatable, efficient processes to help teams at Mission Cloud become self-sufficient and able to accurately measure, monitor, and forecast their key performance indicators (KPIs)
   Creation of data visualizations and dashboards that effectively communicate key business insights and tell a compelling story
   Exercises judgment in selecting methods, evaluating, adapting of complex techniques and evaluation criteria for obtaining results pertaining to data warehousing.
 
  Requirements
 
   Ability to work in a business intelligence / data engineering role, preferably in the technology industry
   Design & implementation experience with distributed applications
   Exhibit advanced wide-ranging experience, using in-depth professional knowledge of database architectures and data pipeline development 
  Ability to handle unstructured, and semi-structured data, working in a data lake environment
   Experience building data lakes in Redshift
   Ability to translate data into insightful dashboards in Amazon Quicksight or a similar tool
   Expertise in designing and implementing AWS-based solutions, including services like Amazon Redshift, AWS Glue, Lambda, AWS Athena, AWS QuickSight
   Demonstrated proficiency in data analytics, data transformation (ETL/ELT), data integration, data warehousing, data lake architecture, and relevant AWS services.
   Experience working with extracting and ingesting data from various data sources using AWS Glue, AWS AppFlow, Lambda, Step Functions, Event Bridge
   Experience working with technologies like Amazon CDK, Cloud Formation, Terraform or equivalent
   Experience working with AWS Cloud Watch, Cloud Trail, Secrets Manager, KMS
   Ability to work across departments and manage multiple stakeholder needs and requests at one time
   Ability to manage and execute overlapping projects of varying durations and complexities
   Knowledge of Finance, Mathematics, Statistics or related field
   AWS Certification (required within 6 months of hire)
 
  Benefits
 
   Access to health, vision and dental insurance with options 100% covered by Mission Cloud for employee and their dependents
   Flexible Spending Accounts (Healthcare & Dependent Care)
   Generous Paid Time Off (FlexPTO, parental leave, volunteering time off)
   Reproductive health benefits
   Pet insurance
   401k matching program
   Life insurance paid by Mission Cloud
   Monthly flex stipend
   Monthly cell phone stipend
   Home office expense benefit
   An internal department dedicated to helping team members on their career path
   Inclusive work environment with several Employee Resource Groups
 
 
   Placement within the range is determined by a variety of factors, including but not limited to knowledge, skills, and ability as evaluated during the interview process. Range: $110000-$139,435
 
 
   Commitment to Diversity and Inclusion
 
 
 
   We are committed to diversity and inclusion. We value every individual’s unique story, experience, and perspective. We aim to amplify the voices of our team members and our community to create a safe, empathetic, and inclusive environment where everyone can contribute to one’s authentic self. Mission Cloud makes every effort to ensure that all employees are compensated fairly regardless of gender, ethnicity, race, or past salary history. We understand that fair compensation practices establish that diversity, fair hiring processes, and fair pay are part of who we are as a company and maintain positive employee morale. We use market data to define salary ranges for each role and regularly review compensation adjustments as needed based on salary range updates.
 
 
 
   Mission Cloud is an Equal Opportunity Employer and participant in the U.S. Federal E-Verify program. Mission Cloud will consider qualified applicants with criminal histories in a manner consistent with The Los Angeles Fair Chance Initiative for Hiring Ordinance.
 
 
 
   About Mission Cloud
 
 
 
   Mission Cloud is an Amazon Web Services (AWS) Premier Consulting Partner and MSP. Clients depend on us to expertly and securely architect, migrate, manage, and optimize their cloud environments.
 
 
   Mission Cloud’s team of AWS Certified Solutions Architects and DevOps Engineers are ready to help you harness the full power of the AWS cloud to transform your business and operations.","<div>
 <div>
  As a Data Engineer, Planning &amp; Analysis, you will report to the Vice President, Planning &amp; Analysis. You will build and maintain Mission Cloud&#x2019;s internal data lake and its data connections to Mission Cloud&#x2019;s Business Intelligence Platform. You will collaborate across multiple departments to understand data sources and reporting needs, and business problems. You will use your technical expertise to automate and simplify reporting and data analysis requirements for teams at Mission Cloud.
 </div>
 <div></div>
 <div>
  <br> This position is 100% remote.
 </div> 
 <h3 class=""jobSectionHeader""><b>Responsibilities </b></h3>
 <ul>
  <li>Build and maintain Mission Cloud&#x2019;s internal data lake</li>
  <li> Establish connections from the data lake to business intelligence tools, including Amazon Quicksight</li>
  <li> Configure AWS services to support ETL and data warehousing</li>
  <li> Build and maintain the process for syncing the data lake with Amazon Quicksight</li>
  <li> Create operational efficiencies and improve data governance by establishing a singular data access point and source of truth for Mission Cloud</li>
  <li> Build repeatable, efficient processes to help teams at Mission Cloud become self-sufficient and able to accurately measure, monitor, and forecast their key performance indicators (KPIs)</li>
  <li> Creation of data visualizations and dashboards that effectively communicate key business insights and tell a compelling story</li>
  <li> Exercises judgment in selecting methods, evaluating, adapting of complex techniques and evaluation criteria for obtaining results pertaining to data warehousing.</li>
 </ul>
 <h3 class=""jobSectionHeader""><b> Requirements</b></h3>
 <ul>
  <li> Ability to work in a business intelligence / data engineering role, preferably in the technology industry</li>
  <li> Design &amp; implementation experience with distributed applications</li>
  <li> Exhibit advanced wide-ranging experience, using in-depth professional knowledge of database architectures and data pipeline development </li>
  <li>Ability to handle unstructured, and semi-structured data, working in a data lake environment</li>
  <li> Experience building data lakes in Redshift</li>
  <li> Ability to translate data into insightful dashboards in Amazon Quicksight or a similar tool</li>
  <li> Expertise in designing and implementing AWS-based solutions, including services like Amazon Redshift, AWS Glue, Lambda, AWS Athena, AWS QuickSight</li>
  <li> Demonstrated proficiency in data analytics, data transformation (ETL/ELT), data integration, data warehousing, data lake architecture, and relevant AWS services.</li>
  <li> Experience working with extracting and ingesting data from various data sources using AWS Glue, AWS AppFlow, Lambda, Step Functions, Event Bridge</li>
  <li> Experience working with technologies like Amazon CDK, Cloud Formation, Terraform or equivalent</li>
  <li> Experience working with AWS Cloud Watch, Cloud Trail, Secrets Manager, KMS</li>
  <li> Ability to work across departments and manage multiple stakeholder needs and requests at one time</li>
  <li> Ability to manage and execute overlapping projects of varying durations and complexities</li>
  <li> Knowledge of Finance, Mathematics, Statistics or related field</li>
  <li> AWS Certification (required within 6 months of hire)</li>
 </ul>
 <h3 class=""jobSectionHeader""><b> Benefits</b></h3>
 <ul>
  <li> Access to health, vision and dental insurance with options 100% covered by Mission Cloud for employee and their dependents</li>
  <li> Flexible Spending Accounts (Healthcare &amp; Dependent Care)</li>
  <li> Generous Paid Time Off (FlexPTO, parental leave, volunteering time off)</li>
  <li> Reproductive health benefits</li>
  <li> Pet insurance</li>
  <li> 401k matching program</li>
  <li> Life insurance paid by Mission Cloud</li>
  <li> Monthly flex stipend</li>
  <li> Monthly cell phone stipend</li>
  <li> Home office expense benefit</li>
  <li> An internal department dedicated to helping team members on their career path</li>
  <li> Inclusive work environment with several Employee Resource Groups</li>
 </ul>
 <div>
   Placement within the range is determined by a variety of factors, including but not limited to knowledge, skills, and ability as evaluated during the interview process. Range: &#x24;110000-&#x24;139,435
 </div>
 <div>
  <b> Commitment to Diversity and Inclusion</b>
 </div>
 <div></div>
 <div>
  <br> We are committed to diversity and inclusion. We value every individual&#x2019;s unique story, experience, and perspective. We aim to amplify the voices of our team members and our community to create a safe, empathetic, and inclusive environment where everyone can contribute to one&#x2019;s authentic self. Mission Cloud makes every effort to ensure that all employees are compensated fairly regardless of gender, ethnicity, race, or past salary history. We understand that fair compensation practices establish that diversity, fair hiring processes, and fair pay are part of who we are as a company and maintain positive employee morale. We use market data to define salary ranges for each role and regularly review compensation adjustments as needed based on salary range updates.
 </div>
 <div></div>
 <div>
  <br> Mission Cloud is an Equal Opportunity Employer and participant in the U.S. Federal E-Verify program. Mission Cloud will consider qualified applicants with criminal histories in a manner consistent with The Los Angeles Fair Chance Initiative for Hiring Ordinance.
 </div>
 <div></div>
 <div>
  <b><br> About Mission Cloud</b>
 </div>
 <div></div>
 <div>
  <br> Mission Cloud is an Amazon Web Services (AWS) Premier Consulting Partner and MSP. Clients depend on us to expertly and securely architect, migrate, manage, and optimize their cloud environments.
 </div>
 <div>
   Mission Cloud&#x2019;s team of AWS Certified Solutions Architects and DevOps Engineers are ready to help you harness the full power of the AWS cloud to transform your business and operations.
 </div>
</div>","https://jobs.lever.co/missioncloud/5d3925fe-0239-4f4e-b728-0f20b27f004f?lever-source=Indeed","60ee2be8d11b8703",,"Full-time",,"Remote","Senior Data Engineer, Planning & Analysis","25 days ago","2023-09-30T11:54:25.301Z","4","2","$130,000 - $156,958 a year","2023-10-25T11:54:25.302Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=60ee2be8d11b8703&from=jasx&tk=1hdjautnrj4ij800&vjs=3"
"Johnson & Johnson","Biosense Webster Inc., part of Johnson & Johnson MedTech, is currently recruiting for a Clinical Data Engineer . This role can be remote from anywhere within the United States. At Biosense Webster, Inc. we have one goal — to ensure those with cardiac arrhythmias can live the lives they want. This means transforming the latest advancements in electrophysiology into a suite of tools that empowers physicians with a range of treatments for the best outcomes. 
     
     Quality products and approaches are achievable only through collaboration with the smartest minds in electrophysiology. For more than 30 years, we’ve been the global market leader in the science and technology of cardiac arrhythmia treatment, working with thousands of electrophysiologists to identify and develop diagnostic and treatment tools. And through onsite training, online courses and our global education centers, we work together to set new standards every day. 
     Learn more about Biosense Webster at www.biosensewebster.com 
     The Data Engineer is a key player in a cross functional team providing subject matter expertise for enhancing and troubleshooting complex Data platform business needs. The ideal candidate is an experienced data professional that exhibits strong skillset within redarning of the data via Tableau or other visualization packages. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing data architecture to support the next generation of products and data initiatives. 
     
     Key Responsibilities: 
     
      Maintain support and be the primary contact for managing, triaging, and resolving issues related to Clinical data managed outside of the Electronic Data Capture System (EDC). 
     
     
      Experienced in the utilization of AWS services and infrastructure to support large and complex data platforms. 
     
     
      Hands-on database administrator skills to manage or identify root cause and resolve issues stemming from the management of structured and/or unstructured data sources. 
     
     
     The base pay range for this position is $90,000 to $140,000 based on experience . The Company maintains highly competitive, performance-based compensation programs. Under current guidelines, this position is eligible for an annual performance bonus. The annual performance bonus is a cash bonus intended to provide an incentive to achieve annual targeted results by rewarding for individual and the corporation’s performance over a calendar/performance year. Bonuses are awarded at the Company’s discretion on an individual basis.
      
     
     Employees may be eligible to participate in Company employee benefit programs such as health insurance, savings plan, pension plan, disability plan, vacation pay, sick time, holiday pay, and work, personal and family time off in accordance with the terms of the applicable plans. Additional information can be found through the link below.
      
     
     https://www.careers.jnj.com/employee-benefits 
     QUALIFICATIONS 
     Required Qualifications: 
     
     Education: 
     
      Minimum of a Bachelors’ Degree required ; Advanced Degree preferred . Desired fields of study include Computer Science, Statistics, Informatics, Information Systems or related quantitative field. 
     
     
     Experience and Skills: 
     
      Minimum 2-4+ years of experience in a Data role, with exposure to the following software/tools: 
     
     o Big data tools: AWS Services (S3, GLUE, Postgres, Lambda and Sagemaker etc.) 
     o Relational SQL and NoSQL databases 
     o Analysis and visualization software such as R, RStudio, Python, and Tableau 
     o Data pipeline and workflow management tools 
     
      Advanced SQL knowledge and experience with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases. 
     
     
      Experience with contributing to the building and optimizing data pipelines, architectures, and data sets. 
     
     
      A working knowledge of manipulating, processing, and extracting value from large datasets. 
     
     
      Experience supporting and working with cross-functional teams in a dynamic environment. 
     
     
      Experience in medical device or pharmaceutical environment preferred. 
     
     
      Excellent written, oral and presentation skills. 
     
     
     Johnson & Johnson is an Affirmative Action and Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, age, national origin, or protected veteran status and will not be discriminated against on the basis of disability.","<div>
 <div>
  <div>
   <div>
    <div>
     <p>Biosense Webster Inc., part of Johnson &amp; Johnson MedTech, is currently recruiting for a <b>Clinical Data Engineer </b>. This role can be remote from anywhere within the United States. At Biosense Webster, Inc. we have one goal &#x2014; to ensure those with cardiac arrhythmias can live the lives they want. This means transforming the latest advancements in electrophysiology into a suite of tools that empowers physicians with a range of treatments for the best outcomes. </p>
     <p></p>
     <p>Quality products and approaches are achievable only through collaboration with the smartest minds in electrophysiology. For more than 30 years, we&#x2019;ve been the global market leader in the science and technology of cardiac arrhythmia treatment, working with thousands of electrophysiologists to identify and develop diagnostic and treatment tools. And through onsite training, online courses and our global education centers, we work together to set new standards every day. </p>
     <p>Learn more about Biosense Webster at www.biosensewebster.com </p>
     <p>The Data Engineer is a key player in a cross functional team providing subject matter expertise for enhancing and troubleshooting complex Data platform business needs. The ideal candidate is an experienced data professional that exhibits strong skillset within redarning of the data via Tableau or other visualization packages. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing data architecture to support the next generation of products and data initiatives. </p>
     <p></p>
     <p><b>Key Responsibilities: </b></p>
     <ul>
      <li>Maintain support and be the primary contact for managing, triaging, and resolving issues related to Clinical data managed outside of the Electronic Data Capture System (EDC). </li>
     </ul>
     <ul>
      <li>Experienced in the utilization of AWS services and infrastructure to support large and complex data platforms. </li>
     </ul>
     <ul>
      <li>Hands-on database administrator skills to manage or identify root cause and resolve issues stemming from the management of structured and/or unstructured data sources. </li>
     </ul>
     <p></p>
     <p>The base pay range for this position is &#x24;90,000 to &#x24;140,000 <b>based on experience </b>. The Company maintains highly competitive, performance-based compensation programs. Under current guidelines, this position is eligible for an annual performance bonus. The annual performance bonus is a cash bonus intended to provide an incentive to achieve annual targeted results by rewarding for individual and the corporation&#x2019;s performance over a calendar/performance year. Bonuses are awarded at the Company&#x2019;s discretion on an individual basis.</p>
     <br> 
     <p></p>
     <p>Employees may be eligible to participate in Company employee benefit programs such as health insurance, savings plan, pension plan, disability plan, vacation pay, sick time, holiday pay, and work, personal and family time off in accordance with the terms of the applicable plans. Additional information can be found through the link below.</p>
     <br> 
     <p></p>
     <p>https://www.careers.jnj.com/employee-benefits </p>
     <h2 class=""jobSectionHeader""><b>QUALIFICATIONS</b></h2> 
     <p><b>Required Qualifications: </b></p>
     <p></p>
     <p><b>Education: </b></p>
     <ul>
      <li>Minimum of a Bachelors&#x2019; Degree <b>required </b>; Advanced Degree <i>preferred </i>. Desired fields of study include Computer Science, Statistics, Informatics, Information Systems or related quantitative field. </li>
     </ul>
     <p></p>
     <p><b>Experience and Skills: </b></p>
     <ul>
      <li>Minimum 2-4+ years of experience in a Data role, with exposure to the following software/tools: </li>
     </ul>
     <p>o Big data tools: AWS Services (S3, GLUE, Postgres, Lambda and Sagemaker etc.) </p>
     <p>o Relational SQL and NoSQL databases </p>
     <p>o Analysis and visualization software such as R, RStudio, Python, and <b>Tableau </b></p>
     <p>o Data pipeline and workflow management tools </p>
     <ul>
      <li>Advanced SQL knowledge and experience with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases. </li>
     </ul>
     <ul>
      <li>Experience with contributing to the building and optimizing data pipelines, architectures, and data sets. </li>
     </ul>
     <ul>
      <li>A working knowledge of manipulating, processing, and extracting value from large datasets. </li>
     </ul>
     <ul>
      <li>Experience supporting and working with cross-functional teams in a dynamic environment. </li>
     </ul>
     <ul>
      <li>Experience in medical device or pharmaceutical environment <i>preferred. </i></li>
     </ul>
     <ul>
      <li>Excellent written, oral and presentation skills. </li>
     </ul>
     <p></p>
     <p>Johnson &amp; Johnson is an Affirmative Action and Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, age, national origin, or protected veteran status and will not be discriminated against on the basis of disability.</p>
    </div>
   </div>
  </div>
 </div>
</div>","https://www.indeed.com/rc/clk?jk=05cfb82e458953ee&atk=&xpse=SoD767I3Jzdyb0yddR0LbzkdCdPP","05cfb82e458953ee",,,,"33 Technology Drive, Irvine, CA 92618","Clinical Data Engineer - Biosense Webster, Inc.","27 days ago","2023-09-28T11:54:32.145Z","4.2","7877","$90,000 - $140,000 a year","2023-10-25T11:54:32.147Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=05cfb82e458953ee&from=jasx&tk=1hdjautnrj4ij800&vjs=3"
"OCHIN","Description: 
  Make a difference at OCHIN
  OCHIN provides leading-edge technology, data analytics, research, and support services to nearly 1,000 community health care sites, reaching nearly 6 million patients nationally. We believe that every individual, no matter their race, ethnicity, background, or zip code, should have fair opportunity to achieve their full health potential. Our work addresses differences in health that are systemic, avoidable, and unjust. We partner, learn, innovate, and advocate, in order to close the gap in health for individuals and communities negatively impacted by racism or other structural inequities.
  At OCHIN, we value the unique perspectives and experiences of every individual and work hard to maintain a culture of belonging.
  Founded in Oregon in 2000, OCHIN employs a growing virtual workforce of more than 1,000 diverse professionals, working remotely across 48 states. We offer a generous compensation package and are committed to supporting our employees’ entire well-being by fostering a healthy work-life balance and equitable opportunity for professional advancement. We are curious, collaborative learners who strive to live our values everyday: learning, heart, belonging and impact. OCHIN is excited to support our continued national expansion and the increasing demand for our innovative tools and services by welcoming new talent to our growing team.
  Position Overview
  The Research Data Engineer will provide high-level professional and technical skills in support of designing, building, and maintaining data pipelines, databases, and cloud platforms to support the needs of the OCHIN Research team.
  In this role, you will be collaborating with an innovative, collaborative team of people moving exciting projects forward and working to improve systems and processes along the way.
 
  Essential Duties
 
   Performing day to day management of on-premises, cloud, and hybrid research databases and database platforms including the Research Data Warehouse
   Integrating and transforming health-related data from a variety of sources and formats such as EHRs, geospatial, claims, and census into analyzable formats for research
   Building and maintaining datasets and data marts
   Monitor and maintain data pipelines proactively to ensure high service availability
   In partnership with Research Data Science staff and leadership, assist with scoping and designing new research data pipelines and platforms to optimize research data solutions
   Create scripts and programs to automate data operations
   Preparing and maintaining technical documentation and metadata
   Providing technical/consultative services to internal and external research partners, investigators, and other research personnel
   Performing other duties as requested by the research team
  Requirements: 
 
  A Master’s level degree in Informatics, Computer Science or related discipline. Equivalent knowledge and skills obtained through a combination of education, training, and experience may meet this requirement.
   At least 5 years of experience in database development and administration in a healthcare and/or health research setting
   At least 3 years’ experience with data warehousing, including ETL techniques
   Strong technical proficiency with SQL required
   High technical proficiency with Microsoft SQL Server, including the ability to create and edit complex queries and T-SQL scripts including dynamic SQL, required; experience with SSIS
   Strong working knowledge of standard desktop computing software packages (word processing, spreadsheets, presentation software, Internet browsers, etc.).
   Strong analytical and problem-solving skills
   Experience with cloud and/or hybrid cloud/on-premises database architectures preferred
   Knowledge of specialized and complex statistical modeling and/or machine learning techniques preferred
 
  Base Pay Overview
  The typical offer range for this role is minimum to midpoint, ($98,819 - $128,465) with the midpoint representing the average pay in a national market scope for this position. Please keep in mind that this range represents the pay range for all positions in the job grade within which this position falls. The actual salary offer will consider a wide range of factors directly relevant to this position, including, but not limited to, skills, knowledge, training, responsibility, and experience, as well as internal equity and alignment with market data.
  Work Location and Travel Requirements
  OCHIN is a 100% remote organization with no physical corporate office location. Employees work remotely from home and many of our positions also support our member organizations on-site for new software installations. Nationwide travel is determined based on OCHIN business needs. Please inquire during the interview process about travel requirements for this position.
 
   Ability to work independently and efficiently from a home office environment
   High Speed Internet Service
   It is a requirement that employees work in a distraction free workplace
 
  We offer a comprehensive range of benefits. See our website for details: https://ochin.org/employment-openings
  Equal Opportunity Statement
  OCHIN is proud to be an equal opportunity employer. We are committed to building a team that represents a variety of backgrounds, perspectives, and skills for the benefit of our staff, our mission, and the communities we serve.
  As an Equal Opportunity and Affirmative Action employer, OCHIN, Inc. does not discriminate on the basis of race, ethnicity, sex, gender identity, sexual orientation, religion, marital or civil union status, age, disability status, veteran status, or any other protected characteristics. All aspects of employment are based on merit, performance, and business needs.
  COVID-19 Vaccination Requirement
  To keep our colleagues, members, and communities safe, OCHIN requires all employees—including remote employees, contractors, interns, and new hires—to be vaccinated with a COVID-19 vaccine, as supported by state and federal public health officials, as a condition of employment. All new hires are required to provide proof of full vaccination or receive approval for a medical or religious exemption before their hire date.","<div>
 Description: 
 <p><b> Make a difference at OCHIN</b></p>
 <p> OCHIN provides leading-edge technology, data analytics, research, and support services to nearly 1,000 community health care sites, reaching nearly 6 million patients nationally. We believe that every individual, no matter their race, ethnicity, background, or zip code, should have fair opportunity to achieve their full health potential. Our work addresses differences in health that are systemic, avoidable, and unjust. We partner, learn, innovate, and advocate, in order to close the gap in health for individuals and communities negatively impacted by racism or other structural inequities.</p>
 <p><b><br> At OCHIN, we value the unique perspectives and experiences of every individual and work hard to maintain a culture of belonging.</b></p>
 <p> Founded in Oregon in 2000, OCHIN employs a growing virtual workforce of more than 1,000 diverse professionals, working remotely across 48 states. We offer a generous compensation package and are committed to supporting our employees&#x2019; entire well-being by fostering a healthy work-life balance and equitable opportunity for professional advancement. We are curious, collaborative learners who strive to live our values everyday: learning, heart, belonging and impact. OCHIN is excited to support our continued national expansion and the increasing demand for our innovative tools and services by welcoming new talent to our growing team.</p>
 <p><b> Position Overview</b></p>
 <p> The <b>Research Data Engineer </b>will provide high-level professional and technical skills in support of designing, building, and maintaining data pipelines, databases, and cloud platforms to support the needs of the OCHIN Research team.</p>
 <p> In this role, you will be collaborating with an innovative, collaborative team of people moving exciting projects forward and working to improve systems and processes along the way.</p>
 <p></p>
 <p><b><br> Essential Duties</b></p>
 <ul>
  <li> Performing day to day management of on-premises, cloud, and hybrid research databases and database platforms including the Research Data Warehouse</li>
  <li> Integrating and transforming health-related data from a variety of sources and formats such as EHRs, geospatial, claims, and census into analyzable formats for research</li>
  <li> Building and maintaining datasets and data marts</li>
  <li> Monitor and maintain data pipelines proactively to ensure high service availability</li>
  <li> In partnership with Research Data Science staff and leadership, assist with scoping and designing new research data pipelines and platforms to optimize research data solutions</li>
  <li> Create scripts and programs to automate data operations</li>
  <li> Preparing and maintaining technical documentation and metadata</li>
  <li> Providing technical/consultative services to internal and external research partners, investigators, and other research personnel</li>
  <li> Performing other duties as requested by the research team</li>
 </ul> Requirements: 
 <ul>
  <li>A Master&#x2019;s level degree in Informatics, Computer Science or related discipline. Equivalent knowledge and skills obtained through a combination of education, training, and experience may meet this requirement.</li>
  <li> At least 5 years of experience in database development and administration in a healthcare and/or health research setting</li>
  <li> At least 3 years&#x2019; experience with data warehousing, including ETL techniques</li>
  <li> Strong technical proficiency with SQL required</li>
  <li> High technical proficiency with Microsoft SQL Server, including the ability to create and edit complex queries and T-SQL scripts including dynamic SQL, required; experience with SSIS</li>
  <li> Strong working knowledge of standard desktop computing software packages (word processing, spreadsheets, presentation software, Internet browsers, etc.).</li>
  <li> Strong analytical and problem-solving skills</li>
  <li> Experience with cloud and/or hybrid cloud/on-premises database architectures preferred</li>
  <li> Knowledge of specialized and complex statistical modeling and/or machine learning techniques preferred</li>
 </ul>
 <p><b> Base Pay Overview</b></p>
 <p><b> The typical offer range for this role is minimum to midpoint, (&#x24;98,819 - &#x24;128,465) with the midpoint representing the average pay in a national market scope for this position.</b> Please keep in mind that this range represents the pay range for all positions in the job grade within which this position falls. The actual salary offer will consider a wide range of factors directly relevant to this position, including, but not limited to, skills, knowledge, training, responsibility, and experience, as well as internal equity and alignment with market data.</p>
 <p><b> Work Location and Travel Requirements</b></p>
 <p> OCHIN is a 100% remote organization with no physical corporate office location. Employees work remotely from home and many of our positions also support our member organizations on-site for new software installations. Nationwide travel is determined based on OCHIN business needs. Please inquire during the interview process about travel requirements for this position.</p>
 <ul>
  <li> Ability to work independently and efficiently from a home office environment</li>
  <li> High Speed Internet Service</li>
  <li> It is a requirement that employees work in a distraction free workplace</li>
 </ul>
 <p> We offer a comprehensive range of benefits. See our website for details: https://ochin.org/employment-openings</p>
 <p><b> Equal Opportunity Statement</b></p>
 <p><i> OCHIN is proud to be an equal opportunity employer. We are committed to building a team that represents a variety of backgrounds, perspectives, and skills for the benefit of our staff, our mission, and the communities we serve.</i></p>
 <p><i> As an Equal Opportunity and Affirmative Action employer, OCHIN, Inc. does not discriminate on the basis of race, ethnicity, sex, gender identity, sexual orientation, religion, marital or civil union status, age, disability status, veteran status, or any other protected characteristics. All aspects of employment are based on merit, performance, and business needs.</i></p>
 <p><b> COVID-19 Vaccination Requirement</b></p>
 <p><i> To keep our colleagues, members, and communities safe, OCHIN requires all employees&#x2014;including remote employees, contractors, interns, and new hires&#x2014;to be vaccinated with a COVID-19 vaccine, as supported by state and federal public health officials, as a condition of employment. All new hires are required to provide proof of full vaccination or receive approval for a medical or religious exemption before their hire date.</i></p>
</div>","https://recruiting.paylocity.com/recruiting/jobs/Details/1974445/OCHIN/RESEARCH-DATA-ENGINEER-REMOTE?source=Indeed_Feed&source=Indeed","d2dcb51303834420",,"Full-time",,"Remote","RESEARCH DATA ENGINEER (REMOTE)","26 days ago","2023-09-29T11:54:29.208Z","3.4","31","$98,819 - $158,111 a year","2023-10-25T11:54:29.209Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=d2dcb51303834420&from=jasx&tk=1hdjautnrj4ij800&vjs=3"
"Thought Industries","Senior Data Engineer
   Thought Industries
   
   We're looking for a 100% remote Senior Data Engineer to help us build out reporting and analytics for our SaaS Learning Management Platform.
  
 
   We are looking for a Senior Data Engineer for our Data Engineering/BI team to use data and technology to transform and grow the way Thought Industry data teams work. You’ll work with existing Data Engineers and Product to expand capability and architecture of our system. The ideal candidates will have strong data infrastructure and data architecture skills, strong operational skills to drive efficiency and speed, strong project leadership, and contribute to the vision for how data engineering can proactively create a positive impact for our customers.
  
 
 
  About Thought Industries
   
   Thought Industries is a startup in the Online Learning space. We enable training and software companies to launch and monetize external learning programs — think Shopify meets Udemy/Coursera. .
   
   Headquartered in Boston, Massachusetts, Thought Industries is one of the world’s fastest-growing online learning companies in the U.S. We are helping consumer brands and for-profit learning organizations change how they build, deploy and grow online learning businesses. Today, hundreds of customers and brands are using the Thought Industries’ Learning Business Platform to transform the way they reach, teach, and engage audiences.
   
   This is a full-time position and candidates should be based and authorized to work in the U.S or Canada. No Recruiters, please.
 
  
 
   
   Responsibilities
  
 
  Define the processes needed to achieve operational excellence in all areas, including project management and system reliability. 
  Evaluate existing tech stack, ETL and data modeling and advise on optimizations 
  Collaborate in a cross-functional organization with Product Managers, Software Engineers, and Infrastructure to understand data needs and deliver on those needs. 
  Drive the design, building, and launching of new data models and data pipelines in production. 
  Manage development of data resources and support new product launches. 
  Drive data quality across our product and related business areas. 
  Manage the delivery of high impact dashboards and data visualizations. 
  Define and manage SLA’s for all data sets and processes running in production. 
  
 
   Skills & Qualifications
  
 
  3-5 years of experience in Data Engineering, BI, or Data Warehousing 
  Capable of completing assigned projects with a high degree of autonomy 
  Data architecture experience 
  Hands-on experience with AWS services (S3, Redshift, RDS Postgres) 
  ETL/ELT 
  Data modeling/warehouse design 
  Experience in SQL or similar languages and development experience in at least one scripting language (Python, Go, etc.) 
  BA/BS in Computer Science, Math, Physics, or other technical fields
 
  
 
   
   Bonus Qualifications
   Experience with:
  
 
  Data visualization tools (e.g. Looker) 
  Data governance and Data lineage 
  RDS Postgres 
  
 
   The development team is completely distributed across the US, and has been since the inception of Thought Industries — we haven't bolted on remote processes due to COVID-19, we've always done it this way, so you'll feel right at home.
   
   Benefits include:
  
 
  Flexible work hours 
  Unlimited PTO 
  Medical, dental and vision coverage 
  Short term disability, long term disability, life insurance, employee assistance program 
  Wellness and meditation 
  Employee rewards program 
  401k 
  And more! 
  
 
   We are a growing, well-funded technology company, with a talented team and a clear vision. This is a unique opportunity to take a lead role at an exciting SaaS software company with a robust cloud-based platform. We hire talented people who are self-motivated and team orientated. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability or veteran status.","<div>
 <div>
  Senior Data Engineer
  <br> Thought Industries
  <br> 
  <br> We&apos;re looking for a 100% remote Senior Data Engineer to help us build out reporting and analytics for our SaaS Learning Management Platform.
 </div> 
 <div>
  <br> We are looking for a Senior Data Engineer for our Data Engineering/BI team to use data and technology to transform and grow the way Thought Industry data teams work. You&#x2019;ll work with existing Data Engineers and Product to expand capability and architecture of our system. The ideal candidates will have strong data infrastructure and data architecture skills, strong operational skills to drive efficiency and speed, strong project leadership, and contribute to the vision for how data engineering can proactively create a positive impact for our customers.
 </div> 
 <div></div>
 <div>
  About Thought Industries
  <br> 
  <br> Thought Industries is a startup in the Online Learning space. We enable training and software companies to launch and monetize external learning programs &#x2014; think Shopify meets Udemy/Coursera. .
  <br> 
  <br> Headquartered in Boston, Massachusetts, Thought Industries is one of the world&#x2019;s fastest-growing online learning companies in the U.S. We are helping consumer brands and for-profit learning organizations change how they build, deploy and grow online learning businesses. Today, hundreds of customers and brands are using the Thought Industries&#x2019; Learning Business Platform to transform the way they reach, teach, and engage audiences.
  <br> 
  <br> This is a full-time position and candidates should be based and authorized to work in the U.S or Canada. No Recruiters, please.
 </div>
 <br> 
 <div>
  <br> 
  <br> Responsibilities
 </div> 
 <ul>
  <li>Define the processes needed to achieve operational excellence in all areas, including project management and system reliability.</li> 
  <li>Evaluate existing tech stack, ETL and data modeling and advise on optimizations</li> 
  <li>Collaborate in a cross-functional organization with Product Managers, Software Engineers, and Infrastructure to understand data needs and deliver on those needs.</li> 
  <li>Drive the design, building, and launching of new data models and data pipelines in production.</li> 
  <li>Manage development of data resources and support new product launches.</li> 
  <li>Drive data quality across our product and related business areas.</li> 
  <li>Manage the delivery of high impact dashboards and data visualizations.</li> 
  <li>Define and manage SLA&#x2019;s for all data sets and processes running in production.</li> 
 </ul> 
 <div>
  <br> Skills &amp; Qualifications
 </div> 
 <ul>
  <li>3-5 years of experience in Data Engineering, BI, or Data Warehousing</li> 
  <li>Capable of completing assigned projects with a high degree of autonomy</li> 
  <li>Data architecture experience</li> 
  <li>Hands-on experience with AWS services (S3, Redshift, RDS Postgres)</li> 
  <li>ETL/ELT</li> 
  <li>Data modeling/warehouse design</li> 
  <li>Experience in SQL or similar languages and development experience in at least one scripting language (Python, Go, etc.)</li> 
  <li>BA/BS in Computer Science, Math, Physics, or other technical fields</li>
 </ul>
 <br> 
 <div>
  <br> 
  <br> Bonus Qualifications
  <br> Experience with:
 </div> 
 <ul>
  <li>Data visualization tools (e.g. Looker)</li> 
  <li>Data governance and Data lineage</li> 
  <li>RDS Postgres</li> 
 </ul> 
 <div>
  <br> The development team is completely distributed across the US, and has been since the inception of Thought Industries &#x2014; we haven&apos;t bolted on remote processes due to COVID-19, we&apos;ve always done it this way, so you&apos;ll feel right at home.
  <br> 
  <br> Benefits include:
 </div> 
 <ul>
  <li>Flexible work hours</li> 
  <li>Unlimited PTO</li> 
  <li>Medical, dental and vision coverage</li> 
  <li>Short term disability, long term disability, life insurance, employee assistance program</li> 
  <li>Wellness and meditation</li> 
  <li>Employee rewards program</li> 
  <li>401k</li> 
  <li>And more!</li> 
 </ul> 
 <div>
  <br> We are a growing, well-funded technology company, with a talented team and a clear vision. This is a unique opportunity to take a lead role at an exciting SaaS software company with a robust cloud-based platform. We hire talented people who are self-motivated and team orientated. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability or veteran status.
 </div>
</div>","https://www.thoughtindustries.com/about-us/careers/?gnk=job&gni=8a7887a880012488018005ce09e14f45&gns=Indeed+Free","35f35d6b34cdeb50",,"Full-time",,"Remote","Senior Data Engineer","21 days ago","2023-10-04T11:54:28.032Z","4.7","3",,"2023-10-25T11:54:28.035Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=35f35d6b34cdeb50&from=jasx&tk=1hdjautnrj4ij800&vjs=3"
"Jobscan","At Jobscan, we’re passionate about empowering job seekers to land more interviews with AI technology. We have helped over 2 million job seekers get more interviews in 100+ countries. Jobscan’s platform benefits job seekers, employers, universities, and communities. We're a fast-growing remote startup. We are completely customer-funded, profitable, and growing exponentially!
  
  
  
    We handle vast amounts of data to help job seekers succeed, and we need an experienced Data Engineer to optimize our pipelines for reliability, efficiency, and quality. As part of our distributed engineering team, you will play a crucial role in shaping the future of our data assets.
  
  
 
  
   What you'll be doing
   
    
      Diagnose and Resolve Issues: Troubleshoot and fix data issues within our existing pipeline, which is built on Segment.
      ETL Development: Design, implement, and maintain ETL processes tailored for BigQuery and MySQL while adhering to privacy and governance principles.
      Data Cleansing: Develop and implement data validation and transformation solutions as an integral part of our ETL workflows.
      Data Integration: Utilize Segment for optimized data collection, integration, and management.
      Stakeholder Collaboration: Work closely with stakeholders to tackle specific data integrity and quality issues.
      Teamwork: Collaborate with our Senior Data Analyst and engineering team to refine data models and architectures.
      SQL Optimization: Write and fine-tune SQL queries for performance and scalability in BigQuery and MySQL environments.
      Documentation: Maintain meticulous documentation for all data processes and updates.
    
   
  
  
 
  
   What you'll need
   
    
      Bachelor’s degree in Computer Science, Engineering, or a related field.
      7+ years of relevant experience in data engineering, especially in data pipeline cleanup and ETL processes.
      Direct experience with Customer Data Platforms (CDP) such as Segment, Rudderstack, or Treasure Data.
      Mastery of SQL with hands-on experience in BigQuery and MySQL.
      Proficient in Google Cloud Platform services, particularly BigQuery and Google Analytics 4.
      Experience with modern programming languages like Python, R, JavaScript, and PHP.
      Exceptional problem-solving and communication skills.
      Proven expertise in data schemas and data cleaning principles.
    
   
  
  
 
  
   Preferred qualifications
   
    
      Specific prior experience with Segment for data integration is a strong plus.
      Capability to read and understand PHP and JavaScript code to collaborate effectively with our engineering team.
      Proven track record in tackling data quality and integrity issues in team settings.
    
   
  
  
 
  
   $140,000 - $175,000 a year
  
  
 
  
   Benefits
  
  
    - Remote work - we trust you to get your work done and make it to your meetings‍‍
  
  
    - Competitive salary + stock options - you should have a piece of what we're building here️
  
  
    - Flexible schedule - we make it easy to take care of the important things, like your family and health‍ ️
  
  
    - Unlimited PTO + 14 Paid Holidays + Paid Sick Days - we want our employees to have time to care for their personal wellness and mental health‍ ️
  
  
    - Paid maternal/parental leave - enjoy time with your family's new addition‍‍‍
  
  
    401(k) + employer match
   Medical, dental, vision, and life insurance with generous employer contributions
   Health savings accounts
   Life insurance
   $1000 office stipend; monthly education and internet stipend
  
  
    - Wellness stipend - use for yoga class, gym membership, or anything that improves your personal wellness‍ ️
  
  
   Apple computer or PC of your choice
   Bi-annual company retreats
  
  
  
    Jobscan is committed to equal pay; diversity, equity, and inclusion; and As we continue to grow, we are always adding more benefits and perks for our team.
  
  
  
    Jobscan provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.","<div>
 <div>
  <div>
   At Jobscan, we&#x2019;re passionate about empowering job seekers to land more interviews with AI technology. We have helped over 2 million job seekers get more interviews in 100+ countries. Jobscan&#x2019;s platform benefits job seekers, employers, universities, and communities. We&apos;re a fast-growing remote startup. We are completely customer-funded, profitable, and growing exponentially!
  </div>
  <div></div>
  <div>
   <br> We handle vast amounts of data to help job seekers succeed, and we need an experienced Data Engineer to optimize our pipelines for reliability, efficiency, and quality. As part of our distributed engineering team, you will play a crucial role in shaping the future of our data assets.
  </div>
 </div> 
 <div>
  <div>
   <h3 class=""jobSectionHeader""><b>What you&apos;ll be doing</b></h3>
   <ul>
    <ul>
     <li><b> Diagnose and Resolve Issues:</b> Troubleshoot and fix data issues within our existing pipeline, which is built on Segment.</li>
     <li><b> ETL Development:</b> Design, implement, and maintain ETL processes tailored for BigQuery and MySQL while adhering to privacy and governance principles.</li>
     <li><b> Data Cleansing:</b> Develop and implement data validation and transformation solutions as an integral part of our ETL workflows.</li>
     <li><b> Data Integration:</b> Utilize Segment for optimized data collection, integration, and management.</li>
     <li><b> Stakeholder Collaboration:</b> Work closely with stakeholders to tackle specific data integrity and quality issues.</li>
     <li><b> Teamwork:</b> Collaborate with our Senior Data Analyst and engineering team to refine data models and architectures.</li>
     <li><b> SQL Optimization:</b> Write and fine-tune SQL queries for performance and scalability in BigQuery and MySQL environments.</li>
     <li><b> Documentation:</b> Maintain meticulous documentation for all data processes and updates.</li>
    </ul>
   </ul>
  </div>
 </div> 
 <div>
  <div>
   <h3 class=""jobSectionHeader""><b>What you&apos;ll need</b></h3>
   <ul>
    <ul>
     <li> Bachelor&#x2019;s degree in Computer Science, Engineering, or a related field.</li>
     <li> 7+ years of relevant experience in data engineering, especially in data pipeline cleanup and ETL processes.</li>
     <li> Direct experience with Customer Data Platforms (CDP) such as Segment, Rudderstack, or Treasure Data.</li>
     <li> Mastery of SQL with hands-on experience in BigQuery and MySQL.</li>
     <li> Proficient in Google Cloud Platform services, particularly BigQuery and Google Analytics 4.</li>
     <li> Experience with modern programming languages like Python, R, JavaScript, and PHP.</li>
     <li> Exceptional problem-solving and communication skills.</li>
     <li> Proven expertise in data schemas and data cleaning principles.</li>
    </ul>
   </ul>
  </div>
 </div> 
 <div>
  <div>
   <h3 class=""jobSectionHeader""><b>Preferred qualifications</b></h3>
   <ul>
    <ul>
     <li> Specific prior experience with Segment for data integration is a strong plus.</li>
     <li> Capability to read and understand PHP and JavaScript code to collaborate effectively with our engineering team.</li>
     <li> Proven track record in tackling data quality and integrity issues in team settings.</li>
    </ul>
   </ul>
  </div>
 </div> 
 <div>
  <div>
   &#x24;140,000 - &#x24;175,000 a year
  </div>
 </div> 
 <div>
  <div>
   <b>Benefits</b>
  </div>
  <div>
   <b> - Remote work</b> - we trust you to get your work done and make it to your meetings&#x200d;&#x200d;
  </div>
  <div>
   <b> - Competitive salary + stock options</b> - you should have a piece of what we&apos;re building here&#xfe0f;
  </div>
  <div>
   <b> - Flexible schedule</b> - we make it easy to take care of the important things, like your family and health&#x200d; &#xfe0f;
  </div>
  <div>
   <b> - Unlimited PTO + 14 Paid Holidays + Paid Sick Days </b>- we want our employees to have time to care for their personal wellness and mental health&#x200d; &#xfe0f;
  </div>
  <div>
   <b> - Paid maternal/parental leave</b> - enjoy time with your family&apos;s new addition&#x200d;&#x200d;&#x200d;
  </div>
  <ul>
   <li> <b>401(k) + employer match</b></li>
   <li><b>Medical, dental, vision, and life insurance </b>with generous employer contributions</li>
   <li><b>Health savings accounts</b></li>
   <li><b>Life insurance</b></li>
   <li><b>&#x24;1000 office stipend; monthly education and internet stipend</b></li>
  </ul>
  <div>
   <b> - Wellness stipend </b>- use for yoga class, gym membership, or anything that improves your personal wellness&#x200d; &#xfe0f;
  </div>
  <ul>
   <li><b>Apple computer or PC of your choice</b></li>
   <li><b>Bi-annual company retreats</b></li>
  </ul>
  <div></div>
  <div>
   <br> Jobscan is committed to equal pay; diversity, equity, and inclusion; and As we continue to grow, we are always adding more benefits and perks for our team.
  </div>
  <div></div>
  <div>
   <i><br> Jobscan provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws</i>.
  </div>
 </div>
</div>","https://jobs.lever.co/jobscan-2/88ce689e-7f61-4027-bfc7-e727038e90f5","0213c2c18b4e7a68",,"Full-time",,"Remote","Senior Data Engineer","26 days ago","2023-09-29T11:54:35.957Z",,,"$140,000 - $175,000 a year","2023-10-25T11:54:35.958Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=0213c2c18b4e7a68&from=jasx&tk=1hdjautnrj4ij800&vjs=3"
"General Mills","Job Description:
 
Employer: General Mills, Inc. 

 
Job Title: Senior Data Engineer (multiple positions) 

Job Requisition: #25002 | 20330.305.6 

Job Location: 1 General Mills Blvd. Minneapolis, MN 55426 | Telecommuting 100% of time is permitted. 

Job Type: Full Time 

Rate of Pay: $133,385 - $174,600 per year 

 
Duties: Work closely with a multidisciplinary agile team to build high quality data pipelines driving analytic solutions that will generate insights from our connected data, enabling General Mills to advance the data-driven decision-making capabilities of our enterprise. Design, develop, optimize, and maintain data architecture and pipelines that adhere to ETL principles and business goals. Solve complex data problems to deliver insights that helps our business to achieve their goals. Create data products for analytics and data scientist team members to improve their productivity. Advise, consult, mentor and coach other data and analytic professionals on data standards and practices. Foster a culture of sharing, re-use, design for scale stability, and operational efficiency of data and analytical solutions. Lead evaluation, implementation and deployment of emerging tools and process for analytic data engineering to improve our productivity as a team. Develop and deliver communication and education plans on analytic data engineering capabilities, standards, and processes. Partner with business analysts and solutions architects to develop technical architectures for strategic enterprise projects and initiatives. 

 Telecommuting 100% of time is permitted. 

 
Requirements: Employer will accept a Bachelor's degree in Computer Science, Management Information Systems, Engineering, or related field and 5 years of post-baccalaureate, progressively responsible experience in job offered or 5 years of post-baccalaureate, progressively responsible experience in data engineering or architecture. 

 
Must have experience in each of the following:
 1. 4 years of experience working with data analysis 
2. 4 years of experience with SQL or Hive QL 
3. 4 years of experience developing and maintaining data warehouses in big data solutions 
4. 4 years of experience with Big Data development using Hadoop 
5. 4 years of experience with Hive, BigQuery , Impala OR Spark 
6. 4 years of experience automating the data pipelines/processes 
7. 4 years of experience with Hadoop ecosystems (HDFS, YARN, Hive, HBase, Sqoop, Spark, and or Hue, ) 
8. 3 years of experience utilizing Agile Development Methodology 
9. 3 years of experience with Git Repositories 

 Telecommuting 100% of time is permitted. Background check and drug testing required. 

Contact: Apply online at https://careers.generalmills.com/careers/ Please refer to job requisition number- #25002 

 The salary range for this position $133,385-$174,600 per year. At General Mills we strive for each employee’s pay at any point in their career to reflect their experiences, performance and skills for their current role. The salary range for this role represents the numerous factors considered in the hiring decision including, but not limited to, education, skills, work experience, certifications, etc. As such, pay for the successful candidate(s) could fall anywhere within the stated range. Beyond base salary, General Mills offers a competitive Total Rewards package focusing on your overall well-being. We are proud to offer a foundation of health benefits, retirement and financial wellbeing, time off programs, wellbeing support and perks. Benefits may vary by role, country, region, union status, and other employment status factors. You may also be eligible to participate in an annual incentive program. An incentive award, if any, depends on various factors, including, individual and organizational performance. 
. 

 
Company Overview:
 We exist to make food the world loves. But we do more than that. Our company is a place that prioritizes being a force for good, a place to expand learning, explore new perspectives and reimagine new possibilities, every day. We look for people who want to bring their best — bold thinkers with big hearts who challenge one other and grow together. Because becoming the undisputed leader in food means surrounding ourselves with people who are hungry for what’s next.","<b>Job Description:</b>
<br> 
<b>Employer:</b> General Mills, Inc. 
<br>
<br> 
<b>Job Title:</b> Senior Data Engineer (multiple positions) 
<br>
<b>Job Requisition:</b> #25002 | 20330.305.6 
<br>
<b>Job Location:</b> 1 General Mills Blvd. Minneapolis, MN 55426 | Telecommuting 100% of time is permitted. 
<br>
<b>Job Type:</b> Full Time 
<br>
<b>Rate of Pay:</b> &#x24;133,385 - &#x24;174,600 per year 
<br>
<br> 
<b>Duties:</b> Work closely with a multidisciplinary agile team to build high quality data pipelines driving analytic solutions that will generate insights from our connected data, enabling General Mills to advance the data-driven decision-making capabilities of our enterprise. Design, develop, optimize, and maintain data architecture and pipelines that adhere to ETL principles and business goals. Solve complex data problems to deliver insights that helps our business to achieve their goals. Create data products for analytics and data scientist team members to improve their productivity. Advise, consult, mentor and coach other data and analytic professionals on data standards and practices. Foster a culture of sharing, re-use, design for scale stability, and operational efficiency of data and analytical solutions. Lead evaluation, implementation and deployment of emerging tools and process for analytic data engineering to improve our productivity as a team. Develop and deliver communication and education plans on analytic data engineering capabilities, standards, and processes. Partner with business analysts and solutions architects to develop technical architectures for strategic enterprise projects and initiatives. 
<br>
<br> Telecommuting 100% of time is permitted. 
<br>
<br> 
<b>Requirements:</b> Employer will accept a Bachelor&apos;s degree in Computer Science, Management Information Systems, Engineering, or related field and 5 years of post-baccalaureate, progressively responsible experience in job offered or 5 years of post-baccalaureate, progressively responsible experience in data engineering or architecture. 
<br>
<br> 
<b>Must have experience in each of the following:</b>
<br> 1. 4 years of experience working with data analysis 
<br>2. 4 years of experience with SQL or Hive QL 
<br>3. 4 years of experience developing and maintaining data warehouses in big data solutions 
<br>4. 4 years of experience with Big Data development using Hadoop 
<br>5. 4 years of experience with Hive, BigQuery , Impala OR Spark 
<br>6. 4 years of experience automating the data pipelines/processes 
<br>7. 4 years of experience with Hadoop ecosystems (HDFS, YARN, Hive, HBase, Sqoop, Spark, and or Hue, ) 
<br>8. 3 years of experience utilizing Agile Development Methodology 
<br>9. 3 years of experience with Git Repositories 
<br>
<br> Telecommuting 100% of time is permitted. Background check and drug testing required. 
<br>
<b>Contact:</b> Apply online at https://careers.generalmills.com/careers/ Please refer to job requisition number- #25002 
<br>
<br> The salary range for this position &#x24;133,385-&#x24;174,600 per year. At General Mills we strive for each employee&#x2019;s pay at any point in their career to reflect their experiences, performance and skills for their current role. The salary range for this role represents the numerous factors considered in the hiring decision including, but not limited to, education, skills, work experience, certifications, etc. As such, pay for the successful candidate(s) could fall anywhere within the stated range. Beyond base salary, General Mills offers a competitive Total Rewards package focusing on your overall well-being. We are proud to offer a foundation of health benefits, retirement and financial wellbeing, time off programs, wellbeing support and perks. Benefits may vary by role, country, region, union status, and other employment status factors. You may also be eligible to participate in an annual incentive program. An incentive award, if any, depends on various factors, including, individual and organizational performance. 
<br>. 
<br>
<br> 
<b>Company Overview:</b>
<br> We exist to make food the world loves. But we do more than that. Our company is a place that prioritizes being a force for good, a place to expand learning, explore new perspectives and reimagine new possibilities, every day. We look for people who want to bring their best &#x2014; bold thinkers with big hearts who challenge one other and grow together. Because becoming the undisputed leader in food means surrounding ourselves with people who are hungry for what&#x2019;s next.","https://www.indeed.com/applystart?jk=1cc34471192a5294&from=vj&pos=top&mvj=0&spon=0&sjdu=YmZE5d5THV8u75cuc0H6Y26AwfY51UOGmh3Z9h4OvXhKYvp8hez65y3sda6E7q4aIv02HSGSZoWequxOEmCHqw&vjfrom=serp&astse=92dda8ad1c9747d0&assa=7089","1cc34471192a5294",,"Full-time",,"1 General Mills Boulevard, Minneapolis, MN 55426","Senior Data Engineer","27 days ago","2023-09-28T11:54:27.730Z","3.9","2801","$133,385 - $174,600 a year","2023-10-25T11:54:27.731Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=1cc34471192a5294&from=jasx&tk=1hdjautnrj4ij800&vjs=3"
"Intone Networks","Description • Experienced in writing effective SQL and stored procedures. • Extensive experience in Snowflake SQL performance Optimization. • Expertise in Snowflake data modeling, ELT/ETL using Snowflake SQL and developing Stored Procs. • Good understanding of Snowflake user management and RBAC. • Provide technical guidance. • Right-sizing recommendations • Identifying technologies applying to application environments • Review and optimize existing Run Book • Implementing best practices for access controls • Ability to document technical as well as management process flows • Knowledge of Azure DevOps services, including Azure Pipelines, Azure Repos, and Azure Artifacts • Experience with Design, implement, and maintain CI/CD pipelines on Azure DevOps • Familiarity with Azure development, Azure administration, or DevOps • Support the deployment and release of DevOps CI/CD pipelines. • Actively develop and implement innovative Azure cloud solutions. • Assist the team in defining technical standards in Azure environments. • Handle end-to-end project cycle from analyzing requirements to crafting tasks, facilitate designs, build, test, and provide operational documentation. • Design, implement and support Microsoft Azure solutions that meets technical, security, and business requirements","Description &#x2022; Experienced in writing effective SQL and stored procedures. &#x2022; Extensive experience in Snowflake SQL performance Optimization. &#x2022; Expertise in Snowflake data modeling, ELT/ETL using Snowflake SQL and developing Stored Procs. &#x2022; Good understanding of Snowflake user management and RBAC. &#x2022; Provide technical guidance. &#x2022; Right-sizing recommendations &#x2022; Identifying technologies applying to application environments &#x2022; Review and optimize existing Run Book &#x2022; Implementing best practices for access controls &#x2022; Ability to document technical as well as management process flows &#x2022; Knowledge of Azure DevOps services, including Azure Pipelines, Azure Repos, and Azure Artifacts &#x2022; Experience with Design, implement, and maintain CI/CD pipelines on Azure DevOps &#x2022; Familiarity with Azure development, Azure administration, or DevOps &#x2022; Support the deployment and release of DevOps CI/CD pipelines. &#x2022; Actively develop and implement innovative Azure cloud solutions. &#x2022; Assist the team in defining technical standards in Azure environments. &#x2022; Handle end-to-end project cycle from analyzing requirements to crafting tasks, facilitate designs, build, test, and provide operational documentation. &#x2022; Design, implement and support Microsoft Azure solutions that meets technical, security, and business requirements","https://intone.com/careers-at-intone/","7fe6957c47f1e935",,,,"Remote","Snowflake Data Engineer","22 days ago","2023-10-03T11:54:38.825Z","4.1","18",,"2023-10-25T11:54:38.826Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=7fe6957c47f1e935&from=jasx&tk=1hdjautnrj4ij800&vjs=3"
"Fable","Fable is a mission-driven start-up based in Silicon Valley, founded in 2019 by global tech industry veteran Padmasree Warrior.
 
 
 
   ️ PURPOSE
 
 
   We are building Fable because stress, anxiety, depression, and social isolation are on the rise and affecting people globally, across all age and income levels — and we can help. Stories promote empathy, emotional intelligence, and other cognitive abilities that can lead to better mental health. Just 30 minutes of reading every day can improve our mental well-being. Fable helps our community make reading a daily healthy habit. Community members enjoy the many benefits of reading while making deeper human connections with other readers. We are backed by top investors, including Redpoint Ventures, Tiger Global, M13, Gaingels, and notable angel investors who believe in our mission and team.
 
 
 
   \uD83C\uDFD7 WHAT WE ARE BUILDING
 
 
   Fable is a community-powered platform for discovering, reading, and discussing books, articles, and podcasts, for deeper connections, upskilling, and mental wellness.
   
 
 
  
 
  Social Reading
 
 
   Fable makes it easy for people to discover, join and build communities to read together based on their interest graph. Fable members read together, sharing highlights, comments and insights. We make reading interactive and fun in the Fable eReader.
 
 
 
   Organized Reading
 
 
   Fable helps people organize all of their reading in one central place. Members can create reading “Lists,” share their lists, follow other members, import their reading lists from other platforms, and rate and review books. We make personalized recommendations based on your reading preferences.
 
 
 
   Healthy Habits
 
 
   Every day, Fable helps members set and reach their reading goals to reap the wellness benefits of reading. Our communities set their own milestones to pace themselves and fill the micro-moments in their lives with stories.
 
 
 
   \uD83E\uDD39 
  ROLE
 
 
   We’re looking for an experienced data engineer with a love of working with data at scale to join our distributed team in building the world’s best platform for social reading.
 
 
 
   This is an independent role where you’ll be working with various stakeholders across Fable, both in and out of engineering, to design and launch new systems for extracting, transforming and storing data. You’ll be called upon to improve Fable’s data system’s reliability, efficiency and legibility and will be expected to scale your solutions to the business environment of a small startup, iterate quickly, and make pragmatic choices around what tools and technologies to adopt.
 
 
 
   \uD83D\uDCAA\uD83C\uDFFD WHAT YOU WILL DO
 
 
 
   Develop data models and pipelines to enable reporting, modeling and machine learning
  Develop a content knowledge graph, combining multiple data sources
  Ensure data quality, perform data audits
  Improve performance of data/analytics infrastructure
  Leverage Google Cloud tools and services to bring data workloads to production
  Collaborate with backend engineering and data teams
  Be an advocate of data-driven thinking and communicate data and metrics to the entire company!
 
 
 
   ✔️ SKILLS YOU WILL NEED
 
 
 
   Thought leadership to ideate with business leaders, identify areas of opportunity, and influence decisions
  Experience working in a cloud environment (GCP, AWS, Azure)
  Experience with DBT or similar data frameworks a plus
  Experience with helping organize disparate data needs and workflows into a consistent, reliable system
  Curiosity to understand business needs and translate them to data solutions
  Experience in data modeling and creating data pipelines
 
 
 
   \uD83D\uDE0A IS THIS YOU?
 
 
 
   You're self-motivated, and take ownership and responsibility
  You love working with smart, fun, sincere and dedicated peers
  You want to be the one to make it happen
  You are resilient and can cope with ambiguity
  Comfortable in a fast-paced and at times unpredictable start-up environment
  Big plus if you have a love for stories and reading!
 
 
 
   \uD83D\uDC50\uD83C\uDFFD YOUR TEAM AT FABLE
 
 
   At Fable, you'll join a passionate, high-performing and empathetic team of people who love stories. We are proud to work on a purpose-driven product with a mission of improving mental wellness. We are a tech company with the soul of an artist. We are an early-stage startup and as such a constant work in progress. We have no time for bureaucracy and are looking for leaders, not spectators. We listen, understand, and consider before we judge. We are committed to diversity and inclusion, and have a set of values that are an integral part of our company culture. Please check out our diversity and inclusion manifesto and company values here.
 
 
 
   \uD83D\uDD11 
  WHY WORK AT FABLE
 
 
   This is a unique opportunity if you are looking to join a small team making a big impact, and work on a fast-growing product while having fun along the way.
 
 
 
   - Annual Base Pay for this role: $120k - $160k (dependent on location and commensurate with experience)
 
 
  Competitive stock options
  Comprehensive health and dental plans
  Flexible vacation days
  Self contributing 401k
  Open and transparent culture
  Parental leave (we believe in life integration not just work-life balance)
  Work from anywhere, any time
 
 
 
   WANT TO LEARN MORE ABOUT FABLE?
 
 
  Check out our Founder’s message, meet our team and read our principles to make sure we are right for you
  Read more about Fable in Fortune and Marie Claire
  Listen to our Founder talk about mental wellness and stories
  Top 50 seed companies to work for 2021
  
 
  We are an equal opportunity employer and embrace diversity at our company. We do not discriminate by race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. We are a gender-balanced team committed to diversity and an inclusive environment.","<div>
 <div>
  Fable is a mission-driven start-up based in Silicon Valley, founded in 2019 by global tech industry veteran Padmasree Warrior.
 </div>
 <div></div>
 <ul>
  <li><br> &#xfe0f;<b> PURPOSE</b></li>
 </ul>
 <div>
   We are building Fable because stress, anxiety, depression, and social isolation are on the rise and affecting people globally, across all age and income levels &#x2014; and we can help. Stories promote empathy, emotional intelligence, and other cognitive abilities that can lead to better mental health. Just 30 minutes of reading every day can improve our mental well-being. Fable helps our community make reading a daily healthy habit. Community members enjoy the many benefits of reading while making deeper human connections with other readers. We are backed by top investors, including Redpoint Ventures, Tiger Global, M13, Gaingels, and notable angel investors who believe in our mission and team.
 </div>
 <div></div>
 <div>
  <b><br> \uD83C\uDFD7 WHAT WE ARE BUILDING</b>
 </div>
 <div>
   Fable is a community-powered platform for discovering, reading, and discussing books, articles, and podcasts, for deeper connections, upskilling, and mental wellness.
  <br> 
 </div>
 <div></div>
 <br> 
 <div>
  <b>Social Reading</b>
 </div>
 <div>
   Fable makes it easy for people to discover, join and build communities to read together based on their interest graph. Fable members read together, sharing highlights, comments and insights. We make reading interactive and fun in the Fable eReader.
 </div>
 <div></div>
 <div>
  <b><br> Organized Reading</b>
 </div>
 <div>
   Fable helps people organize all of their reading in one central place. Members can create reading &#x201c;Lists,&#x201d; share their lists, follow other members, import their reading lists from other platforms, and rate and review books. We make personalized recommendations based on your reading preferences.
 </div>
 <div></div>
 <div>
  <b><br> Healthy Habits</b>
 </div>
 <div>
   Every day, Fable helps members set and reach their reading goals to reap the wellness benefits of reading. Our communities set their own milestones to pace themselves and fill the micro-moments in their lives with stories.
 </div>
 <div></div>
 <div>
  <br> \uD83E\uDD39 
  <b>ROLE</b>
 </div>
 <div>
   We&#x2019;re looking for an experienced data engineer with a love of working with data at scale to join our distributed team in building the world&#x2019;s best platform for social reading.
 </div>
 <div></div>
 <div>
  <br> This is an independent role where you&#x2019;ll be working with various stakeholders across Fable, both in and out of engineering, to design and launch new systems for extracting, transforming and storing data. You&#x2019;ll be called upon to improve Fable&#x2019;s data system&#x2019;s reliability, efficiency and legibility and will be expected to scale your solutions to the business environment of a small startup, iterate quickly, and make pragmatic choices around what tools and technologies to adopt.
 </div>
 <div></div>
 <div>
  <b><br> \uD83D\uDCAA\uD83C\uDFFD WHAT YOU WILL DO</b>
 </div>
 <div></div>
 <ul>
  <li><br> Develop data models and pipelines to enable reporting, modeling and machine learning</li>
  <li>Develop a content knowledge graph, combining multiple data sources</li>
  <li>Ensure data quality, perform data audits</li>
  <li>Improve performance of data/analytics infrastructure</li>
  <li>Leverage Google Cloud tools and services to bring data workloads to production</li>
  <li>Collaborate with backend engineering and data teams</li>
  <li>Be an advocate of data-driven thinking and communicate data and metrics to the entire company!</li>
 </ul>
 <div></div>
 <div>
  <b><br> &#x2714;&#xfe0f; SKILLS YOU WILL NEED</b>
 </div>
 <div></div>
 <ul>
  <li><br> Thought leadership to ideate with business leaders, identify areas of opportunity, and influence decisions</li>
  <li>Experience working in a cloud environment (GCP, AWS, Azure)</li>
  <li>Experience with DBT or similar data frameworks a plus</li>
  <li>Experience with helping organize disparate data needs and workflows into a consistent, reliable system</li>
  <li>Curiosity to understand business needs and translate them to data solutions</li>
  <li>Experience in data modeling and creating data pipelines</li>
 </ul>
 <div></div>
 <div>
  <b><br> \uD83D\uDE0A IS THIS YOU?</b>
 </div>
 <div></div>
 <ul>
  <li><br> You&apos;re self-motivated, and take ownership and responsibility</li>
  <li>You love working with smart, fun, sincere and dedicated peers</li>
  <li>You want to be the one to make it happen</li>
  <li>You are resilient and can cope with ambiguity</li>
  <li>Comfortable in a fast-paced and at times unpredictable start-up environment</li>
  <li>Big plus if you have a love for stories and reading!</li>
 </ul>
 <div></div>
 <div>
  <b><br> \uD83D\uDC50\uD83C\uDFFD YOUR TEAM AT FABLE</b>
 </div>
 <div>
   At Fable, you&apos;ll join a passionate, high-performing and empathetic team of people who love stories. We are proud to work on a purpose-driven product with a mission of improving mental wellness. We are a tech company with the soul of an artist. We are an early-stage startup and as such a constant work in progress. We have no time for bureaucracy and are looking for leaders, not spectators. We listen, understand, and consider before we judge. We are committed to diversity and inclusion, and have a set of values that are an integral part of our company culture. Please check out our diversity and inclusion manifesto and company values here.
 </div>
 <div></div>
 <div>
  <br> \uD83D\uDD11 
  <b>WHY WORK AT FABLE</b>
 </div>
 <div>
   This is a unique opportunity if you are looking to join a small team making a big impact, and work on a fast-growing product while having fun along the way.
 </div>
 <div></div>
 <div>
  <br> - Annual Base Pay for this role: &#x24;120k - &#x24;160k (dependent on location and commensurate with experience)
 </div>
 <ul>
  <li>Competitive stock options</li>
  <li>Comprehensive health and dental plans</li>
  <li>Flexible vacation days</li>
  <li>Self contributing 401k</li>
  <li>Open and transparent culture</li>
  <li>Parental leave (we believe in life integration not just work-life balance)</li>
  <li>Work from anywhere, any time</li>
 </ul>
 <div></div>
 <div>
  <b><br> WANT TO LEARN MORE ABOUT FABLE?</b>
 </div>
 <ul>
  <li>Check out our Founder&#x2019;s message, meet our team and read our principles to make sure we are right for you</li>
  <li>Read more about Fable in Fortune and Marie Claire</li>
  <li>Listen to our Founder talk about mental wellness and stories</li>
  <li>Top 50 seed companies to work for 2021</li>
 </ul> 
 <div>
  We are an equal opportunity employer and embrace diversity at our company. We do not discriminate by race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. We are a gender-balanced team committed to diversity and an inclusive environment.
 </div>
</div>","https://jobs.lever.co/getfable/77187925-bba0-42cc-839f-39c415da18d9?lever-source=Indeed","2ea2f8e64c5398d4",,"Full-time",,"Remote","Senior Data Engineer","25 days ago","2023-09-30T11:54:40.859Z","5","4","$120,000 - $160,000 a year","2023-10-25T11:54:40.861Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=2ea2f8e64c5398d4&from=jasx&tk=1hdjautnrj4ij800&vjs=3"
"Yum! Brands","At Yum!, we are looking for a data engineering leader to lead our dynamic and rapidly scaling team. We are searching for a curious person who thrives from being able to implement methodical solutions and dabble creatively to drive success. For maximum impact at Yum!, this data engineer must be able to partner with key stakeholders in our business to completely understand context and guide which business and technological priorities we should address using data … and how we should go about doing so. 
  The lead data engineer will serve as a domain expert and will develop a rapport with our internal business partners and technologists as an essential, long-term part of the team. 
  We are looking for a candidate who has practical experience with technologies and languages supporting big data pipelines, streaming, ETL, modelling, governance, and reporting. Our lead data engineer will lead and provide hands-on guidance to the team developing and maintaining our platform. The lead data engineer will also make technology and business recommendations, as well as develop key insights based on data analysis and our platform's operational performance. 
  Key Responsibilities: 
  
  Creating and engineering data pipelines for end-to-end delivery of raw and streamed datasets from Yum's cloud platform into data warehouses like Snowflake. 
  Data modelling to facilitate efficient use of the data warehouse and high availability of operational reports and dashboards 
  Dashboard and report design and creation in tools such as Domo or Tableau, based on business requirements 
  Implement and manage production support processes around data lifecycle, data governance, master data management, data quality, coding utilities, storage, reporting and other data integration points. 
  Assist and work closely with business stakeholders, platform development teams and data/research scientists by delivering data driven solutions to leverage company data to drive business outcomes, hence driving the growth of the Yum platform and all the brands our platform supports. 
  Participate and lead design sessions, demos, and prototype sessions, testing and training workshops with business users and other IT associates 
  
 You have: 
  
  A bachelor's degree or equivalent in Computer Science / Information Systems or a related field 
  5+ years hands-on coding SQL, PLSQL, working with databases like Postgres, Mongo, Couchbase 
  5+ years of experience with tools like Domo or Tableau 
  2+ years' experience with Kafka, Snowflake/SnowPipe, hands-on coding skills in languages like Golang, JavaScript, Python, Docker / shell scripting and hands on experience with Debezium or equivalent CDC tooling 
  3+ years' experience working on a and modelling within a massively parallel processing database (MPP) such as Snowflake 
  A collaborative attitude 
  
 We prefer experience with: 
  
  Postgres 
  Debezium 
  Kafka 
  Snowflake 
  Domo 
  JSON data structures 
  Agile methodologies 
  DevOps technologies (docker, AWS, GitLab) 
  Git 
  Microservice architecture 
  Ecommerce 
  
 Salary Range: $108,700 to $149,300 annually + bonus eligibility. This is the expected salary range for this position. Ultimately, in determining pay, we'll consider the successful candidate’s location, experience, and other job-related factors. 
 Benefits: Employees (and their eligible family members) may enroll in the following types of insurance coverage: medical, dental, vision, legal, and accidental death and dismemberment, as well as FSA/HSA (depending on enrolled medical plan). Yum! also provides short-term disability, long-term disability, and life insurance. Employees may enroll in our 401(k) plan. Yum! provides 4 weeks of vacation, paid sick leave, 10 paid holidays, a floating day off and 2 paid days for volunteer time each calendar year. To learn more about working at Yum! -Click here. 
  At Yum!, one of our core values is to Believe in ALL People. This means seeing the value in everyone and unlocking their full potential to be their best self. YUM! Brands, Inc. (including its subsidiaries Yum Restaurant Services Group, LLC (“YRSG”) and Yum Connect, LLC (“Yum Digital and Technology”)(collectively, “Yum”) is proud to be an equal opportunity employer and is committed to equity, inclusion, and belonging for all dimensions of diversity. We do not discriminate based on race, color, religion, sex, sexual orientation, gender identity, national origin, veteran status, disability status, age, or any other protected characteristic. Yum! is committed to working with and providing reasonable accommodation to applicants with disabilities or special needs. 
  US Job Seekers/Employees - Click here to view the “Know Your Rights” poster and supplement and the Pay Transparency Policy Statement.
  The Yum! Brands story is simple. We have the four distinctive, relevant and easy global brands – KFC, Pizza Hut, Taco Bell and The Habit Burger Grill - born from the hopes and dreams, ambitions and grit of passionate entrepreneurs. And we want more of this to create our future!  As the world’s largest restaurant company we have a clear and compelling mission: to build the world’s most love, trusted and fastest-growing restaurant brands. The key and not-so-secret ingredient in our recipe for growth is our unrivaled talent and culture, which fuels our results.  We’re looking for talented, motivated, visionary and team-oriented leaders to join us as we elevate and personalize the customer experience across our 48,000 restaurants, operating in 145 countries and territories around the world!  We put pizza, chicken and tacos in the hands of customers through customized ordering, unique delivery approaches, app experiences, and click and collect services and consumer data analytics creating unique customer dining experiences – and we are only getting started.  Employees may work for a single brand and potentially grow to support all company-owned brands depending on their role. Regardless of where they work, as a company opening an average of 8 restaurants a day worldwide, the growth opportunities are endless. Taco Bell has been named of the 10 Most Innovative Companies in the World by Fast Company; Pizza Hut delivers more pizzas than any other pizza company in the world and KFC’s still use its 75-year-old finger lickin’ good recipe including secret herbs and spices to hand-bread its chicken every day.  Yum! and its brands have offices in Chicago, IL, Louisville KY, Irvine, CA, Plano, TX and other markets around the world. We don’t just say we are a great place to work – our commitments to the world and our employees show it. Yum! has been named to the Dow Jones Sustainability North America Index and ranked among the top 100 Best Corporate Citizens by Corporate Responsibility Magazine in addition to being named to the Bloomberg Gender-Equality Index. Our employees work in an environment where the value of “believe in all people” is lived every day, enjoying benefits including but not limited to: 4 weeks’ vacation PLUS holidays, sick leave and 2 paid days to volunteer at the cause of their choice and a dollar-for-dollar matching gift program; generous parental leave; competitive benefits including medical, dental, vision and life insurance as well as a 6% 401k match – all encompassed in Yum!’s world-famous recognition culture.","<div>
 <p>At Yum!, we are looking for a data engineering leader to lead our dynamic and rapidly scaling team. We are searching for a curious person who thrives from being able to implement methodical solutions and dabble creatively to drive success. For maximum impact at Yum!, this data engineer must be able to partner with key stakeholders in our business to completely understand context and guide which business and technological priorities we should address using data &#x2026; and how we should go about doing so.</p> 
 <p> The lead data engineer will serve as a domain expert and will develop a rapport with our internal business partners and technologists as an essential, long-term part of the team.</p> 
 <p> We are looking for a candidate who has practical experience with technologies and languages supporting big data pipelines, streaming, ETL, modelling, governance, and reporting. Our lead data engineer will lead and provide hands-on guidance to the team developing and maintaining our platform. The lead data engineer will also make technology and business recommendations, as well as develop key insights based on data analysis and our platform&apos;s operational performance.</p> 
 <p><b> Key Responsibilities:</b></p> 
 <ul> 
  <li>Creating and engineering data pipelines for end-to-end delivery of raw and streamed datasets from Yum&apos;s cloud platform into data warehouses like Snowflake.</li> 
  <li>Data modelling to facilitate efficient use of the data warehouse and high availability of operational reports and dashboards</li> 
  <li>Dashboard and report design and creation in tools such as Domo or Tableau, based on business requirements</li> 
  <li>Implement and manage production support processes around data lifecycle, data governance, master data management, data quality, coding utilities, storage, reporting and other data integration points.</li> 
  <li>Assist and work closely with business stakeholders, platform development teams and data/research scientists by delivering data driven solutions to leverage company data to drive business outcomes, hence driving the growth of the Yum platform and all the brands our platform supports.</li> 
  <li>Participate and lead design sessions, demos, and prototype sessions, testing and training workshops with business users and other IT associates</li> 
 </ul> 
 <p><b>You have:</b></p> 
 <ul> 
  <li>A bachelor&apos;s degree or equivalent in Computer Science / Information Systems or a related field</li> 
  <li>5+ years hands-on coding SQL, PLSQL, working with databases like Postgres, Mongo, Couchbase</li> 
  <li>5+ years of experience with tools like Domo or Tableau</li> 
  <li>2+ years&apos; experience with Kafka, Snowflake/SnowPipe, hands-on coding skills in languages like Golang, JavaScript, Python, Docker / shell scripting and hands on experience with Debezium or equivalent CDC tooling</li> 
  <li>3+ years&apos; experience working on a and modelling within a massively parallel processing database (MPP) such as Snowflake</li> 
  <li>A collaborative attitude</li> 
 </ul> 
 <p><b>We prefer experience with:</b></p> 
 <ul> 
  <li>Postgres</li> 
  <li>Debezium</li> 
  <li>Kafka</li> 
  <li>Snowflake</li> 
  <li>Domo</li> 
  <li>JSON data structures</li> 
  <li>Agile methodologies</li> 
  <li>DevOps technologies (docker, AWS, GitLab)</li> 
  <li>Git</li> 
  <li>Microservice architecture</li> 
  <li>Ecommerce</li> 
 </ul> 
 <p><i>Salary Range: &#x24;108,700 to &#x24;149,300 annually + bonus eligibility. </i><b><i>This is the expected salary range for this position. Ultimately, in determining pay, we&apos;ll consider the successful candidate&#x2019;s location, experience, and other job-related factors.</i></b></p> 
 <p><b>Benefits</b>: Employees (and their eligible family members) may enroll in the following types of insurance coverage: medical, dental, vision, legal, and accidental death and dismemberment, as well as FSA/HSA (depending on enrolled medical plan). Yum! also provides short-term disability, long-term disability, and life insurance. Employees may enroll in our 401(k) plan. Yum! provides 4 weeks of vacation, paid sick leave, 10 paid holidays, a floating day off and 2 paid days for volunteer time each calendar year. To learn more about working at Yum! -Click here.</p> 
 <p> At Yum!, one of our core values is to Believe in ALL People. This means seeing the value in everyone and unlocking their full potential to be their best self. YUM! Brands, Inc. (including its subsidiaries Yum Restaurant Services Group, LLC (&#x201c;YRSG&#x201d;) and Yum Connect, LLC (&#x201c;Yum Digital and Technology&#x201d;)(collectively, &#x201c;Yum&#x201d;) is proud to be an equal opportunity employer and is committed to equity, inclusion, and belonging for all dimensions of diversity. We do not discriminate based on race, color, religion, sex, sexual orientation, gender identity, national origin, veteran status, disability status, age, or any other protected characteristic. Yum! is committed to working with and providing reasonable accommodation to applicants with disabilities or special needs.</p> 
 <p><i> US Job Seekers/Employees - </i><i>Click here </i><i>to view the &#x201c;</i>Know Your Rights<i>&#x201d; poster and supplement and the Pay Transparency Policy Statement.</i></p>
 <p><br> The Yum! Brands story is simple. We have the four distinctive, relevant and easy global brands &#x2013; KFC, Pizza Hut, Taco Bell and The Habit Burger Grill - born from the hopes and dreams, ambitions and grit of passionate entrepreneurs. And we want more of this to create our future!<br> <br> As the world&#x2019;s largest restaurant company we have a clear and compelling mission: to build the world&#x2019;s most love, trusted and fastest-growing restaurant brands. The key and not-so-secret ingredient in our recipe for growth is our unrivaled talent and culture, which fuels our results.<br> <br> We&#x2019;re looking for talented, motivated, visionary and team-oriented leaders to join us as we elevate and personalize the customer experience across our 48,000 restaurants, operating in 145 countries and territories around the world!<br> <br> We put pizza, chicken and tacos in the hands of customers through customized ordering, unique delivery approaches, app experiences, and click and collect services and consumer data analytics creating unique customer dining experiences &#x2013; and we are only getting started.<br> <br> Employees may work for a single brand and potentially grow to support all company-owned brands depending on their role. Regardless of where they work, as a company opening an average of 8 restaurants a day worldwide, the growth opportunities are endless. Taco Bell has been named of the 10 Most Innovative Companies in the World by Fast Company; Pizza Hut delivers more pizzas than any other pizza company in the world and KFC&#x2019;s still use its 75-year-old finger lickin&#x2019; good recipe including secret herbs and spices to hand-bread its chicken every day.<br> <br> Yum! and its brands have offices in Chicago, IL, Louisville KY, Irvine, CA, Plano, TX and other markets around the world. We don&#x2019;t just say we are a great place to work &#x2013; our commitments to the world and our employees show it. Yum! has been named to the Dow Jones Sustainability North America Index and ranked among the top 100 Best Corporate Citizens by Corporate Responsibility Magazine in addition to being named to the Bloomberg Gender-Equality Index. Our employees work in an environment where the value of &#x201c;believe in all people&#x201d; is lived every day, enjoying benefits including but not limited to: 4 weeks&#x2019; vacation PLUS holidays, sick leave and 2 paid days to volunteer at the cause of their choice and a dollar-for-dollar matching gift program; generous parental leave; competitive benefits including medical, dental, vision and life insurance as well as a 6% 401k match &#x2013; all encompassed in Yum!&#x2019;s world-famous recognition culture.</p>
</div>","https://jobs.yum.com/jobs/general/united-states/sr-software-engineer-i-data-engineering-lead-/2781","448b77327ddb466d",,,,"Remote","Sr. Software Engineer I (Data Engineering Lead)","22 days ago","2023-10-03T11:54:47.631Z","3.7","733","$108,700 - $149,300 a year","2023-10-25T11:54:47.632Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=448b77327ddb466d&from=jasx&tk=1hdjautnrj4ij800&vjs=3"
"Cypress Consulting","Sr. Data Center Engineer/Deployment Engineer
We are seeking a Deployment Engineer to provide advanced support, guidance and assistance to address specific customer needs. In this position, you will be working as a technology expert in the Routing & Switching space to design, implement, and support (troubleshoot) the deployments within a customer infrastructure. The ideal candidate will also have a level of comfort communicating across all functions within the organization, as well as with clients and partners. Arista configuration and CloudVision usage experience is ideal. Should be familiar with EVPN/VXLAN and proven deployment models. Should also be comfortable with documentation and tools like MS Excel/Viso and word.
Essential Functions of the Job:

 You will provide advanced post-sales support for a large Data Center networking deployment for a large customer.
 Review customer network designs for an EVPN, VxLAN, leaf-spine architecture and make recommendations for deployment
 Migrate or interconnect to/from Cisco, Juniper and other vendors to Arista infrastructure
 Assist with implementation and change controls
 You will assist with proof of concepts (POC) and in-depth testing to validate design scenario
 Provide interface to TAC and internal development teams and the customer
 Designing Network solutions utilizing extensive experience with routing protocols including advanced BGP design as well as extensive EVPN/VXLAN experience.
 Establish and maintaining strong relationships with key partners
 Continue training to maintain expertise
 Ability to understand the client’s business objectives and technical needs
 Ability to meet Service Level Agreements (SLAs) for sales and clients
 Regularly exercises discretion and independent judgment
 Maintain professional relationships with teammates, partners, and clients
 Some travel may be required within assigned territory

Qualifications
Required Skills and Experience

 Bachelor’s degree in Computer Science or equivalent years of experience
 5+ years’ working experience with network technologies including network design and deployments of Campus and Data Center networks. Knowledge of leaf-spine architectures highly desired.
 5+ Years’ minimum experience with Cisco-based technologies focusing on infrastructure and voice
 Experience with Cisco enterprise routing/switching within large data center enterprise customers (Catalyst, Nexus, ASR)
 Expert knowledge in the following areas: Ethernet, VLANs, VxLAN, EVPN, IP Routing, TCP/IP, OSPF, BGP, eBGP, Multicast, QoS
 Background in Perl, Python, Scripting for creating network automation is highly desired
 Excellent customer service and verbal communication skills
 Excellent written skills and the ability to do related documentation and ticket tracking of opportunities/meeting follow-up

Job Type: Full-time
Pay: $115.00 - $125.00 per hour
Benefits:

 Dental insurance
 Health insurance
 Paid time off
 Professional development assistance
 Referral program
 Retirement plan
 Vision insurance

Schedule:

 8 hour shift
 Day shift
 Monday to Friday

Experience:

 Layer 2/3 switching and routing: 2 years (Required)
 Data Center Design: 3 years (Required)
 BGP, SD-WAN, VXLAN: 3 years (Required)
 Consultative/Customer-facing: 2 years (Required)

Work Location: Remote","<p><b>Sr. Data Center Engineer/Deployment Engineer</b></p>
<p>We are seeking a Deployment Engineer to provide advanced support, guidance and assistance to address specific customer needs. In this position, you will be working as a technology expert in the Routing &amp; Switching space to design, implement, and support (troubleshoot) the deployments within a customer infrastructure. The ideal candidate will also have a level of comfort communicating across all functions within the organization, as well as with clients and partners. Arista configuration and CloudVision usage experience is ideal. Should be familiar with EVPN/VXLAN and proven deployment models. Should also be comfortable with documentation and tools like MS Excel/Viso and word.</p>
<p><b>Essential Functions of the Job:</b></p>
<ul>
 <li>You will provide advanced post-sales support for a large Data Center networking deployment for a large customer.</li>
 <li>Review customer network designs for an EVPN, VxLAN, leaf-spine architecture and make recommendations for deployment</li>
 <li>Migrate or interconnect to/from Cisco, Juniper and other vendors to Arista infrastructure</li>
 <li>Assist with implementation and change controls</li>
 <li>You will assist with proof of concepts (POC) and in-depth testing to validate design scenario</li>
 <li>Provide interface to TAC and internal development teams and the customer</li>
 <li>Designing Network solutions utilizing extensive experience with routing protocols including advanced BGP design as well as extensive EVPN/VXLAN experience.</li>
 <li>Establish and maintaining strong relationships with key partners</li>
 <li>Continue training to maintain expertise</li>
 <li>Ability to understand the client&#x2019;s business objectives and technical needs</li>
 <li>Ability to meet Service Level Agreements (SLAs) for sales and clients</li>
 <li>Regularly exercises discretion and independent judgment</li>
 <li>Maintain professional relationships with teammates, partners, and clients</li>
 <li>Some travel may be required within assigned territory</li>
</ul>
<p><b>Qualifications</b></p>
<p><b>Required Skills and Experience</b></p>
<ul>
 <li>Bachelor&#x2019;s degree in Computer Science or equivalent years of experience</li>
 <li>5+ years&#x2019; working experience with network technologies including network design and deployments of Campus and Data Center networks. Knowledge of leaf-spine architectures highly desired.</li>
 <li>5+ Years&#x2019; minimum experience with Cisco-based technologies focusing on infrastructure and voice</li>
 <li>Experience with Cisco enterprise routing/switching within large data center enterprise customers (Catalyst, Nexus, ASR)</li>
 <li>Expert knowledge in the following areas: Ethernet, VLANs, VxLAN, EVPN, IP Routing, TCP/IP, OSPF, BGP, eBGP, Multicast, QoS</li>
 <li>Background in Perl, Python, Scripting for creating network automation is highly desired</li>
 <li>Excellent customer service and verbal communication skills</li>
 <li>Excellent written skills and the ability to do related documentation and ticket tracking of opportunities/meeting follow-up</li>
</ul>
<p>Job Type: Full-time</p>
<p>Pay: &#x24;115.00 - &#x24;125.00 per hour</p>
<p>Benefits:</p>
<ul>
 <li>Dental insurance</li>
 <li>Health insurance</li>
 <li>Paid time off</li>
 <li>Professional development assistance</li>
 <li>Referral program</li>
 <li>Retirement plan</li>
 <li>Vision insurance</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
 <li>Day shift</li>
 <li>Monday to Friday</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Layer 2/3 switching and routing: 2 years (Required)</li>
 <li>Data Center Design: 3 years (Required)</li>
 <li>BGP, SD-WAN, VXLAN: 3 years (Required)</li>
 <li>Consultative/Customer-facing: 2 years (Required)</li>
</ul>
<p>Work Location: Remote</p>",,"019fe648ac2dfa54",,"Full-time",,"Remote","Sr. Data Center Design Engineer/Deployment Engineer -100% Remote","18 days ago","2023-10-07T11:54:59.988Z",,,"$115 - $125 an hour","2023-10-25T11:54:59.991Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=019fe648ac2dfa54&from=jasx&tk=1hdjautnrj4ij800&vjs=3"
"Accrete.AI","The U.S. Government agencies we work with have contracts that require all personnel working on their corresponding contracts to have U.S. citizenship – do you meet this requirement?
  
  
 Accrete is looking for a Data Engineer that will be responsible for supporting production data pipelines, developing the foundation for the Accrete data lake, and implementing best practices from data engineering at Accrete. This will support new and existing applications running on Linux and Windows operating systems in private and public cloud infrastructures. The Data Engineering team at Accrete designs, develops, and maintains data pipelines, batch data analytics, and data stores of various kinds, including analytics and stores in support of artificial intelligence workloads for Accrete AI systems and applications. 
  
 Accrete is an AI prime defense contractor with the U.S. government that creates AI software, enabling its customers to make better decisions, faster. Accrete is on a mission to create AI so powerful it amplifies human reasoning and enables enterprises to grow in previously unimaginable ways. Prior to launching Accrete in 2017, Prashant Bhuyan, Accrete’s Founder and CEO, spent over a decade in high-frequency trading where he and a core team experimented with and developed AI technology that ultimately became the early underpinnings of Accrete.
  Accrete’s solutions enable the Department of Defense to predict covert behavior from foreign adversaries seeking to influence the supply chain; the U.S. Air Force to identify vulnerabilities in microprocessor firmware; major music labels to identify superstars before competitors; auto dealers to automatically generate marketing content from vehicle feature lists; employee benefits brokers to identify the shortest path to the hottest leads; and more. 
  To learn more about Accrete, please visit our website: Accrete.ai 
  
 Responsibilities: 
 
  Design, develop, and support specific scalable pipelines for the movement of data between systems.
   Provide technical guidance in software design and development activities.
   Supervising and overseeing aspects of data engineering on multiple work streams.
   Recommend new technologies to ensure quality and productivity.
   Work closely with other teams, tech leads, architects, and Product Management to bring new data-backed products to market.
  
  
 Requirements: 
 
   2-3+ years of expertise with a data-oriented language, such as Python, C, Rust, Java, Go.
   2-3+ years experience in cloud computing, with AWS, Azure or GCP
   2-3+ years experience designing and building scalable, reliable data pipelines using the following technologies: Hadoop, Hive, Spark, Kafka, Airflow, or similar services
   Experiencing designing for and implementing in cloud-native systems
   Expertise in matching APIs, data access patterns, and data storage formats
   Functional knowledge of event processing systems such as Apache Kafka or similar
   Functional knowledge of the following databases or equivalent: Postgres, MongoDB, Neo4J, Redis
   High collaborative, values mentoring, teaching, pair-programming, and teaching software development best practices to other engineers
   Excellent problem-solving and critical-thinking skills
   Ability to build and maintain effective, trusting partnerships with product managers, architects, and technical leads for individual products and applications
   Excellent communication and leadership skills
  
  
 The base salary range for this role is $135,000 to 140,000. 
  
 Benefits: 
 We offer a competitive salary, benefits package, and opportunities for growth and advancement within the company. If you are an innovative and results-driven leader, we encourage you to apply for this exciting opportunity. 
  Accrete is an equal opportunity/affirmative action employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected Veteran status, age, or any other characteristic protected by law.","<div>
 <ul>
  <li><i>The U.S. Government agencies we work with have contracts that require all personnel working on their corresponding contracts to have U.S. citizenship &#x2013; do you meet this requirement?</i></li>
 </ul> 
 <p></p> 
 <p>Accrete is looking for a Data Engineer that will be responsible for supporting production data pipelines, developing the foundation for the Accrete data lake, and implementing best practices from data engineering at Accrete. This will support new and existing applications running on Linux and Windows operating systems in private and public cloud infrastructures. The Data Engineering team at Accrete designs, develops, and maintains data pipelines, batch data analytics, and data stores of various kinds, including analytics and stores in support of artificial intelligence workloads for Accrete AI systems and applications.</p> 
 <p></p> 
 <p>Accrete is an AI prime defense contractor with the U.S. government that creates AI software, enabling its customers to make better decisions, faster. Accrete is on a mission to create AI so powerful it amplifies human reasoning and enables enterprises to grow in previously unimaginable ways. Prior to launching Accrete in 2017, Prashant Bhuyan, Accrete&#x2019;s Founder and CEO, spent over a decade in high-frequency trading where he and a core team experimented with and developed AI technology that ultimately became the early underpinnings of Accrete.</p>
 <p><br> Accrete&#x2019;s solutions enable the Department of Defense to predict covert behavior from foreign adversaries seeking to influence the supply chain; the U.S. Air Force to identify vulnerabilities in microprocessor firmware; major music labels to identify superstars before competitors; auto dealers to automatically generate marketing content from vehicle feature lists; employee benefits brokers to identify the shortest path to the hottest leads; and more.</p> 
 <p> To learn more about Accrete, please visit our website: Accrete.ai</p> 
 <p></p> 
 <p><b>Responsibilities: </b></p>
 <ul>
  <li>Design, develop, and support specific scalable pipelines for the movement of data between systems.</li>
  <li> Provide technical guidance in software design and development activities.</li>
  <li> Supervising and overseeing aspects of data engineering on multiple work streams.</li>
  <li> Recommend new technologies to ensure quality and productivity.</li>
  <li> Work closely with other teams, tech leads, architects, and Product Management to bring new data-backed products to market.</li>
 </ul> 
 <p></p> 
 <p><b>Requirements:</b></p> 
 <ul>
  <li> 2-3+ years of expertise with a data-oriented language, such as Python, C, Rust, Java, Go.</li>
  <li> 2-3+ years experience in cloud computing, with AWS, Azure or GCP</li>
  <li> 2-3+ years experience designing and building scalable, reliable data pipelines using the following technologies: Hadoop, Hive, Spark, Kafka, Airflow, or similar services</li>
  <li> Experiencing designing for and implementing in cloud-native systems</li>
  <li> Expertise in matching APIs, data access patterns, and data storage formats</li>
  <li> Functional knowledge of event processing systems such as Apache Kafka or similar</li>
  <li> Functional knowledge of the following databases or equivalent: Postgres, MongoDB, Neo4J, Redis</li>
  <li> High collaborative, values mentoring, teaching, pair-programming, and teaching software development best practices to other engineers</li>
  <li> Excellent problem-solving and critical-thinking skills</li>
  <li> Ability to build and maintain effective, trusting partnerships with product managers, architects, and technical leads for individual products and applications</li>
  <li> Excellent communication and leadership skills</li>
 </ul> 
 <p></p> 
 <p>The base salary range for this role is &#x24;135,000 to 140,000.</p> 
 <p></p> 
 <p><b>Benefits: </b></p>
 <p><i>We offer a competitive salary, benefits package, and opportunities for growth and advancement within the company. If you are an innovative and results-driven leader, we encourage you to apply for this exciting opportunity.</i></p> 
 <p><i> Accrete is an equal opportunity/affirmative action employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity, sexual orientation, race, color, religion, national origin, disability, protected Veteran status, age, or any other characteristic protected by law.</i></p>
</div>","https://accrete-ai.breezy.hr/p/109d8851fd2b-data-engineer?","23a44fa83da2854b",,"Full-time",,"Remote","Data Engineer","27 days ago","2023-09-28T11:54:47.619Z",,,"$130,000 - $140,000 a year","2023-10-25T11:54:47.621Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=23a44fa83da2854b&from=jasx&tk=1hdjautnrj4ij800&vjs=3"
"iRhythm Technologies","About Us: 
  iRhythm is a leading digital healthcare company focused on the way cardiac arrhythmias are clinically diagnosed by combining our wearable biosensing technology with powerful cloud-based data analytics and machine-learning capabilities. Our goal is to be the leading provider of first-line ambulatory ECG monitoring for patients at risk for arrhythmias. iRhythm's continuous ambulatory monitoring has already put over 6 million patients and their doctors on a shorter path to what they both need – answers. 
  About this Role: 
  As a Sr Research Data Engineer within the iRhythm Research & Systems Engineering team, you will be exposed to a diverse set of engineering problems spanning Machine Learning, Big Data, Multi-Modal data, Data Privacy, AWS, Batch Processing and Mobile technologies in the context of addressing unmet clinical needs. You will have the opportunity to work with a talented team to develop a deep understanding of our end-to-end systems, helping to drive successful delivery of software projects throughout its development life cycle. 
  Note: This role will work remote from a US based home office. We are unable to offer any sort of sponsorship for this role. 
  About You 
  
  You are a quality-minded individual with ability to go detail-oriented and scientific while having the big picture in mind 
  You enjoy investigative and troubleshooting processes using your excellent analytical and problem-solving skills, along with a strong sense of urgency 
  You are self-motivated and demonstrate initiative in helping others 
  You are independent and thrive in the face of ambiguous, open-ended challenges 
  You have effective presentation and interpersonal skills communicating technical information to a variety of stakeholders 
  You enjoy collaborating with cross functional groups 
  
 Responsibilities 
  
  Work with engineering and cross-functional stakeholders to research, define, and write technical requirements, figure out data or operational workflows, and plan for V&V/release activities 
  Design and code pipelines that efficiently transform our raw data into formats and structures that best serve our researchers (Python and algorithm design) 
  Perform exploratory research with physiological data, with good software practices in mind 
  Develop, test, and document proof-of-concept algorithms and/or software tools 
  Participate and support project management, software development, and software QA in the software development life cycle and regulatory submissions as system expert 
  Update and maintain Design History File, participate in risk management/hazards analysis activities 
  Develop, document, and monitor testing regimes that ensure performant code and data integrity 
  Maintain a clean and well-documented code base and audit trail (git, Bitbucket, Confluence, JIRA) 
  
 Basic Qualifications 
  
  Combined 5+ years of algorithm/software development and systems engineering experience in the Healthcare, Medical Device, or other regulated software industry. 
  Software development experience with one or more of Python, Java, C#, or C/C++ 
  Understanding of statistics and proficiency with data analysis. Proficiency in Linux/Unix/Windows environments 
  BA/BS in science or technical field 
  Experience processing and integrating developmental data, clinical study data, EHR, and claims data. 
  Experience selecting and organizing data for training and validating algorithms, managing constraints and requirements. 
  Demonstrated ability to work with structured and unstructured data across multiple modalities. 
  Hands on experience with technologies leveraging multiple servers for data intensive tasks. 
  Experience with software development in Python, including libraries such as pandas and scikit-learn 
  Experience creating data pipelines that transform raw data into insights. 
  Hands on Experience on Data Analytics Services including Athena, Glue Data Catalog and Quicksight 
  Accessing and parsing data from S3 through python API calls 
  Experience and familiarity with AWS Sagemaker 
  Extract, transform and load data from different formats like JSON, databases and integrate results for algorithm training and validation 
  Database familiarity including Oracle, MySQL, Redshift, DynamoDB and Elastic Cache 
  
 Desired/Preferred Technical Qualifications 
  
  MS/PhD in science or technical field 
  Experience with FDA 21 CFR part 820 and IEC standards 62304, 14971, and 62366 
  Experience working with large amounts of data and tools to handle them e.g. AWS, Spark, SQL 
  Working knowledge of signal processing 
  Experience using or managing Splunk, Tableau 
  
 
  What's in it for you: 
   This is a full-time position with competitive compensation package, excellent benefits including medical, dental, and vision insurance (all of which start on your first day), paid holidays, and PTO! 
   iRhythm also provides additional benefits including 401K (with company match), an Employee Stock Purchase Plan, paid parental leave, pet insurance discount, Cultural Committee/Charity events, and so much more! 
   
   FLSA Status: Exempt 
    As a part of our core values, we ensure a diverse and inclusive workforce. We welcome and celebrate people of all backgrounds, experiences, skills, and perspectives. iRhythm Technologies, Inc. is an Equal Opportunity Employer (M/F/V/D). We will consider for employment all qualified applicants with arrest and conviction records in accordance with all applicable laws. 
    Make iRhythm your path forward. 
    #LI-AR1 
    #LI-Remote 
   
 
 
 
  
   Actual compensation may vary depending on job-related factors including knowledge, skills, experience, and work location.
  
  
   Estimated Pay Range
  
    $119,800—$174,500 USD","<div>
 <p><b>About Us:</b></p> 
 <p> iRhythm is a leading digital healthcare company focused on the way cardiac arrhythmias are clinically diagnosed by combining our wearable biosensing technology with powerful cloud-based data analytics and machine-learning capabilities. Our goal is to be the leading provider of first-line ambulatory ECG monitoring for patients at risk for arrhythmias. iRhythm&apos;s continuous ambulatory monitoring has already put over 6 million patients and their doctors on a shorter path to what they both need &#x2013; answers.</p> 
 <p><b> About this Role:</b></p> 
 <p> As a<b> Sr Research Data Engineer </b>within the iRhythm Research &amp; Systems Engineering team, you will be exposed to a diverse set of engineering problems spanning Machine Learning, Big Data, Multi-Modal data, Data Privacy, AWS, Batch Processing and Mobile technologies in the context of addressing unmet clinical needs. You will have the opportunity to work with a talented team to develop a deep understanding of our end-to-end systems, helping to drive successful delivery of software projects throughout its development life cycle.</p> 
 <p> Note: This role will work remote from a US based home office. We are unable to offer any sort of sponsorship for this role.</p> 
 <p><b> About You</b></p> 
 <ul> 
  <li>You are a quality-minded individual with ability to go detail-oriented and scientific while having the big picture in mind</li> 
  <li>You enjoy investigative and troubleshooting processes using your excellent analytical and problem-solving skills, along with a strong sense of urgency</li> 
  <li>You are self-motivated and demonstrate initiative in helping others</li> 
  <li>You are independent and thrive in the face of ambiguous, open-ended challenges</li> 
  <li>You have effective presentation and interpersonal skills communicating technical information to a variety of stakeholders</li> 
  <li>You enjoy collaborating with cross functional groups</li> 
 </ul> 
 <p><b>Responsibilities</b></p> 
 <ul> 
  <li>Work with engineering and cross-functional stakeholders to research, define, and write technical requirements, figure out data or operational workflows, and plan for V&amp;V/release activities</li> 
  <li>Design and code pipelines that efficiently transform our raw data into formats and structures that best serve our researchers (Python and algorithm design)</li> 
  <li>Perform exploratory research with physiological data, with good software practices in mind</li> 
  <li>Develop, test, and document proof-of-concept algorithms and/or software tools</li> 
  <li>Participate and support project management, software development, and software QA in the software development life cycle and regulatory submissions as system expert</li> 
  <li>Update and maintain Design History File, participate in risk management/hazards analysis activities</li> 
  <li>Develop, document, and monitor testing regimes that ensure performant code and data integrity</li> 
  <li>Maintain a clean and well-documented code base and audit trail (git, Bitbucket, Confluence, JIRA)</li> 
 </ul> 
 <p><b>Basic Qualifications</b></p> 
 <ul> 
  <li>Combined 5+ years of algorithm/software development and systems engineering experience in the Healthcare, Medical Device, or other regulated software industry.</li> 
  <li>Software development experience with one or more of Python, Java, C#, or C/C++</li> 
  <li>Understanding of statistics and proficiency with data analysis. Proficiency in Linux/Unix/Windows environments</li> 
  <li>BA/BS in science or technical field</li> 
  <li>Experience processing and integrating developmental data, clinical study data, EHR, and claims data.</li> 
  <li>Experience selecting and organizing data for training and validating algorithms, managing constraints and requirements.</li> 
  <li>Demonstrated ability to work with structured and unstructured data across multiple modalities.</li> 
  <li>Hands on experience with technologies leveraging multiple servers for data intensive tasks.</li> 
  <li>Experience with software development in Python, including libraries such as pandas and scikit-learn</li> 
  <li>Experience creating data pipelines that transform raw data into insights.</li> 
  <li>Hands on Experience on Data Analytics Services including Athena, Glue Data Catalog and Quicksight</li> 
  <li>Accessing and parsing data from S3 through python API calls</li> 
  <li>Experience and familiarity with AWS Sagemaker</li> 
  <li>Extract, transform and load data from different formats like JSON, databases and integrate results for algorithm training and validation</li> 
  <li>Database familiarity including Oracle, MySQL, Redshift, DynamoDB and Elastic Cache</li> 
 </ul> 
 <p><b>Desired/Preferred Technical Qualifications</b></p> 
 <ul> 
  <li>MS/PhD in science or technical field</li> 
  <li>Experience with FDA 21 CFR part 820 and IEC standards 62304, 14971, and 62366</li> 
  <li>Experience working with large amounts of data and tools to handle them e.g. AWS, Spark, SQL</li> 
  <li>Working knowledge of signal processing</li> 
  <li>Experience using or managing Splunk, Tableau</li> 
 </ul> 
 <div>
  <p><b>What&apos;s in it for you:</b></p> 
  <p> This is a full-time position with competitive compensation package, excellent benefits including medical, dental, and vision insurance (all of which start on your first day), paid holidays, and PTO!</p> 
  <p> iRhythm also provides additional benefits including 401K (with company match), an Employee Stock Purchase Plan, paid parental leave, pet insurance discount, Cultural Committee/Charity events, and so much more!</p> 
  <div> 
   <p><b>FLSA Status: </b>Exempt</p> 
   <p><i> As a part of our core values, we ensure a diverse and inclusive workforce. We welcome and celebrate people of all backgrounds, experiences, skills, and perspectives. iRhythm Technologies, Inc. is an Equal Opportunity Employer (M/F/V/D). We will consider for employment all qualified applicants with arrest and conviction records in accordance with all applicable laws.</i></p> 
   <p> Make iRhythm your path forward.</p> 
   <p> #LI-AR1</p> 
   <p> #LI-Remote</p> 
  </div> 
 </div>
 <p></p>
 <div>
  <div>
   <p>Actual compensation may vary depending on job-related factors including knowledge, skills, experience, and work location.</p>
  </div>
  <p></p>
  <p><b><br> Estimated Pay Range</b></p>
  <div>
    &#x24;119,800&#x2014;&#x24;174,500 USD
  </div>
 </div>
</div>","https://www.irhythmtech.com/company/job-openings?gh_jid=5400674&gh_src=c3f698341us","700c104561e30304",,"Full-time",,"Deerfield, IL","Senior Research Data Engineer","19 days ago","2023-10-06T11:54:48.485Z","3.2","94","$119,800 - $174,500 a year","2023-10-25T11:54:48.517Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=700c104561e30304&from=jasx&tk=1hdjautnrj4ij800&vjs=3"
"Pax8","Pax8 is the leading cloud-based technology marketplace, simplifying the cloud journey for our partners by integrating technology, business intelligence and proactive service to deliver an unparalleled experience. Serving thousands of partners through the indirect sales channel, our mission is to be the world's favorite place to buy cloud products. We are a fast-growing, dynamic and high-energy startup organization, allowing you to make a meaningful impact on the business. Culture is important to us, and at Pax8, it's business, and it 
   IS personal. We are passionate, creative and humorously offbeat. We work hard, keep it fun, and expect the best.
   
   
  
   We Elev8 each other. We Advoc8 for our partners. We Innov8 continuously. We Celebr8 life.
   
   No matter who you are, Pax8 is a place you can call home. We know there's no such thing as a ""perfect"" candidate, so we don't look for the right ""fit"" – instead, we look for the add. We encourage you to apply for a role at Pax8 even if you don't meet 100% of the bullet points. We believe in cultivating an environment with a diversity of perspectives, in hopes that we can all thrive in an inclusive environment. 
   We are only as great as our people. And we have great people all over the world. No matter where you live and work, you're a part of the Pax8 team. This means embracing hybrid- and remote-work whenever possible.
 
  Position Summary: 
 Are you a wizard with Data? At Pax8, we are looking to the future and how data will help get us there. You will join our growing Data Science team who are committed to the creation and expansion of clean accessible data to our internal Pax8 partners. 
  As a Software Engineer - Data, you will work amongst a collaborative team working with data from inception to modeling.This team works with ETL processes in order to pull data from across various data sources and consolidate it into our newly created and centralized Redshift data warehouse. The data you will work with and be creating a central point of truth for will aid several different Pax8 teams stretching across the organization that will be aimed at improving our customers' experience within our marketplace. 
  For you to successfully perform in this role, you will need to be equipped with experience to run with data, end-to-end, including everything from inception to modeling. Strong Python and SQL skills will be needed to navigate and deliver on the architectural areas and testing. You'll work with a vast amount of data, one crucial area will be within real-time streaming data harnessed by the use of Kafka. Having a solutions mindset will also be a key element to your contribution to our team approach to projects. 
  Meet the team and hear what they have to say about our Product & Engineering group: Meet the Team 
  Essential Responsibilities: 
  
  Builds pipelines to ingest new data sources 
  Transforms data to support varied use cases 
  Includes testing in all aspects of the development process 
  Mentors junior and mid-level Engineers 
  Optimizes existing data pipelines and improves existing code quality 
  Makes updates and improvements to deployment processes 
  Participates in project planning and architecture discussions 
  Analyzes potential problems and finds solutions to pressing data issues 
  Participates in on-call rotation 
  
 Ideal Skills, Experience, and Competencies: 
  
  At least Four (4) years of relevant data engineering experience 
  Advanced experience with Python 
  Expert experience with SQL 
  Intermediate experience with a JVM language 
  Exposure to other software development languages 
  Advanced experience with Apache Spark or other distributed processing engines 
  Advanced experience with Apache Kafka or other stream processing frameworks 
  Intermediate experience with Terraform, Docker, Kubernetes, or other similar infrastructure tooling 
  Advanced experience with cloud data tools such as S3, Glue, and Athena 
  Intermediate experience with building CI/CD pipelines 
  Effective problem solving and troubleshooting abilities 
  Ability to consistently achieve results, even under tough circumstances 
  Effective technical leadership abilities 
  Excellent verbal and written communication skills 
  Experience with innovative application design and implementation 
  Ability to make sense of complex, high quantity, and sometimes contradictory information to effectively solve problems 
  
 Required Education & Certifications: 
  
  B.A./B.S. in related field or equivalent work experience 
 
 
  M.S./M.A. in related field or equivalent work experience 
 
 Compensation: 
 
  
   
    Qualified candidates can expect a salary beginning at $140,000 or more depending on experience 
    
  
 
 #LI-Remote #LI-JF1 #Dice-J #BI-Remote
 
  
   Note: Compensation is benchmarked on local Denver Metro area market rates. Qualified candidates in other locations can expect a salary package that may be adjusted based off applicable cost of wages in their respective location.
   
  
  
   At Pax8 we believe that your Total Rewards should include a benefits package that shows how much we value our greatest assets. All 
   FTE Pax8 people enjoy the following benefits:
   
  
   Non-Commissioned Bonus Plans or Variable Commission 
   401(k) plan with employer match 
   Medical, Dental & Vision Insurance 
   Employee Assistance Program 
   Employer Paid Short & Long Term Disability, Life and AD&D Insurance 
   Flexible, Open Vacation 
   Paid Sick Time Off 
   Extended Leave for Life events 
   RTD Eco Pass (For local Colorado Employees) 
   Career Development Programs 
   Stock Option Eligibility 
   Employee-led Resource Groups 
   
  
  
   Pax8 is an EEOC Employer.","<div>
 <div>
  <div>
   Pax8 is the leading cloud-based technology marketplace, simplifying the cloud journey for our partners by integrating technology, business intelligence and proactive service to deliver an unparalleled experience. Serving thousands of partners through the indirect sales channel, our mission is to be the world&apos;s favorite place to buy cloud products. We are a fast-growing, dynamic and high-energy startup organization, allowing you to make a meaningful impact on the business. Culture is important to us, and at Pax8, it&apos;s business, and it 
   <b>IS</b> personal. We are passionate, creative and humorously offbeat. We work hard, keep it fun, and expect the best.
  </div> 
  <div></div> 
  <div>
   We Elev8 each other. We Advoc8 for our partners. We Innov8 continuously. We Celebr8 life.
  </div> 
  <p> No matter who you are, Pax8 is a place you can call home. We know there&apos;s no such thing as a <i>&quot;perfect&quot;</i> candidate, so we don&apos;t look for the right &quot;<i>fit</i>&quot; &#x2013; instead, we look for the add. We encourage you to apply for a role at Pax8 even if you don&apos;t meet 100% of the bullet points. We believe in cultivating an environment with a diversity of perspectives, in hopes that we can all thrive in an inclusive environment.</p> 
  <p> We are only as great as our people. And we have great people all over the world. No matter where you live and work, you&apos;re a part of the Pax8 team. This means embracing hybrid- and remote-work whenever possible.</p>
 </div>
 <p><b> Position Summary: </b></p>
 <p>Are you a wizard with Data? At Pax8, we are looking to the future and how data will help get us there. You will join our growing Data Science team who are committed to the creation and expansion of clean accessible data to our internal Pax8 partners.</p> 
 <p> As a Software Engineer - Data, you will work amongst a collaborative team working with data from inception to modeling.This team works with ETL processes in order to pull data from across various data sources and consolidate it into our newly created and centralized Redshift data warehouse. The data you will work with and be creating a central point of truth for will aid several different Pax8 teams stretching across the organization that will be aimed at improving our customers&apos; experience within our marketplace.</p> 
 <p> For you to successfully perform in this role, you will need to be equipped with experience to run with data, end-to-end, including everything from inception to modeling. Strong Python and SQL skills will be needed to navigate and deliver on the architectural areas and testing. You&apos;ll work with a vast amount of data, one crucial area will be within real-time streaming data harnessed by the use of Kafka. Having a solutions mindset will also be a key element to your contribution to our team approach to projects.</p> 
 <p> Meet the team and hear what they have to say about our Product &amp; Engineering group: Meet the Team</p> 
 <p><b> Essential Responsibilities:</b></p> 
 <ul> 
  <li>Builds pipelines to ingest new data sources</li> 
  <li>Transforms data to support varied use cases</li> 
  <li>Includes testing in all aspects of the development process</li> 
  <li>Mentors junior and mid-level Engineers</li> 
  <li>Optimizes existing data pipelines and improves existing code quality</li> 
  <li>Makes updates and improvements to deployment processes</li> 
  <li>Participates in project planning and architecture discussions</li> 
  <li>Analyzes potential problems and finds solutions to pressing data issues</li> 
  <li>Participates in on-call rotation</li> 
 </ul> 
 <p><b>Ideal Skills, Experience, and Competencies:</b></p> 
 <ul> 
  <li>At least Four (4) years of relevant data engineering experience</li> 
  <li>Advanced experience with Python</li> 
  <li>Expert experience with SQL</li> 
  <li>Intermediate experience with a JVM language</li> 
  <li>Exposure to other software development languages</li> 
  <li>Advanced experience with Apache Spark or other distributed processing engines</li> 
  <li>Advanced experience with Apache Kafka or other stream processing frameworks</li> 
  <li>Intermediate experience with Terraform, Docker, Kubernetes, or other similar infrastructure tooling</li> 
  <li>Advanced experience with cloud data tools such as S3, Glue, and Athena</li> 
  <li>Intermediate experience with building CI/CD pipelines</li> 
  <li>Effective problem solving and troubleshooting abilities</li> 
  <li>Ability to consistently achieve results, even under tough circumstances</li> 
  <li>Effective technical leadership abilities</li> 
  <li>Excellent verbal and written communication skills</li> 
  <li>Experience with innovative application design and implementation</li> 
  <li>Ability to make sense of complex, high quantity, and sometimes contradictory information to effectively solve problems</li> 
 </ul> 
 <p><b>Required Education &amp; Certifications:</b></p> 
 <ul> 
  <li>B.A./B.S. in related field or equivalent work experience</li> 
 </ul>
 <ul>
  <li>M.S./M.A. in related field or equivalent work experience</li> 
 </ul>
 <h4 class=""jobSectionHeader""><b>Compensation:</b></h4> 
 <div>
  <div>
   <ul>
    <li>Qualified candidates can expect a salary beginning at &#x24;140,000 or more depending on experience</li> 
   </ul> 
  </div>
 </div>
 <p>#LI-Remote #LI-JF1 #Dice-J #BI-Remote</p>
 <div>
  <ul>
   <li>Note: Compensation is benchmarked on local Denver Metro area market rates. Qualified candidates in other locations can expect a salary package that may be adjusted based off applicable cost of wages in their respective location.</li>
  </ul> 
  <p></p>
  <div>
   At Pax8 we believe that your Total Rewards should include a benefits package that shows how much we value our greatest assets. All 
   <b>FTE</b> Pax8 people enjoy the following benefits:
  </div> 
  <ul>
   <li>Non-Commissioned Bonus Plans or Variable Commission</li> 
   <li>401(k) plan with employer match</li> 
   <li>Medical, Dental &amp; Vision Insurance</li> 
   <li>Employee Assistance Program</li> 
   <li>Employer Paid Short &amp; Long Term Disability, Life and AD&amp;D Insurance</li> 
   <li>Flexible, Open Vacation</li> 
   <li>Paid Sick Time Off</li> 
   <li>Extended Leave for Life events</li> 
   <li>RTD Eco Pass (For local Colorado Employees)</li> 
   <li>Career Development Programs</li> 
   <li>Stock Option Eligibility</li> 
   <li>Employee-led Resource Groups</li> 
  </ul> 
  <p></p>
  <div>
   Pax8 is an EEOC Employer.
  </div>
 </div>
</div>","https://www.pax8.com/en-us/careers/job-openings/?gh_jid=4992842004&gh_src=3d3b51de4us","a16f0422b86ff2dc",,"Full-time",,"Remote","Sr. Software Engineer - Data","15 days ago","2023-10-10T11:54:53.804Z","3","20",,"2023-10-25T11:54:53.806Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=a16f0422b86ff2dc&from=jasx&tk=1hdjautnrj4ij800&vjs=3"
"CrowdStrike","#WeAreCrowdStrike and our mission is to stop breaches. As a global leader in cybersecurity, our team changed the game. Since our inception, our market leading cloud-native platform has offered unparalleled protection against the most sophisticated cyberattacks. We work on large scale distributed systems, processing over 1 trillion events a day with a petabyte of RAM deployed in our Cassandra clusters - and this traffic is growing daily. We’re looking for people with limitless passion, a relentless focus on innovation and a fanatical commitment to developing and shaping our cybersecurity platform. Consistently recognized as a top workplace, CrowdStrike is committed to cultivating an inclusive, remote-first culture that offers people the autonomy and flexibility to balance the needs of work and life while taking their career to the next level. Interested in working for a company that sets the standard and leads with integrity? Join us on a mission that matters - one team, one fight.
 
 
 
   About the Role
   CrowdStrike is looking to hire a Senior Engineer to the Data Services team to help us take our database systems to the next level. We’re looking for a highly-technical, hands-on engineer, who loves to work with data plane services like Cassandra, ElasticSearch, and Kafka, and is comfortable building automation around large-scale cloud-based critical systems. We’ll be looking at candidate CVs with an eye on achievement - what you’ve accomplished in the past tells us the most about what you can do for us in the future.
 
 
   What You'll Do:
 
 
  
   
     Maintain a deep understanding of the data components - including Cassandra, ElasticSearch, Kafka, Zookeeper, and Spark, and use that understanding to operate and automate properly configured clusters.
   
  
   
     Work with Engineering to roll out new products and features.
   
  
   
     Develop infrastructure services to support the CrowdStrike engineering team’s pursuit of a full devops model.
   
  
   
     Work closely with Engineering and Customer Support to troubleshoot time-sensitive production issues, regardless of when they happen.
   
  
   
     Keep petabytes of critical business data safe, secure, and available.
   
 
 
 
   What You’ll Need:
 
 
  
   
     Experience with large scale datastores using technologies like Cassandra, ElasticSearch, Kafka, Zookeeper, and Spark.
   
  
   
     Experience with large-scale, business-critical Linux environments
   
  
   
     Experience operating within the cloud, preferably Amazon Web Services
   
  
   
     Proven ability to work effectively with both local and remote teams
   
  
   
     Track record of making great decisions, particularly when it matters most
   
  
   
     Rock solid communication skills, verbal and written
   
  
   
     A combination of confidence and independence... with the prudence to know when to ask for help from the rest of the team
   
  
   
     Experience in the information security industry preferred, but not required
   
  
   
     Bachelor’s degree in an applicable field, such as CS, CIS or Engineering
   
 
 
   #LI-SS1
 
 
   #LI-MW1
 
 
   #LI-Remote
 
 
   #HTF
 
 
 
   Benefits of Working at CrowdStrike:
 
 
  
   
     Remote-first culture
   
  
   
     Market leader in compensation and equity awards
   
  
   
     Competitive vacation and flexible working arrangements
   
  
   
     Comprehensive and inclusive health benefits
   
  
   
     Physical and mental wellness programs
   
  
   
     Paid parental leave, including adoption
   
  
   
     A variety of professional development and mentorship opportunities
   
  
   
     Offices with stocked kitchens when you need to fuel innovation and collaboration
   
 
 
 
  
    We are committed to fostering a culture of belonging where everyone feels seen, heard, valued for who they are and empowered to succeed. Our approach to cultivating a diverse, equitable, and inclusive culture is rooted in listening, learning and collective action. By embracing the diversity of our people, we achieve our best work and fuel innovation - generating the best possible outcomes for our customers and the communities they serve.
  
 
 
 
   CrowdStrike is committed to maintaining an environment of Equal Opportunity and Affirmative Action. If you need reasonable accommodation to access the information provided on this website, please contact 
  
   Recruiting@crowdstrike.com
   for further assistance.
 
 
 
   CrowdStrike participates in the E-Verify program.
 
 
  
    Notice of E-Verify Participation
  
 
 
  
    Right to Work
  
  CrowdStrike, Inc. is committed to fair and equitable compensation practices. The base salary range for this position in the U.S. is $105,000 - $195,000 per year + variable/incentive compensation + equity + benefits. A candidate’s salary is determined by various factors including, but not limited to, relevant work experience, skills, certifications and location.","<div>
 <div>
  #WeAreCrowdStrike and our mission is to stop breaches. As a global leader in cybersecurity, our team changed the game. Since our inception, our market leading cloud-native platform has offered unparalleled protection against the most sophisticated cyberattacks. We work on large scale distributed systems, processing over 1 trillion events a day with a petabyte of RAM deployed in our Cassandra clusters - and this traffic is growing daily. We&#x2019;re looking for people with limitless passion, a relentless focus on innovation and a fanatical commitment to developing and shaping our cybersecurity platform. Consistently recognized as a top workplace, CrowdStrike is committed to cultivating an inclusive, remote-first culture that offers people the autonomy and flexibility to balance the needs of work and life while taking their career to the next level. Interested in working for a company that sets the standard and leads with integrity? Join us on a mission that matters - one team, one fight.
 </div>
 <div></div>
 <div>
   About the Role
  <br> CrowdStrike is looking to hire a Senior Engineer to the Data Services team to help us take our database systems to the next level. We&#x2019;re looking for a highly-technical, hands-on engineer, who loves to work with data plane services like Cassandra, ElasticSearch, and Kafka, and is comfortable building automation around large-scale cloud-based critical systems. We&#x2019;ll be looking at candidate CVs with an eye on achievement - what you&#x2019;ve accomplished in the past tells us the most about what you can do for us in the future.
 </div>
 <div>
  <br> What You&apos;ll Do:
 </div>
 <ul>
  <li>
   <div>
     Maintain a deep understanding of the data components - including Cassandra, ElasticSearch, Kafka, Zookeeper, and Spark, and use that understanding to operate and automate properly configured clusters.
   </div></li>
  <li>
   <div>
     Work with Engineering to roll out new products and features.
   </div></li>
  <li>
   <div>
     Develop infrastructure services to support the CrowdStrike engineering team&#x2019;s pursuit of a full devops model.
   </div></li>
  <li>
   <div>
     Work closely with Engineering and Customer Support to troubleshoot time-sensitive production issues, regardless of when they happen.
   </div></li>
  <li>
   <div>
     Keep petabytes of critical business data safe, secure, and available.
   </div></li>
 </ul>
 <div></div>
 <div>
   What You&#x2019;ll Need:
 </div>
 <ul>
  <li>
   <div>
     Experience with large scale datastores using technologies like Cassandra, ElasticSearch, Kafka, Zookeeper, and Spark.
   </div></li>
  <li>
   <div>
     Experience with large-scale, business-critical Linux environments
   </div></li>
  <li>
   <div>
     Experience operating within the cloud, preferably Amazon Web Services
   </div></li>
  <li>
   <div>
     Proven ability to work effectively with both local and remote teams
   </div></li>
  <li>
   <div>
     Track record of making great decisions, particularly when it matters most
   </div></li>
  <li>
   <div>
     Rock solid communication skills, verbal and written
   </div></li>
  <li>
   <div>
     A combination of confidence and independence... with the prudence to know when to ask for help from the rest of the team
   </div></li>
  <li>
   <div>
     Experience in the information security industry preferred, but not required
   </div></li>
  <li>
   <div>
     Bachelor&#x2019;s degree in an applicable field, such as CS, CIS or Engineering
   </div></li>
 </ul>
 <div>
  <br> #LI-SS1
 </div>
 <div>
   #LI-MW1
 </div>
 <div>
   #LI-Remote
 </div>
 <div>
   #HTF
 </div>
 <div></div>
 <div>
   Benefits of Working at CrowdStrike:
 </div>
 <ul>
  <li>
   <div>
     Remote-first culture
   </div></li>
  <li>
   <div>
     Market leader in compensation and equity awards
   </div></li>
  <li>
   <div>
     Competitive vacation and flexible working arrangements
   </div></li>
  <li>
   <div>
     Comprehensive and inclusive health benefits
   </div></li>
  <li>
   <div>
     Physical and mental wellness programs
   </div></li>
  <li>
   <div>
     Paid parental leave, including adoption
   </div></li>
  <li>
   <div>
     A variety of professional development and mentorship opportunities
   </div></li>
  <li>
   <div>
     Offices with stocked kitchens when you need to fuel innovation and collaboration
   </div></li>
 </ul>
 <div></div>
 <div>
  <div>
    We are committed to fostering a culture of belonging where everyone feels seen, heard, valued for who they are and empowered to succeed. Our approach to cultivating a diverse, equitable, and inclusive culture is rooted in listening, learning and collective action. By embracing the diversity of our people, we achieve our best work and fuel innovation - generating the best possible outcomes for our customers and the communities they serve.
  </div>
 </div>
 <div></div>
 <div>
   CrowdStrike is committed to maintaining an environment of Equal Opportunity and Affirmative Action. If you need reasonable accommodation to access the information provided on this website, please contact 
  <div>
   Recruiting@crowdstrike.com
  </div> for further assistance.
 </div>
 <div></div>
 <div>
   CrowdStrike participates in the E-Verify program.
 </div>
 <div>
  <div>
    Notice of E-Verify Participation
  </div>
 </div>
 <div>
  <div>
    Right to Work
  </div>
 </div> CrowdStrike, Inc. is committed to fair and equitable compensation practices. The base salary range for this position in the U.S. is &#x24;105,000 - &#x24;195,000 per year + variable/incentive compensation + equity + benefits. A candidate&#x2019;s salary is determined by various factors including, but not limited to, relevant work experience, skills, certifications and location.
</div>","https://crowdstrike.wd5.myworkdayjobs.com/en-US/crowdstrikecareers/job/USA---Remote/Sr-Systems-Engineer---Data-Services--Remote-_R14626?indeed_organic","86616a549b160eb2",,"Full-time",,"Remote","Sr. Systems Engineer - Data Services (Remote)","18 days ago","2023-10-07T11:55:04.992Z","3.5","35","$105,000 - $195,000 a year","2023-10-25T11:55:04.996Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=86616a549b160eb2&from=jasx&tk=1hdjb07bvk56u800&vjs=3"
"Juul Labs","THE COMPANY: 
  Juul Labs' mission is to impact the lives of the world's one billion adult smokers by eliminating combustible cigarettes. We have the opportunity to address one of the world's most intractable challenges through a commitment to exceptional quality, research, design, and innovation. Backed by leading technology investors, we are committed to the same excellence when it comes to hiring great talent. 
  We are a diverse team that is united by this common purpose and we are hiring the world's best engineers, scientists, designers, product managers, operations experts, and customer service and business professionals. If the opportunity to build your career at one of the fastest growing companies is compelling, read on for more details.
   ROLE AND RESPONSIBILITIES:
  
  Must live in US 
  Data at Juul means working with varied, large, data sets – where we apply analytical methods to help inform and drive business and product decisions. We are looking for an output-focused problem solver with a strong conceptual mindset and superb communication skills. 
  The team sees itself as analytical generalists – we choose the right technique for each problem, pride ourselves on building beautiful systems and dashboards while moving fast, and are ultimately driven by the value and insight data science can generate for the business and our customers. Our work directly impacts and shapes key executive decisions. This role offers tremendous upwards exposure towards senior business leaders and the chance to truly impact the decision making in a startup. 
  KEY RESPONSIBILITIES: 
  
  Modeling the dynamics of the fast moving and competitive market that we operate in and understanding our impact on it 
  Bringing visibility into our complex sales, logistics and supply chain operations through dashboards and standardized metrics 
  Explaining and predicting online and offline consumer choices and consumption patterns 
  Building demand forecasting models and understanding their behavior 
  Creating clean analytical data models and tools to help empower business and operational teams 
  Design robust, reusable and scalable data solutions and data pipeline frameworks to automate the ingestion, processing and delivery of both structured and unstructured data using python 
  Be in active development of large-scale data engineering projects 
  Create data pipelines in airflow, DBT and the general suite of Google Cloud Platform 
  Build, manage, and support data models. Ensure data quality with data tests in Monte Carlo and Datafold 
  Work in a scrum agile environment using Trello 
  Partner with Data Scientists, Data Engineers and Business Analysts to build configurable, scalable, and robust data processing infrastructure 
  Work closely with our sales, operations, research, and finance teams on data storage, retrieval, and analysis 
  Develop new systems and tools to enable stakeholders to consume and understand data more intuitively 
  Create and establish design standards and assurance processes to ensure compatibility and operability of data connections, flows and storage requirements 
  Keep Juul on the cutting edge of data technology 
  Our Data Stack: 
  Airflow, Fivetran 
  Google Cloud Platform - GCP (BigQuery, Storage, Dataflow, Pub/Sub, Cloud Functions/Run, Vertex AI, Cloud Build) 
  DBT 
  Monte Carlo, Datafold 
  Tableau 
  
 PERSONAL AND PROFESSIONAL QUALIFICATIONS: 
  
  4+ years of data engineering or software engineering experience with a focus on data 
  Advanced knowledge in developing using Python for data processing for large-scale datasets and workflows 
  Skilled using python libraries and packages (pandas, pyarrow) in conjunction with the Google Cloud Platform (BigQuery, Storage, Pub/Sub) 
  Knowledge of bash/shell and orchestration tools (e.g. Airflow), is preferred 
  Experience with version control (Git) and containers (Docker) 
  Skilled in analytical SQL in support of data modeling/ transformations and manipulating multiple data formats 
  Foundational expertise of deploying and maintaining machine learning pipelines is a plus 
  
 EDUCATION & EXPERIENCE: 
  Preferred masters degree in Computer Science, Engineering, Math, or equivalent experience 
  JUUL LABS PERKS & BENEFITS: 
  
  A place to grow your career. We'll help you set big goals - and exceed them 
  People. Work with talented, committed and supportive teammates 
  Equity and performance bonuses. Every employee is a stakeholder in our success 
  Cell phone subsidy, commuter benefits and discounts on JUUL products 
  Excellent medical, dental and vision benefits 
 
 Juul Labs is proud to be an equal opportunity employer and is committed to creating a diverse and inclusive work environment for all employees and job applicants, without regard to race, color, religion, sex, sexual orientation, age, gender identity or gender expression, national origin, disability or veteran status. We will consider for employment qualified applicants with arrest and conviction records, pursuant to the San Francisco Fair Chance Ordinance. Juul Labs also complies with the employment eligibility verification requirements of the Immigration and Nationality Act. All applicants must have authorization to work for Juul Labs in the US. #LI-remote
 
  
    SALARY RANGES: Salary varies by role, level and location, and is dependent on the cost of labor in a given geographic region among other factors. These ranges may be modified at any time.  LOCATIONS: Tier 1 Locations: Greater New York City, and San Francisco Bay Area Tier 2 Locations: Greater Boston, Washington DC Metropolitan Area, Seattle/Tacoma, Greater Sacramento, Los Angeles/OC/San Diego Tier 3 Locations: Rest of New England, NY Capital District, Rest of New Jersey, Greater Philadelphia, Pittsburgh, Delaware, Rest of Maryland, Rest of Virginia, North Carolina, Atlanta, Miami-Fort Lauderdale-WPB, Chicagoland, Dallas, Houston, Austin, Minneapolis/St. Paul, Colorado, Phoenix, Reno, Las Vegas, Portland Ore./Vancouver Wash., Rest of California, Hawaii Tier 4 Locations: Rest of US including Alaska and Puerto Rico
  
   Tier 1 Range:
  
    $132,000—$198,000 USD
  
   Tier 2 Range:
  
    $122,000—$183,000 USD
  
   Tier 3 Range:
  
    $114,000—$171,000 USD
  
   Tier 4 Range:
  
    $104,000—$156,000 USD","<div>
 <p><b>THE COMPANY:</b></p> 
 <p> Juul Labs&apos; mission is to impact the lives of the world&apos;s one billion adult smokers by eliminating combustible cigarettes. We have the opportunity to address one of the world&apos;s most intractable challenges through a commitment to exceptional quality, research, design, and innovation. Backed by leading technology investors, we are committed to the same excellence when it comes to hiring great talent.</p> 
 <p> We are a diverse team that is united by this common purpose and we are hiring the world&apos;s best engineers, scientists, designers, product managers, operations experts, and customer service and business professionals. If the opportunity to build your career at one of the fastest growing companies is compelling, read on for more details.</p>
 <p><br> <b> ROLE AND RESPONSIBILITIES:</b></p>
 <br> 
 <p><b> Must live in US</b></p> 
 <p> Data at Juul means working with varied, large, data sets &#x2013; where we apply analytical methods to help inform and drive business and product decisions. We are looking for an output-focused problem solver with a strong conceptual mindset and superb communication skills.</p> 
 <p> The team sees itself as analytical generalists &#x2013; we choose the right technique for each problem, pride ourselves on building beautiful systems and dashboards while moving fast, and are ultimately driven by the value and insight data science can generate for the business and our customers. Our work directly impacts and shapes key executive decisions. This role offers tremendous upwards exposure towards senior business leaders and the chance to truly impact the decision making in a startup.</p> 
 <p><b> KEY RESPONSIBILITIES:</b></p> 
 <ul> 
  <li>Modeling the dynamics of the fast moving and competitive market that we operate in and understanding our impact on it</li> 
  <li>Bringing visibility into our complex sales, logistics and supply chain operations through dashboards and standardized metrics</li> 
  <li>Explaining and predicting online and offline consumer choices and consumption patterns</li> 
  <li>Building demand forecasting models and understanding their behavior</li> 
  <li>Creating clean analytical data models and tools to help empower business and operational teams</li> 
  <li>Design robust, reusable and scalable data solutions and data pipeline frameworks to automate the ingestion, processing and delivery of both structured and unstructured data using python</li> 
  <li>Be in active development of large-scale data engineering projects</li> 
  <li>Create data pipelines in airflow, DBT and the general suite of Google Cloud Platform</li> 
  <li>Build, manage, and support data models. Ensure data quality with data tests in Monte Carlo and Datafold</li> 
  <li>Work in a scrum agile environment using Trello</li> 
  <li>Partner with Data Scientists, Data Engineers and Business Analysts to build configurable, scalable, and robust data processing infrastructure</li> 
  <li>Work closely with our sales, operations, research, and finance teams on data storage, retrieval, and analysis</li> 
  <li>Develop new systems and tools to enable stakeholders to consume and understand data more intuitively</li> 
  <li>Create and establish design standards and assurance processes to ensure compatibility and operability of data connections, flows and storage requirements</li> 
  <li>Keep Juul on the cutting edge of data technology</li> 
  <li>Our Data Stack:</li> 
  <li>Airflow, Fivetran</li> 
  <li>Google Cloud Platform - GCP (BigQuery, Storage, Dataflow, Pub/Sub, Cloud Functions/Run, Vertex AI, Cloud Build)</li> 
  <li>DBT</li> 
  <li>Monte Carlo, Datafold</li> 
  <li>Tableau</li> 
 </ul> 
 <p><b>PERSONAL AND PROFESSIONAL QUALIFICATIONS:</b></p> 
 <ul> 
  <li>4+ years of data engineering or software engineering experience with a focus on data</li> 
  <li>Advanced knowledge in developing using Python for data processing for large-scale datasets and workflows</li> 
  <li>Skilled using python libraries and packages (pandas, pyarrow) in conjunction with the Google Cloud Platform (BigQuery, Storage, Pub/Sub)</li> 
  <li>Knowledge of bash/shell and orchestration tools (e.g. Airflow), is preferred</li> 
  <li>Experience with version control (Git) and containers (Docker)</li> 
  <li>Skilled in analytical SQL in support of data modeling/ transformations and manipulating multiple data formats</li> 
  <li>Foundational expertise of deploying and maintaining machine learning pipelines is a plus</li> 
 </ul> 
 <p><b>EDUCATION &amp; EXPERIENCE:</b></p> 
 <p> Preferred masters degree in Computer Science, Engineering, Math, or equivalent experience</p> 
 <p><b> JUUL LABS PERKS &amp; BENEFITS:</b></p> 
 <ul> 
  <li>A place to grow your career. We&apos;ll help you set big goals - and exceed them</li> 
  <li>People. Work with talented, committed and supportive teammates</li> 
  <li>Equity and performance bonuses. Every employee is a stakeholder in our success</li> 
  <li>Cell phone subsidy, commuter benefits and discounts on JUUL products</li> 
  <li>Excellent medical, dental and vision benefits</li> 
 </ul>
 <h6 class=""jobSectionHeader"">Juul Labs is proud to be an equal opportunity employer and is committed to creating a diverse and inclusive work environment for all employees and job applicants, without regard to race, color, religion, sex, sexual orientation, age, gender identity or gender expression, national origin, disability or veteran status. We will consider for employment qualified applicants with arrest and conviction records, pursuant to the San Francisco Fair Chance Ordinance. Juul Labs also complies with the employment eligibility verification requirements of the Immigration and Nationality Act. All applicants must have authorization to work for Juul Labs in the US. #LI-remote</h6>
 <div>
  <div>
   <p><b> SALARY RANGES:</b><br> Salary varies by role, level and location, and is dependent on the cost of labor in a given<br> geographic region among other factors. These ranges may be modified at any time.<br> <br> <b>LOCATIONS:</b><br> <b>Tier 1 Locations:</b> Greater New York City, and San Francisco Bay Area<br> <b>Tier 2 Locations:</b> Greater Boston, Washington DC Metropolitan Area, Seattle/Tacoma,<br> Greater Sacramento, Los Angeles/OC/San Diego<br> <b>Tier 3 Locations:</b> Rest of New England, NY Capital District, Rest of New Jersey, Greater<br> Philadelphia, Pittsburgh, Delaware, Rest of Maryland, Rest of Virginia, North Carolina,<br> Atlanta, Miami-Fort Lauderdale-WPB, Chicagoland, Dallas, Houston, Austin,<br> Minneapolis/St. Paul, Colorado, Phoenix, Reno, Las Vegas, Portland Ore./Vancouver<br> Wash., Rest of California, Hawaii<br> <b>Tier 4 Locations:</b> Rest of US including Alaska and Puerto Rico</p>
  </div>
  <p><b> Tier 1 Range:</b></p>
  <div>
    &#x24;132,000&#x2014;&#x24;198,000 USD
  </div>
  <p><b> Tier 2 Range:</b></p>
  <div>
    &#x24;122,000&#x2014;&#x24;183,000 USD
  </div>
  <p><b> Tier 3 Range:</b></p>
  <div>
    &#x24;114,000&#x2014;&#x24;171,000 USD
  </div>
  <p><b> Tier 4 Range:</b></p>
  <div>
    &#x24;104,000&#x2014;&#x24;156,000 USD
  </div>
 </div>
</div>","https://boards.greenhouse.io/juullabs/jobs/5459576?gh_src=7b3a85461us","9a4b8a69c3f5c9aa",,,,"Remote","Senior Data Engineer","Today","2023-10-25T11:55:05.057Z",,,,"2023-10-25T11:55:05.058Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=9a4b8a69c3f5c9aa&from=jasx&tk=1hdjafq3r2cc7000&vjs=3"
"UNCOMN","Here at 
  UN
  COMN, our mission is to empower systems thinkers to create elegant solutions to complex problems – to improve the systems that improve our communities. Our team members apply their natural curiosity and grit to discover elegant solutions for our clients’ most complex organizational, logistics, process, data, and technical challenges, with the overall goal of building great businesses that contribute to great communities.
 
  
  
 
   We’re an award-winning firm, one of the country’s fastest-growing and—more importantly—a consistent ‘
  Top Workplace’ as evaluated by our own employees. We are a values-driven organization (see the 
  Core Values section of our website) and we’re looking for new Uncommon Geniuses to join our growing team, so if you’re a person who likes to solve problems, fix things, build things, tweak things, or otherwise show creative flair, you might just be an ""
  UN
  COMN Genius"" and we encourage you to check out the specifics of this position below!
 
  
  
 
   UN
  COMN is seeking a 
  Data Engineer 3 to:
  
 
   Meet with client stakeholders to gather requirements for data engineering tasks
   Collaborate with cross-functional team to design loading and transformation processes within a big data environment
   Analyze raw source data to determine contents and meaning and apply to client requirements
   Design and build database objects for a large supply chain ecosystem
   Design and build processes for loading and transforming data
   Analyze and optimize processes for performance
   Identify, troubleshoot, and fix bugs in data environment
 
 
   5+ years of data engineering in AWS with a focus on ETL 
   
    5+ years PL/pgSQL
     5+ years Python
     3+ years AWS Glue
    
  5+ years data analysis
   3+ years data management in relational databases
   3+ years gathering requirements for data engineering projects
   Ability to build processes that support data transformation, workload management, data structures, dependency, and metadata
   Ability to build and optimize data sets, big data pipelines, and architectures
   Fluency with AWS CLI
   Ability to perform root cause analysis on external and internal processes and data to identify opportunities for improvement and answer questions
   Excellent analytic skills associated with working on unstructured datasets
   Proactive approach to problem-solving
   Natural curiosity and desire to learn customer data and business processes
   Must be eligible to obtain a Secret clearance granted by the US Government as needed
 
  
  
  Preferred 
 
   3+ years of data management using AWS Postgres RDS
   Experience working with supply chain data
   Experience working with EDI
 
 
   Flexible PTO effective on day 1*
   7 Paid Holidays & up to 3 Floating Holidays*
   Eligible for Health Benefits on day 1*
   401K Safe Harbor Match Program*
   Training and Education Assistance*
  
   Must be a full-time employee
  
 
  
  
 
   Don’t meet every single requirement? We’re dedicated to building an uncommon, inclusive, and authentic workplace, so if you’re excited about this role but your experience doesn’t align perfectly with every qualification in the job description, we encourage you to apply anyways. You may be just the right candidate for this or other roles.
 
  
  
  All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, or national origin.","<div>
 <div>
  Here at 
  <b>UN</b>
  <b>COMN</b>, our mission is to empower systems thinkers to create elegant solutions to complex problems &#x2013; to improve the systems that improve our communities. Our team members apply their natural curiosity and grit to discover elegant solutions for our clients&#x2019; most complex organizational, logistics, process, data, and technical challenges, with the overall goal of building great businesses that contribute to great communities.
 </div>
 <br> 
 <div></div> 
 <div>
   We&#x2019;re an award-winning firm, one of the country&#x2019;s fastest-growing and&#x2014;more importantly&#x2014;a consistent &#x2018;
  <b>Top Workplace</b>&#x2019; as evaluated by our own employees. We are a values-driven organization (see the 
  <b>Core Values</b> section of our website) and we&#x2019;re looking for new Uncommon Geniuses to join our growing team, so if you&#x2019;re a person who likes to solve problems, fix things, build things, tweak things, or otherwise show creative flair, you might just be an &quot;
  <b>UN</b>
  <b>COMN </b>Genius&quot; and we encourage you to check out the specifics of this position below!
 </div>
 <br> 
 <div></div> 
 <div>
  <b> UN</b>
  <b>COMN </b>is seeking a 
  <b>Data Engineer 3 </b>to:
 </div> 
 <ul>
  <li> Meet with client stakeholders to gather requirements for data engineering tasks</li>
  <li> Collaborate with cross-functional team to design loading and transformation processes within a big data environment</li>
  <li> Analyze raw source data to determine contents and meaning and apply to client requirements</li>
  <li> Design and build database objects for a large supply chain ecosystem</li>
  <li> Design and build processes for loading and transforming data</li>
  <li> Analyze and optimize processes for performance</li>
  <li> Identify, troubleshoot, and fix bugs in data environment</li>
 </ul>
 <ul>
  <li> 5+ years of data engineering in AWS with a focus on ETL 
   <ul>
    <li>5+ years PL/pgSQL</li>
    <li> 5+ years Python</li>
    <li> 3+ years AWS Glue</li>
   </ul> </li>
  <li>5+ years data analysis</li>
  <li> 3+ years data management in relational databases</li>
  <li> 3+ years gathering requirements for data engineering projects</li>
  <li> Ability to build processes that support data transformation, workload management, data structures, dependency, and metadata</li>
  <li> Ability to build and optimize data sets, big data pipelines, and architectures</li>
  <li> Fluency with AWS CLI</li>
  <li> Ability to perform root cause analysis on external and internal processes and data to identify opportunities for improvement and answer questions</li>
  <li> Excellent analytic skills associated with working on unstructured datasets</li>
  <li> Proactive approach to problem-solving</li>
  <li> Natural curiosity and desire to learn customer data and business processes</li>
  <li> Must be eligible to obtain a Secret clearance granted by the US Government as needed</li>
 </ul>
 <br> 
 <p></p> 
 <p> Preferred</p> 
 <ul>
  <li> 3+ years of data management using AWS Postgres RDS</li>
  <li> Experience working with supply chain data</li>
  <li> Experience working with EDI</li>
 </ul>
 <ul>
  <li> Flexible PTO effective on day 1*</li>
  <li> 7 Paid Holidays &amp; up to 3 Floating Holidays*</li>
  <li> Eligible for Health Benefits on day 1*</li>
  <li> 401K Safe Harbor Match Program*</li>
  <li> Training and Education Assistance*</li>
  <ul>
   <li><i>Must be a full-time employee</i></li>
  </ul>
 </ul>
 <br> 
 <div></div> 
 <div>
  <b> Don&#x2019;t meet every single requirement? We&#x2019;re dedicated to building an uncommon, inclusive, and authentic workplace, so if you&#x2019;re excited about this role but your experience doesn&#x2019;t align perfectly with every qualification in the job description, we encourage you to apply anyways. You may be just the right candidate for this or other roles.</b>
 </div>
 <br> 
 <p></p> 
 <p><i> All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, or national origin.</i></p>
</div>","https://secure6.saashr.com/ta/6161386.careers?ShowJob=520534722","17557d019e22347b",,"Full-time",,"St. Louis, MO 63101","Data Engineer 3 (Remote)","8 days ago","2023-10-17T11:55:06.178Z","4.5","2",,"2023-10-25T11:55:06.180Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=17557d019e22347b&from=jasx&tk=1hdjaruujjrqo801&vjs=3"
"ServiceNow","Company Description
  At ServiceNow, our technology makes the world work for everyone, and our people make it possible. We move fast because the world can’t wait, and we innovate in ways no one else can for our customers and communities. By joining ServiceNow, you are part of an ambitious team of change makers who have a restless curiosity and a drive for ingenuity. We know that your best work happens when you live your best life and share your unique talents, so we do everything we can to make that possible. We dream big together, supporting each other to make our individual and collective dreams come true. The future is ours, and it starts with you. 
 With more than 7,700+ customers, we serve approximately 85% of the Fortune 500®, and we're proud to be one of FORTUNE 100 Best Companies to Work For® and World's Most Admired Companies™. 
 Learn more on Life at Now blog and hear from our employees about their experiences working at ServiceNow. 
 Unsure if you meet all the qualifications of a job description but are deeply excited about the role? We still encourage you to apply! At ServiceNow, we are committed to creating an inclusive environment where all voices are heard, valued, and respected. We welcome all candidates, including individuals from non-traditional, varied backgrounds, that might not come from a typical path connected to this role. We believe skills and experience are transferrable, and the desire to dream big makes for great candidates.
  Job Description
  What you get to do in this role:
 
   Develop re-usable, consistent, best in class dimensional data models to power use cases across the GTM spectrum - sales, partner, marketing and customer
   Develop scalable automated ETL/ELT jobs in Snowflake, to build and maintain these data models
   Use machine learning techniques to develop predictive metrics (Python is a must) and test those in an Azure ML environment
   Write proficient SQL queries and Python code to stand up master data models from raw data sources across disparate systems
   Identify any data integrity issues and deep dive to find root cause
   Enforce company data policies and procedures to ensure data quality and reduce discrepancies
   Secure approvals for data access based on business needs
   Train analysts and data scientists alike on available data sources
   Ensure very large databases and compute clusters operate optimally
   Implement and maintaining database structures and governance
   Develop / maintaining documentation on databases and production tables
   Collaborate across the company’s multiple data teams to meet analytics deliverables
 
  
  Qualifications
  To be successful in this role you have:
 
   5+ years of core data engineering position with advanced experience in SQL and Snowflake is a must
   Advanced scripting skills : R, SAS, Python
   Experience with developing ML models is a plus
   Experience with Azure ML, Azure ML Pipelines, Databricks is a must
   Sales GTM domain expertise in an Enterprise SaaS company is a must
   Data Science Expertise
   Effective problem solving and analytical skills. Ability to manage multiple projects and report simultaneously across different stakeholders
   Structured thinking with ability to easily break down ambiguous problems and propose impactful data modeling designs
   Passion for analyzing large and complex data sets and converting them into the information which drive business decisions
   Attention to detail, organization and effective verbal/written communication skills
   Proven track record in rolling out self-service analytics solutions (e.g. Tableau Server Ask Data, etc)
   Solid decision making, negotiation, and persuasion skills, often in ambiguous situations
   Must be able to work in fast paced environment and be able to adapt to changing requirements.
   Understanding of technology development projects and the full technology development lifecycle
 
 
  JV20
  #DTjobs
  For positions in the Bay Area, we offer a base pay of $133,300 - $226,700, plus equity (when applicable), variable/incentive compensation and benefits. Sales positions generally offer a competitive On Target Earnings (OTE) incentive compensation structure. Please note that the base pay shown is a guideline, and individual total compensation will vary based on factors such as qualifications, skill level, competencies and work location. We also offer health plans, including flexible spending accounts, a 401(k) Plan with company match, ESPP, matching donations, a flexible time away plan and family leave programs (subject to eligibility requirements). Compensation is based on the geographic location in which the role is located, and is subject to change based on work location.
  Additional Information
  ServiceNow is an Equal Employment Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, creed, religion, sex, sexual orientation, national origin or nationality, ancestry, age, disability, gender identity or expression, marital status, veteran status or any other category protected by law. 
 At ServiceNow, we lead with flexibility and trust in our distributed world of work. Click here to learn about our work personas: flexible, remote and required-in-office. 
 If you require a reasonable accommodation to complete any part of the application process, or are limited in the ability or unable to access or use this online application process and need an alternative method for applying, you may contact us at talent.acquisition@servicenow.com for assistance. 
 For positions requiring access to technical data subject to export control regulations, including Export Administration Regulations (EAR), ServiceNow may have to obtain export licensing approval from the U.S. Government for certain individuals. All employment is contingent upon ServiceNow obtaining any export license or other approval that may be required by the U.S. Government. 
 Please Note: Fraudulent job postings/job scams are increasingly common. Click here to learn what to watch out for and how to protect yourself. All genuine ServiceNow job postings can be found through the ServiceNow Careers site.
  
 From Fortune. © 2022 Fortune Media IP Limited All rights reserved. Used under license. 
 Fortune and Fortune Media IP Limited are not affiliated with, and do not endorse products or services of, ServiceNow.","<div>
 <b>Company Description</b>
 <p><br> At ServiceNow, our technology makes the world work for everyone, and our people make it possible. We move fast because the world can&#x2019;t wait, and we innovate in ways no one else can for our customers and communities. By joining ServiceNow, you are part of an ambitious team of change makers who have a restless curiosity and a drive for ingenuity. We know that your best work happens when you live your best life and share your unique talents, so we do everything we can to make that possible. We dream big together, supporting each other to make our individual and collective dreams come true. The future is ours, and it starts with you.</p> 
 <p>With more than 7,700+ customers, we serve approximately 85% of the Fortune 500&#xae;, and we&apos;re proud to be one of FORTUNE 100 Best Companies to Work For&#xae; and World&apos;s Most Admired Companies&#x2122;.</p> 
 <p>Learn more on Life at Now blog and hear from our employees about their experiences working at ServiceNow.</p> 
 <p>Unsure if you meet all the qualifications of a job description but are deeply excited about the role? We still encourage you to apply! At ServiceNow, we are committed to creating an inclusive environment where all voices are heard, valued, and respected. We welcome all candidates, including individuals from non-traditional, varied backgrounds, that might not come from a typical path connected to this role. We believe skills and experience are transferrable, and the desire to dream big makes for great candidates.</p>
 <b><br> Job Description</b>
 <p><b><br> What you get to do in this role:</b></p>
 <ul>
  <li> Develop re-usable, consistent, best in class dimensional data models to power use cases across the GTM spectrum - sales, partner, marketing and customer</li>
  <li> Develop scalable automated ETL/ELT jobs in Snowflake, to build and maintain these data models</li>
  <li> Use machine learning techniques to develop predictive metrics (Python is a must) and test those in an Azure ML environment</li>
  <li> Write proficient SQL queries and Python code to stand up master data models from raw data sources across disparate systems</li>
  <li> Identify any data integrity issues and deep dive to find root cause</li>
  <li> Enforce company data policies and procedures to ensure data quality and reduce discrepancies</li>
  <li> Secure approvals for data access based on business needs</li>
  <li> Train analysts and data scientists alike on available data sources</li>
  <li> Ensure very large databases and compute clusters operate optimally</li>
  <li> Implement and maintaining database structures and governance</li>
  <li> Develop / maintaining documentation on databases and production tables</li>
  <li> Collaborate across the company&#x2019;s multiple data teams to meet analytics deliverables</li>
 </ul>
 <br> 
 <b> Qualifications</b>
 <p><b><br> To be successful in this role you have:</b></p>
 <ul>
  <li> 5+ years of core data engineering position with advanced experience in SQL and Snowflake is a must</li>
  <li> Advanced scripting skills : R, SAS, Python</li>
  <li> Experience with developing ML models is a plus</li>
  <li> Experience with Azure ML, Azure ML Pipelines, Databricks is a must</li>
  <li> Sales GTM domain expertise in an Enterprise SaaS company is a must</li>
  <li> Data Science Expertise</li>
  <li> Effective problem solving and analytical skills. Ability to manage multiple projects and report simultaneously across different stakeholders</li>
  <li> Structured thinking with ability to easily break down ambiguous problems and propose impactful data modeling designs</li>
  <li> Passion for analyzing large and complex data sets and converting them into the information which drive business decisions</li>
  <li> Attention to detail, organization and effective verbal/written communication skills</li>
  <li> Proven track record in rolling out self-service analytics solutions (e.g. Tableau Server Ask Data, etc)</li>
  <li> Solid decision making, negotiation, and persuasion skills, often in ambiguous situations</li>
  <li> Must be able to work in fast paced environment and be able to adapt to changing requirements.</li>
  <li> Understanding of technology development projects and the full technology development lifecycle</li>
 </ul>
 <p></p>
 <p><br> JV20</p>
 <p> #DTjobs</p>
 <p> For positions in the Bay Area, we offer a base pay of &#x24;133,300 - &#x24;226,700, plus equity (when applicable), variable/incentive compensation and benefits. Sales positions generally offer a competitive On Target Earnings (OTE) incentive compensation structure. Please note that the base pay shown is a guideline, and individual total compensation will vary based on factors such as qualifications, skill level, competencies and work location. We also offer health plans, including flexible spending accounts, a 401(k) Plan with company match, ESPP, matching donations, a flexible time away plan and family leave programs (subject to eligibility requirements). Compensation is based on the geographic location in which the role is located, and is subject to change based on work location.</p>
 <b><br> Additional Information</b>
 <p><br> ServiceNow is an Equal Employment Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, creed, religion, sex, sexual orientation, national origin or nationality, ancestry, age, disability, gender identity or expression, marital status, veteran status or any other category protected by law.</p> 
 <p>At ServiceNow, we lead with flexibility and trust in our distributed world of work. Click here to learn about our work personas: flexible, remote and required-in-office.</p> 
 <p>If you require a reasonable accommodation to complete any part of the application process, or are limited in the ability or unable to access or use this online application process and need an alternative method for applying, you may contact us at talent.acquisition@servicenow.com for assistance.</p> 
 <p>For positions requiring access to technical data subject to export control regulations, including Export Administration Regulations (EAR), ServiceNow may have to obtain export licensing approval from the U.S. Government for certain individuals. All employment is contingent upon ServiceNow obtaining any export license or other approval that may be required by the U.S. Government.</p> 
 <p>Please Note: Fraudulent job postings/job scams are increasingly common. Click here to learn what to watch out for and how to protect yourself. All genuine ServiceNow job postings can be found through the ServiceNow Careers site.</p>
 <p><br> </p>
 <p>From Fortune. &#xa9; 2022 Fortune Media IP Limited All rights reserved. Used under license.</p> 
 <p>Fortune and Fortune Media IP Limited are not affiliated with, and do not endorse products or services of, ServiceNow.</p>
</div>","https://careers.servicenow.com/careers/jobs/743999935473343EXT?lang=en-us&trid=35ab2906-b356-4a56-8472-f60d30d2e2f0","d81d17fc1c3957ba",,"Full-time",,"4810 Eastgate Mall, San Diego, CA 92121","Senior Data Engineer","18 days ago","2023-10-07T11:55:11.034Z","3.7","241","$133,300 - $226,700 a year","2023-10-25T11:55:11.035Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=d81d17fc1c3957ba&from=jasx&tk=1hdjb07bvk56u800&vjs=3"
"WorkCog","Job Description:

 Minimum 9+ years exp Mandatory
 Mandatory Tech: We consider the following technologies as mandatory and should be familiar to the candidate:
 GCP
 Data Warehousing
 GIT
 Core Tech: Our ideal candidate should possess strong proficiency in the following technologies:
 Airflow 2years
 Python - 3 Years
 SQL
 Spark - 3 Years
 ETL

Job Type: Contract
Pay: $55.00 - $85.00 per hour
Expected hours: 40 per week
Benefits:

 Referral program

Compensation package:

 Hourly pay

Experience level:

 10 years
 11+ years
 9 years

Schedule:

 8 hour shift

Experience:

 Informatica: 5 years (Preferred)
 SQL: 6 years (Preferred)
 Data warehouse: 7 years (Preferred)

Work Location: Remote","<p><b>Job Description:</b></p>
<ul>
 <li>Minimum 9+ years exp Mandatory</li>
 <li>Mandatory Tech: We consider the following technologies as mandatory and should be familiar to the candidate:</li>
 <li>GCP</li>
 <li>Data Warehousing</li>
 <li>GIT</li>
 <li>Core Tech: Our ideal candidate should possess strong proficiency in the following technologies:</li>
 <li>Airflow 2years</li>
 <li>Python - 3 Years</li>
 <li>SQL</li>
 <li>Spark - 3 Years</li>
 <li>ETL</li>
</ul>
<p>Job Type: Contract</p>
<p>Pay: &#x24;55.00 - &#x24;85.00 per hour</p>
<p>Expected hours: 40 per week</p>
<p>Benefits:</p>
<ul>
 <li>Referral program</li>
</ul>
<p>Compensation package:</p>
<ul>
 <li>Hourly pay</li>
</ul>
<p>Experience level:</p>
<ul>
 <li>10 years</li>
 <li>11+ years</li>
 <li>9 years</li>
</ul>
<p>Schedule:</p>
<ul>
 <li>8 hour shift</li>
</ul>
<p>Experience:</p>
<ul>
 <li>Informatica: 5 years (Preferred)</li>
 <li>SQL: 6 years (Preferred)</li>
 <li>Data warehouse: 7 years (Preferred)</li>
</ul>
<p>Work Location: Remote</p>",,"879c5ddd69953e54",,"Contract",,"Remote","Senior Data Engineer (Contract ""W2"". With 10+ Year's)","12 days ago","2023-10-13T11:55:18.490Z",,,"$55 - $85 an hour","2023-10-25T11:55:18.493Z","US","remote","data engineer","https://www.indeed.com/rc/clk?jk=879c5ddd69953e54&from=jasx&tk=1hdjb07bvk56u800&vjs=3"
