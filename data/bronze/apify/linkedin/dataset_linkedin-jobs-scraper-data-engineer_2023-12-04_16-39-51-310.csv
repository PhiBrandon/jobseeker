"applicationsCount","companyId","companyName","companyUrl","contractType","description","experienceLevel","jobUrl","location","postedTime","posterFullName","posterProfileUrl","publishedAt","salary","sector","title","workType"
"183 applicants","93380834","Sonitalent Corp","https://www.linkedin.com/company/sonitalent-corp?trk=public_jobs_topcard-org-name","Full-time","Job Title – Data Engineers

Job Location -Remote

Duration - 6 Months+

Visa -USC, GC Only

Mode Of Interview - Phone/Skype

Note - Please no job hoppers (1 year+ for each job)! -and need LinkedIn as well – Need genuine profiles asap

Job Description –

What we’re looking, skills wise:


 * 10+ years of experience
 * Azure Databricks (knows the interworking of Databricks so the team can stop calling Microsoft for everything)
 * Azure Data Factory
 * Azure DevOps is a strong plus
 * Strong Python
 * Data lake/data warehouse background
 * Strong SQL (someone with a strong SQL background who switched over to Python for database engineering)
 * Snowflake is a nice to have, not a requirement
 * Excellent communication skills
 * Someone that’s inquisitive, can ask questions, and can focus on the task at hand
 * Fully Remote within the contiguous US
 * Interview process: meet with Dan, homework assignment (containing file types/parsing to ingest and loading into a delta table), 1 hour meeting with the team – hired
   
   

Summary

We are looking for a savvy Data Engineer to join our growing team of analytics experts. The person will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will develop and support a broad range of software capabilities including building data pipelines, managing the ETL/ELT processes, receiving, and delivering data through various interfaces, and processing significant amounts of data related to railcar movements, railcar liability, and financial calculations. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.

Duties And Responsibilities

To perform this job successfully an individual must be able to perform the following essential duties satisfactorily. Other duties may be assigned to address business needs and changing business practices.


 * Participate as a member of an Agile team developing Data Engineering solutions.
 * Engage in requirements gathering and technical design discussions to meet business needs.
 * Design and develop generic, scalable data pipelines in Azure Data factory and Databricks with python for on-prem and cloud data sources
 * Assemble large, complex sets of data that meet non-functional and functional business requirements
 * Leverage your curiosity for solving unstructured data problems and ability to manipulate and optimize large data sets to advance business problem-solving.
 * Contribute to documentation, testing and cross-training of other team members.
 * Work closely with others to assist and resolve production issues.
   
   

Qualifications

The following generally describes requirements to successfully perform the assigned duties.

Minimum Qualifications


 * Bachelor's degree in computer science, computer engineering, a related field, or equivalent experience.
 * 10+ years of data engineering or equivalent experience.
 * 10+ years of hands-on experience in developing and deploying data architecture strategies or engineering practices.
 * 10+ years of experience with complex SQL queries and knowledge of database technologies.
 * Expert-level coding experience with PySpark and Python.
 * Expert-level technical experience with Apache Spark / Azure Databricks.
 * Proficient in using and designing solutions on Azure Cloud infrastructure (particularly Azure Data Factory) and Azure DevOps.
 * Proficient with core business intelligence and data warehousing technology.
 * Proficient designing and developing data integration solutions using ETL tools such as Azure Data Factory and/or SSIS.
 * Proficient with software development practices such as Agile, TDD, and CI/CD.
 * Ability to collaborate and communicate professionally, both verbally and in writing, at all levels of the organization, particularly bridging conversations between data and business stakeholders.
   
   

Preferred Qualifications


 * Experience with Snowflake
 * Experience with graph databases or graph libraries
 * Kafka or other streaming technologies
 * Elastic Search
 * Experience in the rail or other commodities driven industry
   
   

Work Environment And Physical Requirements

Work Environment

The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.

This is a remote position.

Physical Activities And Requirements

Frequency Key

Not Applicable: Activity is not applicable to this occupation

Occasionally: Occupation requires this activity up to 33% of the time (0- 2.5+ hours/day)

Frequently: Occupation requires this activity from 33% - 66% of the time (2.5: 5.5+ hours/day)

Constantly: Occupation requires this activity more than 66% of the time (5.5+ hours/day)","Entry level","https://www.linkedin.com/jobs/view/remote-data-engineers-at-sonitalent-corp-3758444106?trk=public_jobs_topcard-title","United States","3 weeks ago","","","2023-11-07","","Staffing and Recruiting","Remote- Data Engineers","Information Technology"
"Over 200 applicants","81965528","Publishing.com","https://www.linkedin.com/company/publishingcom?trk=public_jobs_topcard-org-name","Full-time","Reports to

Head of Engineering

Summary:

Publishing.com has helped thousands of normal everyday people to become successful self-published authors. And along the way, we also became one the most successful companies in the US (

Want to be at the forefront of the AI revolution? Join us! We are building the most comprehensive AI-powered self-publishing platform and you get to join us at ground zero. Instead of just teaching people how to create successful books, we are going to help them do it.

About you:

You are a data engineer with strong analytical skills and hands-on experience with modern data warehousing and business intelligence solutions. You know how to use code and no-code to ingest data from various sources, how to interpret data and translate it into business insight, and how to prepare reports and dashboards that are easy to understand and digest.

And you have a great attitude!

About this role:

As our first data engineer, you will have the opportunity to make important contributions to various aspects of our data platform. Your main responsibilities are:


 * Build a highly scalable data warehouse
 * Propose, design, and implement data ingestion pipelines (ELT/ETL)
 * Maintain our local and cloud data platforms
 * Understand and interpret business intelligence requirements and translate them into technical solutions
 * Build business analytics and dashboards to address sales and marketing needs
   
   

Required skills:


 * Strong problem solving skills
 * Strong communication skills
 * Strong SQL skills
 * Expert in using data warehousing solutions such as BigQuery, Snowflake, or Databricks
 * Experience with data ingestion services such as Fivetran, Matilion, Segment, or similar
 * Experience with Google Sheets
 * Experience with business analytics for marketing and sales
 * Strong programming skills in JavaScript and Python
 * Experience with HubSpot
 * Experience with Git
 * Experience with agile development
 * Experience working with marketing and sales teams
 * Strong sense of ownership
   
   

Preferred Skills:


 * Experience with Google Clouds
 * Experience building CI/CD pipelines
 * Experience with AWS, Azure, or GCP
 * Experience with Terraform or other IaC solutions
 * Experience with DevOps and SRE best practices
   
   

Why Publishing.com?


 * People love working here and your peers are great - check out our
 * We are growing (fast!)
 * We are located all over the world with 60+ employees. We were remote before remote was a thing! And we will continue to be.
 * Last year we hit $60M in revenue, and we are just getting started!
 * We have all the fun perks you’d expect—flexible vacation policy, competitive vision, dental, and health benefits, 401k plans, and socials (yes, even remotely!)*
 * We are proud of our culture and care about it deeply—we live by our team values and are always trying to make Publishing.com a better company today than it was yesterday.
 * We encourage learning, growth, and continuous improvement so always looking for ways to help our staff grow. From monthly training to hiring mentors, we care about your personal growth!
 * If you want to join a team on the ground floor, this is your chance. We have a grand vision for expanding beyond just an education company to become the one-stop shop for everything publishing related.
 * some benefits are available to our US-based employees only.
   
   

Publishing.com is dedicated to building diverse teams that fuel an authentic workplace and sense of belonging for each and every employee. We know applying for a job can be intimidating, please don't hesitate to reach out - we encourage everyone interested in joining us to apply.

Publishing.com LLC. provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, disability, genetics, gender, sexual orientation, age, marital status, veteran status. In addition to federal law requirements, Publishing.com LLC. complies with applicable state and local laws governing nondiscrimination in employment in every location we have employees in. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training.

Powered by JazzHR

eUT95FbkXw","Mid-Senior level","https://www.linkedin.com/jobs/view/bi-data-engineer-at-publishing-com-3752272754?trk=public_jobs_topcard-title","Austin, TX","1 month ago","","","2023-10-27","","Internet Publishing","BI Data Engineer","Business Development and Sales"
"Over 200 applicants","81422496","SmartIPlace","https://www.linkedin.com/company/smartiplace?trk=public_jobs_topcard-org-name","Full-time","Title: Data Engineer [Remote]




Experience: 8+ years




Visa: USC Only




Skills




 * Minimum of 5-10 years of IT/IS experience
 * Need to have excellent communication skills
 * 5+ years working with Advanced Structured Query Language (SQL) & PL/SQL
 * 5+ years experience with at least Oracle relational database
 * 5+ years experience in software development lifecycle activities
 * 5+ years experience with data loading (ETL, ELT)
 * 5+ years working with data at scale 50+ TB
 * Experience with data warehouses, operational data stores, data hubs
 * Experience in end-to-end design of near-real-time and batch data pipelines
 * Experience working in an Agile environment
 * Experience developing detailed systems design and written test plans
 * Experience preparing installation instructions and coordinating installation procedures
 * Experience documenting data audits, archiving, and restoration processes
 * Experience with version control systems Git
 * Experience with tracking and ticket software (Jira, Confluence, etc.)
 * Familiarity with data architecture, data integration, data governance, and data lineage concepts




EDUCATION




Bachelor's Degree in computer science or a related discipline




Experience




Minimum of 5-10 years of IT/IS experience




Need to have excellent communication skills




5+ years working with Advanced Structured Query Language (SQL) & PL/SQL




5+ years experience with at least Oracle relational database




5+ years experience in software development lifecycle activities




5+ years experience with data loading (ETL, ELT)




5+ years working with data at scale 50+ TB




Experience with data warehouses, operational data stores, data hubs




Experience in end-to-end design of near-real-time and batch data pipelines




Experience working in an Agile environment




Experience developing detailed systems design and written test plans




Experience preparing installation instructions and coordinating installation procedures




Experience documenting data audits, archiving, and restoration processes




Experience with version control systems Git




Experience with tracking and ticket software (Jira, Confluence, etc.)




Familiarity with data architecture, data integration, data governance, and data lineage concepts




NICE TO HAVE




Leading a team




Master's Degree




Health care experience




Professional certifications




Other Programming Language Experience




Experience in Machine learning or Artificial Intelligence




Familiarity with microservices




Familiarity with DevSecOps




2+ years working with one NoSQL database (Hadoop)




Experience with MongoDB, Redshift, Synapse, or others is a plus.




Experience working in one public cloud environment (Azure, AWS, etc.)




Snowflake familiarity




Experience with containerization (Docker, Kubernetes, etc.)




Familiarity with agile and lean methodologies




Familiarity with JSON, XML




PERSONAL




Welcomes new approaches and innovative thinking




Self-organized and responsible with experience in a distributed team




Able to multitask and be responsive/flexible to support customers




Ability to work with others from diverse skill sets and backgrounds




Takes ownership of a situation and sees it through the completion




Able to switch context and complete work processes","Mid-Senior level","https://www.linkedin.com/jobs/view/data-engineer-remote-at-smartiplace-3713002852?trk=public_jobs_topcard-title","United States","2 months ago","","","2023-09-08","","Information Technology & Services","Data Engineer [Remote]","Information Technology"
"69 applicants","2358151","Praescient Analytics","https://www.linkedin.com/company/praescient-analytics?trk=public_jobs_topcard-org-name","Full-time","Praescient Analytics, LLC is a Veteran-led, certified Woman-Owned Small Business (WOSB) founded in 2011 which specializes in implementing advanced analytics solutions across the defense, intelligence, and law enforcement communities. Praescient has extensive experience designing, developing, and integrating solutions for customers including the US Army, Special Operations Command (SOCOM), US Navy (USN), US Marine Corps (USMC), US Coast Guard (USCG), Department of Justice (DOJ), Drug Enforcement Administration (DEA), and Federal Bureau of Investigation (FBI), Immigration and Customs Enforcement (ICE), intelligence community, and local law enforcement agencies, among others.

Praescient Analytics is in search of a Data Engineer to help develop best practices and providing consultative advice across a wide range of big data technologies and data management tools for our client. The Data Engineer will be expected to perform duties such as evaluating the performance of current data integrations, data management tools, and associated processes and procedures within a cloud systems environment. This position is fully remote. US Citizenship and a Public Trust clearance will be REQUIRED.

Primary Responsibilities


 * Manage and maintain ADLS file storage directories to support large multi-stage ETL pipelines
 * Implement pipelines to transfer sever-based data sources into ADLS, or directly to Synapse where appropriate.
 * Establish quality controls for any manual uploads of flat file extracts into ADLS
 * Implement ETL/ELT process and architecture within Azure Synapse and Azure Data Factory, including creating lookup tables where necessary to improve performance or efficiency
 * Implement fraud flag tables by entity and individual, linking all identified fraud flags for easy access via SQL query.
 * Implement aggregate views to quickly summarize key information and enable high performance queries by analysts and other users
 * Document all transformation processes and data architecture, including any derived or aggregate fields created to support the architectural design.
   
   

Required Qualifications


 * 5+ years of hands-on experience maintaining SQL databases, including expert level skills in SQL and T-SQL.
 * 5+ years of hands-on experience working in cloud-based environments with distributed SQL pools.
 * Experience with designing and implementing ETL/ELT processes in support of cloud-based data analytics environments.
   
   

Desired Qualifications


 * Experience with distributed systems utilizing tools such as Apache Hadoop and/or Spark.
 * Experience with Azure cloud offerings.
 * Experience with big data analytic tools such as Databricks or Azure Synapse.
 * Experience working with graph databases such as Neo4j.
 * Working experience in Agile Scrum environments.
 * Experience with source control tools such as GitHub or Azure DevOps
 * Bachelor’s degree in Data Science, Business Intelligence, Computer Science or related fields, or the equivalent combination of education, professional training, and work experience
   
   

If you are a self-starter who is passionate about data analysis and eager to make a meaningful impact, we invite you to apply for this exciting opportunity with Praescient Analytics!


 * Very competitive salary based on qualifications and experience.
 * Comprehensive, Company paid healthcare for you (We pay your premiums and deductibles)
 * 401(k) with company matching
 * Travel & performance incentives
 * 26 days of paid time off
 * $5K annual training allowance
 * $500 book allowance
 * Tuition reimbursement program
   
   

Praescient Analytics is committed to providing equal employment opportunities for all. Employees and applicants without regard to race, color, age, religion, sex, sexual orientation, marital status, gender identity, national origin, legally protected physical or mental disability, genetic information, citizenship status, or status in the uniformed services of the United States, status as a disabled veteran or veteran of the Vietnam era, or on any other basis which is protected under applicable law. This includes a commitment to provide a work environment that is free from all forms of illegal harassment including sexual harassment.

This covers all terms and conditions of employment including (but not limited to):


 * Hiring, placement, promotion, transfer, or demotion of all job classifications;
 * Recruiting, advertising, or solicitation for employment;
 * Treatment during employment;
 * Rates of pay or other forms of compensation;
 * Benefits;
 * Selection for training;
 * Company sponsored social and recreational activities; and
 * Layoff or termination
   
   

Applicants selected will be subject to a government security investigation and must meet eligibility requirements for access to classified information.

US Citizenship Required

Praescient Analytics is an Equal Opportunity Employer.

Interested Candidates: Please forward your resume to recruiting@praescientanalytics.com and please visit our website to apply online at www.praescientanalytics.applicantstack.com/x/openings.","Entry level","https://www.linkedin.com/jobs/view/data-engineer-100%25-remote-at-praescient-analytics-3768491689?trk=public_jobs_topcard-title","Fairfax, VA","2 weeks ago","","","2023-11-17","","IT Services and IT Consulting","Data Engineer (100% Remote)","Information Technology"
"142 applicants","35640959","Alivia Analytics","https://www.linkedin.com/company/aliviaanalytics?trk=public_jobs_topcard-org-name","Full-time","Job Description

Particular Details

Job Title

Big Data Engineer

Position Level

Senior-Level/Experience Professional

Industry

Information Technology

Total Position

01

Job Type

Full Time

Company

Alivia Analytics

About The Company

Alivia Analytics™ is following a mission to bend the healthcare cost curve by using analytic applications and data science to identify cases of fraud, waste, and abuse (FWA). By turning mountains of data into actionable answers, Alivia Analytics™ does the heavy lifting – delivering the accuracy, confidence, and speed you need to solve the healthcare payment integrity challenges. By putting powerful, easy-to-use, advanced technology into the hands of payment integrity business leaders and experts, Alivia Analytics™ empowers users to go beyond recovery to prevention and simulation within days. Alivia takes pride in stating that Alivia is observing the lowest false positive rate in the industry.

Currently, approximately 8 trillion dollars are spent globally in healthcare annually from which FWA accounts for up to 10% of every dollar spent in the healthcare systems. That equates to almost 800 billion dollars lost annually. FWA is a growing problem - as providers find ways to evade traditional forms of rule-based fraud detection, our clients need to enhance their firepower with advanced analytic systems. Our development team and data science team build applications using JavaScript that leverage algorithms built using Python, R, and SQL to identify these bad actors.

About The Position

We have an opportunity for a highly motivated senior-level: Data Engineer to join our rapidly growing team. You will have broad opportunities to succeed and grow, both technically and non-technically. Our development team is the keystone of our corporate structure, directly translating business problems into technical solutions for our global clients. This makes communication skills as important as developing skills. As a start-up, we want you to be able to grow with us. You’ll be able to learn from a management team with a combined 60+ years of technical and medical expertise and a history of successful exits. Our founder has developed 70+ systems in his career, 2 of which are still number 1 in the world today.

About The Role

We are seeking a highly skilled Cloud Data Engineer with at least 5 years of experience in designing, developing, documenting, and integrating applications using Big Data platforms like Snowflake, Databricks, Hadoop, and Hive. The successful candidate will have expertise in deploying these pipelines to cloud infrastructure hosted in AWS or Azure.

Job Description/Key Areas Of Responsibilities


 * Gather requirements from business/user groups to analyze, design, develop, and implement data pipelines according to customer requirements.
 * Process data from Azure/AWS data storage using Databricks and Snowflake
 * Optimize table design and indexing for end-user ease of use as well as workload performance.
 * Work with various input file formats including delimited text files, log files, Parquet files, JSON files, XML files, Excel files, and others.
 * Develop automated ETL procedures to load data from various sources into our application’s data warehouse.
 * Ensure pipeline structure is standardized across different customers, each may have their own unique input data format.
 * Configure monitoring systems to detect failure and performance degradation of ETL pipelines.
 * Work with the DevOps team to design CI/CD pipelines to conduct ETL upgrades.
 * Deploy and leverage cloud infrastructure and services to assist in ETL pipeline definition and automation.
 * Understand data modeling (Dimensional and relational) concepts like Star-Schema Modeling, Schema Modeling, Fact, and Dimension tables.
 * Have strong knowledge of both SQL and No SQL databases.
 * Collaborate with business partners, operations, senior management, etc. on day-to-day operational support.
 * Work with high volumes of data with stringent performance requirements
 * Use programming languages like Python to clean raw data before processing (e.g., removing newline characters/delimiters within fields)
 * Define data quality and validation checks to preemptively detect potential issues.
 * Ensure ETL pipelines are HIPAA-compliant, run with minimal permissions, and securely manage any passwords and secrets used for authentication.
 * Document ETL pipeline logic, structure, and field lineage for review by both technical and non-technical audiences
 * Expertise in processing data from Azure/AWS data storage using Databricks and Snowflake.
   
   

Key Technical Skills Set:

Data Modeling: Understanding of data modeling concepts, including Star-Schema Modeling, Schema Modeling, Fact and Dimension tables.

Database Knowledge: Strong knowledge of both SQL and NoSQL databases.

ETL Development: Developing automated ETL procedures to load data from various sources into a data warehouse.

Ensuring ETL pipelines are HIPAA-compliant and securely manage authentication credentials.

File Formats: Working with various input file formats, including delimited text files, log files, Parquet files, JSON files, XML files, Excel files, etc.

Performance Optimization: Optimizing table design and indexing for end-user ease of use and workload performance.

Monitoring and CI/CD: Configuring monitoring systems to detect pipeline failures and performance degradation.

Collaborating with the DevOps team to design CI/CD pipelines for ETL upgrades.

Data Quality and Validation: Defining data quality and validation checks to preemptively detect potential issues in data.

Programming Languages: Proficiency in programming languages like Python (for data cleaning and preprocessing), R, Java, and Scala.

Version Control: Experience with version control systems like Git for managing code and configurations.

Problem-Solving: Excellent problem-solving skills, including troubleshooting and resolving issues in data pipelines.

Documentation: Documenting ETL pipeline logic, structure, and field lineage for technical and non-technical audiences.

Communication and Collaboration: Strong communication and collaboration skills to work with stakeholders from different backgrounds and levels of expertise.

Data Engineer Requirements


 * Bachelor’s degree in computer science or a related field
 * 5+ years of experience in designing, developing, documenting, and integrating applications using Big Data platforms like Snowflake and Databricks
 * Extensive experience working on both Azure and AWS, ideally using native ETL tooling (e.g., Azure Data Factory)
 * Strong experience in cleaning, pipelining, and analyzing large data sets.
 * Adept in programming languages like R, Python, Java, and Scala
 * Experience with git for version control
 * Excellent problem-solving skills and ability to work independently and as part of a team.
 * Strong communication and collaboration skills, with the ability to work with stakeholders from different backgrounds and levels of expertise.
   
   

Company Description

Alivia Analytics is helping customers Achieve Healthcare Payment Integrity, Finally. By turning mountains of data into actionable answers, Alivia Analytics does the heavy lifting – delivering the accuracy, confidence, and speed our customers need to solve their healthcare payment integrity challenges. Through the Alivia Analytics Healthcare Payment Integrity Suite TM we help private and public healthcare payers achieve payment integrity globally. In the US alone, up to 10% of every dollar spent is attributed to Fraud, Waste, or Abuse which amounts to up to 370 Billion dollars lost annually. If your ambition is to grow your responsibilities and career while building world-class analytic SaaS systems and fixing a huge problem for social good, please come and join us.

Alivia Analytics is helping customers Achieve Healthcare Payment Integrity, Finally. By turning mountains of data into actionable answers, Alivia Analytics does the heavy lifting – delivering the accuracy, confidence, and speed our customers need to solve their healthcare payment integrity challenges. Through the Alivia Analytics Healthcare Payment Integrity Suite TM we help private and public healthcare payers achieve payment integrity globally. In the US alone, up to 10% of every dollar spent is attributed to Fraud, Waste, or Abuse which amounts to up to 370 Billion dollars lost annually. If your ambition is to grow your responsibilities and career while building world-class analytic SaaS systems and fixing a huge problem for social good, please come and join us.","Mid-Senior level","https://www.linkedin.com/jobs/view/senior-big-data-engineer-remote-at-alivia-analytics-3771508465?trk=public_jobs_topcard-title","Boston, MA","1 week ago","","","2023-11-22","","Hospitals and Health Care","Senior Big Data Engineer (Remote)","Engineering and Information Technology"
"Be among the first 25 applicants","90431993","Get It Recruit - Information Technology","https://www.linkedin.com/company/get-it-recruit-information-technology?trk=public_jobs_topcard-org-name","Full-time","Our mission is simple: we want to set people free to do meaningful work. People love our software—and it turns out that people love working here too. We've been recognized as a ""Best Company to Work For,"" and we're proud of our team for creating software that makes an impact in the lives of HR pros and employees all over the world.

What You'll Do

As a Senior Data Engineer on the data platform team, you'll play a crucial role in developing, deploying, and supporting data systems, pipelines, lakes, and lakehouses. Your expertise in automation, performance tuning, and scaling the data platform will be key to your success.

Your Initial Areas Of Focus Will Include

Collaborate with stakeholders to make effective use of core data assets.

Load both streaming and batched data using Spark and Pyspark libraries.

Engineer lakehouse models to support defined data patterns and use cases.

Build scalable data pipelines using a combination of tools, engines, libraries, and code.

Work within an IT managed AWS account and VPC to maintain data platform development, staging, and production environments.

Document data pipelines, cloud infrastructure, and standard operating procedures.

Express data platform cloud infrastructure, services, and configuration as code.

Automate load, scaling, and performance testing of data platform pipelines and infrastructure.

Monitor, operate, and optimize data pipelines and distributed applications.

Ensure appropriate data privacy and security.

Automate continuous upgrades and testing of data platform infrastructure and services.

Build data pipeline unit, integration, quality, and performance tests.

Participate in peer code reviews, code approvals, and pull requests.

Identify, recommend, and implement opportunities for improvement in efficiency, resilience, scale, security, and performance.

What You Need to Get the Job Done (if you don't have all, apply anyway!):

Experience developing, scaling, and tuning data pipelines in Spark with PySpark.

Understanding of data lake, lakehouse, and data warehouse systems, and related technologies.

Knowledge and understanding of data formats, data patterns, models, and methodologies.

Experience storing data objects in Hadoop or Hadoop-like environments such as S3.

Demonstrated ability to deploy, configure, secure, performance tune, and scale EMR and Spark.

Experience working with streaming technologies such as Kafka and Kinesis.

Experience with the administration, configuration, performance tuning, and security of database engines like Snowflake, Databricks, Redshift, Vertica, or Greenplum.

Ability to work with cloud infrastructure, including resource scaling, S3, RDS, IAM, security groups, AMIs, CloudWatch, CloudTrail, and Secrets Manager.

Understanding of security around cloud infrastructure and data systems.

Git-based team coding workflows.

Bonus Skills (Not Required, So Apply Anyway!):

Experience deploying and implementing lakehouse technologies such as Hudi, Iceberg, and Delta.

Experience with Flink, Presto, Dremio, Databricks, or Kubernetes.

Experience with expressing infrastructure as code leveraging tools like Terraform.

Experience and understanding of a zero trust security framework.

Experience developing CI/CD pipelines for automated testing and code deployment.

Experience with QA and test automation.

Exposure to visualization tools like Tableau.

Beyond the technical skills, we're looking for individuals who are:

Clear communicators with team members and stakeholders.

Analytical and perceptive of patterns.

Creative in coding.

Detail-oriented and persistent.

Productive in a dynamic setting.

Schedule

9AM-5PM, Monday-Friday

Work Environment

Remote

If you love to learn, you'll be in good company. You'll likely have a Bachelor's degree in computer science, information systems, or equivalent working experience.

Apply Now!

Employment Type: Full-Time","Mid-Senior level","https://www.linkedin.com/jobs/view/senior-data-engineer-remote-at-get-it-recruit-information-technology-3773058938?trk=public_jobs_topcard-title","Grand Prairie, TX","1 day ago","","","2023-12-03","","Human Resources Services","Senior Data Engineer - Remote","Information Technology"
"71 applicants","164380","Innovative Solutions","https://www.linkedin.com/company/innovative-solutions?trk=public_jobs_topcard-org-name","Full-time","As a member of our Professional Services team, you will assist in the creation, migration, and support of critical customer cloud infrastructure. In this role, you will provide thought leadership and hands-on technical execution to optimize client environments and ensure robust, scalable solutions that meet their needs.

Responsible for:


 * Collaborate with cross-functional teams to understand business needs and requirements, and translate them into effective cloud-based data solutions
 * Design, build, and maintain cloud architecture that supports data storage, processing, and analysis using services in AWS
 * Develop and implement data pipelines, ETL processes, and data transformations to ensure efficient data flow and data integrity
 * Apply machine learning and AI techniques to extract meaningful insights from data, enabling predictive and prescriptive analytics
 * Assist in the deployment and management of AI/ML models to support business objectives
 * Monitor and troubleshoot data pipelines, ensuring optimal performance and scalability
 * Stay current with emerging trends and technologies in data analytics, AI/ML, and cloud computing, and contribute insights to enhance our technical capabilities
 * Working with Account Managers and clients to provide recommendations and technology roadmaps to meet their business needs
 * Actively participating in team meetings and cross-functional interactions
 * Keeping team members and supervisors informed of progress and issues
 * Actively contributing to client project status meetings
 * Contributing to R&D projects to validate/invalidate new services offerings
 * Mentoring and guiding members of the delivery team in a technical and non-technical capacity
 * Remaining current with technology trends and new technologies
 * Proposing the latest technology usage and integration standards
 * Potential need for on-call availability
   
   
   

How you will be successful


 * Living and breathing the “cloud-first” approach
 * Having a high degree of analytical thinking capability to solve complex business problems
 * Obsessively delivering amazing customer experiences
 * Becoming a subject matter expert in one or more cloud platforms in 9-12 months
 * Building trusting relationships with team members and collaborating departments
 * Comfortable with pushing boundaries and technical limits (maintain technical aptitude and knowledge of industry)
 * Always be learning
   
   
   

What Experience You Need


 * 5+ years of professional IT experience
 * 2+ years of professional AWS or Azure experience
 * At least (1) AWS or Azure Certification (Associate level)
 * Programming languages like Python, R, Java, Scala etc. to build data pipelines, perform data analysis and create machine learning models
 * SQL and NoSQL databases like PostgreSQL, MySQL, MongoDB, Cassandra etc. to store and query large datasets
 * Data modeling skills - ability to design conceptual, logical and physical data models
 * Understanding of dimensional modeling, star schemas, data warehouses
 * ETL (Extract, Transform, Load) tools like Informatica, Talend, Pentaho etc. to integrate and move data between systems
 * Big data frameworks like Hadoop, Spark, Kafka etc. for distributed data processing and building data lakes
 * Machine learning frameworks like Tensorflow, PyTorch, Keras, Scikit-Learn for building ML models
 * Knowledge of data architecture patterns like lambda, kappa architecture. Ability to design scalable and flexible data pipelines
   
   
   

Compensation Disclosure:

At Innovative, compensation depends on your role's location, skills required, experience, complexity, travel, and market rate. Our total rewards include base pay, bonuses, incentives, stellar benefits like health insurance and retirement savings, and paid time off. We aim to reward performance and attract top talent with competitive, fair pay and benefits that comply with wage laws. We want to provide full transparency that the top of the salary range represents exceptional, senior level and meeting all required qualifications. Actual offers take experience level into account.","Not Applicable","https://www.linkedin.com/jobs/view/data-engineer-at-innovative-solutions-3773897918?trk=public_jobs_topcard-title","United States","4 days ago","","","2023-11-29","","Transportation, Logistics, Supply Chain and Storage","Data Engineer","Information Technology"
"88 applicants","3513709","Experfy","https://www.linkedin.com/company/experfy?trk=public_jobs_topcard-org-name","Full-time","As a Data Engineer for our Data Platform Engineering team you will join skilled Scala/ Spark engineers and core database developers responsible for developing hosted cloud analytics infrastructure (Apache Spark-based), distributed SQL processing

frameworks, proprietary data science platforms, and core database optimization. This team is responsible for building the automated, intelligent, and highly performant query planner and execution engines, RPC calls between data

warehouse clusters, shared secondary cold storage, etc. This includes building new SQL features and customer-facing functionality, developing novel query optimization techniques for industry-leading performance, and building a database

system that's highly parallel, efficient and fault-tolerant. This is a vital role reporting to exec leadership and senior engineering leadership

Requirements

Responsibilities:


 * Writing Scala code with tools like Apache Spark + Apache Arrow + Apache Kafka to build a hosted, multi-cluster data warehouse for Web3
 * Developing database optimizers, query planners, query and data routing mechanisms, cluster-to-cluster communication, and workload management techniques
 * Scaling up from proof of concept to ""cluster scale"" (and eventually hundreds of clusters with hundreds of terabytes each), in terms of both infrastructure/architecture and problem structure
 * Codifying best practices for future reuse in the form of accessible, reusable patterns, templates, and code bases to facilitate meta data capturing and management
 * Managing a team of software engineers writing new code to build a bigger, better, faster, more optimized HTAP database (using Apache Spark, Apache Arrow, Kafka, and a wealth of other open source data tools)
 * Interacting with exec team and senior engineering leadership to define, prioritize, and ensure smooth deployments with other operational components
 * Highly engaged with industry trends within analytics domain from a data acquisition processing, engineering, management perspective
 * Understand data and analytics use cases across Web3 / blockchains
   
   

Skills & Qualifications


 * Bachelor's degree in computer science or related technical field. Masters or PhD a plus
 * 6+ years experience engineering software and data platforms / enterprise-scale data warehouses, preferably with knowledge of open source Apache stack (especially Apache Spark, Apache Arrow, Kafka, and others)
 * 3+ years experience with Scala and Apache Spark (or Kafka)
 * A track record of recruiting and leading technical teams in a demanding talent market
 * Rock solid engineering fundamentals; query planning, optimizing and distributed data warehouse systems experience is preferred but not required
 * Nice to have: Knowledge of blockchain indexing, web3 compute paradigms, Proofs and consensus mechanisms... is a strong plus but not required
 * Experience with rapid development cycles in a web-based environment
 * Strong scripting and test automation knowledge
 * Nice to have: Passionate about Web3, blockchain, decentralization, and a base understanding of how data/analytics plays into this","Mid-Senior level","https://www.linkedin.com/jobs/view/data-engineer-database-engineering-at-experfy-3590303102?trk=public_jobs_topcard-title","Hollywood, FL","8 months ago","","","2023-04-03","","Technology, Information and Internet","Data Engineer, Database Engineering","Information Technology"
"168 applicants","461893","CorEvitas, LLC","https://www.linkedin.com/company/corevitas?trk=public_jobs_topcard-org-name","Full-time","Position Summary

The Data Engineer will be responsible for designing and developing our data and data pipeline architecture, as well as optimizing data flow and collection for cross-functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure that optimal data delivery architecture is consistent throughout projects. The successful candidate will be self-motivated and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.

This is an important, visible, roll-up-your-sleeves position that enables our biostatisticians, pharmacovigilance experts and epidemiologists to gain insights and critical knowledge, enabling them to make better decisions faster.

Principle Duties And Responsibilities

 * Work together with the Director, Data Engineering and the software development team to realize the data architecture and data processing pipelines on AWS.
 * Create and maintain optimal data pipeline architecture.
 * Assemble large, complex data sets that meet functional / non-functional business requirements.
 * Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
 * Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.
 * Build analytics tools that utilize the data pipeline to provide actionable insights.
 * Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
 * Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.
 * Create data tools for analytics and data scientist team members that assist them in building and optimizing our life sciences offerings.
 * Promote data best-practices across the organization and help build a “culture of data”.
   

Skills/Knowledge

MINIMUM QUALIFICATIONS:

Must have Strong SQL Server Database development, maintenance experience.

Must have working knowledge SSRS reporting.

Must have proficiency with SQL Server functions and stored procedures.

Must be able to debug, profile, tune SQL queries, functions and stored procedures.

NoSQL Database experience is a big plus.

Proficiency in at least one high-level programming language, e.g., Python or Java.

Demonstrated experience developing data pipelines/ETL/ELT processes.

Excellent communication and organizational skills.

Also, Cloud Platform Experience That Include

 * AWS data storage and retrieval experience highly preferred
 * AWS data services, e.g., AWS Glue
 * AWS data processing and analytics services
   

Big Plus

 * Familiarity working with sensitive data and with CFR Part 11, HIPAA, GDPR compliance.
 * Working knowledge of clinical and pharma data
 * Working knowledge of Clinical Data Standards such as CDISC
   

Experience

5+ yeas overall experience, at least 3 in data management and/or data-centric software development. Life sciences background preferred.

Experience in writing production-level SQL and working with various databases and reporting systems.

Education/training

BS/MS degree in Computer Science, Computer Engineering, or other technical discipline.

Special Requirements

Fully Remote, though some Travel may be required for this role

This description is not intended to be a complete statement of the job, but rather to act as a guide

About CorEvitas

CorEvitas, now part of Thermo Fisher Scientific, is a science-led, real-world data intelligence company. Using syndicated registry data and analytic services to understand the post-approval comparative effectiveness and safety of approved therapies, CorEvitas provides biopharmaceutical companies with objective data and clinical insights to demonstrate the value of their products to clinicians, patients, payers, and regulators. The company operates nine major autoimmune and inflammatory registries across the U.S., Canada, and Japan, collecting data from over 400 participating investigator sites, including collection of biosamples linked to the deep clinical data. CorEvitas recently expanded its services to include Pregnancy Registries, through the acquisition of Pregistry. CorEvitas also conducts client-sponsored registries through its Patient Powered Registries business, employing a transformative patient-focused registry model to support research needs for patient-centered outcomes across all therapeutic areas. The company’s regulatory-grade registry data is complemented by its Patient Experience business, supporting evidence-based patient engagement initiatives across the product lifecycle, as well as its Specialty EMR Data business and retinal data set. CorEvitas is headquartered in Waltham, MA and is a portfolio company of Audax Private Equity. www.corevitas.com

CorEvitas is proud to provide equal employment opportunities to all qualified individuals without regard to race, color, religion, sex, gender identity, sexual orientation, pregnancy, age, national origin, physical or mental disability, military or veteran status, genetic information, or any other protected classification. Minorities, women, LGBTQ candidates, Veterans, and individuals with disabilities are encouraged to apply.

CorEvitas participate with E-Verify","Entry level","https://www.linkedin.com/jobs/view/data-engineer-remote-at-corevitas-llc-3736699076?trk=public_jobs_topcard-title","Waltham, MA","1 month ago","","","2023-10-06","","Research Services","Data Engineer, Remote","Strategy/Planning and Information Technology"
"Over 200 applicants","92953763","TrustEngine","https://www.linkedin.com/company/thetrustengine?trk=public_jobs_topcard-org-name","Full-time","Job Summary:




TrustEngine is on a mission to revolutionize the mortgage tech industry and empower borrowers to achieve their financial goals. With our Borrower Intelligence Platform and the ethical use of data and machine learning, we equip lenders with the tools for meaningful conversations that support borrowers in pursuing their life goals. We're transforming the industry, fostering trust, making a positive impact on lives, and creating a future of #NoBorrowerLeftBehind. Join us on this thrilling journey. Unlock the full potential of our organization and accelerate our growth!




Bring your deep expertise in building out data pipelines and implementing data lake architecture. As a senior data engineer, you will elevate your team, foster collaboration, and drive continuous improvement to deliver industry-transforming products with technical excellence. You will contribute to the success of the company by designing and implementing a scalable data platform as the foundation of our tech stack. Collaborating with cross-functional teams, you will serve as a subject matter expert to drive data-driven decision-making and create innovative solutions for our customers.




Join us and shape the future of mortgage tech!




Responsibilities:




 * Design and develop robust data pipelines and ETL processes for ingesting, transforming, and integrating data from various sources into the data lake
 * Optimize data infrastructure for performance, reliability, and scalability, considering factors such as data volume, velocity, and variety
 * Ensure data quality, integrity, and security across the data lake, implementing data governance practices and monitoring mechanisms
 * Collaborate with cross-functional teams, including data scientists, analysts, and software engineers, to understand data requirements and develop solutions that meet business needs
 * Champion engineering best practices and drive improvements in data engineering methodologies, processes, and technologies
 * Stay updated with emerging trends and advancements in data engineering, sharing knowledge and insights with the team and the broader organization
 * Effectively communicate technical concepts, solutions, and recommendations to both technical and non-technical stakeholders




Requirements




Required Qualifications:




 * 4+ years of professional experience in data engineering, building scalable data pipelines and ETL processes
 * Strong proficiency in programming languages commonly used in data engineering, such as Java, Scala, or Python
 * In-depth knowledge of distributed systems, data processing frameworks (such as Hadoop, Spark, or Flink), data lake, and cloud computing platforms (e.g., AWS, Google Cloud, Azure)
 * Proficient in SQL and relational database technologies (e.g., MySQL, PostgreSQL)
 * Strong knowledge of data modeling, and data warehousing concepts
 * Familiarity with data governance, security, access controls, and compliance principles and solutions
 * Demonstrated ability to optimize data pipelines for performance, scalability, and reliability
 * Excellent problem-solving and analytical skills, with a passion for tackling complex data engineering challenges
 * Excellent communication skills, with the ability to contribute to a shared vision




Preferred Qualifications:




 * Experience with real-time data processing and streaming frameworks (e.g., Kafka, AWS Kinesis)
 * Knowledge of modern data lakehouse technologies such as Apache Hudi, Delta Lake, or Apache Iceberg
 * Experience with NoSQL databases (e.g., Redis, Cassandra)
 * Experience with data visualization tools (e.g., Tableau, Metabase) and data exploration techniques
 * Contributions to open-source data engineering projects or active participation in the data engineering community




Education and Experience:




 * Bachelor's degree in Computer Science or a related technical field. Advanced degrees are preferred but not required. Equivalent practical experience will also be considered




Benefits




Our benefits include but are not limited to the following: 100% company paid medical; company matching 401(k), paid maternity and paternity leave, unlimited FTO package, ongoing professional development and certification opportunities, competitive salary, special employee discounts and health and wellness perks.","Mid-Senior level","https://www.linkedin.com/jobs/view/senior-data-engineer-at-trustengine-3722210170?trk=public_jobs_topcard-title","United States","2 months ago","","","2023-09-14","","Technology, Information and Internet","Senior Data Engineer","Engineering"
"47 applicants","5341320","HANDLE® Global","https://www.linkedin.com/company/handleglobal?trk=public_jobs_topcard-org-name","Full-time","Join Us as a Data Engineer - Shape the Future of Healthcare Analytics with HANDLE Global!

Company Overview:

HANDLE Global, a leading healthcare supply chain analytics and fulfillment solutions provider, is dedicated to revolutionizing the healthcare industry by providing cutting-edge software solutions. Our innovative products help healthcare providers optimize operational efficiency, enhance patient care, and drive business growth.

What Sets You Apart:


 * Adventure Awaits: Dive into the cutting-edge world of HANDLE Global. We stand at the vanguard of healthcare supply chain analytics, and we're looking for a Data Engineer who's eager to shape the industry with us.
 * Data Maestro: Spearhead the creation and maintenance of our warehouse data models. As the primary architect and developer for our BI dashboards, you'll ensure our visualizations aren't just accurate, but also insightful, optimizing data extraction and interpretation at every turn.
 * Project Trailblazer:.Embrace the responsibility for ad-hoc analytics and distinct project undertakings. As healthcare evolves, so do our business needs. Your innovative analytics solutions will be instrumental in steering our trajectory.
 * Stakeholder Connector: Liaise seamlessly with internal and external partners, translating business requirements into actionable technical solutions. Your ability to bridge the gap between complex datasets and real-world needs will be invaluable.
 * ELT Enthusiast: Dive into the depths of data source ELT development and maintenance. Your collaborations with our data engineering resources will help streamline customer data pipelines and ensure consistency in our analytics endeavors.
 * Code Curator & Communicator: Own the documentation for your developed code, ensuring it's clear and easily referenced. Beyond that, articulate your findings and methodologies to a diverse audience, making the complex easily comprehensible.
 * Opportunity Harbinger: As challenges arise, seize them. Whether they're on the radar or unforeseen, your ability to navigate, innovate, and drive us forward will set the pace for our growth.
   
   

Qualifications and skills:

Essentials:


 * Bachelor’s degree in Computer Science, IT, or equivalent combination of education, training, and experience.
 * 5+ years of experience with SQL. and 3+ years with Python / Spark
 * Intrigued by the term “modern data stack"" and embody a lifelong learning attitude.
 * Demonstrated prowess in problem-solving.
 * Proficient with cloud data warehouses such as Databricks, Snowflake, BigQuery, etc.
 * Knowledge of data quality standards and schema enforcement.
 * Familiarity with version control platforms like GitLab or Github.
 * Thrives when balancing both analyst and engineering responsibilities.
 * Excellence in documentation - both in interpretation and creation/curation.
 * Enjoys the journey of defining requirements, capturing business logic, and generating value.
   
   

Preferred:


 * Agile or startup environment background
 * Knowledge in constructing medallion (bronze, silver, gold) style pipelines.
 * Expertise in query optimization
 * Experience with dbt, Delta Live Tables, or BI semantics layers for data model building and maintenance
 * Experience with CI/CD pipelines and platforms like AWS/Azure/GCP.
 * Track record of creating comprehensive data dictionaries.
 * Familiarity with data orchestration tools such as Airflow, Prefect, Dagster, etc.
 * A penchant for DAGS and Mermaid.
   
   

HANDLE Global is an equal opportunity employer committed to valuing and promoting diversity. We base all employment decisions on merit, competence, and business needs, without discrimination based on protected statuses. All qualified applicants are encouraged to apply.

Join HANDLE Global, where your expertise will not just shape our company but the future of healthcare analytics. Apply Now and be part of the revolution!

Powered by JazzHR

oKcqriYu96","Mid-Senior level","https://www.linkedin.com/jobs/view/data-engineer-at-handle%C2%AE-global-3769118159?trk=public_jobs_topcard-title","Prospect, KY","1 month ago","","","2023-10-24","","Internet Publishing","Data Engineer","Information Technology"
"Over 200 applicants","654502","Vevo","https://www.linkedin.com/company/vevo?trk=public_jobs_topcard-org-name","Full-time","Vevo is the world's leading music video network, connecting an ever-growing global audience to high quality music video content for more than a decade. Founded by Universal Music Group and Sony Music Entertainment in 2009, Vevo offers fans worldwide a vast array of premium content to choose from, showcasing official music videos alongside a constantly developing lineup of live performances and innovative original programming. From top superstars to rising new talents, Vevo brings incomparable cross-promotional support to artists across the musical spectrum, at every stage of their careers.

Vevo has consistently evolved over the past decade to lead within today's ever-changing media landscape, embracing partnerships with a number of leading distribution platforms to deliver extraordinary content within ad-supported environments. With more than 25B views across television, desktop and mobile devices each month, Vevo brings music videos to the world – when, where, and how fans want them.

As a Data Engineer at Vevo, you will extend and maintain the data pipelines that feed our ever growing data lake. Join a small autonomous team responsible for this data lake and its ingress and egress pipelines. Through this data lake and its data pipelines you will be providing immensely important data to internal business analysts, data scientists, Vevo leadership, as well as content partners in a multi-billion dollar industry.

As a Member Of Our Team, You Will


 * Be responsible for designing, building and supporting components that compose the Vevo data lake and it’s pipelines
 * Help build and extend our data lake by designing and implementing: data pipeline libraries and systems, internal analytics tooling / dashboards, and monitoring and alerting dashboards
 * Provide support for the data pipelines including after-hours support on a rotational basis
 * Work in a collaborative environment with other data engineers, data scientists, and software engineers to achieve important goals for Vevo
   
   

This Describes You


 * You believe in values like effectiveness over efficiency and quality over quantity
 * You desire to continuously improve your team, your product and yourself
 * You have pragmatic communication and problem-solving skills
 * You are opinionated yet humble
   
   

Requirements


 * BS/MS in computer science or equivalent experience in data engineering
 * You love different types of data. i.e. content metadata, viewership metrics, etc.
 * You love to solve difficult and interesting problems using data from various systems
 * You have experience developing and maintaining software in Python
 * You have experience with data pipelines that process large data sets via streams and/or batches
 * You have experience in building services, capable of handling large amounts of data.
 * You have experience building and maintaining tests (unit, integration, etc.) that provide necessary quality checks. TDD experience a plus
 * You have experience with modern persistence stores, primarily SQL; however NoSQL experience is a plus
 * You embrace best practices via pair programming, constructive code reviews, and thorough testing.
 * You thrive in an environment with rapid iterations on platform features
 * You're a team player and work well in a highly collaborative environment, which includes staff in remote locations
   
   

Interested? Great! You might like to know:


 * We are a tight community of fun, upbeat people with a real passion for music and technology
 * We have premier access to music content and new releases of original media content
 * We provide excellent benefits and competitive compensation packages","Entry level","https://www.linkedin.com/jobs/view/data-engineer-at-vevo-3764348307?trk=public_jobs_topcard-title","United States","1 month ago","","","2023-10-28","","Entertainment Providers","Data Engineer","Information Technology"
"25 applicants","18949795","Loaded","https://www.linkedin.com/company/loadedgg?trk=public_jobs_topcard-org-name","Full-time","Loaded and Open World are market leaders in gaming talent management and strategic brand consultancy – in any of our business endeavors we aim to provide high-value services to support the development, marketing, and growth of the video gaming industry and community.

Over the past 7 years, we have facilitated partnerships with world-leading brands such as Netflix, Universal Music Group, Pepsi, Activision, Amazon, Spotify, Gillette, Google, Journey's, and Adidas to either build or deepen their connection to gamers.

We believe that gaming is one of the great connectors, and the communities that grow here change lives, move the culture forward, and bring tremendous value to the brands, creators, and fans that participate in it.

It's time for our team to grow, and we're looking for creative minds, committed hearts, and forward-thinking humans to join us and make the work we do the most impactful it can be.

If that ignites or intrigues you, keep reading...

We're looking for Data/Backend Engineer

The Loaded Product team is looking for a talented Engineer who will work alongside our pipeline engineer to warehouse and make available data to our various analytics products.

Reporting to an engineer turned VP of Product, the ideal candidate must be very pragmatic, productive, open-minded, and passionate about the space and the innovations they are making to it.

On any given day you might...


 * Design and develop sophisticated warehousing schemas, storage systems, and processes
 * Work closely with the data science team to programmatically generate and make available insights and calculations
 * Architect and provision cloud-based infrastructure in an AWS environment
 * Assist in the develop APIs that support our product initiatives
 * Collaborate with coworkers and stakeholders to design and scope new features and roadmap - acting as a gatekeeper for feasibility as it relates to data and its availability as well as being a resource for determining efficient pathing
 * Assisting in efforts to hire and build out our engineering team and process
 * Researching and making long-lasting architectural and design decisions for company-wide tech offerings
   
   
   

Ideally, you have...


 * A pragmatic, productive, and creative development and design philosophy
 * A passion and pride for the work they do and the problems that they solve
 * The ability to work autonomously, taking ownership of projects and confidently moving them forward
 * Opinionated but open-minded and established views about development solutions/processes/architecture/patterns and the ability to express them productively
 * Expertise in building scalable solutions in a modern development environment
 * Experience contributing to and designing data-heavy products
 * The ability to work on a very lean and high-impact team. There are no NPCs at Loaded, only main characters.
   
   
   

You earn bonus points if you...


 * Have strong expertise in the storage and gathering of user data and the privacy considerations that must be made
 * Are able to meaningfully contribute to existing Laravel + Python backend services
 * Have experience building applications and tools in the Discord/live streaming/gaming ecosystem
 * Are an avid Discord user
 * Have a strong understanding of live streaming content and/or consume it often
 * You have worked in or around marketing/management/gaming and understand the nuances of the industries
 * Build side projects in your spare time
 * Are a LIRIK sub.
   
   
   

The most important qualities that a candidate can have are passion, enthusiasm, and most importantly, curiosity. If you feel like you would do a great job, do not let self-doubt or imposter syndrome deter you from applying.

Benefits


 * Medical, Dental, and Vision Insurance
 * Company-paid life insurance, short term and long term disability insurance
 * 401k plan with 4% company matching
 * Flexible work schedule
 * Unlimited PTO
 * Dog-Friendly Office
 * Cell Phone, and Internet Service coverage
 * Team Events, expensable Lunches & Coffee
   
   
   

Loaded Holdings, Inc. is an Equal Opportunity / Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, national origin, disability, or protected Veteran status","Not Applicable","https://www.linkedin.com/jobs/view/data-backend-engineer-at-loaded-3768083232?trk=public_jobs_topcard-title","Los Angeles Metropolitan Area","6 days ago","","","2023-11-28","$160,000.00-$180,000.00","Transportation, Logistics, Supply Chain and Storage","Data/Backend Engineer","Engineering and Information Technology"
"131 applicants","3247643","Lynx Analytics","https://sg.linkedin.com/company/lynx-analytics?trk=public_jobs_topcard-org-name","Full-time","Company Overview

Lynx Analytics was founded in 2010 by a group of INSEAD students and professors with a strong research background in graph analytics. Several of our founders since then became professors and faculty directors of analytics centers at leading US universities. Our founding purpose? To apply graph theory to simplify and solve complex, real-world business problems.

Our mission has evolved over the years, and we currently offer a range of cutting edge data analytics and AI solutions to help companies transform their operations and optimise their commercial performance. Back then, graph theory was mostly the purview of social networking sites. We wanted to expand this technology and help companies leverage their communities to unlock greater growth.

Lynx has offices in Singapore, US, Hong Kong, Hungary, and operations in several other countries such as Canada, Germany, Indonesia. We work with some of the world's largest companies and are constantly looking to expand our knowledge base and geographical footprint. Lynx Analytics' technology is deployed with various Clients internationally and has significant growth potential.

We have a diverse and inclusive global team comprising Professors, PhDs, MSc's, and MBAs from Ivy Leagues, INSEAD and NUS with a broad spectrum of experience in start-ups and blue-chip companies (Google, Databricks, ZS, Abbvie, Amgen, Vodafone, Morgan Stanley, Palantir, Katana Graph to name but a few). It is the combination of our industry insight and experience, scalable proprietary technology, and highly qualified people that drives our compelling value proposition.

We are looking for ambitious, innovative, empathetic and relentless team players to explore the career opportunities that we offer as we continue to scale our operations.

Key Responsibilities

A Data Engineer's responsibility is to implement and deploy data analysis pipelines at various clients of Lynx Analytics. This includes participating in the activities below:


 * Understand deeply the business problem that we are trying to solve by our analytical solution
 * Through continuous consultations with employees of our client, discover the client's existing data sources that are relevant to the problem we try to solve. This includes discussions with client IT, data owners, future business users, etc.
 * Working together with the IT teams of the client, define the technical architecture for the analytical solution that we are to deploy for the client.
 * Implement the data ingestion subsystem: this is the system responsible for moving all the necessary data sources to a single location where the actual analysis will happen.
 * Implement the data analysis pipelines.
 * Integrate the results into business UIs developed by Lynx or pre-existing client software systems
   
   

Requirements


 * Relevant tertiary qualification, preferably at Masters level or above, in Engineering or another relevant discipline with strong academic results
 * Strong programming skills
 * Experience in GCP, Airflow and Spark
 * Solid experience in Python and SQL
 * Good problem-solving skills
 * Good communication skills
   
   

DESIRABLE


 * Experience in Big Data
 * A minimum of 3 years of experience in Data Science or Analytics
 * Industry experience in working for a big enterprise (like our clients)
   
   

What We Offer


 * Opportunity to work on creating innovative, leading-edge data science pipelines using our state of the art, in-house built big graph tool
 * Work closely with the developers of the (big graph) tool you will be building upon
 * Be a member of a very strong team with mathematicians, ex-Googlers, Ivy League professors, MBA alumni and telecommunications industry experts
 * Startup atmosphere","Entry level","https://www.linkedin.com/jobs/view/data-engineer-at-lynx-analytics-3641777500?trk=public_jobs_topcard-title","San Francisco, CA","6 months ago","","","2023-05-28","","Technology, Information and Internet","Data Engineer","Information Technology"
"Over 200 applicants","70516606","Chord","https://www.linkedin.com/company/chordcommerce?trk=public_jobs_topcard-org-name","Full-time","The opportunity

We’re looking for a Senior Data Engineer to elevate Chord as the leader in headless commerce and data. You’ll join a growing team that is building a platform to empower modern and omnichannel brands with smart technology, unique customer experiences, and insightful data.

In this role, you will build foundational infrastructure and associated data models that fuel best-in-class insights and a reporting toolkit intended to empower our customers with solid insights to drive their business. As our next Sr. Data Engineer, you will be a key member of the Data team and will report directly to our Chief of Data.

Chord offers advanced data and ecommerce infrastructure for modern brands that want to have cutting-edge technology and unified data without a large engineering team. We like to think of our product as the tech engine for high-velocity growth. Backed by top investors, Chord is a SaaS company led by industry veterans, and we’re seeking smart, focused, creative talent to help us realize our vision.

Our company is proudly remote-first, distributed across North America.

As a Senior Data Engineer, you will:


 * Chart the course for our data pipelines and data warehouse, optimizing for accuracy, performance, and accessibility
 * Build best-in-class data architecture and frameworks that enable a modern, scalable data product.
 * Lead architecture frameworks and the development of data, experimentation, and analytics solutions in collaboration with cross-functional partners in the Product and Engineering organizations
 * Provide production support for pipeline and data warehouse to tackle issues that arise with data load, data transformation, and reporting.
 * Test and clearly document data assets and warehouse implementations to enable others to understand the implementation and definition of data methodologies easily.
 * Focus on monitoring, CICD methodologies, and pipeline security.
   
   
   

To be successful in this role, you’ll need:


 * Previous experience with distributed systems at scale.
 * Demonstrated ability to build, manage, and optimize core data infrastructure
 * Experience building and maintaining custom ingestion pipelines
 * Experience gathering requirements and creating solutions for high data accessibility and usability
 * Comfortability defining and improving standards and frameworks for maintainable best practices for a high-scale data infrastructure -- you'll maintain and advocate for these standards
 * To be excited by the opportunity to optimize and automate our pipelines
 * Experience with DBT, SQL, Snowflake, and Fivetran.
 * Experience with cloud-based AWS ecosystems and products
 * Experience with real-time data streams, cohorting, data classification, and data lineage is a plus.
   
   
   

Working at Chord, you can expect:


 * An investment in your physical and mental well-being; we offer 100% employee Medical Benefits coverage and a percentage towards dependant coverage.
 * Flexible PTO; we encourage you to take the time you need to be your best self at work.
 * An onboarding package and annual work-from-home stipend to ensure you have everything you need to be successful while working remotely.
 * Generous Parental Leave with a customizable transition back to work program
 * To make an impact! We’re an early-stage company, which means there is space to champion ideas and create and lead initiatives at any level in the organization.
   
   
   

About your application and the interview process

Chord is an equal opportunity employer, and we value diversity. We do not discriminate based on race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. If you’re unsure about your qualifications for this position, we still encourage you to apply.

Our interview process for the role begins with an exploratory conversation with the Hiring Manager. After that, we’ll invite you to a Zoom session with various stakeholders from across our organization. We aim to get to know you and allow you to learn more about our team and product while being respectful of your time.

This is the expected salary range for US-based employment. This is a full-time, salaried position that includes Equity. We set standard ranges for all roles based on function, level, and geographic location, benchmarked against similar-stage growth companies in our market. This salary range represents the full salary range for the position. The starting base pay offered may vary depending on factors including experience, expertise, market demands, and internal parity.","Not Applicable","https://www.linkedin.com/jobs/view/senior-data-engineer-at-chord-3760354231?trk=public_jobs_topcard-title","New York, NY","2 weeks ago","","","2023-11-15","$139,500.00-$198,000.00","Software Development","Senior Data Engineer","Information Technology"
"197 applicants","3022081","uConnect","https://www.linkedin.com/company/uconnect-inc-?trk=public_jobs_topcard-org-name","Full-time","uConnect is on a mission to help more people realize their potential by improving access to career resources and services - early in their lives and throughout their careers. Initially focused on the post-secondary higher education market, uConnect's All-In-One Virtual Career Center is used by schools like UCLA, MIT, and Baton Rouge Community College to radically increase student and alumni engagement with career services, and integrate career planning into the student experience, before, during and after college.

We are committed to lowering barriers to opportunity for students and recent graduates from all backgrounds. Our customers are some of the most dedicated and capable career services professionals in the world and our software helps them be even more successful in their mission of helping students.

We have created a working environment that is fun and collaborative and puts people first. We have a physical office in Cambridge, MA but operate as a remote-first organization, encouraging mobility for all employees, and to work from their chosen environment. uConnect is backed by leading education technology investors including Growth Street Partners, Strada Education and LearnLaunch.

Summary:

uConnect is dedicated to unleashing the full potential of data to drive innovation and impact in our industry. We are currently seeking a skilled Data Engineer to join our dynamic team and lead our data strategy to new heights. As a Data Engineer at uConnect, you'll be at the forefront, demonstrating the ROI of our products to both existing and prospective customers. You'll spearhead initiatives related to analytics tooling, guide scoping conversations, and contribute significantly to data programming. Your role is pivotal in ensuring seamless data flow between diverse teams and tools while upholding regulatory and ethical compliance in our data practices.

Why We Need You:

Data is at the core of our business strategy, and we require a dedicated owner. If you resonate with any of the following profiles, we want to hear from you:


 * Experienced data engineer or analyst with programming skills
 * A startup enthusiast or someone seeking a cross-functional and self-directed role
 * Senior software or DevOps engineer with a passion for data engineering
 * Advanced data analyst with a knack for programming
   
   
   

Primary Responsibilities:

In this role, you will have a direct impact on our data-driven journey by:


 * Discussing business objectives with stakeholders
 * Collaborating with product managers to collect and prioritize data collection needs
 * Building data pipelines and an event-driven analytics model
 * Creating data visualizations and dashboards
 * Understanding and adhering to trust and safety constraints
 * Owning and iterating on our data systems in collaboration with the engineering team
 * Researching and evaluating potential vendors
 * Validating project outcomes across teams
 * Empowering teams across uConnect to meet their data needs
 * Managing and optimizing student product engagement data
 * Developing a strategic data roadmap aligning long-term plans with day-to-day decisions
   
   
   

Secondary Responsibilities:


 * Conducting code reviews for other engineers
 * Researching tools, techniques, and regulatory issues
 * Writing internal documentation about data availability, data modeling, and graph interpretation
 * Consulting API documentation from vendors to evaluate data flow possibilities
 * Testing new data workflows between first- and third-party tools and demonstrating proof-of-concept work
 * Ensuring data security and compliance by enforcing access controls, encryption, monitoring, and regulatory adherence
   
   
   

A Typical Day:

Your daily routine will be diverse and engaging, involving:


 * Active participation in daily engineering standup meetings
 * Collaborative discussions with stakeholders and peers to prioritize data needs
 * Hands-on programming for data pipelines and code reviews
 * In-depth research on data tools, techniques, and regulatory requirements
 * Documentation of critical insights on data availability, modeling, and visualization
 * Consultation of API documentation to evaluate data flow possibilities
 * Rigorous testing of new data workflows and showcasing proof-of-concept results
 * Crafting compelling data visualizations within dashboards
   
   
   

Required Skills, Background & Experiences:

Required Skills:


 * Proven experience in data engineering, including data pipeline development, ETL processes, and data warehousing
 * Strong self-management and team leadership abilities
 * Proficiency in Python, Javascript, and/or PHP being ideal
 * Mid-to-advanced level SQL skills in any flavor
 * Sound statistical analysis skills for meaningful data interpretation
 * A knack for creating precise data visualizations
 * Strong communication skills
 * Working remotely with a cross-functional team and collaborating with other engineers
   
   
   

Preferred Skills, Background & Experiences:


 * Familiarity with BigQuery or similar data tools
 * Exposure to Google Analytics
 * Strong aptitude for self-directed learning and growth
 * Experience in auditing application-level security with product engineers
 * Proficiency in code validation using test automation
 * Hands-on experience in DevOps/CI pipeline management
 * Familiarity with event-driven frameworks like WordPress
 * Knowledge of data collection policies and regulatory constraints
   
   
   

Why You Should Join Us:

At uConnect, you will have the opportunity to:


 * Contribute to our mission and make a meaningful impact
 * Lead and own data systems from discussions to delivery
 * Grow personally as an individual contributor
 * Shape the future of data collection and utilization in our organization
   
   
   

How to Apply:

If you're ready to be part of a team that's transforming the way we use data and meet the requirements outlined above, we invite you to apply now. Help us shape the future of data at uConnect. Together, we'll achieve remarkable results!

EEO Statement

Equal Opportunity Employer

uConnect is proud to be an equal opportunity employer and is committed to maintaining a diverse and inclusive work environment. All qualified applicants will receive considerations for employment without regard to race, color, religion, sex, age, disability, marital status, familial status, sexual orientation, pregnancy, genetic information, gender identity, gender expression, national origin, ancestry, citizenship status, veteran status, and any other legally protected status under federal, state, or local anti-discrimination laws.

View The EEO is the Law poster and its supplement.

uConnect participates in E-Verify. View the E-Verify posters here.

Disability Accommodation

For individuals with disabilities that need additional assistance at any point in the application and interview process, please contact our Manager of People Operations, Karen Lewis at karen.lewis@gouconnect.com.

Annual salary range: $120K - $140K. Commensurate with experience.","Not Applicable","https://www.linkedin.com/jobs/view/data-engineer-at-uconnect-3726901108?trk=public_jobs_topcard-title","Cambridge, MA","2 months ago","","","2023-09-30","$120.00-$140.00","Transportation, Logistics, Supply Chain and Storage","Data Engineer","Information Technology"
"Over 200 applicants","95739137","Mavinsys","https://www.linkedin.com/company/mavinsys?trk=public_jobs_topcard-org-name","Full-time","Hello,



We are from Mavinsys Talent Acquisition team based on One World Trade center, New York. We are specializing in IT services and staffing majorly in lateral hiring/contract. Below is one of our requirement to fill immediately, if you're interested, please share your candidature to joinus@mavinsys.com





Job Title : Data Engineer



Location : Remote



Duration : 12months

 

Job Description;

1) Previous experience as a data engineer or in a similar role

2) Technical expertise with data models, data mining, and segmentation techniques

3) Knowledge of programming languages (e.g. Java and Python)

4) Hands-on experience with SQL database design

5) Great numerical and analytical skills

6) Degree in Computer Science, IT, or similar field; a Master's is a plus

7) Data engineering certification (e.g IBM Certified Data Engineer) is a plus","Entry level","https://www.linkedin.com/jobs/view/data-engineer-at-mavinsys-3737942766?trk=public_jobs_topcard-title","United States","1 month ago","","","2023-10-12","","IT Services and IT Consulting","Data Engineer","Information Technology"
"29 applicants","90431993","Get It Recruit - Information Technology","https://www.linkedin.com/company/get-it-recruit-information-technology?trk=public_jobs_topcard-org-name","Full-time","Are you ready to embark on an exciting journey as an Associate Data Engineer? Join our dynamic team and be an integral part of our Business Intelligence initiatives, supporting one of the leading global pharmaceutical clients.

Position Overview

As an Associate Data Engineer, you'll play a pivotal role in crafting innovative data engineering solutions that drive business impact across the Americas Region. Your mission includes tackling complex data migration and transformation challenges, enabling the acceleration of end-to-end reporting and analytics, and managing data catalog for the business. Your problem-solving skills will be put to the test as you work in partnership with our IT team to ensure data integrity in our ""data lake.""

Key Responsibilities

Identify and resolve quality issues during complex data migration and transformation.

Support BI analysts with fundamental data engineering capabilities to facilitate complex analytics.

Be accountable for data engineering use cases, some of which may have ""limited time"" life cycles.

Write queries and code real-time solutions for fast deployment within a sandbox environment.

Engage in data engineering to support the acceleration of end-to-end reporting and analytics.

Manage new data sources in compliance with HIPAA and HCC, including access protocols.

Collaborate with the IT team to solve data issues in the data lake and answer urgent business priority questions.

Qualifications

Bachelor's or advanced degree in Computer Science, Data Science, Business Administration, Engineering, or a related field with a focus on Information Technology.

Minimum of two years of progressive experience in data warehousing and decision support/reporting environments.

Minimum of two years of software development experience, preferably with data transformation scripting languages such as INFORMATICA, SQL, TALEND, or Python.

Demonstrable expertise in data lifecycle management, including data ingestion, contextualization, and analytics.

Familiarity with large datasets and understanding of data analysis workflows, experience with data visualization tools like Tableau is a plus.

Strong attention to detail, organizational skills, and the ability to manage multiple tasks.

Effective communication skills, both oral and written, for collaborating with business partners.

Benefits

Competitive salary in the range of $37.00 - $42.00 per hour.

Comprehensive benefits package, including 401(k) with matching, dental and health insurance, paid time off, and vision insurance.

Schedule

Full-time position, Monday to Friday.

Equal Opportunity Employer

We are an equal opportunity employer. We provide equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, national origin, age, sex, citizenship, disability, genetic information, gender, sexual orientation, gender identity, marital status, amnesty, or status as a covered veteran in accordance with applicable federal, state, and local laws.

Join us in this exciting and meaningful role as an Associate Data Engineer and make a difference in the world of data and analytics!

Employment Type: Full-Time","Entry level","https://www.linkedin.com/jobs/view/data-engineer-remote-wfh-at-get-it-recruit-information-technology-3766963082?trk=public_jobs_topcard-title","Jacksonville, FL","1 week ago","","","2023-11-27","","Human Resources Services","Data Engineer - Remote | WFH","Information Technology"
"Over 200 applicants","3650994","Tubi","https://www.linkedin.com/company/tubi-tv?trk=public_jobs_topcard-org-name","Full-time","Join Tubi (www.tubi.tv), Fox Corporation's premium ad-supported video-on-demand (AVOD) streaming service leading the charge in making entertainment accessible to all. With over 200,000 movies and television shows, including a growing library of Tubi Originals, 200+ local and live news and sports channels, and 455 entertainment partners featuring content from every major Hollywood studio, Tubi gives entertainment fans an easy way to discover new content that is available completely free. Tubi's library has something for every member of our diverse audience, and we're committed to building a workforce that reflects that diversity. We're looking for great people who are creative thinkers, self-motivators, and impact-makers looking to help shape the future of streaming.

About the Role:

Tubi is a data-driven company, where new types of data are emerging, and the volume of data is growing at an incredible velocity. If you are a data enthusiast that treats data like a product, wires big data technologies together, and is eager to unlock data to its full potential, this is the role for you. As an analytics engineer, you will work directly with the growth team to understand the ever-growing data needs, then turn them into a scalable/repeatable data processing framework. You will work closely with our engineering team to understand our holistic data platform and data architecture. You will help push the boundaries on how data is leveraged for decision-making, and support the next generation of data initiatives.

Responsibilities:


 * Create and maintain ETL data pipelines to support growth marketing
 * Design, develop, and implement data models and pipelines in a scalable and optimal data architect using SQL, python, and big data technologies.
 * Assemble large and complex datasets from various data sources into formats that can be effectively utilized by growth analysts and growth marketers
 * Automate manual process, and optimize internal/external data delivering pipelines
 * Identify data processing improvement, re-design data infrastructure, and implement the optimizations
 * Develop highly governed growth data with clear definitions, manage it closely for quality and changes, and ensure data accessibility at all times
 * Build analytics tools that utilize data pipelines to assist analytics team in building industry leading data products on reporting and measurement
 * Partner with internal business stakeholders, our engineering teams, as well as external vendors
   
   

Your Background:


 * 7+ years of experience designing, building, and optimizing data pipelines and architectures at large scale and extreme complexity.
 * Advanced SQL knowledge and experience working with relational databases, query authoring, as well as experience with a variety of databases.
 * Experience with DBT, Databricks, or similar workflow management tools
 * Experience using Python for data table creation in databricks
   
   

Nice to have:


 * Hands-on experience on Spark and AWS cloud services like EC2 and Redshift
 * Familiarity with data visualization tools like Periscope, Tableau, or Looker
 * Experience working with Performance Marketing data
   
   

Pursuant to state and local pay disclosure requirements, the pay range for this role, with final offer amount dependent on education, skills, experience, and location is listed annually below. This role is also eligible for an annual discretionary bonus, long-term incentive plan and various benefits, including medical/dental/vision, insurance, a 401(k) plan, paid time off, and other benefits in accordance with applicable plan documents.

California, New York City, Westchester County, NY and Seattle, WA

$147,000—$174,000 USD

Colorado and Washington (excluding Seattle, WA)

$134,000—$157,000 USD

Tubi is a division of Fox Corporation, and the FOX Employee Benefits summarized here, covers the majority of all US employee benefits. The following distinctions below outline the differences between the Tubi and FOX benefits:


 * For US-based non-exempt Tubi employees, the FOX Employee Benefits summary accurately captures the Vacation and Sick Time.
 * For all US-based employees, Tubi offers 12 paid ""Tubi Holidays"" in addition to the 10 FOX Corporate Company paid holidays.
 * For all salaried/exempt employees, in lieu of the FOX Vacation policy, Tubi offers a Flexible Time off Policy to manage all personal matters.
 * For all full-time, regular employees, in lieu of FOX Paid Parental Leave, Tubi offers a generous Parental Leave Program, which allows parents twelve (12) weeks of paid bonding leave within the first year of the birth, adoption, surrogacy, or foster placement of a child. This time is 100% paid through a combination of any applicable state, city, and federal leaves and wage-replacement programs in addition to contributions made by Tubi.
 * For all full-time, regular employees, Tubi offers a monthly wellness reimbursement.
   
   

Tubi is proud to be an equal opportunity employer and considers qualified applicants without regard to race, color, religion, sex, national origin, ancestry, age, genetic information, sexual orientation, gender identity, marital or family status, veteran status, medical condition, or disability. Pursuant to the San Francisco Fair Chance Ordinance, we will consider employment for qualified applicants with arrest and conviction records. We are an E-Verify company.

","Mid-Senior level","https://www.linkedin.com/jobs/view/senior-analytics-engineer-at-tubi-3723345967?trk=public_jobs_topcard-title","San Francisco, CA","3 months ago","","","2023-08-31","","Entertainment Providers","Senior Analytics Engineer","Information Technology"
"126 applicants","33239352","Wagmo","https://www.linkedin.com/company/wagmo?trk=public_jobs_topcard-org-name","Full-time","What We Do

Wagmo is a new type of pet health company. We are a pet parent's partner in pet parenting - from everyday care to rainy-day emergencies. We offer wellness plans that keep our members on budget and insurance plans that keep them ready for the unexpected. Our members should always feel confident they are giving their pets the care they deserve.

Our mission is to bring pet parents the confidence they need to give pets the care they deserve. We do that by connecting them to budgeting tools they can bank on, partners they can depend on and advice they can count on.

What's Important To Us

We solve hard problems all day long, but hang out with dogs while we do it. We value authenticity and efficiency, and have no time for egos. We prioritize performance over pedigree, compensate fairly, and never take ourselves too seriously.

Our values are core to who we are and how we operate. We talk about them all the time. These are not just things posted on a wall. We will interview for them, hold each other accountable to them and make sure we work with every single person we interact with in a way that's consistent with these values.

About role:

Wagmo is growing our product features, backend operations and the ways we continue to service our customers to ensure we make pet wellness and care available to all pets and pet parents. We have a diverse, lean team and a culture where we take work seriously without taking ourselves too seriously. As we grow, we are looking to bring on our first Data Engineer.

This role is hybrid, 2 days a week in our NYC office.

Our Tech Stack: Sigma BI, Python and Go (Golang), Javascript, React, Next.js, Flutter, Google Cloud, Kubernetes

What You'll Do


 * Reporting to the Head of Engineering, research and recommend new BI tool
 * Evaluate current data models and business requirements
 * Collaborate and coordinate with engineering and business stakeholders to identify how we should capture data for new initiatives, as well as influence how we should implement changes that would affect existing data
 * Design, build, and maintain the data pipelines and infrastructure required to support the organization's analytics and BI initiatives, along with normalizing the data
 * Conduct A/B in order to build state-of-the-art products
 * Initiate analysis and generate reports to assist with feature tracking, experiments, and product/platform health monitoring
   
   

What You'll Need to Be Successful


 * Cloud-based DWH experience
 * Solid understanding of Dbt - data build tool - highly preferred
 * Data scripting languages such as sql, jinja2, python desired
 * Understanding microservices architecture desired
 * Taking ownership, and responsibility for integrating user domain data
 * Knowledge of ELT tools such as Airbyte, Airflow, Talend (stitch),
   
   

Key Benefits


 * Pay Range: $130-160k + Equity in the company
 * Company paid medical premiums
 * Dental, vision, voluntary life, short term disability and long term disability
 * Company paid Wagmo pet wellness and insurance plans
 * Unlimited paid time off
 * 12 weeks parental time off
 * ClassPass subsidy
 * 401k
 * Company wide open feedback model
   
   

We here at Wagmo strive to build a workforce composed of individuals with diverse backgrounds, abilities, minds, and identities that will help us to grow, not only as a company, but also as individuals. Wagmo is an Equal Opportunity Employer.","Entry level","https://www.linkedin.com/jobs/view/data-engineer-hybrid-nyc-at-wagmo-3655993783?trk=public_jobs_topcard-title","New York, NY","5 months ago","","","2023-07-06","","Technology, Information and Internet","Data Engineer - Hybrid NYC","Information Technology"
"Be among the first 25 applicants","100470210","HireMeFast LLC - Career Accelerator - Land A Job","https://www.linkedin.com/company/hiremefast-llc-career-accelerator-land-a-job?trk=public_jobs_topcard-org-name","Full-time","This is a remote position.

Job Title: Junior Data Engineer

Employment Type: Full-Time

Salary: $64,000 - $76,000 per annum

Experience Required: Minimum 1 year of project experience

How to Apply: visit hiremefast.net to learn more & apply.

About us: HireMeFast is a leading staffing and recruitment agency specializing in connecting businesses with top-tier talent across various industries. Our mission is to bridge the gap between exceptional candidates and organizations needing their skills, expertise, and unique qualities. Our team of experienced and dedicated recruitment specialists utilizes innovative sourcing strategies, and a vast network to identify and attract top talent. We conduct comprehensive procedures to ensure that only the most qualified candidates are presented to our clients.

Position Summary

Join the fast-paced, innovative, and collaborative environment focused on providing an AIOps platform that enhances the intelligence of the CVS Health infrastructure. Work closely with subject matter experts and colleagues to build and scale out machine learning and AI solutions that will detect, predict, and recommend solutions to correct issues before system impact and enhance efficiency, reliability, and performance of CVS Health’s IT operations.

Key Responsibilities include:


 * Data pipeline development: Designed, implemented, and managed data pipelines for extracting, transforming, and loading data from various sources into data lakes for processing, analytics, and correlation.
 * Data modeling: Create and maintain data models ensuring data quality, scalability, and efficiency
 * Develop and automate processes to clean, transform, and prepare data for analytics, ensuring data accuracy and consistency
 * Data Integration: Integrate data from disparate sources, both structured and unstructured to provide a unified view of key infrastructure platform and application data
 * Utilize big data technologies such as Kafka to process and analyze large volumes of data efficiently
 * Implement data security measures to protect sensitive information and ensure compliance with data and privacy regulation
 * Create/maintain documentation for data processes, data flows, and system configurations
 * Performance Optimization- Monitor and optimize data pipelines and systems for performance, scalability and cost-effectiveness
   
   

Characteristics of this role:


 * Team Player: Willing to teach, share knowledge, and work with others to make the team successful.
 * Communication: Exceptional verbal, written, organizational, presentation, and communication skills.
 * Creativity: Ability to take written and verbal requirements and come up with other innovative ideas.
 * Attention to detail: Systematically and accurately research future solutions and current problems.
 * Strong work ethic: The innate drive to do work extremely well.
 * Passion: A drive to deliver better products and services than expected to customers.
   
   

Required Qualifications


 * 2+ years of programming experience in languages such as Python, Java, SQL
 * 2+ years of experience with ETL tools and database management (relational, non-relational)
 * 2+ years of experience in data modeling techniques and tools to design efficient scalable data structures
 * Skills in data quality assessment, data cleansing, and data validation
   
   

Qualifications


 * Knowledge of big data technologies and cloud platforms
 * Experience with technologies like PySpark, Databricks, and Azure Synapse.
   
   

Education

Bachelor’s degree in Computer Science, Information Technology, or related field, or equivalent working experience

Why HireMeFast LLC?

At HireMeFast, we understand that finding the right individuals to join your team is crucial for success and growth of your organization. We are committed to streamlining the hiring process for our clients, ensuring they have access to a diverse pool of highly qualified candidates who are a perfect fit for their specific needs.

","Mid-Senior level","https://www.linkedin.com/jobs/view/junior-data-engineer-at-hiremefast-llc-career-accelerator-land-a-job-3778482384?trk=public_jobs_topcard-title","Denver, CO","3 hours ago","","","","","Software Development","Junior Data Engineer","Information Technology"
"Over 200 applicants","30770905","Leafwell","https://www.linkedin.com/company/leafwellofficial?trk=public_jobs_topcard-org-name","Full-time","Description




Who is Leafwell




Leafwell is a rapidly growing technology and data company that set out to increase access, education, and research into cannabis and to advance its application as medicine.




An exciting opportunity for a Data Engineer to join our growing team has arisen.




What To Expect As a Data Engineer At Leafwell




As a Data Engineer at Leafwell, you will be in charge of creating and orchestrating the Leafwell data pipeline, which means gathering data, creating & automating data transformations and producing actionable insights for Leafwell’s internal stakeholders. You will support critical testing and rollout of new data features. The Data Engineer will build and maintain systems that inform Leafwell’s business stakeholders about Key Performance Indicators (KPIs) and suggest data-driven strategies to optimize those metrics.




Essential Duties And Responsibilities




The Data Engineer will perform the following responsibilities:




 * Acquire, assemble, transform and analyze data
 * Create, manage and orchestrate the data pipeline and it’s infrastructure
 * Present findings, trends, and suggested optimizations
 * Identify new opportunities and threats to the company's business model
 * Update and revise reports, queries, and analytic procedures as necessary
 * Design and implement tracking so that optimization efforts can be measured
 * Identify inefficiencies in data processes and automate where appropriate
 * Write and update international SOPs and internal documentation
 * Support IT systems management with testing, validation, and user support
 * Proactively identify initiatives for data-related improvements




Why Leafwell




At Leafwell, we are passionate about our work and seek out employees who contribute the same level of dedication and enthusiasm. We are only as good as the people we hire, so we aim to be the best employer in order to attract the top talent in the industry.




Do we have your Interest?




Requirements




Our Ideal Candidate




Our Ideal Candidate Will Possess The Following




 * Bachelor's degree in Computer Science, Mathematics, Economics, Information Systems, or another quantitative field
 * 3-5 years of relevant professional experience in Data Engineering / Analytics
 * Advance working knowledge and experience in SQL and relational databases
 * Strong data management abilities, including data analysis, standardization, cleansing, querying, and consolidation of data
 * Data visualization experience (e.g Tableau, Looker, etc.)
 * Exercises self-reliance, initiative, and leadership while keeping stakeholders properly informed and involved
 * Reliably manage numerous duties during a workday, necessitating interactions with people located across the world
 * Technically competent, with the ability to quickly learn new processes and programs, and utilize various software applications
 * Excellent communication and interpersonal skills; ability to work with and appeal to a wide variety of personalities and professional tendencies




Bonus Points If You Have Experience In The Following




 * Cannabis knowledge; industry experience is a plus
 * Used Data Orchestration tools like Dagster or Airflow
 * Used dbt or comparable data transformation software
 * Git
 * Python or R programming
 * Amazon Web Services
 * PostgreSQL and RedShift
 * CRM products
 * Data Visualization in Metabase
 * Effective project management skills and comfort utilizing a project management platform in collaboration with other team members
 * Data Science projects




Benefits Highlights




Our benefits include, but are not limited to:




 * Remote first - Most of our employees are 100% remote. Positions requiring travel enjoy a hybrid environment.
 * Rapid Growth - Our company is rapidly expanding, and our employees are advancing with us!
 * Training and Development - We foster a culture of learning. We encourage our employees to take part in educational, training, and development opportunities.




Leafwell is dedicated to bringing together people from diverse cultures and perspectives. We work hard to provide a welcoming atmosphere where everyone may flourish, have a sense of belonging, and collaborate effectively. As an equal opportunity employer, we do not tolerate any illegal discrimination against job applicants based on their race, color, religion, veteran status, sex, parental status, gender identity or expression, transgender status, sexual orientation, national origin, age, disability, or genetic information. We are committed to going above and above in creating diversity within our firm and follow the regulations upheld by the EEOC.","Entry level","https://www.linkedin.com/jobs/view/data-engineer-at-leafwell-3667433581?trk=public_jobs_topcard-title","Miami, FL","4 months ago","","","2023-07-20","","Technology, Information and Internet","Data Engineer","Information Technology"
"108 applicants","81606529","Steneral Consulting","https://www.linkedin.com/company/steneral-consulting?trk=public_jobs_topcard-org-name","Full-time","Title : Data Engineer

Location : Remote

Duration : 6+ months to start (long term contract (work order are extended 2x year))

Please Fill out the skill matrix: Technology Some Awareness (1) Novice (2) Intermediate (3) Advanced (4) Expert (5) SQL Tableau Desktop /Creator Tableau API Integration AWS - S3 Glue AWS - Athena Redshift AWS - Lambda AWS - SNS/SQS msExcel

Profile: Data Analyst will work closely with Architects and leads in the Interflow team, key business stakeholders including program staff to support the organizations effort in the areas of data Analysis and information management. Role should have strong experience in writing SQL queries to extract data for reports. Candidate should have experience working with large set of data. As these reports are very critical for the company and we are looking for an expert who is capable of handling, consuming and extracting large sets of data for analysis and reports.

Must Haves


 * AWS - Athena
 * AWS - S3
 * Glue
 * hands-on development of reporting applications
 * MS Excel
 * RedShift
 * SQL
 * Tableau Desktop /Creator
   
   

Nice


 * AWS - Lambda
 * AWS - SNS/SQS
 * Tableau API Integration
   
   

Skills


 * BA/BS required (major in an analytical field desired)
 * A minimum 10+ years work-related experience.
 * Experience in hands-on development of reporting applications for the Web in a professional environment
 * The Ethos of continuous improvement and interest in learning new things.
 * Strong analytical thinking and structured problem-solving ability
 * Ability to handle multiple projects and assignments simultaneously and effectively in a cross-functional team environment.
 * Versed on the agile methodology and best practices.
 * Excellent interpersonal and collaboration skill with the ability to work with a diverse set of colleagues, across functions, from different organizations, disciplines, etc.
 * Self-starter, ability to set priorities, work independently and attain goals","Entry level","https://www.linkedin.com/jobs/view/remote-work-need-data-engineer-at-steneral-consulting-3774712978?trk=public_jobs_topcard-title","United States","4 days ago","","","2023-11-30","","IT Services and IT Consulting","Remote Work - Need Data Engineer","Information Technology"
"Over 200 applicants","165158","Netflix","https://www.linkedin.com/company/netflix?trk=public_jobs_topcard-org-name","Full-time","Netflix is the world's leading streaming entertainment service, with over 230 million paid memberships in over 190 countries, enjoying TV series, documentaries, and feature films across various genres and languages. Members can watch or play as much as they want, anytime, anywhere, on any internet-connected screen.

Machine Learning/Artificial Intelligence powers numerous aspects of the Consumer Experience, including innovation in content discovery and personalization for members, identifying and attracting new members to our product, optimizing our payment processing, and many more.

The Opportunity

The Consumer ML Model Compute & Serving Systems team provides the foundation for model computation, inference, and serving for all ML/AI innovation across the various Consumer ML domains. The team is responsible for developing all the building blocks required to run ML models at scale. These building blocks include a model serving platform, an event-driven model compute framework, a compute orchestration engine, and many more.

We are looking for strong software engineers for this team, which is a central focus of our business. You will play a highly cross-functional role, partnering with other engineers, product management, machine learning, and data teams to take Netflix’s ML/AI initiatives to the next level.

If you have a passion for building scalable, robust systems, are interested in pushing the envelope in applying ML algorithms, and operate in a critical part of the stack that strongly influences what our customers see on their screens, then we want to talk to you.


You May Enjoy Working With Us If



 * You strive to embrace best practices and are curious about discovering new and better ways to solve problems.
 * You are self-driven and highly motivated to deliver top-tier solutions while learning from and collaborating with Stunning Colleagues.
 * You are strongly motivated to pick up new domains and ship high-quality, well-tested code.
 * You are excited to work in a multidisciplinary environment (engineering, algorithms, data engineering/science, product experimentation).
 * You are comfortable working in a majority-remote team with partners distributed across (US) geographies & time zones.
   
   
   
   

We Would Love To Work With You If



 * You have experience building high-traffic distributed systems and ML infrastructure. You have experience supporting large-scale ML models with a direct impact on the customer experience with high availability, throughput, and performance.
 * You are adept at building applications in an object-oriented programming language. We work with Java primarily, so an interest in learning and coding in Java is required.
 * You have strong DevOps experience including performance tuning and optimizations, managing deployments, and capacity planning for large applications.
 * You are experienced working with the public cloud like AWS, Azure, or GCP.
 * You are a proactive, effective communicator and have a strong bias towards action.
 * You have a BS/MS in Computer Science, Applied Math, Engineering, or a related field.
   
   
   

At Netflix, we carefully consider a wide range of compensation factors to determine your personal top of market. We rely on market indicators to determine compensation and consider your specific job, skills, and experience to get it right. These considerations can cause your compensation to vary and will also be dependent on your location.

The overall market range for roles in this area of Netflix is typically $100,000 - $464,000. This market range is based on total compensation (vs. only base salary), which is in line with our compensation philosophy. Netflix is a unique culture and environment. Learn more here.","Not Applicable","https://www.linkedin.com/jobs/view/software-engineer-l4-consumer-ml-model-compute-serving-systems-at-netflix-3745413133?trk=public_jobs_topcard-title","United States","2 days ago","","","2023-12-02","","Movies, Videos, and Sound, Technology, Information and Internet, and Entertainment Providers","Software Engineer (L4), Consumer ML Model Compute & Serving Systems","Engineering and Information Technology"
"144 applicants","1035","Microsoft","https://www.linkedin.com/company/microsoft?trk=public_jobs_topcard-org-name","Full-time","The Microsoft Cloud for Sovereignty team is a globally distributed engineering team aiming to bring the power of the Microsoft Public Cloud to the unique challenges of public sector workloads that must adhere to government compliance & security requirements. Combining the flexibility and scale of public clouds such as Azure with needs for data residency, guardrails, policies, and strict security, our multi-disciplinary team-members contribute directly to Microsoft’s mission to empower every organization on the planet to achieve more.

We are looking for a Senior Data Engineer to join us on the journey of delivering the building blocks for digitally transforming sovereign workloads across the world. Are you excited to learn and explore many aspects of the depth and breadth of Azure, Microsoft 365, Microsoft Fabric, and Power Platform? Are you ready to build customer centric products that meet sovereign requirements with Microsoft’s cloud infrastructure & platforms by considering scale, resiliency, and security needs across many countries and regions in the world? Are you interested in joining a continuously learning, globally collaborating team dedicated to innovation and embracing diverse perspectives? Then come and join us!

Microsoft’s mission is to empower every person and every organization on the planet to achieve more, and we’re dedicated to this mission across every aspect of our company. Our culture is centered on embracing a growth mindset and encouraging teams and leaders to bring their best each day. Join us and help shape the future of the world.

Responsibilities


 * Build systems for collection & transformation of complex data sets for use in production systems
 * Design, develop, and maintain data pipelines for reporting, optimization, data collection
 * Collaborate with engineers on building & maintaining back-end services
 * Implement data schema and data management improvements for scale and performance
 * Provide insights into key performance indicators for the product and customer usage
 * Serve as team's authority on data infrastructure, privacy controls and data security
 * Collaborate with appropriate stakeholders to understand user requirements
 * Support efforts for continuous improvement, metrics and test automation
 * Maintain operations of live service as issues arise on a rotational, on-call basis
 * Verify data architecture meets security and compliance requirements and expectations
   
   

Qualifications

Required/Minimum Qualifications:


 * Bachelor's Degree in Computer Science, Math, Software Engineering, Computer Engineering, or related field AND 4+ years experience in business analytics, software development, data modeling or data engineering work
    * Master's Degree in Computer Science, Math, Software Engineering, Computer Engineering, or related field AND 3+ years experience in business analytics, software development, data modeling or data engineering work
    * OR equivalent experience.

 * 3+ years of experience building, managing and scaling database or data store environments
 * 3+ years of experience building data pipelines for systems integration or data analytics
   

Other Requirements


 * Ability to meet Microsoft, customer and/or government security screening requirements are required for this role. These requirements include, but are not limited to the following specialized security screenings: This position will be required to pass the Microsoft Cloud background check upon hire/transfer and every two years thereafter.
   
   

Preferred Qualifications


 * Experience working with large data sets and big data tools (i.e. Spark, Scala, Hadoop, etc.)
 * Experience using Kusto (KQL) query language
 * Experience with Azure Synapse or similar tools
 * Experience monitoring, testing and supporting online systems
 * Experience with technical design, problem solving and debugging
 * Interest in improving engineering systems & practices and creation of high quality software
   
   

Data Engineering IC4 - The typical base pay range for this role across the U.S. is USD $112,000 - $218,400 per year. There is a different range applicable to specific work locations, within the San Francisco Bay area and New York City metropolitan area, and the base pay range for this role in those locations is USD $145,800 - $238,600 per year.

Certain roles may be eligible for benefits and other compensation. Find additional benefits and pay information here: https://careers.microsoft.com/us/en/us-corporate-pay

Microsoft is an equal opportunity employer. Consistent with applicable law, all qualified applicants will receive consideration for employment without regard to age, ancestry, citizenship, color, family or medical care leave, gender identity or expression, genetic information, immigration status, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran or military status, race, ethnicity, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable local laws, regulations and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application process, read more about requesting accommodations.","Not Applicable","https://www.linkedin.com/jobs/view/senior-data-engineer-at-microsoft-3763840562?trk=public_jobs_topcard-title","Atlanta, GA","1 week ago","","","2023-11-22","","Software Development","Senior Data Engineer","Information Technology"
"Over 200 applicants","2539","iHeartMedia","https://www.linkedin.com/company/iheartmedia?trk=public_jobs_topcard-org-name","Full-time","iHeartMedia

Current employees and contingent workers click here to apply and search by the Job Posting Title.

The audio revolution is here – and iHeart is leading it! iHeartMedia, the number one audio company in America, reaches 90% of Americans every month -- a monthly audience that’s twice the size of any other audio company – almost three times the size of the largest TV network – and almost 4 times the size of the largest ad-supported music streaming service. In fact, we have:


 * More #1 rated markets than the next two largest radio companies combined;
 * We’re the largest podcast publisher, with more monthly downloads than the second- and third-largest podcast publishers combined. Podcasting, the fastest-growing new media, today has more monthly users than streaming music services or Netflix;
 * iHeart is the home of many of the country’s most popular and trusted on-air personalities and podcast influencers, who build important connections with hundreds of communities across America;
 * We create and produce some of the most popular and well-known branded live music events in America, including the iHeartRadio Music Festival, the iHeartRadio Music Awards, the iHeartCountry Festival, iHeartRadio Fiesta Latina and the iHeartRadio Jingle Ball Tour;
 * iHeartRadio is the #1 streaming radio digital service in America;
 * Our social media footprint is 7 times larger than the next largest audio service; and
 * We have the only complete audio ad technology stack in the industry for all forms of audio, from on demand to broadcast radio, digital streaming radio and podcasting, which bring data, targeting and attribution to all forms of audio at an unparalleled scale. As a result, we’re able to combine our strong leadership position in audience reach, usage and ad tech with powerful tools and insights for our sales organizations to help them build success for their clients at a more efficient cost than any other option.
   
   
   

Because we reach almost every community in America, we’re committed to providing a range of programming that reflects the diversity of the many communities we serve – and our company reflects that same kind of diversity. Our company values stress collaboration, curiosity, welcoming dissent, accepting mistakes in the pursuit of new ideas, and respect for everyone.

Only one company in America has the #1 position in everything audio: iHeartMedia!

If you’re excited about this role but don’t feel your experience aligns perfectly with the job description, we encourage you to apply anyway. At iHeartMedia we are dedicated to building a diverse, inclusive, and authentic workplace and are looking for teammates passionate about what we do!

What We Need:

We are looking for talented, energetic, curious problem-solvers who will be a great fit into our fast moving culture. In turn, we will help you acquire specific skills where needed.

What You'll Do:

The Data Engineering Intern will take part in a number of exciting ongoing projects including platform engineering and data engineering projects, which include:


 * Develop applications using modern Big Data and Cloud technologies leveraging industry leaders such as Google and Amazon Cloud
 * Develop / automate high performing, scalable and innovative Data Pipelines in a fast paced and agile environment leveraging
 * Apply principles of SDLC and methodologies like Scrum, Kanban, CI/CD, Software and Product Security, Scalability, Coding standards, Documentation Practices, Refactoring and Testing Techniques
   
   
   

What You'll Need:


 * Pursuing a Bachelor’s / Master’s degree in Computer Science, MIS, or a related technical field. 3.0 GPA (strongly preferred)
 * Basic Python or Javascript skills and SQL knowledge
 * A basic understanding of relational database systems
 * An understanding of Cloud technologies, distributed systems, modern operating systems and networking or have basic skills and eager and willing to learn
 * Must be able to work in an agile development environment and comfortable managing shifting priorities
 * A great attitude and hunger for learning and an ability to work within a team environment
 * Excellent analytical, written and oral communication skills
   
   
   

What You'll Bring:

Compensation:

Salary to be determined by multiple factors including but not limited to relevant experience, knowledge, skills, other job-related qualifications, and alignment with market data.

$24.00 - $30.00

Location:

San Antonio, TX: 20880 Stone Oak Parkway, 78258

Position Type:

Seasonal

Time Type:

Full time

Pay Type:

Hourly

Benefits:

iHeartMedia’s benefits offering is flexible and offers a variety of choices to meet the diverse needs of our changing workforce, including the following:


 * Employer sponsored medical, dental and vision with a variety of coverage options (employees meeting ACA measurement)
 * A 401K plan
 * Employee Assistance Program (EAP) at no cost – services include telephonic counseling sessions, consultation on legal and financial matters, emotional well-being, family and caregiving
 * A range of additional voluntary programs, such as spending accounts, student loan refinancing, accident insurance and more!
   
   
   

The Company is an equal opportunity employer and will not tolerate discrimination in employment on the basis of race, color, age, sex, sexual orientation, gender identity or expression, religion, disability, ethnicity, national origin, marital status, protected veteran status, genetic information, or any other legally protected classification or status.

Non-Compete will be required for certain positions and as allowed by law.

Our organization participates in E-Verify. Click here to learn about E-Verify.","Internship","https://www.linkedin.com/jobs/view/data-engineer-internship-at-iheartmedia-3767518163?trk=public_jobs_topcard-title","Texas, United States","6 days ago","","","2023-11-27","","Broadcast Media Production and Distribution","Data Engineer Internship","Information Technology"
"Over 200 applicants","10630755","Varo Bank","https://www.linkedin.com/company/varobank?trk=public_jobs_topcard-org-name","Full-time","Varo is an entirely new kind of bank. All digital, mission-driven, FDIC insured and designed for the way our customers live their lives. A bank for all of us.

About The Sr. Engineer, Data Engineering Role

We’re looking to hire a Senior Engineer to contribute to the development of Varo’s data platform. The Data Engineering team collaborates closely with product, data science and machine learning teams to design, build, and scale high leverage datasets that enable analytics, and experimentation. As a Senior Data Engineer you will contribute to the vision for data architecture and data quality across multiple company verticals. If you are interested in working with an impressive team of Data pros who collaborate and challenge each other, and want to solve interesting problems to propel the company’s growth, apply now.

What You'll Be Doing


 * Design, build and maintain Varo’s data pipelines to process data into and out of Varo’s Data Lake
 * Work with AWS big data technologies including EMR, Glue, S3, EKS, Lambda, Athena, RDS
 * Develop and maintain the data strategy for Varo in terms of capabilities and control mechanisms that support company responsibilities as a regulated national bank
 * Provide technical leadership in the area of data systems development including data ingestion, data curation, data storage, high-throughput data processing and analytics
 * Work with business partners on requirements, clarification and results
 * Participate in developing and enforcing data security & access control policies
 * Develop effective controls for a resilient data ingestion process
 * Support application data integration design and build efforts, including real-time capabilities
 * Conduct code reviews in accordance with team processes and standards
   
   

You'll Bring The Following Required Skills And Experiences


 * Bachelor's degree in Computer Science, MIS, Engineering or related field, or relevant work experience
 * 6-8 years of data modeling, data pipelines, data lake, data warehouse experience
 * 6+ years programming experience in Python (will also consider Java, Kotlin, C, C++)
 * 6+ years’ experience working within the AWS Big Data/Hadoop Ecosystem (EMR, Glue and Athena)
 * Experience with other AWS components like Cloudwatch, EKS, KMS, Lambdas, S3 is a strong plus
 * Experience with downstream consumption patterns (reports, dashboards) is a plus
 * Experience in Hadoop, HDFS, Hive, Presto, REST/SOAP API, Spark2, Airflow is a plus
   
   

We recognize not everyone will have all of these requirements. If you meet most of the criteria above and you’re excited about the opportunity and willing to learn, we’d love to hear from you!

About Varo

Varo launched in 2017 with the vision to bring the best of fintech into the regulated banking system. We’re a new kind of bank – all-digital, mission-driven, FDIC-insured, and designed around the modern American consumer.

As the first consumer fintech to be granted a national bank charter in 2020, we make financial inclusion and opportunity for all a reality by empowering everyone with the products, insights, and support they need to get ahead. Through our core product offerings and suite of customer-first features, we aim to address a broad range of consumer needs while profitably serving underserved communities that have been historically excluded from the traditional financial system.

We are growing quickly in our hub locations of San Francisco, Salt Lake City, and Charlotte along with colleagues located across the country. We have been recognized among Fast Company’s Most Innovative Companies, Forbes’ Fintech 50, and earned the No. 7 spot on Inc. 5000’s list of fastest-growing companies across the country.

Varo. A bank for all of us.

Our Core Values


 * Customers First
 * Take Ownership
 * Respect
 * Stay Curious
 * Make it Better
   
   

Learn More About Varo By Following Us

Facebook - https://www.facebook.com/varomoney

Instagram - www.instagram.com/varobank

LinkedIn - https://www.linkedin.com/company/varobank

Twitter - https://twitter.com/varobank

Engineering Blog - https://medium.com/engineering-varo

SoundCloud - https://soundcloud.com/varobank

Varo is an equal opportunity employer. Varo embraces diversity and we are committed to building teams that represent a variety of backgrounds, perspectives, and skills. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.

Beware of fraudulent job postings!

Varo will never ask for payment to process documents, refer you to a third party to process applications or visas, or ask you to pay costs. Never send money to anyone suggesting they can provide work with Varo. If you suspect you have received a phony offer, please e-mail careers@varomoney.com with the pertinent information and contact information.

CCPA Notice At Collection For California Employees And Applicants

https://varomoney.box.com/s/q7eockvma9nd2b0utwryruh4ze6gf8eg","Mid-Senior level","https://www.linkedin.com/jobs/view/sr-data-engineer-at-varo-bank-3727770459?trk=public_jobs_topcard-title","United States","2 weeks ago","","","2023-11-16","","Banking","Sr. Data Engineer","Information Technology"
"Over 200 applicants","165158","Netflix","https://www.linkedin.com/company/netflix?trk=public_jobs_topcard-org-name","Full-time","At Netflix, we want to entertain the world and are constantly innovating on how entertainment is imagined, created, and delivered to a global audience. We currently stream content in more than 30 languages in 190 countries, topping over 240 million paid subscribers, and are expanding into new forms of entertainment such as gaming. Engineering teams within Netflix work hard every day to scale and innovate this content production and member experience in an ever-growing complex software landscape.

We are looking for software engineers and distributed systems engineers to work on many of our core backend systems that directly impact our API platform, messaging, personalization, sign-up, partnership, payments, and billing systems.

As an engineer in this function, you are passionate about solving challenging problems with direct business impact across a wide variety of complex distributed systems. You will be expected to work cross-functionally and drive large projects throughout their lifecycle from identifying the problem to launching in production.

Does this sound like something you’d like to do? Awesome, let’s talk specifics.

What we need from you



 * 6-8+ years of professional experience designing, building, and shipping software.
 * You are skilled in object-oriented programming and write excellent code. You own what you build, beyond just your code. You have a passion for quality.
 * You are proficient in systems design. You can craft systems and solutions to realize a range of product and engineering goals.
 * You have experience working with large-scale, low-latency distributed systems.
 * You love driving real business impact by applying your software engineering, analytical, and collaborative skills across engineering, product, design, ML/data science, and consumer insights.
 * You can work through ambiguity, multi-task effectively, and are highly motivated to lead cross-functional projects and deliver high-quality solutions.
 * You strive to improve yourself and help your colleagues grow. Your co-workers think of you as someone who is kind and does excellent work.
 * You communicate effectively in both written and verbal form. You are a great listener and give all voices an opportunity to express themselves.
 * You are self-driven and impact-oriented. You are proactive and can move fast or pivot to meet the changing needs of the product.
   
   
   

We seek to grow inclusive and diverse teams that will enhance our perspectives, skill sets, and behaviors. We highly encourage you to apply if your background will complement us, even if your experience doesn't precisely match the job description. Your skills and passion will stand out—and set you apart—especially if your career has taken some extraordinary twists and turns.

At Netflix, we carefully consider a wide range of compensation factors to determine your personaltop of market. We rely on market indicators to determine compensation and consider your specific job, skills, and experience to get it right. These considerations can cause your compensation to vary and will also be dependent on your location.

The overall market range for roles in this area of Netflix is typically $388,000 - $600,000.

This market range is based on total compensation (vs. only base salary), which is in line with our compensation philosophy. Netflix is a unique culture and environment. Learn more here.","Not Applicable","https://www.linkedin.com/jobs/view/software-engineer-l5-consumer-engineering-at-netflix-3734639613?trk=public_jobs_topcard-title","United States","2 weeks ago","","","2023-11-17","","Movies, Videos, and Sound, Technology, Information and Internet, and Entertainment Providers","Software Engineer (L5) - Consumer Engineering","Engineering and Information Technology"
"Over 200 applicants","22688","Atlassian","https://au.linkedin.com/company/atlassian?trk=public_jobs_topcard-org-name","Full-time","Overview

Atlassian is looking for a Senior Data Engineer to join our Data Engineering Team. You will build top-notch data solutions and applications that inspire important decisions across the organization. You will be reporting to the Senior Data Engineering Manager.

You'll have flexibility in where you work – whether in an office, from home (remote), or a combination of the two.

Responsibilities

A typical day may involve collaborating with partners, you will design data models, acquisition processes, and applications to address needs. With experience in large-scale data processing systems (batch and streaming), you will lead business growth and enhance product experiences. And will collaborate with Technology Teams, Global Analytical Teams, and Data Scientists across programs.

You'll take ownership of problems from end-to-end: extracting/cleaning data, and understanding generating systems. Improving the quality of data by adding sources, coding rules, and producing metrics is crucial as requirements evolve. Agility and smart risk-taking are important qualities in this industry where digital innovation meets partner/customer needs over time.

Qualifications

On your first day, we'll expect you to have:


 * BS in Computer Science or equivalent experience with 5+ years as Data Engineer or similar role
 * Programming skills in Python & Java (good to have)
 * Design data models for storage and retrieval to meet product and requirements
 * Build scalable data pipelines using Spark, Airflow, AWS data services (Redshift, Athena, EMR), Apache projects (Spark, Flink, Hive, and Kafka)
 * Familiar with modern software development practices (Agile, TDD, CICD) applied to data engineering
 * Enhance data quality through internal tools/frameworks detecting DQ issues. Working knowledge of relational databases and SQL query authoring
   
   
   

We’d Be Super Excited If You Have


 * Followed a Kappa architecture with any of your previous deployments and domain knowledge of Financial and People System
   
   
   

Compensation

At Atlassian, we strive to design equitable, explainable, and competitive compensation programs. To support this goal, the baseline of our range is higher than that of the typical market range, but in turn we expect to hire most candidates near this baseline. Base pay within the range is ultimately determined by a candidate's skills, expertise, or experience. In the United States, we have three geographic pay zones. For this role, our current base pay ranges for new hires in each zone are:

Zone A: $163,300 - $217,700

Zone B: $147,000 - $196,000

Zone C: $135,600 - $180,700

This role may also be eligible for benefits, bonuses, commissions, and equity.

Please visit go.atlassian.com/payzones for more information on which locations are included in each of our geographic pay zones. However, please confirm the zone for your specific location with your recruiter.

Our Perks & Benefits

Atlassian offers a variety of perks and benefits to support you, your family and to help you engage with your local community. Our offerings include health coverage, paid volunteer days, wellness resources, and so much more. Visit go.atlassian.com/perksandbenefits to learn more.

About Atlassian

At Atlassian, we're motivated by a common goal: to unleash the potential of every team. Our software products help teams all over the planet and our solutions are designed for all types of work. Team collaboration through our tools makes what may be impossible alone, possible together.

We believe that the unique contributions of all Atlassians create our success. To ensure that our products and culture continue to incorporate everyone's perspectives and experience, we never discriminate based on race, religion, national origin, gender identity or expression, sexual orientation, age, or marital, veteran, or disability status. All your information will be kept confidential according to EEO guidelines.

To provide you the best experience, we can support with accommodations or adjustments at any stage of the recruitment process. Simply inform our Recruitment team during your conversation with them.

To learn more about our culture and hiring process, visit go.atlassian.com/crh .","Mid-Senior level","https://www.linkedin.com/jobs/view/senior-data-engineer-at-atlassian-3736870803?trk=public_jobs_topcard-title","United States","6 days ago","","","2023-11-28","","Software Development","Senior Data Engineer","Information Technology"
"Over 200 applicants","1035","Microsoft","https://www.linkedin.com/company/microsoft?trk=public_jobs_topcard-org-name","Full-time","Data Privacy has never been more top of mind than it is today. Governments around the world are passing new privacy legislation at a fast pace, and companies are investing in many aspects of data compliance to meet these new regulations. There is a significant opportunity here to empower every company to deliver data privacy solutions that meet these regulations and that build customer trust.

If you are looking for an opportunity to move the needle on data privacy, then the Microsoft Security Platform, Data Governance and Privacy (SPDGP) team is the right place for you. Within SPDGP, the Data Privacy Platform (DPP) team has been providing internal data privacy products for all of Microsoft since 2018, operating these products at high scale and availability. The DPP team is now leveraging this experience to create new data privacy products that will soon benefit all our customers, Microsoft included. There is a steady stream of new requirements for both our internal and external products that is creating exciting opportunities to work at high scale, impact, and visibility.

We are looking for a Senior Data Engineer to help us build next generation data privacy platform.

Microsoft is becoming the largest cybersecurity company in the world and what better time to be a part of this movement, than by joining the Microsoft Security.

Our culture is centered on embracing a growth mindset, a theme of inspiring excellence, and encouraging teams and leaders to bring their best each day. In doing so, we create life-changing innovations that impact billions of lives around the world.

Responsibilities


 * Work on the latest technology in compute and big data to build a massively scalable unified cloud service framework, get to work on heterogeneous transport protocols, apply innovative algorithms for solving placement and load balancing problems at scale.
 * Think strategically and deliver on those challenges, and along the way, change the world.
 * Collaborate with cross-functional teams to ensure seamless integration and functionality of the data privacy platform across Microsoft.
 * Address and resolve complex technical challenges to ensure the system's robustness, scalability, and efficiency.
 * Stay updated with the latest industry trends and technologies to ensure the storage system remains at the forefront of industry standards.
 * Mentor other developers and provide technical guidance to ensure best practices and quality standards are maintained throughout the development process.
   
   

Qualifications

Required/Minimum Qualifications:


 * Bachelor's Degree in Computer Science, Math, Software Engineering, Computer Engineering, or related field AND 4+ years experience in business analytics, data science, software development, data modeling or data engineering work
    * Master's Degree in Computer Science, Math, Software Engineering, Computer Engineering, or related field AND 3+ years experience in business analytics, data science, software development, data modeling or data engineering work
    * OR equivalent experience.

 * 4+ years experience with PySpark programming writing SPARK big data processing pipelines.
 * 4+ years of experience with Open Source Big Data Stack such as Hadoop, HIVE, SPARK, HBASE etc.
   

Other Requirements


 * Ability to meet Microsoft, customer and/or government security screening requirements are required for this role. These requirements include, but are not limited to the following specialized security screenings: This position will be required to pass the Microsoft Cloud background check upon hire/transfer and every two years thereafter.
   
   

Preferred/ Additional Qualifications


 * Bachelor's Degree in Computer Science or related technical field AND 8+ years technical engineering experience with coding in languages including, but not limited to, C, C++, C#, Java, JavaScript, or Python
    * OR Master's Degree in Computer Science or related technical field AND 6+ years technical engineering experience with coding in languages including, but not limited to, C, C++, C#, Java, JavaScript, or Python
    * OR equivalent experience.

 * Experience using agile methodologies or test-driven development (TDD).
 * Have a desire to work collaboratively, solve problems with groups, find win/win solutions and celebrate successes.
 * Solve problems by always leading with passion and empathy for customers.
 * Understanding of data structures, algorithms, and distributed systems
   

Data Engineering IC4 - The typical base pay range for this role across the U.S. is USD $112,000 - $218,400 per year. There is a different range applicable to specific work locations, within the San Francisco Bay area and New York City metropolitan area, and the base pay range for this role in those locations is USD $145,800 - $238,600 per year.

Certain roles may be eligible for benefits and other compensation. Find additional benefits and pay information here: https://careers.microsoft.com/us/en/us-corporate-pay

#ENGGJOBS #MSEC #MSFTSECURITY #SPDGP

Microsoft is an equal opportunity employer. Consistent with applicable law, all qualified applicants will receive consideration for employment without regard to age, ancestry, citizenship, color, family or medical care leave, gender identity or expression, genetic information, immigration status, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran or military status, race, ethnicity, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable local laws, regulations and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application process, read more about requesting accommodations.","Not Applicable","https://www.linkedin.com/jobs/view/senior-data-engineer-at-microsoft-3758160764?trk=public_jobs_topcard-title","Redmond, WA","2 weeks ago","","","2023-11-14","","Software Development","Senior Data Engineer","Information Technology"
"189 applicants","28873198","Pango Group","https://www.linkedin.com/company/pango-co?trk=public_jobs_topcard-org-name","Full-time","Pango Group, an Aura Company, helps customers monitor, manage, and protect against the risks associated with their identities and personal information in a digital world. Backed by WndrCo, Warburg Pincus and General Catalyst, Pango Group is dedicated to creating the world’s most comprehensive portfolio of industry-leading cybersecurity solutions. Our vision is to become THE go-to resource for every cyber protection need individuals may face - today and in the future.

Join us for the ride!

About The Role

At Pango, data is at the core of most of our decision making and our Data Platform is one of the pillars supporting the company’s growth. The data that pass through the Platform are used by both internal and external customers. We are looking for a Data Engineer to help us continue improving the architecture, enhancing, and maintaining the Data Platform.

Day to Day


 * Design, build, deploy, and support scalable pipelines (scaling in both volume and variety of data sources) confirming data privacy and security requirements are met.
 * Administer and maintain the infrastructure powering our analytics and data assets by continuously monitoring and improving system performance and cost.
 * Provide technical mentorship to other team members ensuring knowledge is disseminated and high quality assets are delivered.
 * Drive data quality across the organization; develop best practices and frameworks for driving high quality work.
 * Define and manage SLAs for data sets and processes running in production
 * Evaluate, recommend, and build prototypes of new data engineering technology
 * Participate in technical architecture discussions and peer code reviews
 * Manage data migrations/conversions and troubleshoot data processing issues, assisting other team members when necessary.
 * Other duties and responsibilities as needed.
   
   

What You Bring To The Table


 * 5+ years of experience in the data space with several years in Data Engineering working with a modern, cloud-hosted tech stack; AWS experience critical
 * Proficiency in Spark and Distributed Computing, Databricks a plus
 * Track record of architecting, designing and deploying high performance systems with reliable monitoring and logging practicesStrong Knowledge of Python and SQL, Scala a plus
 * Experience with job orchestration technologies like Airflow
 * Strong communication skills and the ability to work asynchronously across multiple time zones; open to shifted work hours to accommodate team meetings
 * Knowledge of CI/CD, version control and the command line
 * Experience working with third party APIs for ingestion and integration
 * Experience with Stream-processing systems (Kafka, or equivalent)
   
   

It Would Be Great If You Also Had


 * Knowledge of container services (Docker/Kubernetes)
 * Infrastructure as Code tools like Terraform
   
   

Remote

As Part Of Pango Group, You Will

Solve real customer problems. Pango Group’s point solutions allow consumers to address their immediate cyber protection needs. Our mandate is to continuously anticipate our customers’ evolving digital security needs to create best-in-class solutions aimed at keeping them safe.

See your impact. We are a scrappy, nimble organization where individual contributions are needed and valued. You will see your impact every day.

Accelerate your career. As we expand, you will have the opportunity to learn new technologies, products, and markets in a fast-paced, growth-oriented environment.

Most importantly, you’ll get to work with other talented people at a company where people matter. If you want to put your fingerprint on an organization and leapfrog your growth, this is the place for you.

In keeping with our beliefs and goals, no employee or applicant will face discrimination or harassment based on race, color, ancestry, national origin, religion, age, gender, marital domestic partner status, sexual orientation, gender identity, disability status, or veteran status. Above and beyond discrimination or harassment based on “protected categories,” Pango Group is committed to being an inclusive community where all feel welcome. Whether blatant or hidden, barriers to success have no place at Pango Group.

Important privacy information for California job applicants can be found here.","Entry level","https://www.linkedin.com/jobs/view/data-engineer-at-pango-group-3775824112?trk=public_jobs_topcard-title","United States","4 days ago","","","2023-11-30","","IT Services and IT Consulting","Data Engineer","Information Technology"
"Over 200 applicants","35653234","Tailscale","https://ca.linkedin.com/company/tailscale?trk=public_jobs_topcard-org-name","Full-time","About Tailscale

Tailscale builds software that makes it easy to securely interconnect people and their devices, no matter where they are. From hobbyists to multinational corporations, teams of every size use Tailscale each day to protect their networks, share access to internal tools, and more. We're building a future for the Internet that's easy, small and safe, like it used to be. Founded in 2019 and fully distributed, we're backed by Accel, CRV, Insight, Heavybit, and Uncork Capital.

Description

We’re seeking a talented and motivated full-time Data Engineer to join our team. We’re looking for an individual who can own the Data and Analytics infrastructure to power the rest of the company. The ability to think on one’s feet, enjoy collaborating with highly technical teams, and be comfortable working asynchronously is essential.

Responsibilities:


 * Operate and optimize our current stack of data tools: Snowflake, dbt, Rudderstack, HIghtouch, Fivetran, and Looker
 * Guide evolution of the product and the service by normalizing data about the operation of the service to allow analysis of how people do and do not make use of the service.
 * Assist in the study of user acquisition and activation by developing data strategies to provide these insights.
 * Add metrics to the underlying systems and code to get better visibility into user behavior.
 * Collaborate with Sales and Marketing to ensure their tools (Salesforce and Hubspot) are enriched with accurate and timely data to drive effective actions
   
   

Requirements:


 * A great deal of experience as a Data Engineer working with analysts, which should include significant data warehousing, data integration, Extract/Transform/Load, or analytics pipelines.
 * Knowledge of SQL including schema design, query optimization, and migration of data to new schemas over time.
 * Knowledge of widely used tools like Snowflake and dbt.
 * Comfort owning this part of the service. We’ve got a good start but need someone who can take ownership and drive it forward.
 * Ability to give and process constructive feedback, as well as work independently.
 * Ability to communicate openly, supporting and championing innovation and inclusivity.
 * Flexibility to adjust to the dynamic nature of a startup.
 * Understanding of how to translate from low-level data into a form that can drive actionable business insights
   
   

For this position, our target salary range is $145,000 - $265,000 USD (paid in your local currency). As a company, we strive to maintain fair and equitable compensation practices within our team across all roles and all levels.

What We Offer


 * An inclusive, flexible environment where you can be your authentic self. We recognize the impact diverse voices and backgrounds have on the growth of our people, our product, and our company. And that flexibility in how and when you work empowers our team to balance work and life.
 * A competitive total compensation package. This includes base salary and an equity incentive plan.
 * Comprehensive group benefits with no waiting period. Take advantage of coverage for health, vision, dental, and more for you and your family!
 * Remote-first with the opportunity to work from anywhere—enjoy a change of scenery wherever you can get wifi and join our company retreats with fellow Tailscalars!
 * Support for your personal and professional development. Grow your career thoughtfully with $1500 USD annually for professional development, leverage our conference budget to learn from experts in your field, or take advantage of mentorship, coaching, and internal promotion opportunities.
 * Paid time off and a healthy work-life balance. Take advantage of 4 weeks of vacation each year, uncapped sick time when you need it, and support for any situation life throws your way!
 * A build-your-own home office setup. You choose your own company-owned laptop (MAC or PC), receive a monthly home internet reimbursement, and $1000 USD to customize your workstation to make it your own.
 * Generous parental leave program from your first day. We care about your life outside of work and encourage new parents to take advantage of parental leave top-ups for up to 26 weeks.","Entry level","https://www.linkedin.com/jobs/view/data-engineer-at-tailscale-3747269358?trk=public_jobs_topcard-title","United States","4 days ago","","","2023-11-30","","Software Development","Data Engineer","Information Technology"
"Over 200 applicants","3067","Tech Mahindra","https://in.linkedin.com/company/tech-mahindra?trk=public_jobs_topcard-org-name","Full-time","Our client is looking for Strong Data Engineer with 6+ year’s experience.

Required Skills: ADF pipelines, SQL, Kusto, Power BI, Cosmos (Scope Scripts). Power Bi, ADX (Kusto), ADF, ADO, Python/C#.

Good to have – Azure anomaly Alerting, App Insights, Azure Functions, Azure Fabric

","Mid-Senior level","https://www.linkedin.com/jobs/view/data-engineer-at-tech-mahindra-3727254438?trk=public_jobs_topcard-title","United States","1 month ago","Revathi Rajeshwar","https://www.linkedin.com/in/revathi-rajeshwar","2023-11-03","","IT Services and IT Consulting","Data Engineer","Engineering and Information Technology"
"Over 200 applicants","31820","Little Caesars Pizza","https://www.linkedin.com/company/little-caesars?trk=public_jobs_topcard-org-name","Full-time","Build a Bigger, Better, Bolder Future

Imagine working for a company that measures its success based off the growth of its colleagues, a company that invests in its future by investing in you. Little Caesars is a company where our colleagues make an impact.




Your Mission:

In this role, you will be responsible for moderately complex designing and growing our Business Intelligence product in all aspects of our Business Intelligence solution, such as building new BI features, data modeling, machine integration, dashboard creation, Analyze Business Intelligence project requirements and perform the design, coding, testing, and documentation steps required to complete the project. A responsibility of this role will be providing technical assistance to less experienced staff.




How You’ll Make an Impact:

 * Perform moderately complex data analysis and data profiling against source systems and data warehouse.
 * Apply data warehousing best practices to define, design, and develop data transformation rules.
 * Perform intermediate ETL design, development, and support for moving large data volumes from various sources into the various destinations after significant cleansing, transformation, and processing.
 * Handle job stream design, development, and implementation.
 * Experienced partnering with the business to develop reports and dashboards to meet requirements, including: Develop test plans, test scripts, and test conditions based on the business and system requirements, in addition to testing solutions to validate whether requirements have been met, Identify project tasks and providing estimates for those tasks.
 * Provide end-user support in post deployment phase; assess and evaluate all feedback to ensure that the requirements necessary to correct issues are addressed.
 * Analyze business requirements and work with client areas to produce functional specifications for use as reference in creation of the reporting and analytical solutions.
 * Develop and maintain relationships with business customers to identify and develop additional analytic and reporting capabilities.
 * Define and prepare advanced data delivery designs and provide development support to leverage technology investments to support business needs.
 * Coordinate prototype reviews and validating solutions meet requirements and service-level agreements.
 * Continue to apply concepts and develop technical knowledge of features and functions of business intelligence tools.
 * Determine the level of effort for development work given a set of requirements and determine estimate to complete through implementation.
 * Understand and identify the correct resources to help define and validate both requirements and specifications.
 * Resolve system issues and respond to suggestions for improvements.
 * Communicate and monitor compliance with existing practices.
 * Maintain existing business reporting and analytical solutions by providing production support for troubleshooting and regular processing needs.
 * Train and assist users with use of the business intelligence toolset.
 * Provide guidance in the creation of reporting and analytical solutions.




Who You Are:

 * Bachelor’s in Computer Science, equivalent degree, or related course work in systems or computer programming. Equivalent experience may be considered in lieu of formal degree.
 * Minimum of five (5) years of programming experience with advanced SQL experience leveraging complex SQL statements, PL/SQL, and analytics functions to perform data profiling, data validation, and performance tuning.
 * Minimum of three (3) years’ experience with Microsoft BI Platform (SQL Server, SSIS, SSAS, SSRS, Integration, Reporting, and Analysis tools.
 * Demonstrated experience with other Business Intelligence tools (Cognos, Business Objects, Tableu, Qlikview) is a plus.
 * Evidence of excellent analytical and problem-solving skills. Ability to find creative solutions to solve problems while staying within practice guidelines.
 * Possess In-depth knowledge of data warehousing and business intelligence concepts and methods. Sound understanding of business intelligence best practices, relational data structures, dimensional data modeling, and data warehouse and reporting techniques.
 * Proof of experience contributing to and creating data warehouse project deliverables such as source system analysis, business definitions, data dictionary, source to target mappings, and code.
 * In-depth experience preparing advanced report and dashboard designs.
 * Evidence of Tableau Server administration and dashboard development experience.
 * Evidence of ability to document complex business processes.
 * Demonstrated excellence in communication skills working with business areas and other non-technical groups, translating requests into requirements with the ability to express ideas effectively, verbally, and in writing.
 * Experience working in a client-facing role with the ability to build strong relationships.
 * Must have the ability to work independently and within a team environment.
 * Evidence of ongoing pursuit of technical knowledge and experience.
 * Ability to participate in an on-call rotation performing weekend and after-hours support.
 * Ability to work various schedule to support a 24 X 7 shift which can include nights, weekends and holidays.




Where You’ll Work:

 * In a normal office environment where there is no physical discomfort due to temperature, noise, dust and the like.

","Associate","https://www.linkedin.com/jobs/view/data-engineer-at-little-caesars-pizza-3752877354?trk=public_jobs_topcard-title","United States","3 weeks ago","","","2023-11-08","","Food and Beverage Services","Data Engineer","Information Technology"
"Over 200 applicants","10578787","Trustech","https://www.linkedin.com/company/trustechinc?trk=public_jobs_topcard-org-name","Full-time","Join Our Team as a Junior Data Engineer!

Are you passionate about transforming raw data into valuable insights? We are seeking a Junior Data Engineer to join our dynamic team and contribute to our data-driven initiatives. This is an exciting opportunity to grow your career in a supportive environment while making a significant impact.




Remote

Direct hire




Top Skills Needed:

 * Data Engineering: 1-3 years of hands-on experience in data engineering, with a strong understanding of data processing, data pipelines, and data architecture.
 * Python: Proficiency with Python programming (1-3 years), including libraries commonly used in data manipulation and analysis.
 * Intermediate level SQL: Solid understanding and experience working with SQL for data querying and manipulation.




Bonus Skills (Not Necessary, but Valuable):

While not mandatory, candidates with experience or knowledge in the following areas will be highly regarded:

 * Data Analyst Settings: Previous exposure to working in data analysis environments, understanding data visualization and interpretation.
 * Database Design: Familiarity with database design principles and optimization techniques.
 * ETL (Extract, Transform, Load): Exposure to ETL processes and tools for data integration and transformation.
 * Pattern Recognition: Experience or interest in applying pattern recognition techniques to derive insights from data.




Why Join Us:

 * Learning & Growth: Opportunity to learn and grow in a collaborative and supportive environment with mentorship available.
 * Cutting-edge Technology: Work with the latest tools and technologies in the field of data engineering.
 * Impactful Work: Contribute to projects that directly impact our business and clients.




Preferred Qualifications:

 * Bachelor's degree in Computer Science, Engineering, Mathematics, or a related field (or equivalent practical experience).
 * Strong problem-solving skills and the ability to thrive in a fast-paced environment.
 * Excellent communication skills and the ability to collaborate effectively within a team.




If you're passionate about data engineering, possess these skills, and are eager to grow your career, we'd love to hear from you! Join us in leveraging data to drive innovation and make a difference.","Entry level","https://www.linkedin.com/jobs/view/junior-data-engineer-at-trustech-3770182317?trk=public_jobs_topcard-title","Utah, United States","2 days ago","Emma Whiting","https://www.linkedin.com/in/emma-whiting","2023-12-01","","IT Services and IT Consulting","Junior Data Engineer","Information Technology"
"119 applicants","2579812","ezCater","https://www.linkedin.com/company/ezcater?trk=public_jobs_topcard-org-name","Full-time","ezCater is the most trusted provider of corporate food solutions and is purpose-built for business. ezCater's corporate food platform and flexible, scalable food solutions allow organizations to centralize and track their food spend, and fulfill everything from daily employee meals to client meetings and company all-hands. ezCater backs this up with business-grade, best-in-class, customer service and an unmatched nationwide footprint. We're backed by top investors including Insight, Iconiq, Lightspeed, GIC, SoftBank, and Quadrille.

Are you passionate about data? How about leveraging data to drive meaningful impact across a fast growing two-sided marketplace? Have opinions on how to best enable data scientists across a billion-dollar company to do elastic workforce planning or real-time customer lifetime value prediction? Then we should definitely talk!

The Data Technology team at ezCater is growing! As we look towards 2023 and beyond, data is a key strategic component across the company - from advanced, real-time machine learning to business intelligence and data governance. Data is our differentiator and how we will drive real, meaningful impact to the $60+billion Catering industry.

We are hiring a Senior Data Engineer to join our expanding team in solving complex data and platform challenges to accelerate our growing business. The ideal candidate lives and breathes data while driving systems and architecture best practices. They care about driving business impact through producing solid and efficient infrastructure alongside accurate and performant data. You will have the opportunity to work directly with executive stakeholders as we embark on a massive-scale data modeling effort across the organization, so flexibility and the ability to translate business requests to implementation are key.

What You’ll Do


 * Write and ship a lot of code. Mostly within dbt (SQL, Jinja)
 * Work directly with analysts and stakeholders to refine requirements, nail down logic, and debug and qualify produced data sets to ensure they meet the underlying business needs
 * Design and develop high-performance data pipelines. Adhering to SDLC, including CI/CD, best practices
 * Identify opportunities to optimize or scale existing parts of our stack
 * Utilize tooling and automation to improve developer efficiency
 * Monitor data systems to ensure quality and availability while seeking to drive down costs
 * Contribute to the team processes and community
 * Mentor other Data Engineers
 * Be part of our innovation and transformation story
   
   

What You Have


 * Strong experience with data warehousing, data lakes, ELT process, and enterprise data platforms such as Snowflake (preferred), Redshift & BigQuery
 * Experience with building performant data pipelines across disparate systems
 * Experience with cloud platforms such as AWS (preferred), GCP & Azure
 * Mastery in SQL and experience in Python
 * Ability to work independently and collaboratively
 * An open mind and willingness to be flexible. We have a large and complex business, and believe in driving real value out of every project we do
 * Our stack is Snowflake, dbt, Fivetran, Airflow, AWS, Sagemaker, MLFlow, Kubernetes + Docker, Monte Carlo, Hightouch and Python for custom ETL and data science integrations. Experience with the above is a nice-to-have, but a desire to learn is a must.
 * A sharp mind, a soft heart and a large funny bone
   
   

The national cash compensation range for this role is $141,000 - $175,000 per year

ezCater does not sponsor applicants for work visas or legal permanent residence.

What You’ll Get From Us

You’ll get a terrifically compelling opportunity, in an environment of radical transparency, open access to all the data, and collaborative colleagues at every level of our organization. You’ll also get sane working hours and great flexibility around work/life balance.

Have people in your life – of any age – who always, often, or sometimes need your help? We make room for that. Have a bad thing or a good thing happen to you? We make room for that, too.

Oh, and you’ll get all this: Market salary, stock options that you’ll help make worth a lot, the usual holidays, all-you-can-eat vacation, 401K with ezCater match, health/dental/FSA, long-term disability insurance, remote-hybrid work from our awesome Boston or Denver offices OR your home OR a mixture of both home and office (you choose!), a tremendous amount of responsibility and autonomy, wicked awesome co-workers, cupcakes (and many more goodies) when you’re in one of our offices, and knowing that you helped get this rocket ship to the moon.

ezCater is an equal opportunity employer. We embrace humans of every background, appearance, race, religion, color, national origin, gender, gender identity, sexual orientation, age, marital status, veteran status, and disability status. At the same time, we do not employ jerks, even brilliant ones.

For information on how ezCater collects and uses job applicants' personal information, visit our Job Applicant Privacy Policy.","Mid-Senior level","https://www.linkedin.com/jobs/view/senior-data-engineer-remote-at-ezcater-3773885584?trk=public_jobs_topcard-title","Boston, MA","4 days ago","","","2023-11-29","","Technology, Information and Internet","Senior Data Engineer (Remote)","Information Technology"
"179 applicants","23718","FinThrive","https://www.linkedin.com/company/finthrive?trk=public_jobs_topcard-org-name","Full-time","About FinThrive




FinThrive is advancing the healthcare economy. We rethink revenue management to pave the way for a healthcare system that ensures every transaction and patient experience is addressed holistically. We’re making breakthroughs in technology—developing award-winning revenue management solutions that adapt with healthcare professionals, freeing providers and payers from complexity and inefficiency, so they can focus on doing their best work. Our end-to-end revenue management platform delivers a smarter, smoother revenue experience that increases revenue, reduces costs, expands cash collections, and ensures regulatory compliance across the entire revenue cycle continuum. We’ve delivered over $10 billion in net revenue and cash to more than 3,245 customers worldwide. When healthcare finance becomes effortless, the boundaries of what’s possible in healthcare expand. For more information on our new vision for healthcare revenue management, visit finthrive.com




About Award-winning Culture of Customer-centricity and Reliability

At FinThrive we’re proud of our agile and committed culture, which has led to certification as a ""Great Place to Work"" since 2017. We’re honored to have also been ranked #21 among the Best Workplaces in Healthcare for 2023, and we know that it's our collective dedication that makes FinThrive an exceptional place to work.




 * Find balance with our remote-friendly organization
 * Take time to recharge and pursue your passions
 * Be part of a positive and supportive work environment
 * Grow and excel your career with training and education

Our Perks and Benefits

FinThrive is committed to continually enhancing the employee experience by actively seeking new perks and benefits. For the most up-to-date offerings visit finthrive.com/careers-benefits.



Impact you will make

The Staff Data Engineer is responsible for designing, developing, and supporting highly scalable data warehousing solutions and expected to work with the latest Microsoft BI tools to create solutions that meet internal and external stakeholders’ business needs.



What you will do

 * Participate in analysis, design, development, deployment, and support of Data Warehouse solutions
 * Work closely with Lead and Architect in implementing best practices and managing production and development/test environments
 * Work with stakeholders to understand business needs and develop highly scalable solutions and make recommendations to help solve problems or improve processes
 * Understanding and working with multiple data sources to meet business rules and supports analytical needs
 * Create Databrick notebooks, stored procedures, SSIS packages, and using other methods to import/translate/manipulate data
 * Analyze potential data quality issues to determine the root cause and creating effective solutions
 * Optimize processes involving large data sets to improve performance
 * Participate in on-going evolution, improvement, and automation of data warehouse solutions
 * Mentor other developers and analysts





What you will bring

 * Bachelor’s Degree in Computer Science
 * 6+ years of experience working directly with data warehouses and Business Intelligence
 * 6+ years’ experience designing, developing, testing, and supporting of ETL, and OLAP cubes
 * 4+ years’ experience on Microsoft BI stack (SSIS, SSAS, and SSRS)
 * 1+ years’ experience on Microsoft Azure data warehouse stack and/or Databricks or equivalent cloud
 * Advanced SQL Server programming and knowledge of data warehousing best practices
 * Working experience of OLAP technologies, and dimensional modeling
 * Working experience in Microsoft Azure Data Warehouse technologies such as Azure Data Factory, Polybase, U-SQL, SQL Data warehouse, Azure Analysis Services or equivalent
 * The ability to extrapolate database schema to meet the business needs of the application
 * Excellent problem solving and analytical skills
 * Excellent verbal and written communication skills





What we would like to see

 * 2+ years’ experience on Microsoft Azure data warehouse stack and/or Databricks or equivalent cloud
 * .NET or Java programming skills
 * Healthcare Revenue Cycle Experience in a data science environment
 * Working experience in at least one of the BI tools (i.e., Power BI, Tableau, Spotfire, MicroStrategy)
 * Working experience in Databricks






Statement of EEO

FinThrive values diversity and belonging and is proud to be an Equal Employment Opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status. We're committed to providing reasonable accommodation for qualified applicants with disabilities in our job application and recruitment process.

","Mid-Senior level","https://www.linkedin.com/jobs/view/senior-data-engineer-at-finthrive-3770146987?trk=public_jobs_topcard-title","United States","3 days ago","","","2023-12-01","","Hospitals and Health Care","Senior Data Engineer","Information Technology"
"Over 200 applicants","2688310","Upstart","https://www.linkedin.com/company/upstart-network?trk=public_jobs_topcard-org-name","Full-time","About Upstart

Upstart is a leading AI lending marketplace partnering with banks and credit unions to expand access to affordable credit. By leveraging Upstart's AI marketplace, Upstart-powered banks and credit unions can have higher approval rates and lower loss rates across races, ages, and genders, while simultaneously delivering the exceptional digital-first lending experience their customers demand. More than two-thirds of Upstart loans are approved instantly and are fully automated.

Upstart is a digital-first company, which means that most Upstarters can live and work anywhere in the U.S. We also have offices in San Mateo, California; Columbus, Ohio; and Austin, Texas.

Most Upstarters join us because they connect with our mission of enabling access to effortless credit based on true risk. If you are energized by the impact you can make at Upstart, we’d love to hear from you!

The Team and Role

As a Senior Analytics Engineer at Upstart, you will design and develop robust and scalable data models for analytical stakeholders to empower them to retrieve meaningful insights. Our analytics engineering team is centralized within the company and partners closely with data engineering, data platform, software engineers, ML engineers and cross functional analytics squads to stitch together datasets to create data models that are ready for consumption.

In addition to architecting data models, we implement Analytics Engineering best practices around data governance, data quality, data orchestration and pipeline optimization. We enable analysts to think like software engineers by defining, documenting and ensuring adoption of best practices when making contributions to the analytical code-base.

Position Location - This role is available in the following locations: San Mateo, California; Columbus, Ohio; Austin, Texas; or Remote USA

Time Zone Requirements - This team operates in East and West Coast time zones.

Travel Requirements - This team has regular on-site collaboration sessions. These occur three (3) days per quarter at alternating locations. If you need to travel to make these meetups, Upstart will cover all travel related expenses.

How you’ll make an impact:


 * Understand how data is produced and consumed at a deep level; you will need to be highly collaborative with the teams that produce and consume the data to create an end-to-end solution that maximizes the value of our data.
 * Lead efforts to design data models and build curated data sets that are the foundation of reporting and analytics at Upstart.
 * Collaborate with data engineers and data platform teams to create analytics pipelines.
 * Meet consumers of data where they are by wearing an educator’s hat and training them on how to use BI tools and dashboards.
 * Be a close strategic partner to analytics squads to participate in decision making on the analytics road map for Upstart.
   
   

What we’re looking for:


 * Minimum requirements:
    * 5+ year(s) of experience as a Data Engineer / Analytics Engineer / BI Engineer.
    * Strong understanding of data modeling concepts in both transactional and analytical databases.
    * Proven ability to lead cross-team projects to create data models for analytics/reporting purposes.
    * Excellent communication and collaboration skills, particularly when explaining technical or complex matters to less technical co-workers.
    * Expertise in SQL, Python and ETL optimization techniques.



 * Preferred qualifications:
    * Familiarity with business intelligence visualization tools such as Looker, Tableau, Power BI, etc.
    * Experience creating curated data models using a data mesh architecture.
    * Experience with cloud computing platforms like Amazon Web Services (AWS) and Google Cloud.
    * Experience with building Looker data models (LookML), analytics engineering frameworks like dbt, orchestration tools like Airflow, schema design, and dimensional data modeling.
    * Thorough understanding of data lake/warehouse architectures (BigQuery, Databricks, Redshift).
    * Experience prioritizing goals based on the larger picture, while being comfortable getting into the details as needed.
      

What you'll love:


 * Competitive Compensation (base + bonus & equity)
 * Comprehensive medical, dental, and vision coverage with Health Savings Account contributions from Upstart
 * 401(k) with 100% company match up to $4,500 and immediate vesting and after-tax savings
 * Employee Stock Purchase Plan (ESPP)
 * Life and disability insurance
 * Generous holiday, vacation, sick and safety leave
 * Supportive parental, family care, and military leave programs
 * Annual wellness, technology & ergonomic reimbursement programs
 * Social activities including team events and onsites, all-company updates, employee resource groups (ERGs), and other interest groups such as book clubs, fitness, investing, and volunteering
 * Catered lunches + snacks & drinks when working in offices
   
   

At Upstart, your base pay is one part of your total compensation package. The anticipated base salary for this position is expected to be within the below range. Your actual base pay will depend on your geographic location–with our “digital first” philosophy, Upstart uses compensation regions that vary depending on location. Individual pay is also determined by job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process.

In addition, Upstart provides employees with target bonuses, equity compensation, and generous benefits packages (including medical, dental, vision, and 401k).

United States | Remote - Anticipated Base Salary Range

$155,800—$215,500 USD

Upstart is a proud Equal Opportunity Employer. We are dedicated to ensuring that underrepresented classes receive better access to affordable credit, and are just as committed to embracing diversity and inclusion in our hiring practices. We celebrate all cultures, backgrounds, perspectives, and experiences, and know that we can only become better together.

If you require reasonable accommodation in completing an application, interviewing, completing any pre-employment testing, or otherwise participating in the employee selection process, please email candidate_accommodations@upstart.com

https://www.upstart.com/candidate_privacy_policy

","Mid-Senior level","https://www.linkedin.com/jobs/view/senior-analytics-engineer-at-upstart-3742114116?trk=public_jobs_topcard-title","United States","5 days ago","","","2023-11-29","","Information Services, Technology, Information and Internet, and Financial Services","Senior Analytics Engineer","Information Technology and Engineering"
"118 applicants","3513709","Experfy","https://www.linkedin.com/company/experfy?trk=public_jobs_topcard-org-name","Full-time","As a Data Engineer for our Data Platform Engineering team you will join skilled Scala/ Spark engineers and core database developers responsible for developing hosted cloud analytics infrastructure (Apache Spark-based), distributed SQL processing

frameworks, proprietary data science platforms, and core database optimization. This team is responsible for building the automated, intelligent, and highly performant query planner and execution engines, RPC calls between data

warehouse clusters, shared secondary cold storage, etc. This includes building new SQL features and customer-facing functionality, developing novel query optimization techniques for industry-leading performance, and building a database

system that's highly parallel, efficient and fault-tolerant. This is a vital role reporting to exec leadership and senior engineering leadership

Requirements

Responsibilities:


 * Writing Scala code with tools like Apache Spark + Apache Arrow + Apache Kafka to build a hosted, multi-cluster data warehouse for Web3
 * Developing database optimizers, query planners, query and data routing mechanisms, cluster-to-cluster communication, and workload management techniques
 * Scaling up from proof of concept to ""cluster scale"" (and eventually hundreds of clusters with hundreds of terabytes each), in terms of both infrastructure/architecture and problem structure
 * Codifying best practices for future reuse in the form of accessible, reusable patterns, templates, and code bases to facilitate meta data capturing and management
 * Managing a team of software engineers writing new code to build a bigger, better, faster, more optimized HTAP database (using Apache Spark, Apache Arrow, Kafka, and a wealth of other open source data tools)
 * Interacting with exec team and senior engineering leadership to define, prioritize, and ensure smooth deployments with other operational components
 * Highly engaged with industry trends within analytics domain from a data acquisition processing, engineering, management perspective
 * Understand data and analytics use cases across Web3 / blockchains
   
   

Skills & Qualifications


 * Bachelor's degree in computer science or related technical field. Masters or PhD a plus
 * 6+ years experience engineering software and data platforms / enterprise-scale data warehouses, preferably with knowledge of open source Apache stack (especially Apache Spark, Apache Arrow, Kafka, and others)
 * 3+ years experience with Scala and Apache Spark (or Kafka)
 * A track record of recruiting and leading technical teams in a demanding talent market
 * Rock solid engineering fundamentals; query planning, optimizing and distributed data warehouse systems experience is preferred but not required
 * Nice to have: Knowledge of blockchain indexing, web3 compute paradigms, Proofs and consensus mechanisms... is a strong plus but not required
 * Experience with rapid development cycles in a web-based environment
 * Strong scripting and test automation knowledge
 * Nice to have: Passionate about Web3, blockchain, decentralization, and a base understanding of how data/analytics plays into this
   
   

Apply for this job","Mid-Senior level","https://www.linkedin.com/jobs/view/data-engineer-database-engineering-at-experfy-3590300667?trk=public_jobs_topcard-title","New York, NY","8 months ago","","","2023-04-03","","Technology, Information and Internet","Data Engineer, Database Engineering","Information Technology"
"Over 200 applicants","16162082","Kin Insurance","https://www.linkedin.com/company/kin-insurance?trk=public_jobs_topcard-org-name","Full-time","The world has changed. Why hasn’t insurance?

Kin’s mission is to reimagine home insurance For Every New Normal. While other insurers struggle to handle a fast-changing world, Kin is built for the future and is prepared to meet its challenges head on while helping our customers do the same.

Kin is proud to be one of BuiltIn Chicago’s 2021 and 2022 Best Mid Sized Companies to work for, and Forbes 2021 Best Startup Employers in North America. Simply put, our people are what make us great, and we need forward-thinking, inspired game-changers like you to join us in our mission.

So, what’s the role?

Data is central to Kin’s operations and success. As a data engineer, you will be part of a data management team that supports and enables our product, operations, analytics, and data science teams, amongst others. As we scale, you will be integral in how we manage, structure, and store our data, as well as develop new solutions related to data architecture and ETL pipelines.

A day in the life could include:


 * Creating, designing, and maintaining ETL pipelines
 * Working with data science and BI teams to create data sets to be used in various projects
 * Participating in recurring scrum events
 * Collaborating with cross-functional team members
 * Providing subject matter expertise and support
   
   

I’ve got the skills… but do I have the necessary ones?


 * 3+ years of data engineering experience
 * Experience with the entire ETL pipeline: Data Integration tools, Databases, Big Data Platforms, and cloud based data platforms.
 * Exposure to and support of data visualization tools, such as Looker.
 * Experience in building from the ground up a modern next generation data warehouse platform.
   
   

Oh, and don’t worry, we’ve got you covered!


 * Medical, Dental, Vision, Disability and Life Insurance
 * Flexible PTO policy
 * Remote work
 * Generous equity package
 * 401K with company match
 * Parental leave
 * Continuing education and professional development
 * The excitement of joining a high-growth Insurtech company and seeing your work make an impact
   
   

About Kin

In an industry that hasn't budged in more than 100 years, our technology transforms the user experience, cuts inefficiencies that waste billions of consumer dollars, and customizes coverage homeowners want. We believe insurance was always meant to be a digital product – we’re making that a reality.

Our approach to the industry makes us unique, and the people at Kin help us excel. We’re a team of problem solvers, collaborators, builders, and dreamers who are passionate about creating positive change in the lives of our customers and in our industry. Kin is more than just our name – it’s how we treat each other. That’s one of the many reasons we’ve been recognized as a great place to work by Built In, Forbes, and Fast Company.

EEOC Statement

Kin is proud to be an Equal Employment Opportunity and Affirmative Action Employer. We don't just accept difference – we honor it, nurture it, and celebrate it. We don’t discriminate based on race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.

Kin encourages applications from all backgrounds, communities and industries, and are committed to having a team that is made up of diverse skills, experiences and abilities.

Remote","Entry level","https://www.linkedin.com/jobs/view/data-engineer-at-kin-insurance-3702676448?trk=public_jobs_topcard-title","United States","1 week ago","","","2023-11-23","","Insurance","Data Engineer","Information Technology"
"Over 200 applicants","1893","Bayer","https://de.linkedin.com/company/bayer?trk=public_jobs_topcard-org-name","Full-time","At Bayer we’re visionaries, driven to solve the world’s toughest challenges and striving for a world where 'Health for all Hunger for none’ is no longer a dream, but a real possibility. We’re doing it with energy, curiosity and sheer dedication, always learning from unique perspectives of those around us, expanding our thinking, growing our capabilities and redefining ‘impossible’. There are so many reasons to join us. If you’re hungry to build a varied and meaningful career in a community of brilliant and diverse minds to make a real difference, there’s only one choice.

Sr Data Engineer

Bayer Research and Development Services LLC’s Chesterfield, MO, seeks a senior Data Engineer. This is a remote position within the United States that will work closely with Products & Engineering IT teams, R&D IT and other global partners to influence the Field Solutions data engineering strategy and design as well as extend new and existing data repositories to enable the future of Crop Protection research and discovery; collaborate with other CropScience R&D organizations to ensure co-development in reusable data architecture and systems; automate extraction, transport, transformation, and preprocessing of data across global Field Solutions to ensure data are FAIR and usable for advanced data modeling and analytics and coordinate efforts with multiple hubs globally using common workflows and requiring a strong ability to engage with end users across disciplines to ensure data is broadly accessible and informative.

Your Tasks And Responsibilities

The primary responsibilities of this role, Senior Data Engineer, are to:


 * Lead and participate in design sessions with Engineering teams, Data Scientists, Product Managers, business, and Information Technology (IT) stakeholders, that result in documentation for data processing, storage and delivery solutions
 * Understand business capability needs and processes as they relate to IT solutions through partnering with Product Managers and business and functional IT stakeholders, and apply this knowledge to defining business problems that need to be solved
 * Initiate and lead evaluation of new technologies including performing POCs and presenting results to others, with a goal of providing technical recommendations
 * Help the team establish and improve processes and methodologies, like SCRUM or Kanban, and/or lead piloting new ones
 * Implement data solutions according to design documentation using a variety of tools and programming languages, like AWS and GCP cloud solutions, Kafka, SQL and non-SQL databases, Python, Scala, Go etc., and follow team’s established processes and methodologies;
 * Facilitate and participate in code reviews, retrospectives, functional and integration testing and other team activities focused on improving quality of delivery
 * Provide reliable estimates for large scale projects
 * Initiate collaboration with Product Owners, other engineers and data stewards within the team and across data, technical platforms and product teams on planning and aligning roadmaps, delivery dates and integration efforts
 * Coach and mentor junior and aspiring Data Engineers on the team and across the data and engineering communities
 * Facilitate various cross team efforts, like Scrum of Scrums and Release Planning, focused on large scale roadmap alignments, sharing information, solving broad variety of problems, or improving processes
 * Effectively discuss work or provide detail to the right level of audience, business partners, data scientists, engineering teams etc.
 * Create and maintain design and code documentation in GitHub, Haystack, SharePoint and/or other repositories used by the team.
   
   

Who You Are

Your success will be driven by your demonstration of our LIFE values. More specifically related to this position, Bayer seeks a candidate who possesses the following:

Required Qualifications:


 * Educational preparation or applied experience in at least one of the following areas, Engineering, Operation Research, Statistics, Biostatistics, Bioinformatics, Genomics, Computational Biology, Applied Mathematics, Computer Science or other related quantitative discipline
 * Strong level of experience building data models using R, Python or other statistical and/or mathematical programming packages
 * Strong experience with engineering data intensive software using streaming and resource-based design principles
 * Technical expertise and advocate of software development best practices (Version Control, Code Documentation and Review, Cloud Based Sequence Analysis, Database Management)
 * Experience with at least one cloud native data warehouse database, BigQuery, Redshift, Snowflake etc.
 * Experience with DevOps methodologies including Infrastructure as Code concept
 * Experience with Cloud native technologies for processing data at scale and delivering data pipelines including Kafka, Spark, AWS SQS, Lambda, Step functions, ECS, Fargate, Athena, BigQuery, GCP PubSub, Cloud functions, Cloud Run, Kubernetes
 * Demonstrated advanced business acumen, people and project leadership competencies, and technical expertise
 * Excellent communication skills with the ability to communicate complex qualitative analysis in a clear, precise and actionable manner and deliver presentations to large audiences, executive leadership, and externally at conference and collaborations
 * Strong organizational skills, interpersonal skills, written and oral communications
 * Strong problem solving and proven project delivery skills
 * Demonstrated experience with global multi-disciplinary teams and learning the science
 * Creative, proactive, bold and out-of-box thinking.
   
   

Preferred Qualifications:


 * Bachelor’s degree with nine years of experience or Master’s degree with six years of experience or Ph.D. degree with three years of professional and/or post doc experience
   
   

Employees can expect to be paid a salary between $121,700 to $182,500. Additional compensation may include a bonus or commission (if relevant). Other benefits include health care, vision, dental, retirement, PTO, sick leave, etc. If selected for this role, the offer may vary based on market data/ranges, an applicant’s skills and prior relevant experience, certain degrees and certifications, and other relevant factors.

YOUR APPLICATION

Bayer offers a wide variety of competitive compensation and benefits programs. If you meet the requirements of this unique opportunity, and want to impact our mission Science for a better life, we encourage you to apply now. Be part of something bigger. Be you. Be Bayer.

To all recruitment agencies: Bayer does not accept unsolicited third party resumes.

Bayer is an Equal Opportunity Employer/Disabled/Veterans

Bayer is committed to providing access and reasonable accommodations in its application process for individuals with disabilities and encourages applicants with disabilities to request any needed accommodation(s) using the contact information below.

Bayer is an E-Verify Employer.

Location: United States : Residence Based : Residence Based

Division: Crop Science

Reference Code: 805594

Contact Us

Email: hrop_usa@bayer.com","Mid-Senior level","https://www.linkedin.com/jobs/view/sr-data-engineer-at-bayer-3760460725?trk=public_jobs_topcard-title","United States","5 days ago","","","2023-11-29","","Chemical Manufacturing, Biotechnology Research, and Pharmaceutical Manufacturing","Sr Data Engineer","Information Technology"
"187 applicants","3513709","Experfy","https://www.linkedin.com/company/experfy?trk=public_jobs_topcard-org-name","Full-time","As a Data Engineer for our Data Platform Engineering team you will join skilled Scala/ Spark engineers and core database developers responsible for developing hosted cloud analytics infrastructure (Apache Spark-based), distributed SQL processing

frameworks, proprietary data science platforms, and core database optimization. This team is responsible for building the automated, intelligent, and highly performant query planner and execution engines, RPC calls between data

warehouse clusters, shared secondary cold storage, etc. This includes building new SQL features and customer-facing functionality, developing novel query optimization techniques for industry-leading performance, and building a database

system that's highly parallel, efficient and fault-tolerant. This is a vital role reporting to exec leadership and senior engineering leadership

Requirements

Responsibilities:


 * Writing Scala code with tools like Apache Spark + Apache Arrow + Apache Kafka to build a hosted, multi-cluster data warehouse for Web3
 * Developing database optimizers, query planners, query and data routing mechanisms, cluster-to-cluster communication, and workload management techniques
 * Scaling up from proof of concept to ""cluster scale"" (and eventually hundreds of clusters with hundreds of terabytes each), in terms of both infrastructure/architecture and problem structure
 * Codifying best practices for future reuse in the form of accessible, reusable patterns, templates, and code bases to facilitate meta data capturing and management
 * Managing a team of software engineers writing new code to build a bigger, better, faster, more optimized HTAP database (using Apache Spark, Apache Arrow, Kafka, and a wealth of other open source data tools)
 * Interacting with exec team and senior engineering leadership to define, prioritize, and ensure smooth deployments with other operational components
 * Highly engaged with industry trends within analytics domain from a data acquisition processing, engineering, management perspective
 * Understand data and analytics use cases across Web3 / blockchains
   
   

Skills & Qualifications


 * Bachelor's degree in computer science or related technical field. Masters or PhD a plus
 * 6+ years experience engineering software and data platforms / enterprise-scale data warehouses, preferably with knowledge of open source Apache stack (especially Apache Spark, Apache Arrow, Kafka, and others)
 * 3+ years experience with Scala and Apache Spark (or Kafka)
 * A track record of recruiting and leading technical teams in a demanding talent market
 * Rock solid engineering fundamentals; query planning, optimizing and distributed data warehouse systems experience is preferred but not required
 * Nice to have: Knowledge of blockchain indexing, web3 compute paradigms, Proofs and consensus mechanisms... is a strong plus but not required
 * Experience with rapid development cycles in a web-based environment
 * Strong scripting and test automation knowledge
 * Nice to have: Passionate about Web3, blockchain, decentralization, and a base understanding of how data/analytics plays into this","Mid-Senior level","https://www.linkedin.com/jobs/view/data-engineer-database-engineering-at-experfy-3590297967?trk=public_jobs_topcard-title","San Francisco, CA","8 months ago","","","2023-04-03","","Technology, Information and Internet","Data Engineer, Database Engineering","Information Technology"
"Over 200 applicants","583166","Avenue Code","https://www.linkedin.com/company/avenuecode?trk=public_jobs_topcard-org-name","Full-time","About the company:

Avenue Code is the leading software consultancy focused on delivering end-to-end development solutions for digital transformation across every vertical. We’re privately held, profitable, and have been on a solid growth trajectory since day one. We care deeply about our clients, our partners, and our people. We prefer the word ‘partner’ over ‘vendor’, and our investment in professional relationships is a reflection of that philosophy. We pride ourselves on our technical acumen, our collaborative problem-solving ability, and the warm professionalism of our teams.

More than 3 years of working on diversity and inclusion strategies, Avenue Code still believes that recognizing the difference and promoting a safe place, job opportunities, representativeness, and support is the best way to make the social transformation happen. All job openings aim to promote employment and recognition of diversity pillars.

About the opportunity:

We are looking for a passionate, hard-working, and talented Data Engineer to join our team. You will be working with one of our biggest clients on products with a high impact on their revenue.

Responsibilities:


 * You will architect, develop, and test large scale data solutions, to provide efficient analytical and reporting capabilities across global and regional sales and finance teams.
 * You will develop highly scalable data pipelines to load data from various source systems, use Apache Airflow to orchestrate, schedule and monitor the workflows.
 * Build generic and reusable solutions that can scale and utilize various technologies and frameworks to solve our complex business requirements.
 * You will be required to understand existing solutions, fine-tune them and support them as needed.
   
   

Required Qualifications:

We know that each person has gone through different career paths that have provided further knowledge. If you don’t check all the points: go ahead! Apply anyways! Your experience is much more than a technology checklist.


 * Strong expertise in dimensional modeling and data warehousing.
 * Design and development experience with Cloud Data warehouses like Snowflake, Redshift, BigQuery etc.
 * Hands on experience with Big-Data platforms like Spark, Dremio, Hadoop, MapReduce, Hive etc.
 * Proficiency in design and development of custom ETL pipelines using SQL and scripting languages (Python/ Shell/ Golang) and workflow management tools like Airflow.
 * Proficiency in advanced SQL, performance tuning.
 * Experience with cloud computing platforms like AWS or Google Cloud.
   
   

Nice to Have:

More than 3 years of working on diversity and inclusion strategies, Avenue Code still believes that recognizing the difference and promoting a safe place, job opportunities, representativeness, and support is the best way to make the social transformation happen. All job openings aim to promote employment and recognition of diversity pillars.

Avenue Code discloses salary range information in compliance with state and local pay transparency obligations. It considers a wide range of factors such as internal equity, geographic location, relevant education, qualifications, certifications, experience, skills, seniority, business or organizational needs and others. At Avenue Code it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. A reasonable estimate of the current range for US Based Data Engineer is 110 K to 200 K per year.

","Entry level","https://www.linkedin.com/jobs/view/data-engineer-at-avenue-code-3775646864?trk=public_jobs_topcard-title","United States","4 days ago","","","2023-11-29","","IT Services and IT Consulting","Data Engineer","Information Technology"
"Over 200 applicants","13365425","Juul Labs","https://www.linkedin.com/company/juul-labs?trk=public_jobs_topcard-org-name","Full-time","THE COMPANY:

Juul Labs’ mission is to impact the lives of the world’s one billion adult smokers by eliminating combustible cigarettes. We have the opportunity to address one of the world’s most intractable challenges through a commitment to exceptional quality, research, design, and innovation. Backed by leading technology investors, we are committed to the same excellence when it comes to hiring great talent.

We are a diverse team that is united by this common purpose and we are hiring the world’s best engineers, scientists, designers, product managers, operations experts, and customer service and business professionals. If the opportunity to build your career at one of the fastest growing companies is compelling, read on for more details.

ROLE AND RESPONSIBILITIES:

Must live in US

Data at Juul means working with varied, large, data sets – where we apply analytical methods to help inform and drive business and product decisions. We are looking for an output-focused problem solver with a strong conceptual mindset and superb communication skills.

The team sees itself as analytical generalists – we choose the right technique for each problem, pride ourselves on building beautiful systems and dashboards while moving fast, and are ultimately driven by the value and insight data science can generate for the business and our customers. Our work directly impacts and shapes key executive decisions. This role offers tremendous upwards exposure towards senior business leaders and the chance to truly impact the decision making in a startup.

KEY RESPONSIBILITIES:


 * Modeling the dynamics of the fast moving and competitive market that we operate in and understanding our impact on it
 * Bringing visibility into our complex sales, logistics and supply chain operations through dashboards and standardized metrics
 * Explaining and predicting online and offline consumer choices and consumption patterns
 * Building demand forecasting models and understanding their behavior
 * Creating clean analytical data models and tools to help empower business and operational teams
 * Design robust, reusable and scalable data solutions and data pipeline frameworks to automate the ingestion, processing and delivery of both structured and unstructured data using python
 * Be in active development of large-scale data engineering projects
 * Create data pipelines in airflow, DBT and the general suite of Google Cloud Platform
 * Build, manage, and support data models. Ensure data quality with data tests in Monte Carlo and Datafold
 * Work in a scrum agile environment using Trello
 * Partner with Data Scientists, Data Engineers and Business Analysts to build configurable, scalable, and robust data processing infrastructure
 * Work closely with our sales, operations, research, and finance teams on data storage, retrieval, and analysis
 * Develop new systems and tools to enable stakeholders to consume and understand data more intuitively
 * Create and establish design standards and assurance processes to ensure compatibility and operability of data connections, flows and storage requirements
 * Keep Juul on the cutting edge of data technology
 * Our Data Stack:
 * Airflow, Fivetran
 * Google Cloud Platform - GCP (BigQuery, Storage, Dataflow, Pub/Sub, Cloud Functions/Run, Vertex AI, Cloud Build)
 * DBT
 * Monte Carlo, Datafold
 * Tableau
   
   

PERSONAL AND PROFESSIONAL QUALIFICATIONS:


 * 4+ years of data engineering or software engineering experience with a focus on data
 * Advanced knowledge in developing using Python for data processing for large-scale datasets and workflows
 * Skilled using python libraries and packages (pandas, pyarrow) in conjunction with the Google Cloud Platform (BigQuery, Storage, Pub/Sub)
 * Knowledge of bash/shell and orchestration tools (e.g. Airflow), is preferred
 * Experience with version control (Git) and containers (Docker)
 * Skilled in analytical SQL in support of data modeling/ transformations and manipulating multiple data formats
 * Foundational expertise of deploying and maintaining machine learning pipelines is a plus
   
   

EDUCATION & EXPERIENCE:

Preferred masters degree in Computer Science, Engineering, Math, or equivalent experience

JUUL LABS PERKS & BENEFITS:


 * A place to grow your career. We’ll help you set big goals - and exceed them
 * People. Work with talented, committed and supportive teammates
 * Equity and performance bonuses. Every employee is a stakeholder in our success
 * Cell phone subsidy, commuter benefits and discounts on JUUL products
 * Excellent medical, dental and vision benefits
   
   

Juul Labs is proud to be an equal opportunity employer and is committed to creating a diverse and inclusive work environment for all employees and job applicants, without regard to race, color, religion, sex, sexual orientation, age, gender identity or gender expression, national origin, disability or veteran status. We will consider for employment qualified applicants with arrest and conviction records, pursuant to the San Francisco Fair Chance Ordinance. Juul Labs also complies with the employment eligibility verification requirements of the Immigration and Nationality Act. All applicants must have authorization to work for Juul Labs in the US.

SALARY RANGES:

Salary varies by role, level and location, and is dependent on the cost of labor in a given

geographic region among other factors. These ranges may be modified at any time.

LOCATIONS:

Tier 1 Locations: Greater New York City, and San Francisco Bay Area

Tier 2 Locations: Greater Boston, Washington DC Metropolitan Area, Seattle/Tacoma,

Greater Sacramento, Los Angeles/OC/San Diego

Tier 3 Locations: Rest of New England, NY Capital District, Rest of New Jersey, Greater

Philadelphia, Pittsburgh, Delaware, Rest of Maryland, Rest of Virginia, North Carolina,

Atlanta, Miami-Fort Lauderdale-WPB, Chicagoland, Dallas, Houston, Austin,

Minneapolis/St. Paul, Colorado, Phoenix, Reno, Las Vegas, Portland Ore./Vancouver

Wash., Rest of California, Hawaii

Tier 4 Locations: Rest of US including Alaska and Puerto Rico

Tier 1 Range:

$132,000—$198,000 USD

Tier 2 Range:

$122,000—$183,000 USD

Tier 3 Range:

$114,000—$171,000 USD

Tier 4 Range:

$104,000—$156,000 USD

","Mid-Senior level","https://www.linkedin.com/jobs/view/senior-data-engineer-at-juul-labs-3748329225?trk=public_jobs_topcard-title","United States","2 weeks ago","","","2023-11-15","","Manufacturing","Senior Data Engineer","Information Technology"
"Over 200 applicants","84180","Pluralsight","https://www.linkedin.com/company/pluralsight?trk=public_jobs_topcard-org-name","Full-time","Job Description

The Opportunity

Our Data Engineering team, within our Data Services Organization, builds and maintains the infrastructure essential to delivering high-volume, business-critical data to the organization to enable data-driven decisions.

We are focused on expanding our curated and modeled data that unify sources of truth across our multiple products and domains. You’ll have the opportunity and empowerment to guide the data engineering team on best practices using modern data tools like Snowflake, Kafka, and dbt.

This is an ideal opportunity for someone that wants to contribute to our growing data platform and warehouse by writing and implementing production data pipelines. You will get your hands dirty with lots of data and make a large impact with delivering datasets to stakeholders.

Who You Are


 * 2+ years of experience designing and delivering data warehouses and marts to support business analytics
 * 2+ years of experience designing and developing data curation and integration processes
 * 2+ years of experience in SQL development on RDBMS (Snowflake and Postgres preferred) and performance tuning
 * Experience with streaming & real-time data processing using a technology like Spark, Kafka, ksqlDB, or Databricks, etc.
 * Experience with dimensional data modeling/data workflow diagrams (conceptual, logical, and physical)
 * Experience with source control and deployment workflows for ETL (dbt, Fivetran, github actions, gitlab, airflow, etc.)
 * Experience working with AWS services such as DynamoDB, Glue, Lambda, Step Functions, S3, CloudFormation
 * Hands on experience with scripting languages (Python, BASH, etc)
   
   

What You’ll Own


 * Contribution to Data Warehouse models and curation
 * Support and evolution of data environment to deliver high-quality data, speed, and availability
 * Curation of source-system data to deliver trusted data sets
 * Involvement on data cataloging and data management efforts
 * Production ETL performance tuning and environment-level resource consumption and management
 * Migration of POC pipelines to production data processes
   
   

Experience You’ll Need


 * Capability to manipulate and analyze complex, high-volume data from a variety of sources
 * Experience designing and building end-to-end data models and pipelines as well as alerting
 * Experience in data modeling for batch processing and streaming data feeds; structured and unstructured data
   
   

Working at Pluralsight

Founded in 2004 and trusted by Fortune 500 companies, Pluralsight is the technology skills platform organizations and individuals in 150+ countries count on to create progress for the world.

Our platform helps technologists master their craft and take control of their careers. We empower businesses everywhere to build adaptable teams, speed up release cycles and become scalable, reliable and secure. We come to work every day knowing we’re helping our customers build the skills that power innovation.

And we don’t let fear, egos or drama distract us from our mission. Our mission to advance the world's tech workforce is what drives us and our values are at the helm of how we work together. It’s our commitment to practicing them day in, day out that enables our performance. We’re adults, and we treat each other that way. We have the autonomy to do our jobs, transparency to eliminate office politics and trust each other to do the right thing. We thrive in an environment with creativity around every corner, challenges that keep us on our toes, and peers who inspire us to be the best we can be. We bring different viewpoints, backgrounds and experiences, and united by our mission, we are one.

Bring yourself. Pluralsight is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age or veteran status.

https://www.youtube.com/watch?v=-HTcEYIzA6c&t=1s

","Entry level","https://www.linkedin.com/jobs/view/data-engineer-at-pluralsight-3762302382?trk=public_jobs_topcard-title","United States","4 days ago","Spencer Winegar","https://www.linkedin.com/in/spencer-winegar-3a43412","2023-11-30","","E-Learning Providers","Data Engineer","Information Technology"
"125 applicants","157258","Care.com","https://www.linkedin.com/company/care-com?trk=public_jobs_topcard-org-name","Full-time","About Care.com

Care.com is a consumer tech company with heart. We're on a mission to solve a human challenge we all face: finding great care for the ones we love. We're moms and dads and pet parents. We have parents and grandparents, so we understand that everyone, at some point in their lives, could use a helping hand. Our culture and our products reflect that.

Here, entrepreneurs, self-starters, team players, and big thinkers unite behind a common cause. Here, we're applying data analytics, AI, and the latest technologies to solve universal problems and connect people in new ways. If you like having autonomy, if you thrive on collaboration and building new things, and if you're all about using your talent for good, Care.com is the place for you.

What Your Days Will be Like:

The Data Engineer will be focusing on building out data feeds and tooling from our application platform and enable rapid ingestion into our centralized Data Lake/Data Warehouse. The Data Engineer will work across business areas and application teams at Care.com to rationalize data and design, build and maintain reusable data feeds which and ultimately empower analytics consumption at Care.com.

The ideal candidate will have professional experience building data pipelines in a technical environment. S/he will have an understanding of application development, data warehousing, demonstrate strong business judgment, and be able to prioritize in a fast-paced environment.

What You'll Be Working On:


 * Collaborate and partner with application teams to understand data collection/generation and design and partner to build and implement data feeds from our product tech stack
 * Design, develop, and build code for rapid feeds and ingestion into the Data Warehouse
 * Identify data sources used for building out data architecture diagrams/models
 * Establish engineering practices and setup frameworks for ""Data as a Service""
 * Collaborate with relevant delivery teams, including infrastructure, operations, site reliability engineering, product development, and others to perform evaluations, POCs, and ultimately implement and operationalize new technology.
 * Solve code level problems quickly and efficiently
 * Participate in demos and code reviews
 * Promote software best approach, standards, and processes
 * Shape development processes to promote a high-quality output while continuing to iterate quickly
 * Incorporate best practices for security, performance, and data privacy into data pipelines
   
   

What You'll Need to Succeed:


 * BS or MS in Computer Science or relevant engineering experience
 * 5+ years work experience in Data Engineering/data pipelines
 * 3+ years SQL experience is a must
 * 1+ years Unix/batch scripting preferred
 * 1+ years Python experience is a plus
 * 1+ years Windows server admin experience is a plus
 * Experience interfacing with business teams and turning requirements and vision into a technical reality
 * MySQL & Vertica Experience a plus/preferred
 * AWS experience is a plus
 * Ability to drive efforts from start to finish as a self-motivator
 * Knowledge in Data Warehousing is a MUST
 * Proven ability to maintain performance level in a fast-paced agile environment
 * Pragmatic and realistic with solutions
   
   

For a list of our Perks + Benefits, click here!

Care.com supports diverse families and communities and seeks employees who are just as diverse. As an equal opportunity employer, Care.com recognizes the power of a diverse and inclusive workforce and encourages applications from individuals with varied experiences, perspectives, and backgrounds. Care.com is committed to providing reasonable accommodations for qualified individuals with disabilities. If you need assistance or accommodation, please reach out to talent@care.com.

Company Overview:

Available in more than 20 countries, Care.com is the world's leading platform for finding and managing high-quality family care. Care.com is designed to meet the evolving needs of today's families and caregivers, offering everything from household tax and payroll services and customized corporate benefits packages covering the care needs of working families, to innovating new ways for caregivers to be paid and obtain professional benefits. Since 2007, families have relied on Care.com's industry-leading products—from child and elder care to pet care and home care. Care.com is an IAC company (NASDAQ: IAC).

Salary Range: 110,000 to 145,000. The base salary range above represents the anticipated low and high end of the national salary range for this position. Actual salaries may vary and may be above or below the range based on various factors including but not limited to work location, experience, and performance. The range listed is just one component of Care.com's total compensation package for employees. Other rewards may include annual bonuses and short- and long-term incentives. In addition, Care.com provides a variety of benefits to employees, including health insurance coverage, life, and disability insurance, a generous 401K employer matching program, paid holidays, and paid time off (PTO).","Entry level","https://www.linkedin.com/jobs/view/data-engineer-at-care-com-3743209378?trk=public_jobs_topcard-title","New York, NY","1 month ago","","","2023-10-19","","Technology, Information and Internet","Data Engineer","Information Technology"
"64 applicants","972069","APN Consulting Inc.","https://www.linkedin.com/company/apn-consulting-inc.?trk=public_jobs_topcard-org-name","Full-time","APN Consulting has an immediate need for a direct client requirement:
Data Engineer
Duration: Full Time/Permanent
Remote Role
As a Senior Data Engineer on the client IT Data and Analytics Team, we will count on you to develop and maintain scalable data pipelines, enhance data models, and provide subject matter expertise in data acquisition and consumption pipelines on Azure, Databricks and Confluent. It involves developing best practices, reusable code, libraries, and frameworks for cloud-based data warehousing and ETL, and uses multi-cloud, programming languages like Java, Scala, Python, RDBMS, NoSQL databases, and design enterprise data warehouse platforms.
By fostering collaboration and aligning with business objectives, you will have the opportunity to elevate data models, drive data-driven decision-making, and enhance data accessibility throughout the organization. Join us in championing data-driven decision-making and becoming an essential contributor to our data-driven success.

Our future colleague.

We'd love to meet you if your professional track record includes these skills:

 * 7+ years of hands-on experience designing and implementing data engineering solutions using Azure cloud offerings including Data Factory, Azure Synapse, ADLS, and Azure Functions and Logic Apps.
 * In-depth understanding and experience in data engineering and data warehousing best practices, data modeling, data security, and governance principles throughout the data lifecycle.
 * Hands-on development experience with Relational DBs (MS SQL Server, PostgreSQL, Oracle), as well as complex stored procedures and functions using SQL and optimization of SSIS packages and Redshift database queries.
 * Experience designing and building data pipelines using APIs, BigQuery and Streaming ingestion methods. Proficiency in Spark, Python and ability work with telemetry using Azure monitor, App insights.
 * Solid understanding of DevSecOps, Git integration, Code deployment using CI/CD pipelines, managing work in an agile environment using Azure Boards, Confluence. Ability to troubleshoot and deploy using Redgate tools and Visual Studio is a must.

These additional qualifications are a plus, but not required to apply:

 * Familiarity with Modern Data Platforms technologies like Snowflake, Matillion, Bryte Flow, and Tableau is a plus.
 * Knowledge in Azure IoT, Azure HDInsight + Spark, Kafka and Azure Stream Analytics is a bonus.
 * Experience with Informatica - IICS task flows development and maintenance.
 * Data bricks, Microsoft Azure Data Engineer, and/or other cloud certifications..","Entry level","https://www.linkedin.com/jobs/view/data-engineer-at-apn-consulting-inc-3715821843?trk=public_jobs_topcard-title","United States","2 months ago","","","2023-09-12","","IT Services and IT Consulting","Data Engineer","Information Technology"
"117 applicants","157258","Care.com","https://www.linkedin.com/company/care-com?trk=public_jobs_topcard-org-name","Full-time","About Care.com

Care.com is a consumer tech company with heart. We're on a mission to solve a human challenge we all face: finding great care for the ones we love. We're moms and dads and pet parents. We have parents and grandparents, so we understand that everyone, at some point in their lives, could use a helping hand. Our culture and our products reflect that.

Here, entrepreneurs, self-starters, team players, and big thinkers unite behind a common cause. Here, we're applying data analytics, AI, and the latest technologies to solve universal problems and connect people in new ways. If you like having autonomy, if you thrive on collaboration and building new things, and if you're all about using your talent for good, Care.com is the place for you.

What Your Days Will be Like:

The Data Engineer will be focusing on building out data feeds and tooling from our application platform and enable rapid ingestion into our centralized Data Lake/Data Warehouse. The Data Engineer will work across business areas and application teams at Care.com to rationalize data and design, build and maintain reusable data feeds which and ultimately empower analytics consumption at Care.com.

The ideal candidate will have professional experience building data pipelines in a technical environment. S/he will have an understanding of application development, data warehousing, demonstrate strong business judgment, and be able to prioritize in a fast-paced environment.

What You'll Be Working On:


 * Collaborate and partner with application teams to understand data collection/generation and design and partner to build and implement data feeds from our product tech stack
 * Design, develop, and build code for rapid feeds and ingestion into the Data Warehouse
 * Identify data sources used for building out data architecture diagrams/models
 * Establish engineering practices and setup frameworks for ""Data as a Service""
 * Collaborate with relevant delivery teams, including infrastructure, operations, site reliability engineering, product development, and others to perform evaluations, POCs, and ultimately implement and operationalize new technology.
 * Solve code level problems quickly and efficiently
 * Participate in demos and code reviews
 * Promote software best approach, standards, and processes
 * Shape development processes to promote a high-quality output while continuing to iterate quickly
 * Incorporate best practices for security, performance, and data privacy into data pipelines
   
   

What You'll Need to Succeed:


 * BS or MS in Computer Science or relevant engineering experience
 * 5+ years work experience in Data Engineering/data pipelines
 * 3+ years SQL experience is a must
 * 1+ years Unix/batch scripting preferred
 * 1+ years Python experience is a plus
 * 1+ years Windows server admin experience is a plus
 * Experience interfacing with business teams and turning requirements and vision into a technical reality
 * MySQL & Vertica Experience a plus/preferred
 * AWS experience is a plus
 * Ability to drive efforts from start to finish as a self-motivator
 * Knowledge in Data Warehousing is a MUST
 * Proven ability to maintain performance level in a fast-paced agile environment
 * Pragmatic and realistic with solutions
   
   

For a list of our Perks + Benefits, click here!

Care.com supports diverse families and communities and seeks employees who are just as diverse. As an equal opportunity employer, Care.com recognizes the power of a diverse and inclusive workforce and encourages applications from individuals with varied experiences, perspectives, and backgrounds. Care.com is committed to providing reasonable accommodations for qualified individuals with disabilities. If you need assistance or accommodation, please reach out to talent@care.com.

Company Overview:

Available in more than 20 countries, Care.com is the world's leading platform for finding and managing high-quality family care. Care.com is designed to meet the evolving needs of today's families and caregivers, offering everything from household tax and payroll services and customized corporate benefits packages covering the care needs of working families, to innovating new ways for caregivers to be paid and obtain professional benefits. Since 2007, families have relied on Care.com's industry-leading products—from child and elder care to pet care and home care. Care.com is an IAC company (NASDAQ: IAC).

Salary Range: 110,000 to 145,000. The base salary range above represents the anticipated low and high end of the national salary range for this position. Actual salaries may vary and may be above or below the range based on various factors including but not limited to work location, experience, and performance. The range listed is just one component of Care.com's total compensation package for employees. Other rewards may include annual bonuses and short- and long-term incentives. In addition, Care.com provides a variety of benefits to employees, including health insurance coverage, life, and disability insurance, a generous 401K employer matching program, paid holidays, and paid time off (PTO).","Entry level","https://www.linkedin.com/jobs/view/data-engineer-at-care-com-3743206753?trk=public_jobs_topcard-title","Denver, CO","1 month ago","","","2023-10-19","","Technology, Information and Internet","Data Engineer","Information Technology"
"165 applicants","70382927","Pulivarthi Group (PG)","https://www.linkedin.com/company/pulivarthigroup?trk=public_jobs_topcard-org-name","Full-time","Follow us on Linkedin: https://www.linkedin.com/company/pulivarthigroup/

Pulivarthi Group LLC is a Global Staffing & IT Technology Solutions company, with our prime focus of providing world class solutions to our customers with the right talent. We combine the expertise of our team and the culture of your company to help you with the solution that is affordable and innovative using high quality standards and technologies.

We’ve served some of the largest healthcare, financial services, and government entities in the U.S.


 * Data Engineer - FT only - Azure SQL with data lakes experience
   
   

Data Engineer -

the resource requirement for this Data Engineer role needed more clarification.

Our customer is requesting a resource who knows Azure CI/CD (Continuous Integration / Continuous Deployment) to build and integrate their DevOps pipeline.

Once the DevOps pipelines are mapped, they need the Data Engineer to connect to all the Data Sources, primarily their Data Lake.

The Customer (based in Houston) is interested in a resource in a time zone not too far removed from the Central Time Zone to allow real time collaboration.","Mid-Senior level","https://www.linkedin.com/jobs/view/data-engineer-need-candidates-from-texas-remote-at-pulivarthi-group-pg-3770685212?trk=public_jobs_topcard-title","United States","1 week ago","","","2023-11-20","","Staffing and Recruiting","Data Engineer  - Need candidates from Texas ( REMOTE )","Information Technology"
"Over 200 applicants","157258","Care.com","https://www.linkedin.com/company/care-com?trk=public_jobs_topcard-org-name","Full-time","About Care.com

Care.com is a consumer tech company with heart. We're on a mission to solve a human challenge we all face: finding great care for the ones we love. We're moms and dads and pet parents. We have parents and grandparents, so we understand that everyone, at some point in their lives, could use a helping hand. Our culture and our products reflect that.

Here, entrepreneurs, self-starters, team players, and big thinkers unite behind a common cause. Here, we're applying data analytics, AI, and the latest technologies to solve universal problems and connect people in new ways. If you like having autonomy, if you thrive on collaboration and building new things, and if you're all about using your talent for good, Care.com is the place for you.

What Your Days Will be Like:

The Data Engineer will be focusing on building out data feeds and tooling from our application platform and enable rapid ingestion into our centralized Data Lake/Data Warehouse. The Data Engineer will work across business areas and application teams at Care.com to rationalize data and design, build and maintain reusable data feeds which and ultimately empower analytics consumption at Care.com.

The ideal candidate will have professional experience building data pipelines in a technical environment. S/he will have an understanding of application development, data warehousing, demonstrate strong business judgment, and be able to prioritize in a fast-paced environment.

What You'll Be Working On:


 * Collaborate and partner with application teams to understand data collection/generation and design and partner to build and implement data feeds from our product tech stack
 * Design, develop, and build code for rapid feeds and ingestion into the Data Warehouse
 * Identify data sources used for building out data architecture diagrams/models
 * Establish engineering practices and setup frameworks for ""Data as a Service""
 * Collaborate with relevant delivery teams, including infrastructure, operations, site reliability engineering, product development, and others to perform evaluations, POCs, and ultimately implement and operationalize new technology.
 * Solve code level problems quickly and efficiently
 * Participate in demos and code reviews
 * Promote software best approach, standards, and processes
 * Shape development processes to promote a high-quality output while continuing to iterate quickly
 * Incorporate best practices for security, performance, and data privacy into data pipelines
   
   

What You'll Need to Succeed:


 * BS or MS in Computer Science or relevant engineering experience
 * 5+ years work experience in Data Engineering/data pipelines
 * 3+ years SQL experience is a must
 * 1+ years Unix/batch scripting preferred
 * 1+ years Python experience is a plus
 * 1+ years Windows server admin experience is a plus
 * Experience interfacing with business teams and turning requirements and vision into a technical reality
 * MySQL & Vertica Experience a plus/preferred
 * AWS experience is a plus
 * Ability to drive efforts from start to finish as a self-motivator
 * Knowledge in Data Warehousing is a MUST
 * Proven ability to maintain performance level in a fast-paced agile environment
 * Pragmatic and realistic with solutions
   
   

For a list of our Perks + Benefits, click here!

Care.com supports diverse families and communities and seeks employees who are just as diverse. As an equal opportunity employer, Care.com recognizes the power of a diverse and inclusive workforce and encourages applications from individuals with varied experiences, perspectives, and backgrounds. Care.com is committed to providing reasonable accommodations for qualified individuals with disabilities. If you need assistance or accommodation, please reach out to talent@care.com.

Company Overview:

Available in more than 20 countries, Care.com is the world's leading platform for finding and managing high-quality family care. Care.com is designed to meet the evolving needs of today's families and caregivers, offering everything from household tax and payroll services and customized corporate benefits packages covering the care needs of working families, to innovating new ways for caregivers to be paid and obtain professional benefits. Since 2007, families have relied on Care.com's industry-leading products—from child and elder care to pet care and home care. Care.com is an IAC company (NASDAQ: IAC).

Salary Range: 110,000 to 135,000. The base salary range above represents the anticipated low and high end of the national salary range for this position. Actual salaries may vary and may be above or below the range based on various factors including but not limited to work location, experience, and performance. The range listed is just one component of Care.com's total compensation package for employees. Other rewards may include annual bonuses and short- and long-term incentives. In addition, Care.com provides a variety of benefits to employees, including health insurance coverage, life, and disability insurance, a generous 401K employer matching program, paid holidays, and paid time off (PTO).","Entry level","https://www.linkedin.com/jobs/view/data-engineer-at-care-com-3765031801?trk=public_jobs_topcard-title","Austin, TX","2 weeks ago","","","2023-11-16","","Technology, Information and Internet","Data Engineer","Information Technology"
"149 applicants","81606529","Steneral Consulting","https://www.linkedin.com/company/steneral-consulting?trk=public_jobs_topcard-org-name","Full-time","Hi,




Please find attached Job Description. If you are interested please do share with me your updated resume or call me on ""+1 3026017375"".




Job Title:- DATA ENGINEER (MARTECH, ADTECH, DATA PROFILING)




Work Location:- remote




Duration: 18 month




Work Authorization:- Citizen, GC




Interview : Zoom




Job Title: Data Engineer (MarTech, AdTech, Data Profiling)




Location: REMOTE or Memphis, TN




I am looking for candidates who have HANDS ON experience w/ MarTech and/or AdTech and IT SHOWS ON THE RESUME. If your candidate does NOT have this experience, do NOT send them to me for review.




Job Description




Data Engineer excited about marketing tech, ad-tech, and customer data profiling. Responsible for working with large data sets and developing data pipelines that move data from source systems to segmentation systems, advertising platforms, data warehouses, data lakes, and other data storage and processing systems. The data engineer will prepare data for synthesis and analysis by CDP and other marketing tech systems.




Requirements




 * Solid programming skills (J2EE, Python), statistics knowledge, analytical skills, and an understanding of big data technologies
 * Strong knowledge of software engineering principles and techniques
 * Data Warehousing & ETL
 * Data architecture & pipelining
 * Understanding of Salesforce and Adobe Marketing stacks
 * Experience with third party databases, libraries, interfaces, and internet protocols
 * Knowledge of LINUX, J2EE, Relational and Document Databases, JSON, Shell Scripting, automation




Kirti Rani




Associate Talent Acquisition -North America




Desk: +1 3026017375




kirti@steneral.com




In my absence please reach out to Mr. Harish Sharma at harish@steneral.com & 3027216151","Entry level","https://www.linkedin.com/jobs/view/data-engineer-martech-adtech-data-profiling-remote-at-steneral-consulting-3697172445?trk=public_jobs_topcard-title","United States","3 months ago","","","2023-08-22","","IT Services and IT Consulting","Data Engineer (MarTech, AdTech, Data Profiling)_____________remote","Information Technology"
"Over 200 applicants","1258873","Anika Systems","https://www.linkedin.com/company/anika-systems?trk=public_jobs_topcard-org-name","Full-time","Anika Systems is a fast growing, woman-owned small business that specializes in providing innovative IT solutions for federal government agencies. Our expertise lies in accelerating delivery in Data and Analytics, Intelligent Automation, Application Development, and IT Modernization. We are currently expanding our Federal team and are seeking a passionate and talented Data Engineer. This opportunity is 100% remote.

Must be a U.S. Citizen with the ability to obtain and maintain a government suitability clearance

Responsibilities


 * Designs, develops, builds, analyzes, evaluates, and installs database management systems to include database modeling and design, relational database architecture, metadata, and repository creation and configuration management.
 * Defines and oversees database organizations, standards, controls, procedures, and documentation. Provides technical consulting in the definition, design, and creation of a database environment.
 * Advises applications development staff and users on data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.
 * Designs and implements databases with respect to access methods, access time, batch processes, device allocation, validation checks, organization, protection and security, documentation, and statistical methods.
 * Uses data mapping, data mining, and data transformational analysis tools to design and develop databases.
 * Determines data storage and optimum storage requirements.
 * Prepares system requirements, source analysis, and process analyses and designs throughout the database implementation.
   
   

Required Skills And Experience


 * BA/BS and 1 year of relevant experience
 * Experience with DataBricks, SQL, Python
 * Applications development experience with data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.
 * Excellent communication skills (both oral and written) with process-oriented organizational skills to ensure project success
   
   

Desired Skills And Experience


 * Demonstrated successful development experience with the full life cycle of government database management systems including database modeling and design, relational database architecture, metadata and repository creation, and configuration management.
   
   

Powered by JazzHR

TlKWIorucJ","Mid-Senior level","https://www.linkedin.com/jobs/view/data-engineer-at-anika-systems-3766103708?trk=public_jobs_topcard-title","Leesburg, VA","2 weeks ago","","","2023-11-15","","Internet Publishing","Data Engineer","Information Technology"
"Over 200 applicants","41080","Pierce","https://www.linkedin.com/company/pierce-professional-resources?trk=public_jobs_topcard-org-name","Full-time"," * Strategy Creation: Collaborate with cross-functional teams to define the data engineering strategy aligned to business objectives, including data modeling that unifies data assets across a range of source systems used to manage the operations of our partnering hospitals.
 * Pipeline Development: Define and execute processes needed to develop, test, deploy, and maintain high quality data pipelines. Oversee the end-to-end development of data pipelines from source data extraction through to production-grade analytical dataset delivery, ensuring data quality and security throughout the pipeline.
 * Performance Optimization: Continuously monitor and optimize data processing performance and efficiency. Identify and address bottlenecks, optimize query performance, and improve overall system stability.
 * Data Governance: Establish and enforce data quality management policies, data access controls, and data privacy standards.
 * Technical Leadership: Stay abreast of the latest developments in engineering tools and best practices. Provide guidance to the team about technical challenges.
 * Documentation: Maintain clear and comprehensive documentation of data pipelines, architecture, and processes to ensure knowledge sharing and team continuity.
 * Third-party Management: Evaluate and manage relationships with third-party vendors and tools, making informed decisions about when to leverage external solutions.
   
   

Requirements


 * 6+ years in data engineering roles with progressively increasing responsibilities.
 * Deep understanding of data modeling, data architecture, and data integration best practices.
 * Strong hands-on experience with Apache Spark and cloud computing technologies.
 * Advanced proficiency in Python and SQL.
 * Familiarity with data governance, security, and privacy principles.
 * Comfort using collaboration tools such as GitHub or equivalent to manage development life cycle.
 * Excellent data modeling and engineering skills, and a talent for translating business objectives into technical solutions.
 * High energy, humble team player with “get it done” attitude, seeking collaboration with colleagues.
 * Ability to manage multiple projects simultaneously
 * Experience engineering in Databricks strongly preferred.
 * 3+ years of software engineering with python in a production environment.
 * Experience with the Azure cloud ecosystem.
 * Experience developing production-ready, real-time machine learning model serving pipelines.
 * Comfort developing in the Apache Spark Structured Streaming paradigm.
 * Experience working in the veterinary services industry, or a private equity-backed services company.
 * Working knowledge of Microsoft Excel and Office 365.","Mid-Senior level","https://www.linkedin.com/jobs/view/senior-data-engineer-at-pierce-3748512527?trk=public_jobs_topcard-title","United States","1 month ago","","","2023-10-25","","Technology, Information and Internet","Senior Data Engineer","Information Technology"
"Over 200 applicants","18267776","Mudflap","https://www.linkedin.com/company/mudflap?trk=public_jobs_topcard-org-name","Full-time","The Mudflap mobile app connects professional truck drivers with fuel stops in the $800B trucking industry. We help truckers save thousands of dollars on diesel fuel (their #1 business expense), while providing our truck stop partners with access to new, hard-to-reach customers.

We are extremely product and customer centric, and engineers play a key role in major company wide projects. Data Engineers strengthen the existing capabilities of our Data Warehouse and support transformation of the company to make data driven decisions. We use a modern, cloud based data warehouse as well as other modern data technology.

This is an opportunity to build at a rapid pace and continue creating a platform that hundreds of thousands of SMB trucking companies use to run their business - and love!

In this role, you will:


 * Assist in the development, testing, and deployment of data pipelines using SQL and/or Python
 * Contribute to and maintain documentation of data flows and schema design
 * Collaborate with senior developers to troubleshoot issues and optimize performance
 * Collaborate with business stakeholders to understand needs and collect feedback on solutions
 * Participate in code reviews to uphold high-quality standards
 * Provide support to end users in building reporting and analytics
 * Assist with the estimation of development tasks
 * Take ownership of your projects from beginning to end
 * Exhibit enthusiasm about sharing and adopting best practices with your team
 * Be flexible--you understand the needs of a startup
 * Be detail-oriented, a fast learner, and scrappy
   
   
   

You have:


 * 3-7 years of Data Engineering experience with a focus on large data sets
 * Experience with at least one cloud data platform (Snowflake, Redshift, BigQuery)
 * Advanced SQL knowledge and Python experience is a plus
 * Knowledge of Data Warehouse methodologies and modeling
 * Experience with source control tools such as Git, SVN, and TFS
   
   
   

Perks and Benefits (What we offer):


 * A flexible, remote-first company
 * A team of talented individuals on a mission to change a $1 trillion industry for the better
 * A high bar for quality and commitment to self-improvement
 * An open mind to new ideas and methodologies
 * Competitive salary and benefit options
 * Opportunities and support for massive career growth
   
   
   

The salary range for this role is $143,000 - $193,500. This information reflects a base salary range for this position based on current market data, which may be subject to change as new market data becomes available. The candidate's skills, experience, and other relevant factors will determine the exact compensation. This position may also be eligible for additional incentives such as equity awards or short-term incentives. Our benefits include medical/dental/vision insurance, 401(k) with company match, WFH stipend and PTO.

Company overview (Who we are):

Mudflap is well-funded by top-tier venture investors, including QED, Matrix, Commerce Ventures, NFX and 500 Startups, and our core team hails from Disney, Pandora, Meta/Facebook, Postmates, HomeAway (VRBO), Procore, DoorDash, Capital One, Uber, and Zillow.

Here are the core values that we believe in and look for in new teammates:


 * Put Customers First: We put our customers and partners at the center of everything we do.
 * Be Curious: Have a learning mindset. Ask, wonder, dig, investigate. Share the joy of discovery.
 * Sweat The Details: We create magical experiences by obsessing over every detail.
 * Do The Right Thing: We operate with integrity and exercise good judgment, even if no one is watching.
 * Find A Way: We push past roadblocks to get the best outcomes for our customers and our teammates.
 * Own It: Show up prepared, be present, and make it count.","Entry level","https://www.linkedin.com/jobs/view/data-engineer-at-mudflap-3767199474?trk=public_jobs_topcard-title","United States","2 weeks ago","","","2023-11-17","","Transportation/Trucking/Railroad","Data Engineer","Information Technology"
"103 applicants","157258","Care.com","https://www.linkedin.com/company/care-com?trk=public_jobs_topcard-org-name","Full-time","About Care.com

Care.com is a consumer tech company with heart. We're on a mission to solve a human challenge we all face: finding great care for the ones we love. We're moms and dads and pet parents. We have parents and grandparents, so we understand that everyone, at some point in their lives, could use a helping hand. Our culture and our products reflect that.

Here, entrepreneurs, self-starters, team players, and big thinkers unite behind a common cause. Here, we're applying data analytics, AI, and the latest technologies to solve universal problems and connect people in new ways. If you like having autonomy, if you thrive on collaboration and building new things, and if you're all about using your talent for good, Care.com is the place for you.

Office Locations: (This is a hybrid position)


 * NY, NY 10011
 * Austin, TX 78746
 * Shelton, CT 06484
   
   

What Your Days Will be Like:

The Data Engineer will be focusing on building out data feeds and tooling from our application platform and enable rapid ingestion into our centralized Data Lake/Data Warehouse. The Data Engineer will work across business areas and application teams at Care.com to rationalize data and design, build and maintain reusable data feeds which and ultimately empower analytics consumption at Care.com.

The ideal candidate will have professional experience building data pipelines in a technical environment. S/he will have an understanding of application development, data warehousing, demonstrate strong business judgment, and be able to prioritize in a fast-paced environment.

What You'll Be Working On:


 * Collaborate and partner with application teams to understand data collection/generation and design and partner to build and implement data feeds from our product tech stack
 * Design, develop, and build code for rapid feeds and ingestion into the Data Warehouse
 * Identify data sources used for building out data architecture diagrams/models
 * Establish engineering practices and setup frameworks for ""Data as a Service""
 * Collaborate with relevant delivery teams, including infrastructure, operations, site reliability engineering, product development, and others to perform evaluations, POCs, and ultimately implement and operationalize new technology.
 * Solve code level problems quickly and efficiently
 * Participate in demos and code reviews
 * Promote software best approach, standards, and processes
 * Shape development processes to promote a high-quality output while continuing to iterate quickly
 * Incorporate best practices for security, performance, and data privacy into data pipelines
   
   

What You'll Need to Succeed:


 * BS or MS in Computer Science or relevant engineering experience
 * 5+ years work experience in Data Engineering/data pipelines
 * 3+ years SQL experience is a must
 * 1+ years Unix/batch scripting preferred
 * 1+ years Python experience is a plus
 * 1+ years Windows server admin experience is a plus
 * Experience interfacing with business teams and turning requirements and vision into a technical reality
 * MySQL & Vertica Experience a plus/preferred
 * AWS experience is a plus
 * Ability to drive efforts from start to finish as a self-motivator
 * Knowledge in Data Warehousing is a MUST
 * Proven ability to maintain performance level in a fast-paced agile environment
 * Pragmatic and realistic with solutions
   
   

For a list of our Perks + Benefits, click here!

Care.com supports diverse families and communities and seeks employees who are just as diverse. As an equal opportunity employer, Care.com recognizes the power of a diverse and inclusive workforce and encourages applications from individuals with varied experiences, perspectives, and backgrounds. Care.com is committed to providing reasonable accommodations for qualified individuals with disabilities. If you need assistance or accommodation, please reach out to talent@care.com.

Company Overview:

Available in more than 20 countries, Care.com is the world's leading platform for finding and managing high-quality family care. Care.com is designed to meet the evolving needs of today's families and caregivers, offering everything from household tax and payroll services and customized corporate benefits packages covering the care needs of working families, to innovating new ways for caregivers to be paid and obtain professional benefits. Since 2007, families have relied on Care.com's industry-leading products—from child and elder care to pet care and home care. Care.com is an IAC company (NASDAQ: IAC).

Salary Range: 110,000 to 145,000. The base salary range above represents the anticipated low and high end of the national salary range for this position. Actual salaries may vary and may be above or below the range based on various factors including but not limited to work location, experience, and performance. The range listed is just one component of Care.com's total compensation package for employees. Other rewards may include annual bonuses and short- and long-term incentives. In addition, Care.com provides a variety of benefits to employees, including health insurance coverage, life, and disability insurance, a generous 401K employer matching program, paid holidays, and paid time off (PTO).","Entry level","https://www.linkedin.com/jobs/view/data-engineer-at-care-com-3743207672?trk=public_jobs_topcard-title","Austin, TX","1 month ago","","","2023-10-19","","Technology, Information and Internet","Data Engineer","Information Technology"
"Over 200 applicants","27054604","Otter Products","https://www.linkedin.com/company/otterproducts?trk=public_jobs_topcard-org-name","Full-time","Overview

Otter Products is currently recruiting for a Data Engineer to join our Data team! You can be based in our Fort Collins, CO office with flexibility to work a portion of your time remotely, or 100% Remote in the U.S.

The Data Engineer applies professional experience, concepts, and company objectives toward building and operationalizing the minimally inclusive data necessary for the enterprise data initiatives following industry standard practices and tools. The bulk of the Data Engineer's work would be in designing, managing and optimizing data pipelines, then moving these data pipelines effectively into production for key data and analytics consumers like business/data analysts, data scientists or other stakeholder that needs curated data for data and analytics use cases across the enterprise. The Data Engineer also ensures compliance with data governance and data security requirements while creating, improving and operationalizing these integrated and reusable data pipelines to enable faster data access, integrated data reuse and vastly improved time-to-solution for data and analytics initiatives. Additionally, the Data Engineer will be expected to collaborate with other data team members and data consumers working on the models and algorithms developed by them toward optimization for data quality, security, and governance to put them into production leading to potentially large productivity gains.

About Otter Products

Otter Products, we grow to give. From our founder's garage in 1998 to the global technology leader we are today, Otter Products continues to drive growth through innovation.

Through our industry-leading brands - OtterBox, Liviri and OtterCares - we provide our partners the number-one selling and most trusted products in our categories. Our philanthropic spirit is the foundation on which we foster our partner relationships, allowing us to grow and to give - together.

By way of our charitable arm, the OtterCares Foundation, we support our communities and invest in our future through education that inspires kids to change the world.

And even as our global community of Otters continues to grow, our founder's core values are still at the heart of everything we do. We measure our success by our ability to give back to our communities and strengthen opportunities for all.

For more information visit otterproducts.com

Responsibilities


 * Complete analysis, design and development of BI solutions using Azure SQL Server
 * Familiarity with Data Warehouse concepts
 * Experience coding the extraction, transformation, and loading (ETL) processes
 * Database development primarily in SSIS, Data Factory and SQL
 * Partner with the business to determine end user database/reporting needs and requirements
 * Generate ad hoc reports using Power BI or SSRS
 * Collaborate with other developers to create and implement best approach solution(s)
 * Troubleshoot Azure tools, systems, and software
 * Review queries for performance issues, making changes as needed
 * Collaborate with team to performance-tune Azure application as necessary
 * Assist with the analysis and extraction of relevant information from large amounts of historical business data to feed data science initiatives
 * Participate and support the design and documentation of processes for large scale data analyses, model development, model validation and model implementation
 * Support and maintain a positive safety culture by following all safety policies and procedures and actively contributing to a safe working environment
 * Other duties as required
   
   

Qualifications


 * Bachelor's degree required. Degree in Computer Science or Mathematics preferred
 * Minimum of four years of experience in an IT or analytical role required. Experience in database development, report writing and/or statistics preferred.
   
   

EEO

Otter Products, LLC is an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, age, marital status, pregnancy, sex, sexual orientation, gender, gender identity or expression, national origin, disability, veteran status, or any other characteristic or status protected by law.

For US Based Roles Only - Compensation Range Minimum

USD $90,000.00/Yr.

For US Based Roles Only - Compensation Range Maximum

USD $120,000.00/Yr.

Additional Total Rewards

Profit Sharing Program Eligible, Benefits Eligible - Full Time- check out otterproducts.com/careers/why for more info

","Entry level","https://www.linkedin.com/jobs/view/data-engineer-at-otter-products-3766728302?trk=public_jobs_topcard-title","Fort Collins, CO","2 weeks ago","","","2023-11-16","","Manufacturing","Data Engineer","Information Technology"
"Over 200 applicants","165755","Bosch USA","https://www.linkedin.com/company/boschusa?trk=public_jobs_topcard-org-name","Full-time","Company Description




As one of the largest North American automotive suppliers, Bosch develops Driver Assistance functions like Adaptive Cruise Control (ACC), Predictive Emergency Brake Systems (PEBS), Lane Departure Warning/Keeping Systems (LDW, LKS), Predictive Pedestrian Protection, Road Sign recognition, head light control, Advanced Parking Assistance, and many more.




We develop state of the art systems as well as advanced features leading to partly/highly automated driving; we have all of the necessary sensors in our portfolio (e.g. radar, camera, ultrasonic sensors, etc.) Join us to become part of the exciting and growing field of Driver Assistance.




Job Description




We are on the mission to turn latest technology into outstanding Bosch products and services. In our team we are developing the perception of the next generation of automatic parking systems. We are driving the development of the computer vision, ultrasonic sensor perception and creating the necessary SW to turn the sensor raw information into a vector space representation.




This vector space representation is the foundation for industry leading automatic parking functions. With our latest sensor generations, we have successfully introduced machine learning in our products. This was the first step on an exciting journey. There is a lot of opportunity ahead.




When you are the kind of person who combines a deep software engineering background with a we “can make it happen” attitude, let’s have a more detailed chat.




 * As a Senior Data Engineer you will develop and operate data pipelines delivering data from our engineering and the customer fleet to power our machine learning pipelines and provide data for decision making.
 * You will shape the future of automatic parking systems and establish best practices for embedded AI projects.
 * You will enable function developers to make use of your pipeline artifacts
 * As part of an agile team your ideas will be heard and impact the decision-making process. With our goal to invent for life, you will work on solutions that are both, innovative and ethical.
 * You will collaborate with scientists, electrical engineers, and machine learning engineers, ML Ops engineers to have real world impact.
 * Lifelong learning is crucial for long term success and we encourage you to stay current with latest research by visiting conferences and sharing your knowledge throughout the enterprise.




Qualifications




Basic Qualifications:




 * Education: Bachelor's or Master's degree in Computer Science, Electric Engineering, other Engineering discipline or foreign equivalent
 * 3+ years of experience with building data pipelines within a Cloud or Cloud-hybrid setup; in-depth understanding of relational database systems (e.g. Oracle, MS SQLServer).
 * 3+ years of experience with distributed computing frameworks (e.g. k8s, Spark)
 * 3+ years of experience in object-oriented software development, (e.g. Python, Java, or Go).
 * 2+ years of experience with Linux.




Preferred




 * Education: successfully completed Master's degree in Computer Science or other engineering discipline
 * Experience with recent non-relational storage technologies (NoSQL and distributed)
 * Experience with workflow automation tools (e.g. Jenkins, Ansible)
 * Experience with ECU SW re-simulation
 * Experience with various messaging systems (e.g. Kafka)
 * Experience in designing data models and choice of respective data formats
 * Experience with in-vehicle data collection skills, structured and analytical connectivity




Additional Information




All your information will be kept confidential according to EEO guidelines.




By choice, we are committed to a diverse workforce - EOE/Protected Veteran/Disabled.




BOSCH is a proud supporter of STEM (Science, Technology, Engineering & Mathematics) Initiatives




 * FIRST Robotics (For Inspiration and Recognition of Science and Technology)
 * AWIM (A World In Motion)","Mid-Senior level","https://www.linkedin.com/jobs/view/senior-data-engineer-xc-at-bosch-usa-3581292494?trk=public_jobs_topcard-title","Plymouth, MI","8 months ago","","","2023-03-29","","Automotive","Senior Data Engineer - XC","Engineering"
"Over 200 applicants","2671714","Information Resource Group","https://www.linkedin.com/company/information-resource-group?trk=public_jobs_topcard-org-name","Full-time","Title: AWS Redshift Data Engineer




Location: 100% Remote




Duration: 12+ Months




Job Overview




We are seeking a talented and experienced AWS Redshift Data Engineer / Consultant to join our team in designing, developing, and optimizing data pipelines and ETL processes for our AWS Redshift-based data lake house. In this role, you will collaborate closely with cross-functional teams, leveraging your expertise in SQL, Redshift stored procedures, AWS DMS, Airflow, Python scripting and other pertinent AWS services to ensure the seamless ingestion, integration, transformation and orchestration of data. Your experience with complex ETL pipelines, Changed Data Capture (CDC), Slowly Changing Dimension (SCD) strategies will be instrumental in creating a scalable, high-performance data environment. By adhering to best practices and industry standards, you will collaborate with our engineering and data teams to design forward thinking solutions.




Key Responsibilities




 * Collaborate with data engineering and development teams to design, develop, test, and maintain robust and scalable ELT/ETL pipelines using SQL scripts, Redshift stored procedures, and other AWS tools and services.
 * Collaborate with our engineering and data teams to understand business requirements and data integration needs, translate them into effective data solutions, that yield top-quality outcomes.
 * Architect, implement, and manage end-to-end data pipelines, ensuring data accuracy, reliability, data quality, performance, and timeliness.
 * Employ AWS DMS and other services for efficient data ingestion from on-premises databases into Redshift.
 * Design and implement ETL processes, encompassing Changed Data Capture (CDC) and Slow Changing Dimension (SCD) logics, to seamlessly integrating data from diverse source systems.
 * Provide expertise in Redshift database optimization, performance tuning, and query optimization.
 * Design and implement efficient orchestration workflows using Airflow, ensuring seamless coordination of complex ETL processes.
 * Integrate Redshift with other AWS services, such as AWS DMS, AWS Glue, AWS Lambda, Amazon S3, Airflow, and more, to build end-to-end data pipelines.
 * Perform data profiling and analysis to troubleshoot data-related challenges / issues and build solutions to address those concerns.
 * Proactively identify opportunities to automate tasks and develop reusable frameworks.
 * Work closely with version control team to maintain a well-organized and documented repository of codes, scripts, and configurations using Git.
 * Provide technical guidance and mentorship to fellow developers, sharing insights into best practices, tips, and techniques for optimizing Redshift-based data solutions.




Qualifications And Skills




 * Bachelor's or Master's degree in Computer Science, Information Technology, or a related field.
 * Extensive hands-on experience designing, developing, and maintaining data pipelines and ETL processes on AWS Redshift, including data lakes and data warehouses.
 * Proficiency in SQL programming and Redshift stored procedures for efficient data manipulation and transformation.
 * Hands-on experience with AWS services such as AWS DMS, Amazon S3, AWS Glue, Redshift, Airflow, and other pertinent data technologies.
 * Strong understanding of ETL best practices, data integration, data modeling, and data transformation.
 * Experience with complex ETL scenarios, such as CDC and SCD logics, and integrating data from multiple source systems.
 * Demonstrated expertise in AWS DMS for seamless ingestion from on-prem databases to AWS cloud.
 * Proficiency in Python programming with a focus on developing efficient Airflow DAGs and operators.
 * Experience in converting Oracle scripts and Stored Procedures to Redshift equivalents.
 * Familiarity with version control systems, particularly Git, for maintaining a structured code repository.
 * Proficiency in identifying and resolving performance bottleneck and fine-tuning Redshift queries,
 * Strong coding and problem-solving skills, and attention to detail in data quality and accuracy.
 * Ability to work collaboratively in a fast-paced, agile environment and effectively communicate technical concepts to non-technical stakeholders.
 * Proven track record of delivering high-quality data solutions within designated timelines.
 * Experience working with large-scale, high-volume data environments.
 * The ideal candidate possesses several years of hands-on experience working with Redshift and other AWS services and a proven track record of delivering high-performing, scalable data platforms and solutions within the AWS cloud.
 * AWS certifications related to data engineering or databases are a plus.","Entry level","https://www.linkedin.com/jobs/view/aws-redshift-data-engineer-100%25-remote-at-information-resource-group-3726807454?trk=public_jobs_topcard-title","United States","2 months ago","","","2023-09-25","","IT Services and IT Consulting","AWS Redshift Data Engineer - 100% REMOTE","Information Technology"
"187 applicants","2010798","Tiger Analytics","https://www.linkedin.com/company/tiger-analytics?trk=public_jobs_topcard-org-name","Full-time","Tiger Analytics is a fast-growing advanced analytics consulting firm. Our consultants bring deep expertise in Data Science, Machine Learning and AI. We are the trusted analytics partner for multiple Fortune 500 companies, enabling them to generate business value from data. Our business value and leadership has been recognized by various market research firms, including Forrester and Gartner. We are looking for top-notch talent as we continue to build the best global analytics consulting team in the world.




The Big Data Azure Engineer will be responsible for architecting, designing, and implementing advanced analytics capabilities. These capabilities include batch and streaming analytics, machine learning models, natural language generation, and other emerging technologies in the field of advanced analytics.




Requirements




 * Bachelor’s degree in Computer Science or similar field
 * 4+ years of experience in traditional and modern Big Data technologies (HDFS, Hadoop, Hive, Pig, Sqoop, Kafka, Apache Spark, hBase, Oozie, No SQL databases)
 * Experience in Java/Python/Scala
 * Experience extracting/querying/joining large data sets at scale
 * Experience building data platforms using Azure stack
 * Experience building data ingestion pipelines using Azure Data Factory to ingest structured and unstructured data
 * Strong knowledge on Azure Storage schematics such as Gen1 and Gen2
 * Experience in harmonizing raw data into a consumer-friendly format using Azure Databricks
 * Knowledge of Azure networking, security, key vaults, etc.
 * Experience in data wrangling, advanced analytic modeling, and AI/ML capabilities is preferred
 * Experience utilizing Snowflake to build data marts with the data residing in Azure storage is a plus
 * Strong communication and organizational skills




Benefits




This position offers an excellent opportunity for significant career development in a fast-growing and challenging entrepreneurial environment with a high degree of individual responsibility.","Associate","https://www.linkedin.com/jobs/view/big-data-engineer-azure-at-tiger-analytics-3590302059?trk=public_jobs_topcard-title","Chicago, IL","8 months ago","","","2023-04-03","","Technology, Information and Internet","Big Data Engineer (Azure)","Information Technology"
"58 applicants","1683442","Lorven Technologies Inc.","https://www.linkedin.com/company/lorventech?trk=public_jobs_topcard-org-name","Full-time","Job Title: Data Engineer(Redshift)

Job Location: Los Angeles, CA - Onsite

Duration : Fulltime / 12 Months Contract

Job Description

Need Strong experience in below skill sets.

 * Informatica
 * Python
 * SQL
 * Redshift","Mid-Senior level","https://www.linkedin.com/jobs/view/data-engineer-redshift-fulltime-12-months-contract-los-angeles-ca-onsite-at-lorven-technologies-inc-3681083408?trk=public_jobs_topcard-title","United States","4 months ago","","","2023-07-28","","IT Services and IT Consulting","Data Engineer (Redshift) -- Fulltime / 12 Months Contract -- Los Angeles, CA - Onsite","Information Technology"
"76 applicants","31349149","eStaffing Inc.","https://www.linkedin.com/company/estaffinginc?trk=public_jobs_topcard-org-name","Full-time","Title: Data Engineer

Duration: C2H for 6months

Location: Remote

Client: Health care services

Requirement

Job Description

Must-Haves

 * Experience working with Kafka, Spark Streams, Snowflake, Grafana, Prometheus, and AWS services (S3, Kinesis, TimeStream, Redshift, CloudWatch, EKS)
 * Experience working with HL7 data
 * Experience coding with Python/Java
 * Strong SQL, Data Warehousing, and Data Lake fundamentals
 * Hands-on experience with Linux (RHEL/Debian) operating system
 * Knowledge of version control systems such as Git
 * Experience consuming and building APIs
 * Experience utilizing Agile methodology for development
 * Experience evaluating and implementing technologies in a production capacity
 * Experience building a streaming data platform from ground up
 * Ability to manage one's tasks and work with stakeholders to understand the requirements as well provide solutions.","Entry level","https://www.linkedin.com/jobs/view/data-engineer-remote-100%25-at-estaffing-inc-3617898159?trk=public_jobs_topcard-title","United States","7 months ago","","","2023-05-02","","Staffing and Recruiting","Data Engineer | REMOTE 100%","Other"
"Over 200 applicants","163102","Clear Capital","https://www.linkedin.com/company/clearcapital?trk=public_jobs_topcard-org-name","Full-time","Clear Capital is building the future of real estate data, and we need your help! We are seeking experienced product builders: with your talent as a Senior Data Engineer, help us reach our goals of knowing more about a property than anybody else and in the process making home ownership valuations more fair and equitable for millions of people.

Become part of an innovative team supporting and developing the data and machine learning products that will shape Clear Capital’s future. The Senior Data Engineer role at Clear Capital will work closely with product teams to build next generation data products. Working alongside Software Engineers, Data Quality Analysts, ML Engineers, Data Scientists, and ML Ops Engineers who, like you, are dedicated to build exceptional software.

We are looking for a Senior Data Engineer to assist with the development and implementation of systems leveraging structured and unstructured data to deliver data and data science solutions at scale. As Senior Data Engineer at Clear Capital, you are committed to enabling the best work of others on the team. You help yourself and your team to consistently “level up.” You think ahead to anticipate the needs of others and provide concise information for decision-making.

What You Can Expect

Competitive compensation and immediate contribution!

Inclusive benefits package offerings 401k plans and customizable benefits including dental, vision, medical, etc. for you and your dependents.

An innovative culture that understands the importance of quality of work over quantity.

Company supported and employee-driven ambassador groups that promote diversity, working on a hybrid schedule and philanthropy.

Learning and development programs to help advance your career and personal growth.

What We Value

Wherever it leads, Whatever it takes! We believe in making the impossible possible!

Thrive personally, grow professionally―be happy!

Innovate, learn, lead- Knowledge and growth is never ending!

We believe in hiring nice people because anything is possible when you have the team's support.

Improving the lives around us- A smile could change the entire world.

Be the most trusted, respected, and loved real estate valuation company in the world.

About Us

Clear Capital is a national real estate valuation technology company with a simple purpose: build confidence in real estate decisions to strengthen communities and improve lives. Our goal is to provide customers with a complete understanding of every U.S. property through our field valuation services and analytics tools, and improve their workflows with our platform technologies. Our commitment to excellence — wherever it leads, whatever it takes® — is embodied by team members.

Clear Capital is an equal opportunity employer.

To all recruitment agencies: Clear Capital does not accept agency resumes. Please do not forward resumes to our jobs alias, Clear Capital employees, or any other company location. Clear Capital is not responsible for any fees related to unsolicited resumes.","Mid-Senior level","https://www.linkedin.com/jobs/view/sr-data-engineer-at-clear-capital-3756673393?trk=public_jobs_topcard-title","United States","2 weeks ago","","","2023-11-19","","Financial Services","Sr. Data Engineer","Information Technology"
"83 applicants","41999","Catalist","https://www.linkedin.com/company/catalist?trk=public_jobs_topcard-org-name","Full-time","For over 17 years, Catalist has been a leader in civic data and data science innovation. Our mission is to provide progressive organizations with the data, software, and services needed to better identify, understand, and communicate with the people they need to engage and mobilize. Our clients include the largest, most influential organizations in the U.S. active in civic engagement, advocacy, and political campaigns.

Catalist is home to a dedicated, creative team of technologists, data scientists, and campaign experts committed to using our talents and technology to nurture a vibrant and growing progressive community.

As a Data Engineer at Catalist, you will work closely with the Analytics and Technology departments to design, build, support, and maintain various data pipelines and processes with an end goal of providing data and intelligence to the progressive community. This role primarily involves translating data architecture designs into functional processes, code, and systems.

The ideal candidate will be a highly motivated individual with excellent technical skills, a strong desire to learn new skills, and an interest in progressive politics. Catalist values creativity and problem-solving. Our work is on the cutting edge of data-driven politics, and your support will help Democratic candidates and progressive organizations conduct successful advocacy and electoral campaigns.

This position reports to the Deputy Chief Data Officer. The Data Engineer is a part of a growing Data team that supports all underlying work at Catalist.

Principle Duties & Responsibilities


 * Develop scalable, production processes, code, and systems for data ingest, transformation, modeling, and reporting across a variety of platforms
 * Translate mock-ups and designs into functional processes, code, and systems
 * Create architecture designs when needed
 * Provide quality assurance and testing on processes, code, systems, and products
 * Become an internal subject matter expert on various datasets and support other Catalist departments/teams on usage of those datasets
 * Execute ad-hoc data and database maintenance tasks as requiredProject manage cross-departmental efforts, with direct responsibility for stakeholder engagement, management, and execution of technical elements
   
   
   

Requirements


 * BS or BA in a technical field, or relevant experience
 * 1-2 years of experience working with SQL databases
 * Experience with data cleaning or ETL processes
 * Experience with distributed computing systems and/or distributed datastores (particularly the Hadoop ecosystem)
 * Experience managing projects
 * Familiarity with Catalist data, progressive politics, voter files, and/or commercial data
 * Background check required
   
   
   

Preferred Skills & Abilities


 * Willingness to be a problem solver and produce results in a fast paced environment
 * Ability to be creative and personable, and articulate ideas clearly
 * Experience working with SQL databases
 * Proficiency with Python or another object-oriented programming language (R, Java, Scala, etc…)
 * Experience working in cloud environments (AWS, GCP, etc.)
 * Experience working in command line environments such as Bash
 * Experience with a version control tool such as git or github
   
   
   

Benefits

Medical, Dental, Vision, Prescription Drug

Catalist offers Medical, Dental, Vision, and Prescription Drug coverage for eligible staff and their eligible dependents. Catalist’s Medical plan is a comprehensive PPO program including Prescription Drug coverage with 85% of the premium paid by Catalist. Dental and Vision coverage is provided at no cost to employees.

Group Term Life Insurance and Long-Term & Short-Term Disability Coverage

Group Term Life Insurance and Long-Term and Short-Term Disability coverage is available for eligible staff. These benefits are provided at no cost to Catalist employees.

401(k) Safe Harbor Plan

A 401(k) Safe Harbor Plan is available to eligible staff with a 3% contribution from Catalist from the date of hire. Employees may contribute pre-tax or post-tax from their salary up to the legal limits set forth by the IRS.

Medical and Dependent Care Flexible Spending Accounts (FSAs)

Catalist offers an FSA Program that gives eligible staff the ability to pay out-of-pocket medical/dental/vision/child care expenses from pre-tax earnings.

Transit Benefits

Catalist also makes available a Transit benefit FSA program to eligible employees using pre-tax contributions with a company match.

Professional Development and Remote Work Expenses

Eligible employees may be reimbursed up to $750 each year for professional development / education and remote work expenses.

Student Loan PayDown or SaveUp

Catalist offers a Student Loan PayDown and College SaveUp benefit for eligible staff.

Vacation, Personal Leave, Sick Leave Benefits

Catalist offers generous vacation benefits to all eligible staff. Eligible employees also receive:


 * 14 Paid Holidays
 * Personal Days
 * Sick Leave
 * Parental Leave
   
   
   

Hybrid Office/Remote Work

Certain positions at Catalist are eligible for Office/Remote Hybrid or full Remote status.","Not Applicable","https://www.linkedin.com/jobs/view/data-engineer-at-catalist-3744951279?trk=public_jobs_topcard-title","Washington, DC","1 month ago","","","2023-10-27","$64,000.00-$74,000.00","Political Organizations","Data Engineer","Information Technology"
"Over 200 applicants","13658482","EPSoft","https://www.linkedin.com/company/epsoft-co?trk=public_jobs_topcard-org-name","Full-time","Requirements




 * Data Engineer with Integration (ETL/Informatica), Database (SQL Server/Oracle) and Automation (API, Python scripting etc.)experience
 * Experienced in large-scale application development testing Cloud/ On Prem Data warehouse, Data Lake, Data science
 * Experience with multi-year, large-scale projects
 * Expert technical skills with hands-on testing experience using SQL queries.
 * Extensive experience with both data migration and data transformation testing
 * Extensive experience DBMS like Oracle, Teradata, SQL Server, DB2, Redshift, Postgres and Sybase.
 * Extensive testing Experience with SQL/Unix/Linux.
 * Extensive experience using Python scripting and Cloud Technologies.
 * API/RESTAssured automation, building reusable frameworks, and good technical expertise/acumen
 * Java/Java Script - Implement core Java, Integration, Core Java and API.
 * Functional/UI/ Selenium - BDD/Cucumber, Specflow, Data Validation/Kafka, Big Data, also automation experience using Cypress.
 * API/Rest API - Rest API and Micro Services using JSON, SoapUI.
 * Extensive experience in Map reduce using tools like Hadoop, Hive, Pig, Kafka, S4, Map R.
 * Experience in testing storage tools like S3, HDFS
 * Experience with one or more industry-standard defect or Test Case management Tools
 * Great communication skills (regularly interacts with cross functional team members)","Mid-Senior level","https://www.linkedin.com/jobs/view/data-engineer-at-epsoft-3689428796?trk=public_jobs_topcard-title","United States","3 months ago","","","2023-08-11","","Software Development","Data Engineer","Information Technology"
"165 applicants","1441","Google","https://www.linkedin.com/company/google?trk=public_jobs_topcard-org-name","Full-time","Note: Google’s hybrid workplace includes remote roles. By applying to this position you will have an opportunity to share your preferred working location from the following:

Remote locations: Indiana, USA; Michigan, USA; Ohio, USA; Pennsylvania, USA.Minimum qualifications:


 * Bachelor's degree in Electrical Engineering, Power Engineering, a related technical field, or equivalent practical experience.
 * 5 years of experience in mission critical facility design and construction.
 * Experience in design, construction, and commissioning of medium or low voltage electrical distribution systems, AC/DC systems, and associated power management or SCADA tools.
   
   

Preferred qualifications:


 * Professional Engineering (PE) license.
 * Experience in mission critical facilities and their electrical/mechanical infrastructure.
 * Experience in Estimating, Electrical design, Operation and Commissioning of substations, switchgear, ATP/ATS, emergency power systems and their control systems, power monitoring, and electrical protection.
   
   

About The Job

Our thirst for technology is a part of everything we do. The Data Center Engineering team takes the physical design of our data centers into the future. Our lab mirrors a research and development department -- cutting-edge strategies are born, tested and tested again. Along with a team of great minds, you take on complex topics like how we use power or how to run state-of-the-art, environmentally-friendly facilities. You're a visionary who optimizes for efficiencies and never stops seeking improvements -- even small changes that can make a huge impact. You generate ideas, communicate recommendations to senior-level executives and drive implementation alongside facilities technicians.

With your technical expertise, you ensure compliance with codes and standards, develop infrastructure improvements and serve as an expert in your specialty (e.g., cooling, electrical).

In this role, you will work with electrical and mechanical engineers, provide project design and field engineering services, project implementation, and participation in all phases of a project life-cycle. You will act as the interface between internal customers and the project team. You will be Involved in all the site capital projects from construction to modification of existing infrastructures in participation and preparation of all types of documents including, Statement of Work (SOW), Total Cost of Ownership (TCO) analysis, drawing markups, budget, schedule, final startup/commissioning reports, and review and acceptance of as-builts and review of submittals.

Behind everything our users see online is the architecture built by the Technical Infrastructure team to keep it running. From developing and maintaining our data centers to building the next generation of Google platforms, we make Google's product portfolio possible. We're proud to be our engineers' engineers and love voiding warranties by taking things apart so we can rebuild them. We keep our networks up and running, ensuring our users have the best and fastest experience possible.

The US base salary range for this full-time position is $136,000-$203,000 + bonus + equity + benefits. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process.

Please note that the compensation details listed in US role postings reflect the base salary only, and do not include bonus, equity, or benefits. Learn more about benefits at Google .

Responsibilities


 * Provide technical support to Data Center Services and Operations teams to define electrical system design requirements for multiple data center projects from inception through completion.
 * Develop, implement, and manage the data center electrical designs at site starting from basis of design to issued for construction data center services documents for new data center projects build outs, infrastructure upgrades, and renovations.
 * Respond to site specific engineering Requests For Information (RFI) in coordination with the Engineer of Record (EOR).
 * Collaborate with the core Engineering team to provide site specific requirements during the development of specific Basis of Design (BOD) and coordinate system level schematics with EOR.
 * Own and manage site-level power system issues during the project execution phase. Identify and resolve issues with cross-functional teams, maintain all data center related electrical system design requirements and interface documents.
   
   
   

Google is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing our Accommodations for Applicants form .","Not Applicable","https://www.linkedin.com/jobs/view/data-center-electrical-engineer-google-data-centers-at-google-3760580556?trk=public_jobs_topcard-title","Pennsylvania, United States","3 days ago","","","2023-12-01","","Information Services and Technology, Information and Internet","Data Center Electrical Engineer, Google Data Centers","Information Technology and Engineering"
"74 applicants","1441","Google","https://www.linkedin.com/company/google?trk=public_jobs_topcard-org-name","Full-time","Note: Google’s hybrid workplace includes remote roles. By applying to this position you will have an opportunity to share your preferred working location from the following:

Remote locations: Indiana, USA; Michigan, USA; Ohio, USA; Pennsylvania, USA.Minimum qualifications:


 * Bachelor's degree in Electrical Engineering, Power Engineering, a related technical field, or equivalent practical experience.
 * 5 years of experience in mission critical facility design and construction.
 * Experience in design, construction, and commissioning of medium or low voltage electrical distribution systems, AC/DC systems, and associated power management or SCADA tools.
   
   

Preferred qualifications:


 * Professional Engineering (PE) license.
 * Experience in mission critical facilities and their electrical/mechanical infrastructure.
 * Experience in Estimating, Electrical design, Operation and Commissioning of substations, switchgear, ATP/ATS, emergency power systems and their control systems, power monitoring, and electrical protection.
   
   

About The Job

Our thirst for technology is a part of everything we do. The Data Center Engineering team takes the physical design of our data centers into the future. Our lab mirrors a research and development department -- cutting-edge strategies are born, tested and tested again. Along with a team of great minds, you take on complex topics like how we use power or how to run state-of-the-art, environmentally-friendly facilities. You're a visionary who optimizes for efficiencies and never stops seeking improvements -- even small changes that can make a huge impact. You generate ideas, communicate recommendations to senior-level executives and drive implementation alongside facilities technicians.

With your technical expertise, you ensure compliance with codes and standards, develop infrastructure improvements and serve as an expert in your specialty (e.g., cooling, electrical).

In this role, you will work with electrical and mechanical engineers, provide project design and field engineering services, project implementation, and participation in all phases of a project life-cycle. You will act as the interface between internal customers and the project team. You will be Involved in all the site capital projects from construction to modification of existing infrastructures in participation and preparation of all types of documents including, Statement of Work (SOW), Total Cost of Ownership (TCO) analysis, drawing markups, budget, schedule, final startup/commissioning reports, and review and acceptance of as-builts and review of submittals.

Behind everything our users see online is the architecture built by the Technical Infrastructure team to keep it running. From developing and maintaining our data centers to building the next generation of Google platforms, we make Google's product portfolio possible. We're proud to be our engineers' engineers and love voiding warranties by taking things apart so we can rebuild them. We keep our networks up and running, ensuring our users have the best and fastest experience possible.

The US base salary range for this full-time position is $136,000-$203,000 + bonus + equity + benefits. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process.

Please note that the compensation details listed in US role postings reflect the base salary only, and do not include bonus, equity, or benefits. Learn more about benefits at Google .

Responsibilities


 * Provide technical support to Data Center Services and Operations teams to define electrical system design requirements for multiple data center projects from inception through completion.
 * Develop, implement, and manage the data center electrical designs at site starting from basis of design to issued for construction data center services documents for new data center projects build outs, infrastructure upgrades, and renovations.
 * Respond to site specific engineering Requests For Information (RFI) in coordination with the Engineer of Record (EOR).
 * Collaborate with the core Engineering team to provide site specific requirements during the development of specific Basis of Design (BOD) and coordinate system level schematics with EOR.
 * Own and manage site-level power system issues during the project execution phase. Identify and resolve issues with cross-functional teams, maintain all data center related electrical system design requirements and interface documents.
   
   
   

Google is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing our Accommodations for Applicants form .","Not Applicable","https://www.linkedin.com/jobs/view/data-center-electrical-engineer-google-data-centers-at-google-3760579276?trk=public_jobs_topcard-title","Indiana, United States","3 days ago","","","2023-12-01","","Information Services and Technology, Information and Internet","Data Center Electrical Engineer, Google Data Centers","Information Technology and Engineering"
"116 applicants","16162082","Kin Insurance","https://www.linkedin.com/company/kin-insurance?trk=public_jobs_topcard-org-name","Full-time","The world has changed. Why hasn’t insurance?

Kin’s mission is to reimagine home insurance For Every New Normal. While other insurers struggle to handle a fast-changing world, Kin is built for the future and is prepared to meet its challenges head on while helping our customers do the same.

Kin is proud to be one of BuiltIn Chicago’s 2021 and 2022 Best Mid Sized Companies to work for, and Forbes 2021 Best Startup Employers in North America. Simply put, our people are what make us great, and we need forward-thinking, inspired game-changers like you to join us in our mission.

So, what’s the role?

Data is central to Kin’s operations and success. As a senior data engineer, you will be part of a data management team that supports and enables our product, operations, analytics, and data science teams, amongst others. As we scale, you will be integral in how we manage, structure, and store our data, as well as develop new solutions related to data architecture and ETL pipelines.

A day in the life could include:


 * Creating, designing, and maintaining ETL pipelines
 * Working with data science and BI teams to create data sets to be used in various projects
 * Taking on more advanced design and architectural decisions in the lifecycle of our data processing
 * Participating in recurring scrum events
 * Collaborating with cross-functional team members
 * Providing subject matter expertise and support
   
   

I’ve got the skills… but do I have the necessary ones?


 * 5+ years of data engineering and/or dba experience
 * Experience with the entire ETL pipeline: Data Integration tools, Databases, Big Data Platforms, and cloud based data platforms.
 * Exposure to and support of data visualization tools, such as Looker, Tableau and Microstrategy.
 * Experience in building from the ground up a modern next generation data warehouse platform.
 * Advanced proficiency with SQL or PL/SQL optimization and development.
 * Experience working with AWS-based Data Platforms is preferred.
 * Deep understanding of data architecture as it relates to business goals and objectives
 * Strong scripting skills in one or more of the following: Bash, Python, and/or Ruby
   
   

Oh, and don’t worry, we’ve got you covered!


 * Medical, Dental, Vision, Disability and Life Insurance
 * Flexible PTO policy
 * Remote work
 * Generous equity package
 * 401K with company match
 * Parental leave
 * Continuing education and professional development
 * The excitement of joining a high-growth Insurtech company and seeing your work make an impact
   
   

About Kin

In an industry that hasn't budged in more than 100 years, our technology transforms the user experience, cuts inefficiencies that waste billions of consumer dollars, and customizes coverage homeowners want. We believe insurance was always meant to be a digital product – we’re making that a reality.

Our approach to the industry makes us unique, and the people at Kin help us excel. We’re a team of problem solvers, collaborators, builders, and dreamers who are passionate about creating positive change in the lives of our customers and in our industry. Kin is more than just our name – it’s how we treat each other. That’s one of the many reasons we’ve been recognized as a great place to work by Built In, Forbes, and Fast Company.

EEOC Statement

Kin is proud to be an Equal Employment Opportunity and Affirmative Action Employer. We don't just accept difference – we honor it, nurture it, and celebrate it. We don’t discriminate based on race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.

Kin encourages applications from all backgrounds, communities and industries, and are committed to having a team that is made up of diverse skills, experiences and abilities.

Remote","Mid-Senior level","https://www.linkedin.com/jobs/view/senior-data-engineer-at-kin-insurance-3764349895?trk=public_jobs_topcard-title","United States","1 week ago","","","2023-11-22","","Insurance","Senior Data Engineer","Information Technology"
"Over 200 applicants","22337144","Alto Pharmacy","https://www.linkedin.com/company/altopharmacy?trk=public_jobs_topcard-org-name","Full-time","Alto Pharmacy is a full-service, digitally-powered pharmacy that makes it simple to live your healthiest life by providing an easier, more supportive and more affordable pharmacy experience. We’re redefining what a pharmacy can do, with fast and reliable prescription delivery, tools like treatment reminders and medication bundling, direct access to care specialists, and support with insurance and cost savings. By focusing on the person behind the prescription, our model boosts adherence, improves health outcomes, and keeps our customers returning month after month. Learn more at www.alto.com.

The Data Engineering team at Alto owns the platform and infrastructure that enables data driven decision making, powers innovative pharmacy specific machine learning applications, and provides a high quality experience for internal users (analysts, data scientists, business partners) and external customers (other businesses Alto partners with to realize our mission to improve the quality of life for our patients).

Accelerate Your Career as You


 * Develop the next generation of our analytics and reporting platform to enable low latency insights
 * Refine our dimensional models and improve data self service
 * Partner with data scientists to improve the usability and performance of Kubeflow, our ML platform
 * Partner with analysts to improve the usability and performance of our modeling pipelines
   
   

Minimum Qualifications

A Bit About You


 * 3+ years of data engineering experience
 * Strong technical proficiency with Python and SQL
 * Experience with data modeling and dimensionalization best practices
 * Experience with ETL pipelines, data analysis, BI tools, and cloud services
 * Strong sense of ownership and pride in your work
 * Comfortable working at startup pace and focus
   
   

Additional Physical Job Requirements


 * Read English, comprehend, and follow simple oral and written instructions. The worker is required to have close visual acuity to perform an activity such as: preparing and analyzing data and figures; transcribing; viewing a computer terminal; extensive reading. Assessing the accuracy, neatness and thoroughness of the work assigned.
 * Communicating with others to exchange information. Expressing or exchanging ideas by means of the spoken word; those activities where detailed or important spoken instructions must be conveyed to other workers accurately, loudly, or quickly.
 * Perceiving the nature of sounds at normal speaking levels with or without correction, and having the ability to receive detailed information through oral communication, and making fine discriminations in sound.
 * Frequent repeating motions required to operate a computer that may include the wrists, hands and/or fingers.
 * Sedentary work: Sitting most of the time, exerting up to 10 pounds of force occasionally and/or a negligible amount of force frequently or constantly to lift, carry, push, pull or otherwise move objects, including the human body. Walking and standing are required only occasionally.
   
   

Salary And Benefits

Salary Range: $145,000 - $165,000

Commission Eligible: No

Equity Eligible: Yes

Travel: Yes. Up to 15% of the time.

Location Requirement: Employment at Alto is limited to individuals residing in the following states: Washington, California, Nevada, Colorado, Texas, and New York.

Benefits: Full-time: Medical, Dental, Vision, 401(k), Group Life, AD&D, Employer paid STD/LTD, generous PTO and parental leave.

Alto Pharmacy is an Equal Opportunity Employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, gender, sexual orientation, national origin, genetic information, age, disability, veteran status, or any other legally protected basis.

Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records. We are an E-Verify company.

To learn about Alto's privacy practices including compliance with applicable privacy laws, please click here

","Associate","https://www.linkedin.com/jobs/view/data-engineer-at-alto-pharmacy-3756084142?trk=public_jobs_topcard-title","United States","3 days ago","","","2023-12-01","","Hospitals and Health Care","Data Engineer","Information Technology"
"Over 200 applicants","79809662","NBC Sports Next","https://www.linkedin.com/company/nbc-sports-next?trk=public_jobs_topcard-org-name","Full-time","Company Description

We create world-class content, which we distribute across our portfolio of film, television, and streaming, and bring to life through our theme parks and consumer experiences. We own and operate leading entertainment and news brands, including NBC, NBC News, MSNBC, CNBC, NBC Sports, Telemundo, NBC Local Stations, Bravo, USA Network, and Peacock, our premium ad-supported streaming service. We produce and distribute premier filmed entertainment and programming through Universal Filmed Entertainment Group and Universal Studio Group, and have world-renowned theme parks and attractions through Universal Destinations & Experiences. NBCUniversal is a subsidiary of Comcast Corporation.

Here you can be your authentic self. As a company uniquely positioned to educate, entertain and empower through our platforms, Comcast NBCUniversal stands for including everyone. Our Diversity, Equity and Inclusion initiatives, coupled with our Corporate Social Responsibility work, is informed by our employees, audiences, park guests and the communities in which we live. We strive to foster a diverse, equitable and inclusive culture where our employees feel supported, embraced and heard. Together, we’ll continue to create and deliver content that reflects the current and ever-changing face of the world.

Come join us as we work together as one team to innovate and deliver what’s Next.

NBC Sports Next is where sports and technology intersect. We’re a subdivision of NBC Sports and home to all NBCUniversal digital applications in sports and technology within our three groups: Youth & Recreational Sports and Golf.

At NBC Sports Next we’re fueled by our mission to innovate, create larger-than-life events and connect with sports fans through technology that provides the ultimate in immersive experiences.

Golf fuses the team behind products and services like GolfNow, TeeOff and GolfPass, which better connects golfers and golf facilities around the world through innovative solutions like cloud-based golf course management and SmartPlay contactless technology and services that create optimum golfing experiences.

Job Description

GolfNow has an exciting opportunity for an experienced Data Engineer. Working alongside the Data Engineering Team, you'll manage the full lifecycle of our data warehousing needs. You'll read and write complex queries, demonstrate the ability to create database objects (tables, views, stored procedures, user-defined functions), and create and maintain ETL pipelines. Our data warehouse and data operations are built on top of Microsoft and AWS technologies including MS SQL Server, SSIS, PowerShell, and other AWS cloud technologies. To perform this job successfully, an individual would need to be able to understand complex business processes, gather requirements, work efficiently, and verify their results.

Responsibilities Include But Are Not Limited To


 * Work within a small team of passionate data engineers and data scientists.
 * Compile user requirements and specifications for reports.
 * Contribute to the management of the day-to-day operations of running our Data Warehouse.
 * Build, analyze, and manage reports and dashboards for business stakeholders.
 * Respond to users to troubleshoot and/or improve existing reports.
 * Collaborate with internal QA on customer acceptance testing.
 * Develop SQL scripts and objects to support reporting functionality and performance.
 * Build data pipelines and ETLs for loading source system data into the data warehouse for further reporting and analysis.
 * Assist in building scalable data models to support reporting and tracking of key business and product metrics.
 * Help identify better practices, tools, and relevant trends that can positively influence the data operations across the business.
 * Other duties may be assigned as needed by management.
 * Strong ability to actively engage in a remote workspace - including, but not limited to, virtual team meetings, connection via team channels (Teams, Slack and Outlook) and ad hoc meetings as requested.
 * Experience working through complex issues to completion
 * Other duties may be assigned as needed by management.
   
   

Qualifications

All candidates must meet the qualifications below:


 * A minimum of 2+ years of data engineering experience is required
 * Bachelor’s Degree in Computer Science or related field/relevant industry experience in data engineering
 * Working experience developing and refactoring SQL Stored Procedures
 * A minimum of 1 years working experience with Python
 * Solid experience and knowledge of T-SQL and Microsoft SQL Server Database Platforms
 * 1-2 years experience with AWS cloud environment
 * Experience with AWS ETL tools such as Glue and Lambda
 * Experience with Apache Airflow
 * Experience using source control with Github or Team Foundation Server.
 * Experience with modeling data structures in both transactional and analytical platforms.
 * Experience with PowerShell scripting is a plus
 * Experience with SSIS is a plus
 * Experience with one of the following BI Tools. (SSRS, Tableau, Power BI)
   
   

Desired Qualifications Are As Follows


 * Experience working in Agile environment
 * Experience managing SDLC process with Atlassian tools. (Jira, Confluence)
 * Able and eager to learn new technologies.
 * Able to easily transition between high-level strategy and day-to-day implementation.
 * Excellent teamwork and collaboration skills.
 * Results-oriented and self-motivated.
   
   

Fully Remote: This position has been designated as fully remote, meaning that the position is expected to contribute from a non-NBCUniversal worksite, most commonly an employee’s residence.

Additional Information

NBCUniversal's policy is to provide equal employment opportunities to all applicants and employees without regard to race, color, religion, creed, gender, gender identity or expression, age, national origin or ancestry, citizenship, disability, sexual orientation, marital status, pregnancy, veteran status, membership in the uniformed services, genetic information, or any other basis protected by applicable law. NBCUniversal will consider for employment qualified applicants with criminal histories in a manner consistent with relevant legal requirements, including the City of Los Angeles Fair Chance Initiative For Hiring Ordinance, where applicable.

If you are a qualified individual with a disability or a disabled veteran, you have the right to request a reasonable accommodation if you are unable or limited in your ability to use or access nbcunicareers.com as a result of your disability. You can request reasonable accommodations in the US by calling 1-818-777-4107 and in the UK by calling +44 2036185726.","Associate","https://www.linkedin.com/jobs/view/data-engineer-nbc-sports-next-at-nbc-sports-next-3773999986?trk=public_jobs_topcard-title","Orlando, FL","6 days ago","","","2023-11-27","","Broadcast Media Production and Distribution, Entertainment Providers, and Media Production","Data Engineer - NBC Sports Next","Other"
"Over 200 applicants","81606529","Steneral Consulting","https://www.linkedin.com/company/steneral-consulting?trk=public_jobs_topcard-org-name","Full-time","Title: Data Engineer




Location: Remote (EST hours)




LinkedIn is required




Please send the candidate's resume with their contact information included. If their background looks like a fit I will give them a quick call to discuss the position. Include a link to their LinkedIn profile with your submission.




Job Description




 * We have flexibility in terms of hours but we are aligning to EST as that's where our HQ is located. We work with offshore and we do tend to start earlier than some with stand ups at 8am.
 * We need a Cloudera expert who will help us with the migration so Cloudera CDP expertise & hands-on experience is a must.




Job Summary




This position is responsible for being an expert with Cloudera Data Platform and related technologies including Hadoop, Hive, PostgreSQL, Impala, Amazon Web Services (AWS), Linux, database management and data migrations. The position is responsible for architecting and building a next generation data platform using reusable, scalable code with the ability to scale very large data volumes. This position requires the ability to assess the current project's challenges and offer solutions leading to the launch of the next generation product.




Demonstrated hands-on experience with Cloudera Data Platform.




Demonstrated work experience with distributed, scalable Big Data programming model and technologies such as Hadoop, Hive, Pig, etc.




Deep technical Expertise in the Cloudera Data Platform ecosystem components.




Experience automating via Terraform, Python, CDP CLI, AWS CLI, etc.




Experience designing and configuring cluster right sizing strategies.




Cloudera CDP Certification(s)




Hortonworks Hadoop Certification(s)




8+ years' experience in dimensional data modeling, ETL development, and Data Warehousing.




5+ years' experience in Business Consulting-ETL Processes.




4-6 years' experience using BIG Data (BD)-Apache Hadoop (HDFS)




Base/Hive/Pig/Mahout/Flume/Scoop/MapReduce/Yarn.




1+ year experience in Cloud Dev and Migration-AWS-Analytics/DW/Redshift.","Entry level","https://www.linkedin.com/jobs/view/data-engineer-at-steneral-consulting-3632565777?trk=public_jobs_topcard-title","United States","5 months ago","","","2023-06-12","","IT Services and IT Consulting","Data Engineer","Information Technology"
"Over 200 applicants","11925","Novanta Inc.","https://www.linkedin.com/company/novanta-inc?trk=public_jobs_topcard-org-name","Full-time","Build a career powered by innovations that matter! At Novanta, our innovations power technology products that are transforming healthcare and advanced manufacturing—improving productivity, enhancing people’s lives and redefining what’s possible. We create for our global customers engineered components and sub-systems that deliver extreme precision and performance for a range of mission-critical applications—from minimally invasive surgery to robotics to 3D metal printing.




Novanta is one global team with over 26 offices located in The Americas, Europe and Asia-Pacific. Looking for a great place to work? You have found it with a culture that embraces teamwork, collaboration and empowerment. Come explore Novanta.




This position is part of Novanta’s Corporate and Shared Services global teams. Novanta’s Corporate and Shared Services teams play an important role in executing the company’s strategic mission and operations. Included in Corporate and Shared Services are the business functions including Finance, Accounting, Human Resources, Information Technology, Legal, Compliance, Corporate Development and Corporate Marketing. The Corporate and Shared Services teams work closely with all Novanta business units to support operating initiatives contributing to the organization’s financial success.




Job Summary




As a Data Analytics Engineer, you will be responsible for building and maintaining the data layer for our analytics stack, top to bottom. Your role will span multiple disciplines from data engineering to data analytics and visualization across all stages of data maturity for the purpose of delivering robust Business Intelligence solutions. You will consider software engineering best practices including version control, automated testing, documentation, code review and continuous integration, as essential to any data stack.




Primary Responsibilities




 * Design, develop and maintain scaled, automated, user-friendly systems, reports, dashboards, etc.
 * Write complex, production-quality (i.e., accurate, performant, and maintainable) data transformation code to solve the needs of analysts, and business stakeholders (ex. MS SQL Server, Oracle, and Snowflake)
 * Analyze assigned projects for data quality issues. Troubleshoot and resolve issues as they arise.
 * Automate standard report creation and sharing using tools or scripts
 * Convert raw data into consumable information applying business logic and utilizing clean engineering workflows
 * Ensure that data, systems, architecture, business logic, and metrics are well-documented
 * Support the acquisition of external data sets, interpreting data layouts, structures, fields, and values to incorporate new data into the core analytics database
 * Serve as a catalyst for sharing knowledge, information, and ideas throughout the company as it relates to business intelligence
 * Interface with business customers to gather data and metrics requirements, then driving analytic projects to solve complex challenges
 * Draw insights from data and clearly communicate findings to stakeholders and external customers
 * Provide exceptional customer service to stakeholders through project execution and timely delivery of solutions




Required Experience, Education, Skills And Competencies




 * Bachelor’s degree in Information Technology preferred or relevant experience.
 * 3+ years - Experience with MS SQL Server and Snowflake
 * 3+ years - Experience with ETL/ELT Tools (ex. Mulesoft, API, Informatica)
 * 3+ years - Experience using Power BI, Tableau, or similar data visualization tool
 * Expert SQL Fluency (Well versed in CTEs and window functions)
 * Demonstrated ability in data modeling, ETL/ELT, data pipelines, EDW
 * Experienced building data warehouse infrastructure and BI tables
 * Motivated individual with strong analytic, problem solving, and troubleshooting skills




Travel Requirements




 * Less than 20%




Compensation And Benefits




 * The base pay for this position ranges from $90,000 to $120,000 depending on the geographic market
 * Dependent on the position offered, annual bonuses and other forms of compensation may be provided as part of the compensation package.
 * Novanta supports all aspects of your life. This position provides a full range of benefits including paid parental and family leave.




Novanta is proud to be an equal employment opportunity and affirmative action workplace. We consider all qualified applicants without regard to race, color, religion, sex (including pregnancy), sexual orientation, gender identity or expression, national origin, military and veteran status, disability, genetics, or any other category protected by federal law or Novanta policy.




Please call +1 781-266-5700 if you need a disability accommodation for any part of the employment process.","Mid-Senior level","https://www.linkedin.com/jobs/view/data-analytics-engineer-at-novanta-inc-3735181462?trk=public_jobs_topcard-title","United States","2 weeks ago","","","2023-11-18","","Appliances, Electrical, and Electronics Manufacturing","Data Analytics Engineer","Information Technology"
"Over 200 applicants","9804","Mercury Insurance","https://www.linkedin.com/company/mercury-insurance?trk=public_jobs_topcard-org-name","Full-time","Join an amazing team that is consistently recognized for our achievements and culture, including our most recent Forbes award of being one of America's Best Midsize Employers for 2023!

Position Summary

As a Data Engineer II you will create production data pipelines for our advanced analytics and data science teams – as well as collaborate with other technical personnel on internal & external data sources and infrastructure needs. The Data Engineer II will assist in design, evaluate, and test data infrastructures and be a subject matter expert for all things data across the organization.

Essential Job Functions

Design, build, and launch collections of high-quality big data/data lake solutions on Cloud platform preferably AWS, Snowflake that support multiple use cases across all departments, all products, and all states.

Solve our most challenging data integration problems, utilizing optimal ETL patterns, frameworks, query techniques, sourcing from structured and unstructured data sources.

Assist in owning existing processes running in production, optimizing complex code through advanced algorithmic concepts.

The Data Engineer is an expert in all data lakes, data warehouses, and data cubes within Mercury, with no gaps in knowledge. Can efficiently and accurately extract and manipulate data from any source.

Collaborate with teams of data analysts and data scientists, who research and integrate algorithms to develop solutions to address complex data problems. Influence all functions across the organization to identify data opportunities to drive profitable growth. Proactively identify pain points that Analytics & Data Science face with our existing data models.

Leverages existing data infrastructure to fulfill all data-related requests, perform necessary data housekeeping, data cleansing, normalization, hashing, and implementation of required data model changes. Analyzes data to spot anomalies, trends and correlate similar data sets. Designs, develops and implements natural language processing software modules.

Education

Other functions may be assigned


 * Bachelor's degree in Computer Engineering, Computer Science, Mathematics, Electrical Engineering, Information Systems, or related field
 * Actuarial experience/exams preferred.
 * Or equivalent combination of education and/or experience
   
   

Experience

3 or more years of experience in data analytics, data engineering, and/or data science

3 or more years of experience in development of big data/data lake solutions on Cloud platforms, preferably AWS (S3, Glue/EMR, Athena, AppFlow) or Snowflake

3 or more years of experience in Python, Java and/or Scala programming

3 or more years of experience in writing SQL statements and query performance tuning

3 or more years of experience in RDMS or MPP databases, preferably AWS Redshift or Snowflake

Knowledge And Skills


 * A high-level specialist who regularly interacts and works with senior management.
 * Expert at analyzing data to identify gaps and inconsistencies
 * Able to multitask, prioritize, and manage time effectively.
 * The ability to think conceptually, analytically and creatively comfortable with ambiguity.
 * Experience managing and communicating data plans and data models to internal clients.
 * Demonstrated solid understanding, and passion for, all areas of data/analytics engineering best practices.
 * Demonstrated expert skills in data mining and data analytics
 * Expert in Python and/or SQL programming; some experience with R preferred
 * Solid experience with cloud-based advanced data and analytics environment
 * Knowledge of working with AWS, GitHub, and other cloud-based infrastructure
 * Expert data skills and the ability to work with large structured and unstructured data sources
 * Excellent problem-solving skills required
 * Excellent analytical and critical thinking required
 * Excellent written and verbal communication skills required
 * Demonstrate Company’s Core Values
   
   

Why choose a career at Mercury?

At Mercury, we have been guided by our purpose to help people reduce risk and overcome unexpected events for more than 60 years. We are one team with a common goal to help others. Everyone needs insurance and we can’t imagine a world without it.

Our team will encourage you to grow, make time to have fun, and work together to make great things happen. We embrace the strengths and values of each team member. We believe in having diverse perspectives where everyone is included, to serve customers from all walks of life.

We care about our people, and we mean it. We reward our talented professionals with a competitive salary, bonus potential, and a variety of benefits to help our team members reach their health, retirement, and professional goals.

Learn more about us here: https://www.mercuryinsurance.com/about/careers

We Offer Many Great Benefits, Including


 * Competitive compensation
 * Flexibility to work from home/hybrid in
 * Company vehicle + gas card
 * Paid time off (vacation time, sick time, 9 paid Company holidays, volunteer hours)
 * Incentive bonus programs (potential for holiday bonus, referral bonus, and performance-based bonus)
 * Medical, dental, vision, life, and pet insurance
 * 401 (k) retirement savings plan with company match
 * Engaging work environment
 * Promotional opportunities
 * Education assistance
 * Professional and personal development opportunities
 * Company recognition program
 * Health and wellbeing resources, including free mental wellbeing therapy/coaching sessions, child and eldercare resources, and more
   
   

Mercury Insurance is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, status as a protected veteran, or any other characteristic protected by federal, state, or local law.","Entry level","https://www.linkedin.com/jobs/view/data-engineer-ii-remote-at-mercury-insurance-3768165697?trk=public_jobs_topcard-title","Brea, CA","2 weeks ago","","","2023-11-17","$87,628.00-$166,102.00","Insurance","Data Engineer II (Remote)","Information Technology"
"99 applicants","11553382","Material Bank®","https://www.linkedin.com/company/materialbank?trk=public_jobs_topcard-org-name","Full-time","Material Bank is a fast-paced, high-growth technology company and created the world's largest material marketplace for the Architecture and Design industry, providing the fastest and most powerful way to start and manage a design project. Learn more about us at www.materialbank.com or see below.

--

Material Bank is seeking a Senior Data Engineer with extensive data architecture and data platform design experience to power massive scale SaaS/eCommerce applications.

The ideal candidate is self-motivated, experienced with variety of technologies in AWS, ecosphere, thrives in a dynamic work environment, and demonstrates a flexible and adaptable work. As part of our data group, you will be a major participant in architecting, building, and maintaining scalable solutions that supports data sharing throughout the ecosystem for the world’s largest community of products and materials.  Operating at depth will be key to be successful in this role – this candidate’s technical and management ability will require constant learning and growth.

This individual is also responsible for developing and maintaining an application integration architecture blueprint for the organization. This role requires an individual who is extremely detail-oriented, strategic in their approach and comfortable with change in a dynamic work environment.

This is a remote position based anywhere in the United States. Most (not all) of your team will be in US East Coast time zone.

What you’ll do:


 * Design, develop, implement, and translate business requirements and the overall organizational data strategy, including standards, principles, data sources, storage, pipelines, data flow, data processing, and data security/ governance policies
 * Develop and maintain the technology stack (Airflow, Airbyte, Redshift, AWS Athena) to include commercial or open-source integration platforms
 * Is motivated to understand the challenges in the SEO space, and has a passion for solving problems and not just delivering features
 * Influence strategic thinking across the team. Advocates for best practices, investigates new technologies and mentors other engineers.
 * Contribute to critical steps within the Software Development Life Cycle components that impact enterprise-wide development / architecture standards.
 * Nurture a culture of transparency, pragmatism, and no ego, while scaling the Engineering organization.
 * Provide oversight in standards adherence through reviews of project work including detailed technical specifications and application code.
 * Work with project teams in definition of requirements for integration strategies of their projects into the Enterprise Architecture.
 * Partner with leaders of other disciplines to ensure proposed solutions align with information, technology, infrastructure, business, and security architectures.
   
   
   

What you’ll bring:


 * BA/BS degree preferred, and 10+ years of relevant work experience
 * Knowledge of enterprise architecture, systems architecture, integration architecture and data architecture standards, frameworks, and practices
 * Knowledge of common system integration methods and technologies including Web services, SOAP, JSON, XML Schema Definition (XSD), Extensible Markup Language (XML), Business process automation and orchestration tools and software
 * Expertise in solution design and development, vendor management, enterprise application support, in addition to strong business understanding and project management
 * Experience with the implementation of Continuous Integration practices and tooling to support automated build, delivery, and regression test of commercial software
 * Proven experience in managing relationships with vendors/consultants, IT teams and internal stakeholders
 * Experience working with API’s from commercial software applications, such as CRM or ERP systems
 * A passion for all things tech and has a drive to experiment with new technologies to see where they can benefit the business
 * Experience in a management or supervisory role on an integration development team
 * Excellent communication skills
 * Hands-on, collaborative working style, with the ability to build relationships and earn trust with multiple teams and key stakeholders quickly
   
   
   

What you’ll get from us:


 * Our people: If you thrive in an inclusive, innovative, and fast-paced organization, look no further! You will get to work alongside some of the brightest minds - Join a genuinely fun and supportive workplace where we keep our employees consistently engaged through internal communication and corporate events
 * Relaxation and Celebrations: Generous PTO, Sick Days, Paid National Holidays, and even more (ask us about this when we connect).
 * Health Benefits: We contribute to your medical, dental, vision and short-term/long-term disability plans and have a strong employee assistance program.
 * Plan for your Retirement: 401(k) eligible after your first 90 day's employed!
 * Giving Back: We sponsor multiple events throughout the year to help out our communities. You will receive time off to give back as well.
 * Growth: We’ll help you take your career to the next level. We want you to be creative and take initiative which will allow you to grow and create within the company. Most importantly, be the best at what matters!
 * Flexible Work Schedules: With business units and employees across the globe, Material Technologies has embraced a hybrid  working model allowing department leaders to decide on the best approach for their respective teams, whether that be remote, in person, or a little of both.
   
   
   

About Material Bank

Material Bank is the world’s largest material marketplace for the architecture and design industry, providing the fastest and most powerful way to search and sample materials. Material Bank connects design professionals to hundreds of manufacturers through facilitating brand discovery, rep engagement, and material sampling.

Material Bank has transformed the way an entire industry discovers and samples materials. By removing the friction that exists in the process, we drive business between architects and designers (members) and our Brand Partners (clients).

Our powerful material database and proprietary robotic distribution facility allow members to order samples until midnight (ET) to be delivered free of charge anywhere in the US, in one box, by 10:30 AM the next morning.

Connect with us and discover your career at Material Bank.

--

Material Bank is proud to be an equal opportunity employer. We value diversity, and all applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, age, national origin, veteran or disability status or other status protected under any applicable federal, state or local law.","Mid-Senior level","https://www.linkedin.com/jobs/view/senior-data-engineer-at-material-bank%C2%AE-3774071889?trk=public_jobs_topcard-title","United States","4 days ago","","","2023-11-30","","Design Services","Senior Data Engineer","Information Technology"
"Over 200 applicants","165158","Netflix","https://www.linkedin.com/company/netflix?trk=public_jobs_topcard-org-name","Full-time","At Netflix, we want to entertain the world and are constantly innovating on how entertainment is imagined, created and delivered to a global audience. We currently stream content in more than 30 languages in 190 countries, topping over 233 million paid subscribers.

We launched a new ad-supported tier in November 2022 to offer our members more choice in how they consume their content. Our new tier allows us to attract new members at a lower price point, while also creating a compelling path for advertisers to reach audiences that are deeply engaged.

Our Team

The Ads Platform Engineering teams build advertising systems and integrations that powers the delivery of ads using our world class content delivery ecosystem. We use a number of Netflix investments and innovations to power our ads - unique mix of client and server side ad insertions, state of the art content delivery system, ad encoding recipes, content understanding and metadata etc. We deliver ads in a manner that’s thoughtful of our member’s viewing experience and drive great outcomes for advertisers. We also ensure that advertiser brand safety is ensured during serving, members only see the most appropriate ads for them.

Our team is new and yet faced with the enormous ambitions of building highly performant advertising systems and delivering high impact to our business by monetizing our incredible slate of content. As one of the newest entrants in the Connected TV advertising space that’s rapidly growing, we seek to build unique value propositions that help us differentiate from the competition and become a market leader in record time.

We are looking for highly motivated engineers working in the advertising space who are excited to join us on this journey.


Skills & Experience We’re Seeking



 * Well grounded domain experience in ANY ONE of these 4 areas :
 * 1- (Ad Delivery) Building publisher side ad tech systems such as ad servers, bidders and pacers, yield optimizers or their demand side counterparts (DSPs).
 * 2 - (Identity and Audience Targeting) Built identity graphs, integrated with DMPs and worked on enabling audience targeting, 1P or 3P segments
 * 3 - (Measurement) Familiarity with brand lift measurement, incrementality based measurement, outcomes measurement etc.
 * 4 - (Sales Enablement) Guaranteed and non-guaranteed inventory reservations and management, Sales tooling, Integrations with OMS, CRM systems etc.
 * General understanding of the advertising marketplace and landscape
 * Worked closely with ad operations to enable campaign setup, targeting, reporting and advertiser needs
 * Developed many cloud based applications and comfortable with modern programming languages (preferably object oriented),
 * Familiar with A/B testing and experimentation.
   
   
   
   

Nice To Haves



 * Contributed to an ads industry technology standard (e.g VAST, OpenRTB) or worked on an industry consortium effort, working group etc.
 * Familiarity with legal compliance and changing landscape of ads regulations around the world.
 * Experience working in the CTV space and knowledge of it’s unique constraints
   
   
   

At Netflix, we carefully consider a wide range of compensation factors to determine your personal top of market. We rely on market indicators to determine compensation and consider your specific job, skills, and experience to get it right. These considerations can cause your compensation to vary and will also be dependent on your location.

The overall market range for roles in this area of Netflix is typically $100,000 - $700,000

This market range is based on total compensation (vs. only base salary), which is in line with our compensation philosophy. Netflix is a unique culture and environment. Learn more here.

We are an equal opportunity employer and celebrate diversity, recognizing that diversity of thought and background builds stronger teams. We approach diversity and inclusion seriously and thoughtfully. We do not discriminate on the basis of race, religion, color, ancestry, national origin, caste, sex, sexual orientation, gender, gender identity or expression, age, disability, medical condition, pregnancy, genetic makeup, marital status, or military service.","Not Applicable","https://www.linkedin.com/jobs/view/software-engineer-l5-ads-platform-engineering-at-netflix-3669206397?trk=public_jobs_topcard-title","United States","1 week ago","","","2023-11-25","","Movies, Videos, and Sound, Technology, Information and Internet, and Entertainment Providers","Software Engineer L5 - Ads Platform Engineering","Engineering and Information Technology"
"Over 200 applicants","7947803","Juniper Square","https://www.linkedin.com/company/juniper-square?trk=public_jobs_topcard-org-name","Full-time","About Juniper Square

Our mission is to unlock the full potential of private markets. Privately owned assets like commercial real estate, private equity, and venture capital make up half of our financial ecosystem yet remain inaccessible to most people. We are digitizing these markets, and as a result, bringing efficiency, transparency, and access to one of the most productive corners of our financial ecosystem. If you care about making the world a better place by making markets work better through technology – all while contributing as a member of a values-driven organization – we want to hear from you.

Juniper Square offers employees a variety of ways to work, ranging from a fully remote experience to working full-time in one of our physical offices. We invest heavily indigital-firstoperations, allowing our teams to collaborate effectively across most US states, 2 Canadian Provinces, and Mexico. We also have physical offices in San Francisco, CA and Austin, TX, for employees who prefer to work in an office some or all of the time.

GP Experience

Juniper Square serves two sides of the private capital markets, the investment managers (GPs) and the investors (LPs). The GP eXperience team (i.e., GPX) is responsible for Juniper Square’s product offering for General Partners (GPs). This is our core product that enables all other innovation at Juniper Square as we unlock and improve the world’s private capital markets. Our platform handles billions of dollars of transactions each month and we are actively expanding into additional private asset classes such as Venture Capital& Private Equity. Come help us innovate in fundraising, reporting, asset-ownership mapping, and more.

The Team

The Data Engineering team is responsible for Data pipelines that serve multiple types of customers including internal Juniper Square users for Business Intelligence and GPs for Analytics on their data. We support the ability for these customers to create and manage their custom dashboards. We also support the ability for other Product Engineering teams to add metrics to track product usage for the features they launch into production.

About Your Role

Juniper Square is growing rapidly, and our data needs are growing even faster, so we’re growing our Data Engineering Team. As a Senior Data Engineer your role will be pivotal to evolving our existing data and reporting experiences. You’ll build out pipelines to gather data from multiple sources and make it available for analysis. You will shape both internal and external analytics products to help guide business-critical decisions, enhance their workflows, and improve decision-making.

What You’ll Do


 * Design and implement sophisticated data models in SQL.
 * Work closely with the other Software Engineers to ensure sound, scalable implementation.
 * Act as a technical expert on our team regarding all things data, especially as the data team grows and evolves.
 * Introduce new technologies to evolve and enhance our data pipeline capabilities.
 * Document data models, architectural decisions and data dictionaries to enable collaboration, maintainability and usability of our analytics platforms and code.
 * Assist with governance, guidance, code reviews, and access controls so that we maintain consistency, quality, and business confidentiality as we scale analytics access across the company and to customers.
 * Externally: learn our application data schema, and develop a fluency in how to transform it to enhance customer’s decision-making with data.
 * Internally: guide product and development teams, advising on instrumentation and laying development foundations for product usage reporting.
 * Fulfill projects with minimal guidance but with an appropriate sense of when and how to collaborate with others.
 * Build scalable, highly performant infrastructure for delivering clear business insights from a variety of raw data sources.
   
   

Qualifications


 * Bachelor's degree in Computer Science, or equivalent work experience
 * 5+ years of experience building ETL (Extraction Transform Load) or ELT (Extraction Load Transform) pipelines from scratch
 * Strong command of relational databases (Postgresql preferred), data modeling and database design
 * Strong command of Python and experience using Python for Data Pipelines
 * Experience with cloud based services (AWS RDS preferred)
 * Experience developing on (or administering) BI / data visualization platforms (ex. Looker, Tableau, PowerBI, Mode, Data Studio, Domo, QlikView etc.).
 * Basic understanding of data warehouses such as Amazon Redshift, Google BigQuery, Snowflake etc.
 * Demonstrated history of translating data into clear and actionable narratives and communicating opportunities and challenges relevant to stakeholders.
 * You must be flexible and adaptable—you will be operating in a fast-paced startup environment.
   
   

At Juniper Square, we believe building a diverse workforce and an inclusive culture makes us a better company. If you think this job sounds like a fit, we encourage you to apply even if you don’t meet all the qualifications.

Benefits

Compensation for this position includes a base salary, equity, and a variety of benefits. The U.S. base salary range for this role is $160,000 - $200,000 and the Canadian base salary range for this role is $200,000 to $250,000 CAD. Actual base salaries will be based on candidate-specific factors, including experience, skillset, and location, and local minimum pay requirements as applicable. Your recruiter can provide further details.


 * Competitive salary and meaningful equity
 * Health, dental, and vision care for you and your family
 * Unlimited vacation policy and paid holidays
 * Generous paid family leave, medical leave, and bereavement leave policies
 * 401k retirement savings plan
 * Healthcare FSA and commuter benefits programs
 * Freedom to customize your work and technology setup as you see fit
 * Professional development stipend
 * Monthly work from home wellness stipend while we're all remote
 * Mental wellness coverage including live coaching and therapy sessions
 * Home office productivity allowance to help create an ideal work from home setup
   
   

#Juniper-US

#Juniper-Canada

","Mid-Senior level","https://www.linkedin.com/jobs/view/senior-data-engineer-at-juniper-square-3705285662?trk=public_jobs_topcard-title","Denver, CO","1 day ago","","","2023-12-03","","Software Development","Senior Data Engineer","Engineering"
"Over 200 applicants","66938931","Charlie Health","https://www.linkedin.com/company/charlie-health?trk=public_jobs_topcard-org-name","Full-time","Why Charlie Health?

Young people across the country need our help. The sad reality is that a mental health crisis has taken hold of our most vulnerable population—leading to record levels of depression, anxiety, substance abuse, and self-harm. From Manhattan to Montana, this reality is compounded by issues of access, both geographic and financial. The mental health landscape is systemically broken, and our young people are suffering as a result.

Charlie Health has set out on a mission to reimagine how high acuity care is delivered to young people and families in crisis. Our initial offering is a virtual intensive outpatient program, which places peers with similar mental health experiences and goals into customized virtual groups. Our team of masters-level clinicians lead groups multiple times per week to deliver a higher level of care.

Our goal is to help young people and families heal together. Through a combination of exceptional medical and psychological care, engaged community partnerships, and best-in-class technology, we provide an unparalleled approach to recovery support that serves individual needs in an integrated way. Join us in our mission to ensure that every young person—regardless of location or socioeconomic status—can get the care that they deserve.

About The Role

The data team at Charlie Health services all parts of our business by sourcing, curating, and activating internally and externally sourced datasets. As a software engineer on the data team, you will be responsible for building ELT pipelines, developing custom DAGs, transforming and warehousing data with DBT, and building integrations between our systems.

Our team is comprised of passionate, forward-thinking professionals eager to take on the challenge of the mental health crisis and play a formative role in providing life-saving solutions. We are looking for a candidate who is inspired by our mission and excited by the opportunity to build a business that will impact millions of lives in a profound way.

Duties & Essential Job Functions


 * Develop, release, and maintain high quality data pipelines using Python, FiveTran, DBT, and Snowflake
 * Own and guide the development of our data infrastructure
 * Develop custom integrations using Dagster
 * Configure reverse ETL integrations using Hightouch
 * Identify bottlenecks and implement improvements to our data engineering processes, tools, and procedures. We’re early and the expectation of folks joining at this stage is that you’ll play a huge part in setting and improving how we work
 * Promote a culture of collaboration and learning across engineering, product, and design team via mentoring, documentation, presentations, or other knowledge-sharing methods
 * Ensure our data is always available by participating in our on-call rotation
   
   

Requirements


 * Bachelor’s degree in Information Systems, Computer Science, Data Science, Analytics, Mathematics, or equivalent practical experience
 * 5+ years experience as a software engineer, with at least 3 years of experience in a data engineering role
 * Deep expertise in SQL. You understand CTEs, aggregation functions, window functions, partitioning and clustering approaches to run correct and highly-performant queries
 * High proficiency in Python and experience using common data engineering libraries such as Pandas, Numpy, and Great Expectations
 * Experience with a modern data stack. Experience with our tools - FiveTran, Snowflake, DBT, Dagster, Hightouch, and Tableau - is a big plus
 * Experience with data exploration, profiling, governance, visualization, and activation
 * Proven ability to thrive in an ambiguous and rapidly changing environment
 * Experience working with sensitive data in a regulated environment
 * Expertise in healthcare is a plus
   
   

Benefits

Charlie Health is pleased to offer comprehensive benefits to all full-time, exempt employees. Read more about our benefits here.

So—what do you think?

If you’ve made it this far, well, we’re excited to meet you too. Just one more thing that we want you to remember: we pride ourselves on our meritocratic, performance-driven culture. There are lives on the line, and we have young people to save. There’s no room for complacency. Your scope of responsibility and opportunity to make a difference will be uncapped at Charlie Health, but we need your commitment that you will work tirelessly for our patients, parents, and partners. At the end of day, our team is committed to helping you succeed at Charlie Health because when you succeed, our patients succeed, and we get one step closer to solving the mental health crisis. We’re hopeful that this role will give you the experience to go and do whatever you want in life but the fulfillment to make you never want to leave our team. We look forward to solving the mental health crisis, together.

Please do not call our public clinical admissions line in regards to this or any other job posting.

Please be cautious of potential recruitment fraud. If you are interested in exploring opportunities at Charlie Health, please go directly to our Careers Page: https://www.charliehealth.com/careers/current-openings. Charlie Health will never ask you to pay a fee or download software as part of the interview process with our company. In addition, Charlie Health will not ask for your personal banking information until you have signed an offer of employment and completed onboarding paperwork that is provided by our People Operations team. All communications with Charlie Health Talent and People Operations professionals will only be sent from @charliehealth.com email addresses. Legitimate emails will never originate from gmail.com, yahoo.com, or other commercial email services.

Recruiting agencies, please do not submit unsolicited referrals for this or any open role. We have a roster of agencies with whom we partner, and we will not pay any fee associated with unsolicited referrals.

At Charlie Health, we value being an Equal Opportunity Employer. We strive to cultivate an environment where individuals can be their authentic selves. Being an Equal Opportunity Employer means every member of our team feels as though they are supported and belong. We value diverse perspectives to help us provide essential mental health and substance use disorder treatments to all young people.

Charlie Health applicants are assessed solely on their qualifications for the role, without regard to disability or need for accommodation.

","Mid-Senior level","https://www.linkedin.com/jobs/view/senior-data-engineer-at-charlie-health-3650034167?trk=public_jobs_topcard-title","United States","2 hours ago","","","","","Mental Health Care","Senior Data Engineer","Information Technology"
"164 applicants","5548","Premera Blue Cross","https://www.linkedin.com/company/premera-blue-cross?trk=public_jobs_topcard-org-name","Full-time","Join Our Team: Do Meaningful Work and Improve People’s Lives

Our purpose, to improve customers’ lives by making healthcare work better, is far from ordinary. And so are our employees. Working at Premera means you have the opportunity to drive real change by transforming healthcare.

To better serve our customers, we’re creating a culture that promotes employee growth, collaborative innovation, and inspired leadership.

Forbes ranked Premera among America’s 2023 Best Midsize Employers because we are committed to creating an environment where employees can do their best work and where best-in-class talent comes, stays, and thrives!

As a Data Engineer III, you will create, modify, and test the code, forms, and scripts that construct the data sets that drive our Data Science and Business Intelligence capabilities. This work is derived from specifications developed in collaboration with those consuming or interacting with those data sets. In addition, you may develop and write solutions to store, locate, and retrieve specific documents, data, and information and distribute through multiple methods. You will provide solutions to a wide range of difficult problems, with solutions that are imaginative, practical, and consistent with organizational objectives.

This is a virtual/telecommuter position, working from home.

What you’ll do:


 * Solve business and data engineering problems using data centric programming and scripting skills to create data models and pipelines.
 * Consult with the business to create understanding of the needs, pace, and direction for our business partners, translate these needs into requirements and specifications, and maintain contact with the customers through project completion.
 * Lead all phases of architecture, conceptualization, design, development, testing, and production support of BI solutions.
 * Architect, design, and build best-of-class production processes that ensure security, efficiency, and availability of analytic tools and data.
 * Collaborate with data scientists and analysts to further understand business problems.
 * Program and manage APIs for data exchange.
 * Lead and conduct unit and system testing to ensure design is still relevant and implementation is producing a useful, maintainable, reliable product.
 * Mentor and train other team members by introducing them to new technologies, methods, and learning resources.
 * Act as a resource to the technical community as well as business partners.
   
   

What you’ll bring:


 * Bachelor’s Degree in Computer Science, Computer Engineering, or similar area, and 5-10 years of experience in data integration, design, and management. Additionally, you will have knowledge of the software development lifecycle, relational database theory, and skills to utilize one or more programming languages. At a minimum, candidates must possess the equivalent in education, experience, and skills of a Bachelor’s Degree in a related field and (5) years of relevant experience. (Required)
 * Familiarity with healthcare specific regulatory requirements for data management.
 * Experience providing data integration services within healthcare organizations.
 * Significant knowledge of health care and the health care industry; knowledge of health insurance concepts and terms.
 * (2) years of experience in Azure Data Integration tools such as Azure Data Factory, Azure Logic Apps, and Azure Functions.
 * (2) years of experience in Snowflake (SaaS) Data Warehouse Platform.
 * (3) years of experience in data integration, design, and/or management.
 * Advanced data processing programming skills across SQL-based and Hadoop-based technologies.
 * Strong understanding of REST API and web services.
 * Working knowledge of Agile and Scrum project methodologies utilizing TFS, Jira/Confluence.
 * Ability to use Kimball methodology for dimensional data modeling, 3rd Normal form DW.
   
   

What We Offer


 * Medical, vision and dental coverage
 * Life and disability insurance
 * Retirement programs (401K employer match and pension plan)
 * Wellness incentives, onsite services, a discount program and more
 * Tuition assistance for undergraduate and graduate degrees
 * Generous Paid Time Off to reenergize
 * Free parking
   
   

Equal employment opportunity/affirmative action:

Premera is an equal opportunity/affirmative action employer. Premera seeks to attract and retain the most qualified individuals without regard to race, color, religion, sex, national origin, age, disability, marital status, veteran status, gender or gender identity, sexual orientation, genetic information or any other protected characteristic under applicable law.

If you need an accommodation to apply online for positions at Premera, please contact Premera Human Resources via email at careers@premera.com or via phone at 425-918-4785.

Premera is hiring in the following states, with some limitations based on role or city: Alaska, Arizona, Arkansas, California, Colorado, Florida, Georgia, Idaho, Iowa, Kansas, Kentucky, Maine, Michigan, Minnesota, Missouri, Montana, Nevada, New Hampshire, New Mexico, North Carolina, Oklahoma, Oregon, South Carolina, South Dakota, Tennessee, Texas, Utah, Washington, Wisconsin.

The pay for this role will vary based on a range of factors including, but not limited to, a candidate’s geographic location, market conditions, and specific skills and experience.

National Salary Range:

$91,000.00 - $154,700.00

National Plus Salary Range:

$102,800.00 - $174,800.00


 * National Plus salary range is used in higher cost of labor markets including Western Washington and Alaska.","Not Applicable","https://www.linkedin.com/jobs/view/data-engineer-iii-at-premera-blue-cross-3775809984?trk=public_jobs_topcard-title","United States","4 days ago","","","2023-11-30","","Insurance","Data Engineer III","Information Technology"
"Over 200 applicants","22688","Atlassian","https://au.linkedin.com/company/atlassian?trk=public_jobs_topcard-org-name","Full-time","Overview

Working at Atlassian

Atlassians can choose where they work – whether in an office, from home, or a combination of the two. That way, Atlassians have more control over supporting their family, personal goals, and other priorities. We can hire people in any country where we have a legal entity. Interviews and onboarding are conducted virtually, a part of being a distributed-first company.

This is a remote position that requires you to be based in the United States.

Your future team

The GTM Data Science team is comprised of people with backgrounds in Marketing, Data Science, Data Engineering and Reporting. We connect the dots from data and analytics to how our go-to-market teams impact our growth trajectory and promote data-driven decision-making. For us, it is all about helping Atlassian grow by delivering data in the right way, at the right time, to partners across the company.

As a Senior Analytics Engineer, you'll report to the Data Science Manager, Top of the Funnel Insights. You'll help our team and the Data engineering team set the standards for marketing data.

Responsibilities

In this role, you will:


 * Create the definitions and contracts for data objects and the pipelines that are used to understand our business
 * Partner with the GTM Data Engineering Team to provide business logic to move data pipelines into production
 * Onboard new marketing channels and data sources to support measurement and optimization
 * Oversee the top of funnel metrics and ensure consistency in their usage across dashboards and reports by maintaining great documentation
 * Apply software engineering best practices to Analytics Code
   
   
   

Qualifications

Your background:


 * 5+ years of experience in Data Engineering or Data Science roles with a background in paid marketing
 * Advanced proficiency in SQL and proficiency in Python
 * Experience with Databricks, Tableau, ETL, marketing APIs and data sources
 * Experience solving complex data problems and curiosity to detect and explain data anomalies
   
   
   

Compensation

At Atlassian, we strive to design equitable and explainable compensation programs. To support this goal, the baseline of our range is higher than that of the typical market range, but in turn we expect to hire most candidates near this baseline. Base pay within the range is ultimately determined by a candidate's skills, expertise, or experience.

Role

In the United States, we have three geographic pay zones. For this role, our current base pay ranges for new hires in each zone are:

Zone A: $175,100 - $233,400

Zone B: $157,600 - $210,100

Zone C: $145,300 - $193,800

This role may also be eligible for benefits, bonuses, commissions, and equity. Please visit go.atlassian.com/payzones for more information on which locations are included in each of our geographic pay zones. However, please confirm the zone for your specific location with your recruiter.

Our Perks & Benefits

Atlassian offers a variety of perks and benefits to support you, your family and to help you engage with your local community. Our offerings include health coverage, paid volunteer days, wellness resources, and so much more.

Visit go.atlassian.com/perksandbenefits to learn more.

About Atlassian

At Atlassian, we're motivated by a common goal: to unleash the potential of every team. Our software products help teams all over the planet and our solutions are designed for all types of work. Team collaboration through our tools makes what may be impossible alone, possible together.

We believe that the unique contributions of all Atlassians create our success. To ensure that our products and culture continue to incorporate everyone's perspectives and experience, we never discriminate based on race, religion, national origin, gender identity or expression, sexual orientation, age, or marital, veteran, or disability status. All your information will be kept confidential according to EEO guidelines.

To provide you the best experience, we can support with accommodations or adjustments at any stage of the recruitment process. Simply inform our Recruitment team during your conversation with them.

To learn more about our culture and hiring process, visit go.atlassian.com/crh .

","Mid-Senior level","https://www.linkedin.com/jobs/view/senior-analytics-engineer-at-atlassian-3739944841?trk=public_jobs_topcard-title","United States","3 days ago","","","2023-12-01","","Software Development","Senior Analytics Engineer","Information Technology"
"196 applicants","98140277","Sumitomo Pharma America, Inc.","https://www.linkedin.com/company/sumitomo-pharma-america?trk=public_jobs_topcard-org-name","Full-time","Sumitomo Pharma America (SMPA) is focused on delivering therapeutic and scientific breakthroughs in areas of critical patient need spanning psychiatry & neurology, oncology, urology, women’s health, rare disease, and cell & gene therapies. The company’s diverse portfolio includes several marketed products and a robust pipeline of early- to late-stage assets. Building on Sumitomo Pharma’s 125-year legacy of innovation, SMPA leverages proprietary in-house technology platforms and advanced analytics capabilities to accelerate discovery, development, and help bring novel therapies to patients sooner. SMPA is a Sumitomo Pharma company. [For more information, visit Sumitomo-pharma.com]

Job Overview

We are currently seeking a dynamic, highly motivated, thoughtful, and hands-on Associate Data Engineer to join our Data Engineering team. Data Engineering is a broad team that develops and operates the data platform used by developers throughout SMPA. The team implements all data ingestion, storage and analytics tooling and provides other ad hoc database needs.

Job Duties And Responsibilities


 * Partner with product owners and developers to provide end-to-end data solutions for technology tools and products.
 * Build and optimize tools for data ingestion and storage using best coding practices.
 * Automate and maintain data processing pipelines, implement modern data transformation infrastructure and continuously improve the efficiency of our platform.
 * Serve as a subject matter expert on big data analytics projects that provide insights for business and technical stakeholders.
   
   

Key Core Competencies


 * Strong analytical skills.
 * Flexible: Adaptable and can thrive in a changing dynamic environment.
 * Problem Solver: Able to identify practical solutions to complex issues.
 * Self-Motivated: Takes initiative, cares about the outcome, and owns a task to completion.
 * Team player with strong communication skills and the ability to work with minimal supervision.
 * Quick and scrappy learner who adapts well to a fast-moving environment and gets things done; experience in high-growth or startup environments a plus.
   
   

Education And Experience


 * BA/BS degree with strong academic performance, preferably in a quantitative field.
 * 4+ years experience with Python, database development, Linux, and AWS (S3, EC2, SNS, Lambda, SQS).
 * Strong ANSI SQL skills and data analysis experience.
 * Experience with Pyspark, containers, big data and/or healthcare data preferred.
 * Team collaboration with CI/CD tools such as Bamboo or Jenkins, code control such as Github or Bitbucket and infrastructure tools like Teraform.
 * Knowledge of Agile and desire to work in an incredibly fast-moving environment.
 * Preference for individuals that have experience within Supply Chain, Healthcare, Insurance and Pharmaceutical Industries.
   
   

Disclaimer: The above statements are intended to describe the general nature and level of work being performed by people assigned to this classification. They are not to be construed as an exhaustive list of all responsibilities, duties, and skills required of personnel so classified. All personnel may be required to perform duties outside of their normal responsibilities from time to time, as needed.

The base salary range for this role is $104,900 to $137,700. Base salary is part of our total rewards package which also includes the opportunity for merit-based salary increases, short incentive plan participation, eligibility for our 401(k) plan, medical, dental, vision, life and disability insurances and leaves provided in line with your work state. Our robust time-off policy includes unlimited paid time off, 11 paid holidays plus additional time off for a shut-down period during the last week of December, 80 hours of paid sick time upon hire and each year thereafter. Total compensation, including base salary to be offered, will depend on elements unique to each candidate, including candidate experience, skills, education and other factors permitted by law.

Confidential Data: All information (written, verbal, electronic, etc.) that an employee encounters is considered confidential.

Compliance :Achieve and maintain Compliance with all applicable regulatory, legal and operational rules and procedures, by ensuring that all plans and activities for and on behalf of Sumitomo Pharma America (SMPA) and affiliates are carried out with the ""best"" industry practices and the highest ethical standards.

Mental/Physical Requirements: Fast paced environment handling multiple demands. Must be able to exercise appropriate judgment as necessary. Requires a high level of initiative and independence. Excellent written and oral communication skills required. Requires ability to use a personal computer for extended periods of time.

Sumitomo Pharma America (SMPA) is an Equal Employment Opportunity (EEO) and Affirmative Action employer

Sumitomo Pharma America (SMPA) is committed to a culturally diverse workforce. All qualified applicants will receive consideration for employment without regard to race; color; creed; religion; national origin; age; ancestry; nationality; marital, domestic partnership or civil union status; sex, gender, gender identity or expression; affectional or sexual orientation; disability; veteran or military status or liability for military status; domestic violence victim status; atypical cellular or blood trait; genetic information (including the refusal to submit to genetic testing) or any other characteristic protected by law.

For more information about EEO and the Law, please visit the following pages:

Equal Employment Opportunity is THE LAW

EEO is the Law Poster Supplement

Pay Transparency","Mid-Senior level","https://www.linkedin.com/jobs/view/associate-data-engineer-at-sumitomo-pharma-america-inc-3775100131?trk=public_jobs_topcard-title","United States","5 days ago","","","2023-11-29","","Pharmaceutical Manufacturing","Associate Data Engineer","Information Technology"
"Over 200 applicants","2701866","Core Group Resources","https://www.linkedin.com/company/coregroupresources?trk=public_jobs_topcard-org-name","Full-time","Data Engineer I Description

You will be an independent contributor on a fast-growing Data Engineering team pursuing a vision of analytics-driven mining at Freeport. Your expertise in data engineering and software engineering will enable and empower our organization to build and deploy data driven solutions to production. At Freeport we understand that our data does not reach its full potential until it is analyzed, and insights effectively communicated to the enterprise. You will work in close collaboration with mining operations, subject matter experts, data scientists, and software engineers to develop advanced, highly automated data products. You will be a champion of DataOps, and agile practices; actively participating in project teams to drive value.


 * Agile Project Work: Work in cross-functional, geographically distributed agile teams of highly skilled data engineers, software/machine learning engineers, data scientists, DevOps engineers, designers, product managers, technical delivery teams, and others to continuously innovate analytic solutions.
    * Design, develop, and review real-time/bulk data pipelines from a variety of sources (streaming data, APIs, data warehouse, messages, images, video, etc)
    * Follow established design patterns for data ingest, transformation, and egress
    * Develop documentation of Data Lineage and Data Dictionaries to create a broad awareness of the enterprise data model and its applications
    * Apply best practices within DataOps (Version Control, P.R. Based Development, Schema Change Control, CI/CD, Deployment Automation, Test Automation, Shift left on Security, Loosely Coupled Architectures, Monitoring, Proactive Notifications)

 * Problem Solving/Project Management: Constructively challenge while soliciting participation in problem solving to enrich possible solutions.
 * Architecture: Utilize modern cloud technologies and employ best practices from DevOps/DataOps to produce enterprise quality production Python and SQL code with minimal errors. Participate in regular code review sessions and collaboratively discuss opportunities for continuous improvement in all solutions.
 * Self-Development: Flexibly seek out new work or training opportunities to broaden experience. Independently research latest technologies and openly discuss applications within the department.
 * Perform other duties as requested.
   

Data Engineer I Qualifications


 * Bachelor’s degree in engineering, computer science, analytical field (Statistics, Mathematics, etc.) or related discipline and three (3) years of relevant work experience
   
   

OR


 * Master’s or Ph.D. in engineering, computer science, analytical field (Statistics, Mathematics, etc.) or related discipline and one (1) year of relevant work experience
 * Strong experience in at least two areas:
    * Knowledgeable Practitioner of SQL development with experience designing high quality, production SQL codebases
    * Knowledgeable Practitioner of Python development with experience designing high quality, production Python codebases
    * Knowledgeable Practitioner in data engineering, software engineering, and ML systems architecture
    * Knowledgeable Practitioner of data modeling
    * Experience applying software development best practices in data engineering projects, including Version Control, P.R. Based Development, Schema Change Control, CI/CD, Deployment Automation, Test Driven Development/Test Automation, Shift left on Security, Loosely Coupled Architectures, Monitoring, Proactive Notifications using Python and SQL
    * Data science experience wrangling data, model selection, model training, modeling validation, e.g., Operational Readiness Evaluator and Model Development and Assessment Framework, and deployment at scale
      

Preferred Qualifications


 * Working knowledge of Azure Stream Architectures, DBT, Schema Change tools, Data Dictionary tools, Azure Machine Learning Environment, GIS Data
 * Working knowledge of Software Engineering and Object Orient Programming Principles
 * Working knowledge of Distributed Parallel Processing Environments such as Spark or Snowflake
 * Working knowledge of problem solving/root cause analysis on Production workloads
 * Working knowledge of Agile, Scrum, and Kanban
 * Working knowledge of workflow orchestration using tools such as Airflow, Prefect, Dagster, or similar tooling
 * Working knowledge with CI/CD and automation tools like Jenkins or Azure DevOps
 * Experience with containerization tools such as Docker
 * Strong verbal and written communication skills in English language
   
   ","Entry level","https://www.linkedin.com/jobs/view/data-engineer-i-remote-at-core-group-resources-3743884470?trk=public_jobs_topcard-title","Phoenix, AZ","2 weeks ago","","","2023-11-14","","Staffing and Recruiting","Data Engineer I - Remote","Information Technology"
"Over 200 applicants","7947803","Juniper Square","https://www.linkedin.com/company/juniper-square?trk=public_jobs_topcard-org-name","Full-time","About Juniper Square

Our mission is to unlock the full potential of private markets. Privately owned assets like commercial real estate, private equity, and venture capital make up half of our financial ecosystem yet remain inaccessible to most people. We are digitizing these markets, and as a result, bringing efficiency, transparency, and access to one of the most productive corners of our financial ecosystem. If you care about making the world a better place by making markets work better through technology – all while contributing as a member of a values-driven organization – we want to hear from you.

Juniper Square offers employees a variety of ways to work, ranging from a fully remote experience to working full-time in one of our physical offices. We invest heavily indigital-firstoperations, allowing our teams to collaborate effectively across most US states, 2 Canadian Provinces, and Mexico. We also have physical offices in San Francisco, CA and Austin, TX, for employees who prefer to work in an office some or all of the time.

GP Experience

Juniper Square serves two sides of the private capital markets, the investment managers (GPs) and the investors (LPs). The GP eXperience team (i.e., GPX) is responsible for Juniper Square’s product offering for General Partners (GPs). This is our core product that enables all other innovation at Juniper Square as we unlock and improve the world’s private capital markets. Our platform handles billions of dollars of transactions each month and we are actively expanding into additional private asset classes such as Venture Capital& Private Equity. Come help us innovate in fundraising, reporting, asset-ownership mapping, and more.

The Team

The Data Engineering team is responsible for Data pipelines that serve multiple types of customers including internal Juniper Square users for Business Intelligence and GPs for Analytics on their data. We support the ability for these customers to create and manage their custom dashboards. We also support the ability for other Product Engineering teams to add metrics to track product usage for the features they launch into production.

About Your Role

Juniper Square is growing rapidly, and our data needs are growing even faster, so we’re growing our Data Engineering Team. As a Senior Data Engineer your role will be pivotal to evolving our existing data and reporting experiences. You’ll build out pipelines to gather data from multiple sources and make it available for analysis. You will shape both internal and external analytics products to help guide business-critical decisions, enhance their workflows, and improve decision-making.

What You’ll Do


 * Design and implement sophisticated data models in SQL.
 * Work closely with the other Software Engineers to ensure sound, scalable implementation.
 * Act as a technical expert on our team regarding all things data, especially as the data team grows and evolves.
 * Introduce new technologies to evolve and enhance our data pipeline capabilities.
 * Document data models, architectural decisions and data dictionaries to enable collaboration, maintainability and usability of our analytics platforms and code.
 * Assist with governance, guidance, code reviews, and access controls so that we maintain consistency, quality, and business confidentiality as we scale analytics access across the company and to customers.
 * Externally: learn our application data schema, and develop a fluency in how to transform it to enhance customer’s decision-making with data.
 * Internally: guide product and development teams, advising on instrumentation and laying development foundations for product usage reporting.
 * Fulfill projects with minimal guidance but with an appropriate sense of when and how to collaborate with others.
 * Build scalable, highly performant infrastructure for delivering clear business insights from a variety of raw data sources.
   
   

Qualifications


 * Bachelor's degree in Computer Science, or equivalent work experience
 * 5+ years of experience building ETL (Extraction Transform Load) or ELT (Extraction Load Transform) pipelines from scratch
 * Strong command of relational databases (Postgresql preferred), data modeling and database design
 * Strong command of Python and experience using Python for Data Pipelines
 * Experience with cloud based services (AWS RDS preferred)
 * Experience developing on (or administering) BI / data visualization platforms (ex. Looker, Tableau, PowerBI, Mode, Data Studio, Domo, QlikView etc.).
 * Basic understanding of data warehouses such as Amazon Redshift, Google BigQuery, Snowflake etc.
 * Demonstrated history of translating data into clear and actionable narratives and communicating opportunities and challenges relevant to stakeholders.
 * You must be flexible and adaptable—you will be operating in a fast-paced startup environment.
   
   

At Juniper Square, we believe building a diverse workforce and an inclusive culture makes us a better company. If you think this job sounds like a fit, we encourage you to apply even if you don’t meet all the qualifications.

Benefits

Compensation for this position includes a base salary, equity, and a variety of benefits. The U.S. base salary range for this role is $160,000 - $200,000 and the Canadian base salary range for this role is $200,000 to $250,000 CAD. Actual base salaries will be based on candidate-specific factors, including experience, skillset, and location, and local minimum pay requirements as applicable. Your recruiter can provide further details.


 * Competitive salary and meaningful equity
 * Health, dental, and vision care for you and your family
 * Unlimited vacation policy and paid holidays
 * Generous paid family leave, medical leave, and bereavement leave policies
 * 401k retirement savings plan
 * Healthcare FSA and commuter benefits programs
 * Freedom to customize your work and technology setup as you see fit
 * Professional development stipend
 * Monthly work from home wellness stipend while we're all remote
 * Mental wellness coverage including live coaching and therapy sessions
 * Home office productivity allowance to help create an ideal work from home setup
   
   

#Juniper-US

#Juniper-Canada

","Mid-Senior level","https://www.linkedin.com/jobs/view/senior-data-engineer-at-juniper-square-3705283923?trk=public_jobs_topcard-title","Chicago, IL","1 day ago","","","2023-12-03","","Software Development","Senior Data Engineer","Engineering"
"Over 200 applicants","1645","Harvard Medical School","https://www.linkedin.com/company/harvard-medical-school?trk=public_jobs_topcard-org-name","Full-time","Position Description

The Center for Computational Biomedicine (CCB) is a new center within the Blavatnik Institute at Harvard Medical School. Our mission is to provide cutting-edge computational capabilities, data analysis, and data integration technologies to support medical and biological research within the Medical School. Based at the Harvard Medical School Longwood Campus, we are part of a vibrant community of scientists, physicians, and engineers whose goal is to advance the boundaries of knowledge and improve patient care. The working environment combines the best features of a startup (fast pace, flexibility, flat hierarchies) with those of one of the leading medical schools (excellent benefits, outstanding opportunities for learning, great resources, name recognition).

CCB is looking for an individual to join the Data and Analytic Platforms Group, a group of engineers and scientists developing data warehousing and analytic solutions in support of epidemiology, healthcare economics, machine learning, and basic science research.

The Group works to reduce the burden on faculty by developing centrally managed and shareable data solutions to be used across research silos. We curate very large public and private healthcare utilization (insurance claims, electronic health record), multi-omics, environmental exposure, and social determinants data sets, provision access to those curated data sets, and develop analytic frameworks to accelerate reproducible academic research on top of them. Collectively these data sets contain information relating to hundreds of millions of patients.

This position reports to the Director of the CCB Data and Analytic Platforms Group. Primary responsibilities will include designing and implementing relational database architecture (schema, indexing, stored procedures, ETL processes, etc.) to warehouse multi-terabyte data sets in Microsoft SQL Server. This will include periodically evaluating various query performance metrics to ensure real-time availability to the research community and recommending modifications to the underlying database platform to resolve any identified issues. The bulk of this design work will be left up with the candidate, while a small portion will involve refactoring (or strategically deciding to abandon) existing ETL / indexing strategies. The data sets will be staged into a combination of proprietary schemas as well as the open-source i2b2 data model.

Additional opportunities will be available for the candidate to interact with individual scientific research teams to help improve their workflows.

Basic Qualifications



 * Minimum of seven years’ post-secondary education or relevant work experience
   
   

Additional Qualifications And Skills



 * Bachelor’s Degree in Computer Science or related degree preferred. At least 5 years experience as a software systems architect, including experience developing solutions with both relational database systems and at least one of the following languages: Java, Python, R.
 * Master’s Degree in a related field (Computer Science / Electrical Engineering, Bioinformatics, Statistics, Data Science, etc.) preferred.
 * Excellent communication skills, both written and oral
 * Experience with Microsoft SQL Server or cloud-based data warehousing technologies
 * Experience designing and maintaining multi-terabyte analytic relational databases, including index and query optimization
 * Experience orchestrating and optimizing Extract-Transform-Load (ETL) processes for multi- terabyte data warehouses
 * Comfort doing basic system administration in a Linux environment Comfort doing basic system administration in a Windows environment Experience with relational database index optimization
 * Experience with containerized (Docker or Singularity) workflows/paradigms
 * Experience with non-relational database systems (graph, key/value, document, array data stores) Experience with the R statistical computing platform
 * Experience with Java Experience with Python
 * Experience with high-performance computing
 * Comfort independently exploring distributed computing and database technologies and generating executive reports
 * Experience with public cloud platforms (AWS, Azure, Google Cloud)
   
   

Additional Information

This is a 12-month term appointment with the possibility of renewal contingent on funding.

The health of our workforce is a priority for Harvard University. With that in mind, we strongly encourage all employees to be up-to-date on CDC-recommended vaccines.

Please note that we are currently conducting a majority of interviews and onboarding remotely and virtually. We appreciate your understanding.

Harvard University offers an outstanding benefits package including:



 * Time Off: 3 - 4 weeks paid vacation, paid holiday break, 12 paid sick days, 12.5 paid holidays, and 3 paid personal days per year.
 * Medical/Dental/Vision: We offer a variety of excellent medical plans, dental & vision plans, all coverage begins as of your start date.
 * Retirement: University-funded retirement plan with full vesting after 3 years of service.
 * Tuition Assistance Program: Competitive tuition assistance program, incredibly affordable classes directly at the Harvard Extension School, and discounted options through participating Harvard grad schools.
 * Transportation: Harvard offers a 50% discounted MBTA pass as well as additional options to assist employees in their daily commute.
 * Wellness options: Harvard offers programs and classes at little or no cost, including stress management, massages, nutrition, meditation, and complementary health services.
 * Harvard access to athletic facilities, libraries, campus events, and many discounts throughout metro Boston.
   
   

The Harvard Medical School is not able to provide visa sponsorship for this position.

Not ready to apply? Join our Talent community to keep in touch and learn about future opportunities!

( https://www.gem.com/form?formID=16341e35-cbc6-4904-88a3-09b35763307e )

Job Function

Information Technology, Research

Department Office Location

USA - MA - Boston

Job Code

I1359P IT Data Architect Prof V

Work Format

Remote

Sub-Unit

Salary Grade

059

Department

Center for Computational Biomedicine

Union

00 - Non Union, Exempt or Temporary

Time Status

Full-time

Pre-Employment Screening

Criminal, Identity

Schedule

35 hrs. per week | Monday - Friday | 9:00 am - 5:00 pm

Commitment to Equity, Diversity, Inclusion, and Belonging

We are committed to cultivating an inclusive workplace culture of faculty, staff, and students with diverse backgrounds, styles, abilities, and motivations. We appreciate and leverage the capabilities, insights, and ideas of all individuals. Harvard Medical School Mission and Community Values

EEO Statement

We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, gender identity, sexual orientation, pregnancy and pregnancy-related conditions, or any other characteristic protected by law.","Not Applicable","https://www.linkedin.com/jobs/view/data-engineer-56664br-at-harvard-medical-school-3675208445?trk=public_jobs_topcard-title","Boston, MA","1 week ago","","","2023-11-21","","Higher Education","Data Engineer (56664BR)","Information Technology"
"Over 200 applicants","84180","Pluralsight","https://www.linkedin.com/company/pluralsight?trk=public_jobs_topcard-org-name","Full-time","Job Description

The Opportunity

Our Data Engineering team, within our Data Services Organization, builds and maintains the infrastructure essential to delivering high-volume, business-critical data to the organization to enable data-driven decisions.

We are focused on expanding our curated and modeled data that unify sources of truth across our multiple products and domains. You’ll have the opportunity and empowerment to guide the data engineering team on best practices using modern distributed data tools like Snowflake, Spark, Kafka, and dbt.

This is an ideal opportunity for someone that has strong opinions on how things should be done and loves figuring out what the right solution is for the scenario at hand. Your voice will be heard and will be given the opportunity to make an impact with the direction and delivery of our data platform to internal stakeholders.

Who You Are


 * 5+ years of experience designing and delivering data warehouses and marts to support business analytics
 * Expertise in streaming & real-time data processing using a technology like Spark, Kafka, ksqlDB, or Databricks, etc. and best practices on production deployment of these platforms
 * Strong foundation in SQL development on RDBMS (Snowflake and Postgres preferred)
 * Experience with dimensional data modeling/data workflow diagrams (conceptual, logical, and physical)
 * Experience with source control and deployment workflows for ETL (dbt, Fivetran, airflow, etc.)
 * Experience working with AWS services such as DynamoDB, Glue, Lambda, Step Functions, S3, CloudFormation
 * Hands on experience with scripting languages (Python, BASH, etc)
 * Experience with metadata management and data quality
 * Knowledge of software engineering best practices with experience with implementing CI/CD (Gitlab, Github Actions, Teamcity, etc.), monitoring & alerting for production systems
   
   

What You’ll Own


 * Data Warehousing and modeling delivery
 * Support and evolution of data environment to deliver high-quality data, speed, and availability
 * Curation of source-system data to deliver trusted data sets
 * Involvement on data cataloging and data management efforts
 * Production ETL performance tuning and environment-level resource consumption and management
 * Migration of POC pipelines to production data processes
   
   

Experience You’ll Need


 * Strong capability to manipulate and analyze complex, high-volume data from a variety of sources
 * Strong experience designing and building end-to-end data models and pipelines as well as alerting
 * Knowledge of data management fundamentals and data storage principles
 * Experience in data modeling for batch processing and streaming data feeds; structured and unstructured data
   
   

Working at Pluralsight

Founded in 2004 and trusted by Fortune 500 companies, Pluralsight is the technology skills platform organizations and individuals in 150+ countries count on to create progress for the world.

Our platform helps technologists master their craft and take control of their careers. We empower businesses everywhere to build adaptable teams, speed up release cycles and become scalable, reliable and secure. We come to work every day knowing we’re helping our customers build the skills that power innovation.

And we don’t let fear, egos or drama distract us from our mission. Our mission to advance the world's tech workforce is what drives us and our values are at the helm of how we work together. It’s our commitment to practicing them day in, day out that enables our performance. We’re adults, and we treat each other that way. We have the autonomy to do our jobs, transparency to eliminate office politics and trust each other to do the right thing. We thrive in an environment with creativity around every corner, challenges that keep us on our toes, and peers who inspire us to be the best we can be. We bring different viewpoints, backgrounds and experiences, and united by our mission, we are one.

Bring yourself. Pluralsight is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age or veteran status.

","Mid-Senior level","https://www.linkedin.com/jobs/view/sr-data-engineer-at-pluralsight-3762297865?trk=public_jobs_topcard-title","United States","3 days ago","Spencer Winegar","https://www.linkedin.com/in/spencer-winegar-3a43412","2023-12-01","","E-Learning Providers","Sr Data Engineer","Information Technology"
"127 applicants","1441","Google","https://www.linkedin.com/company/google?trk=public_jobs_topcard-org-name","Full-time","Note: Google’s hybrid workplace includes remote roles. By applying to this position you will have an opportunity to share your preferred working location from the following:

Remote locations: Indiana, USA; Michigan, USA; Ohio, USA; Pennsylvania, USA.Minimum qualifications:


 * Bachelor's degree in Electrical Engineering, Power Engineering, a related technical field, or equivalent practical experience.
 * 5 years of experience in mission critical facility design and construction.
 * Experience in design, construction, and commissioning of medium or low voltage electrical distribution systems, AC/DC systems, and associated power management or SCADA tools.
   
   

Preferred qualifications:


 * Professional Engineering (PE) license.
 * Experience in mission critical facilities and their electrical/mechanical infrastructure.
 * Experience in Estimating, Electrical design, Operation and Commissioning of substations, switchgear, ATP/ATS, emergency power systems and their control systems, power monitoring, and electrical protection.
   
   

About The Job

Our thirst for technology is a part of everything we do. The Data Center Engineering team takes the physical design of our data centers into the future. Our lab mirrors a research and development department -- cutting-edge strategies are born, tested and tested again. Along with a team of great minds, you take on complex topics like how we use power or how to run state-of-the-art, environmentally-friendly facilities. You're a visionary who optimizes for efficiencies and never stops seeking improvements -- even small changes that can make a huge impact. You generate ideas, communicate recommendations to senior-level executives and drive implementation alongside facilities technicians.

With your technical expertise, you ensure compliance with codes and standards, develop infrastructure improvements and serve as an expert in your specialty (e.g., cooling, electrical).

In this role, you will work with electrical and mechanical engineers, provide project design and field engineering services, project implementation, and participation in all phases of a project life-cycle. You will act as the interface between internal customers and the project team. You will be Involved in all the site capital projects from construction to modification of existing infrastructures in participation and preparation of all types of documents including, Statement of Work (SOW), Total Cost of Ownership (TCO) analysis, drawing markups, budget, schedule, final startup/commissioning reports, and review and acceptance of as-builts and review of submittals.

Behind everything our users see online is the architecture built by the Technical Infrastructure team to keep it running. From developing and maintaining our data centers to building the next generation of Google platforms, we make Google's product portfolio possible. We're proud to be our engineers' engineers and love voiding warranties by taking things apart so we can rebuild them. We keep our networks up and running, ensuring our users have the best and fastest experience possible.

The US base salary range for this full-time position is $136,000-$203,000 + bonus + equity + benefits. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process.

Please note that the compensation details listed in US role postings reflect the base salary only, and do not include bonus, equity, or benefits. Learn more about benefits at Google .

Responsibilities


 * Provide technical support to Data Center Services and Operations teams to define electrical system design requirements for multiple data center projects from inception through completion.
 * Develop, implement, and manage the data center electrical designs at site starting from basis of design to issued for construction data center services documents for new data center projects build outs, infrastructure upgrades, and renovations.
 * Respond to site specific engineering Requests For Information (RFI) in coordination with the Engineer of Record (EOR).
 * Collaborate with the core Engineering team to provide site specific requirements during the development of specific Basis of Design (BOD) and coordinate system level schematics with EOR.
 * Own and manage site-level power system issues during the project execution phase. Identify and resolve issues with cross-functional teams, maintain all data center related electrical system design requirements and interface documents.
   
   
   

Google is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. See also Google's EEO Policy and EEO is the Law. If you have a disability or special need that requires accommodation, please let us know by completing our Accommodations for Applicants form .","Not Applicable","https://www.linkedin.com/jobs/view/data-center-electrical-engineer-google-data-centers-at-google-3760575740?trk=public_jobs_topcard-title","Ohio, United States","3 days ago","","","2023-12-01","","Information Services and Technology, Information and Internet","Data Center Electrical Engineer, Google Data Centers","Information Technology and Engineering"
"Over 200 applicants","165158","Netflix","https://www.linkedin.com/company/netflix?trk=public_jobs_topcard-org-name","Full-time","Our cross-platform team delivers novel product features on Netflix, including the flagship TV application. Our work helps members discover and connect with stories they'll love. Your work will delight millions of customers worldwide on hundreds of different TV devices, from game consoles to smart TVs to cable boxes.

Our responsibilities include working on large-scale A/B tests of features - ranging from highly visible product experiences to core infrastructure that enables existing and future innovations. We care deeply about UI performance, quality, and accessibility of the Netflix TV experience. To bring new kinds of engaging and cinematic experiences to life, we partner closely with many cross-functional teams including product management, experience design, and creative production.

We are looking for an experienced Web/Javascript Software Engineer to drive innovations in the TVUI member experiences space. People who excel on our team are growth-oriented, intensely curious, selfless, and collaborative.

What you’ll be doing



 * Build, improve and optimize experiment-driven product features on the TV app that pushes the boundaries of what’s possible on performance and memory constrained devices.
 * Almost all of your code will be TypeScript and JavaScript (React-based), building on top of our purpose-built TV platform.
 * Partner with engineering teams, designers and product managers to define and refine concepts that will shape the future of entertainment on TV-connected devices.
 * Collaborate with our device platform and backend service teams to create functionality to support your projects.
 * Effective at developing strong relationships with cross-functional teams through clear communication.
   
   
   

What sets you apart?



 * Self-starter comfortable with identifying opportunities, seeking feedback and context to drive forward-looking technical solutions
 * Effective at developing strong relationships with cross-functional teams through clear communication, curiosity, and selflessness
 * Taking a thoughtful, practical approach to problem-solving that considers tradeoffs and avoids over-engineering
 * Experience building product experiences for memory and performance-constrained devices
 * Expertise in TypeScript/JavaScript, and can navigate different languages and technology stacks (eg. Node.js, GraphQL, etc)
 * Deeply cares about the quality & performance of the product shipped
   
   
   

Why Netflix?



 * Work with other high-performing, inclusive, and supportive engineers who are invested in each other’s growth
 * Solve technical challenges and deploy solutions to serve the best-in-class streaming experience to millions of users worldwide
 * Help evolve the way people watch content online and connect with Netflix
 * The Netflix culture. Not just a memo, but something we practice daily
 * Join our team of product-minded engineers and be part of the streaming revolution. For decades we’ve set the standard, and we’re just getting started.
   
   
   

At Netflix, we carefully evaluate a wide range of compensation factors to arrive at your personal top of market. We rely on market indicators to guide compensation and consider your specific job, skills, and experience to get it right. These considerations can cause your compensation to vary and will also be dependent on your location.

The overall market range for roles in this area of Netflix is typically $300,000 - $900,000.

This market range is based on total compensation (vs. only base salary), which is in line with our compensation philosophy. Netflix is a unique culture and environment. Learn more here.

We are an equal opportunity employer and celebrate diversity, recognizing that diversity of thought and background builds stronger teams. We approach diversity and inclusion seriously and thoughtfully. We do not discriminate on the basis of race, religion, color, ancestry, national origin, caste, sex, sexual orientation, gender, gender identity or expression, age, disability, medical condition, pregnancy, genetic makeup, marital status, or military service.","Not Applicable","https://www.linkedin.com/jobs/view/software-engineer-l5-tv-user-interface-at-netflix-3760644622?trk=public_jobs_topcard-title","United States","5 days ago","","","2023-11-29","","Movies, Videos, and Sound, Technology, Information and Internet, and Entertainment Providers","Software Engineer (L5) - TV User Interface","Engineering and Information Technology"
"Over 200 applicants","16159170","Data Ideology, LLC","https://www.linkedin.com/company/data-ideology-llc?trk=public_jobs_topcard-org-name","Full-time","Data Engineer -- REMOTE



Data Ideology

At DI, we provide Data & Analytics expertise to drive measurable business outcomes, often solving complex business problems for our clients. Our data analytics advisory services enable our customers to transform data into insights by driving a culture of empowerment and ownership of results. Our team consists of highly motivated individuals who are passionate about learning, understanding, collaborating, and who are intellectually curious.  For more information about Data Ideology visit www.dataideology.com .



Data Engineer - Full-time (FT)

We are looking for a Data Engineer to join our growing team. Data Engineer will leverage their business and technical knowledge to develop production ready data models by integrating multiple sources of data while also working with business and technical teams to understand business strategy and objectives, gather information, and ensure business requirements are being fulfilled throughout the entire data & analytics lifecycle.

 

Key Responsibilities



To perform in this position successfully, an individual must be able to perform each essential duty satisfactorily.  Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.  Other duties may be assigned to meet business needs. 

 * Ability to collect and understand business requirements and translate those requirements into an actionable data warehouse plan.
 * Knowledge of multi-dimensional and tabular design patterns and ability to identify solutions that leverage these modeling techniques.
 * Ability to work within the SDLC framework in multiple environments and understand the complexities and dependencies of the data warehouse build within those constraints.
 * Ability to define and implement best practices across database design and ETL.
 * Ability to direct the work of others, including but not limited to directing ETL development demonstrating an understand key concepts of ETL/ELT including best practices for optimization and scheduling.

 

Supervisory Responsibilities:  None 

 

Qualifications

Education and Experience: 

 * Proven understanding of data warehousing, Data Architecture, and BI.
 * Experience with data pipelines and architecture/engineering.
 * Knowledge of modern apps and data platforms.
 * Cloud based project implementation.
 * SnowflakeDB experience a plus.

Knowledge, Skills and Abilities: 

 * BI/Data Warehousing (3+ years)
 * Cloud platforms (1+ years)
 * ETL (3+ years)
 * SQL/ SSIS (3+ years)

Work Environment:

 * Remote work from home.
 * Hours of work and days are generally Monday through Friday. Specific business hours will depend on client needs.

Physical Demands:

 * Must be able to remain in a stationary position 50% of the time.
 * The person in this position must occasionally move about inside the office to access file cabinets, library stacks, office machinery, etc.
 * Constantly operates a computer and other office productivity machinery, such as a calculator, copy machine, and printer.
 * The person in this position frequently communicates with clients and coworkers. Must be able to exchange accurate information in these situations.

Benefits:  

 * Unlimited Discretionary Time Off Policy
 * Insurance (medical, dental, vision) for employees
 * 100% company paid - short and long-term disability insurance for employees
 * 100% company paid - life insurance and AD&D insurance for employees
 * 100% company paid – employee assistance program
 * Retirement plans with company match
 * Training and Certification Reimbursement annually
 * Performance-based incentive program
 * Commission incentive program
 * Profit Sharing Plan
 * Referral Bonuses



Data Ideology is an EEO Employer","Entry level","https://www.linkedin.com/jobs/view/data-engineer-at-data-ideology-llc-3736285599?trk=public_jobs_topcard-title","Pittsburgh, PA","1 month ago","","","2023-10-16","","IT Services and IT Consulting","Data Engineer","Information Technology"
"199 applicants","2328049","Balsam Brands","https://www.linkedin.com/company/balsam-brands?trk=public_jobs_topcard-org-name","Full-time","Job Description

As Data Engineer, you will be responsible for designing and developing robust and scalable data warehousing solutions. The Data Engineer will be responsible for building data solutions based on the business requirements. Data solutions may involve retrieval, transformation, storage, and delivery of the data. The Data Engineer must follow standards and implement best practices while writing code and provide production support for the enterprise data warehouse. Our ideal candidate is a skillful data wrangler who enjoys building data solutions from the ground up and optimizing their performance.

This full-time position reports to the Manager of Data Engineering and can work remote from any U.S. state where Balsam Brands is currently setup as an employer, which includes: CA, CO, FL, GA, ID, IL, IN, KS, KY, MD, MA, MO, NJ, NC, OH, OR, PA, TN, TX, VA, and WA. This role can also work locally in our Redwood City, CA or Boise, ID office location. Our local teams work in a hybrid model, which currently includes Tuesday and Wednesday in-office.

To ensure sufficient overlap with functional and cross-functional team members globally, some flexibility with this role's regular work schedule will be required. Most of our teams have overlap with early morning and/or early evening PST. Specific scheduling needs for this role will be discussed in the initial interview.

What You’ll Do


 * Be accountable for building and maintaining the data infrastructure for the organization
 * Collaborate with systems analysts and cross functional partners to understand data requirements
 * Champion data warehouse, create denormalized data foundation layer and normalized data marts
 * Define strategies to capture all data sources and impact of business process changes on data coming from those sources
 * Work on all aspects of the data warehouse/BI environment including architecture, design, development, automation, caching and performance tuning
 * Continually explore new technologies like Big Data, Artificial Intelligence, Generative AI, Machine Learning, and Predictive Data Modeling
   
   
   

What You Bring To The Table


 * 5+ years of professional experience in the data engineering field
 * Demonstrated history of designing and building schemas, tables, views, and data pipelines
 * Experience in cloud technologies like Azure, AWS
 * Experience in Azure Data Factory (ADF) or equivalent ETL tool
 * Knowledge and experience of working with SQL and relational databases like SQL Server, Oracle, Postgres and MySQL
 * Ability to understand and tell the story embedded in the data at the core of our business
 * Ability to communicate with non-technical audience from a variety of business functions
 * Strong knowledge of coding standards, best practices and data governance
   
   
   

Travel for remote team members: At Balsam Brands, we believe that time spent together, in-person, collaborating and building relationships is important to who we are. For our newest remote Brandits, we will arrange travel to one of our local offices within your first three months of employment so you can meet and train with your new team in-person. You may also get to travel an additional 1 – 2 times a year for events such as team retreats, offsites, or learning and development opportunities.

Notes: This is a full-time, permanent position with benefits. Please submit a cover letter and resume, and only apply if you are able to live and work full-time in one of the states listed in this posting. State locations and specifics are subject to change as our hiring requirements shift.

About Us: Balsam Brands is a global, eCommerce retailer with roots in holiday and home décor. We strive for excellence in everything we do and present a unique opportunity for those seeking to have a meaningful impact in a people-first company that values relationship building, authenticity, and doing the right thing. We have steadily growing teams in Boise, the Bay Area, Dublin, the Philippines - and most recently, Windsor, Canada!

The company's mission is to create joy together. We empower our team and partners to love what they do, provide products and experiences that inspire meaningful moments with family and friends, and give back to our families and communities in impactful ways. When you join Balsam Brands, you'll find a culture of caring people doing challenging work and building a welcoming workplace.


 * Check out our flagship brand, Balsam Hill: www.balsamhill.com
 * Balsam Brands in Forbes: https://bit.ly/balsambrandsforbes
 * Balsam Brands on LinkedIn: http://www.linkedin.com/company/balsam-brands/
 * Glassdoor: https://bit.ly/balsambrands-glassdoor
   
   
   

Benefits

At Balsam Brands, we strive to offer a competitive compensation and benefits package. For permanent, full-time team members, our current package includes:


 * Competitive compensation, including a cash-based incentive plan; salary is reviewed yearly and may be adjusted as part of the normal compensation review process
 * Comprehensive Medical, Dental, and Vision coverage, with 100% of monthly premiums covered for team members, and 85%+ employer-paid premiums for other coverage tiers that include dependents
 * Up to $2,000 annual funding toward HSA accounts
 * Medical, transit, dependent care FSA
 * Infertility coverage offered on all medical plans
 * Generous parental leave program and flexible return options
 * Company-paid life and AD&D insurance
 * Company-paid short and long-term disability insurance
 * 401(k) with dollar-for-dollar company match up to $4,000 per calendar year
 * Employee Assistance Program (EAP) and other mental health and wellness perks
 * Paid holidays, annual shutdown week, PTO, and volunteer time-off (VTO) packages
 * Paid 5-week sabbatical leave after 10 years of employment
 * Annual continuous learning benefit up to $1,000 per person, per fiscal year
 * Up to $300 flexible reimbursement to support setup of new team member's work-from-home environment
 * Generous team member merchandise discount
 * Valuable extras: identity theft protection, subsidized parking, monthly wellness, pet insurance, accident & critical illness insurance
   
   
   

The base pay range for this position is: $129,000 to $162,000. Where an individual falls within that range will vary based on several factors including geographic location and may vary depending on candidate qualifications and experience, applicable skills, and other job-related factors. We benchmark our pay ranges against current external data sources and regularly review compensation for our team members. Balsam Brands is committed to providing our team members with an internally fair, externally competitive, and fiscally prudent total compensation package administered in a simple and consistent manner.

At Balsam Brands, we strive to build a diverse, equitable, and inclusive team to fulfill our purpose to create joy together. Balsam Brands is proud to be an equal opportunity employer. We encourage people from all backgrounds, ages, abilities, and experiences to apply. We do not discriminate on the basis of race, ethnicity, religion, national origin, citizenship, marital or family status, disability, sexual orientation, gender identity or expression, pregnancy or caregiver status, veteran status, or any other legally protected status. We will ensure that individuals with disabilities are provided reasonable accommodations to participate in the job application and interview process, to perform essential job functions, and to receive other benefits and privileges of employment.

#DICE

Additional Information

All your information will be kept confidential according to EEO guidelines.","Mid-Senior level","https://www.linkedin.com/jobs/view/data-engineer-remote-option-at-balsam-brands-3762697303?trk=public_jobs_topcard-title","Redwood City, CA","1 week ago","","","2023-11-20","","Retail","Data Engineer (Remote Option)","Engineering"
"Over 200 applicants","234625","Zscaler","https://www.linkedin.com/company/zscaler?trk=public_jobs_topcard-org-name","Full-time","About Zscaler

Zscaler (NASDAQ: ZS) accelerates digital transformation so that customers can be more agile, efficient, resilient, and secure. The Zscaler Zero Trust Exchange is the company’s cloud-native platform that protects thousands of customers from cyberattacks and data loss by securely connecting users, devices, and applications in any location.

With more than 10 years of experience developing, operating, and scaling the cloud, Zscaler serves thousands of enterprise customers around the world, including 450 of the Forbes Global 2000 organizations. In addition to protecting customers from damaging threats, such as ransomware and data exfiltration, it helps them slash costs, reduce complexity, and improve the user experience by eliminating stacks of latency-creating gateway appliances.

Zscaler was founded in 2007 with a mission to make the cloud a safe place to do business and a more enjoyable experience for enterprise users. Zscaler’s purpose-built security platform puts a company’s defenses and controls where the connections occur—the internet—so that every connection is fast and secure, no matter how or where users connect or where their applications and workloads reside.

Position: Data Engineer

Location: Remote within United States

Responsibilities/What You’ll Do


 * Collaborate with Data & Technical architects, integration and engineering teams to capture inbound/outbound data pipeline requirements, conceptualize and develop solutions.
 * Support the evaluation and implementation of the current and future data applications/technologies to support the evolving Zscaler business needs.
 * Collaborate with IT business engagement & applications engineer teams, enterprise data engineering and business data partner teams to identify data source requirements.
 * Profile and quantify quality of data sources, develop tools to prepare data and build data pipelines for integrating into Zscaler’s data warehouse in Snowflake.
 * Continuously optimize existing data integrations, data models and views while developing new features and capabilities to meet our business partners needs.
 * Work with Data Platform Lead to design and implement data management standards and best practices.
 * Continue to learn and develop next generation technology/ data capabilities that enhance our data engineering solutions.
 * Develop large scale and mission-critical data pipelines using modern cloud and big data architectures.
   
   
   

Qualifications/Your Background:


 * 3 - 5 years of experience in data warehouse design & development.
 * Proficiency in building data pipelines to integrate business applications (salesforce, Netsuite, Google Analytics etc) with Snowflake
 * Must have proficiency in data modeling techniques (Dimensional) – able to write structured and efficient queries on large data sets
 * Must have hands-on experience in Python to extract data from APIs, build data pipelines.
 * Completely proficient in advanced SQL, Python/Snowpark(PySpark)/Scala (any Object Oriented language Concepts), ML libraries.
 * Strong hands-on experience in ELT Tools like Matillion, Fivetran, Talend, IDMC (Matillion preferred) , data transformational tool – DBT and in using AWS services like EC2, s3, lambda, glue.
 * Solid understanding of CI/CD process, git versioning, & advanced snowflake concepts like warehouse optimizations, SQL tuning/pruning
 * Experience in using data orchestration workflows using open-source tools like Apache Airflow, Prefect
 * Knowledge of data visualization tools such as Tableau, and/or Power BI
 * Must demonstrate good analytical skills, should be detail-oriented, team-player and must have ability to manage multiple projects simultaneously.
   
   
   

Zscaler’s salary ranges are benchmarked and are determined by role and level. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations and could be higher or lower based on a multitude of factors, including job-related skills, experience, and relevant education or training.

The base salary range listed for this full-time position excludes commission/ bonus/ equity (if applicable) + benefits.

Base Pay Range

$110,000 — $135,000 USD

By applying for this role, you adhere to applicable laws, regulations, and Zscaler policies, including those related to security and privacy standards and guidelines.

Zscaler is proud to be an equal opportunity and affirmative action employer. We celebrate diversity and are committed to creating an inclusive environment for all of our employees. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex (including pregnancy or related medical conditions), age, national origin, sexual orientation, gender identity or expression, genetic information, disability status, protected veteran status or any other characteristics protected by federal, state, or local laws.

See more information by clicking on the Know Your Rights: Workplace Discrimination is Illegal link.

Pay Transparency

Zscaler complies with all applicable federal, state, and local pay transparency rules. For additional information about the federal requirements, click here .

Zscaler is committed to providing reasonable support (called accommodations or adjustments) in our recruiting processes for candidates who are differently abled, have long term conditions, mental health conditions or sincerely held religious beliefs, or who are neurodivergent or require pregnancy-related support.

","Entry level","https://www.linkedin.com/jobs/view/data-engineer-at-zscaler-3737060329?trk=public_jobs_topcard-title","San Jose, CA","1 week ago","Yogesh Chhabra,CPHR","https://ca.linkedin.com/in/yogeshchhabra","2023-11-22","","Computer and Network Security","Data Engineer","Information Technology"
"Over 200 applicants","11046773","Plexus Resource Solutions","https://uk.linkedin.com/company/plexus-resource-solutions?trk=public_jobs_topcard-org-name","Full-time","This role requires experience in the Web3/Cryptocurrency/Blockchain Field!




Data Engineer (Python) 🚀🚀

Cryptoasset industry

$100k - $145k USD

Remote - US Based







Company Background:




Our client is a pioneering company in the crypto/blockchain industry, leveraging data to drive innovation and provide actionable insights. Their mission is to empower institutions, asset managers, and industry stakeholders with transparent and comprehensive crypto asset data.



As a Python Data Engineer in our organization, you will play a crucial role in supporting network data products within the crypto/blockchain space. This role offers the opportunity to work with talented individuals passionate about decentralized economies, contributing to groundbreaking research, and creating data-driven solutions that impact the future of finance.



Role:



 * Research emerging fields in network data, focusing on data collection and monitoring.
 * Interpret raw node data for infrastructure applications and data exporters.
 * Contribute to documentation and assist clients with custom research and data requests.
 * Work with external clients or vendors to solve problems and acquire custom data.



Requirements:



 * 3+ years of experience in a data engineering/science role
 * Proficiency in Python and associated frameworks
 * Professional experience in the blockchain/crypto space is a must
 * Solid understanding of blockchain operations and underlying data structures.
 * Proficiency in SQL, preferably PostgreSQL.



Benefits:

 * $100k - $145k USD
 * Tokens
 * Equity
 * Fully remote with flexible working hours (US Based)





If you're interested apply below 🚀🚀","Mid-Senior level","https://www.linkedin.com/jobs/view/data-engineer-at-plexus-resource-solutions-3765433006?trk=public_jobs_topcard-title","New York, NY","2 weeks ago","Saam S.","https://uk.linkedin.com/in/saamsadeghian","2023-11-20","$100,000.00-$145,000.00","Capital Markets, Software Development, and Financial Services","Data Engineer","Information Technology"
"Over 200 applicants","1124131","Pinterest","https://www.linkedin.com/company/pinterest?trk=public_jobs_topcard-org-name","Full-time","About Pinterest:

Millions of people across the world come to Pinterest to find new ideas every day. It’s where they get inspiration, dream about new possibilities and plan for what matters most. Our mission is to help those people find their inspiration and create a life they love. In your role, you’ll be challenged to take on work that upholds this mission and pushes Pinterest forward. You’ll grow as a person and leader in your field, all the while helping Pinners make their lives better in the positive corner of the internet.

Creating a life you love also means finding a career that celebrates the unique perspectives and experiences that you bring. As you read through the expectations of the position, consider how your skills and experiences may complement the responsibilities of the role. We encourage you to think through your relevant and transferable skills from prior experiences.

Our new progressive work model is called PinFlex, a term that’s uniquely Pinterest to describe our flexible approach to living and working. Visit our PinFlex landing page to learn more.

The Enterprise Data Platform team is looking for a Senior Data Engineer who has experience with Building a Data Platform on top of a Cloud Data Warehouse. You will be working closely with privacy and security teams to improve security and privacy of Data in Snowflake. In this role you will also advise other Data Analysts and Data Engineers from the Business teams at Pinterest on the best practices for handling data and deriving insights using Snowflake. You will gradually become the Subject Matter Expert on Snowflake and also help build out the Modern Data Stack at Pinterest.

What You'll Do:


 * Build Scripts in Python to Implement privacy and security requirements on Snowflake
 * Build Airflow operators in python to ingest data and trigger transformations on Snowflake
 * Define and advocate the best practices for storing and analyzing data inside Snowflake
 * Become a Subject Matter Expert and advocate for Enterprise Data Platform
   
   
   

What We're Looking For:


 * Strong skills in Python and SQL
 * Experience with Building scripts in python for managing Data Warehouse
 * Experience with any Cloud Data Warehouse either as an Administrator or a Developer
 * Experience working on privacy and security requirements of a Cloud Data Warehouse
   
   
   

This position is not eligible for relocation assistance.

At Pinterest we believe the workplace should be equitable, inclusive, and inspiring for every employee. In an effort to provide greater transparency, we are sharing the base salary range for this position. The position is also eligible for equity. Final salary is based on a number of factors including location, travel, relevant prior experience, or particular skills and expertise.

Information regarding the culture at Pinterest and benefits available for this position can be found here.

US based applicants only

$114,750—$236,000 USD

Our Commitment To Diversity:

Pinterest is an equal opportunity employer and makes employment decisions on the basis of merit. We want to have the best qualified people in every job. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other characteristic under federal, state, or local law. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you require an accommodation during the job application process, please notify accessibility@pinterest.com for support.

Our Commitment To Diversity:

Pinterest is an equal opportunity employer and makes employment decisions on the basis of merit. We want to have the best qualified people in every job. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other characteristic under federal, state, or local law. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you require an accommodation during the job application process, please notify accessibility@pinterest.com for support.

","Mid-Senior level","https://www.linkedin.com/jobs/view/senior-data-engineer-at-pinterest-3725735255?trk=public_jobs_topcard-title","California, United States","1 week ago","","","2023-11-24","","IT Services and IT Consulting, Software Development, and Technology, Information and Internet","Senior Data Engineer","Information Technology"
"Be among the first 25 applicants","2199951","Searchspring","https://www.linkedin.com/company/searchspring?trk=public_jobs_topcard-org-name","Full-time","Who We Are

Searchspring provides ecommerce retailers with the industry leading software platform for site search, product merchandising, and personalization. With offices in San Antonio, Denver, Colorado Springs, Portland, Toronto, and remote across the globe, Searchspring employees are dedicated to fostering an environment that enables everyone to thrive, both personally and professionally, and feel supported, engaged, and valued, every step of the way.

Backed by a growth equity firm with extensive resources and expertise in helping software and technology companies navigate transformational growth, Searchspring helps our customers, like PuraVida, Fabletics, SKIMS, West Elm, Specialized, and RipCurl increase cart size, conversion, and repeat customers. Join us as we change the way the world shops online and achieve our mission to deliver the ultimate shopper experience.

Who You Are


 * You strive to be the best and are known to go above and beyond.
 * You are open minded, always searching for solutions and don’t wait for problems to find you.
 * You are scrappy, resourceful and thoughtful when finding answers.
 * You are a team player who possesses high levels of emotional intelligence that allows you to work collaboratively with the team and clients.
   
   

What You’ll Do

As a Software Engineer, you play a critical role in the design, development, and delivery of complex software solutions that power our product and business operations at Searchspring. You utilize your extensive technical and subject matter expertise to drive the success of our projects and mentor more junior team members. Your passion for software development drives the decisions you make in your work, and the ideas you contribute in discussions strongly influence the quality of your team’s output and the process by which it operates.

This role is within the team responsible for the core of our platform, which is focused on the systems that index our customers’ product catalog data and the APIs that power their site experience by serving query requests for that data. The ideal candidate will have experience building and maintaining such systems in a distributed cloud environment, as well as solving such business problems in the technical e-commerce space.

How You Will Succeed


 * Collaborating with members of Engineering and Product to gather and analyze requirements, and translate them into deliverable technical solutions,
 * Architecting, implementing, deploying, and maintaining software solutions that are scalable and performant; assisting in leading the design and development thereof
 * Demonstrating ownership over your team’s technical contributions by ensuring a high standard of quality and accuracy
 * Driving productivity via impactful technical contributions, technical guidance, and mentorship to junior team members, fostering their professional growth
 * Conducting meaningful code reviews, identifying areas of improvement, and suggesting architectural enhancements
 * Troubleshooting complex issues and performing root cause analyses
 * Assisting in live production issues and fire calls where necessary
 * Actively participating in team processes, including planning, estimation, and retrospective meetings
 * Staying up to date with industry trends and evaluating emerging technologies to ensure the use of the most suitable tools and techniques; sharing that knowledge within the local community
   
   

What We’re Looking For


 * Bachelor's or Master's degree in Computer Science, or equivalent work experience
 * 5+ years of professional experience in software development, with a pronounced
 * emphasis on backend development; experience in e-commerce search preferred
 * Extensive proficiency in programming languages, with emphasis on development in Golang
 * Experience with advanced engineering using Elasticsearch; additional Elastic training and/or credentials preferred
 * Experience with development and cloud deployment of containerized microservices; experience with AWS and Kubernetes preferred.
 * Strong understanding of software architecture, design patterns, and best practices
 * Proven track record of delivering complex software projects on time and with high quality.
 * Exceptional problem-solving skills and ability to analyze and resolve technical challenges
 * Strong communication skills, with the ability to collaborate effectively with cross-functional teams
 * Demonstrated leadership abilities, including mentoring and guiding junior team members.
 * Strong commitment to continuous learning, self-improvement, and staying up to date with technology advancements
   
   

Benefits And Company Perks


 * Open PTO
 * Company-paid health, dental, and vision insurance
 * Medical and dependent care flexible spending accounts (FSA)
 * Company-paid Short Term and Long Term Disability coverage
 * Company-paid Life and AD&D coverage
 * Voluntary benefits, including critical illness, accident insurance, legal services, and pet insurance
 * 401(k) plan with employer match
 * $1000 Continuing education stipend
 * $500 Charitable donation matching
 * Flexible work environment. Work from one of our offices, hybrid, or remote.
   
   

Salary Range (if applicable for the role)

Base salary ranges from $XX,XXX-$XX,XXX (include a brief bonus or variable compensation if applicable)

Actual compensation is influenced by a wide array of factors including, but not limited to, skills, experience and specific work location.

Searchspring is an equal-opportunity employer and proud to foster a workplace free from discrimination and harassment. We strongly believe that diverse backgrounds, experiences, and perspectives are essential in cultivating an inclusive culture and building an innovative, successful organization. All qualified applicants are considered for employment, without regard to age, race, color, religion, sex, national origin, disability, veteran status, sexual orientation, gender identity, or any other protected status. If you require accommodation during the application process, please don't hesitate to contact us.

Powered by JazzHR

EBKfWSIYVv","Mid-Senior level","https://www.linkedin.com/jobs/view/software-engineer-at-searchspring-3775694712?trk=public_jobs_topcard-title","Denver City, TX","2 weeks ago","","","2023-11-16","","Internet Publishing","Software Engineer","Engineering and Information Technology"
"Over 200 applicants","2987720","BigRio","https://www.linkedin.com/company/bigrio?trk=public_jobs_topcard-org-name","Full-time","Data Engineer

Boston, MA- Remote

Contract to Hire

Must hold or be eligible for Public Trust




About BigRio:

BigRio is a remote technology consulting firm headquartered in Boston. We deliver a range of solutions including custom machine learning/AI integrations and data warehousing and processing solutions. Our comprehensive approach serves clients from a variety of industries as a result of our ability to consistently, and quickly deliver cutting-edge and cost-conscious software solutions.

You will join our client's team as a Data Engineer who will work with our clients.




Position Summary:

As a Data Engineer, you will be responsible for developing and maintaining data pipelines using cloud technologies, architecting modern data warehousing platforms, and ensuring the efficient flow of data across the organization. Your expertise in cloud services, data engineering, and collaboration with offshore teams will be instrumental in achieving our data-driven goals.




Key Responsibilities:

 * Develop and maintain data pipelines using cloud technologies, with a minimum of 7 years of experience.
 * Architect and optimize modern data warehousing platforms using technologies such as Snowflake or Redshift (5+ years of experience).
 * Utilize cloud services, including S3, Step Functions, Glue, and Airflow, to streamline data processes.
 * Proficient in Python for data transfers and extractions (ETL and ELT).
 * Develop and deploy ETL solutions, including tools like Informatica or similar technologies.
 * Collaborate within an agile development process (Scrum, Kanban, etc.).
 * Familiarity with CI/CD concepts and implementation.
 * Create comprehensive technical documentation to ensure knowledge sharing and transfer.
 * Utilize Airflow and DAG development to manage workflow orchestration.
 * Exhibit proficiency in BI and data analysis, contributing to end-to-end development in data platform environments.
 * Proactively identify and address potential issues to prevent data pipeline disruptions.
 * Write high-quality, fully tested code to build ETL/ELT data pipelines on cloud platforms.
 * Coordinate and collaborate with offshore teams, demonstrating prior experience in the onsite-offshore model.




Qualifications:

 * Bachelor's degree in Computer Science, Engineering, or a related field (Master's preferred).
 * 7+ years of experience in data engineering and pipeline development.
 * 5+ years of experience in modern data warehousing architecture, specifically with Snowflake or Redshift.
 * Strong knowledge of cloud services, including S3, Step Functions, Glue, and Airflow.
 * Proficiency in Python for data transfers and extractions.
 * Experience in ETL development and deployment, including tools like Informatica.
 * Familiarity with agile development methodologies and CI/CD concepts.
 * Demonstrated ability to create clear and comprehensive technical documentation.
 * Experience with Airflow and DAG development.
 * Strong BI and data analysis capabilities.
 * Proven track record of proactively addressing issues and preventing data pipeline disruptions.
 * Excellent coding skills with a focus on testing and quality.
 * Experience coordinating with offshore teams and working in the onsite-offshore model.










Equal Opportunity Statement

BigR.io is an equal opportunity employer. We prohibit discrimination and harassment of any kind based on race, religion, national origin, sex, sexual orientation, gender identity, age, pregnancy, status as a qualified individual with disability, protected veteran status, or other protected characteristic as outlined by federal, state, or local laws. BigR.io makes hiring decisions based solely on qualifications, merit, and business needs at the time. All qualified applicants will receive equal consideration for employment.","Mid-Senior level","https://www.linkedin.com/jobs/view/data-engineer-at-bigrio-3766796997?trk=public_jobs_topcard-title","Massachusetts, United States","2 weeks ago","Jim Deely","https://www.linkedin.com/in/jimdeely","2023-11-16","","Software Development","Data Engineer","Information Technology and Engineering"
"123 applicants","11787695","ASCENDING Inc.","https://www.linkedin.com/company/ascendingllc?trk=public_jobs_topcard-org-name","Full-time","Our client, one of the largest Amazon Web Services (AWS) partners for data services, is looking for a true Mid level Big Data Engineer to contribute to join their team of technologists to build and contribute to large-scale, innovative projects. Technological and career growth opportunities are a natural and every day part of the working environment.


 * This role is only available for W2 or individual contracts. Please no C2C.
 * 100% Remote Work.
   
   

Responsibilities:


 * Analyze system requirements and design responsive algorithms and solutions.
 * Use big data and cloud technologies to produce production quality code.
 * Engage in performance tuning and scalability engineering.
 * Work with team, peers and management to identify objectives and set priorities.
 * Perform related SDLC engineering activities like sprint planning and estimation.
 * Work effectively in small agile teams.
 * Provide creative solutions to problems.
 * Identify opportunities for improvement and execute.
   
   

Requirements:


 * Minimum 5 years of proven professional experience working in the IT industry.
 * Degree in Computer Science or related domains.
 * Experience with cloud based Big Data technologies.
 * Experience with big data technologies like Hadoop, Spark and Hive.
 * AWS experience is a big plus.
 * Proficiency in Hive / Spark SQL / SQL. Experience with Spark.
 * Experience with one or more programming languages like Scala & Python & Java.
 * Ability to push the frontier of technology and independently pursue better alternatives.
 * Kubernetes or AWS EKS experience will be a plus.
   
   

Thanks for applying!

Powered by JazzHR

U3GJMKlbkr","Mid-Senior level","https://www.linkedin.com/jobs/view/data-engineer-at-ascending-inc-3776467012?trk=public_jobs_topcard-title","Rockville, MD","3 days ago","","","2023-11-30","","Internet Publishing","Data Engineer","Information Technology"
"Over 200 applicants","9956","RealPage, Inc.","https://www.linkedin.com/company/realpage?trk=public_jobs_topcard-org-name","Full-time","We are seeking a highly skilled and motivated Data Engineer to join our team. As a Data Engineer, you will be responsible for Designing, developing, and maintaining data pipelines that ingest, transform, and enrich data from various sources into data platform and feature stores using Python, SQL and other cloud native data engineering services to serve analytics needs and machine learning model development needs. The ideal candidate will have a solid background in Data Engineering with proficiency and experience in building cloud based data platforms. Experience with AWS and or GCP is preferred.

In This Role, You’ll Have The Opportunity To


 * Build and enhance our cloud data platform and feature store
 * Help the Data Science team build and deploy Machine Learning models
 * Continually learn, grow, and expand your knowledge, while also supporting others’ learning experiences
 * Collaborate with our talented Product, Data Science and Engineering teams as well as other parts of the RealPage business to deliver great products
 * Utilize best practices for architecture, implementation, testing, monitoring, logging, and deployments
 * Take on ad-hoc projects as they arise & always be willing to support your team where they need you
 * Communicate and exchange accurate information to others via telephone or internet video applications
   
   
   
   

Primary Responsibilities


 * Contribute to the detailed design and architecture of the data platform ensuring consistency, efficiency and reusability of data components and processes
 * Perform data cleansing and validation using SQL/Python and other tools and frameworks to remove or correct erroneous, incomplete, or inconsistent data
 * Responsible for applying data transformations and business logic to enhance, enrich, or standardize data
 * Handle large-scale and complex data sets using distributed systems & parallel computing to improve the performance of the pipelines
 * Monitor, troubleshoot, and optimize the performance and cost of data pipelines
 * Research and evaluate tools and technologies to improve the data platform capabilities
 * Participate in design discussions and code reviews
 * Work in an Agile environment with daily stand-ups and 2 week sprints
 * Partner and work with DevOps, Data Science, Engineering, Product and with other internal team members
   
   
   
   

Required Knowledge/Skills/Abilities


 * 3+ years of Data engineering experience
 * 3+ years of experience writing complex SQLs
 * 3+ years of experience building data pipelines using Python
 * Experience building / working with cloud data platforms like Redshift / Snowflake / BigQuery
 * Strong attention to detail, performance and quality
 * Experience working in a fast-paced, Agile environment
 * Strong communication skills
   
   
   
   

Preferred Knowledge/Skills/Abilities


 * Experience working with cloud platforms like AWS and or GCP
 * Experience working with Machine Learning Pipelines
 * Experience with prompt engineering for GenAI
 * Experience working with distributed programmimg frameworks like Apache Spark / Flink / Storm etc","Not Applicable","https://www.linkedin.com/jobs/view/data-engineer-python-sql-at-realpage-inc-3750654473?trk=public_jobs_topcard-title","Richardson, TX","1 week ago","☁ John Laury ☁","https://www.linkedin.com/in/dallastechrecruiter","2023-11-21","","IT Services and IT Consulting and Software Development","Data Engineer (Python/SQL)","Information Technology"
"Over 200 applicants","52186828","Venture Search","https://uk.linkedin.com/company/venture-search?trk=public_jobs_topcard-org-name","Full-time","Trading-House/Market-Maker

Software Engineer

C++, Java, C#

Remote OR Hybrid

New York

Base+Bonus+Profit Share




A leading market-maker is furthering its revenue streams by investing in a new Fixed Income start-up team within the firm. Your role will see you aid in the development of an automated market-making system, working alongside other key members of the research and trading team. The team has plans to become the biggest liquidity provider in the market.




Responsibilities of Core Engineer, Remote:

 * Experience with order management systems
 * Must have experience within financial services, ideally trading
 * Strong programming skills in C++, Python, Java or C#
 * Experience with market-making/low-latency technology would be hugely beneficial
 * Experience with GUI development is beneficial
 * Experience with FIX API Trading Connectivity
 * Experience working in a trading/hedge fund environment




e: adam.harris@venturesearch.com","Associate","https://www.linkedin.com/jobs/view/software-engineer-at-venture-search-3774724700?trk=public_jobs_topcard-title","New York, United States","4 days ago","Adam Harris","https://uk.linkedin.com/in/adam-harris-0b09ab71","2023-11-30","$150,000.00-$150,000.00","Capital Markets, Financial Services, and Investment Management","Software Engineer","Engineering, Information Technology, and Finance"
"49 applicants","80895049","Begin","https://www.linkedin.com/company/begin-learning?trk=public_jobs_topcard-org-name","Full-time","BEGiN has an exciting opportunity for a Senior Data Engineer to join our growing team! This role will be remote in Ontario, Canada.

BEGiN is an award-winning educational technology company with world-wide impact. With products that are as effective as they are fun, BEGiN’s family of brands builds critical skills for school and life.

We’re a diverse team of talented people passionate about creating educational content kids love. At BEGiN, we have the rare opportunity to make a dent in the universe by bringing high-quality at-home learning to kids globally!

Reporting into our Director, Data Engineering, the Senior Data Engineer will implement reliable data pipelines upholding the best practices that are pivotal to analytics, data science, and reporting across the organization as well as develop the data platform capabilities to ensure data platform performance, scalability and maintainability.

You will:


 * Own efficacy and quality of data pipelines and ETL processes that bring data into the enterprise data warehouse.
 * Develop, maintain, and improve tools to enable team members to rapidly consume and understand data.
 * Design and architect scalable infrastructure to build, train, and deploy machine learning models, ETL, and CI/CD with an eye on efficiency.
 * Be hands-on with multiple cloud technologies, tools and programming languages (Python, PySpark, SQL, AWS, GCP, Databricks, etc.)
   
   

Responsibilities:


 * Work closely with the other data engineers,, data scientists/analysts, and our product engineering team to translate requirements into deliverable data pipelines.
 * Execute the strategy for the data platform to support the business while optimizing performance and minimizing cost.
 * Partner with stakeholders and engineering teams to deliver solutions in an iterative and incremental manner, leveraging lean and agile principles, fostering an environment of learning and collaboration.
 * Ensure that our applications and operational data remain in sync and all integrations are flowing with no data errors.
 * Lead root cause analysis, prioritize and manage data quality and remediation, and ensure data integrity to all downstream data systems.
 * You will be an expert on understanding how data is collected, maintained, and interpreted and be knowledgeable on the official sources of data in scope to address use case requirements and business needs.
   
   

Must Haves:


 * Bachelor degree in Computer Science or related field.
 * Deep understanding of Spark (Databricks) and expertise on Data Warehousing approaches in the Databricks Lakehouse.
 * Expert in Python/PySpark/Spark SQL and follow/evolve established SDLC, coding best practices, version control etc.
 * Data Platform Architecture experience in AWS and/or GCP.
 * Previous hands-on experience with data modeling.
 * 5+ years of experience as a data engineer.
 * Excellent communication skills tailored for target audience.
 * Experience in BI tools (i.e. Looker)
   
   

Nice-to-Haves:


 * Graduate degree in Computer Science or related field.
 * Understanding of Analytics use cases (i.e. customer360, marketing channel optimization etc).
 * Prior experience with AWS Infrastructure (Networking, VPCs etc).
 * Prior experience with tools such as Fivetran, Airflow, Metarouter, Terraform etc
   
   

We like people who:


 * Are open to suggestions, collaborative, and thrive in team environments.
 * Love and are willing to learn new technologies and styles.
 * Are scrappy, entrepreneurial with the ability to turnaround high-quality projects quickly without depending on a large team.
   
   

What you’ll get:


 * BEGiN offers competitive compensation including equity and full benefits.
 * Smart, passionate, and engaged co-workers.
 * Excellent top-tier Medical/Dental/Vision benefits
 * The chance to have a big impact, quickly
 * The rare opportunity to make a dent in the universe. We’re bringing a love of reading and learning to children globally!
   
   

BEGiN is a proud equal opportunity employer. All qualified applicants will be considered without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status.

At BEGiN, we are committed to building a diverse team of talented people who are passionate about creating educational content kids love. We believe in fostering a culture where productivity can flourish, one that is empathetic, respectful, and inclusive. At BEGiN, we know that diversity, equity, and inclusion aren’t just an idea, a one-time initiative, or phrases to throw into a job post: they’re a daily practice and an ongoing conversation. We survey our team about inclusivity, run training on DEI topics, and have a committee to ensure we are all continuing to learn and grow.","Mid-Senior level","https://www.linkedin.com/jobs/view/senior-data-engineer-at-begin-3767891033?trk=public_jobs_topcard-title","Ontario, CA","2 weeks ago","","","2023-11-17","","E-Learning Providers","Senior Data Engineer","Information Technology"
"Over 200 applicants","2010798","Tiger Analytics","https://www.linkedin.com/company/tiger-analytics?trk=public_jobs_topcard-org-name","Full-time","Description




Tiger Analytics is pioneering what AI and analytics can do to solve some of the toughest problems faced by organizations globally. We develop bespoke solutions powered by data and technology for several Fortune 100 companies. We have offices in multiple cities across the US, UK, India, and Singapore, and a substantial remote global workforce.




We are expanding our Data Engineering practice and looking for Sr. Azure Data Engineers to join our growing team of analytics experts. The right candidate will have strong analytical skills and the ability to combine data from different sources and will strive for efficiency by aligning data systems with business goals.




This is a remote role for applicants based in USA.




Requirements




 * Bachelor’s degree in Computer Science or similar field
 * 8+ years experience in Data Engineering + several years in Analytics space
 * Strong Proficiency in Scala - coding experience a must
 * Strong Proficiency in Kafka and ADF for data pipelines /migration experience a must (Azure Synapses)
 * Experience with real time streaming, Kafka, and API Integration
 * Experience in PySpark
 * Strong Proficiency in Python programming
 * Strong Proficiency in SQL queries
 * Experience building data pipelines using Azure stack
 * Experience using Apache spark
 * Good working experience on Delta Lake and ETL processing
 * Prior experience of working in a Unix environment
 * Experience in harmonizing raw data into a consumer-friendly format using Azure Databricks
 * Experience extracting/querying/joining large data sets at scale
 * Experience building data ingestion pipelines using Azure Data Factory to ingest structured and unstructured data
 * Experience in data wrangling, advanced analytic modeling is preferred
 * Strong communication and organizational skills




Benefits




This position offers an excellent opportunity for significant career development in a fast-growing and challenging entrepreneurial environment with a high degree of individual responsibility.","Mid-Senior level","https://www.linkedin.com/jobs/view/data-engineer-azure-scala-kafka-at-tiger-analytics-3719413249?trk=public_jobs_topcard-title","United States","2 months ago","","","2023-09-13","","Technology, Information and Internet","Data Engineer - Azure (Scala/Kafka)","Information Technology"
"55 applicants","100144902","Team Remotely Inc","https://www.linkedin.com/company/team-remotely-inc?trk=public_jobs_topcard-org-name","Full-time","This is a remote position.

Junior Software Engineer (US/Canada Residents Only, 1 year experience, remote)

Team Remotely Inc. is a staffing and recruitment agency that offers a comprehensive solution for talent acquisition, including sourcing, vetting, pay rolling, and managing talent. Whether you need contract staffing, direct hire, direct sourcing, talent pools, or diversity initiatives, our model can support your hiring strategy.

Hiring Type: Full-Time

Base Salary: $65K-$75K Per Annum.

How to Apply: Please visit teamremotely.com to learn more & apply.

Requirements

Job Description:

We are looking for a talented and motivated Junior Software Engineer to join our team. As a Junior Software Engineer, you will play a vital role in the design, development, and maintenance of our software applications. You will collaborate with experienced engineers and contribute to the entire software development lifecycle. This is an excellent opportunity to grow your skills, gain hands-on experience, and work on exciting projects in a supportive and innovative environment.

Responsibilities:


 * Collaborate with the development team to understand project requirements and objectives
 * Participate in the design and implementation of software applications
 * Write clean, efficient, and well-documented code following best practices
 * Assist in the integration of APIs and third-party services
 * Conduct testing and debugging to ensure the functionality and performance of applications
 * Collaborate with designers to ensure a visually appealing and user-friendly interface
 * Contribute to code reviews and provide constructive feedback to improve code quality
 * Stay up-to-date with the latest software development trends and technologies
 * Document technical specifications and project details
   
   

Qualifications:


 * Bachelor's degree in Computer Science, Software Engineering, or a related field (or equivalent work experience)
 * Solid understanding of software development principles and best practices
 * Proficiency in at least one programming language (e.g., Java, C++, Python, Ruby)
 * Familiarity with web development languages such as HTML, CSS, and JavaScript
 * Knowledge of databases and experience with SQL or NoSQL
 * Understanding of version control systems (e.g., Git)
 * Strong problem-solving and analytical skills
 * Excellent communication and collaboration abilities
 * Ability to work independently and within a team
 * Eagerness to learn and adapt to new technologies
   
   

Preferred Qualifications:


 * Experience with software development projects (personal or professional)
 * Familiarity with web frameworks (e.g., Spring, Django, Ruby on Rails)
 * Knowledge of front-end frameworks/libraries like React, Angular, or Vue.js
 * Understanding of Agile development methodologies
 * Basic understanding of software testing principles and methodologies
   
   

Why work with Team Remotely?

Team Remotely Inc. is a staffing platform offering a seamless experience for employers and candidates. Employers can post job openings and specify their requirements, while candidates can create profiles and upload resumes.

The team of Team Remotely continuously learns and adapts based on previous successful placements, constantly improving its matching capabilities. This ensures that the recommendations provided by Team Remotely are tailored and accurate, increasing the likelihood of a successful match between employers and candidates. By providing intelligent and data-driven solutions, they strive to enhance the efficiency and effectiveness of the hiring process, ultimately helping companies find the best talent and individuals find their dream jobs.

","Entry level","https://www.linkedin.com/jobs/view/junior-software-engineer-at-team-remotely-inc-3778193745?trk=public_jobs_topcard-title","Houston, TX","9 hours ago","","","","","Software Development","Junior Software Engineer","Information Technology"
"Be among the first 25 applicants","90431993","Get It Recruit - Information Technology","https://www.linkedin.com/company/get-it-recruit-information-technology?trk=public_jobs_topcard-org-name","Full-time","At our company, we've fostered a culture built on quality, trust, ambition, and accountability. What sets us apart is not just our innovative self-service platform, but the enjoyment we find in working together. We take pride in continuously enhancing the user experience for our customers and streamlining our operations for efficiency. Our startup spirit fuels our growth mindset, and we support each other in building the future of Connected TV.

We're on the lookout for individuals who naturally seek more, want to own their projects, and aspire to make a meaningful impact in their careers. If you're ready to be part of the next stage of our growth, we want to hear from you.

Position: Data Analyst

What You'll Do

Become an expert on our platforms, UI, databases, and data pipelines.

Analyze and audit data for integrity, providing insights associated with our product iterations.

Conduct analytics and create reports to present data to internal customers and stakeholders.

Verify and QA various facets of our data pre and post-development to ensure business needs are met.

Investigate, triage, and debug data incidents, providing recommendations for resolution.

What You'll Bring

A bachelor's degree in a quantitative/analytical or marketing field (preferred).

3+ years of experience in data analytics or a similar role.

Experience with high-volume environments or data warehousing.

Proficiency in SQL, with the ability to write scalable, maintainable queries (preferably in Postgres).

Background in supporting products from inception to deployment within Product Development teams.

Advanced Excel/spreadsheet and analytical skills.

Demonstrated ability to interpret in-depth analyses, uncover actionable insights, and effectively communicate findings.

Ability to create compelling data visualizations for reports and dashboards.

Strong problem-solving mentality.

Excellent communication and presentation skills.

Self-starter attitude with the ability to thrive in a fast-paced and fun environment.

Outstanding multitasking skills and extreme attention to detail.

MNTN Perks

Work from home anywhere in the US.

Open-ended vacation policy with an annual vacation allowance.

Enjoy a three-day weekend every month of the year.

Competitive compensation.

100% healthcare coverage.

401k plan.

Flexible Spending Account (FSA) for dependent, medical, and dental care.

Access to coaching, therapy, and professional development.

Employment Type: Full-Time","Entry level","https://www.linkedin.com/jobs/view/data-analyst-remote-wfh-at-get-it-recruit-information-technology-3773521211?trk=public_jobs_topcard-title","Manchaca, TX","6 hours ago","","","","","Human Resources Services","Data Analyst - Remote | WFH","Information Technology"
"168 applicants","6449047","brightwheel","https://www.linkedin.com/company/brightwheel?trk=public_jobs_topcard-org-name","Full-time","Our Mission and Opportunity

Early education is one of the greatest determinants of childhood outcomes, is a must for working families, and has a lasting social and economic impact. Brightwheel’s vision is to enable high quality early education for every child — by giving teachers meaningfully more time with students each day, engaging parents in the development of their kids, and supporting the small businesses that make up the backbone of the $175 billion early education market. Brightwheel is the most loved technology brand in early education globally, trusted by thousands of educators and millions of families.

Our Team

We are a fully remote team with employees across every time zone in the US. Our team is passionate, talented, and customer-focused. Our exceptional investor group includes Addition, Bessemer Venture Partners, Chan Zuckerberg Initiative, GGV Capital, Lowercase Capital, Emerson Collective, and Mark Cuban.

We believe that everyone—from our employees to the students, teachers, and administrators we serve— should be given the opportunity to learn and thrive, whatever their background may be. We celebrate diversity in all forms because it allows our team and the communities we serve to reach their full potential and do their best work.

Who You Are

Brightwheel is seeking a Senior Data Engineer to join the Data Engineering team.

As a Senior Data Engineer at Brightwheel, you will play a key role in the implementation and evolution of our data platform. You will partner with technical leadership to craft and implement our data strategy. You will build and scale data pipelines that transform billions of records, across numerous systems, into measurable data that enable insights for our Analytics team.

You are passionate about data engineering and possess deep technical skills. You have contributed to building data platforms from the ground up. You have experience juggling multiple projects with shifting priorities while continuing to deliver value to the business. You are a curious, detail oriented, self-starter who wants to take full ownership of high impact projects with visibility throughout the organization.

What You’ll Do


 * Use modern tooling to build robust, extensible, and performant data models in a cloud-based data warehouse that will drive business intelligence for the company
 * Build extensible data acquisition and integration solutions to meet business requirements and reporting needs
 * Troubleshoot, improve and scale existing data pipelines, models and solutions
 * Build upon data engineering's CI/CD deployments, and infrastructure-as-code for provisioning AWS services
   
   
   

Required Qualifications, Skills, & Abilities


 * 3+ years of work experience as a data engineer, coding in Python
 * 1+ years experience deploying data processing infrastructure into AWS / cloud environments
 * Advanced understanding of how at least one big data processing technology works under the hood (e.g. Spark / Hadoop / HDFS / Redshift / BigQuery / Snowflake / Parquet / Avro / Kinesis / Kafka)
 * Experience with building ETL pipelines within Airflow / Python
 * Excellent analytical, problem solving, and troubleshooting skills to manage complex process and technology issues without much guidance
   
   
   

Preferred Experience


 * 2+ years experience building data models with dbt in a cloud based data warehouse platform
 * Experience with deploying Infrastructure as Code within a cloud environment
 * Building data ingestion from large scale transactional data stores
 * Hubspot / Salesforce / Mixpanel (Clickstream data analytics)
 * Serverless / event driven architecture (Glue / Lambda)
 * CubeJS
 * Parquet / Avro file storage
   
   
   

Brightwheel is committed to creating a diverse and inclusive work environment and is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity, gender expression, sexual orientation, national origin, genetics, disability, age, or veteran status.

Brightwheel is committed to internal pay equity and offers a competitive compensation package, including base salary, equity, and benefits. Our benefits package includes premium medical, dental, and vision benefits, generous paid parental leave, a flexible paid time off policy, a monthly wellness and productivity stipend, and a Learning & Development stipend.

For cash compensation, brightwheel sets standard ranges for all roles based on function, level, and geographic location, benchmarked against similar-stage growth companies. Multiple factors determine final offer amounts, including geographic location, candidate experience, and expertise. If you have questions about the compensation band for your region, please ask your recruiter.","Not Applicable","https://www.linkedin.com/jobs/view/senior-data-engineer-at-brightwheel-3725571214?trk=public_jobs_topcard-title","United States","2 months ago","","","2023-09-27","$135,000.00-$187,000.00","Transportation, Logistics, Supply Chain and Storage","Senior Data Engineer","Information Technology"
"77 applicants","10948712","DataTribe","https://www.linkedin.com/company/datatribe-?trk=public_jobs_topcard-org-name","Full-time","Do you want to help build the next generation network security planning solution?




Company Overview: Sixmap is working on leading edge network intrusion detection technology that enables enterprises and network operators to gain insights into their complete network attack surface and identify network vulnerabilities at unheard of speed and comprehensiveness. Sixmap’s platform can complete IPv4 scans with deep and configurable service interrogation that is orders of magnitude faster than anything currently available. The team is building the world’s first platform to perform comprehensive IPv6 scans, previously thought to be impossible.




Position Summary: We are looking for a data-oriented senior software engineer to help build the core network mapping and interrogation engine. Candidates should have deep hands-on experience working on data pipelines, ETL, data analysis processes, and database technologies in addition to a solid understanding of TCP/IP networking. The ideal candidate should be a well-rounded developer but be particularly strong in backend business-logic-oriented software development. Come join us, if you are ready to change the world of network security while having some fun along the way.




Position Requirements:




To be considered for this position, you must:




 * Be a development athlete with at least 5 years’ experience and have a passion in understanding users’ needs and system requirements and turning them into working software
 * Have a BS degree or higher in computer science, electrical/computer engineering, or related technical field
 * Be fully fluent in Python and common data analysis Python libraries, C++, SQL, Airflow or other ETL / data pipeline tools, and preferably be a polyglot comfortable in many additional programming languages.
 * Be an expert in using relational databases and NoSQL data stores - PostgreSQL experience is a must.
 * Be experienced with Linux environments.
 * Have experience working on container-based cloud infrastructure frameworks such as Docker or Kubernetes within common cloud service providers such as AWS, GCP, or Azure.
 * Be experienced using Agile methodologies, operating cloud dev-ops, and coordinating with product development teams
 * Have the ability to thrive when presented a complex challenge in a fast-paced, performance-oriented culture with intelligent people
 * Have exceptional level of integrity, raw intelligence, creativity, energy and passion
 * Operate efficiently with individual responsibility in a highly collaborative environment




Powered by JazzHR




4lnILZIcTW","Mid-Senior level","https://www.linkedin.com/jobs/view/senior-software-data-engineer-at-datatribe-3667158755?trk=public_jobs_topcard-title","Columbia, MD","5 months ago","","","2023-06-22","","Internet Publishing","Senior Software / Data Engineer","Information Technology"
"Over 200 applicants","3603143","Tier4 Group","https://www.linkedin.com/company/tier4group?trk=public_jobs_topcard-org-name","Full-time","Are you a Senior Data Engineer looking to move into a hybrid Engineer/ Architecture role? Do you have experience building highly customized data analytics solutions using Azure Databricks that wants to work for one of the fastest growing companies in the US?



You will work with large data sets to develop and create Azure Databricks notebooks, dashboards, models and visualizations that assists the various business units in their decision-making process. You will also maintain data quality, define master data rules, and serve as a liaison between the business and critical data transformation initiatives.



5+ years of experience as a Senior Data Engineer and Architect working within data analytics, business intelligence, and ETL processes using Azure Data Lake, Azure Data Factory, and Azure Databricks is required.



Experience building a data lake analytical platform (DLAP) from scratch using Azure is a must.



This is a fully remote, direct hire permanent opportunity that offers a competitive salary, bonus, and excellent benefits.



If you’re Senior Data Engineer seeking an advancement opportunity to work with leading edge data technology for one of the fastest growing companies in the US, please apply for immediate consideration!

","Mid-Senior level","https://www.linkedin.com/jobs/view/senior-data-engineer-1357247-at-tier4-group-3566631438?trk=public_jobs_topcard-title","United States","2 months ago","Kim Kilgoar","https://www.linkedin.com/in/kimkilgoar","2023-09-11","","IT Services and IT Consulting","Senior Data Engineer (1357247)","Information Technology"
"122 applicants","40956","Liberty Personnel Services, Inc.","https://www.linkedin.com/company/liberty-personnel-services-inc?trk=public_jobs_topcard-org-name","Full-time","Job Details:

Data Engineer SQL / Python / ETL / Azure

My client a top place to work in Philadelphia just called and needs to hire asap a great Data Engineer with SQL / Python / ETL and any Azure or AWS a big plus. This is a contract to perm role. BS is preferred. This is a remote role but the client would like someone local for occasional meetings onsite. Any spark would be a plus!!

If you are interested, please forward your resume to kevin@libertyjobs.com

Kevin McCarthy

#associate

#mid-senior","Associate","https://www.linkedin.com/jobs/view/data-engineer-sql-python-etl-azure-local-but-remote-at-liberty-personnel-services-inc-3774927262?trk=public_jobs_topcard-title","Philadelphia, PA","4 days ago","","","2023-11-30","","Staffing and Recruiting","Data Engineer SQL / Python / ETL / Azure (Local but Remote)","Information Technology"
"Over 200 applicants","2341274","NextRow Digital","https://www.linkedin.com/company/nextrow-digital?trk=public_jobs_topcard-org-name","Full-time","This is a remote position for an AWS Data Engineer with ETL and Analytical Reporting experience for our Iowa office. This position requires in-depth knowledge of AWS Data Integration Services such as Glue, as well as experience with Microsoft SQL Server, Microsoft SQL Server Integration Services, and MySQL.




Role Expectations




The successful candidate will spend a good portion of their time in transitioning already developed AWS data pipelines and procedures that are built for Health and Human Services department. The candidate is also expected to work in concert with resident Data Engineers, Data Analysts and Report Developers to enhance, develop and automate recurring data requests and troubleshooting related issues.




You will be primarily focused on backend development with AWS Data Integration and Storage Services tech stack (AWS Glue, AWS Lambda, AWS Spark, AWS Data Migration Services, AWS RDS, Amazon S3, Amazon Redshift, Amazon Dynamo).




The successful candidate will be required to follow standard practices for migrating changes to the test and production environments and provide postproduction support. When not working on enhancement requests or problem reports, you would concentrate on performance tuning.




Individual should work well in a team and independently as needed.




Responsibilities




Design and implement scalable and efficient data pipelines and ETL processes using AWS services such as AWS Glue, AWS Lambda, and Apache Spark.




Develop and maintain data models, schemas, and data transformation logic to support data integration, data warehousing, and analytics needs.




Collaborate with stakeholders to understand business requirements and translate them into technical data solutions.




Implement data ingestion processes from various data sources such as databases, APIs, and streaming platforms into AWS data storage services like Amazon S3 or Amazon Redshift.




Optimize data pipelines for performance, scalability, and cost-efficiency, utilizing AWS services like Amazon EMR, AWS Glue, and AWS Athena.




Ensure data quality, integrity, and security by implementing appropriate data governance practices, data validation rules, and access controls.




Monitor and troubleshoot data pipelines, identifying and resolving issues related to data processing, data consistency, and performance bottlenecks.




Collaborate with data scientists, analysts, and other stakeholders to support data-driven initiatives and provide them with the necessary datasets and infrastructure.




Stay updated with the latest AWS data engineering trends, best practices, and technologies, and proactively identify opportunities for improvement.




Mentor and provide guidance to junior members of the data engineering team, fostering a culture of knowledge sharing and continuous learning.




Key Requirements




Bachelor's or master's degree in computer science, Data Engineering, or a related field.




Minimum of 5 years of professional experience as a Data Engineer, with a focus on AWS data services and technologies.




Strong expertise in designing and implementing ETL processes using AWS Glue, AWS Lambda, Apache Spark, or similar technologies.




Proficient in programming languages such as Python, Scala, or Java, with experience in writing efficient and maintainable code for data processing and transformation.




Hands-on experience with AWS data storage services like Amazon S3, Amazon Redshift, or Amazon DynamoDB.




In-depth understanding of data modeling, data warehousing, and data integration concepts and best practices.




Familiarity with big data technologies such as Hadoop, Hive, or Presto is a plus.




Solid understanding of SQL and experience with database technologies like PostgreSQL, MySQL, or Oracle.




Excellent problem-solving skills, with the ability to analyze complex data requirements and design appropriate solutions.




Strong communication and collaboration skills, with the ability to work effectively in a team-oriented environment.




Required/Desired Skills Skill Required /Desired Amount of Experience Bachelor's or master's degree in computer science, Data Engineering, or a related field. Required Professional experience as a Data Engineer, with a focus on AWS data services and technologies. Required 5 Years Strong expertise in designing and implementing ETL processes using AWS Glue, AWS Lambda, Apache Spark, or similar technologies. Required 5 Years Proficient in programming languages such as Python, Scala, or Java, with experience in writing efficient and maintainable code for data processing a Required 5 Years Hands-on experience with AWS data storage services like Amazon S3, Amazon Redshift, or Amazon DynamoDB. Required 5 Years In-depth understanding of data modeling, data warehousing, and data integration concepts and best practices. Required 5 Years Familiarity with big data technologies such as Hadoop, Hive, or Presto is a plus. Desired 0 Solid understanding of SQL and experience with database technologies like PostgreSQL, MySQL, or Oracle. Required 5 Years Excellent problem-solving skills, with the ability to analyze complex data requirements and design appropriate solutions. Required 0 Strong communication and collaboration skills, with the ability to work effectively in a team-oriented environment Required 0 *INTERVIEW DATES: Interviews will be conducted on [July 19 and 21] ; candidates needs to be available for interviews on the date(s) provided.","Entry level","https://www.linkedin.com/jobs/view/ia-remote-role-aws-data-engineer-at-nextrow-digital-3661832656?trk=public_jobs_topcard-title","United States","4 months ago","","","2023-07-12","","IT Services and IT Consulting","IA- Remote Role-AWS DATA ENGINEER","Information Technology"
"Over 200 applicants","11071789","ISC (Integrated Specialty Coverages, LLC)","https://www.linkedin.com/company/iscmga?trk=public_jobs_topcard-org-name","Full-time","About Integrated Specialty Coverages




Integrated Specialty Coverages LLC (ISC) is a growth stage technology and data-driven insurance company leading innovation in the market.




Backed by one of the leading private equity firms, KKR, and led by a forward-thinking management team, ISC is combining the worlds of insurance and technology to create an Insurtech powerhouse. As a leading online distributor of insurance products for a range of industries and “Main Street USA”, we are looking for the right people to help us in our mission of achieving exponential growth. We strive to be the number one place to go for brokers and agents to source insurance. To accomplish this, we’re building a digitally focused team that deeply understands the intersection between user experience, data, and AI/ML to optimize the way we engage with our customers and partners.




Our ISC Engineering Team consists of a diverse group of people who have passion for the products we create and for the environment we’ve built to ensure their success. One thing is for sure, you’ll be surrounded with engineers, analysts, product owners and other peers who like to dive deep, enjoy a challenge and have fun while doing it! We’ve also created a fun, casual work environment that fosters openness, a sense of accomplishment and creativity. It’s a place where you can mentor others to grow, and also be mentored by some of the top technologists and visionaries in our industry.







Job Summary




The Data Engineer role will be the technical liaison between multiple groups including a data science team, the engineering team, product management, and business stakeholders. You do not need any insurance knowledge prior, however, you must quickly dive deep into the insurance world and ask questions to become a subject matter expert. You will be responsible for building a data platform to facilitate the data science team. You must be a self-starter that can build out features such as a data pipeline from scratch. There will be support from both engineering and data science for any buildout.




Job Responsibilities




 * Create and maintain optimal data pipeline architecture.
 * Assemble large, complex data sets that meet functional / non-functional business requirements.
 * Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
 * Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using AWS technologies, SQL, and Python.
 * Work with stakeholders including the Executive, Product, Data Science, and Engineering teams to assist with data-related technical issues and support their data infrastructure needs.
 * Work with data science and analytics teams to strive for greater functionality in our data systems.










Minimum Qualifications:




 * Advanced working SQL knowledge and experience working with relational databases, strong query authoring (SQL) as well as working familiarity with a variety of databases (Snowflake, Redshift, MySQL, MSSQL, etc.)
 * Experience building and optimizing data pipelines, architecture and data sets.
 * Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
 * Build processes supporting data transformation, data structures, metadata, dependency, and workload management.
 * A successful history of transforming, processing and extracting value from large disconnected, datasets from a variety of data sources (Flat files, Excel, databases, etc.)
 * Strong analytic skills related to working with unstructured datasets.
 * Strong project management and organizational skills.
 * Experience supporting and working with cross-functional teams in a dynamic environment.
 * Ability to mentor/guide/collaborate with other team members.




We are looking for a candidate with 3-5 years of experience in a Data Engineer role. They should also have experience using the following software/tools:




 * Experience with relational and MPP databases, including Snowflake and MySQL.
 * Experience developing software in an agile environment from the requirements stage to production.
 * Experience with version control: git
 * Experience with container technologies: Docker
 * Experience with data pipeline and workflow management tools: Airflow, Jenkins, AWS Glue, Azkaban, Luigi, etc.
 * Experience with AWS cloud services: EC2, ECS, Batch, S3, EMR, RDS, Redshift
 * Experience with other cloud services: Snowflake, Airflow
 * Experience with object-oriented/object function scripting languages: Python, Java, C++, etc.
 * Experience with data modeling and data warehouse design
 * Experience with data visualization tools (PowerBI, QuickSight)







Additional Information




The starting pay scale for this position is $100,000 - $115,000. Actual starting pay will be based on factors such as skills, qualifications, training, and experience. In addition, the company offers comprehensive benefits including medical, dental and vision insurance, 401(k) plan with match, paid time off, and other benefits.




ISC's salary ranges are determined by role and level. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations and could be higher or lower based on a multitude of factors, including job-related skills, experience, and relevant education or training.







Benefits of Working at ISC

 * Competitive vacation and flexible working arrangements
 * Comprehensive and inclusive health benefits
 * A variety of professional development and mentorship opportunities
 * Choice of technology whether at home or in the office







ISC’s Ownership Behaviors

*Do the Right Thing *Be Relentless. Pursue Excellence. *Personal Responsibility & Own Outcomes

*Try Fast. Learn Fast. Fail Fast. Think Big. *Build Extraordinary Owners & Join Forces *Grit & Determination




Applicants with disabilities may contact ISC HR department via e-mail, and other means to request and arrange accommodation. If you need assistance to accommodate a disability, you may request accommodation at any time. Please contact ISC at HR@ISCMGA.com.







ISC believes in creating long-term relationships by being responsive and relevant and by consistently delivering value to our community of customers. Specifically, with our employees, we focus on attracting, developing, and retaining the best talent for our business, challenging our people, demonstrating a “can-do” attitude, and fostering a collaborative and mutually supportive environment. ISC is an equal opportunity employer and a participant in the US federal E-Verify program (US).







Diversity creates a healthier atmosphere: All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, gender, gender identity, sexual orientation, marital status, medical condition, genetic information, mental or physical disability, military or veteran status, or any other characteristic protected by local, state, or Federal law.","Associate","https://www.linkedin.com/jobs/view/data-engineer-at-isc-integrated-specialty-coverages-llc-3768723452?trk=public_jobs_topcard-title","United States","5 days ago","","","2023-11-28","","Insurance","Data Engineer","Other"
"118 applicants","81606529","Steneral Consulting","https://www.linkedin.com/company/steneral-consulting?trk=public_jobs_topcard-org-name","Full-time","Fully Remote but looking from DE, MD, NC, VA, DC, WV onlyFully Remote but looking from DE, MD, NC, VA, DC, WV only




Responsibilities Include




Immediate need for a Lead Cloudera Engineer with specific knowledge in Apache Solr. The Lead Cloudera Engineer is responsible for orchestrating, deploying, maintaining and scaling cloud OR on-premise infrastructure targeting big data and platform data management (e.g., data warehouses, data lakes) including data access APIs.




 * Hands-on/advanced (expert preferred) level experience in administrating and engineering relational databases (ex. MySQL, PostgreSQL), Big Data systems (ex. Cloudera Data Platform Private Cloud and Public Cloud), Apache Solr as SME, ETL (ex. Ab Initio), BI (ex. MicroStrategy), automation tools (ex. Ansible, Terraform, Bit Bucket) and experience working cloud solutions (specifically data products on AWS) are necessary. 40% of the week will be hands-on technical support.
 * Leading several teams of Big Data Administrators/Engineers to ensure project cohesiveness/completion. 60% of the week will be lead related responsibilities.




Required Skills




 * Education Level: Bachelor's Degree. In lieu of a Master's degree, an additional 4 years of relevant work experience is required in addition to the required work experience.
 * At least 8 years of overall experience with all the tasks involved in administration of big data and Meta Data Hub such as Cloudera.
 * Must have a minimum of 3 years of Lead experience (teams of 5+ is preferred)
 * Must have expert level knowledge of Solr
 * Excellent communication skills both written and verbal. Expert
 * Experience with Advance knowledge of UNIX and SQL




Preferred




 * Experience with Ab Initio, EMR, S3, Dynamo DB, Mongo DB, ProgreSQL, RDS, DB2
 * DevOps (CI/CD Pipeline) is a Plus.
 * Prior experience with migration from on-premise to AWS Cloud. Cloudera Data Platform Private Cloud big plus","Mid-Senior level","https://www.linkedin.com/jobs/view/lead-data-engineer-at-steneral-consulting-3633724130?trk=public_jobs_topcard-title","United States","5 months ago","","","2023-06-13","","IT Services and IT Consulting","Lead Data Engineer","Information Technology"
"Over 200 applicants","37573759","QuantumBricks","https://www.linkedin.com/company/quantumbricks-inc?trk=public_jobs_topcard-org-name","Full-time","Job Title: Senior Data Engineer (druid Engineer)




Loc: Remote




Exp: 6+ Yrs




Job Description




Minimum required qualifications:




At least 5 years of experience in Data Engineering.




Working knowledge of modelling, loading, optimizing large amounts of data into druid to be queried by a low latency web applications via API.




Prior working experience in building data pipelines using Spark, Hive, Python, Airflow or similar technologies.




Prior working experience in Optimizing performance of data pipelines jobs.




Working knowledge of data modelling, data processing infrastructure, including databases, data lakes, and data warehouses.




Quick learner, self-starter, being able to operate with little to no guidance.




Good communication skills and being productive in a fast-paced development environment.




Nice to Have, but not mandatory




Prior experience and working knowledge of microservices




Prior Software engineering experience




Prior knowledge of implementing real-time data processing pipelines.




Prior experience and working knowledge of data bricks.




Prior knowledge and working experience of AWS technologies like SQS, EC2","Mid-Senior level","https://www.linkedin.com/jobs/view/senior-data-engineer-druid-engineer-at-quantumbricks-3634715047?trk=public_jobs_topcard-title","United States","5 months ago","","","2023-06-14","","IT Services and IT Consulting","Senior Data Engineer (Druid Engineer)","Information Technology"
"168 applicants","87146262","PharmaLogic Holdings Corp.","https://www.linkedin.com/company/pharmalogic-holdings-corp?trk=public_jobs_topcard-org-name","Full-time","Our company is looking to fill the role of data engineer. This role will work closely with our operations and finance team to support our data warehouse and transform the way we read our data. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing our company’s data architecture to support our next generation of products and data initiatives.

Responsibilities for data engineer


 * Create and maintain optimal data pipeline architecture,
 * Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
 * Help build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources.
 * Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
 * Work with stakeholders to assist with data-related technical issues and support their data infrastructure needs.
   
   

The ideal candidate must have:


 * A minimum of 2 years of relevant work experience
 * Advanced working SQL, and MySQL development experience
 * Experience with PowerBi, Domo, or related BI products
 * Experience with BI / data visualization tools
 * Working experience with Azure preferred and / or AWS.
 * Experience in the fields of data warehousing, and business intelligence.
 * Excellent computer science fundamentals and problem-solving skills
 * Experience in creating logical and physical data models
 * Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
 * Experience with multiple programming languages.
 * Strong analytic skills related to working with unstructured datasets.
 * Data engineer certifications are a plus.","Entry level","https://www.linkedin.com/jobs/view/data-engineer-at-pharmalogic-holdings-corp-3777118247?trk=public_jobs_topcard-title","United States","3 days ago","","","2023-12-01","","Pharmaceutical Manufacturing","Data Engineer","Information Technology"
"Over 200 applicants","164876","Mattress Firm","https://www.linkedin.com/company/mattressfirm?trk=public_jobs_topcard-org-name","Full-time","Mattress Fir is excited to announce the opening of the (Remote) Data Engineer role! The Data Engineer implements methods to improve data reliability and quality. The individual in this role combines raw information from different sources to create consistent and machine-readable formats. This role also develops and tests architectures that enable data extraction and transformation for predictive or prescriptive modeling.

Essential Responsibilities


 * Analyze and organize raw data
 * Developing and maintaining datasets
 * Improving data quality and efficiency
 * Interpret trends and patterns
 * Conduct complex data analysis and report on results
 * Prepare data for prescriptive and predictive modeling
 * Build algorithms and prototypes
 * Combine raw information from different sources
 * Explore ways to enhance data quality and reliability
 * Identify opportunities for data acquisition
 * Develop analytical tools and programs
 * Complete all required training modules and certifications prior to the due date
 * Ensure all safety policies and procedures are followed to ensure a safe work environment for all
 * Communicate professionally with all internal and external contacts
 * Follow all Company policies and execute Company standards on appearance and functionality as well as appropriate brand representation
 * Communicates any concerns or issues to leadership to ensure proper efficiency of department and company operations
   
   

Non-Essential Responsibilities


 * Collaborate with data scientists and architects on several projects
 * Execute company initiatives and other activities requested by supervisor
 * Updates job knowledge by participating in educational opportunities.
 * Contributes ideas on ways to optimize or improve the team, the department, and the Company
   
   

Education


 * Bachelor's Degree Degree in Computer Science, IT, or similar field required
 * Master's Degree Degree in Computer Science, IT, or similar field preferred
   
   

Professional Experience


 * 3+ Years Previous experience as a data engineer or in a similar role required
 * 3+ Years Hands-on experience with SQL database design required
   
   

Skills List


 * IT Expertise: Technical expertise with data models, data mining, and segmentation techniques
 * Analytical : Great numerical and analytical skills
   
   

Competencies


 * Results Driven - Competitive Awareness Stays informed of industry trends and technology of the competitive landscape. Analyzes the information to improve overall team performance and future planning. Looks for ways to win in the local marketplace or in business unit. Surfaces ideas to improve competitiveness and identify obstacles.
 * Communication - Listening Ability to accurately receive and interpret messages. Practices attentive, active listening with co-workers and/or guests with patience and can accurately restate opinions of others.
 * Interpersonal Skills - Build Trust Ability to build a common identity with others that aligns with company goals and fosters mutual respect. Demonstrates integrity by living the company values in their relationships. Shows respect for other’s contributions and gives honest feedback that is positive and constructive. Seeks to understand before being understood.
 * Mattress Firm Core Values Customer First, Integrity Always, Teams Win, Results Matter, Data Driven, Give Back
 * Core Competencies - Technical Expertise, job skills and technical knowledge are up to date.
 * Communication - Written Expressing yourself clearly to the organization in a variety of styles such as constructing a logical argument, electronic mail, note taking or summarizing. Reviews and edits materials to improve clarity while ensuring messages are targeted for the intended audience.
 * Technical Proficiency - Initiative Maintains a high-level of energy and self-motivation to achieve goals. Looks beyond his/her job requirement to make a contribution and offer support to the overall organization.
 * Technical Proficiency - Personal Development Quickly identifies when there is a need to change personal or interpersonal behavior, as well as leadership style. Is self-aware and knows how others respond to his/her influence and performance. Is sensitive to personal demands and requirements and is willing to adjust, accordingly.
 * Technical Proficiency - Knowledge Understands the job requirements of the role and demonstrates both soft and hard skills needed to be proficient.
 * Results Driven - Time Utilization Uses time effectively while concentrating on more important priorities. Quickly modifies behavior to deal effectively with change in the work environment. Efficiently manages shifting priorities to drive the best outcome for the business while supporting the team.
 * Results Driven - Takes Measured Risks Willing to make difficult decisions while understanding when to gain consensus. Ability to calculate risk factors in measuring potential obstacles and assess possible outcomes. Consistently seeks information to support informed decision making.
   
   

Knowledge


 * SQL High
 * Python High
 * Cloud Platforms High
   
   

Licenses and Certifications


 * Data engineering certification (e.g GCP Data Engineer) is a plus Upon Hire preferred
   
   

Physical Demands

Office Environment


 * Sitting for up to 8+ hours
 * Standing occasionally
 * Walking occasionally
 * Talking frequently
 * Hearing Frequently
 * Usage of hands and fingers
 * Reaching with hands and arms
   
   

*Must be a US Citizen without the need for sponsorship*

Pay Range

$110,000-130,000

Now don’t fall asleep out there… the sooner that we receive your application, the closer you are to the career of your dreams!

DIVERSE CANDIDATES ARE ENCOURAGED TO APPLY

Mattress Firm is an equal employment opportunity employer and is committed to maintaining a non-discriminatory work environment, and does not discriminate against any applicant or employee for employment on the basis of race, color, religion, sex, national origin, age, disability, veteran status, marital status, sexual orientation, gender identity, or any other characteristic protected by applicable law. Mattress Firm is committed to creating a dynamic work environment that values diversity and inclusion, respect and integrity, customer focus, and innovation.

","Entry level","https://www.linkedin.com/jobs/view/data-engineer-at-mattress-firm-3751156893?trk=public_jobs_topcard-title","United States","1 month ago","","","2023-11-02","","Retail","Data Engineer","Information Technology"
"97 applicants","11745204","Vibrant Emotional Health","https://www.linkedin.com/company/vibrantforall?trk=public_jobs_topcard-org-name","Full-time","Position Title: Data Engineer II

Salary Range: $92,000-$105,800*

Department: Information Technology

Reports to: Manager, Data Engineering

Location: Remote

Schedule: M-F, 9-5 ET


 * New hires are typically brought into the organization between the minimum to midpoint of the salary range posted depending on qualifications, internal equity, and the budgeted amount for the role.
   
   
   

Formerly the Mental Health Association of New York City (MHA-NYC), Vibrant Emotional Health’s groundbreaking solutions have delivered high quality services and support, when, where and how people need it for over 50 years. Through our state-of-the-art technology-enabled services, community wellness programs, and advocacy and education work, we are building a society in which emotional wellness can be a reality for everyone.

Position Summary

The Data Engineer II is responsible for the development of data pipelines, the orchestration and planning of data transformations and the development and support of data automations. This role interfaces with the Analytics and Research team around the design of data points, measures, and the implementation of models; with Data Governance roles around the alignment of policy and definitions with their implementation; and with Dev Ops, Security and Software Development teams around the broader organization’s use of data infrastructure.

Duties/Responsibilities


 * Develop patterns for data ingestion using Fivetran, AWS Lambda, and related technologies.
 * Orchestrate data transformations using DBT, database functions, materialized views and similar patterns.
 * Write tests, integrity checks, conduct performance monitoring and tuning of data systems.
 * Write anomaly detection routines to support alerting and monitoring.
 * Design and develop secure, high performance, API’s to support data requests.
 * Establish patterns for automation of forecasts, scoring, and support of ML/AI implementations.
 * Establish patterns for publishing and distributing data sets.
 * Establish and maintain archiving and retention schedules.
 * Ensure data systems conform with data governance and data security best practices.
   
   
   

Required Skills/Abilities


 * Extensive experience building and supporting data infrastructure.
 * Direct experience working with Snowflake, DBT and Fivetran.
 * Strong experience with AWS services such as Data Migration Services, RDS, Lambda, API Gateway, S3, etc.
 * Expert knowledge of SQL / PSQL.
 * Ability to code in Python and R (additional languages a +)
 * Strong track record of managing high availability systems.
 * Initiative to solve complex problems; takes an outside in perspective to identify innovative solutions.
 * We value candidates who have demonstrated commitment to the goal of working with people to achieve mental and emotional wellbeing with dignity and respect.
   
   
   

Required Qualifications


 * Bachelors’ or Masters’ Degree in an analytics or engineering focused discipline or equivalent experience and knowledge.
 * 1+ years of data engineering experience developing and maintaining high availability systems.
   
   
   

Excellent comprehensive benefits, including medical, dental, vision, supplemental income insurance, pre-tax transit/parking, pre-tax FSA for medical and dependent care, and 401K available. 4 weeks’ vacation, plum benefits, etc.

Studies have shown that women and people of color are less likely to apply for jobs unless they believe they are able to perform every task in the job description. We are most interested in finding the best candidate for the job, and that candidate may be one who come from a less traditional background. Vibrant will consider any equivalent combination of knowledge, skills, education and experience to meet minimum qualifications. If you are interested in applying, we encourage you to think broadly about your background and skill set for the role.

Vibrant Emotional Health is an equal opportunity employer. Applicants are considered for positions without regard to veteran status, uniformed service member status, race, creed, color, religion, gender, gender identity, sex, sexual orientation, citizenship status, national origin, marital status, age, physical or mental disability, genetic information, caregiver status or any other category protected by applicable federal, state or local laws.

""Please be aware that fictitious job openings, consulting engagements, solicitations, or employment offers may be circulated on the Internet in an attempt to obtain privileged information, or to induce you to pay a fee for services related to recruitment or training. Vibrant does NOT charge any application, processing, or training fee at any stage of the recruitment or hiring process. All genuine job openings will be posted on our careers page and all communications from the Vibrant recruiting team and/or hiring managers will be from an @vibrant.org email address""

","Entry level","https://www.linkedin.com/jobs/view/data-engineer-ii-at-vibrant-emotional-health-3754513086?trk=public_jobs_topcard-title","United States","1 month ago","","","2023-11-02","","Hospitals and Health Care","Data Engineer II","Information Technology"
"127 applicants","81606529","Steneral Consulting","https://www.linkedin.com/company/steneral-consulting?trk=public_jobs_topcard-org-name","Full-time","Must have LinkedIn




Need strong experience need senior consultants DBT experience needs to be strong as well




Title : GCP Data Engineer




Location : Remote ( need to work in Est timezone )




Duration : till end of this year and will have more extensions




GCP ACTIVE CERTIFICAION IS MUST HAVE




What You'll Do




 * Design, develop, and optimize data pipelines, architectures, and data sets in Google Cloud using tools such as BigQuery and DBT.
 * Use SQL and DBT for data transformation and manipulation, developing complex SQL queries and implementing optimization techniques for improved performance.
 * Utilize your expertise in Python to automate data pipeline processes, ensuring accuracy and efficiency.
 * Collaborate with our team of data analysts and data scientists, providing them with clean, reliable data for their analytical work.
 * Ensure data governance and security practices are applied across all data initiatives.
 * Conduct regular reviews and tests of our data systems to ensure data integrity and quality.
 * Stay up-to-date with industry trends and innovations, continuously improving and expanding your knowledge and skills.




What You Have




 * Google Cloud Certification (Professional Data Engineer)
 * Deep expertise in DBT (Data Build Tool) for defining, testing, and documenting data transformations in BigQuery.
 * Proficiency in creating and maintaining DBT models, understanding the use of macros, sources, and snapshots.
 * Understanding of advanced DBT concepts like incremental models, custom schemas, and the use of variables.
 * Experience with DBT Cloud, including setting up deployments, automated tests, and version control.
 * Outstanding proficiency in SQL, with the ability to write complex, efficient queries for data extraction and transformation.
 * Extensive experience with SQL optimization techniques, aiming to improve query performance and database scalability.
 * Knowledge of advanced SQL concepts like window functions, common table expressions (CTEs), and stored procedures.
 * Extensive experience in Google Cloud, including hands-on experience with Google Cloud data services such as BigQuery, Pub/Sub, Dataflow, and Dataproc.
 * Proven ability to design and implement Google Cloud-based data architectures, adhering to best practices for security, performance, and reliability.
 * Solid understanding of data warehousing concepts and data modeling principles.
 * Proficient in Python for data automation tasks.
 * Understanding of data governance and security practices.
 * Google Cloud Certification - Professional Data Engineer
 * Strong communication skills, with the ability to explain complex concepts to non-technical stakeholders.
 * Excellent problem-solving skills and attention to detail.","Entry level","https://www.linkedin.com/jobs/view/remote-work-need-gcp-certified-data-engineer-at-steneral-consulting-3691192389?trk=public_jobs_topcard-title","United States","3 months ago","","","2023-08-15","","IT Services and IT Consulting","Remote work - Need GCP certified Data Engineer","Information Technology"
"100 applicants","90876612","HireKeyz Inc","https://www.linkedin.com/company/hire-keyz?trk=public_jobs_topcard-org-name","Full-time","Job Role: Data Engineer Lead(with QE Exp.)




Location: Remote




Employment: Contract




Job Description




Looking for a Data Engineer with knowledge in Python programming, DWH, Strong in PL/SQL programming, Optimization of stored Procedures and knowledge in Test Automation ( using Python) .","Mid-Senior level","https://www.linkedin.com/jobs/view/data-engineer-lead-with-qe-exp-remote-at-hirekeyz-inc-3728527548?trk=public_jobs_topcard-title","United States","2 months ago","","","2023-09-28","","Staffing and Recruiting","Data Engineer Lead(with QE Exp.) - Remote","Information Technology"
"178 applicants","81606529","Steneral Consulting","https://www.linkedin.com/company/steneral-consulting?trk=public_jobs_topcard-org-name","Full-time","Job Title: Data Engineer with EMR and/or EHR




Location: 100% Remote {Someone open to work Eastern Standard Time (EST)}




Duration: 12+ Months




Experience Level: 10+ Years




Job Description




10+ years in IT with at least 3+ years' experience in data warehousing, modelling, end-to-end BI solutions




Strong SQL, Spark, and PySpark programming skills for data analysis.




Experience with hospital/provider solutions like (EMH, EMR codes, Billing, etc.):




Strong understanding of Data Engineering Solutions, Data modelling, and Software Engineering principles and best practices




Experience in developing data platforms/ Big data and cloud technologies (e.g., Azure)




Advanced knowledge of SQL and query optimization techniques and approaches




Experience designing, developing, and supporting Power BI data sources and reports.




Able to work as a team member and willing to work independently when required.




Strong troubleshooting and problem-solving skills




Experience working in an Agile/SCRUM SDLC environment.




Problem-solving aptitude, with a willingness to work in a fast-paced product development environment and hands-on mentality to do whatever it takes to deliver a successful product.","Entry level","https://www.linkedin.com/jobs/view/100%25-remote-work-need-data-engineer-with-emr-and-or-ehr-at-steneral-consulting-3640714850?trk=public_jobs_topcard-title","United States","5 months ago","","","2023-06-21","","IT Services and IT Consulting","100% remote work - Need Data Engineer with EMR and/or EHR","Information Technology"
"52 applicants","9414559","Sayari | Commercial Risk Intelligence","https://www.linkedin.com/company/sayarilabs?trk=public_jobs_topcard-org-name","Full-time","About Sayari:

Sayari is a venture-backed and founder-led global corporate data provider and commercial intelligence platform, serving financial institutions, legal and advisory service providers, multinationals, journalists, and governments. Thousands of analysts and investigators in over 30 countries rely on our products to safely conduct cross-border trade, research front-page news stories, confidently enter new markets, and prevent financial crimes such as corruption and money laundering. Our company culture is defined by a dedication to our mission of using open data to prevent illicit commercial and financial activity, a passion for finding novel approaches to complex problems, and an understanding that diverse perspectives create optimal outcomes. We embrace cross-team collaboration, encourage training and learning opportunities, and reward initiative and innovation. If you like working with supportive, high-performing, and curious teams, Sayari is the place for you.

Position Description:

Sayari is looking for a Data Engineer specializing in web crawling to join its Data Engineering team! Sayari has developed a robust web crawling project that collects hundreds of millions of documents every year from a diverse set of sources around the world. These documents serve as source records for Sayari’s flagship graph product, which is a global network of corporate and trade entities and relationships. As a member of Sayari's data team your primary objective will be to work on maintaining and improving Sayari’s web crawling framework, with an emphasis on scalability and reliability. You will work with our Product and Software Engineering teams to ensure our crawling deployment meets product requirements and integrates efficiently with our ETL pipelines.

Job Responsibilities:


 * Investigate and implement web crawls for new sources
 * Maintain and improve existing crawling infrastructure
 * Improve metrics and reporting for web crawling
 * Help improve and maintain ETL processes
 * Contribute to development and design of Sayari’s data product
   
   
   

Required Skills & Experience:


 * Experience with Python
 * Experience managing web crawling at scale, any framework, Scrapy is a plus
 * Experience working with Kubernetes
 * Experience working collaboratively with git
   
   
   

Desired Skills & Experiences:


 * Experience with Apache projects such as Spark, Avro, Nifi, and Airflow
 * Experience with datastores Postgres
 * Experience working on a cloud platform like GCP, AWS, or Azure
 * Working knowledge of API frameworks, primarily REST
 * Understanding of or interest in knowledge graphs
   
   
   

Benefits:


 * Limitless growth and learning opportunities
 * A collaborative and positive culture - your team will be as smart and driven as you
 * A strong commitment to diversity, equity & inclusion
 * Exceedingly generous vacation leave, parental leave, floating holidays, flexible schedule, & other remarkable benefits
 * Outstanding competitive compensation & commission package
 * Comprehensive family-friendly health benefits, including full healthcare coverage plans, commuter benefits, & 401K matching
   
   
   

Sayari is an equal opportunity employer and strongly encourages diverse candidates to apply. We believe diversity and inclusion mean our team members should reflect the diversity of the United States. No employee or applicant will face discrimination or harassment based on race, color, ethnicity, religion, age, gender, gender identity or expression, sexual orientation, disability status, veteran status, genetics, or political affiliation. We strongly encourage applicants of all backgrounds to apply.","Not Applicable","https://www.linkedin.com/jobs/view/data-engineer-web-crawling-at-sayari-commercial-risk-intelligence-3767219961?trk=public_jobs_topcard-title","United States","2 weeks ago","","","2023-11-17","","Transportation, Logistics, Supply Chain and Storage and Internet Publishing","Data Engineer - Web Crawling","Information Technology"
"127 applicants","12899484","Idelic","https://www.linkedin.com/company/idelic?trk=public_jobs_topcard-org-name","Full-time","About Idelic




Idelic is focused on driving the best possible insurance outcomes for the transportation industry through the combination of process and technology. The Idelic program combines the Driver Safety Playbook with Safety Suite®, the first end-to-end driver performance management platform, to help fleets consolidate their most relevant driver data into one platform, analyze it with proprietary machine learning models, and take action on new insights. Empowering fleets with a data-driven view of fleet safety through Idelic’s advanced AI-based Driver Watch List, broad integration network and proven driver professional development plans (PDPs), the Idelic Program is the most proactive and effective way to prevent crashes, reduce losses and produce better insurance outcomes.




Overview Of The Role




Collaborate with our product, customer success, and technical teams to research, design, and enhance Idelic's Data Services Platform. Innovate in an industry full of data and experience and create invaluable solutions to our customer’s problems. Your focus will be on developing new features, fixing bugs, and optimizing our Data Services Platform.




What You'll Do




 * Build new integrations, features, and support our Data Services Platform
 * Develop and maintain efficient and scalable data pipelines and ETL/ELT processes to acquire and transform data from various sources into usable formats
 * Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc
 * Build processes supporting data quality, data transformation, data structures, metadata, dependency, and workload management
 * Owning existing processes running in production, problem solving and optimization
 * Lead & contribute to the development of tools, infrastructure, and architecture to scale our business and our customers
 * Participate in internal reviews of code, software components, and systems and make data-driven decisions on how they should evolve
 * Communicate effectively and participate with team members in an Agile environment
 * Work on any task and help solve problems when needed
   
   

WHAT YOU'LL NEED TO SUCCEED




 * 6+ years of experience as a software developer in an ETL environment
 * Experience with ETL tools (Informatica, Talend, etc.)
 * Strong proficiency with Python, Google Go (Golang), or similar language
 * Experience with AWS cloud services: EC2, EMR, RDS, Redshift, Glue, lambda, etc
 * Experience with relational SQL and NoSQL databases such as Postgres or Cassandra
 * Experience reviewing code and mentoring less experienced developers
 * Strong quantitative and analytical background & process
 * Knowledge of streaming technologies (Kafka, RabbitMQ)
 * Knowledge of Jenkins, Github and Reporting tools
 * Experience writing unit, integration, and end-to-end test code
 * Proven ability to work in a collaborative and fast-paced environment
   
   

WHAT WILL SET YOU APART




 * Experience in the Logistics / Transportation industry
 * Experience with a variety of data sources (API, CSV, SFTP, RDBMS, etc.)
 * Experience with distributed technologies like Kubernetes
 * Experience working in an entrepreneurial or enterprise environment
   
   

WHAT MAKES IDELIC A GREAT PLACE TO WORK




 * Competitive Compensation Package Including Options
 * Unlimited Paid Time Off (PTO)
 * Medical, Dental, Vision, Disability, and Life Insurance
 * 401(k) with Company Matching Funds
 * Paid Parental Leave, Adoption Assistance, and Paid Military Leave
 * Paid Volunteer Time to Support Your Local Community
   
   

If you’ve made it this far, we hope you're feeling excited about this role. Even if you don't feel you meet every single requirement, we still encourage you to apply. You may be surprised! We're eager to meet people who are passionate, believe in Idelic’s mission, and can contribute to our team in a variety of ways—not just candidates who check all the boxes.




We do not and shall not discriminate on the basis of race, color, religion (creed), gender, gender expression, age, national origin (ancestry), disability, marital status, sexual orientation, or military status in any of its activities or operations. We at Idelic are committed to providing an inclusive and welcoming environment for all members of our team, contractors, vendors, and clients. Idelic is an equal-opportunity employer. Our success depends heavily on the effective utilization of qualified people, regardless of their race, ancestry, religion, color, sex, age, national origin, sexual orientation, gender, identity, disability, veteran status, or any characteristic protected by law.




TYPICAL PHYSICAL DEMANDS & WORKING CONDITIONS




The physical demands that are described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. While performing the duties of this job, the employee is regularly required to hear and see. The employee is regularly required to stand and sit/ The employee is regularly required to practice manual dexterity sufficient to operate standard office equipment. Specific vision abilities required by this job include close vision and distant vision. While performing the duties of this job, the employee is exposed to standard office equipment. Occasionally called upon to work hours in excess of your normal daily schedule.




The salary range for this position is $86,000 to $100,000. Actual compensation within that range will depend on the candidate's experience, skills, qualifications, and all applicable state laws.","Not Applicable","https://www.linkedin.com/jobs/view/data-engineer-ii-at-idelic-3729741105?trk=public_jobs_topcard-title","United States","1 month ago","","","2023-10-06","$86,000.00-$100,000.00","Software Development","Data Engineer II","Information Technology"
"84 applicants","3108492","Changeis, Inc.","https://www.linkedin.com/company/changeis-inc-?trk=public_jobs_topcard-org-name","Full-time","Changeis, Inc. is an award-winning 8(a) certified, woman-owned small business that provides management consulting and engineering services to the public sector. Changeis' work has resulted in the successful execution of numerous programmatic initiatives, development of acquisition-sensitive deliverables, and establishment of a variety of long-term innovative strategic priorities for its customers. Changeis focuses on delivering unparalleled expertise in the areas of strategy and transformation management, investment analysis and acquisition management, governance, and innovation management. Inc. magazine has ranked the management consulting firm, Changeis Inc., among the top 1000 firms on its 35th annual Inc. 5000, the most prestigious ranking of the nation's fastest-growing private companies. Changeis offers a full benefit package that includes medical, dental, and vision, short and long term disability, retirement plan with immediate vesting and company match, and a generous annual leave plan.

The Data Engineer will partner with a Federal Agency Office of Human Resources, focusing on essential areas such as business management, strategic planning, and decision-making. By developing and maintaining data architectures, engaging in acquisition/contract management, and applying expertise in information technology, data analytics, and knowledge management, the Data Engineer will significantly contribute to the optimization and innovation of organizational processes. The Data Engineer will collaborate with product design and engineering teams to understand their needs, and then research and devise innovative statistical models for data analysis. By communicating findings to all stakeholders and using analytics for meaningful insights, they will enable smarter business processes and stay abreast of current technical and industry developments.

Roles And Responsibilities


 * Collaborating with clients and end-users to understand their mission, architecture, and security requirements.
 * With a focus on the client's goals, build a design that will scale to meet their evolving needs.
 * Recommend tools and capabilities based on your research of the environment and new technology.
 * Design the standard for future development, so you'll craft an architecture that smoothly works with existing infrastructure without compromising security.
 * Identify new opportunities to build platform-based solutions to help your customers meet their toughest challenges.
 * Developing and running ETL (Extract, Transform, Load) processes to manage data flow and ensure data quality.
   
   

Requirements


 * 8+ years of experience with designing, developing, operationalizing, and maintaining complex data applications at enterprise scale.
 * 5+ years of experience with creating software for retrieving, parsing, and processing structured and unstructured data.
 * 5+ years of experience with developing scalable ETL workflows for reporting and analytics.
 * 5+ years of experience with using Python, SQL, Scala, or Java
 * 3+ years of experience with AWS.
 * 3+ years of experience with using Docker, Kubernetes, and Helm.
 * 3+ years of experience with Distributed data and computing tools, including Spark, Databricks, Hadoop, Nifi, Kafka, Scala, or Java.
 * Experience with developing scripts and programs for converting several types of data into usable formats, supporting project team to scale, and monitoring and operating data platforms.
 * Ability to supervise others and lead projects and deliverables in a collaborative, cross-functional team environment.
 * Prefer candidates holding active AWS certifications.","Entry level","https://www.linkedin.com/jobs/view/data-engineer-at-changeis-inc-3759561188?trk=public_jobs_topcard-title","Arlington, VA","3 weeks ago","","","2023-11-09","","Technology, Information and Internet","Data Engineer","Information Technology"
"40 applicants","1747652","IT Pros","https://www.linkedin.com/company/it-pros---philadelphia?trk=public_jobs_topcard-org-name","Full-time","Company Description

Access opportunities with thousands of US-based companies seeking Data - Engineers/Scientists/Architects via IT Pros, a tech recruitment firm with a decade of experience since 2011.

Work Location: Hybrid + 100% USA-Remote Work Schedule Options


 * 100% USA-Remote
 * 1-2 Days In-Office
 * 2-3 Days In-Office
 * 3-4 Days In-Office
 * 5 Days In-Office
   
   

Company Locations:


 * New York City, New York
 * Los Angeles, California
 * Chicago, Illinois
 * Houston, Texas
 * Phoenix, Arizona
 * Philadelphia, Pennsylvania
 * San Antonio, Texas
 * San Diego, California
 * Dallas, Texas
 * San Jose, California
 * Austin, Texas
 * Jacksonville, Florida
 * San Francisco, California
 * Columbus, Ohio
 * Indianapolis, Indiana
 * Fort Worth, Texas
 * Charlotte, North Carolina
 * Seattle, Washington
 * Denver, Colorado
 * Washington, D.C.
 * Boston, Massachusetts
 * Raleigh-Durham, North Carolina
 * Boulder, Colorado
 * Portland, Oregon
 * Atlanta, Georgia
   
   

Compensation: Based on Years of Experience and Accomplishment


 * 3-5 Years = $130,000 - $150,000
 * 6-8 Years = $150,000 to $170,000
 * 9+ Years = $170,000 to $180,000+
   
   

Benefits:


 * Medical, Dental, and Vision Insurance
 * Life insurance, Long Term Disability, and Short Term Disability
 * Paid Time Off (PTO)
 * Plus more...
   
   

Job Description

Job descriptions for Data - Engineers/Scientists/Architects positions will be provided upon a successful match.

Qualifications

Successful Data - Engineers/Scientists/Architects will meet the following criteria:


 * Must be located and authorized to work in the United States without any work restrictions, now or in the future.
 * Must have experience with one or more of the following: ETL, SQL, NoSQL, Hadoop, Spark, Kafka, Apache Nifi, Talend, Informatica, AWS, Azure, Google Cloud (GCP), Python, Pandas, R, Matplotlib, Seaborn, Tableau, TensorFlow, PyTorch, AWS Redshift, Azure Synapse.
 * Must have a solid work track record of delivering results
 * Must have excellent communication skills
   
   

Additional Information

Your application will be reviewed by a real human. Feedback will be provided. Your patience is appreciated. We look forward to having the opportunity to work with you.","Mid-Senior level","https://www.linkedin.com/jobs/view/data-engineer-scientist-architect-%24130k-%24180k-hybrid-and-remote-work-options-at-it-pros-3777294915?trk=public_jobs_topcard-title","Pittsburgh, PA","1 day ago","","","2023-12-03","","Staffing and Recruiting","Data - Engineer/Scientist/Architect | $130K-$180K | Hybrid AND Remote Work Options","Information Technology"
"Over 200 applicants","35548088","ZETTALOGIX INC","https://www.linkedin.com/company/zettalogixinc?trk=public_jobs_topcard-org-name","Full-time","Remote Business Intelligence Engineer




Location:MINNEAPOLIS, Minnesota




Remote Work: 100%




Primary Skills




Cloud




Please share me resume to Laxmi@zettalogix.com




Required Skills




Job Description:




Seeking to add a Strong PowerBI consultant (with a strong nice to have if they also have




AWS Cloud Architect experience as well)




Description




Supports and may lead the design and governance of large-scale Business Intelligence solutions




that deliver business value across the organization.




The Enterprise Data Business Intelligence Engineer will conduct data analysis in support of a variety of analytic solutions.




Capture, develop, and document data definitions, business rules, and data quality requirements.




Develop Semantic layer solutions used throughout the enterprise for reporting and analytic solutions based on business requirements.




Optimizes the performance of enterprise business intelligence tools by defining data to filter and index that add value to the user




Partner with stakeholders to understand data requirements and with internal and/or external resources to develop tools and services such as segmentation, modeling, dashboard development, decision aids, and business case analysis to support the organization.




Develops standards, policies and procedures for the form, structure and attributes of the business intelligence tools and systems




Provide data recommendations and implement data strategies to meet business needs




Ensure data integrity by implementing quality assurance practices.




Identify and define both process and data improvements.




Ensure compliance with deliverable reporting requirements by performing quality data audits and analysis.




Researches new technology and develops business cases to support enterprise wide business intelligence solutions.




Designs and coordinates a curriculum for coaching and training customers in the use of business intelligence tools to enhance business decision-making capability.




This role will join s RPA Team. The Role does not require and RPA Experience although a background in RPA is a bonus.




Role Notes From The Manager




 * Bonus if candidate is able to assist the current cloud architect on the RPA Team to implement a cloud strategy for Handwriting OCR and in Manual set-up
 * Perform manual process of submitting requests to vendor, work with the vendor, validate and test, and assist with connecting to cloud to the cloud
 * Create Dashboards and perform Analytics and extract data from log files with PowerBI




Must Have Skills




 * Cloud validating and testing experinece (perfers AWS but Azure is ok)
 * Strong PowerBI Background with Dashboard set up and engineering ability.","Entry level","https://www.linkedin.com/jobs/view/business-intelligence-engineer-at-zettalogix-inc-3692578389?trk=public_jobs_topcard-title","United States","3 months ago","","","2023-08-16","","Information Technology & Services","Business Intelligence Engineer","Business Development and Sales"
"38 applicants","18599053","Stellent IT","https://www.linkedin.com/company/stellent-it?trk=public_jobs_topcard-org-name","Full-time","Azure Data Engineer

Remote

Phone + Skype

Job description: Skills Required:

Ability to work in a dynamic environment with changing requirements.

Good communication and presentation skills

Working experience with T-SQL and relational databases including currently supported versions of Microsoft SQL Server in Azure.

Design, develop and maintain Stored Procedures, Functions and Views

Working experience with development and maintenance of Data Factory data pipelines.

Working experience with development and maintenance of data pipelines on the DataBricks platform in Azure.

Working experience using DevOps CI/CD in Azure.

Skills Preferred

Proficient with the following: T-SQL, SSIS, data pipelines, and ETL concepts & practices

Understanding of data lakehouse architecture in Azure.

Write high performance SQL queries using Joins, Cross Apply, Aggregate Queries, Merge, Pivot

Provide best approaches for development and maintenance of data pipelines.

Debug existing code and troubleshoot for issues.

Roles & Responsibilities

Work with a highly dynamic team focused towards Digital Transformation

Understand the domain and business processes to implement successful data pipelines.

Provide work status, and coordinate with Data Engineers

Manage customer deliverables and regularly report the status via Weekly/Monthly reviews.

(""Believe you can and you're halfway there."")

Theodore Roosevelt

Yogesh Sharma | Sr. Tech Recruiter

P: +1 201-627-7507

E: Yogesh@stellentit.com

LinkedIn | Vendor list","Entry level","https://www.linkedin.com/jobs/view/azure-data-engineer-remote-at-stellent-it-3774054026?trk=public_jobs_topcard-title","United States","4 days ago","","","2023-11-30","","Staffing and Recruiting","Azure Data Engineer, Remote","Information Technology"
"51 applicants","368900","ClifyX","https://www.linkedin.com/company/clifyx?trk=public_jobs_topcard-org-name","Full-time","For all the below Job Title : AI & GenAI (Generative Artificial Intelligence) Exp Mandatory ---
Contract or FTE Any Location Open to Travel or Relocate Plz Check all the options...
Client ---Client

12.Big Data Engineer:
Specialized expertise in big data technologies and tools.
Proficiency in working with distributed systems.
Experience with data ingestion and processing at scale.
GenAI (Generative Artificial Intelligence) Roles:","Entry level","https://www.linkedin.com/jobs/view/big-data-engineer-at-clifyx-3746813130?trk=public_jobs_topcard-title","United States","1 month ago","","","2023-10-25","","Staffing and Recruiting","Big Data Engineer","Engineering and Information Technology"
"35 applicants","31349149","eStaffing Inc.","https://www.linkedin.com/company/estaffinginc?trk=public_jobs_topcard-org-name","Full-time","Client: Health and Care Services

Title: Big Data Developer (Big Data Software Engineer)

Type: C2H & C2C

Location: 100% REMOTE

Job Requirements

Technical Requirements

 * Bachelor's degree in Computer Engineering or Computer Science and 7+ years of post-bachelor's degree experience in related field.
 * 5+ years of Big Data Software Engineering experience Spark, Hadoop and similar frameworks
 * 4+ years of experience in Apache Spark/Hadoop Framework
 * 3+ years' experience programming in Scala and Python required
 * 2+ years' experience in Java, PL/SQL and/or SQL (overall should have general programming experience with one of more of these languages)
 * 2+ year of experience working with Azure Cloud including having at least 1+ year of both HDInsight and Databricks experience.
 * Experience with Apache Kakfa and HBase would be very helpful for this role but not required.
 * Strong technical skills and ability to communicate complex technology solutions to clients, technical, business and management teams.
 * Strong Analytical and critical thinking skills with ability to manage and solve multiple complex problems.
 * Experience in Agile Development methodologies
 * Familiarity with code versioning tools like Git , CICD tools like Jenkins and XL Deploy","Entry level","https://www.linkedin.com/jobs/view/big-data-engineer-developer-remote-100%25-at-estaffing-inc-3617897361?trk=public_jobs_topcard-title","United States","7 months ago","","","2023-05-02","","Staffing and Recruiting","BIG DATA ENGINEER/DEVELOPER | REMOTE 100%","Other"
"Over 200 applicants","14023177","Counterpart","https://www.linkedin.com/company/yourcounterpart?trk=public_jobs_topcard-org-name","Full-time","Counterpart is a modern management & professional liability insurance platform for the 21st century workplace. We offer products and services to help companies align the risks and incentives of creating great organizations. Our platform is designed to measure risk more efficiently using the most advanced rating system in the industry, while also proactively mitigating the risks of our insureds. Ultimately, we provide businesses with the framework to be the best versions of themselves. That's where you come in.

As a Data Analyst, you will balance being a prolific individual contributor to our business intelligence, reporting and analysis, while steering the development of our products and services through the creation, maintenance, and application of data. You will help bridge the gap between Engineering, Data, & Business with insights while supporting the development of our data services.

AS A DATA ANALYST YOU WILL:


 * Find insights into performance across the entire organization and own data visualizations.
 * Develop complex analyses and reporting related to our underwriting, operations, business development, product performance, and user experience.
 * Assist the data science, operations, and insurance teams with ad hoc analysis, data normalization, data cleansing and process improvement.
 * Support the integration and production of new data sources in a manner that minimizes development cycles while maximizing potential business applications.
 * Continuously challenge how we can improve our underwriting while reducing the number of questions that need to be answered in an application.
 * Maintain a clean production environment such that the data models can be easily interpreted and built upon by other data science and engineering team members.
 * Present your work, findings, and opinions to both technical and non-technical stakeholders.
   
   

WE LOOK FOR TEAMMATES WHO HAVE:


 * Bachelor/Master in quantitative discipline (computer science, actuarial science, mathematics, statistics, economics, physics, engineering or related field).
 * Best in class skills with Business Intelligence tools (i.e. Looker/Tableau/PowerBI)
 * Ability to balance competing priorities and focus on key initiatives, by estimating timelines and keeping team/documentation updated with status of projects
 * Experience and excitement using modern cloud computing and cloud databases/tools (i.e. AWS, Snowflake, DBT, Airflow)
 * A passion for solving challenging mathematical problems and an interest in exploring new machine learning tools and technologies.
 * 2+ years experience with Python.
 * Communications skills for translating technical or statistical analysis results into business recommendations.
 * Bias to practical action and creativity using data (we value past or present projects that support this).
 * Interest/experience in behavioral economics and ""nudging""
 * Bonus points for past experience in an insurance industry and/or experience with data scientist techniques (e.g. Factor Analysis, Cluster Analysis, CART, MARS, Neural Networks).
   
   

WHO YOU WILL WORK WITH:


 * Tanner Hackett, CEO & Founder: Having founded two other major startups, including Button and Lazada, Tanner now spends his time focused on mental health through his philanthropy, Openminded.org, in addition to reading, surfing, yoga, and enjoying the outdoors.
 * Chris Shafer, Special Projects: Chris is a published scientific author, having studied the neural correlates of gratitude at the lauded Brain and Creativity Institute. He has since helped to launch numerous businesses in which he held a variety of critical roles from product management to business operations to strategic partnerships. Chris received his bachelor's degree in Biological Sciences from the University of Southern California.
 * Tobias Schuler, Head of Data & Analytics Advisor: Tobias was previously the Head of Data and Analytics at Digital Partners, a Munich Re company. Tobias led a team that built out data integrations, business intelligence and advanced analytics across all insurtech partners spanning various P&C lines of businesses. Tobias is also a FCAS and has built systems to enable leading class insights for underwriting, actuarial, claims and finance experts while focusing on democratizing data. He enjoys traveling internationally and spending time with his 2 young daughters.
 * Elizabeth Barsalou, Data Scientist: Before joining Counterpart, Elizabeth worked as a full stack data scientist in small business lending for Kabbage and BHG. She specialized in building data science models, infrastructure and strategies to extend credit to small businesses. She lives in San Francisco and spends her free time singing opera and playing with her dogs.
 * Stanley Wang, Director, Pricing Analytics: Before joining Counterpart, Stanley worked as an actuary within the pricing solutions and methods team at USAA to combine actuarial pricing with data science models. Before that, he was a leading data scientist for Digital Partners, a Munich Re Company where he specialized in building data science models and insights for leading insurtech companies. He has had many other relevant roles such as capital modeling, risk management etc. He lives in New York with his growing family.
   
   

What We Offer


 * Unlimited Vacation: We offer flexible time off, allowing you to take time when you need it.
 * Work from Anywhere: Counterpart is a fully distributed company, meaning there is no office. We allow employees to work from wherever they do their best work, and invite the team to meet in person a couple times per year.
 * Stock Options
 * Health, Dental, and Vision Coverage
 * 401(k) Retirement Plan
 * Parental Leave
 * Home Office Allowance: to set up your home office with the necessary equipment and accessories.
 * Book stipend
 * Professional Development Reimbursement
 * No working birthdays: Take your birthday off, giving you the opportunity to relax, enjoy your special day, and spend time with loved ones.
 * Charitable Contribution Matching
   
   

COUNTERPART'S VALUES


 * Conjoin Expectations - it is the cornerstone of autonomy. Ensure you are aware of what is expected of you and clearly articulate what you expect of others.
 * Speak Boldly & Honestly - the only failure is not learning from mistakes. Don't cheat yourself and your colleagues of the feedback needed when expectations aren't being met.
 * Be Entrepreneurial - control your own destiny. Embrace action over perfection while navigating any obstacles that stand in the way of your ultimate goal.
 * Practice Omotenashi (""selfless hospitality"") - trust will follow. Consider every interaction with internal and external partners an opportunity to develop trust by going above and beyond what is expected.
 * Hold Nothing As Sacred - create routines but modify them routinely. Take the time to reflect on where the business is today, where it needs to go, and what you have to change in order to get there.
 * Prioritize Wellness - some things should never be sacrificed. We create an environment that stretches everyone to grow and improve, which is fulfilling, but is only one part of a meaningful life.
   
   

We are committed to being a welcoming and inclusive workplace for everyone, and we are intentional about making sure people feel respected, supported and connected at work—regardless of who you are or where you come from. We value and celebrate our differences and we believe being open about who we are allows us to do the best work of our lives.

We are an Equal Opportunity Employer. We do not discriminate against qualified applicants or employees on the basis of race, color, religion, gender identity, sex, sexual preference, sexual identity, pregnancy, national origin, ancestry, citizenship, age, marital status, physical disability, mental disability, medical condition, military status, or any other characteristic protected by federal, state, or local law, rule, or regulation.","Entry level","https://www.linkedin.com/jobs/view/data-analyst-at-counterpart-3761258428?trk=public_jobs_topcard-title","United States","3 weeks ago","","","2023-11-10","","Technology, Information and Internet","Data Analyst","Information Technology"
"Over 200 applicants","80140883","Adame Services","https://www.linkedin.com/company/adameservices?trk=public_jobs_topcard-org-name","Full-time","Term: (1 year+)




Location: St. Louis (client location) working hours Central Standard Time




REMOTE: Candidate may work FULL TIME REMOTE




Work Status: USC / Green Card




(no C2C to your firm, no C2C to the candidate)




Skills




 * Azure Cloud Data Engineer
 * Cosmos DB, T-SQL
 * Microsoft Azure Cloud PaaS
 * Git/Jenkins/Bitbucket
 * Azure Cloud workflows
 * Azure cloud managed DBs/Systems, Managed SQL Instance
 * Big Data solutions: Delta Lake by Databricks




Required




 * Must be presently authorized to work in the U.S. without a requirement for work authorization sponsorship by our company for this position now or in the future.
 * Must be committed to incorporating security into all decisions and daily job responsibilities
 * 3+ of related experience in Cosmos DB/Similar DB technology
 * Experience with configuration management and building automation capabilities such as Git/Jenkins/Bitbucket
 * Experience with Microsoft Azure Cloud workflows
 * Experience in T-SQL and scripting skills.
 * Knowledge on Microsoft cloud managed DBs/Systems, e.g. Managed SQL Instance, Cosmos DB, Databricks Delta Lake
 * Experience with Big Data solutions such as Delta Lake by Databricks and SQL DBMSs
 * Independently analyze, solve, and correct issues in real time, providing problem resolution end-to-end
 * Identify new opportunities and help refine automation of regular processes, track issues, and document changes
 * Solve/Assist in complex query tuning and schema refinement
 * Expert in troubleshooting performance issues
 * Experience rightsizing Database object workflow for cost management
 * Ability to multi-task and context-switch effectively between different activities and teams
 * Join the on-call rotation with other Engineers
 * Must be able to both collaborate in a team-oriented environment and work independently with direction
 * Must be able to work in a fast-paced environment with the ability to handle multiple tasks




Preferred




 * Bachelor's degree in Computer Science, Computer Information Systems, Management Information Systems, or related field preferred
 * Azure SQL DB etc. will be a big plus
 * Experience with Microsoft Azure platform technologies like Databricks applications, Event Hub, Data Factory, Azure SQL, Synapse Analytics, Delta
 * Lake, Cosmos DB, or DevOps
 * Prior experience with large-scale projects
 * Experience with API development
 * Knowledge and working experience with Agile methodologies
 * Familiarity with JDBC connections to data sources","Entry level","https://www.linkedin.com/jobs/view/azure-cloud-data-engineer-w-cosmos-db-at-adame-services-3706394774?trk=public_jobs_topcard-title","United States","3 months ago","","","2023-08-31","","Staffing and Recruiting","Azure Cloud Data Engineer w/ Cosmos DB","Information Technology"
"91 applicants","2010798","Tiger Analytics","https://www.linkedin.com/company/tiger-analytics?trk=public_jobs_topcard-org-name","Full-time","Tiger Analytics is a global AI and analytics consulting firm. With data and technology at the core of our solutions, we are solving problems that eventually impact the lives of millions globally. Our culture is modeled around expertise and respect with a team-first mindset. Headquartered in Silicon Valley, you’ll find our delivery centers across the globe and offices in multiple cities across India, the US, UK, Canada, and Singapore, including a

substantial remote global workforce.

We’re Great Place to Work-Certified™. Working at Tiger Analytics, you’ll be at the heart of an AI revolution. You’ll work with teams that push the boundaries of what is possible and build solutions that energize and inspire.

Requirements

Curious about the role? What your typical day would look like?

As a Principal Data Engineer (Azure), you would have hands on experience working on Azure as cloud, Databricks and some exposure/experience on Data Modelling. You will build and learn about a variety of analytics solutions & platforms, data lakes, modern data platforms, data fabric solutions, etc. using different Open Source, Big Data, and Cloud technologies on Microsoft Azure.


 * Design and build scalable & metadata-driven data ingestion pipelines (For Batch and Streaming Datasets)
 * Conceptualize and execute high-performance data processing for structured and unstructured data, and data
   
   

harmonization


 * Schedule, orchestrate, and validate pipelines
 * Design exception handling and log monitoring for debugging
 * Ideate with your peers to make tech stack and tools-related decisions
 * Interact and collaborate with multiple teams (Consulting/Data Science & App Dev) and various stakeholders to meet deadlines, to bring Analytical Solutions to life
   
   

What do we expect?


 * Experience in implementing Data Lake with technologies like Azure Data Factory (ADF), PySpark, Databricks, ADLS,
   
   

Azure SQL Database


 * A comprehensive foundation with working knowledge of Azure Synapse Analytics, Event Hub & Streaming
   
   

Analytics, Cosmos DB, and Purview


 * A passion for writing high-quality code and the code should be modular, scalable, and free of bugs (debugging
   
   

skills in SQL, Python, or Scala/Java).


 * Enthuse to collaborate with various stakeholders across the organization and take complete ownership of
   
   

deliverables.


 * Experience in using big data technologies like Hadoop, Spark, Airflow, NiFi, Kafka, Hive, Neo4J, Elastic Search
 * Adept understanding of different file formats like Delta Lake, Avro, Parquet, JSON, and CSV
 * Good knowledge of building and designing REST APIs with real-time experience working on Data Lake or
   
   

Lakehouse projects.


 * Experience in supporting BI and Data Science teams in consuming the data in a secure and governed manner
 * Certifications like Data Engineering on Microsoft Azure (DP-203) or Databricks Certified Developer (DE) are
   
   

valuable addition.

Note: The designation will be commensurate with expertise and experience. Compensation packages are among the best in the industry.

Job Requirement


 * Mandatory: Azure Data Factory (ADF), PySpark, Databricks, ADLS, Azure SQL Database
 * Optional: Azure Synapse Analytics, Event Hub & Streaming Analytics, Cosmos DB and Purview
 * Strong programming, unit testing & debugging skills in SQL, Python or Scala/Java
 * Some experience of using big data technologies like Hadoop, Spark, Airflow, NiFi, Kafka, Hive, Neo4J, Elastic Search
 * Good Understanding of different file formats like Delta Lake, Avro, Parquet, JSON and CSV
 * Experience of working in Agile projects and following DevOps processes with technologies like Git, Jenkins & Azure DevOps
 * Good to have:
 * Experience of working on Data Lake & Lakehouse projects
 * Experience of building REST services and implementing service-oriented architectures
 * Experience of supporting BI and Data Science teams in consuming the data in a secure and governed manner
 * Certifications like Data Engineering on Microsoft Azure (DP-203) or Databricks Certified Developer (DE)
   
   

Benefits

This position offers an excellent opportunity for significant career development in a fast-growing and challenging entrepreneurial environment with a high degree of individual responsibility.","Associate","https://www.linkedin.com/jobs/view/principal-data-engineer-azure-at-tiger-analytics-3739548031?trk=public_jobs_topcard-title","Jersey City, NJ","1 month ago","","","2023-10-13","","Technology, Information and Internet","Principal Data Engineer (Azure)","Information Technology"
"Over 200 applicants","51711435","Check","https://www.linkedin.com/company/check-technologies?trk=public_jobs_topcard-org-name","Full-time","Check is leading a revolution in the payroll industry. Payroll is the backbone of the US economy, and yet it has not undergone significant innovation in decades. We invented the embedded payroll space to change that (see: Check: Solving the $150 Billion Payroll Problem, Bank On It; Episode 438), both for the business owners who spend countless hours per week running payroll and for the millions of Americans who depend on their next paycheck. At Check, our mission is to empower the most innovative software platforms to create new payroll businesses, and enable those businesses to transform the way companies pay their employees.

Check's culture is intensely caring: to each other as teammates, to our partners, and to the businesses and workers we serve. Our teams are made up of creative problem solvers and empathetic cross-functional collaborators who can simplify the complexities of payroll. We bring hustle and grit to our day-to-day, while building enduring solutions and relationships.

Check is hiring a full stack engineer to expand our embedded payroll platform: the intuitive APIs, delightful UIs, and product-backed services that simplify creating, growing, and running a payroll business for our partners. This is the perfect role for an engineer who wants to have an outsized business impact. Our technical stack includes Python, Django, React, TypeScript, Postgres, and AWS.

In this role, you will:


 * Scope, develop, and lead large technical projects with significant business impact
 * Build tools that support and automate complex payroll workflows for our partners and internal teams
 * Influence the culture, values, and processes of a growing engineering team
 * Use data-driven analysis to identify opportunities to achieve our business goals
 * Provide input on product and strategic decision-making
   
   

You may be a fit for this role if you:


 * Have a strong product/user orientation
 * Focus on iterative value delivery
 * Have led projects that simplify complex concepts through software
 * Enjoy navigating ambiguity and making decisions independently
 * Have interest in designing both frontend UIs and backend APIs
 * Love working with cross-functional stakeholders
   
   

We use:


 * Python (type-annotated) and Django for our API
 * React and TypeScript for our frontends
 * Postgres for our database
 * AWS for our cloud infrastructure
 * GitLab for issue tracking and CI/CD
   
   

What we offer:

At Check we value transparency and trust. It's important to us that every employee and candidate feels confident that they'll be treated (and compensated) fairly on our team. The engineering team specifically operates on a flat titling system, meaning everyone is either a Software Engineer or an Engineering Manager. We structure our titles this way to maximize equity, inclusion and opportunity within our team, and to minimize seniority bias. We want all engineers, regardless of job level, to feel empowered to lead projects they're passionate about and to collaborate with their fellow Checketeers as equals.

To that end, we will consider candidates with a variety of backgrounds and levels of expertise for this role, and we will determine the appropriate job level for each candidate based on their unique experience and qualifications. The expected annual salary for this role is between $160,000 and $220,000, depending upon the job level.

In the US, Check offers company-sponsored medical, dental, vision, short-term/long-term disability and basic life insurance coverage to all full-time employees, effective on your first day of work. We also provide stock options, flexible PTO and sick leave, 16 weeks of fully paid parental leave for all new parents and flexible return-to-work, 9 annual holidays, a 401k retirement plan, and a $100 monthly stipend for home internet and mobile phone expenses. Benefits may vary outside the US.

Check is proud to be an Equal Opportunity employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. Check is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at hello@checkhq.com with ""accommodations"" in the subject line.

Check participates in E-Verify and will provide the federal government with Form I-9 information from all new employees to confirm that they are authorized to work in the U.S. Check does not use E-Verify to pre-screen applicants.","Entry level","https://www.linkedin.com/jobs/view/software-engineer-at-check-3673087302?trk=public_jobs_topcard-title","San Francisco, CA","5 months ago","","","2023-06-27","","Technology, Information and Internet","Software Engineer","Engineering and Information Technology"
"Over 200 applicants","1534","The Home Depot","https://www.linkedin.com/company/the-home-depot?trk=public_jobs_topcard-org-name","Full-time","Position Purpose

The Software Engineer II is responsible for independently developing and assisting in the design of a product that our customers and associates love. As a Software Engineer II, you will be part of a dynamic team with engineers of all experience levels who help each other build and grow technical and leadership skills while creating, deploying, and supporting production applications. In addition, Software Engineer IIs may be involved in configuration, security, resilience, performance tuning and production monitoring.

Key Responsibilities


 * 60% Delivery and Execution - Collaborates and pairs with other product team members (UX, engineering, and product management) to create secure, reliable, scalable software solutions; Documents, reviews and ensures that all quality and change control standards are met; Works with Product Team to ensure user stories that are developer-ready, easy to understand, and testable; Writes custom code or scripts to automate infrastructure, monitoring services, and test cases; Writes custom code or scripts to do destructive testing to ensure adequate resiliency in production; Program configuration/modification and setup activities on large projects using HD approved methodology; Configures commercial off the shelf solutions to align with evolving business needs Creates meaningful dashboards, logging, alerting, and responses to ensure that issues are captured and addressed proactively
 * 20% Learning - Actively seeks ways to grow and be challenged using both formal and informal development channels; Learns through successful and failed experiment when tackling new problems
 * 20% Plans and Aligns - Collaborates with other team members in agile processes; Assists in creating new and better ways for the team to be successful; Relates openly and comfortably with diverse groups of people; Builds partnerships and works collaboratively with others to meet shared objectives
   
   

Direct Manager/Direct Reports


 * This position typically repots to Software Engineer Manager or Sr. Manager
 * This position has 0 Direct Reports
   
   

Travel Requirements


 * No travel required.
   
   

Physical Requirements


 * Most of the time is spent sitting in a comfortable position and there is frequent opportunity to move about. On rare occasions there may be a need to move or lift light articles.
   
   

Working Conditions


 * Located in a comfortable indoor area. Any unpleasant conditions would be infrequent and not objectionable.
   
   

Minimum Qualifications


 * Must be eighteen years of age or older.
 * Must be legally permitted to work in the United States.
   
   

Preferred Qualifications


 * 1-3 years of relevant work experience
 * Experience in writing SQL queries against a relational database
 * Experience in version control systems
 * Experience in front end technology such as HTML, CSS, and Javascript/Typescript frameworks
 * Experience in an object-oriented programming language (preferably Java)
 * Experience in source code version control
 * Experience in Relational or noSQL database technology
 * Experience in cloud computing techniques
 * Experience in CI/CD tools
 * Experience in microservice-based architecture
 * Experience with modern debugging and root cause analysis techniques
 * Exposure to security frameworks for user and services authorization and authentication
 * Exposure to creating and executing unit, functional, destructive and performance tests
   
   

Minimum Education


 * The knowledge, skills and abilities typically acquired through the completion of a bachelor's degree program or equivalent degree in a field of study related to the job.
   
   

Preferred Education


 * No additional education
   
   

Minimum Years Of Work Experience


 * 2
   
   

Preferred Years Of Work Experience


 * No additional years of experience
   
   

Minimum Leadership Experience


 * None
   
   

Preferred Leadership Experience


 * None
   
   

Certifications


 * None
   
   

Competencies


 * Global Perspective
 * Manages Ambiguity
 * Nimble Learning
 * Self-Development
 * Collaborates
 * Cultivates Innovation
 * Situational Adaptability
 * Communicates Effectively
 * Drives Results
 * Interpersonal Savvy","Entry level","https://www.linkedin.com/jobs/view/software-engineer-ii-remote-at-the-home-depot-3763054232?trk=public_jobs_topcard-title","Austin, TX","3 weeks ago","","","2023-11-11","","Retail","Software Engineer II (Remote)","Engineering and Information Technology"
"120 applicants","81606529","Steneral Consulting","https://www.linkedin.com/company/steneral-consulting?trk=public_jobs_topcard-org-name","Full-time","Job: Sr. Data Software Engineer

Location: 100% Remote

Duration: 6 month Contract to Hire (USC/ GC Holder ONLY)

Top Skills

SQLAzure
 * Data Factory
 * DBT
 * ETL work
 * Proven ability to complete projects in a timely manner while clearly measuring progress
 * Strong software engineering fundamentals (data structures, algorithms, async programming patterns, object-oriented design, parallel programming)
 * Strong understanding and demonstrated experience with at least one popular programming language (.NET or Java) and SQL constructs.
 * Experience writing and maintaining frontend client applications, Angular preferred
 * Strong experience with revision control (Git)
 * Experience with cloud-based systems (Azure / AWS / GCP).
 * High level understanding of big data design (data lake, data mesh, data warehouse) and data normalization patterns
 * Demonstrated experience with Queuing technologies (Kafka / SNS / RabbitMQ etc)
 * Demonstrated experience with Metrics, Logging, Monitoring and Alerting tools
 * Strong communication skills
 * Strong experience with use of RESTful APIs
 * High level understanding of HL7 V2.x / FHIR based interface messages.
 * High level understanding of system deployment tasks and technologies. (CI/CD Pipeline, K8s, Terraform).
   
   

Project/Day To Day


 * Communicate with business leaders to help translate requirements into functional specification
 * Develop broad understanding of business logic and functionality of current systems
 * Analyze and manipulate data by writing and running SQL queries
 * Analyze logs to identify and prevent potential issues from occurring
 * Deliver clean and functional code in accordance with business requirements
 * Consume data from any source, such a flat files, streaming systems, or RESTful APIs
 * Interface with Electronic Health Records
 * Engineer scalable, reliable, and performant systems to manage data
 * Collaborate closely with other Engineers, QA, Scrum master, Product Manager in your team as well as across the organization
 * Build quality systems while expanding offerings to dependent teams
 * Comfortable in multiple roles, from Design and Development to Code Deployment to and monitoring and investigating in production systems.","Mid-Senior level","https://www.linkedin.com/jobs/view/remote-work-need-sr-data-software-engineer-at-steneral-consulting-3735082431?trk=public_jobs_topcard-title","United States","1 month ago","","","2023-10-10","","IT Services and IT Consulting","Remote Work - Need Sr Data Software Engineer","Engineering and Information Technology"
"192 applicants","950219","Vodastra Technologies","https://www.linkedin.com/company/vodastratechnologies?trk=public_jobs_topcard-org-name","Full-time","Industry: Healthcare / Health Services

Job Category: Medical / Health - Healthcare IT

JOB SUMMARY:

This job is responsible for designing and engineering data solutions for the enterprise and, working closely with business, analytic and IT stakeholders, assists with the development and maintenance of these solutions. This includes coding data ingestion pipelines, transformations and delivery programs/logic for people and systems to access data for operational and/or analytic needs. Duties include but are not limited to the coding, testing, and implementation of ingestion and extraction pipelines, transformation and cleansing processes, and processes that load and curate data in conformed, fit-for-purpose data structures. The incumbent is expected to partner with others throughout the organization (including other engineers, architects, analysts, data scientists, and non-technical audiences) in their daily work. The incumbent will work with cross-functional teams to deliver and maintain data products and capabilities that support and enable strategies at business unit and enterprise levels. The incumbent is expected to utilize technologies such as, but not limited to: Google Cloud Platform.

ESSENTIAL RESPONSIBILITIES:

In partnership with other business, platform, technology, and analytic teams across the enterprise, design, build and maintain well-engineered data solutions in a variety of environments, including traditional data warehouses, Big Data solutions, and cloud-oriented platforms. Create high performance cloud and big data systems to be used with operational and analytic applications. Work with internal and external platforms and systems to connect and align on data sourcing, flow, structure, and subject matter expertise. Work with business stakeholders and strategic partners to implement and support operational and analytic platforms. This may include products purchased by the organization that must be ingested or modeled/derived data maintained by enterprise platforms and data consumers. Working across multiple, disparate systems and platforms, design, code, test, implement, and maintain scalable and extensible frameworks that support data engineering services. Align with security, data governance and data quality programs by driving assigned components of metadata management, data quality management, and the application of business rules. Develop and maintain associated data engineering processes and participate in required operating procedures as part of the enterprise’s overall information management activities. Includes data cleansing, standardization, technical metadata documentation, and the de-identification and/or tokenization of data. Develop, optimize and/or maintain machine learning and AI engineering processes (MLOps) that are deployed to cloud or big data environments. These may be based on prototypes built by data scientists or capability frameworks implemented to allow data scientists to build efficiently in production environments. Develop tasks across multiple projects with limited need for guidance. This includes providing guidance and education to Intermediate and Junior contributors within team. Manage relationships with customers of the function. Attend meetings with customers on a stand-alone basis or with team as needed. Establish standards and patters for high performance data ingestion, transformation, and delivery of data analytic needs. Keep current with Big Data technologies in order to recommend best tools in order to perform current and future work Other duties as assigned or requested.

EDUCATION:

Required

Bachelor's Degree in Software Engineering, Information Systems, Computer Science, Data Science or related field

EXPERIENCE:

Required

5 years in Data platform development, data engineering, software development, or data science

3 years in Big data or cloud data platform Preferred

3 years in Healthcare Industry

3 years of Data Warehousing

3 years of Database Administration

SKILLS:

SQL Data Warehousing

Problem-Solving

Communication Skills

Analytical Skills

Spark or Python or related tool

Cloud Technologies

Powered by JazzHR

g4JEOLNM0S","Mid-Senior level","https://www.linkedin.com/jobs/view/senior-big-data-engineer-%E2%80%93-fully-remote-at-vodastra-technologies-3756990715?trk=public_jobs_topcard-title","New York, NY","1 month ago","","","2023-11-03","","Internet Publishing","SENIOR BIG DATA ENGINEER – FULLY REMOTE","Engineering and Information Technology"
"37 applicants","1408309","Quick Services LLC (QSL)","https://www.linkedin.com/company/quickservicesllc?trk=public_jobs_topcard-org-name","Full-time","CLEARANCE REQUIRED: Top Secret with SCI Eligibility

This job is located in: Tampa, Florida

QSL is looking for highly motivated individuals to support U.S. Special Operations Command in various locations.

We are looking for an experienced Data Engineer to join our team! In this role, you will support robust and repeatable data manipulation, large scale infrastructure for data ingestion, and stunning data visualization for custom client applications.

Qualifications And Job Summary


 * 5+ years of Data Engineering or Data Management experience.
 * Strong SQL skills, and ability to identify issues with data.
 * Understanding of database and database design
 * Demonstrated experience with cleaning, processing, managing, and optimizing performance of large volumes of data.
 * Demonstrated experience within an AWS environment (AWS certifications preferred)
 * Familiarity with industry best-practices for software-hardware optimization when processing large sets of data.
 * Experience with machine learning, with statistical modeling, time-series forecasting and/or geospatial analytics is preferred.
 * Experience with Hadoop, Spark or other parallel storage/computing processes is a plus.
 * Ability to travel to and work on site at clients as needed.
 * Excellent communication skills both verbal and written
 * Hands on experience with cloud native data warehousing and data lake solutions with Azure ADF, Redshift, Snowflake, etc
 * Engineering experience and seasoned coder in one or more relevant language: Python, Java, Scala, SQL, etc
   
   

Why work for QSL?

Our founders, Mel Wick and Bill Cronin, retired from storied careers in the Special Operations Forces (SOF) Community. Like many Americans and military veterans, they felt a strong desire to support the nation’s response to the 9/11/2001 terrorist attacks on the World Trade Center and the Pentagon in any way they could. They established QSL to do just that, Stay in the Fight! QSL is built on a SOF culture, emphasizing selfless-service and teamwork. Our employees work to ensure that warfighters have every possible resource and all necessary support to safely accomplish their missions in defense of our nation.

QSL's Benefit Package

Because we believe our employees are our most valuable asset, offering a competitive comprehensive compensation package is very important to us. It is the goal of QSL to attract and retain the highest level of experience and technical talent necessary for successful performance. In order to accomplish this, we feel that it is necessary to provide satisfying work, an excellent work environment, and we continually monitor the marketplace to ensure that our total compensation/benefit package remains competitive.

Listed below are some of our standard benefits. We combine all traditional paid time off (Federal holidays, sick time, leave time personal days, jury duty, bereavement, etc.) into one category which allows employees flexibility in how they use their leave time and enables them to better balance their career with their personal needs.


 * Combined Paid Time Off (PTO)
 * Medical, Dental, Life Insurance
 * Disability (Short-Term and Long-Term)
 * Vision Insurance (CONUS-based employees)
 * Flexible Spending Account (FSA)
 * 401(k) Retirement Plan
 * Employee Referral Bonus Program
 * Employee Discount Programs
 * Critical Illness and Accident Insurance
 * Employee Assistance Program
   
   

We are an Equal Opportunity Employer. We do not and will not discriminate in employment and personnel practices based on race, sex, age, disability, veteran status, religion, national origin or any other basis prohibited by applicable law. Hiring, transferring, and promotion practices are performed without regard to the above listed items. EEO/AAP, M, F, V, D.","Entry level","https://www.linkedin.com/jobs/view/data-engineer-at-quick-services-llc-qsl-3766831893?trk=public_jobs_topcard-title","United States","1 week ago","","","2023-11-22","","Defense and Space Manufacturing","Data Engineer","Strategy/Planning and Information Technology"
"Over 200 applicants","93380834","Sonitalent Corp","https://www.linkedin.com/company/sonitalent-corp?trk=public_jobs_topcard-org-name","Full-time","Role: Data Analyst & Testing Engineer




Location: Washington, DC 100% Remote/ Must be in EST Time Zone




Duration: 6-12+ months




Visa: USC, GC, GC EAD, H4 EAD USC/GC would be highly preferred




Interview: MS Team Video




Positions: 2




They must have Freddie Mac or Fannie Mae experience or (Ginnie Mae or Ginny Mae).




Mortgage OR Financial, Oracle, SQL, Gherkin, Python OR Java, Data Analysis, Modeling, Linux, AWS




Top Technical Skills




 * Secondary Mortgage Or Financial experience
 * Oracle/SQL advanced SQL
 * Test Case, Plans, Strategy Acceptance Criteria
 * BDD...Gherkin
 * Python.... for scripting for Data analysis
 * Python and Java..... Ability to read for code analysis
 * Data Modeling
 * AWS/Linux/Unix




Top 3 Soft Skills




 * Good Communications... to all levels
 * Agile
 * Strong Documentation (for internal controls)




Company / Project Description / Business Driver




Supporting AWS Cloud Migration, Enterprise Mandate to get to the Cloud




Job Description




 * Analyze, document and articulate business requirements for complex mathematical, business, and financial modeling logic for software coding. Design and execute test cases for modeling and analytical software applications to ensure they meet business needs and model requirements.
 * Develop detailed specifications of application and document in a form that it can be used for coding application. This includes architecture diagrams, process flows, and other information or processes needed to describe required system changes for development, QA, and other internal customers.
 * Collaborate with managers or practitioners in the business unit to determine systems requirements and functionalities needed in new or revised application.
 * Extract data requirements through various methods including individual and group discussions, independent data analysis and by extracting from a suite of mathematical functions and code.
 * Confer with business units and technical staff to understand data usage, lineage and attributes. Perform or review coding done to render specifications into application functionalities, screens, or outputs.
 * Develop test plan and/or test application in development status or debug application in production mode.
 * Document or review documentation of steps in specification development, coding, testing and user acceptance for future reference and for internal control purposes.
 * May provide support to applications in production by tracking production problems and troubleshooting them to sustain application in production.
 * Participate in project meetings to plan rewrite of addition to application in production or being revised for release into production.
 * May mentor or provide technical guidance to less experienced staff.
 * Lead cross functions to ensure application enhancements quality meets expected business results. May lead test strategy and facilitate customer and end user testing through test & learn development iterations and closing the loop on customer feedback. Manage end to end business process impacts for cross functional/complex solutions
 * Promote productive relationships between stakeholders and technology partners, providing clarity & ensuring processes are streamlined.","Entry level","https://www.linkedin.com/jobs/view/data-analyst-testing-engineer-at-sonitalent-corp-3657150422?trk=public_jobs_topcard-title","United States","4 months ago","","","2023-07-07","","Staffing and Recruiting","Data Analyst & Testing Engineer","Information Technology"
"54 applicants","2259276","Nexwave","https://www.linkedin.com/company/nexwave-talent-management-solutions-pvt-ltd?trk=public_jobs_topcard-org-name","Full-time","Role: GCP Data Analyst

Location: Austin, TX ( Onsite from day 1 )




Full-Time position




Tech Mahindra : Google





 * clean-up and organize unstructured data from different sources
 * discover correlations and insights from a unstructured set of data
 * independently run researches and analyses on different data sources
 * use (even in a basic way) tools and framework to process and visualize data
 * with a knowledge for community and people orgs domains as a plus.




Thankyou

Mahesh

pmahesh@nexwaveinc.com","Mid-Senior level","https://www.linkedin.com/jobs/view/gcp-data-analyst-at-nexwave-3773539351?trk=public_jobs_topcard-title","Austin, TX","2 hours ago","Venky Sunkara","https://www.linkedin.com/in/venkysunkaranexwave","","","IT Services and IT Consulting","GCP Data Analyst","Engineering and Information Technology"
"82 applicants","10331477","Veda","https://www.linkedin.com/company/veda-data-solutions?trk=public_jobs_topcard-org-name","Full-time","Veda helps patients get the care they need by untangling complex data management problems using advanced scientific approaches and in-depth collaboration. Our technology reflects what our people provide: quality without ego, honesty backed by science, and warmth in an industry not known for having much heat.

Veda is made up of talented professionals that are driven to do meaningful work to change healthcare from the inside out. We are also friends, parents, partners and caregivers. Veda’s benefits reflect our values—we offer fully paid, low or no-deductible medical, dental and vision insurance for our employees and their families. We ensure that employees can take time off to recharge and have flexibility to care for themselves and their families.

Veda is looking for sharp-minded do-gooders who share our values:

Collaboration Working together to identify solutions to current problems

Openness Actively listening, sharing and holding space for new ideas, perspectives and people

Integrity Doing the right thing, honestly and transparently.

Grit Displaying passion and perseverance to achieve our goals.

Ready to build the future with us?

This Data Engineer will report to the Technical Engineering Manager for the Quantym team. You will be a trusted contributor to the design of our data pipeline and application architecture, keeping security, cost, & performance in mind. You will work within a cross-disciplinary team of data engineers, front end designers, data scientists, and cloud architects throughout all stages of development - from brainstorming to coding to ongoing support.

As a Data Engineer on the team, you will help move our software development practices forward by guiding others on design and best practices. You'll mentor other engineers, conduct code reviews, and help write technical requirements across product-driven teams. Most importantly, you're excited to be an integral part of a team that values quick iteration, embraces new tech, and relies on data-driven decision making to deliver value for customers and the firm at large to move our products forward.

About You:


 * You are Self-motivated, driven to continue learning
 * You are proficient in shipping production Python and SQL code
 * You have contributed to hiring, interviewing and training peer and junior engineers
 * You recognize what it takes to add value as a member of high-performing engineering teams. You can identify key success factors, inspire a culture of learning and knowledge sharing, and proactively intervene when goals are not met
 * You have a background in product development, and a love for wrangling data and delivering solutions to business problems on a fast-paced team
 * You have the most fun when you get to share ideas and solve complex problems by applying excellent organizational skills and collaborative values
 * You’re comfortable working throughout the Software Development Lifecycle: refining requirements, designing solutions, testing changes, and delivering in collaboration with Engineering Management, Product, and Customer-facing teams
   
   
   

Required Qualifications:


 * 4+ years of experience shipping production python code
 * 4+ years of professional experience working in a developer role (devops or similar) that heavily involves high-frequency, customer-related development.
   
   
   

Including but not limited to:


 * Database design and data modeling experience including performance optimization
 * Experience working in report development tools like tableau, powerBI
 * Experience working in a variety of data stores (sql and no-sql) and file formats.
 * Writing automated tests
 * using infrastructure as code, CICD pipelines, containerization technologies
 * Supporting, designing, building, and maintaining high volume/high frequency data pipelines for large scale, complex data sets
 * Working with event-driven and/or microservice ecosystems
 * Maintaining, expanding and evolving legacy code and frameworks
   
   
   

Preferred Qualifications/ Desired Experience:


 * Designing applications that make use of AWS services including Lambda, S3, Glue, RDS, DynamoDB, ECS, EMR, Data Lake Formation
 * ETL and orchestration engines including Spark, airflow, Informatica
 * Building production applications with Javascript or Typescript
 * Python data science libraries like Pandas, NumPy, Matplotlib; familiarity with these is appreciated but the focus of this work is backend, not data science/model development
 * Working in highly regulated industries like finance, health care or defense
   
   
   

Our COVID Commitment: Veda is committed to prioritizing the health, safety and emotional well being of our employees and their families. Veda has always embraced the benefit of each employee working remotely, collaboratively.

All employees are required to be located within the USA.

We look forward to learning more about you -- apply to join the Veda team today!","Not Applicable","https://www.linkedin.com/jobs/view/senior-data-engineer-at-veda-3766989452?trk=public_jobs_topcard-title","United States","6 days ago","","","2023-11-27","","Transportation, Logistics, Supply Chain and Storage","Senior Data Engineer","Information Technology"
"Over 200 applicants","22582874","Argano","https://www.linkedin.com/company/argano?trk=public_jobs_topcard-org-name","Full-time","Provide high quality, technical delivery as a member of an Argano 4 Microsoft project team. Primarily responsible for analysis, design, and development of data engineering solutions to ingest, store and transform data for data analytics and data migration in support of business applications such as Dynamics 365.

Key Responsibilities


 * Collaborate with stakeholders to understand needs, model tables using data modeling best practices, and develop data orchestration processes to ensure the timely delivery of high-quality data
 * Work with client stakeholders to understand data-oriented project requirements
 * Think and work agile, including automated testing, continuous integration, and deployment
 * Manage numerous project tasks concurrently and strategically, prioritizing when necessary
 * Proven ability to work as part of a virtual team of technical consultants working from different locations (including offshore) around project delivery goals
   
   

Required Qualifications, Experience & Skills


 * 5 years' experience consulting in Data or Dynamics business application domain
 * Expert experience with T-SQL language
 * Experience in creating Data Factory pipelines to orchestrate ingestion and transformation data for use in analytics and system integration
 * Experience with the Dynamics 365 data model
 * Experience using modern Azure data services such as Azure Synapse, Azure SQL Database, Azure Data Lake Storage Gen 2, Microsoft Fabric
 * Familiarity with security configuration and security policies, and best practices within Azure
 * Curious and tenacious when it comes to leveraging new Azure technology to deliver novel solutions for clients
   
   

Additional Qualifications & Skills


 * Azure Data Engineer Associate Certification preferred (DP-203)
 * Familiar with data lakehouse patterns and practices preferred
 * Bachelor’s Degree in Information Systems Management, Computer Science, or related field
 * Demonstrated ability to lead project workstream including interfacing in a client facing role
 * Familiar with Agile implementation methodology
 * Experience working with a multicultural and/or multilingual team across several time zones remotely
   
   

The base compensation range for this position is $92,000 - $112,000 USD commensurate with experience. Argano also offers a performance-based bonus and strong benefits package including Medical, Dental, Vision, 401K, Paid Parental Leave and Flexible Time Off.

#Arbela","Entry level","https://www.linkedin.com/jobs/view/data-engineer-at-argano-3725505920?trk=public_jobs_topcard-title","United States","2 months ago","","","2023-09-27","","IT Services and IT Consulting","Data Engineer","Information Technology"
"Over 200 applicants","82819087","Harbor Health","https://www.linkedin.com/company/harbor-health-team?trk=public_jobs_topcard-org-name","Full-time","Harbor Health looking for a Senior Software Engineer to become a member of our team! Harbor Health is an entirely new multi-specialty clinic group in Austin, TX utilizing a modern approach to co-create health with those who get, give, and pay for it, allowing everyone to fully flourish. Join us as we build a fully integrated system that connects care to a better payment model that truly puts the human being at the center.

As a Senior Software Engineer with a focus on data engineering at Harbor Health, you will play a pivotal role in our data transformation initiatives. You will work with a team of dedicated professionals using AWS, Snowflake, and DBT Labs to create efficient data pipelines. Your expertise in Python, SQL, and SQL performance optimization, along with your ability to take a proactive and collaborative approach to your work, will be essential in shaping the future of healthcare data analytics.

Our Sr. Software Engineer will be responsible for:


 * Collaborate with cross-functional teams to design, develop, and maintain data engineering solutions.
 * Develop and optimize ETL processes using DBT Labs and other technologies.
 * Design and implement data models and data transformations for our Snowflake data warehouse.
 * Create robust, scalable, and maintainable code using Python.
 * Write efficient SQL queries and be an expert at SQL performance tuning.
 * Participate in daily standup meetings with the team to ensure alignment and effective collaboration.
 * Independently manage and execute projects while being adaptable and productive, whether working alone or in a team.
   
   

Successful Sr. Software Engineer's will have:


 * Bachelor's degree in Computer Science, Software Engineering, or a related field (or equivalent work experience).
 * 5+ years in a Data Engineering role with proven experience as a software engineer
 * Demonstrated strong proficiency in AWS, Snowflake, and DBT Labs.
 * Advanced skills in Python for data engineering and transformation.
 * Expertise in SQL and SQL performance optimization.
 * Curious and action-oriented mindset, self-motivated, and able to work effectively both independently and in a team.
 * Excellent problem-solving skills and the ability to drive projects to completion.
   
   

Additional Skills & Experiences Preferred include:


 * Kimball data modeling experience
 * Inmon data modeling experience
 * Experience working with healthcare data and healthcare quality metrics such as HEDIS measures.
   
   

Ability to travel as needed, less than 10% of time.

If you are passionate about health care and you want to create something new together, we want you to be apart of our team!

Powered by JazzHR

8Lt6GnsYsF","Mid-Senior level","https://www.linkedin.com/jobs/view/senior-software-engineer-data-engineering-at-harbor-health-3745476262?trk=public_jobs_topcard-title","Texas, United States","1 month ago","","","2023-10-20","","Internet Publishing","Senior Software Engineer (Data Engineering)","Engineering and Information Technology"
"Over 200 applicants","18383806","Descript","https://www.linkedin.com/company/descript?trk=public_jobs_topcard-org-name","Full-time","Our vision is to build the next generation platform to enable easy and fast creation of audio and video content powered by cutting-edge AI. Building a revolutionary way to record, transcribe, edit and mix spoken audio and video comes with a series of unique challenges and requires solving hard and complex problems. As Descript continues to grow, with generative AI at the heart of our mission to empower our users, we are actively seeking a Senior AI Platform Engineer.

This pivotal role involves deploying and optimizing our cutting-edge AI models for audio and video content creation. The ideal candidate for this role has extensive experience building infrastructure to efficiently train and deploy state of the art models, improving performance of model training and inference, strong low-level understanding of GPU workloads, and thrives in a fast moving, high-ownership environment.

What You'll Do


 * Design, implement, and maintain our AI infrastructure supporting our machine learning life cycle, including data ingestion, training, evaluation, and deployment.
 * Collaborate with Machine Learning Engineers and Data Scientists to understand their infrastructure needs and ensure our AI systems are robust, scalable, and efficient.
 * Work on optimizing and scaling of our models and algorithms for efficient inference.
 * Deploy, monitor, and manage AI models in production.
 * Implement rigorous security protocols to protect sensitive data and prevent abuse.
 * Collaborate with cross-functional teams to drive product requirements and improve our AI platform.
 * Stay up-to-date with the latest industry trends and technologies, and be ready to present potential use cases to the team.
   
   

What You Bring


 * Experience in deploying and managing AI models in production.
 * Knowledge of Python, C/C++, CUDA, and experience profiling GPU performance and distributed training runs.
 * Experience with machine learning frameworks like PyTorch, TensorFlow or similar.
 * Familiarity with cloud platforms (AWS, Google Cloud, Azure) and container technologies (Docker, Kubernetes).
 * Strong problem-solving abilities and excellent communication skills.
   
   

Nice to Have:


 * Experience with generative AI models.
 * Familiarity with audio and video processing.
 * Knowledge of DevOps best practices.
   
   

At our current size and stage, we embrace a flat organizational structure and value the expertise and contributions of every team member. As such, we have a unified job title for our engineering roles where everyone, including those with Staff-level scope, is considered a Software Engineer. While titles may not change, we are actively seeking senior and above Software Engineers to join our team.

The base salary range for this role is $160,000- $215,000/year. Final offer amounts will carefully consider multiple factors, including prior experience, expertise, location, and may vary from the amount above.

About Descript

Descript is building a simple, intuitive, fully-powered editing tool for video and audio — an editing tool built for the age of AI. We are a team of 125 — with a proven CEO and the backing of some of the world's greatest investors (OpenAI, Andreessen Horowitz, Redpoint Ventures, Spark Capital).

Descript is the special company that's in possession of both product market fit and the raw materials (passionate user community, great product, large market) for growth, but is still early enough that each new employee has a measurable influence on the direction of the company.

Benefits include a generous healthcare package, catered lunches, and flexible vacation time. We currently have offices in San Francisco and Montreal, and are open to folks working remotely between PT and ET time zones. Whether you love WFH or can't wait to get back to being in person, we're interested in offering an environment that works for you.

Descript is an equal opportunity workplace—we are dedicated to equal employment opportunities regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity, or Veteran status. We believe in actively building a team rich in diverse backgrounds, experiences, and opinions to better allow our employees, products, and community to thrive.","Entry level","https://www.linkedin.com/jobs/view/software-engineer-ai-ml-at-descript-3648295510?trk=public_jobs_topcard-title","San Francisco, CA","5 months ago","","","2023-06-23","","Technology, Information and Internet","Software Engineer - AI/ML","Engineering and Information Technology"
"Be among the first 25 applicants","66758487","RIT Solutions, Inc.","https://www.linkedin.com/company/rit-solutions-inc?trk=public_jobs_topcard-org-name","Full-time","100% remote

*CANDIDATES CANNOT LIVE IN - VT, RI, NY, NJ, NH, MI, ME, MA, DE, CO, CT, CA, and AZ*

LinkedIn profile required

Position Summary

The Senior Data Engineer for the Azure infrastructure will be responsible for the day to day operations of a large data warehouse, and will work closely with the business, product team, and the technical staff to ensure alignment to goals and objectives. Utilizing experience with Big Data, this position will drive consensus on designs of stable, reliable and effective dynamic ETL pipelines leveraging Azure Synapse Analytics Pipelines.

Essential Job Functions


 * Drive consensus on designs of stable, reliable and effective dynamic ETL pipelines leveraging Azure Synapse Analytics Pipelines.
 * Perform root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
 * Design, implement, and document data load processes from disparate data sources into Azure Synapse Pipelines.
 * Work with Continuous Integration/Delivery using Azure DevOps and Github.
 * Provide data management, monitoring, troubleshooting and support to client success
 * Create various triggers to automate the pipeline in Azure Synapse Analytics Pipelines.
 * Tune SQL queries in Azure SQL DB, Azure Synapse and solve complex data challenges and deliver insights that help our customers achieve their goals.
 * Self-organize as part of a small-size scrum team and apply data engineering skills.
 * Follows industry best practices and meets company's security and performance and requirements
   
   

Knowledge, Skills and Abilities


 * Minimum three (3) years' experience with MS SQL/T-SQL
 * Minimum three (3) years' experience with Azure SQL
 * Minimum three (3) years' experience with Apache Spark (PySpark)
 * Minimum of three (3) years' experience with Azure Data Factory or Azure Synapse building dynamic ETL pipelines
 * Minimum three (3) years' experience building dynamic Spark notebooks in Azure Synapse Spark or Azure Databricks
 * Minimum three (3) years' experience with Python
 * Minimum two (2) years' experience with a public cloud (AWS, Microsoft Azure, Google Cloud)
 * Experience working with parquet, json , delta, avro and csv files
 * In-depth understanding of data management (e.g. permissions, recovery, security and monitoring)
 * Experience with Data Warehouse Architecture
 * Strong analytic skills related to working with structured, semi-structured and unstructured datasets.
 * Excellent analytical and organization skills required
 * Ability to understand user requirements
 * Client service mindset
 * Excellent verbal and written communication skills
 * Excellent problem solving skills
 * Familiarity with Agile frameworks a plus
   
   

Education and Experience


 * Bachelor's degree in related discipline or combination of equivalent education and experience
 * 7-10 years of experience in similar field","Entry level","https://www.linkedin.com/jobs/view/data-engineer-remote-at-rit-solutions-inc-3768004842?trk=public_jobs_topcard-title","Dallas, TX","6 days ago","","","2023-11-28","","Staffing and Recruiting","Data Engineer -remote","Information Technology"
"48 applicants","42280","Enterra Solutions, LLC","https://www.linkedin.com/company/enterra-solutions-llc?trk=public_jobs_topcard-org-name","Full-time","LOCATION: U.S. Eastern Time Zone

Must reside in the US – preferably in the Eastern Time Zone. Remote working permitted. Must be eligible to work in the US without sponsorship now or in the future. This is a full-time position with benefits. Contractors will not be considered for this position.

Who we are:

Enterra provides solutions that leverage sophisticated machine learning, artificial intelligence (ontologies, inference engines and rules) and natural language processing to provide highly actionable insights and recommendations to business users. Today, our solutions impact just about every aspect of the products you buy at your local store – from what is available to how it is priced and even where it is placed on the shelf. Our SolaaS (Solution as a Service) solutions are deployed within private clouds – principally on Azure. We help transform market-leading companies into true data-driven digital enterprises.

What you will do:

The ideal candidate must be collaborative, and deadline driven. Because of the nature of our work and our technology, successful candidates must take a growth mindset and be comfortable with ambiguity, with the ability to take a proactive, structured approach to achieve results. Results-orientation and deadline driven are critical in our fast-paced environment.

The successful candidate will join a diverse team to:


 * Build unique high-impact business solutions utilizing advanced technologies for use by world class clients.
 * Create and maintain the underlying data pipeline architecture for the solution offerings from raw client data to final solution output.
 * Create, populate, and maintain data structures for machine learning and other analytics.
 * Use quantitative and statistical methods to derive insights from data.
 * Guide the data technology stack used to build Enterra's solution offerings.
 * Combine machine learning, artificial intelligence (ontologies, inference engines and rules) and natural language processing under a holistic vision to scale and transform businesses — across multiple functions and processes.
   
   

Responsibilities Include:


 * Work with other Enterra personnel to develop and enhance commercial quality solution offerings
    * Create and maintain optimal data pipeline architecture, incorporating data wrangling and Extract-Transform-Load (ETL) flows.
    * Assemble large, complex data sets to meet analytical requirements – analytics tables, feature-engineering etc.
    * Build the infrastructure required for optimal, automated extraction, transformation, and loading of data from a wide variety of data sources using SQL and other 'big data' technologies such as Databricks.
    * Build automated analytics tools that utilize the data pipeline to derive actionable insights.
    * Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
    * Design and develop data integrations and data quality framework
    * Develop appropriate testing strategies and reports for the solution as well as data from external sources.
    * Evaluate new technology for use within Enterra.

 * Work with other Enterra and client personnel to administer and operate client-specific instances of the Enterra solution offerings
 * Configure the data pipelines to accommodate client-specific requirements to onboard new clients.
 * Perform regular operations tasks to ingest new and changing data – implement automation where possible.
 * Implement processes and tools to monitor data quality - investigate and remedy any data-related issues in daily solution operations.
   
   

Requirements:


 * Bachelor's degree in Computer Science or a STEM (Science, Technology, Engineering or Math) field required
 * Minimum of 3 years hands on experience as a data engineer or similar position.
 * Minimum of 3 years commercial experience with Python or Scala Programming Language
 * Minimum of 3 years SQL and experience working with relational databases (Postgres preferred).
 * Experience with at least one of the following – Databricks, Spark, Hadoop or Kafka
 * Demonstratable knowledge and experience developing data pipelines to automate data processing workflows
 * Demonstratable experience in data modeling
 * Demonstratable knowledge of data warehousing, business intelligence, and application data integration solutions
 * Demonstratable experience in developing applications and services that run on a cloud infrastructure Azure preferred
 * Excellent problem-solving and communication skills
 * Ability to thrive in a fast-paced, remote environment.
 * Comfortable with ambiguity with the ability to build structure and take a proactive approach to drive results.
 * Attention to detail – quality and accuracy in work is essential.
   
   

The following additional skills would be beneficial:


 * Knowledge of one or more of the following technologies: Data Science, Machine Learning, Natural Language Processing, Business Intelligence, and Data Visualization.
 * Knowledge of statistics and experience using statistical or BI packages for analyzing large datasets (Excel, R, Python, Power BI, Tableau etc.).
 * Experience with container management and deployment, e.g., Docker and Kubernetes","Entry level","https://www.linkedin.com/jobs/view/data-engineer-at-enterra-solutions-llc-3681509550?trk=public_jobs_topcard-title","Princeton, NJ","4 months ago","","","2023-07-20","","Technology, Information and Internet","Data Engineer","Information Technology"
"Over 200 applicants","80140883","Adame Services","https://www.linkedin.com/company/adameservices?trk=public_jobs_topcard-org-name","Full-time","Senior Data Engineer role




100% remote




Data Engineer Job Responsibilities




 * Develops and maintains scalable data pipelines and builds out new API integrations to support continuing increases in data volume and complexity.
 * Collaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.
 * Implements processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it.
 * Writes unit/integration tests, contributes to engineering wiki, and documents work.
 * Performs data analysis required to troubleshoot data related issues and assist in the resolution of data issues.
 * Works closely with a team of frontend and backend engineers, product managers, and analysts.
 * Defines company data assets (data models), spark, sparkSQL, and hiveSQL jobs to populate data models.
 * Designs data integrations and data quality framework.
 * Designs and evaluates open source and vendor tools for data lineage.
 * Works closely with all business units and engineering teams to develop strategy for long term data platform architecture.




Data Engineer Qualifications / Skills




 * Knowledge of best practices and IT operations in an always-up, always-available service
 * Experience with or knowledge of Agile Software Development methodologies
 * Excellent problem solving and troubleshooting skills
 * Process oriented with great documentation skills
 * Excellent oral and written communication skills with a keen sense of customer service




Education, Experience, And Licensing Requirements




 * BS or MS degree in Computer Science or a related technical field
 * 4+ years of Python or Java development experience
 * 4+ years of SQL experience (No-SQL experience is a plus)
 * 4+ years of experience with schema design and dimensional data modeling
 * Ability in managing and communicating data warehouse plans to internal clients
 * Experience designing, building, and maintaining data processing systems
 * Experience working with either a Map Reduce or an MPP system on any size/scale","Mid-Senior level","https://www.linkedin.com/jobs/view/senior-data-engineer-at-adame-services-3637898630?trk=public_jobs_topcard-title","United States","5 months ago","","","2023-06-18","","Staffing and Recruiting","Senior Data Engineer","Information Technology"
"Over 200 applicants","76522381","Rogo","https://www.linkedin.com/company/rogodata?trk=public_jobs_topcard-org-name","Full-time","Why Rogo?

Rogo will be the biggest Financial Services Artificial Intelligence company in the world. We're creating a category-defining AI company built on top of foundational AI models like GPT-4.


 * Exceptional early users: high-paying contracts with the world's largest investment banks, hedge funds, private equity firms, and consultants.
 * Massive demand: extensive waitlist of firms waiting for deployment.
 * World-class team: we take talent density very seriously. We like working with incredibly smart, driven people.
 * Cutting-edge technology: Work directly with the world's most advanced LLMs, AI, and RAG to build the future of generative AI and redefine finance.
 * Top-of-market cash and equity compensation.
   
   

Challenges:

We are building systems that can automate the most complex knowledge work in the world, e.g., financial analysis, research, due diligence, and more.


 * Creating financial research that's worth paying attention to: aggregating, analyzing, and producing insights from real-time information. Say goodbye to equity research.
 * Dealing with the most sensitive data in the world: client data from the largest financial services companies on earth.
 * Working past the edge of published AI research: tackling problems beyond the complexity of existing AI benchmarks.
 * Unsolved product, architectural, and business problems: natural language interfaces, prohibitively expensive evaluation of models, massive marginal costs, versioning/training/segregating models per task, client, and so on.
   
   

As a founding Data Engineer at Rogo, you will help build out our real-time data pipelines for millions of unstructured financial documents to feed our financial LLM. It’s cutting-edge data engineering at the AI frontier.

Hard Requirements:


 * 3+ years of industry experience as a data engineer
 * Highly proficient with Python and SQL, and an intuitive understanding of multi-threading, multi-processing, asyncio, and other concurrency primitives
 * Experience with at least one of: Postgres, Snowflake or Elasticsearch
 * Experience deploying and monitoring mission-critical ETL pipelines with large and heterogenous datasources
 * Experience working with Apache Airflow
 * Experience with AWS or other cloud environment
   
   

Bonus Requirements:


 * Experience at a hypergrowth startup
 * Financial Services work experience
 * Experience with Typescript
 * Experience with stream processing
 * Knowledge of Datadog and other Telemetry tooling
   
   

You'll fit in at Rogo if...


 * You have fun solving hard problems: we're tackling tech/product/business problems that are unsolved. It's super exciting.
 * You like to work hard: we feel lucky to work on these problems, and we enjoy pouring our all into solving them.
 * You care deeply about talent density: we care deeply about working with people who are super smart and motivated.
 * You have eclectic interests: whether you're a sci-fi aficionado, history buff, strategy game guru, policy wonk, or movie trivia expert, you'll find kindred spirits here.
   
   

Compensation Range: $120K - $160K

","Mid-Senior level","https://www.linkedin.com/jobs/view/founding-data-engineer-at-rogo-3749759981?trk=public_jobs_topcard-title","New York, NY","1 month ago","","","2023-10-26","","Software Development","Founding Data Engineer","Information Technology"
"194 applicants","81606529","Steneral Consulting","https://www.linkedin.com/company/steneral-consulting?trk=public_jobs_topcard-org-name","Full-time","Remote role, CST work hours

Need only 1 strong candidate on this

Asset Management domain exp needed

must have strong and extensive exp with DBT

Responsibilities


 * Play a key role in delivering data-driven interactive experiences to our clients
 * Work closely with our clients in understanding their needs and translating them to technology solutions
 * Provide expertise as a technical resource to solve complex business issues that translate into data integration and database systems designs
 * Problem solving to resolve issues and remove barriers throughout the lifecycle of client engagements
 * Ensuring all deliverables are high quality by setting development standards, adhering to the standards and participating in code reviews
 * Participate in integrated validation and analysis sessions of components and subsystems on production servers
 * Mentor, support and manage team members
   
   

Skills And Experience


 * Experience in the implementation, execution, and maintenance of Data Integration technology solutions
 * Experience advancing and supporting information management practices within business processes, applications and technology that underpin the data discipline (e.g. establishing data quality processes, performing data analysis, participating in technology implementation planning, implementing data integration processes, etc)
 * Expertise in Snowflake data modelling, ELT using snowpipe, implementing stored procedures and standard DWH and ETL concepts. Strong experience in DBT (data build tool), Snowflake and Python
 * Expertise in Snowflake concepts like setting up resource monitors, RBAC controls, virtual warehouse, query performance tuning, Zero copy clone, time travel and understand how to use these features
 * Experience in Data Migration from RDBMS to Snowflake cloud data warehouse
 * Experience with enterprise cloud economics. Understanding of enterprise data management concepts (Data Governance, Data Engineering, Data Science, Data Lake, Data Warehouse, Data Sharing, Data Applications)
 * Hands-on expertise with SQL and SQL analytics
 * Industry benchmarking experience in major industries such as: Financial Services and Retail
   
   

Good-to-Have Skills


 * Certifications for any of the cloud services like AWS, Snowflake, GCP or Azure
 * Experience working with code repositories and continuous integration
 * Understanding of development and project methodologies","Entry level","https://www.linkedin.com/jobs/view/data-engineer-with-dbt-snowflake-python-and-asset-management-at-steneral-consulting-3770675727?trk=public_jobs_topcard-title","United States","1 week ago","","","2023-11-20","","IT Services and IT Consulting","Data Engineer with DBT, Snowflake, Python and Asset Management","Information Technology"
"120 applicants","100144902","Team Remotely Inc","https://www.linkedin.com/company/team-remotely-inc?trk=public_jobs_topcard-org-name","Full-time","This is a remote position.

Junior Data Analyst (US/Canada Residents Only, 1 year experience, remote)

Team Remotely Inc. is a staffing and recruitment agency that offers a comprehensive solution for talent acquisition, including sourcing, vetting, pay rolling, and managing talent. Whether you need contract staffing, direct hire, direct sourcing, talent pools, or diversity initiatives, our model can support your hiring strategy.

Hiring Type: Full-Time

Base Salary: $64K-$76K Per Annum.

How to Apply: Please visit teamremotely.com to learn more & apply.

Role Responsibilities:


 * Work in close collaboration with the Business Intelligence Lead, Federal Data Lead, and other Program teams
 * Develop, maintain, and improve BI tools, build and enhance standard operating procedures (SOPs)
 * Manage various data sets and active Google workbooks with adjacent contract teams, monitor and analyze financial health information at the project and program levels
 * Communicate with client leadership to assess data needs and emerging requirements
 * Work with large data sets, workbooks, and spreadsheets to manipulate and manage program-level information using macros, queries, scripts, etc.
 * Gather requirements and lead the development of long-term data management tools, processes, and solutions based on organizational needs.
 * Be comfortable working with collaboration tools such as; Google Suite, Microsoft Office
 * Providing general support to the client including, but not limited to, analysis, data calls, financial management, risk management, audits, and project management-related tasks.
   
   

Qualifications:


 * Bachelor's Degree in business, business intelligence, data or information management, or similar.
 * Proficient in Google Scripts
 * Minimum 1 year of data or information management and/or data analysis experience.
 * Experience using Microsoft Excel and Google Sheets (macros, imports, query functions).
 * Experience with developing in Google App Script is a plus.
 * Experience using SQL Developer is a plus.
 * Excellent written and verbal communication skills.
   
   

Why work with Team Remotely?

Team Remotely Inc. is a staffing platform offering a seamless experience for employers and candidates. Employers can post job openings and specify their requirements, while candidates can create profiles and upload resumes.

The team of Team Remotely continuously learns and adapts based on previous successful placements, constantly improving its matching capabilities. This ensures that the recommendations provided by Team Remotely are tailored and accurate, increasing the likelihood of a successful match between employers and candidates. By providing intelligent and data-driven solutions, they strive to enhance the efficiency and effectiveness of the hiring process, ultimately helping companies find the best talent and individuals find their dream jobs.

","Entry level","https://www.linkedin.com/jobs/view/junior-data-analyst-at-team-remotely-inc-3778481378?trk=public_jobs_topcard-title","New York, NY","3 hours ago","","","","","Software Development","Junior Data Analyst","Information Technology"
"Over 200 applicants","17943955","Wiliot","https://il.linkedin.com/company/wiliot?trk=public_jobs_topcard-org-name","Full-time","Wiliot was founded by the team that invented one of the technologies at the heart of 5G. Their next vision was to develop an IoT sticker, a computing element that can power itself by harvesting radio frequency energy, bringing connectivity and intelligence to everyday products and packaging, things previously disconnected from the IoT. This revolutionary mixture of cloud and semiconductor technology is being used by some of the world’s largest consumer, retail, food, and pharmaceutical companies to change the way we make, distribute, sell, use, and recycle products. We’re driven by our passion for sustainability, ESG and waste reduction.

Our investors include Softbank, Amazon, Alibaba, Verizon, NTT DoCoMo, Qualcomm and PepsiCo.

We are growing fast and need people that want to be part of the journey, commercializing Sensing as a Service and enabling “Intelligence for Everyday Things”.

We are seeking an experienced data engineer to join our team and evolve our data and ML infrastructure. You will play a crucial role in bridging the gap between our clients, data science, data analytics, platform, and cloud teams. You will collaborate closely to understand their data requirements, design, and implement scalable data solutions, and provide ongoing support to ensure successful integration and utilization of our IoT products. You will be responsible for managing data pipelines, optimizing data workflows, and transforming raw data into structured data solutions.

Responsibilities:


 * Develop machine learning applications according to requirements
 * Partner with data scientists to co-deliver data products
 * Engage directly with clients to understand their data needs and provide data engineering expertise.
 * Collaborate with cross-functional teams to design and implement data pipelines that meet client requirements and align with Wiliot's infrastructure.
 * Develop efficient data workflows to enable seamless data ingestion, transformation, storage, and retrieval.
 * Implement data quality rules/checks, data validation, and monitoring processes to ensure data integrity and accuracy.
 * Troubleshoot data-related issues and provide timely resolution to ensure client satisfaction.
 * Provide training and documentation to clients on data integration best practices and usage of Wiliot's data products.
   
   

Requirements:


 * Bachelor's or Master's degree in Computer Science, Data Engineering, or a related field.
 * Proven experience as a Data Engineer, preferably in a client-facing role.
 * Strong programming skills in languages such as Python, Java, or Scala.
 * Proficiency in designing and implementing data pipelines using technologies like Apache Kafka, Apache Spark, or similar frameworks.
 * Experience in Databricks preferred
 * Understanding of batch vs streaming based data pipelines and ML models
 * Familiarity with cloud-based data storage and processing platforms such as AWS, GCP, or Azure.
 * Experience with SQL and NoSQL databases, data modeling, and database optimization techniques.
 * Strong communication and interpersonal skills to effectively interact with clients and internal teams.
 * Ability to manage multiple projects and prioritize tasks in a fast-paced environment.","Mid-Senior level","https://www.linkedin.com/jobs/view/data-engineer-at-wiliot-3714374758?trk=public_jobs_topcard-title","Dallas, TX","2 months ago","","","2023-09-15","","Technology, Information and Internet","Data Engineer","Information Technology"
"Be among the first 25 applicants","18955016","Gauntlet","https://www.linkedin.com/company/gauntlet-xyz?trk=public_jobs_topcard-org-name","Full-time","Gauntlet is DeFi’s risk manager. We drive capital efficiency while maintaining economic safety for some of the largest crypto protocols with our simulations. Gauntlet manages risk and incentives for over $9 Billion in assets.

Gauntlet continuously publishes cutting-edge research, making us the most cited peer-reviewed articles in the DeFi industry. We’re a Series B company with :60 employees operating remote-first with a home base in New York City.

Gauntlet’s mission is to help make blockchain protocols and smart contracts safer and more trustworthy for users. Building decentralized systems creates new challenges for protocol developers, smart contract developers, and asset holders that are not seen in traditional development and investing.

Gauntlet is building a blockchain simulation and testing platform that leverages battle-tested techniques from other industries to emulate interactions in crypto networks. Simulation provides transparency and greatly reduces the cost of experimentation so that teams can rapidly design, launch, and scale new decentralized systems.

Responsibilities


 * Design, build, and maintain robust scalable ETL pipelines that ingest data from DeFi protocols, blockchain networks, and various data sources
 * Contribute to making our data platform world-class
 * Collaborate with stakeholders to understand the data requirements and provide necessary support
 * Implement quality control measures to ensure data integrity and quality
 * Actively participate in code and design reviews, providing constructive feedback to peers and maintaining high coding standards
 * Keep up-to-date with the latest industry trends, technologies, and best practices to ensure the continuous improvement of data platform
 * Contribute to the creation of technical documentation and user guides
 * Troubleshoot, diagnose, and resolve software defects, ensuring optimal system performance and reliability
   
   
   

Qualifications


 * 5+ years of professional engineering experience
 * Proficient in writing code in Python and SQL
 * Hands-on experience with OLAP database such as BigQuery
 * Experience with modern data transformation tools such as DBT
 * Experience with workflow orchestration tools such as Dagster and Airflow
 * Experience with distributed data processing frameworks
 * Strong communication skills and the ability to work collaboratively in a distributed team environment
   
   
   

Bonus Points


 * Experience working in the crypto industry is a plus but not required
 * Enthusiasm for the space, especially DeFi, is very much desired
 * Smart contract development experience (e.g. Solidity)
 * Experience with building machine learning models at scale
 * Published or presented research in the space
   
   
   

Benefits and Perks


 * Remote first - work from anywhere in the US & CAN!
 * Regular in-person company retreats and cross-country ""office visit"" perk
 * 100% paid medical, dental and vision premiums for employees
 * Laptop, monitor, keyboard and mouse setup provided
 * $1,000 WFH stipend
 * Monthly reimbursement for home internet, phone, and cellular data
 * Unlimited vacation
 * 100% paid parental leave of 12 weeks
 * Fertility benefits
 * Opportunity for incentive compensation
   
   
   

Please note at this time our hiring is reserved for potential employees who are able to work within the contiguous United States and Canada. Should you need alternative accommodations, please note that in your application.

The national pay range for this role is $150,000 - $180,000 base plus additional On Target Earnings potential by level and equity in the company. Our salary ranges are based on paying competitively for a company of our size and industry, and are one part of many compensation, benefits and other reward opportunities we provide. Individual pay rate decisions are based on a number of factors, including qualifications for the role, experience level, skill set, and balancing internal equity relative to peers at the company.

","Not Applicable","https://www.linkedin.com/jobs/view/data-engineer-at-gauntlet-3765636920?trk=public_jobs_topcard-title","Los Angeles Metropolitan Area","1 month ago","","","2023-10-30","","Transportation, Logistics, Supply Chain and Storage","Data Engineer","Information Technology"
"Over 200 applicants","71303412","Edge & Node","https://www.linkedin.com/company/edgeandnode?trk=public_jobs_topcard-org-name","Full-time","Edge & Node stands as the revolutionary vanguard of web3, a vision of a world powered by individual autonomy, shared self-sovereignty and limitless collaboration. Established by trailblazers behind The Graph, we’re on a mission to make The Graph the internet’s unbreakable foundation of open data. Edge & Node invented and standardized subgraphs across the industry, solidifying The Graph as the definitive way to organize and access blockchain data. Utilizing a deep expertise in developing open-source software, tooling, and protocols, we empower builders and entrepreneurs to bring unstoppable applications to life with revolutionary digital infrastructure.




Edge & Node acts on a set of unwavering principles that guide our journey in shaping the future. We champion a decentralized internet—free from concentrated power—where collective consensus aligns what is accepted as truth, rather than authoritative dictation. Our commitment to censorship resistance reinforces our vision of an unyielding information age free from the grasp of a single entity. By building for open-source, we challenge the stagnant landscape of web2, recognizing that true innovation thrives in transparency and collaboration. We imagine a permissionless future where the shackles imposed by central gatekeepers are not only removed, but relegated to the dustbin of a bygone era. And at the foundation of it all, our trust shifts from malevolent middlemen to trustless systems, leveraging smart contracts to eliminate the age-old vulnerabilities of misplaced trust.




The Data Science team works closely with teams across Edge & Node to deliver high quality data for product research & development and go to market, as well as business analytics. We work across the data lifecycle from infrastructure to data analytics.




We are looking for an early-career Data Engineer to be focused on developing and maintaining data science pipelines. Ideally, the team would like to bring on someone who has experience with the current tools being used by the team which include, but are not limited to, Redpanda, Materialize, and GCP. In this role, you will monitor and maintain reliability of the Redpanda cluster, streaming database, DBT jobs, QoS oracle, and other data engineering systems. You’ll be expected to learn Materialize and help migrate BigQuery models to reduce costs. In addition, you will help establish and maintain good standards around documentation and internal educational tools and respond to data engineering/devops requests in our incident management process.




What You’ll Be Doing




 * Learning our infrastructure and data engineering toolset
 * Partnering closely with our Data Science and SRE teams to perform various data warehouse jobs and periodic RedPanda/streaming database devops tasks
 * Manage historical data models in BigQuery/DBT
 * Develop pipelines to support dashboards and perform devops tasks to support dashboards




What We Expect




 * Experience with one or more of the following: BigQuery, ETL automation/workflow tools (DBT), BI/dashboarding tools (Apache Superset/Metabase), streaming data platforms (Apache Kafka, Redpanda, or Confluent), or other data engineering and data warehouse toolsets/environments
 * Some experience or knowledge of container orchestration tools such as Kubernetes and Kustomize preferred
 * Some experience or knowledge of monitoring and alerting (Grafana dashboards) preferred
 * Some experience or knowledge of SQL–able to create and manage tables within a SQL database
 * Proficiency in one or more programming languages, such as Python, R, or Rust
 * Must be able to to serve on-call shifts and support devops needs
 * Ability to create documentation and communicate with a a variety of audiences
 * Clear communication skills (written and verbal) to document processes and architectures
 * Ability to work well within a multinational team environment
 * Preference to be physically located in The Americas, however the team is open to candidates in European time zones or other locations




About the Graph




The Graph is the indexing and query layer of web3. The Graph Network’s self service experience for developers launched in July 2021. Developers build and publish open APIs, called subgraphs, that applications can query using GraphQL. The Graph supports indexing data from multiple different networks including Ethereum, NEAR, Arbitrum, Optimism, Polygon, Avalanche, Celo, Fantom, Moonbeam, IPFS, and PoA with more networks coming soon. To date, tens-of-thousands of subgraphs have been deployed on the hosted service, and now subgraphs can be deployed directly on the network. Over 28,000 developers have built subgraphs for applications such as Uniswap, Synthetix, KnownOrigin, Art Blocks, Balancer, Livepeer, DAOstack, Audius, Decentraland, and many others.




If you are a developer building an application or web3 application, you can use subgraphs for indexing and querying data from blockchains. The Graph allows applications to efficiently and performantly present data in a UI and allows other developers to use your subgraph too! You can deploy a subgraph to the network using the newly launched Subgraph Studio or query existing subgraphs that are in the Graph Explorer. The Graph would love to welcome you to be Indexers, Curators and/or Delegators on The Graph’s mainnet. Join The Graph community by introducing yourself in The Graph Discord for technical discussions, join The Graph’s Telegram chat, and follow The Graph on Twitter, LinkedIn, Instagram, Facebook, Reddit, and Medium! The Graph’s developers and members of the community are always eager to chat with you, and The Graph ecosystem has a growing community of developers who support each other.




The Graph Foundation oversees The Graph Network. The Graph Foundation is overseen by the Technical Council. Edge & Node, StreamingFast, Messari, Semiotic and The Guild are five of the many organizations within The Graph ecosystem.","Mid-Senior level","https://www.linkedin.com/jobs/view/data-engineer-at-edge-node-3768704344?trk=public_jobs_topcard-title","United States","5 days ago","","","2023-11-28","","Software Development","Data Engineer","Engineering, Information Technology, and Other"
"125 applicants","17914363","Smile ID (formerly Smile Identity)","https://uk.linkedin.com/company/usesmileid?trk=public_jobs_topcard-org-name","Full-time","Smile Identity builds trust.

Smile Identity is Africa's leading identity verification, and digital Know Your Customer (KYC) provider. We help companies scale rapidly across Africa by confirming the true identity of their users in real time, using any smartphone or computer. Our technology is powered by proprietary Machine Learning algorithms designed specifically for African faces, devices and network connections.

Our team is a diverse group of hardworking, truth-seeking, and fun-loving Smilers spanning 5 offices, 10 countries and 8 time zones. Our products are already making waves across many industries, from Banking to Fintech and Telecoms. We recently announced a $20M Series B raise and are backed by leading global investors, including Norsken22, Costanoa, CRE, Future Africa, Susa Ventures, Commerce Ventures, Courtside Ventures, Two Culture Capital, Latitude, Valuestream Ventures, Intercept Ventures and Vinod Khosla who are supporting us every step of the way.

Do you like working alongside a team of intelligent individuals? Do you want to have fun while making a real difference? Here at Smile Identity, you'll get the freedom and autonomy you need to do your best work, the flexibility to be creative, and the opportunity to grow and put your unique stamp on our mission.

What are you waiting for? Come with us on this amazing journey!

The Role

We are looking for a data engineer who loves bringing order to chaos. The individual in this role will maintain our data warehouse and associated data infrastructure and provide the entire organization with the data they need to be successful. This role is open to candidates across the globe. You will be working with colleagues ranging from the US West Coast to Eastern Africa, with that in mind candidates working in timezones between US Eastern and GMT are preferred.

What You Will Do


 * Work with our entire organization based in the US, London, Berlin, Lagos, Nairobi, and Cape Town to centralize our data and maintain our data warehouses/lakes.
 * You will select the right tools and services to bring our data together and provide a solid foundation for all our product and business analytics. Your north star? Empowering the entire organization with data to make the best possible decisions.
 * Design, build and launch extremely efficient and reliable data pipelines to move data.
 * Architect, build and launch new data models that provide intuitive analytics.
 * Manage the delivery of high impact dashboards, tools and data visualizations
 * Build data expertise and own data quality, including defining and managing SLAs for data sets.
 * Partner with leadership, engineers, commercial, and data scientists to understand data needs.
 * Influence short- and long-term strategy with cross-functional teams to drive impact.
 * Educate your partners: Use your data and analytics experience to discover opportunities, identifying and addressing gaps in existing logging and processes.
   
   

Requirements


 * 4+ years experience with data infrastructure, ETL design, data warehousing, schema design and dimensional data modeling
 * 2+ years of experience in SQL, Python, or similar languages
 * Experience with designing and implementing real-time pipelines
 * Experience with code management tooling such as Git, Github
 * Experience with data migrations in production settings
 * You have a deep understanding of modern data tooling and infrastructure
 * You are comfortable working independently with periodic guidance from engineering & business teams
 * You are a strong believer in scale and automation
 * You are entrepreneurial — you take initiative, solve problems and love to troubleshoot.
 * You are a great collaborator and can communicate effectively. You enjoy teaching and learning from your colleagues
 * You are not ideological about programming languages or tools. You have opinions but are open to discussion and tradeoffs
 * You are a pragmatist
 * You are a seeker of truth
   
   

Preferred Qualifications


 * Experience querying big data using Spark, Presto, Hive, Impala, etc.
 * Experience with data quality and validation
 * Experience with SQL performance tuning and E2E process optimization
 * Experience creating reports and dashboards with modern business intelligence tools (Tableau, Metabase preferred)
 * Experience working with Postgres, Hevo, and cloud or on-prem Big Data/MPP analytics platform (i.e. Snowflake, AWS Redshift, Google BigQuery, Azure Data Warehouse, Netezza, Teradata, or similar).
 * Interest or experience in working in the African Fintech Ecosystem
 * Experience in a high-growth team and/or startup experience
 * Ability to communicate and prioritise effectively with a distributed team around the world
   
   

Compensation


 * Salary commensurate with experience
 * Stock options
 * Healthcare
 * Opportunities for travel (Post-Covid19)
   
   

Autonomy and a chance to work at a mission-driven company with purpose

What Success Looks Like

Successes in your first 3 months include


 * Take the time and learn the ins and outs of our data warehouse and dashboards. Investigate how data is being logged in our systems and what existing data pipelines exist across our two data warehouses (Postgres and Redshift).
 * Evaluate existing dashboards, data quality, and pipelines and identify gaps.
 * Get introduced to the Product and Engineering team and their bi-weekly sprint processes. At this point you are mostly observing the dynamics and taking on tasks by the team, while building a partnership and exploring support opportunities.
   
   

In your first 6 months


 * Based on your initial exposure to our data stack, you have already built several improvements based on the gaps you've identified. You are able to manage the flow of data across the stack. You have extended our system capabilities as needed and have improved efficiency and simplicity of shared tools and libraries.
 * You are deeply embedded in how we set up data logging and are able to manage and successfully execute on data requirements from teams across the company (e.g. Engineering, Product, CVML, Data Science, Marketing, Commercial).
 * You proactively develop technical methodologies or tools which can solve important classes of problems. You can evangelize these methodologies and tools to other data scientists and engineers to scale multiple people.
   
   

In your first 12 months


 * You are the company expert in our data, infrastructure, and technical architecture, and are actively involved in product and business operations to either improve existing data tools or suggest new methodologies to accelerate team execution, including influencing data best practices.
 * You drive scalable solutions across teams.
 * You are able to solve challenging technical problems faced by multiple teams and provide significant technical advice to newer or less-technical analysts.","Entry level","https://www.linkedin.com/jobs/view/data-engineer-at-smile-id-formerly-smile-identity-3641780519?trk=public_jobs_topcard-title","San Francisco, CA","6 months ago","","","2023-05-28","","Technology, Information and Internet","Data Engineer","Information Technology"
"Over 200 applicants","164748","LivePerson","https://www.linkedin.com/company/liveperson?trk=public_jobs_topcard-org-name","Full-time","LivePerson (NASDAQ: LPSN) is a global leader in trustworthy and equal AI for business. Hundreds of the world’s leading brands — including HSBC, Chipotle, and Virgin Media — use our Conversational Cloud platform to engage with millions of consumers safely and responsibly. We power a billion conversational interactions every month, providing a uniquely rich data set and safety tools to unlock the power of Generative AI and Large Language Models for better business outcomes.

At LivePerson, we foster an inclusive workplace culture that encourages meaningful connection, collaboration, and innovation. Every mind is invited to ask questions and actively seek new ways to achieve success and reach their full potential. We operate as one with a growth mindset. This means spotting opportunities, solving ambiguities and seeking effective solutions to challenges that make things better.

Overview

Welcome to the Web Experiences team at LivePerson!We are looking for a software engineer II to join our team. You will work in a fast-paced environment on multiple projects spanning different company areas, aiming to enhance and simplify how brands engage with their customers through messaging. In this role, collaboration is key as you work across various internal teams, external partners, and large customers to deliver top-notch products for our brands and their customers. This team oversees, among other products, the messaging window, which serves as the face of LivePerson and represents the largest communication channel across the globe.

You Will


 * Work collaboratively in a cross-functional team (PM, UX, engineering, science) team to transform user stories into prototypes & production code
 * Thoughtfully apply UI design principles & best practices for RWD
 * Building & deploying user-facing features for multiple browsers
 * Identifying & eliminating perf. & scale issues that ripple from front-end to back-end
 * Required to participate in on-call rotation
 * Be globally minded & inclusive - we are a global team and value the diverse contributions of others.
   
   

You Have


 * 5+ years of software engineering experience in creating Web Intensive applications
 * Deep knowledge and understanding of client-side architecture and experience in building large scale and high performance web applications
 * Proficient in understanding of web markup, including HTML5, CSS3
 * Experience using CSS preprocessors such as SASS or LESS
 * Experience using build tools such as Webpack, Gulp or Grunt.
 * Good understanding of asynchronous request handling, partial page updates, and AJAX
 * Proficient understanding of cross-browser compatibility issues and ways to work around them
 * Proficient understanding of code versioning tools, such as Git
 * Good understanding of scalability design principles
 * Understanding principles of design and knowledge of user experience and accessibility
 * Self-starter with strong motivation and execution capabilities
 * Knowledge of frontend testing frameworks such as Jasmine and Karma is a plus
 * Proficiency with JavaScript ES6 and Typescript is a plus
   
   

Benefits

The salary range for this role will be between $130,000 to $150,000. Final compensation will be determined by a variety of factors, including, but not limited to, your location, skills, experience, education, and/or certifications. During the phone screening, the recruiter will provide the location-specific salary range for this role. Regardless of your personal situation or where you are in the world, LivePerson offers comprehensive and great benefits programs to meet your needs:


 * Health: medical, dental, vision and wellbeing.
 * Time away: Public holidays and discretionary PTO package for flexible days off with manager approval.
 * Financial: 401K, ESPP, Basic life and AD&D insurance, long-term and short-term disability
 * Family: parental leave, maternity support, fertility services.
 * Development: tuition reimbursement, native AI learning.
 * Additional: 24/7 access to professional counselors, voluntary insurance coverage, exclusive perks and discounts
   
   

Why You’ll Love Working Here

LivePerson is a hub for the ever-curious and proactive, offering a flexible work-life balance tailored to individual needs. With offices and WeWork locations worldwide, our flexible work policy provides our teams the freedom to work from their preferred environment. We're very proud to have earned recognition from Fast Company, Newsweek, and BuiltIn for being a top innovative, beloved, and remote-friendly workplace.

Belonging at LivePerson

We are proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances. We also consider qualified applicants with criminal histories, consistent with applicable federal, state, and local law.

We are committed to the accessibility needs of applicants and employees. We provide reasonable accommodations to job applicants with physical or mental disabilities. Applicants with a disability who require reasonable accommodation for any part of the application or hiring process should inform their recruiting contact upon initial connection.

","Entry level","https://www.linkedin.com/jobs/view/software-engineer-ii-at-liveperson-3776482377?trk=public_jobs_topcard-title","Atlanta Metropolitan Area","3 days ago","","","2023-12-01","","Software Development","Software Engineer II","Engineering and Information Technology"
"185 applicants","81606529","Steneral Consulting","https://www.linkedin.com/company/steneral-consulting?trk=public_jobs_topcard-org-name","Full-time","Hi,




Please find attached Job Description. If you are interested please do share with me your updated resume or call me on ""+1 3026017375"".




Job Title:- Data Engineer




Work Location:- remote(PST/MST)




Duration: 3-6 month




Work Authorization:- Citizen




Interview : video/skype




Data Engineer




Remote




but mist reside in MST or PST time zones




3-6 months




Data Engineer




End Client: Kaiser Permante




Duration: 3-6 months




Work schedule: 40 hrs/week




Estimated/Targeted Start: ASAP




On-site/Remote: Remote




Potential for client hire: No




Additional requirements: US Citizens only,




Candidates must be PST or MST




Interview Process: 2-3 video interviews




Skills




Minimum of 5-10 years of IT/IS experience




Need To Have Excellent Communication Skills




5+ years working with Advanced Structured Query Language (SQL) & PL/SQL




5+ years experience with at least Oracle relational database




5+ years experience in software development lifecycle activities




5+ years experience with data loading (ETL, ELT)




5+ years working with data at scale 50+ TB




Experience With Data Warehouses, Operational Data Stores, Data Hubs




Experience in end-to-end design of near-real-time and batch data pipelines




Experience Working In An Agile Environment




Experience developing detailed systems design and written test plans




Experience Preparing Installation Instructions And Coordinating Installation Procedures




Experience documenting data audits, archiving, and restoration processes




Experience With Version Control Systems Git




Experience with tracking and ticket software (Jira, Confluence, etc.)




Familiarity with data architecture, data integration, data governance, and data lineage concepts




EDUCATION




Bachelor's Degree in computer science or a related discipline




Experience




Minimum of 5-10 years of IT/IS experience




Need to have excellent communication skills




5+ years working with Advanced Structured Query Language (SQL) & PL/SQL




5+ years experience with at least Oracle relational database




5+ years experience in software development lifecycle activities




5+ years experience with data loading (ETL, ELT)




5+ years working with data at scale 50+ TB




Experience with data warehouses, operational data stores, data hubs




Experience in end-to-end design of near-real-time and batch data pipelines




Experience working in an Agile environment




Experience developing detailed systems design and written test plans




Experience preparing installation instructions and coordinating installation procedures




Experience documenting data audits, archiving, and restoration processes




Experience with version control systems Git




Experience with tracking and ticket software (Jira, Confluence, etc.)




Familiarity with data architecture, data integration, data governance, and data lineage concepts




NICE TO HAVE




Leading a team




Master's Degree




Health care experience




Professional certifications




Other Programming Language Experience




Experience in Machine learning or Artificial Intelligence




Familiarity with microservices




Familiarity with DevSecOps




2+ years working with one NoSQL database (Hadoop)




Experience with MongoDB, Redshift, Synapse, or others is a plus.




Experience working in one public cloud environment (Azure, AWS, etc.)




Snowflake familiarity




Experience with containerization (Docker, Kubernetes, etc.)




Familiarity with agile and lean methodologies




Familiarity with JSON, XML




PERSONAL




Welcomes new approaches and innovative thinking




Self-organized And Responsible With Experience In a Distributed Team




Able to multitask and be responsive/flexible to support customers




Ability to work with others from diverse skill sets and backgrounds




Takes ownership of a situation and sees it through the completion




Able to switch context and complete work processes




Kirti Rani




Associate Talent Acquisition -North America




Desk: +1 3026017375




kirti@steneral.com




In my absence please reach out to Mr. Harish Sharma at harish@steneral.com & 3027216151","Entry level","https://www.linkedin.com/jobs/view/data-engineer-remote-pst-or-mst-at-steneral-consulting-3713013303?trk=public_jobs_topcard-title","United States","2 months ago","","","2023-09-08","","IT Services and IT Consulting","Data Engineer______________remote(PST or MST)","Information Technology"
"122 applicants","81606529","Steneral Consulting","https://www.linkedin.com/company/steneral-consulting?trk=public_jobs_topcard-org-name","Full-time","Job Description
 * 10+ years of overall experience
 * 5+ years of experience working on any cloud platform
 * Experience enabling new tools in cloud platforms and making it enterprise ready
 * Worked with large multi tenancy environment
 * Hands on experience with the following
    * IAM Provisioning
    * VPC Networks and VPC Subnets.
    * Cloud compute(cloud function, Kubernetes and VMs)","Entry level","https://www.linkedin.com/jobs/view/data-engineer-10%2B-years-at-steneral-consulting-3641969074?trk=public_jobs_topcard-title","United States","5 months ago","","","2023-06-22","","IT Services and IT Consulting","Data Engineer - 10+ Years","Information Technology"
"155 applicants","79809662","NBC Sports Next","https://www.linkedin.com/company/nbc-sports-next?trk=public_jobs_topcard-org-name","Full-time","Company Description

NBC Sports Next is where sports and technology intersect. We’re a subdivision of NBC Sports and home to all NBCUniversal digital applications in sports and technology within our three groups: Youth & Recreational Sports; Golf; and Betting, Gaming & Emerging Media.

At NBC Sports Next, we make playing sports better through innovative technology and immersive experiences for athletes, coaches, players and fans. We equip more than 30MM players, coaches, athletes, sports administrators and fans in 40 countries with more than 25 sports solution products, including SportsEngine, the largest youth sports club, league and team management platform; GolfNow, the leading online tee time marketplace and provider of golf course operations technology; GolfPass the ultimate golf membership that connects golfers to exclusive content, tee time credits, and coaching, tips; TeamUnify, swim team management services; GoMotion, sports and fitness business software solutions; and NBC Sports Edge, a leading platform for fantasy sports information and betting-focused tools.

At NBC Sports Next we’re fueled by our mission to innovate, create larger-than-life events and connect with sports fans through technology that provides the ultimate in immersive experiences.

Golf fuses the team behind products and services like GolfNow, TeeOff and GolfPass, which better connects golfers and golf facilities around the world through innovative solutions like cloud-based golf course management and SmartPlay contactless technology and services that create optimum golfing experiences.

Come join us as we work together as one team to innovate and deliver what’s Next.

Job Description

GolfNow has an exciting opportunity for an experienced Data Engineer III. In this role as part of the Data Engineering Team, you work to manage the full lifecycle of our data warehousing needs. You will read and write complex queries, demonstrate the ability to create database objects (tables, views, stored procedures, user-defined functions) and create and maintain ETL pipelines. Our data warehouse and data operations are built on top of Microsoft and AWS technologies including MS SQL Server, SSIS, PowerShell, and other AWS cloud technologies. To perform this job successfully, an individual would need to be able to understand complex business processes, gather requirements, work efficiently, and verify their results.

Responsibilities Include But Are Not Limited To


 * Work within a small team of passionate data engineers and data scientists.
 * Compile user requirements and specifications for reports.
 * Contribute to the management of the day-to-day operations of running our Data Warehouse.
 * Build, analyze and manage reports and dashboards for business stakeholders.
 * Respond to users to troubleshoot and/or improve existing reports.
 * Collaborate with internal QA on customer acceptance testing.
 * Develop SQL scripts and objects to support reporting functionality and performance.
 * Build data pipelines and ETLs for loading source system data into the data warehouse for further reporting and analysis.
 * Assist in building scalable data models to support reporting and tracking of key business and product metrics.
 * Help identify better practices, tools, and relevant trends that can positively influence the data operations across the business.
 * Other duties may be assigned as needed by management.
   
   

Qualifications

All candidates must meet the qualifications below:


 * A minimum of 5 years of data engineering experience is required.
 * Bachelor’s Degree in Computer Science or related field/relevant industry experience in data engineering.
 * Strong understanding of AWS architecture and ETL processing
 * Advanced knowledge of TSQL tuning
 * 5+ years of experience working with Python
 * Advanced experience and knowledge of T-SQL Microsoft SQL Server Database Platforms.
 * Experience working with, assessing and improving cloud frameworks
 * Experience understanding and enhancing cloud data engineering and analytics data strategies
 * Experience with AWS cloud environment.
 * Experience with AWS ETL tools including Glue, Lambda and step functions
 * Experience with Apache Airflow
 * Experience working with Terraform
 * Working experience developing and refactoring SQL Stored Procedures.
 * Experience using source control with Git or Team Foundation Server.
 * Experience with modeling data structures in both transactional and analytical platforms.
 * Experience with one of the following BI Tools: Tableau, Power BI
   
   

Desired Qualifications Are As Follows


 * Experience with SSIS is a plus
 * Experience with PowerShell scripting is a plus
 * Strong experience mentoring junior data engineers
 * Experience working in Agile environment
 * Experience working with SSRS reports
 * Experience managing SDLC process with Atlassian tools. (Jira, Confluence)
 * Able and eager to learn new technologies.
 * Able to easily transition between high-level strategy and day-to-day implementation.
 * Excellent teamwork and collaboration skills.
 * Results-oriented and self-motivated.
   
   

Fully Remote: This position has been designated as fully remote, meaning that the position is expected to contribute from a non-NBCUniversal worksite, most commonly an employee’s residence.

Additional Information

NBCUniversal's policy is to provide equal employment opportunities to all applicants and employees without regard to race, color, religion, creed, gender, gender identity or expression, age, national origin or ancestry, citizenship, disability, sexual orientation, marital status, pregnancy, veteran status, membership in the uniformed services, genetic information, or any other basis protected by applicable law. NBCUniversal will consider for employment qualified applicants with criminal histories in a manner consistent with relevant legal requirements, including the City of Los Angeles Fair Chance Initiative For Hiring Ordinance, where applicable.

If you are a qualified individual with a disability or a disabled veteran, you have the right to request a reasonable accommodation if you are unable or limited in your ability to use or access nbcunicareers.com as a result of your disability. You can request reasonable accommodations in the US by calling 1-818-777-4107 and in the UK by calling +44 2036185726.","Associate","https://www.linkedin.com/jobs/view/data-engineer-iii-nbc-sports-next-at-nbc-sports-next-3770036816?trk=public_jobs_topcard-title","Orlando, FL","2 weeks ago","","","2023-11-19","","Broadcast Media Production and Distribution, Entertainment Providers, and Media Production","Data Engineer III - NBC Sports Next","Engineering"
"Over 200 applicants","1313464","GSquared Group","https://www.linkedin.com/company/gsquared-solutions?trk=public_jobs_topcard-org-name","Full-time","Senior Data Engineer

Atlanta, Georgia

Contract Opportunity ( Must already live in the Atlanta area, onsite occasionally for fun activities but mostly remote with occasional onsite).







We have a great opportunity for a Data Engineer with experience using AWA Cloud technologies. This is a long-term contract opportunity in the Atlanta area!




As the Data Engineer, you will build and manage a data pipeline for an AWS cloud-based data platform. If you have strong technical, analytical, programming and critical thinking skills as well as data transformation and data modeling experience – we want to talk to you!




How will you make an impact?




 * Build and manage the data pipeline (ingest, transformation, distribution, quality rules, data storage) for a cloud-based data platform
 * Design, develop and maintain automated data solutions
 * Develop new and existing data processing
 * Support and troubleshoot the data environment
 * Document technical artifacts for developed solutions




Qualifications & Experience

 * 4+ years of data engineering experience
 * Spark and PySpark required
 * Databricks expeirence required
 * Python or Java experience preferred","Mid-Senior level","https://www.linkedin.com/jobs/view/senior-data-engineer-at-gsquared-group-3756011059?trk=public_jobs_topcard-title","Atlanta Metropolitan Area","3 weeks ago","Shannon Coleman LaPres","https://www.linkedin.com/in/shannoncolemanlapres","2023-11-09","","IT Services and IT Consulting","Senior Data Engineer","Information Technology"
"Over 200 applicants","29308","StevenDouglas","https://www.linkedin.com/company/stevendouglashq?trk=public_jobs_topcard-org-name","Full-time","Title: Sr. Data Engineer

Location: Remote

Work Requirements: Those authorized to work in the United States without sponsorship are encouraged to apply: Citizen, GC Holders or GC-EAD

This role is not open to third party vendors.




Summary:

As a Senior Data Engineer, you'll design and implement cutting-edge Big Data and Business Intelligence solutions on Microsoft Azure. You'll create data warehouses, pipelines, and ML model execution pipelines, champion engineering design standards, collaborate with a talented team, contribute to Agile product development, and mentor junior engineers. If you're an experienced professional seeking a dynamic environment to showcase your expertise, apply today!




Responsibilities:




 * Lead 80% of your time on Data Engineering and 20% on Architecture.
 * Design and implement innovative Big Data and BI solutions on Microsoft Azure.
 * Develop robust data warehouses, pipelines, and Azure ML model execution pipelines.
 * Promote engineering design and development standards.
 * Collaborate closely with the team, technical leads, and stakeholders.
 * Actively participate in Agile ceremonies and contribute to product development.
 * Write efficient code and create comprehensive unit tests.
 * Proactively communicate progress, address issues, and manage risks.
 * Mentor junior engineers and foster their professional growth.




Requirements:




 * 10 years of hands-on experience in large-scale distributed data architecture.
 * Extensive experience designing and implementing data pipelines (10 years).
 * Proficiency in Azure data services and ETL/ELT tools (5 years).
 * Strong hands-on experience with Python, including object-oriented programming and unit testing (5 years).
 * Advanced knowledge of Python parallel processing and data analysis libraries.
 * Expertise in data modeling and familiarity with Microsoft SQL technologies.
 * Broad experience in multi-tenant data architecture with diverse data stores and processing engines.
 * Skills in data integration through APIs and web services.
 * Familiarity with Azure DevOps, CI/CD, Git, Jenkins, Jira, and Confluence.
 * 5+ years of experience with Azure Data Lake and Azure DataBricks.
 * 10+ years of experience in designing and developing complex data architecture solutions.
 * 5 years of hands-on experience with Python.","Mid-Senior level","https://www.linkedin.com/jobs/view/senior-data-engineer-at-stevendouglas-3656156588?trk=public_jobs_topcard-title","United States","1 month ago","","","2023-10-31","","IT Services and IT Consulting, Information Services, and Software Development","Senior Data Engineer","Engineering and Information Technology"
"Over 200 applicants","29121514","RightPro Staffing","https://www.linkedin.com/company/rightpro-staffing?trk=public_jobs_topcard-org-name","Full-time","Data Engineer




Summary:

Responsible for the technical implementation of reporting and analytic tools across the network. Transform data to provide timely, meaningful, and actionable insight to clinical teams and leaders to drive organizational improvement. Collaboration with end-users throughout the organization to determine information and data needs and conceptualize, design, and develop data visualization solutions into clear communications for key stakeholders and decision-makers.




Responsibilities:

 * Discuss and gather data requirements from multiple categories of stakeholders and translate them into concise actionable tasks.
 * Experience operationalizing advanced analytical and data science models to deliver impactful results to the right audience at the right time.
 * Extract, query, transform, visualize, and interpret data on multiple platforms.
 * Collaborate with internal and external teams to understand business needs/issues, troubleshoot problems, conduct root cause analysis, and develop cost-effective resolutions for data anomalies.
 * Team leader within a group or on cross-functional teams; accept team leads stretch assignments and resources for colleagues with less experience. Present to the broader senior leadership team. Understand other business units and how they affect the team and work.
 * Identify, design, and implement internal process improvements optimizing data delivery and automating manual processes.
 * Apply quantitative analysis and data visualization to tell the story behind the numbers while supporting data-driven decision-making. Use technical skills, problem-solving, and business knowledge to deliver custom datasets to clients that meet or exceed expectations. Implement proactive improvements to processes and methods for gathering and aggregating data. Find creative solutions to problems with limited information.
 * Participate in continually improving processes and procedures for enhancing the efficiency and effectiveness of Data Engineering services to analytic users.
 * Experience with data architecture
 * Collaborate with technology and platform management partners to optimize data sourcing and processing rules to ensure appropriate data quality.




Requirements:

 * MUST HAVE 10+ total data engineering experience including premise to cloud.
 * MUST HAVE 10+ years of SQL experience.
 * MUST HAVE 3-5+ years of current or recent Healthcare industry experience (within the last 2 years)
 * MUST HAVE 3+ years of EMR experience in Healthcare, Cerner strongly preferred.
 * MUST HAVE 2+ years of programming with Python.
 * MUST HAVE 2+ years of experience with AWS Cloud and Redshift.
 * Strong experience with ETL and Data Migration is A MUST
 * Ability to partner with executive stakeholders and technical staff.","Mid-Senior level","https://www.linkedin.com/jobs/view/data-engineer-at-rightpro-staffing-3766914665?trk=public_jobs_topcard-title","United States","1 week ago","Tom Mix","https://www.linkedin.com/in/tommix","2023-11-27","","Hospitals and Health Care","Data Engineer","Information Technology and Engineering"
"57 applicants","18611540","NauWork","https://www.linkedin.com/company/nauwork?trk=public_jobs_topcard-org-name","Full-time","A NauWork client is seeking a Data Engineer to join their team. The position is fully remote or hybrid based in San Diego, California.

This client is a leading medical staffing agency with a mission to help others live better by helping healthcare professionals and the patients they serve. They’ve received multiple awards and accolades for “Best Places to Work” from companies like Glassdoor and Modern Healthcare.

As a Data Engineer II with a specialization in MS Power BI, you will lead the development and maintenance of data-driven solutions, creating compelling visualizations, and ensuring data integrity. Your deep understanding of data infrastructure and data visualization will help drive strategic decisions, streamline operations, and empower our team with actionable insights.

Responsibilities:

Data Pipeline Development & Management:


 * Design, construct, install, and maintain large-scale processing systems and other infrastructure.
 * Manage and optimize data pipelines, ensuring data availability, accuracy, and optimal performance.
   
   
   

MS Power BI Development & Management:


 * Develop, maintain, and optimize Power BI dashboards and reports tailored to business needs.
 * Collaborate with stakeholders to identify opportunities for data-driven decision-making and to define metrics and KPIs.
 * Ensure consistency and integrity of data visualizations across all Power BI reports.
   
   
   

Data Analysis & Optimization:


 * Work with cross-functional teams to gather requirements, understand business challenges, and provide data-driven solutions.
 * Continuously analyze data processes and tools for improvement and scalability.
   
   
   

Data Governance & Integrity:


 * Collaborate with data governance teams to ensure data quality, compliance, and consistency.
 * Develop and maintain documentation on data pipelines, data models, and data dictionaries.
   
   
   

Team Collaboration & Leadership:


 * Collaborate with IT, analytics, and business teams to ensure seamless integration of systems and tools.
   
   
   

Required Experience:


 * 5+ years of experience in data engineering with a strong emphasis on data visualization.
 * Bachelor’s degree in computer science, engineering, information systems, or a related field.
 * Proven expertise in MS Power BI development, including DAX, data modeling, and performance tuning.
 * Strong experience in SQL, ETL processes, and data warehouse design.
 * Familiarity with cloud platforms like Azure or AWS.
   
   
   

Preferred Experience:


 * Experience in the healthcare or recruiting industry.
 * Familiarity with data governance principles and practices.
 * Excellent communication skills, both written and verbal.
 * Strong analytical and problem-solving skills with a keen attention to detail.
   
   
   

To Learn More:


 * 503-388-9585
 * 833-NAU-WORK
 * nauwork.com/careers
   
   
   

Category: Technology - System Software

Position: Data Engineer

Location: [Remote] San Diego, California

Job Type: Direct-Hire, Full-Time","Entry level","https://www.linkedin.com/jobs/view/data-engineer-at-nauwork-3776572928?trk=public_jobs_topcard-title","San Diego, CA","4 weeks ago","","","2023-11-06","","Staffing and Recruiting","Data Engineer","Information Technology"
"Over 200 applicants","3722337","Maven Clinic","https://www.linkedin.com/company/mavenclinic?trk=public_jobs_topcard-org-name","Full-time","Maven is the world’s largest virtual clinic for women and families on a mission to make healthcare work for all of us. Maven’s award-winning digital programs provide clinical, emotional, and financial support all in one platform, spanning fertility & family building, maternity & newborn care, parenting & pediatrics, and menopause. Employers and health plans trust Maven’s end-to-end platform to improve clinical outcomes, reduce healthcare costs, and provide equity in benefits programs. Founded in 2014 by CEO Kate Ryder, Maven has raised $300 million in funding from top healthcare and technology investors including General Catalyst, Oak HC/FT, Sequoia, Dragoneer Investment Group, and Lux Capital.

An award-winning culture working towards an important mission – Maven Clinic is a recipient of over 30 workplace and innovation awards, including:


 * TIME 100 Most Influential Companies (2023)
 * CNBC Disruptor 50 List (2022, 2023)
 * Fast Company Most Innovative Companies (2020, 2023)
 * Built In Best Places to Work (2023)
 * Fortune Best Workplaces NY (2020, 2021, 2022, 2023)
 * Great Place to Work certified (2020, 2021, 2022, 2023)
 * Fast Company Best Workplaces for Innovators (2022)
 * Built In LGBTQIA+ Advocacy Award (2022)
   
   

About The Role

As a staff level Data Engineer at Maven Clinic, you will be responsible for driving the technical vision and roadmap for our Data Engineering and Data Platform teams. You will lead the design, implementation, and maintenance of a performant and cost efficient data pipelines (both batch and streaming), as well as contribute to the overall Maven data architecture, with the goal of expanding our data warehouse functionality and uplevel the data analytics and machine learning capabilities. You will work closely with cross-functional teams to ensure the delivery of high-quality data sets and features, that empower our key business and product decision makings, and eventually lead to Maven’s growth and customer success.

Key Responsibilities


 * Lead the design, implementation and maintenance of highly performant and cost efficient data pipelines
 * In partnership with data platform and backend teams, contribute to the architecture design of a highly scalable and reliable data systems
 * Collaborate with product and service engineers in reviewing the modeling of the source datasets, guiding the holistic data vision between various data systems
 * Drive technical design discussions in all related data domains and provide guidance to team members on best practices, coding standards, and architecture principles.
 * Mentor and guide and mid-level data engineers, helping to develop their technical skills and cultivate a culture of continuous learning and improvement.
 * Identify and evaluate emerging technologies, tools, and trends that can drive innovation and improve the efficiency and effectiveness of our data engineering processes.
   
   

Qualifications


 * Bachelor's or Master's degree in Computer Science or related field, or equivalent experience.
 * Minimum of 7 years of experience in data engineering or relevant backend development, with a proven track record of building highly scalable, performant, and reliable data pipelines.
 * Extensive experience in using SQL or similar data processing language to process and analyze data
 * Extensive experience with data modeling for large distributed data warehouses (or a similar cloud based) solutions, is able to discuss in-depth about general principles and trade-offs of different modeling approaches.
 * Hands-on experience with one of the workflow orchestration tools (eg. Apache Airflow)
 * Excellent collaboration and communication skills, with a demonstrated ability to work effectively with cross-functional business partners, especially with teams outside of the engineering group
 * Working proficiency in one of the programming languages (Java, Python, Go, etc.)
 * Product mindset to embrace business needs and produce scalable data/engineering solutions
 * Experience leading technical design discussions and providing guidance on best practices, coding standards, and architecture principles.
 * Strong problem-solving and analytical skills, with a proven ability to deliver high-quality code in a fast-paced environment.
   
   

At Maven Clinic, we are committed to building a world-class digital healthcare platform that empowers women and families to live healthier and more fulfilling lives. If you are a seasoned engineer with a passion for building scalable, performant, and reliable systems, and want to make a real impact in the world of healthcare, we would love to hear from you.

For candidates in NYC or CO, the salary range for this role is $195,000 - $300,000 per year. You may also be entitled to receive a bonus, stock options, and benefits. Individual pay decisions are based on a number of factors, including qualifications for the role, experience level, and skillset.

At Maven we believe that a diverse set of backgrounds and experiences enrich our teams and allow us to achieve above and beyond our goals. If you do not have experience in all of the areas detailed above, we hope that you will share your unique background with us in your application and how it can be additive to our teams.

Benefits & Perks:

We are reimagining what a supportive workplace looks like, from the inside out. On top of standards such as employer-covered health, dental, and insurance plan options, and generous PTO, we offer an all-of-you, inclusive approach to benefits:


 * Maven for Mavens: access to the full platform and specialists, including care for everything from mental health, reproductive health, family planning, pediatrics.
 * Whole-self care through wellness partnerships
 * Weekly breakfast, lunch, and get-togethers
 * 16 weeks 100% paid parental leave, flexible time upon return, and $1.5K/mo for 2 months, new parent stipend (for Mavens who've been with us at least six months)
 * Udemy, annual professional development stipend, and access to a personal career coach through Maven
 * 401K matching for US-based employees (immediately vesting)
   
   

These benefits are applicable to Maven Clinic Co., US-based, full-time employees only. 1099/Contract Providers are ineligible for these benefits.

Maven is an affirmative action and equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, disability, age, sexual orientation, gender identity, national origin, veteran status, or genetic information. Maven is committed to providing access, equal opportunity and reasonable accommodation for individuals with disabilities in employment, its services, programs, and activities. Maven Clinic interview requests and job offers only originate from an @mavenclinic.com email address (e.g jsmith@mavenclinic.com). Maven Clinic will never ask for sensitive information to be delivered over email or phone. If you receive a scam issue or a security issue involving Maven Clinic please notify us at: security@mavenclinic.com. For general and additional inquiries, please contact us at careers@mavenclinic.com.","Mid-Senior level","https://www.linkedin.com/jobs/view/staff-data-engineer-at-maven-clinic-3756636081?trk=public_jobs_topcard-title","United States","2 days ago","","","2023-12-02","","Hospitals and Health Care","Staff Data Engineer","Information Technology"
"Over 200 applicants","22688","Atlassian","https://au.linkedin.com/company/atlassian?trk=public_jobs_topcard-org-name","Full-time","Overview

Atlassian is looking for a Senior Data Engineer to join our Data Engineering Team. You will build top-notch data solutions and applications that inspire important decisions across the organization. You will be reporting to the Senior Data Engineering Manager.

You'll have flexibility in where you work – whether in an office, from home (remote), or a combination of the two.

Responsibilities

A typical day may involve collaborating with partners, you will design data models, acquisition processes, and applications to address needs. With experience in large-scale data processing systems (batch and streaming), you will lead business growth and enhance product experiences. And will collaborate with Technology Teams, Global Analytical Teams, and Data Scientists across programs.

You'll take ownership of problems from end-to-end: extracting/cleaning data, and understanding generating systems. Improving the quality of data by adding sources, coding rules, and producing metrics is crucial as requirements evolve. Agility and smart risk-taking are important qualities in this industry where digital innovation meets partner/customer needs over time.

Qualifications

On your first day, we'll expect you to have:


 * BS in Computer Science or equivalent experience with 5+ years as Data Engineer or similar role
 * Programming skills in Python & Java (good to have)
 * Design data models for storage and retrieval to meet product and requirements
 * Build scalable data pipelines using Spark, Airflow, AWS data services (Redshift, Athena, EMR), Apache projects (Spark, Flink, Hive, and Kafka)
 * Familiar with modern software development practices (Agile, TDD, CICD) applied to data engineering
 * Enhance data quality through internal tools/frameworks detecting DQ issues. Working knowledge of relational databases and SQL query authoring
   
   
   

We’d Be Super Excited If You Have


 * Followed a Kappa architecture with any of your previous deployments and domain knowledge of Financial and People System
   
   
   

Compensation

At Atlassian, we strive to design equitable, explainable, and competitive compensation programs. To support this goal, the baseline of our range is higher than that of the typical market range, but in turn we expect to hire most candidates near this baseline. Base pay within the range is ultimately determined by a candidate's skills, expertise, or experience. In the United States, we have three geographic pay zones. For this role, our current base pay ranges for new hires in each zone are:

Zone A: $163,300 - $217,700

Zone B: $147,000 - $196,000

Zone C: $135,600 - $180,700

This role may also be eligible for benefits, bonuses, commissions, and equity.

Please visit go.atlassian.com/payzones for more information on which locations are included in each of our geographic pay zones. However, please confirm the zone for your specific location with your recruiter.

Our Perks & Benefits

Atlassian offers a variety of perks and benefits to support you, your family and to help you engage with your local community. Our offerings include health coverage, paid volunteer days, wellness resources, and so much more. Visit go.atlassian.com/perksandbenefits to learn more.

About Atlassian

At Atlassian, we're motivated by a common goal: to unleash the potential of every team. Our software products help teams all over the planet and our solutions are designed for all types of work. Team collaboration through our tools makes what may be impossible alone, possible together.

We believe that the unique contributions of all Atlassians create our success. To ensure that our products and culture continue to incorporate everyone's perspectives and experience, we never discriminate based on race, religion, national origin, gender identity or expression, sexual orientation, age, or marital, veteran, or disability status. All your information will be kept confidential according to EEO guidelines.

To provide you the best experience, we can support with accommodations or adjustments at any stage of the recruitment process. Simply inform our Recruitment team during your conversation with them.

To learn more about our culture and hiring process, visit go.atlassian.com/crh .","Mid-Senior level","https://www.linkedin.com/jobs/view/senior-data-engineer-at-atlassian-3737407633?trk=public_jobs_topcard-title","Mountain View, CA","5 days ago","","","2023-11-29","","Software Development","Senior Data Engineer","Information Technology"
"Over 200 applicants","165158","Netflix","https://www.linkedin.com/company/netflix?trk=public_jobs_topcard-org-name","Full-time","About The Team
Media Infrastructure Platform - Storage team provides a content media storage infrastructure to enable Netflix to store and access media files at scale, reliably, and efficiently to meet the demands of a globally distributed studio. Our products are highly leveraged by Netflix Studio and Content Engineering, for example, for the encoding pipeline, post production pipeline, content asset management, marketing applications, etc. We are partnering closely with a variety of cross-functional teams including Netflix engineering platform, data science engineering, and content business engineering, as well as product managers. The team current owns a few key services:
++ An object storage built on top of S3 with a set of features like Netflix access control, security, chunking, client side encryption, full file checksum, etc.
++ A cloud-based file and folder solution with a set of features like files and folder change events/notifications, lifecycle management, media sequence detection, etc.
Besides the cloud services the team currently owns, the team is also looking to grow into a more diverse storage space to meet the needs of business growth.
The Media Infrastructure Platform - Storage team is part of a media-focused engineering group which provides highly available infrastructure for content production and processing, storage and access, across all Netflix productions and licensed content. Infrastructure pieces like massive scale media processing platforms (1 , 2) media storage, workflows (Conductor), media asset management, collaboration, reporting, data movement , and data processing are some of the key services we build. All of this is custom built on top of Amazon Web Services (AWS) infrastructure.

You Will Be Successful In This Role If You


 * You are self-motivated and can work independently, while also being able to collaborating with engineers on the team as well as across functional teams
 * You thrive in ambiguity. Complex and fuzzy requirements are more common than clearly defined customer expectations
 * You are passionate about code quality, and engineering best practices
 * You enjoy building a robust, scalable, and highly available infrastructure to empower other engineers, with a focus on developer experience, observability and operational excellence
 * You have excellent communication skills
   
   
   

Qualifications


 * Have 3+ years of experience with working on large-scale resilient distributed systems
 * Have proficiency in Java or other OO programming languages, as well as OO design principles
 * Have a good understanding of concepts like multi-threading and parallelism
 * Experience in designing and developing microservices
 * Experience in data modeling and API design
 * Good understanding of software observability
   
   
   

Nice To Have


 * Experience with UI/UX work as full stack engineer
 * Experience in building platform/infrastructure
 * Experience with cloud storage or file system
   
   

At Netflix, we carefully consider a wide range of compensation factors to determine your personal top of market. We rely on market indicators to determine compensation and consider your specific job, skills, and experience to get it right. These considerations can cause your compensation to vary and will also be dependent on your location.
The overall market range for roles in this area of Netflix is typically $100,000 - $700,000
This market range is based on total compensation (vs. only base salary), which is in line with our compensation philosophy. Netflix is a unique culture and environment. Learn more here.
We are an equal opportunity employer and celebrate diversity, recognizing that diversity of thought and background builds stronger teams. We approach diversity and inclusion seriously and thoughtfully. We do not discriminate on the basis of race, religion, color, ancestry, national origin, caste, sex, sexual orientation, gender, gender identity or expression, age, disability, medical condition, pregnancy, genetic makeup, marital status, or military service.","Not Applicable","https://www.linkedin.com/jobs/view/software-engineer-l4-media-infrastructure-platform-storage-at-netflix-3725924151?trk=public_jobs_topcard-title","United States","1 week ago","","","2023-11-25","","Movies, Videos, and Sound, Technology, Information and Internet, and Entertainment Providers","Software Engineer L4 - Media Infrastructure Platform Storage","Engineering and Information Technology"
"48 applicants","18482786","Hour Consulting","https://ca.linkedin.com/company/hourconsultingrecruitmentandhrsolutions?trk=public_jobs_topcard-org-name","Full-time","Our client is in the Higher Education Technology space (Ed-Tech) and they are committed to supporting students succeed.

They are a SaaS company with supporting students actively using our platform globally.

Understanding students and institutions, as well as their behaviors through data, lies at the foundation of our work to improve student success. Every data point in the systems is important to helping us achieve that goal, so they are looking for people with a strong background in data engineering and analytics to help us design, build, scale, as well as maintain our data pipelines and models. As a Senior Data Science Engineer, you will be working with a variety of internal teams across engineering, product, and business to help solve their data needs. Your work will directly and tangibly impact the success of millions of students across the world.

In terms of the role and responsibilities, you will

 * Identify the data needs of our engineering, product, and business teams, understand their specific requirements for metrics and analysis, then build efficient, scalable, accurate, and complete data pipelines to enable data-informed decisions across the company
 * Architect data pipelines and models that power internal analytics for our teams, as well as customer-facing data visualization product features
 * Drive the collection of new data and the evolution of existing data sources, collaborate with the engineering teams to manage our product instrumentation strategies and data structures
 * Help the product and engineering teams understand and generalize statistical models from our research efforts, and help build data systems that would allow these models to be used directly in our product to drive student success
 * Work with Product Management to ensure productive, fast-moving sprints that deliver the maximum value to our customers
 * Help to continuously improve the team processes of our engineering team



You should

 * Have at least 5-7 years of experience in a Data Engineering or Data Science role, with a focus on instrumenting data collection, building data pipelines and conducting data-intensive analysis
 * Have a strong engineering background and are interested in data
 * Care deeply about the integrity of data, have a good nose for inconsistencies in data, and be able to pinpoint the issue to ensure that the team is not making decisions based on inaccurate or incomplete data
 * Have extensive experience of a scientific computing language (e.g. Python) and SQL
 * Have experience building an Amazon Web Services-based system that processes data across multiple data stores and technologies, including MySQL, Redis, Elasticsearch
 * Know the best practices of how different types of data should be visualized in different contexts
 * Be comfortable using multiple communication and collaboration tools to work effectively with colleagues across North America and Europe



What will make you stand out

 * Experience working with Python or Java web applications
 * Experience working in the higher-ed technology space, particularly in a Data Engineering related role
 * Experience leading a team
 * Experience working with a remote or distributed team



What you can expect

 * A chance to work towards an amazing mission of helping students succeed as a team member of a global tech startup
 * Remote-friendly work environment: Ability to work from anywhere in the EST in the US
 * Generous paid vacation time
 * Continuous learning and growth culture with many opportunities to develop professionally
 * Health Benefits including health, dental, life , disability insurance and travel coverage
 * Participation in matching 401K plan
 * Home Office Set up support with a company laptop, equipment and support to set up your home office
 * A chance to work with a global collaborative, friendly and diverse team","Mid-Senior level","https://www.linkedin.com/jobs/view/senior-data-engineer-at-hour-consulting-3726911630?trk=public_jobs_topcard-title","United States","2 months ago","","","2023-09-05","","Staffing and Recruiting","Senior Data Engineer","Information Technology"
"Over 200 applicants","1270591","SpiderOak","https://www.linkedin.com/company/spideroak?trk=public_jobs_topcard-org-name","Full-time","We're only as strong as our weakest link.




In the past, launch operations were vertically integrated, allowing space organizations to build cybersecurity systems based on internal trust. Today, the horizontal integration of ground stations, spacecraft, and payloads means trusting third parties with mission-critical data.




SpiderOak is committed to building secure and reliable software that protects our customers’ data. Our approach is fundamentally different from most mainstream security companies. Instead of adding layers of security on top of an insecure system, we build software that is inherently secure.




SpiderOak builds need-to-know technology that supports customers working in hostile environments. Traditional systems trust IT infrastructure to maintain the whole security system, even though the news is rife with evidence that this model does not work. Our software combines end to end encryption with a distributed ledger (aka Blockchain) technology to offer best-in-class security with no backdoors.




Who We Need




 * A curious, creative, Software Engineer with relevant work experience in an Agile/Scrum environment. You'll have a desire to continue to build your skills and knowledge in Cryptography, or you'll bring some experience with Cryptography to SpiderOak.
 * Someone with experience working in Rust (and C++) who can provide us with knowledge and experience in writing reliable, maintainable code.
 * You will thrive at SpiderOak if you have a sense of initiative and are a self-starter who enjoys working collaboratively across cross-functional teams, with the unique ability to work in a fully remote/virtual environment.




What You'll Do




 * Develop and/or maintain advanced knowledge of computing system integrations and make recommendations or decisions on software and hardware configurations and developments.
 * Provide analysis and recommended design of applications, middleware, and system architecture to solve complex problems.
 * Apply a structured approach to design and implementation of systems and processes.
 * Translate mission and customer requirements into capabilities, testing, and validation of services.
 * Analyze and allocates requirements to system architecture components and oversee the development, testing, and validation of systems and services.
 * Coordinate the integration of systems and services and oversee deployments.
 * Extend our current codebase to provide new capabilities and enhanced assurance and performance.




What To Expect




 * In the first year in the role, you'll be learning to collaborate with multiple teams in order to understand our product, Orbitsecure. An ability to understand hardware configurations, and how to develop or alter solutions around multiple environments, will be key to your success. All this leads to our ultimate goal: Successfully deploying Orbitsecure in space.
 * Some travel, up to 10%, may be required.
 * SpiderOak Inc. and Mission Systems is a 100% remote-based working environment. Our culture is based on trust and flexibility. We believe our employees are the key to our success, and welcome new ideas and talents.
 * We provide a wide-range of benefits including health care coverage, a 401(k) plan w/employer match, employer paid life insurance policy, short-term and long-term disability policies, paid maternity and paternity leave, PTO, paid sick leave, and other benefits.




What We Provide




SpiderOak Inc. and Mission Systems is a 100% remote-based working environment. Our culture is built on trust and flexibility. We believe our employees are the key to our success, and welcome new ideas and talents.




We are an equal opportunity employer that is committed to diversity and inclusion in the workplace. We prohibit discrimination and harassment of any kind based on race, color, sex, religion, sexual orientation, national origin, disability, genetic information, pregnancy, or any other protected characteristic as outlined by federal, state, or local laws. In other words, we welcome you just as you are!




To learn more about SpiderOak Inc. and Mission Systems, visit our website at:




Powered by JazzHR




nWYdNY8sRZ","Entry level","https://www.linkedin.com/jobs/view/software-engineer-at-spideroak-3707174719?trk=public_jobs_topcard-title","Austin, TX","3 months ago","","","2023-08-29","","Internet Publishing","Software Engineer","Engineering and Information Technology"
"Over 200 applicants","71550138","The Providencia Group","https://www.linkedin.com/company/the-providencia-group?trk=public_jobs_topcard-org-name","Full-time","TITLE: Data Engineer




LOCATION: Remote




TRAVEL: Minimal




About Us




The Providencia Group is led by a purpose: to address global challenges and make an impact that matters through delivering transformative solutions. This purpose defines who we are and extends to relationships with our clients, our people, and our communities. We combine purpose, innovation, and experience to deliver impactful results.




About The Team




We are problem solvers working with leading agencies and organizations to help them address many of today’s most complex challenges. Our world-class team of technologists, program managers, and subject matter experts is uniquely qualified to address ever-evolving, large-scale challenges. In an imperfect world, The Providencia Group puts capability and purpose into action.




What You’ll Be Part Of – TPG Culture




At TPG, we expect incredible tangible results. TPG professionals play a unique role in delivering these results. We reach across disciplines and borders to serve our global organization. We provide a roadmap focusing on people, our work, and continuous improvement. We see people as people, take care of each other, commit to the mission, move quickly and bravely, get better every day, and seek truth. We are the backbone of TPG.




What You’ll Do




The Data Engineer plays a critical role in managing and processing data to support our organization's analytical and operational needs. This position involves working with data pipelines, databases, and architectures to ensure data quality and accessibility.




Responsibilities Include, But Are Not Limited To




 * Analyze raw data from various sources
 * Develop and maintain datasets, schemas and models
 * Improve data quality and efficiency
 * Document data flows and mappings
 * Collaborate with data analysts, scientists and other stakeholders
 * Duties are performed via a government approved computer system
 * Employees are required to possess strong computer skills in MS Word and Excel
 * Perform related duties as assigned, within your scope of practice – management reserves the right to revise these duties as necessary
 * Develop internal/external reports for information that is coming from ORR
 * Assist with buildout of data dictionary through SharePoint
 * Analyze data sets to ensure alignment to overall organizational focus
 * Advise on updated reporting to capture data through mapping and format it in a useable format that can be easily interpreted by other departments
 * Bring innovation and creativity to legacy technologies and programs to drive/shift changes




Minimum Qualifications & Skills




 * Bachelor’s degree in computer science, engineering or related field
 * Preferred programming languages include SQL, Python, R, Tableau, and PowerBI
 * Experience in data engineering, development or analysis
 * Knowledge of data structures, algorithms and programming languages
 * Proficiency in database systems, tools and frameworks
 * Data visualization, communication and presentation skills
 * Must possess strong computer skills in MS Office, including Excel, Word, Teams
 * Ability to type 45 wpm




Work Environment




This is a remote opportunity where occasional travel could be required. Since this is a remote role, a dedicated workspace conducive to full videoconferencing (camera and audio) for facilitating webinars and online discussions.




Work Schedule




This is a full-time position, but hours could vary depending on needs. May include travel, evenings, and weekends to meet different time zones and projects.




Condition of employment




 * Complete a rigorous culture and competency testing process
 * Complete a Drug Test
 * Must be at least 21 years of age
 * A valid US Driver’s license
 * Have the ability to obtain a Public Trust Clearance




Security Clearance Requirements




 * Applicants selected will be subject to a government background investigation and may need to meet eligibility requirements for access to classified information.
 * Must be a U.S Citizen or Permanent Resident.
 * Residency requirement - 3 consecutive years in the last 5 years.




Physical Demands




 * Standing/Walking/Mobility: Must have mobility to attend meeting with other managers and employees.
 * Climbing/Stooping/Kneeling: 10% of the time.
 * Lifting/Pulling/Pushing: 10% of the time.
 * Fingering/Grasping/Feeling: Must be able to write, type and use a telephone system 100% of the time.
 * Sitting: Sitting for prolonged and extended periods of time.




For more information about the company please visit our website at https://www.theprovidenciagroup.com




Providencia is an Equal Opportunity Employer and does not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, disability or any other federal, state or local protected class.","Entry level","https://www.linkedin.com/jobs/view/data-engineer-at-the-providencia-group-3733292753?trk=public_jobs_topcard-title","United States","2 weeks ago","","","2023-11-17","","Business Consulting and Services","Data Engineer","Information Technology"
"Over 200 applicants","93380834","Sonitalent Corp","https://www.linkedin.com/company/sonitalent-corp?trk=public_jobs_topcard-org-name","Full-time","Role: MySQL Data Engineer with Java

Location: Remote

Key skills: Data Engineer, MySql, SQL, Java

MUST Have

8+ years’ experience as a Data Engineer/ Developer

Well versed with MySQL database queries and creation of database views.

Development experience with REST, SOAP, LDAP, MySQL

Development experience with Java applications","Entry level","https://www.linkedin.com/jobs/view/mysql-data-engineer-with-java-at-sonitalent-corp-3750133947?trk=public_jobs_topcard-title","United States","1 month ago","","","2023-10-30","","Staffing and Recruiting","MySQL Data Engineer with Java","Information Technology"
