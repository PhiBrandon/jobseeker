job_title,company_name,location,job_link,job_id,job_description
Core Infrastructure Engineer - Networking,Kraken Digital Asset Exchange,European Union (Remote),https://www.linkedin.com/jobs/view/3756657176/?eBP=CwEAAAGMRKeZGIQFUvL5I6VvwidXinOBO9b-K6PpFLD8QzndQf6dQTOpKZ4kIwqoiEaZFY8AHZwaTtfrr3JhPOrtoOzawFazaaK4Ar5xSgOxQgrBl4oaDs07-WKr6JKzwxP2d7ckr9YVQ-P56C56jX6HA21CfhZtL1cdHjHGS8C_bag6XIao923NY-cWn5bh7Iz4FO9E26qU1wZaiavs7g1gYuXuTulF2rspxHuOqo21CMhhpDJbeEjUhJZw5iczv4AbbNcR9kCF-D8vRncm6TkyDGWHuYr5D1-DP_oM841Z-QzM3Z7re6p96Jjiz2ToJqFVaV_UcYZ6AP_dp9cWIs1dSHPg9RdneSMrjNRO_pZw3DBTaHVYB9x32IDAKFfJoFhHlwu-Hp20h2eZRw&refId=v5p1jII4oRcnpACm%2FDi5Wg%3D%3D&trackingId=nTh0XboLccUsHWUfWpekpg%3D%3D&trk=flagship3_search_srp_jobs,3756657176,"About the job
            
 
Building the Future of Crypto Our Krakenites are a world-class team with crypto conviction, united by our desire to discover and unlock the potential of crypto and blockchain technology.What makes us different?Kraken is a mission-focused company rooted in crypto values. As a Krakenite, you’ll join us on our mission to accelerate the global adoption of crypto, so that everyone can achieve financial freedom and inclusion. For over a decade, Kraken’s focus on our mission and crypto ethos has attracted many of the most talented crypto experts in the world.Before you apply, please read the Kraken Culture page to learn more about our internal culture, values, and mission.As a fully remote company, we have Krakenites in 60+ countries who speak over 50 languages. Krakenites are industry pioneers who develop premium crypto products for experienced traders, institutions, and newcomers to the space. Kraken is committed to industry-leading security, crypto education, and world-class client support through our products like Kraken Pro, Kraken NFT, and Kraken Futures.Become a Krakenite and build the future of crypto!Proof of workThe teamIf you thrive in a challenging, fun and fast-paced environment, the Core Infrastructure Engineer at Kraken is for you! Not only is this role a strategic hands-on role that is critical to the continuing success of Kraken; you will join a highly dedicated team that is responsible for managing the entire network and compute infrastructure for one of the leading cryptocurrency exchanges in the world. We are looking for a dynamic and innovative self-starter with the ability to think “outside of the box” to solve complex problems in a sustainable way.Find our more about Core Infrastructure Engineering in our latest engineering blogpost!This role is fully remote. We prefer candidates in European Timezones, to cover current needs.The opportunity Design, implement, manage, and defend a “zero trust”, defense-in-depth networkImplement and evolve enterprise network access controlsManage and deploy ""infrastructure as code""Write good quality policies, procedures, and technical documentationCoordinate with internal development teams and other stakeholders to ensure network security principles are ""built-in"" from the beginningWork with Engineering and IT Security teams to ensure that product features are securely deployed and monitoredMonitor and maintain all physical infrastructure (network, security, and compute), cloud data center environments, remote office locations, and respond to critical network problems to maximize service uptimeAssist in various projects with network related requirementsMentor and evangelize security practices through cross-functional work with internal stakeholders and teamsAssume on-call responsibilities and duties per the scheduleTravel as needed to support physical datacenter and office locations
Skills You Should HODL 5+ years of network engineering background including offensive/defensive securityDeep experience in public (AWS or Azure or Google), private and/or hybrid cloud infrastructureExperience with network security management tools and techniquesFamiliarity with security testing tools (performance and threat-based)Proficient in network and security design, implementation, and administration leveraging industry standard platforms from vendors such as: Palo Alto Networks, Cisco, Juniper, and ArubaExperience with Switching (Capacity Planning & VLAN’s), Routing (OSPF, EIGRP, BGP, ECMP, PBF), WAN Technologies (MPLS, VPLS, VPN), public cloud networking, and Security (IPS, RBAC, etc.)Previous experience monitoring and management of intrusion detection systems and firewall devicesDemonstrated experience researching, building and implementing defensive security systems that are used against internal and external attack vectorsExcellent communication skills: demonstrated ability to explain complex technical issues to both technical and non-technical audiencesExcellent analytical and problem-solving skillsAbility to perform well under pressure, high attention to detailStrong desire / interest in learning new technologyHighly motivated and passionate about infrastructure engineering and securityThis role requires the ability to travel. For this reason, a valid passport will be mandatory
Location Tagging: #EUKraken is powered by people from around the world and we celebrate all Krakenites for their diverse talents, backgrounds, contributions and unique perspectives. We hire strictly based on merit, meaning we seek out the candidates with the right abilities, knowledge, and skills considered the most suitable for the job. We encourage you to apply for roles where you don't fully meet the listed requirements, especially if you're passionate or knowledgable about crypto!As an equal opportunity employer, we don’t tolerate discrimination or harassment of any kind. Whether that’s based on race, ethnicity, age, gender identity, citizenship, religion, sexual orientation, disability, pregnancy, veteran status or any other protected characteristic as outlined by federal, state or local laws.Stay in the knowFollow us on TwitterLearn on the Kraken BlogConnect on LinkedIn"
Junior Data Engineer,HireMeFast LLC - Career Accelerator - Land A Job,"Milwaukee, WI (Remote)",https://www.linkedin.com/jobs/view/3780155059/?eBP=JOB_SEARCH_ORGANIC&refId=v5p1jII4oRcnpACm%2FDi5Wg%3D%3D&trackingId=USGDSSl0xKtM9lKhGT6ygA%3D%3D&trk=flagship3_search_srp_jobs,3780155059,"About the job
            
 
This is a remote position. Job Title: Junior Data Engineer Employment Type: Full-Time Salary: $64,000 - $76,000 per annum Experience Required:  Minimum 1 year of project experience How to Apply: visit hiremefast.net  to learn more & apply.About us: HireMeFast is a leading staffing and recruitment agency specializing in connecting businesses with top-tier talent across various industries. Our mission is to bridge the gap between exceptional candidates and organizations needing their skills, expertise, and unique qualities. Our team of experienced and dedicated recruitment specialists utilizes innovative sourcing strategies, and a vast network to identify and attract top talent. We conduct comprehensive procedures to ensure that only the most qualified candidates are presented to our clients.Position SummaryJoin the fast-paced, innovative, and collaborative environment focused on providing an AIOps platform that enhances the intelligence of the CVS Health infrastructure. Work closely with subject matter experts and colleagues to build and scale out machine learning and AI solutions that will detect, predict, and recommend solutions to correct issues before system impact and enhance efficiency, reliability, and performance of CVS Health’s IT operations.Key Responsibilities include: Data pipeline development: Designed, implemented, and managed data pipelines for extracting, transforming, and loading data from various sources into data lakes for processing, analytics, and correlation. Data modeling: Create and maintain data models ensuring data quality, scalability, and efficiency Develop and automate processes to clean, transform, and prepare data for analytics, ensuring data accuracy and consistency Data Integration: Integrate data from disparate sources, both structured and unstructured to provide a unified view of key infrastructure platform and application data Utilize big data technologies such as Kafka to process and analyze large volumes of data efficiently Implement data security measures to protect sensitive information and ensure compliance with data and privacy regulation Create/maintain documentation for data processes, data flows, and system configurations Performance Optimization- Monitor and optimize data pipelines and systems for performance, scalability and cost-effectiveness 
Characteristics of this role: Team Player: Willing to teach, share knowledge, and work with others to make the team successful. Communication: Exceptional verbal, written, organizational, presentation, and communication skills. Creativity: Ability to take written and verbal requirements and come up with other innovative ideas. Attention to detail: Systematically and accurately research future solutions and current problems. Strong work ethic: The innate drive to do work extremely well. Passion: A drive to deliver better products and services than expected to customers. 
Required Qualifications 2+ years of programming experience in languages such as Python, Java, SQL 2+ years of experience with ETL tools and database management (relational, non-relational) 2+ years of experience in data modeling techniques and tools to design efficient scalable data structures Skills in data quality assessment, data cleansing, and data validation 
Qualifications Knowledge of big data technologies and cloud platforms Experience with technologies like PySpark, Databricks, and Azure Synapse. 
EducationBachelor’s degree in Computer Science, Information Technology, or related field, or equivalent working experienceWhy HireMeFast LLC?At HireMeFast, we understand that finding the right individuals to join your team is crucial for success and growth of your organization. We are committed to streamlining the hiring process for our clients, ensuring they have access to a diverse pool of highly qualified candidates who are a perfect fit for their specific needs."
Data Engineer,Otter Products,"Fort Collins, CO (Remote)",https://www.linkedin.com/jobs/view/3766728302/?eBP=JOB_SEARCH_ORGANIC&refId=v5p1jII4oRcnpACm%2FDi5Wg%3D%3D&trackingId=LO3%2F5gcrdN0zZE7u5d7riw%3D%3D&trk=flagship3_search_srp_jobs,3766728302,"About the job
            
 
Overview Otter Products is currently recruiting for a Data Engineer to join our Data team! You can be based in our Fort Collins, CO office with flexibility to work a portion of your time remotely, or 100% Remote in the U.S.The Data Engineer applies professional experience, concepts, and company objectives toward building and operationalizing the minimally inclusive data necessary for the enterprise data initiatives following industry standard practices and tools. The bulk of the Data Engineer's work would be in designing, managing and optimizing data pipelines, then moving these data pipelines effectively into production for key data and analytics consumers like business/data analysts, data scientists or other stakeholder that needs curated data for data and analytics use cases across the enterprise. The Data Engineer also ensures compliance with data governance and data security requirements while creating, improving and operationalizing these integrated and reusable data pipelines to enable faster data access, integrated data reuse and vastly improved time-to-solution for data and analytics initiatives. Additionally, the Data Engineer will be expected to collaborate with other data team members and data consumers working on the models and algorithms developed by them toward optimization for data quality, security, and governance to put them into production leading to potentially large productivity gains. About Otter Products Otter Products, we grow to give. From our founder's garage in 1998 to the global technology leader we are today, Otter Products continues to drive growth through innovation.Through our industry-leading brands - OtterBox, Liviri and OtterCares - we provide our partners the number-one selling and most trusted products in our categories. Our philanthropic spirit is the foundation on which we foster our partner relationships, allowing us to grow and to give - together.By way of our charitable arm, the OtterCares Foundation, we support our communities and invest in our future through education that inspires kids to change the world.And even as our global community of Otters continues to grow, our founder's core values are still at the heart of everything we do. We measure our success by our ability to give back to our communities and strengthen opportunities for all.For more information visit otterproducts.com Responsibilities  Complete analysis, design and development of BI solutions using Azure SQL ServerFamiliarity with Data Warehouse conceptsExperience coding the extraction, transformation, and loading (ETL) processesDatabase development primarily in SSIS, Data Factory and SQLPartner with the business to determine end user database/reporting needs and requirementsGenerate ad hoc reports using Power BI or SSRSCollaborate with other developers to create and implement best approach solution(s)Troubleshoot Azure tools, systems, and softwareReview queries for performance issues, making changes as neededCollaborate with team to performance-tune Azure application as necessaryAssist with the analysis and extraction of relevant information from large amounts of historical business data to feed data science initiativesParticipate and support the design and documentation of processes for large scale data analyses, model development, model validation and model implementationSupport and maintain a positive safety culture by following all safety policies and procedures and actively contributing to a safe working environmentOther duties as required
 Qualifications  Bachelor's degree required. Degree in Computer Science or Mathematics preferredMinimum of four years of experience in an IT or analytical role required. Experience in database development, report writing and/or statistics preferred.
 EEO Otter Products, LLC is an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, age, marital status, pregnancy, sex, sexual orientation, gender, gender identity or expression, national origin, disability, veteran status, or any other characteristic or status protected by law. For US Based Roles Only - Compensation Range Minimum USD $90,000.00/Yr. For US Based Roles Only - Compensation Range Maximum USD $120,000.00/Yr. Additional Total Rewards Profit Sharing Program Eligible, Benefits Eligible - Full Time- check out otterproducts.com/careers/why for more info"
Data Engineer,"ISC (Integrated Specialty Coverages, LLC)","Carlsbad, CA (Remote)",https://www.linkedin.com/jobs/view/3778293528/?eBP=JOB_SEARCH_ORGANIC&refId=v5p1jII4oRcnpACm%2FDi5Wg%3D%3D&trackingId=vncw5RgtUEa7dy56g%2Bubtg%3D%3D&trk=flagship3_search_srp_jobs,3778293528,"About the job
            
 
About Integrated Specialty CoveragesIntegrated Specialty Coverages LLC (ISC) is a growth stage technology and data-driven insurance company leading innovation in the market.Backed by one of the leading private equity firms, KKR, and led by a forward-thinking management team, ISC is combining the worlds of insurance and technology to create an Insurtech powerhouse. As a leading online distributor of insurance products for a range of industries and ""Main Street USA"", we are looking for the right people to help us in our mission of achieving exponential growth. We strive to be the number one place to go for brokers and agents to source insurance. To accomplish this, we're building a digitally focused team that deeply understands the intersection between user experience, data, and AI/ML to optimize the way we engage with our customers and partners.Job SummaryThe Data Engineer role will be the technical liaison between multiple groups including a data science team, the engineering team, product management, and business stakeholders. You do not need any insurance knowledge prior, however, you must quickly dive deep into the insurance world and ask questions to become a subject matter expert. You will be responsible for building a data platform to facilitate the data science team. You must be a self-starter that can build out features such as a data pipeline from scratch. There will be support from both engineering and data science for any buildout.Job Responsibilities Create and maintain optimal data pipeline architecture. Assemble large, complex data sets that meet functional / non-functional business requirements. Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using AWS technologies, SQL, and Python. Work with stakeholders including the Executive, Product, Data Science, and Engineering teams to assist with data-related technical issues and support their data infrastructure needs. Work with data science and analytics teams to strive for greater functionality in our data systems. 
Minimum Qualifications: Advanced working SQL knowledge and experience working with relational databases, strong query authoring (SQL) as well as working familiarity with a variety of databases (Snowflake, Redshift, MySQL, MSSQL, etc.)Experience building and optimizing data pipelines, architecture and data sets. Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. Build processes supporting data transformation, data structures, metadata, dependency, and workload management. A successful history of transforming, processing and extracting value from large disconnected, datasets from a variety of data sources (Flat files, Excel, databases, etc.)Strong analytic skills related to working with unstructured datasets. Strong project management and organizational skills. Experience supporting and working with cross-functional teams in a dynamic environment. Ability to mentor/guide/collaborate with other team members. 
We are looking for a candidate with 3-5 years of experience in a Data Engineer role. They should also have experience using the following software/tools: Experience with relational and MPP databases, including Snowflake and MySQL. Experience developing software in an agile environment from the requirements stage to production. Experience with version control: gitExperience with container technologies: DockerExperience with data pipeline and workflow management tools: Airflow, Jenkins, AWS Glue, Azkaban, Luigi, etc. Experience with AWS cloud services: EC2, ECS, Batch, S3, EMR, RDS, RedshiftExperience with other cloud services: Snowflake, AirflowExperience with object-oriented/object function scripting languages: Python, Java, C++, etc. Experience with data modeling and data warehouse designExperience with data visualization tools (PowerBI, QuickSight)
The starting annual pay scale for this position is listed below. Actual starting pay will be based on factors such as skills, qualifications, training, and experience. In addition, the company offers comprehensive benefits including medical, dental and vision insurance, 401(k) plan with match, paid time off, and other benefits.ISC's salary ranges are determined by role and level. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations and could be higher or lower based on a multitude of factors, including job-related skills, experience, and relevant education or training.National Pay Range$100,000—$110,000 USDBenefits of Working at ISC Competitive vacation and flexible working arrangementsComprehensive and inclusive health benefitsA variety of professional development and mentorship opportunitiesChoice of technology whether at home or in the office
ISC's Ownership Behaviors Do the Right Thing *Be Relentless. Pursue Excellence. *Personal Responsibility & Own OutcomesTry Fast. Learn Fast. Fail Fast. Think Big. *Build Extraordinary Owners & Join Forces *Grit & Determination
Applicants with disabilities may contact ISC HR department via e-mail, and other means to request and arrange accommodation. If you need assistance to accommodate a disability, you may request accommodation at any time. Please contact ISC at HR@ISCMGA.com.ISC believes in creating long-term relationships by being responsive and relevant and by consistently delivering value to our community of customers. Specifically, with our employees, we focus on attracting, developing, and retaining the best talent for our business, challenging our people, demonstrating a ""can-do"" attitude, and fostering a collaborative and mutually supportive environment. ISC is an equal opportunity employer and a participant in the US federal E-Verify program (US).Diversity creates a healthier atmosphere: All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, gender, gender identity, sexual orientation, marital status, medical condition, genetic information, mental or physical disability, military or veteran status, or any other characteristic protected by local, state, or Federal law."
ENTRY LEVEL DATA ENGINEER (REMOTE),SynergisticIT,"Jacksonville, FL (Remote)",https://www.linkedin.com/jobs/view/3767587909/?eBP=JOB_SEARCH_ORGANIC&refId=v5p1jII4oRcnpACm%2FDi5Wg%3D%3D&trackingId=59xKpZ3tK9col1Tr%2Fwcmfg%3D%3D&trk=flagship3_search_srp_jobs,3767587909,"About the job
            
 
SYNERGISTICIT wants every candidate to know that the Job Market is Challenging and to stand out, you need to have exceptional skills and technologies and that's where we come in to make sure you get the attention which you needSynergisticIT understands the complex nature of the job market and how difficult it can be to secure a position, especially for fresh graduates. Therefore, we assist and help tech-savvies to convert their passions into professions. We go above and beyond to keep you working in your niche.As we focus on long-term success, we provide complete career development solutions. From job search to upskilling portfolio and interview preparation, we can guide you at every step of your career.SynergisticIT spares no efforts to connect you with a large network of tech giants, including Google, Apple, PayPal, Dell, Cisco, Client, etc. Presently, we are actively looking for ENTRY LEVEL DATA ENGINEER a driven mindset. Get the right opportunity and gain experience in building web-centric solutions on Java.Who Should Apply : Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or anyone looking to make their career in IT IndustryWe also assist in filing for STEM extension and H1b and Green card filing.Candidates who are serious about their future in the IT Industry and have set big goals for themselves.Candidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. We also offer Skill Enhancement Programs if the candidates are missing skills or experience which our clients need with great outcomesCandidates can benefit from skill enhancement if they fall into the below categories. If they are qualified with enough skills then no need for skill enhancementCandidates Who Lack ExperienceHave had a break in careersLack Technical Competencycandidates who want to get employed and make a career in the Tech Industry Please Also Check The Below Linkshttps://www.synergisticit.com/candidate-outcomes/https://www.synergisticit.com/java-track/https://www.synergisticit.com/data-science-track/https://www.synergisticit.com/which-is-the-best-option-for-tech-job-seekers-staffing-companies-consulting-companies-bootcamps-or-synergisticit/https://www.synergisticit.com/contact-us/If the skills are not a match candidates can opt for Skill enhancement. Or their resume can be sent out to clients to see if responses are achievableREQUIRED SKILLS For Java/Software Programmers Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, ITHighly motivated, self-learner, and technically inquisitiveExperience in programming language Java and understanding of the software development life cycleKnowledge of Core Java , javascript , C++ or software programmingSpring boot, Microservices and REST API's experienceExcellent written and verbal communication skills
For data Science/Machine learningRequired Skills Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, ITHighly motivated, self-learner, and technically inquisitiveExperience in programming language Java and understanding of the software development life cycleKnowledge of Statistics, Python, data visualization toolsExcellent written and verbal communication skills
Preferred skills: NLP, Text mining, Tableau, Time series analysisTechnical skills are required by clients for selection even if its Junior or entry level position each additional Technical skill helps a candidate's resume to be picked by clients over other job seekers.Clients hire candidates with the right technical skills which they need and reject candidates who lack the required technical skills.No third party candidates or c2c candidatesPlease apply to the postingNo phone calls please. Shortlisted candidates would be reached out"
Data Engineer,Gauntlet,Los Angeles Metropolitan Area (Remote),https://www.linkedin.com/jobs/view/3765636920/?eBP=JOB_SEARCH_ORGANIC&refId=v5p1jII4oRcnpACm%2FDi5Wg%3D%3D&trackingId=GkAjshz8YijrrU7qIHvtEQ%3D%3D&trk=flagship3_search_srp_jobs,3765636920,"About the job
            
 
Gauntlet is DeFi’s risk manager. We drive capital efficiency while maintaining economic safety for some of the largest crypto protocols with our simulations. Gauntlet manages risk and incentives for over $9 Billion in assets.Gauntlet continuously publishes cutting-edge research, making us the most cited peer-reviewed articles in the DeFi industry. We’re a Series B company with :60 employees operating remote-first with a home base in New York City.Gauntlet’s mission is to help make blockchain protocols and smart contracts safer and more trustworthy for users. Building decentralized systems creates new challenges for protocol developers, smart contract developers, and asset holders that are not seen in traditional development and investing.Gauntlet is building a blockchain simulation and testing platform that leverages battle-tested techniques from other industries to emulate interactions in crypto networks. Simulation provides transparency and greatly reduces the cost of experimentation so that teams can rapidly design, launch, and scale new decentralized systems.Responsibilities Design, build, and maintain robust scalable ETL pipelines that ingest data from DeFi protocols, blockchain networks, and various data sourcesContribute to making our data platform world-classCollaborate with stakeholders to understand the data requirements and provide necessary supportImplement quality control measures to ensure data integrity and qualityActively participate in code and design reviews, providing constructive feedback to peers and maintaining high coding standardsKeep up-to-date with the latest industry trends, technologies, and best practices to ensure the continuous improvement of data platformContribute to the creation of technical documentation and user guidesTroubleshoot, diagnose, and resolve software defects, ensuring optimal system performance and reliability
Qualifications 5+ years of professional engineering experienceProficient in writing code in Python and SQLHands-on experience with OLAP database such as BigQueryExperience with modern data transformation tools such as DBTExperience with workflow orchestration tools such as Dagster and AirflowExperience with distributed data processing frameworksStrong communication skills and the ability to work collaboratively in a distributed team environment
Bonus Points Experience working in the crypto industry is a plus but not requiredEnthusiasm for the space, especially DeFi, is very much desiredSmart contract development experience (e.g. Solidity)Experience with building machine learning models at scalePublished or presented research in the space
Benefits and Perks Remote first - work from anywhere in the US & CAN!Regular in-person company retreats and cross-country ""office visit"" perk100% paid medical, dental and vision premiums for employeesLaptop, monitor, keyboard and mouse setup provided$1,000 WFH stipendMonthly reimbursement for home internet, phone, and cellular dataUnlimited vacation100% paid parental leave of 12 weeksFertility benefitsOpportunity for incentive compensation
Please note at this time our hiring is reserved for potential employees who are able to work within the contiguous United States and Canada. Should you need alternative accommodations, please note that in your application.The national pay range for this role is $150,000 - $180,000 base plus additional On Target Earnings potential by level and equity in the company. Our salary ranges are based on paying competitively for a company of our size and industry, and are one part of many compensation, benefits and other reward opportunities we provide. Individual pay rate decisions are based on a number of factors, including qualifications for the role, experience level, skill set, and balancing internal equity relative to peers at the company."
Staff Scala Developer - 100% Remote,Signify Technology,United States (Remote),https://www.linkedin.com/jobs/view/3759037297/?eBP=CwEAAAGMRKeZGCevz3A3JEEJ3Aq7D_QZaMeZGSLT6rluTQqKYC4edxOHYQkUKNWzykJOCeFdl7pEt7kCt2cy0mK13x4d5xGgRtVthMsm6CrZ4fis8jaMogDuCMvLdBzZuDly_v-7Let_pl_lD9rOnOTlBmHK3565Xg9dhagbcUEXAcutgO8_2cJqUk7k4Oo1TMEuWxO1U33HriCvCn7OLnEwu7xuJKJDcsvIBuGTPwAlOhdsaatyci9L4uqCZpaYMCP9DqpjrI0sDKyy8x9rUy1AItdL66LgqBLQRApU_huRGmRdn-wJqFg_K8AC-zlF0hXki7zY1qbZWEEzCOpKJ-DCQZBl3_XbQZ2es0FKwDL4udfTBIo_IuHygHZyMxiOPgNjkth_pzqg5IQlPQ&refId=v5p1jII4oRcnpACm%2FDi5Wg%3D%3D&trackingId=hKEBJJzJH7ioo3moO%2F8D5w%3D%3D&trk=flagship3_search_srp_jobs,3759037297,"About the job
            
 
Lead Scala Developers - ZIO 12 month Contract to Hire100% Remote (United States)Compensation: $100 - $130 per hourOur client, an established name in the streaming industry, have asked us to partner with them exclusively on hiring additional Staff Software Developers to join one of our client's established team of experienced Functional Scala Developers.You will be working on a greenfield development project of our client's primary customer facing streaming platform which will replace their existing platform and be rolled out globally.This will be a 12 month contract into hire position working 100% remotely.Candidates will ideally be located within EST or if not, willing to work on that schedule.About you: Expert using Scala within ZIO
Expediated interview 2 step interview process.(No abstract or academic live coding puzzles) Please note we are unable to offer employer sponsorship and will not consider applications from third parties."
Staff Front End Software Engineer,Esusu,United States (Remote),https://www.linkedin.com/jobs/view/3768803346/?eBP=CwEAAAGMRKeZGIogjBwL38MiRSo5G6qxLQwAARenUerlXpJ3LTQE9q4IQyory44w35ueXAQB5XRed5pFNxnEQOj3GAamyvrNw2fUkxvNakl3QFxJ-z2Q-q6KnL3gSTyUKVV2vVy1IQ_zjJoO9CBhh0wjOuvkaMA6WcElNk2DQ2bOoC2GnXvy9yewYb-wyeCQwFbi84RxCebzetQSdInwZH0f1G816kxGTuAxptL9YEC87KLkOf_mEH_kz5zh0LhFjGCbfgi8fMYJOD4IqyYYWm1TvVnIVYL3Vw-tapEd4iICJ7__ElVEZTA5nkYWPQjmxbCqGpXPJV1A2zNThDs1rfGJmg9FEIKNX9-Ejko&refId=v5p1jII4oRcnpACm%2FDi5Wg%3D%3D&trackingId=3xEFDBbXqu2hQ2UfN1J0TQ%3D%3D&trk=flagship3_search_srp_jobs,3768803346,"About the job
            
 
Democratize Access To Credit Together we’ll dismantle barriers to housing for working families and use data to eliminate the racial wealth gap. The ability to build credit from rental payments has the potential to give over 45 million renters with little to no credit a pathway into the financial system. Those individuals then stand to save over $200,000 in reduced lifetime interest payments, build home equity by qualifying for mortgages, and build wealth by passing job screening requirements. The transformational impact of this cannot be understated. Engineering Team We are building a team of changemakers. It is not enough to have the intention to do good - Esusu engineers translate intent into action. We believe that accomplishing our ambitious goals cannot be done by lone heroes. Lasting change requires work by synergistic teams, and we look for Engineers who are highly skilled at collaboration.The ChallengeWe are seeking an experienced Staff Front-end Software Engineer with React, TypeScript, and related technologies expertise to play a pivotal role in the future growth of our company. You will be responsible for providing front-end guidance for an agile team of mission-driven and talented front-end and back-end engineers.Our back-end is microservice based, hosted in AWS and mainly written in Go. Our data infrastructure has historically been on MongoDB, but we also have numerous PostgreSQL databases and we are evolving a data lake based on Snowflake. Our front-ends are currently React apps written in Typescript. We are continuously innovating, and always open to new solutions. Because Esusu is a financial services company, we are obsessive about every aspect of security. We are Test-Driven-Development enthusiasts, and we are evolving towards a DevOps environment where your team is responsible for the full spectrum of development, testing, deployment, and maintenance, including working with product and operations teams to create customer-focused solutions. Even though we are 100% remote, Esusu’s culture is intensely collaborative.You will be working on our next generation consumer facing apps, designing and building new features/functionality that will benefit millions of people. You will help set our future generation front-end framework and best practices. These products will drive the future growth of our company, and you will be pivotal in their development. This will require you to strip problems down to their essential elements, then conceive simple, elegant solutions –always planning for scale, always thinking from a customer and product-oriented perspective.This role will report to the company’s Consumer Products Engineering Manager.What You Will DeliverYou and your team will create a suite of consumer applications that will help millions of people in their financial journey and keep a roof over their heads.You will lead the evolution of the front end of our consumer web app.You will collaborate with engineers, product managers and user experience (UX) designers to design and develop modern, consistent, fast, responsive user interfaces using leading UX design patterns and best practices.You will mentor other developers on the team in their work.You will help maintain existing web apps, including testing, fixing bugs, troubleshooting, adding new features, and maintaining documentation.You will design and develop new custom components to meet project requirements.You will interface with product and business stakeholders to help to develop the product roadmap of our consumer facing services.Core CompetenciesSuperb programming and software development skills –You can independently devise and implement solutions to problems with minimal explanation needed.Strong communication skills –You can efficiently translate between technical and non-technical audiences and have strong writing skills.High standards –Your work is of the highest quality and you continue to raise the bar within your immediate team and our organization.Balance velocity with long-term goals –You balance thinking big with delivering the right thing in an agile and speedy manner.Heart of a teacher –You are a capable mentor and able to inspire and empower others on your team.Basic QualificationsVery Strong Experience with front end web technologies.Strong experience developing web applications with React or React Native.Superb communication, design, and technical writing skills - you can independently devise and implement solutions to problems with minimal explanation needed.Experience with TDD practices and testing tools such as Jest, Mocha, JUnit, Cucumber, Selenium, or Playwright.Experience creating solutions in an Agile environment.Experience working in a DevOps environment (we are on AWS).Experience working in a CI/CD environment.Experience in app instrumentation and analyzing and utilizing the results, particularly concerning usability and engagement.Above and BeyondExperience building visualization-intensive applications, using tools such as D3.js.Familiarity with back-end technologies including Nodejs, Go, Python, SQL, AWS Lambda, infrastructure as code etc.Experience in a FinTech or PropTech startupExperience working with globally distributed teamsExperience in SOC2 certified organizationsBenefitsCompetitive Salary - for Series B startup - $170,000-190,000 annualRestricted Stock Units (RSU)Full Medical, Dental, Vision InsuranceFitness/Gym Stipend401K PlanPaid Parental LeaveRemote Work Environment - we are 100% virtual, spread across 5 continents.Flexible PTO PolicyMission driven company with strong cultureThis job is eligible only for the following states: Alabama, Arizona, California, Colorado, Connecticut, Florida, Georgia, Hawaii, Idaho, Illinois, Indiana, Iowa, Kansas, Massachusetts, Maine, Maryland, Michigan, Missouri, Minnesota, Montana, New Hampshire, New Jersey, New Mexico, New York, North Carolina, Ohio, Oklahoma, Oregon, Pennsylvania, Tennessee, Texas, Utah, Vermont, Virginia, Washington, and West Virginia. We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.© Esusu Inc. All rights reserved, Esusu is proud to be an equal opportunity workplace. We celebrate diversity and are committed to creating an inclusive environment for all employees. We do not discriminate on the basis of race, religion, color, gender identity, sexual orientation, age, disability, veteran status, or other applicable legally protected characteristics. We encourage people of different backgrounds, experiences, abilities, and perspectives to apply."
Entry-Level Data Scientist Engineer   - US/Canada Only,Phoenix Recruitment LLC,"Denver, CO (Remote)",https://www.linkedin.com/jobs/view/3780149992/?eBP=JOB_SEARCH_ORGANIC&refId=v5p1jII4oRcnpACm%2FDi5Wg%3D%3D&trackingId=6YRUTecOm%2BGbG2X1Sivt7Q%3D%3D&trk=flagship3_search_srp_jobs,3780149992,"About the job
            
 
This is a remote position.Entry-Level Data Scientist Engineer - US/Canada Only, 1 year of project experienceEmployment Type: Full-timeBase Salary: $63K-$73KPhoenix Recruitment offers a variety of recruiting services to assist both employers and employees. They specialize in marketing open positions, recruiting, and helping employers to find qualified candidates across various industries. Phoenix Recruitment has expertise in streamlining the hiring process. They can help ensure that the process is efficient, well organized, and compliant with relevant regulations.Skills and Abilities: Strong knowledge of R or Python for data analysis and modeling. Proficiency in statistical programs such as R, SAS, MATLAB, or Python. Familiarity with spreadsheets (VBA) and database applications (Access, Oracle, SQL, or equivalent technology). Basic understanding of SQL, Javascript, XML, JSON, and HTML. Ability to learn new methods quickly and work under deadlines. Excellent teamwork and communication skills. Strong analytical and problem-solving abilities. Basic understanding of SQL, Javascript, XML, JSON, and HTML. 
Preferred: Knowledge of actuarial concepts and life, health, and/or annuity products. Experience with statistical modeling techniques such as GLM, Decision Trees, Time Series, Regression, etc. Familiarity with Microsoft DeployR. Exposure to insurance risk analysis. Basic experience in computational finance, econometrics, statistics, and math. Knowledge of SQL and VBA. Familiarity with R or Python for predictive modeling 
Why Phoenix Recruitment LLC?Phoenix Recruitment often has an extensive network of employers and candidates. This network allows them to tap into a pool of qualified candidates and connect them with suitable job opportunities. They can also leverage their connections to help employers find the right talent efficiently. Outsourcing the recruitment process to a specialized agency can save your time and resources, avoid delays, reduce administrative burdens, and increase the chances of finding the right fit for your organization."
Data Engineer,Shipwell,"Austin, TX (Remote)",https://www.linkedin.com/jobs/view/3756146639/?eBP=JOB_SEARCH_ORGANIC&refId=v5p1jII4oRcnpACm%2FDi5Wg%3D%3D&trackingId=uVB6eng8zr9J1wIwI7fQ8w%3D%3D&trk=flagship3_search_srp_jobs,3756146639,"About the job
            
 
About ShipwellAt Shipwell, we empower supply chain efficiency and service effectiveness at scale. The Shipwell platform includes capabilities previously out of most shippers' technical reach and affordability today. Our solution combines everything shippers need, from transportation management and visibility to procurement, in a comprehensive, easy-to-use platform. It will adapt and scale as market and business demand change, allowing shippers to operate, manage, and optimize the shipping process seamlessly. Industry experts have recognized Shipwell's traction in the market and have differentiated Shipwell as a leader in the logistics industry. Awards include Gartner Magic Quadrant for TMS 2023, 2022, 2021, Food Logistics' 2022 Top Software & Technology Providers, and FreightWaves' FreightTech 2022 and 2021 Awards for Innovation and Disruption in Freight Industry. Shipwell was also named the fourth fastest-growing company in North America on the 2021, 2022, and 2023 Deloitte Technology Fast 500 and Forbes 2020 Next Billion-Dollar Startup.Our CultureShipwell is a fast-paced, high-energy start-up that strives to build the future of shipping every day. Diversity of thought and cross-department collaboration is very important to us. We deliver open, honest, careful communication and work as hard as we play. We create & deliver solutions that are revolutionizing the industry, which brings excitement and purpose to our work. If you are looking for a place that will help you tap into your best work-self and give you hands-on experience building something big, then we invite you to come and build the future of shipping with us!About The RoleAs a Data Engineer, you will contribute to the design, development, and maintenance of data pipelines. Your responsibilities will encompass extracting, transforming, and loading (ETL) data from various sources, developing and executing data integrity standards, and optimizing performance. You will collaborate closely with cross-functional teams and contribute to the development of scalable and efficient data architectures. This role provides a dynamic opportunity to leverage your expertise in data engineering and learn more about the data infrastructure at Shipwell. This will enable you to drive insights and support data-driven decision-making within our organization.What we're looking for: Experience working with large-scale data model refactoring for better performance, interpretability, and maintainabilityExperience designing and implementing data models and analytics reports in a cloud environmentProven track record of implementing data engineering best practices in all aspects of the data pipeline, i.e. ETL, data integrity, and monitoringDemonstrable proficiency in Python, SQL, and DBT. Experience with Looker is a plusExperience with version control tools (GitHub, GitLab) and Agile methodologies. Bachelor's Degree in a quantitative field such as Physics, Engineering, Computer Science, or demonstrated equivalent quantitative experience. Excellent communication skills to effectively collaborate with different teams within the data org
What you'll do when you get here: Collaborate closely with our engineering, analytics, and data science teams as we take our data infrastructure to the next levelRight at the start you will be creating Looker dashboards for our customers, implementing DBT in our data pipelines, and creating monitoring wherever our data is usedYou will own all of the data processes you createYou will become the expert on our data infrastructure and make critical decisions in our path forwardYou will have the opportunity to grow your skill set and take part in projects at the forefront of GenAI, ML, and data science in the logistics industry
Why Shipwell: Enjoy working remotely with the added perk of a home office reimbursementUnlimited Paid Time Off (PTO)A robust healthcare package that includes medical, dental & vision benefits, short-term and long-term disability, AD&S coverage, and flexible/health savings accounts40K program where Shipwell matches up to 4%A yearly learning and development budgetSubsidized internet, cell phone, fitness, and educational reimbursementsVirtual team-building events where fun and connection take center stage Join a vibrant, inclusive workplace shaped by friendly, talented individualsReceive a technology package including a MacBook ProEmployee Recognition Program to celebrate and incentivize hard work and success!
The Salary Range for this role is between $90,000 - $130,000/year. Compensation is based on a number of factors including market location, job-related knowledge, skills, and experience.Shipwell is an equal opportunity employer and welcomes all qualified applicants regardless of race, ethnicity, religion, gender, gender identity, sexual orientation, disability status, protected veteran status, or any other characteristic protected by law. We celebrate diversity and believe that experience comes in different forms. Diversity in our team makes for better problem-solving, more creative thinking, and ultimately a better product and company culture.Even more important than your resume is a clear demonstration of impact, dedication, and the ability to thrive in a fast-paced and collaborative environment. Shipwell strives to have an inclusive work environment; so if you are hard-working & good at what you do then please come as you are. We want you to contribute, grow, & learn at Shipwell.We are looking forward to adding new perspectives to our team!For more information about Shipwell visit shipwell.com, or connect with us on Twitter @shipwell, LinkedIn, and Facebook.com/Shipwellinc"
Data Engineer -Remote,"RIT Solutions, Inc.","Denver, CO (Remote)",https://www.linkedin.com/jobs/view/3768007496/?eBP=JOB_SEARCH_ORGANIC&refId=v5p1jII4oRcnpACm%2FDi5Wg%3D%3D&trackingId=Rhdqjz8jT54Vb%2BbPtz3qIQ%3D%3D&trk=flagship3_search_srp_jobs,3768007496,"About the job
            
 
Duration: 6+ months CTHMust have a valid LinkedIn profileRequired Skills  3-6 years of experience (no more than 6 years) Start up environment experience Python, SQL, and DBT"
Entry Level Data Engineer (Remote),SynergisticIT,"Savannah, GA (Remote)",https://www.linkedin.com/jobs/view/3767597153/?eBP=JOB_SEARCH_ORGANIC&refId=v5p1jII4oRcnpACm%2FDi5Wg%3D%3D&trackingId=DKeyB6smNSImoHs76AlEyA%3D%3D&trk=flagship3_search_srp_jobs,3767597153,"About the job
            
 
The Job Market is Challenging due to almost 300,000 Tech Layoffs since October 2022 due to which thousands of laid off Techies are competing with existing Jobseekers. Entry level Job seekers struggle to get responses to their applications forget about getting client interviews. As the Saying goes ""when the Going gets tough the Tough get going” Candidates who want to make a tech career they need to differentiate themselves by ensuring they have exceptional skills and technologies to be noticed by clients.Since 2010 Synergisticit has helped Jobseekers differentiate themselves by providing candidates the requisite skills and experience to outperform at interviews and clients. Here at SynergisticIT We just don't focus on getting you a Job we make careers.All Positions are open for all visas and US citizensWe are matchmakers we provide clients with candidates who can perform from day 1 of starting work. In this challenging economy every client wants to save $$$'s and they want the best value for their money. Jobseekers need to self-evaluate if they have the requisite skills to meet client requirements and needs. Clients now post covid can also hire remote workers which increases even more competition for jobseekers.We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like apple, google, PayPal, western union, Client, visa, Walmart labs etc. to name a few.We have an excellent reputation with the clients. Currently, We are looking for entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Machine Learning engineers for full time positions with clients.Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in IT IndustryWe assist in filing for STEM extension and also for H1b and Green card filing to Candidates We also offer optionally Skill and technology enhancement programs for candidates who are either missing skills or are lacking Industry/Client experience with Projects and skills. Candidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. If they are qualified with enough skills and have hands on project work at clients then they should be good to be submitted to clients. Shortlisting and selection is totally based on clients discretion not ours.please check the below links to see success outcomes of our candidateshttps://www.synergisticit.com/candidate-outcomes/We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023https://youtu.be/Rfn8Y0gnfL8?si=p2V4KFv5HukJXTrnhttps://youtu.be/-HkNN1ag6Zk?si=1NRfgsvL_HJMVb6Qhttps://www.youtube.com/watch?v=OAFOhcGy9Z8https://www.youtube.com/watch?v=EmO7NrWHkLMhttps://www.youtube.com/watch?v=NVBU9RYZ6UIhttps://www.youtube.com/watch?v=Yy74yvjatVgFor preparing for interviews please visit https://www.synergisticit.com/interview-questions/We are looking for the right matching candidates for our clientsPlease apply via the job postingREQUIRED SKILLS For Java /Full Stack/Software Programmer Bachelor's degree or Master's degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, ITHighly motivated, self-learner, and technically inquisitiveExperience in programming language Java and understanding of the software development life cycleProject work on the skillsKnowledge of Core Java, JavaScript, C++ or software programmingSpring boot, Microservices, Docker, Jenkins and REST API's experienceExcellent written and verbal communication skills
For data Science/Machine learning PositionsRequired Skills Bachelor's degree or Master's degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, ITProject work on the technologies neededHighly motivated, self-learner, and technically inquisitiveExperience in programming language Java and understanding of the software development life cycleKnowledge of Statistics, SAS, Python, Computer Vision, data visualization toolsExcellent written and verbal communication skills
Preferred skills: NLP, Text mining, Tableau, PowerBI, TensorFlowIf you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team.No phone calls please. Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates"
Data Engineer (Remote),Cascade Debt,United States (Remote),https://www.linkedin.com/jobs/view/3755336571/?eBP=JOB_SEARCH_ORGANIC&refId=v5p1jII4oRcnpACm%2FDi5Wg%3D%3D&trackingId=WSgQRS%2Fi9HzpH2EguFa5FQ%3D%3D&trk=flagship3_search_srp_jobs,3755336571,"About the job
            
 
Do you want to work in a small team where you can make a real difference in a company? Would you embrace the challenge of building a high impact platform that is used globally? Then we want to speak with you!About CascadeCascade is a fintech startup backed by Canadian and US investors that empowers companies to grow by democratizing access to institutional debt. We are building tools that modernize how companies raise and manage debt, lowering the barriers to entry to the $7 trillion specialty finance and alternative credit market for companies around the world. Founded just last year, we are gearing up for our next phase of technical development and are seeking a talented Data Engineer to lead the way.About The RoleAs a Data Engineer you’ll be an early member of a growing team building a pioneering platform in the debt infrastructure space. You will design, build, and launch efficient, scalable, and reliable data pipelines to move and transform data .What You’ll Do  Building databases, data lakes and data ingestion pipelines to integrate customer databases and datasets to our systems  Ingesting financial datasets from external customers, then u pdating and maintaining accurate and complete data mappings to ensure that our product s are displaying high quality data  M onitoring and alerting across our data pipelines in order to make sure that our data ingests are reliable and correct  Translat ing business goals and stakeholder requirements into new data flows and deliver analytical insights ensuring that the data flows create real business value continuously 
Skills And Experience  2 + years experience in a data engineering or data analyst role  E xperience with relational SQL and NoSQL databases, including PostgreSQL , MySQL, MariaDB, MongoDB, Bigtable, etc.  E xperience working with ETL technologies , such as Databricks, Fivetran , or dbt  Proficiency in writing SQL queries and knowledge of analytical data warehouses such as RedShift, BigQuery , and Snowflake  Some experience with CI/CD automation such as GitHub Actions  Developing APIs and integrating with 3rd party APIs  Bonus: experience in fintech / SaaS / credit analysis 
People Who Thrive At Cascade Are  Self-starters, who take the initiative to tackle challenges in a remote work environment  Good communicators, who can operate without ego to discuss, learn, grow, and help others do the same  Passionate about creating great digital experiences for users  Problem solvers who are not satisfied with the status quo 
Benefits Remote: we are a remote-first company and are very flexible on hours as long as things get done Home-office: there’s a $1000 USD home office allowance to set yourself up Equity: we expect you to have an owner-mentality, and have the equity plan to match Benefits: health, dental, vision, and more Perks: we offer free lunches weekly and off-site trips Job satisfaction: we offer autonomy, ample opportunities for mastery, and an opportunity to make a difference for companies around the world"
Data Engineer,Anika Systems,"Leesburg, VA (Remote)",https://www.linkedin.com/jobs/view/3766103708/?eBP=JOB_SEARCH_ORGANIC&refId=v5p1jII4oRcnpACm%2FDi5Wg%3D%3D&trackingId=DHNreY4jZqs1i4ls3VIBDQ%3D%3D&trk=flagship3_search_srp_jobs,3766103708,"About the job
            
 
Anika Systems is a fast growing, woman-owned small business that specializes in providing innovative IT solutions for federal government agencies. Our expertise lies in accelerating delivery in Data and Analytics, Intelligent Automation, Application Development, and IT Modernization. We are currently expanding our Federal team and are seeking a passionate and talented Data Engineer. This opportunity is 100% remote.Must be a U.S. Citizen with the ability to obtain and maintain a government suitability clearanceResponsibilities Designs, develops, builds, analyzes, evaluates, and installs database management systems to include database modeling and design, relational database architecture, metadata, and repository creation and configuration management.Defines and oversees database organizations, standards, controls, procedures, and documentation. Provides technical consulting in the definition, design, and creation of a database environment.Advises applications development staff and users on data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.Designs and implements databases with respect to access methods, access time, batch processes, device allocation, validation checks, organization, protection and security, documentation, and statistical methods.Uses data mapping, data mining, and data transformational analysis tools to design and develop databases.Determines data storage and optimum storage requirements.Prepares system requirements, source analysis, and process analyses and designs throughout the database implementation.
Required Skills And Experience BA/BS and 1 year of relevant experienceExperience with DataBricks, SQL, PythonApplications development experience with data-based solutions to business problems, data architectures, database management system facilities and capabilities, and the operation and tuning of databases.Excellent communication skills (both oral and written) with process-oriented organizational skills to ensure project success
Desired Skills And Experience Demonstrated successful development experience with the full life cycle of government database management systems including database modeling and design, relational database architecture, metadata and repository creation, and configuration management.
Powered by JazzHRTlKWIorucJ"
Entry Level Software Engineer - Data Backend Engineer (Remote - Canada),Yelp,United States (Remote),https://www.linkedin.com/jobs/view/3752881757/?eBP=JOB_SEARCH_ORGANIC&refId=v5p1jII4oRcnpACm%2FDi5Wg%3D%3D&trackingId=4vXalxIuLNMGC1iWw%2FoidA%3D%3D&trk=flagship3_search_srp_jobs,3752881757,"About the job
            
 
Yelp engineering culture is driven by our values: we’re a cooperative team that values individual authenticity and encourages creative solutions to problems. All new engineers deploy working code their first week, and we strive to broaden individual impact with support from managers, mentors, and teams. At the end of the day, we’re all about helping our users, growing as engineers, and having fun in a collaborative environment. Interested in exploring data and finding innovative ways to collect it, curate it, and make it easily accessible for others to use? How about doing that at an enormous scale? Yelp’s datasets contain billions of interactions between users and local businesses around the globe. If working with our vast amount of data sounds exciting, come and join us!The Marketplace Data Observability team is responsible for managing, governing, and monitoring all of Yelp’s Ads data to protect our revenue stream and also support offline analysis that aids in making long term company decisions. We build tools that help us to better understand our data quality, and we strive to uncover data issues before they impact our consumers.As a Data Backend Engineer on this team, you will be responsible for building elegant and scalable data products that serve critical, up-to-date, structured information to support different types of analytics within Yelp. As a core contributor to our growing data modeling and data warehousing engineering efforts, you will help design and own mission-critical data flow pipelines and datastores to enable decisions including effective A/B testing and company investments. This opportunity is fully remote and does not require you to be located in any particular area in Canada. We welcome applicants from throughout Canada. We’d love to have you apply, even if you don’t feel you meet every single requirement in this posting. At Yelp, we’re looking for great people, not just those who simply check off all the boxes.  Build systems that can effectively store and crunch terabytes of data.Design and develop data models for efficient data storage, retrieval, and reporting.Create and maintain conceptual, logical, and physical data models using industry-standard modeling tools.Collaborate with cross-functional teams, including engineers, data analysts, business analysts, and data scientists, to understand data requirements and translate them into effective data models.Participate in data integration efforts, including ETL processes and data migration.Stay up-to-date with industry best practices, emerging technologies, and trends related to data warehousing.Support on-call rotations as needed to operate the team.
  Understanding of high performing and scalable data systems.Experience in building and orchestrating ETL pipelines.Experience with Data Lake or Data Warehouse landscape.A hunger for tracking down root causes and fixing them in systematic ways.Ability to communicate effectively to technical and non-technical cohorts alike. Exposure to some of the following technologies: Python, AWS Redshift, AWS Athena / Apache Presto, Big Data technologies (e.g S3, Hadoop, Hive, Spark, Flink, Kafka etc), NoSQL systems like Cassandra, DBT."
Remote Opportunity: Quantexa Data Engineer,SPAR Information Systems LLC,United States (Remote),https://www.linkedin.com/jobs/view/3680073191/?eBP=JOB_SEARCH_ORGANIC&refId=v5p1jII4oRcnpACm%2FDi5Wg%3D%3D&trackingId=TfWzIN230JNEU68%2BCkOqQg%3D%3D&trk=flagship3_search_srp_jobs,3680073191,"About the job
            
 
Hello All, Hope you are doing great!! Please go through the job descriptions and let me know your interest. Role: Quantexa Data engineer  Location: Remote Duration: Long Term Contract Job Description: 1. End to End usage of Quantexa including guiding our client team on how to use from both front end and data ingestion Reason they are asking for Certification 2. Resolve Entity issues with data 3. Knowledge of Scala and Hive which requires coding experience using parquet 4. Advanced SQL knowledge requiring writing SQL queries 5. Experience having worked with GCP data lake 6. Quantexa certification requiredThanks & Regards,Satnam SinghDirect: 201 623 3660Email : Satnam.singh@sparinfosys.com"
Data Engineer,Catalist,"Washington, DC (Remote)",https://www.linkedin.com/jobs/view/3744951279/?eBP=JOB_SEARCH_ORGANIC&refId=v5p1jII4oRcnpACm%2FDi5Wg%3D%3D&trackingId=ZG0v%2FIN5BC%2FqS%2FqP0nuPjg%3D%3D&trk=flagship3_search_srp_jobs,3744951279,"About the job
            
 
For over 17 years, Catalist has been a leader in civic data and data science innovation. Our mission is to provide progressive organizations with the data, software, and services needed to better identify, understand, and communicate with the people they need to engage and mobilize. Our clients include the largest, most influential organizations in the U.S. active in civic engagement, advocacy, and political campaigns.Catalist is home to a dedicated, creative team of technologists, data scientists, and campaign experts committed to using our talents and technology to nurture a vibrant and growing progressive community.As a Data Engineer at Catalist, you will work closely with the Analytics and Technology departments to design, build, support, and maintain various data pipelines and processes with an end goal of providing data and intelligence to the progressive community. This role primarily involves translating data architecture designs into functional processes, code, and systems.The ideal candidate will be a highly motivated individual with excellent technical skills, a strong desire to learn new skills, and an interest in progressive politics. Catalist values creativity and problem-solving. Our work is on the cutting edge of data-driven politics, and your support will help Democratic candidates and progressive organizations conduct successful advocacy and electoral campaigns.This position reports to the Deputy Chief Data Officer. The Data Engineer is a part of a growing Data team that supports all underlying work at Catalist.This position is included in our CWA bargaining unit.Principle Duties & Responsibilities Develop scalable, production processes, code, and systems for data ingest, transformation, modeling, and reporting across a variety of platformsTranslate mock-ups and designs into functional processes, code, and systemsCreate architecture designs when neededProvide quality assurance and testing on processes, code, systems, and productsBecome an internal subject matter expert on various datasets and support other Catalist departments/teams on usage of those datasetsExecute ad-hoc data and database maintenance tasks as requiredProject manage cross-departmental efforts, with direct responsibility for stakeholder engagement, management, and execution of technical elements
Requirements BS or BA in a technical field, or relevant experience1-2 years of experience working with SQL databasesExperience with data cleaning or ETL processesExperience with distributed computing systems and/or distributed datastores (particularly the Hadoop ecosystem)Experience managing projectsFamiliarity with Catalist data, progressive politics, voter files, and/or commercial dataBackground check required
Preferred Skills & Abilities Willingness to be a problem solver and produce results in a fast paced environmentAbility to be creative and personable, and articulate ideas clearlyExperience working with SQL databasesProficiency with Python or another object-oriented programming language (R, Java, Scala, etc…)Experience working in cloud environments (AWS, GCP, etc.)Experience working in command line environments such as BashExperience with a version control tool such as git or github
BenefitsMedical, Dental, Vision, Prescription DrugCatalist offers Medical, Dental, Vision, and Prescription Drug coverage for eligible staff and their eligible dependents. Catalist’s Medical plan is a comprehensive PPO program including Prescription Drug coverage with 85% of the premium paid by Catalist. Dental and Vision coverage is provided at no cost to employees.Group Term Life Insurance and Long-Term & Short-Term Disability CoverageGroup Term Life Insurance and Long-Term and Short-Term Disability coverage is available for eligible staff. These benefits are provided at no cost to Catalist employees.401(k) Safe Harbor PlanA 401(k) Safe Harbor Plan is available to eligible staff with a 3% contribution from Catalist from the date of hire. Employees may contribute pre-tax or post-tax from their salary up to the legal limits set forth by the IRS.Medical and Dependent Care Flexible Spending Accounts (FSAs)Catalist offers an FSA Program that gives eligible staff the ability to pay out-of-pocket medical/dental/vision/child care expenses from pre-tax earnings.Transit BenefitsCatalist also makes available a Transit benefit FSA program to eligible employees using pre-tax contributions with a company match.Professional Development and Remote Work ExpensesEligible employees may be reimbursed up to $750 each year for professional development / education and remote work expenses.Student Loan PayDown or SaveUpCatalist offers a Student Loan PayDown and College SaveUp benefit for eligible staff.Vacation, Personal Leave, Sick Leave BenefitsCatalist offers generous vacation benefits to all eligible staff. Eligible employees also receive:  14 Paid Holidays Personal Days Sick Leave Parental Leave
Hybrid Office/Remote WorkCertain positions at Catalist are eligible for Office/Remote Hybrid or full Remote status."
"Data Engineer (Associate Consultant), June 2024",UDig,"Nashville, TN (Remote)",https://www.linkedin.com/jobs/view/3716039960/?eBP=JOB_SEARCH_ORGANIC&refId=v5p1jII4oRcnpACm%2FDi5Wg%3D%3D&trackingId=WHaNeoRj22exXv9ZjX5fjw%3D%3D&trk=flagship3_search_srp_jobs,3716039960,"About the job
            
 
Can UDig It? UDig designs, builds, and implements technology solutions that deliver on business objectives. People join our team because they are excited to have an impact on the future of our organization, passionate about delivering meaningful results, and energized by solving complex problems. Oh, and they like working with other driven, smart, and authentic people!UDig is proud to be recognized as a ""Best Places to Work"" in Virginia and Tennessee. We're committed to fostering a transparent environment where leadership is accessible, professional development opportunities are abundant, and you are empowered to achieve your career goals. You'll collaborate, develop deep relationships with teammates, and accelerate your success while having fun!If you're graduating in May 2024 and excited by the idea of impacting organizations by building data solutions to achieve business goals, you should consider joining our growing technology team.How Things Go DownAs a new, entry-level teammate, you'll take part in our ""Breaking Ground"" training program designed to help you quickly get up-to-speed on UDig's delivery approach within a project team. The program covers: UDig delivery process and methodologies such as gathering requirements and delivering projects using agileTechnical guidance and instruction on the best practices from Data leaders at UDigDelivering executive and technical presentationsCoaching to prepare you to for success on your first client project at UDig
A typical day might entail: Project standups and check-insDesign and development of data pipelines, BI reports, and morePlanning and participating in project demosContributing to internal UDig projectsCareer and job growth through UDig-provided trainingCreating long-lasting friendships with teammates through fun activities as well as your projects (including social events, winning a prize for guessing the weight of pumpkin, or sharing experience on cool tech side projects)
UDig believes it is important for Associate Consultants going through our Breaking Ground program to reside in one of our major markets (Richmond, VA or Nashville, TN metro area) for at least the first year of employment.Want to learn more? Hear directly from our teammates by checking out our 'A Day in the Life' blog series: https://www.udig.com/digging-in/breaking-ground-2023/Sound Interesting? Here's the Technical PieceBelow are a few boxes you should be able to check if you're interested: Don't check every box? Go ahead and apply! 1+ year of experience with relational database management systems such as Microsoft SQL Server, MySQL, or OracleUnderstanding of the full lifecycle of design, development, and implementation of data solutions including architecture design, development, testingSome experience in ETL development or reporting is a plusAbility to present technical ideas and high-level concepts and solutions to internal and external team members with varying degrees of technical knowledgeBachelor's Degree in in computer science, information technology, or any science or business disciplineAbility to take your work seriously but also seriously enjoy a good time with your UDig colleagues
What's in It For You? At UDig, you will build your career alongside talented and experienced developers while gaining experience and having fun! We provide continuous challenges and professional development opportunities. Additionally, we offer:  Competitive salary, merit reviews, and career advancement pathsFlexible, hybrid environmentIndividual $1500 Training BudgetRegular team building and social activities (virtual and/or in-person)Transparent culture with strong communication and access to leadershipGreat benefits like Generous Paid Time Off, company holidays, and parental leaveMultiple Single and Family Health Insurance plans to choose fromDental & Vision coverageShort-Term and Long-Term disabilityOptional accident and critical illness coverageMatching 401(k)and more!"
Data Engineer  - Need candidates from Texas ( REMOTE ),Pulivarthi Group (PG),United States (Remote),https://www.linkedin.com/jobs/view/3770685212/?eBP=JOB_SEARCH_ORGANIC&refId=v5p1jII4oRcnpACm%2FDi5Wg%3D%3D&trackingId=eu3m7EccrxGO5F7rolv5wA%3D%3D&trk=flagship3_search_srp_jobs,3770685212,"About the job
            
 
Follow us on Linkedin: https://www.linkedin.com/company/pulivarthigroup/Pulivarthi Group LLC is a Global Staffing & IT Technology Solutions company, with our prime focus of providing world class solutions to our customers with the right talent. We combine the expertise of our team and the culture of your company to help you with the solution that is affordable and innovative using high quality standards and technologies.We’ve served some of the largest healthcare, financial services, and government entities in the U.S. Data Engineer - FT only - Azure SQL with data lakes experience
Data Engineer -the resource requirement for this Data Engineer role needed more clarification.Our customer is requesting a resource who knows Azure CI/CD (Continuous Integration / Continuous Deployment) to build and integrate their DevOps pipeline.Once the DevOps pipelines are mapped, they need the Data Engineer to connect to all the Data Sources, primarily their Data Lake.The Customer (based in Houston) is interested in a resource in a time zone not too far removed from the Central Time Zone to allow real time collaboration."
Data Engineer,Team Remotely Inc,"Denver, CO (Remote)",https://www.linkedin.com/jobs/view/3779588500/?eBP=JOB_SEARCH_ORGANIC&refId=v5p1jII4oRcnpACm%2FDi5Wg%3D%3D&trackingId=QObADXMCpPTrRDsJVoNKJg%3D%3D&trk=flagship3_search_srp_jobs,3779588500,"About the job
            
 
This is a remote position. Data Engineer (US/Canada Residents Only, 1 year experience, remote)Team Remotely Inc. is a staffing and recruitment agency that offers a comprehensive solution for talent acquisition, including sourcing, vetting, pay rolling, and managing talent. Whether you need contract staffing, direct hire, direct sourcing, talent pools, or diversity initiatives, our model can support your hiring strategy. Hiring Type: Full-Time Base Salary: $57K-$68K Per Annum. How to Apply: Please visit teamremotely.com to learn more & apply.Responsibilities Designing user interface changes for web-based DB applications. Reviewing application requirements and interface designs. Developing and implementing highly responsive user interface components using react concepts. Writing application interface codes using JavaScript following react.js workflows. Troubleshooting interface software and debugging application codes. Developing and implementing front-end architecture to support user interface concepts. Monitoring and improving front-end performance. 
Qualifications And Experience Bachelor’s degree in computer science, information technology, or a similar field. Previous experience working as a react.js developer. In-depth knowledge of JavaScript, CSS, HTML, jQuery, and front-end languages. Knowledge of REACT tools including React.js, Webpack, Enzyme, Redux, and Flux. Experience with user interface design. Knowledge of performance testing frameworks, including Mocha and Jest. Experience with browser-based debugging and performance testing software. Troubleshooting skills. Project management skills. Problem-solving skills. Verbal communication skills. 
 Why work with Team Remotely?Team Remotely Inc. is a staffing platform offering a seamless experience for employers and candidates. Employers can post job openings and specify their requirements, while candidates can create profiles and upload resumes.The team of Team Remotely continuously learns and adapts based on previous successful placements, constantly improving its matching capabilities. This ensures that the recommendations provided by Team Remotely are tailored and accurate, increasing the likelihood of a successful match between employers and candidates. By providing intelligent and data-driven solutions, they strive to enhance the efficiency and effectiveness of the hiring process, ultimately helping companies find the best talent and individuals find their dream jobs."
Data Engineer,Leafwell,"Miami, FL (Remote)",https://www.linkedin.com/jobs/view/3667433581/?eBP=JOB_SEARCH_ORGANIC&refId=v5p1jII4oRcnpACm%2FDi5Wg%3D%3D&trackingId=em53Q3Nmu7GvEZK369MU7w%3D%3D&trk=flagship3_search_srp_jobs,3667433581,"About the job
            
 
DescriptionWho is LeafwellLeafwell is a rapidly growing technology and data company that set out to increase access, education, and research into cannabis and to advance its application as medicine.An exciting opportunity for a Data Engineer to join our growing team has arisen.What To Expect As a Data Engineer At LeafwellAs a Data Engineer at Leafwell, you will be in charge of creating and orchestrating the Leafwell data pipeline, which means gathering data, creating & automating data transformations and producing actionable insights for Leafwell’s internal stakeholders. You will support critical testing and rollout of new data features. The Data Engineer will build and maintain systems that inform Leafwell’s business stakeholders about Key Performance Indicators (KPIs) and suggest data-driven strategies to optimize those metrics.Essential Duties And ResponsibilitiesThe Data Engineer will perform the following responsibilities: Acquire, assemble, transform and analyze dataCreate, manage and orchestrate the data pipeline and it’s infrastructurePresent findings, trends, and suggested optimizationsIdentify new opportunities and threats to the company's business modelUpdate and revise reports, queries, and analytic procedures as necessaryDesign and implement tracking so that optimization efforts can be measuredIdentify inefficiencies in data processes and automate where appropriateWrite and update international SOPs and internal documentationSupport IT systems management with testing, validation, and user supportProactively identify initiatives for data-related improvements
Why LeafwellAt Leafwell, we are passionate about our work and seek out employees who contribute the same level of dedication and enthusiasm. We are only as good as the people we hire, so we aim to be the best employer in order to attract the top talent in the industry.Do we have your Interest?RequirementsOur Ideal CandidateOur Ideal Candidate Will Possess The Following Bachelor's degree in Computer Science, Mathematics, Economics, Information Systems, or another quantitative field3-5 years of relevant professional experience in Data Engineering / AnalyticsAdvance working knowledge and experience in SQL and relational databasesStrong data management abilities, including data analysis, standardization, cleansing, querying, and consolidation of dataData visualization experience (e.g Tableau, Looker, etc.)Exercises self-reliance, initiative, and leadership while keeping stakeholders properly informed and involvedReliably manage numerous duties during a workday, necessitating interactions with people located across the worldTechnically competent, with the ability to quickly learn new processes and programs, and utilize various software applicationsExcellent communication and interpersonal skills; ability to work with and appeal to a wide variety of personalities and professional tendencies
Bonus Points If You Have Experience In The Following Cannabis knowledge; industry experience is a plusUsed Data Orchestration tools like Dagster or AirflowUsed dbt or comparable data transformation softwareGitPython or R programmingAmazon Web ServicesPostgreSQL and RedShiftCRM productsData Visualization in MetabaseEffective project management skills and comfort utilizing a project management platform in collaboration with other team membersData Science projects
Benefits HighlightsOur benefits include, but are not limited to: Remote first - Most of our employees are 100% remote. Positions requiring travel enjoy a hybrid environment. Rapid Growth - Our company is rapidly expanding, and our employees are advancing with us!Training and Development - We foster a culture of learning. We encourage our employees to take part in educational, training, and development opportunities.
Leafwell is dedicated to bringing together people from diverse cultures and perspectives. We work hard to provide a welcoming atmosphere where everyone may flourish, have a sense of belonging, and collaborate effectively. As an equal opportunity employer, we do not tolerate any illegal discrimination against job applicants based on their race, color, religion, veteran status, sex, parental status, gender identity or expression, transgender status, sexual orientation, national origin, age, disability, or genetic information. We are committed to going above and above in creating diversity within our firm and follow the regulations upheld by the EEOC."
Data Engineer,SnapX.ai,"Dallas, TX (Remote)",https://www.linkedin.com/jobs/view/3780166756/?eBP=JOB_SEARCH_ORGANIC&refId=v5p1jII4oRcnpACm%2FDi5Wg%3D%3D&trackingId=A6eUDeU0bNgvCyqXbYkHjg%3D%3D&trk=flagship3_search_srp_jobs,3780166756,"About the job
            
 
Dear Partner, Good morning,greetings from Snaprecruit LLC!Submission you please review the below role,If you are available.  MUST HAVE:e-commerce(retail) experience  5-7 years experience with Python
 Experience working with Google BigQuery and SQL

Desired Skills and Experience
                IT"
Data Engineer,ZAG Technical Services,United States (Remote),https://www.linkedin.com/jobs/view/3771403464/?eBP=JOB_SEARCH_ORGANIC&refId=v5p1jII4oRcnpACm%2FDi5Wg%3D%3D&trackingId=anZpI7KAe9c6C1bi398m7w%3D%3D&trk=flagship3_search_srp_jobs,3771403464,"About the job
            
 
We are looking for a tech savvy Data Engineer to contribute to the building, operating, and scaling of next generation data platforms and tools that will power data-driven analytics capabilities throughout the ZAG products and services portfolio, spanning areas such as business intelligence, reporting, and data analytics.The ideal team member is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. Must be comfortable supporting the data needs of multiple clients and teams, systems, and products within the modernization of ecosystems. The right-fit person will be excited by the prospect of optimizing or even re-designing data processes to support next generation of advanced analytics capabilities, which in-turn translates to implementing strategic enhancements to maintain competitive advantage in the industry.The successful team member will leverage their proficiency to… Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Build the analytics processes required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Data Management and cloud based ‘big data’ technologies.Build analytics tools that utilize technologies to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics.Collaborate with client teams to identify and solution regional and local requirements within the context of the standard global business model for enterprise analytics.Participates in meetings, discussions, strategy sessions where changes, improvements, and enhancements are proposed, evaluated, and approved. 
The exceptional team member will have or demonstrate progressive experiences in…  Bachelor’s degree or higher in Computer Science or related fieldExperience working with BI and data warehousing toolsExperience working with enterprise data, where security is paramount and data governance is criticalAbility to work independently and in a team environment, and effectively engage all levels of the organizationUnderstanding of distributed systems driving large-scale data processing and analytics with a successful history of manipulating, processing, and extracting value from large, disconnected datasets.Ability to communicate effectively (listening, presenting, and questioning)Strong organizational, written, and communication skills
About ZAGZAG Technical Services empowers our clients’ success through intelligent business solutions that increase employee productivity, network security and team collaboration. We aim to ensure that our client’s IT investments drive ROI, provide a competitive advantage and help businesses grow. We do this by living our core set of values: Integrity Foremost, Client-Centered, Accountable Always, Teamwork Throughout, and Exceptional is the Goal.What ZAG Has To Offer Competitive compensation package Benefits including medical, dental, vision, 401K and life insurance 
ZAG Technical Services is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all of our employees.ZAG Technical Services is an E-Verify participant and will provide the Social Security Administration (SSA) and, if necessary, the Department of Homeland Security (DHS), with information from each new employee’s Form I-9 to confirm work authorization."
Data Engineer,theSkimm,"New York, NY (Remote)",https://www.linkedin.com/jobs/view/3754572564/?eBP=JOB_SEARCH_ORGANIC&refId=v5p1jII4oRcnpACm%2FDi5Wg%3D%3D&trackingId=SXN3IemrVXqo18eYD2ofTg%3D%3D&trk=flagship3_search_srp_jobs,3754572564,"About the job
            
 
theSkimm'We're hiring a Data Engineer.About Our Team And What We'll Build TogetherWe're looking for an experienced Data Engineer and mentor to join theSkimm's Tech team. Our mission is to enable our partners across theSkimm, from Editorial and Audio to Marketing and Advertising, to achieve their goals with the best systems and processes we can offer. We build tools throughout the stack, share knowledge across departments, and learn quickly so we can take best advantage of what's coming.As we grow, we're looking for a Data Engineer who will help solidify and expand our pipelines and maintain our data warehouse. Our business is run on detailed analysis of how our products perform with our members, which features they love and which can be improved, and which product and marketing campaigns are bringing in the best quality users. You'll be responsible for working alongside analysts, product, marketing, and other teams to define new data collection components and measurement schemes that support each new product and feature.How You'll Contribute To Our Mission Create and maintain data pipelines to provide insights and drive business decisionsEstablish theSkimm's data warehousing strategy (ex. Kimball, Data Vault, etc.)Maintain theSkimm's data infrastructure on our AWS accountsCollaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization. Write unit/integration tests, contributes to engineering wiki, and documents workImplement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it
You're ready for this! Here's a bit more about what we're looking for 3+ years of industry experience measuring product performance and user behaviorExperience working with a variety of data management technologies, including RedShift, Kinesis/Kafka, Glue, Spark, Postgres, Airflow, dbt, and others. Experience implementing BI reporting tools such as LookerExperience interfacing with engineers, product managers and analysts to understand data needsKnowledge of a variety of measurement beacons, SDKs, APIs including Google Analytics, Amplitude, Braze, email service providersA commitment to building secure, resilient, fault-tolerant architectures, with clear documentation and procedures in place for supportA focus on accuracy and detail as you build, ensuring data pipelines produce clear, predictable resultsAbility to thrive in a dynamic, fast-paced, collaborative, and high-growth environmentFacility in presenting and discussing the trade-offs in employing different engineering solutions to a problem, valuing pragmatism over idealismAn empathetic leadership style that encourages open communication and trustUnderstanding of the typical metrics a subscription and advertising-supported business needs to measure successExperience with ML techniques as applied to behavioral segmentation or anomaly identification is a plusFamiliarity using developer tools that increase productivity and facilitate the development of resilient code (eg. Docker, CircleCI, Serverless) is a plusFamiliarity and enthusiasm for theSkimm: a passion for our audience and mission is a plus
The expected annual base salary for this role is $125,000-$135,000. We'll consider a variety of factors when determining the offered base salary including an evaluation of a candidate's skills, abilities, experience, location, market demands, and internal parity.Why our employees love working hereWe are mission driven and values driven:We are collectively building an impactful brand to empower generations of informed, confident women. Our vision is for every woman to have the info they need to navigate a complex world and the decisions in it. Every role has a purpose and allows us to provide useful information succinctly and contextualized. We tee her up to take action - whichever action is right for her, wherever she is.We are values driven at every point - how we work, the way we work and our culture come straight from our core values. Each of us supports this incredible brand and takes part in its continued growth, fulfilling the needs of our audience each and every day.We put you and your personal lives first - we have a generous benefits policy that demonstrates we are listening to what our employees need: Unlimited vacation policy and generous holiday observancesEncouraged time off on your birthday and Skimm'versary A hybrid working model with flexibility for remote workComprehensive insurance plans and commuter benefitsAuto-enrollment into an Empower 401(k) plan starting on your first day and employer contribution available after one year of employment Rewards for every work anniversaryOne month paid sabbatical after your five year Skimm'versary as a full-time employee
We support our parents at all points on their journey:  18 weeks of paid parental leave (adoption, fostering and surrogacy included)Bereavement leave for pregnancy lossPhased return to work schedule availabilityFlexibility based on parental schedulesAccess to family building and fertility benefitsFlexible and broad-scale child support program
Your well being is our priority:  We honor Sacred Time in our workplaceWe have aligned company ""time off"" during the summer to truly allow for us all to break from our work at the same timeWe offer fitness membership reimbursementOne Medical Membership is included with our benefits
We have a vibrant, collaborative, and supportive culture (we celebrate and have fun!): Weekly company updates led by our executive teamEmployee Resource GroupsAnnual employee award ceremony to celebrate individual accomplishmentsClubs and activities designed to meet other employees like book clubs, new hire buddy programs, off-sites, and wellness focused classesNumerous culture events that enable our workforce, in the office and remote, to connect and have fun
Your career and development are a priority:  Annual learning and development stipendLEAD@theSkimm, our leadership development programDEI initiatives to ensure we are the best partners and colleagues to each otherCareer development guidance and planning
And more… Competitive salary and equity packagesThe opportunity to be part of a values-driven, hardworking, and diverse group of people building a media company that makes it easier to live smarter
Our story, Skimm'dWe are a digital media company, dedicated to succinctly giving women the information they need to make confident decisions. We make it easier to live smarter.At our core, we are writers, editors, producers, designers, marketers, engineers, analysts, sellers, creatives, and strategists all working together to achieve this goal.Every day we're breaking down the news, trends, policies, and politics that impact women so that they can navigate their daily lives and futures – from managing their paychecks to casting their ballots – with confidence. We provide our dedicated audience of millions with reliable, non-partisan, information, informing and empowering them while fitting into their daily routines.Since disrupting the media landscape and defining a new category a decade ago, we have become a trusted source for our audience of millions by seamlessly integrating into their existing routines, fundamentally changing the way they consume news and make decisions. Today theSkimm ecosystem includes the Daily Skimm, the Daily Skimm: Weekend, Skimm Money and Skimm Your Life newsletters, B2B marketer's newsletter The SKM Report, ""9 to 5ish with theSkimm"" podcast, theSkimm mobile app. We also house Skimm Studios which creates innovative in-house video and audio content, and our in-house creative agency SKM Lab, which conceptualizes, develops, and produces innovative solutions and content for brands to engage with generations of informed women. Our first book, How to Skimm Your Life debuted at #1 on The New York Times Best Seller list. Through Skimm Impact, our purpose-driven platform, we are proud to support get-out-the-vote efforts with Skimm Your Ballot, which has spurred more than two million voting-related actions across the last four election cycles. We have mobilized hundreds of companies to join our #ShowUsYourLeave movement, which has created transparency and pushed for progress for Paid Family Leave in the U.S. And we're empowering women to take agency of their lives and control of their futures through our State of Women initiative, grounded in a study conducted by The Harris Poll.Come join us!"
Data Engineer,Lynx Analytics,"San Francisco, CA (Remote)",https://www.linkedin.com/jobs/view/3641777500/?eBP=JOB_SEARCH_ORGANIC&refId=v5p1jII4oRcnpACm%2FDi5Wg%3D%3D&trackingId=Ltf4SXFXOq%2B7yLnOWpb2ZQ%3D%3D&trk=flagship3_search_srp_jobs,3641777500,"About the job
            
 
Company OverviewLynx Analytics was founded in 2010 by a group of INSEAD students and professors with a strong research background in graph analytics. Several of our founders since then became professors and faculty directors of analytics centers at leading US universities. Our founding purpose? To apply graph theory to simplify and solve complex, real-world business problems.Our mission has evolved over the years, and we currently offer a range of cutting edge data analytics and AI solutions to help companies transform their operations and optimise their commercial performance. Back then, graph theory was mostly the purview of social networking sites. We wanted to expand this technology and help companies leverage their communities to unlock greater growth.Lynx has offices in Singapore, US, Hong Kong, Hungary, and operations in several other countries such as Canada, Germany, Indonesia. We work with some of the world's largest companies and are constantly looking to expand our knowledge base and geographical footprint. Lynx Analytics' technology is deployed with various Clients internationally and has significant growth potential.We have a diverse and inclusive global team comprising Professors, PhDs, MSc's, and MBAs from Ivy Leagues, INSEAD and NUS with a broad spectrum of experience in start-ups and blue-chip companies (Google, Databricks, ZS, Abbvie, Amgen, Vodafone, Morgan Stanley, Palantir, Katana Graph to name but a few). It is the combination of our industry insight and experience, scalable proprietary technology, and highly qualified people that drives our compelling value proposition.We are looking for ambitious, innovative, empathetic and relentless team players to explore the career opportunities that we offer as we continue to scale our operations.Key ResponsibilitiesA Data Engineer's responsibility is to implement and deploy data analysis pipelines at various clients of Lynx Analytics. This includes participating in the activities below: Understand deeply the business problem that we are trying to solve by our analytical solutionThrough continuous consultations with employees of our client, discover the client's existing data sources that are relevant to the problem we try to solve. This includes discussions with client IT, data owners, future business users, etc. Working together with the IT teams of the client, define the technical architecture for the analytical solution that we are to deploy for the client. Implement the data ingestion subsystem: this is the system responsible for moving all the necessary data sources to a single location where the actual analysis will happen. Implement the data analysis pipelines. Integrate the results into business UIs developed by Lynx or pre-existing client software systems
Requirements Relevant tertiary qualification, preferably at Masters level or above, in Engineering or another relevant discipline with strong academic resultsStrong programming skillsExperience in GCP, Airflow and SparkSolid experience in Python and SQLGood problem-solving skillsGood communication skills
DESIRABLE Experience in Big DataA minimum of 3 years of experience in Data Science or AnalyticsIndustry experience in working for a big enterprise (like our clients)
What We Offer Opportunity to work on creating innovative, leading-edge data science pipelines using our state of the art, in-house built big graph toolWork closely with the developers of the (big graph) tool you will be building uponBe a member of a very strong team with mathematicians, ex-Googlers, Ivy League professors, MBA alumni and telecommunications industry expertsStartup atmosphere"
Entry Level Data Engineer (Remote),SynergisticIT,"Augusta, GA (Remote)",https://www.linkedin.com/jobs/view/3767593225/?eBP=JOB_SEARCH_ORGANIC&refId=BZF6grSNM9xRxSJxnO7rwQ%3D%3D&trackingId=yKS125PffnV143JuCY%2Bt0Q%3D%3D&trk=flagship3_search_srp_jobs,3767593225,"About the job
            
 
The Job Market is Challenging due to almost 300,000 Tech Layoffs since October 2022 due to which thousands of laid off Techies are competing with existing Jobseekers. Entry level Job seekers struggle to get responses to their applications forget about getting client interviews. As the Saying goes ""when the Going gets tough the Tough get going” Candidates who want to make a tech career they need to differentiate themselves by ensuring they have exceptional skills and technologies to be noticed by clients.Since 2010 Synergisticit has helped Jobseekers differentiate themselves by providing candidates the requisite skills and experience to outperform at interviews and clients. Here at SynergisticIT We just don't focus on getting you a Job we make careers.All Positions are open for all visas and US citizensWe are matchmakers we provide clients with candidates who can perform from day 1 of starting work. In this challenging economy every client wants to save $$$'s and they want the best value for their money. Jobseekers need to self-evaluate if they have the requisite skills to meet client requirements and needs. Clients now post covid can also hire remote workers which increases even more competition for jobseekers.We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like apple, google, PayPal, western union, Client, visa, Walmart labs etc. to name a few.We have an excellent reputation with the clients. Currently, We are looking for entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Machine Learning engineers for full time positions with clients.Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in IT IndustryWe assist in filing for STEM extension and also for H1b and Green card filing to Candidates We also offer optionally Skill and technology enhancement programs for candidates who are either missing skills or are lacking Industry/Client experience with Projects and skills. Candidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. If they are qualified with enough skills and have hands on project work at clients then they should be good to be submitted to clients. Shortlisting and selection is totally based on clients discretion not ours.please check the below links to see success outcomes of our candidateshttps://www.synergisticit.com/candidate-outcomes/We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023https://youtu.be/Rfn8Y0gnfL8?si=p2V4KFv5HukJXTrnhttps://youtu.be/-HkNN1ag6Zk?si=1NRfgsvL_HJMVb6Qhttps://www.youtube.com/watch?v=OAFOhcGy9Z8https://www.youtube.com/watch?v=EmO7NrWHkLMhttps://www.youtube.com/watch?v=NVBU9RYZ6UIhttps://www.youtube.com/watch?v=Yy74yvjatVgFor preparing for interviews please visit https://www.synergisticit.com/interview-questions/We are looking for the right matching candidates for our clientsPlease apply via the job postingREQUIRED SKILLS For Java /Full Stack/Software Programmer Bachelor's degree or Master's degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT Highly motivated, self-learner, and technically inquisitive Experience in programming language Java and understanding of the software development life cycle Project work on the skills Knowledge of Core Java, JavaScript, C++ or software programming Spring boot, Microservices, Docker, Jenkins and REST API's experience Excellent written and verbal communication skills 
For data Science/Machine learning PositionsRequired Skills Bachelor's degree or Master's degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT Project work on the technologies needed Highly motivated, self-learner, and technically inquisitive Experience in programming language Java and understanding of the software development life cycle Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools Excellent written and verbal communication skills 
Preferred skills: NLP, Text mining, Tableau, PowerBI, TensorFlowIf you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team.No phone calls please. Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates"
GSA Data Engineer,Nike,"Beaverton, OR (Remote)",https://www.linkedin.com/jobs/view/3693334852/?eBP=JOB_SEARCH_ORGANIC&refId=BZF6grSNM9xRxSJxnO7rwQ%3D%3D&trackingId=ZU8L2adcf3cwdTF5CAtDlg%3D%3D&trk=flagship3_search_srp_jobs,3693334852,"About the job
            
 
Work options: RemoteTitle: GSA Data EngineerLocation: Remote, USDuration: 1 Year ContractJob Duties Engineer data solutions in support of Sustainability reporting and analytics initiativesEngage with product owner, analysts, visualization developers, and business partners to understand capability requirements, and to develop and support data solutions based on product backlog priorities 
SkillsPreference will be shown to candidates who can provide a link to their open-source code portfolio (a link to your profile on github.com, bitbucket.com, gitlab.com, or another public VCS is sufficient)Required Technical CompetenciesCandidates should have demonstrated, in a professional capacity, all of the competencies listed for each of the following three subject areas:General Purpose Python Programming Python has been your primary coding language (daily use) for at least 3 yearsYou have authored distributable Python packages (packages that can be built, installed, and distributed using setup tools, pip, and twine)You have a solid understanding of how pip dependency resolution worksYou are proficient in authoring and automating unit and integration tests for Python packages using (minimally) unit tests, pytest, and toxYou are meticulous about code quality, including readability, know your PEP8 guidelines inside and out, and are capable of authoring code which will pass validation by commonly used static analysis tools including mypy and flake8 Database Design and SQLYou are proficient in authoring readable, well-structured, SQL SELECT statements using ISO/ANSI-standard SQLYou have hands-on professional experience in data warehouse design and modeling, including authoring DDL statements. Version Control and CI/CDYou have experience with trunk-based development (feature branching) using git for version control, with fully automated deployments (CI/CD).
Database Design and SQL You are proficient in authoring readable, well-structured, SQL SELECT statements using ISO/ANSI-standard SQLYou have hands-on professional experience in data warehouse design and modeling, including authoring DDL statements. Version Control and CI/CDYou have experience with trunk-based development (feature branching) using git for version control, with fully automated deployments (CI/CD).
Desired Technical Competencies General Purpose Python Programming You have a deep understanding of Python’s standard library and python internals. You understand python memory management, how CPython implements built-in data structures, and which data structures are best suited for different scenariosYou understand and can compare/contrast CPython’s built-in concurrency models, when to use each, and what obstacles might prevent the use of each mechanism
Database Design, SQL, and Object Relational Models You are adept at performance-tuning SQL queries for both OLAP and OLTP databasesYou understand and are prepared to discuss how and when/where to utilize more esoteric and/or modern SQL features such as window functions and common table expressions.You understand and are prepared to discuss the performance implications of columnar vs relational databases.You have hands-on experience in managing database schema migrations (ideally using SQLAlchemy’s ORM + Alembic). 
Version Control and CI/CD  You have experience with trunk-based development (feature branching) using git for version control, with fully automated deployments (CI/CD)
Cloud Infrastructure and Amazon Web Services  You have hands-on experience using boto3 to interact with Amazon Web Services’ resource APIs, particularly Amazon S3 (Simple Storage Service).You have hands-on experience authoring unit and integration tests utilizing local stack to emulate AWS resources.You have hands-on experience using HashiCorp Terraform to manage cloud infrastructure.You have hands-on experience developing serverless ASGI applications using AWS lambda and AWS API Gateway 
Web API Server and Client Development You have experience planning and executing the design and development of web APIs using a modern Python ASGI framework (preferably FastAPI).You have authored, validated, and maintained OpenAPI documents describing your web APIs accurately.You have experience developing and testing Python web API client libraries based on an OpenAPI document.
Distributed Computing and Apache Spark You have experience using Apache Spark for the ingestion and manipulation of data sets that are too large to process efficiently in memory.You have hands-on experience translating algorithms and procedures designed by topical subject matter experts, having varying levels of engineering experience, into well-designed data pipelines.You have experience configuring and tuning Spark clusters to optimize use of computing resources for varying workloads.You understand and can discuss: when and why to use distributed computing frameworks, such as Apache Spark, versus alternate concurrency models such as asyncio or multiprocessing. 
Database Design, SQL, and Object Relational Models You have experience modeling databases using SQLAlchemy’s ORM framework.You have experience managing database versions and schema migrations using SQLAlchemy with Alembic. 
Required Soft SkillsCandidates should have demonstrated, in a professional capacity, all or most of the following skills: You are proficient in communicating effectively and efficiently within a hybrid remote/in-person team structure:You are meticulous about managing your calendar to accurately reflect your free/busy hoursYou respect and seek to learn digital communications etiquette—including region-specific, industry-specific, and organization-specific etiquetteYou proactively initiate constructive discussions while curating and targeting your communications with respect for your colleagues’ time and schedulesYou are adept at discovering and navigating the complex bureaucratic resources of a large organization."
Remote Work - Need Azure Fabric Data Engineer,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3759325599/?eBP=JOB_SEARCH_ORGANIC&refId=BZF6grSNM9xRxSJxnO7rwQ%3D%3D&trackingId=DIi3mrObnHodIrP7n3JTYg%3D%3D&trk=flagship3_search_srp_jobs,3759325599,"About the job
            
 
The client is placing heavy emphasis on Microsoft and Azure Fabric experienceInformation Management Engineer – Take the identified information management sources and assist migration to the Client Data Lake, Azure Fabric. The engineer will transform information management (de-dup, clean and enrich).Take the identified data sources and bring them into our Data Lake, Azure Fabric. Once the data is in, the engineer will need to be able to transform data (de-dup, clean and enrich the data). We are focused on using a medallion data model for data architecture. The tools we are working with are: Delta Parquet files, python, spark, notebooks, T-SQL, Fabric lakehouse/warehouse, Dataflows Gen2, Data Pipelines, Direct Lake, shortcuts, ADF."
Data Engineer - Remote | WFH,Get It Recruit - Information Technology,"Cleveland, OH (Remote)",https://www.linkedin.com/jobs/view/3765609897/?eBP=JOB_SEARCH_ORGANIC&refId=BZF6grSNM9xRxSJxnO7rwQ%3D%3D&trackingId=bgQf8pFYkXnr8xoOpEZvSQ%3D%3D&trk=flagship3_search_srp_jobs,3765609897,"About the job
            
 
Join an innovative team at a leading company in the engineering and manufacturing industry, where we are committed to leveraging data for continuous improvement and operational excellence. This hybrid position allows you to enjoy the flexibility of remote work on Mondays and Fridays.Why Join UsAt our company, we prioritize your well-being and professional growth. We offer a comprehensive benefits package, including excellent health benefits, a 6% 401(k) match, generous paid vacation and sick time, 12 paid holidays, and more.Role OverviewAs a Data Engineer, you will play a pivotal role in transforming raw data into actionable insights, driving improvements in engineering and manufacturing processes, enhancing product quality, and optimizing operational efficiency. Collaborate with dynamic cross-functional teams to extract, analyze, and visualize data, influencing strategic decisions through your valuable insights.Key ResponsibilitiesData Collection and Integration:Gather, clean, and integrate data from diverse sources, including production databases, sensor data, and external datasets.Data AnalysisConduct exploratory data analysis to identify patterns, trends, and anomalies.Perform statistical analysis and hypothesis testing to derive meaningful insights.Data VisualizationCreate informative and visually appealing dashboards and reports using tools like Tableau, Power BI, or Python libraries.Predictive ModelingDevelop predictive models to forecast manufacturing performance, quality, and operational outcomes.Root Cause AnalysisInvestigate production issues and quality concerns by analyzing data to identify root causes and recommend corrective actions.Performance MonitoringEstablish and maintain key performance indicators (KPIs) to track manufacturing and engineering metrics.Generate regular reports to communicate performance trends to stakeholders.Cross-functional CollaborationCollaborate with engineering, manufacturing, and quality assurance teams to understand their data needs and provide analytical support.Continuous ImprovementSuggest process improvements and optimizations based on data analysis.Data Security And ComplianceEnsure data security and compliance with relevant regulations, such as GDPR or industry-specific standards.Infrastructure And DevelopmentBuild and maintain the infrastructure for collecting, storing, and processing data.Design, develop, and manage data pipelines, ETL processes, and data warehouses.Required ExperienceProven experience as a Sr. Data Analyst and/or Data Engineer in an engineering or manufacturing environment.Proficiency in data analysis tools and programming languages, such as Python, R, SQL, and Excel.Strong data visualization skills with tools like Tableau or Power BI.Excellent knowledge of statistical analysis and machine learning techniques.Ability to communicate complex findings and insights effectively to both technical and non-technical stakeholders.Strong problem-solving skills and attention to detail.Familiarity with manufacturing processes and quality control is a plus.Experience with enterprise ERPs (SAP S4HANA, Oracle EBS), CRMs (Salesforce, Hubspot), and IoT.Proficient in working with cloud computing platforms including AWS, GCP, and Azure.Expertise in configuring and optimizing cloud-based data solutions for scalability, reliability, and performance.Bachelor's Degree in Data Science, Statistics, Engineering, or Computer Science Required.Apply Now and be part of a team shaping the future of data-driven innovation in engineering and manufacturing!Employment Type: Full-Time"
Data Engineer - Remote | WFH,Get It Recruit - Information Technology,"Cedar Park, TX (Remote)",https://www.linkedin.com/jobs/view/3775426204/?eBP=JOB_SEARCH_ORGANIC&refId=BZF6grSNM9xRxSJxnO7rwQ%3D%3D&trackingId=Mqbpkgdo7IaYIiFo5pXh2Q%3D%3D&trk=flagship3_search_srp_jobs,3775426204,"About the job
            
 
We are a dynamic and innovative company looking for a talented individual to join our team as a Data Engineer. In this role, you'll play a crucial part in supporting our Data and Process Governance Policies and Standards, contributing to the optimization of workflows and data processing to deliver impactful insights to the shop floor. If you're passionate about data engineering, visualization, and maintaining a technical foundation, we'd love to hear from you.ResponsibilitiesCollaborate with cross-functional teams to implement and support Data and Process Governance Policies and Standards.Foster a strong business insight mindset to enable data visualizations that enhance decision-making.Maintain the technical foundation of data visualizations and optimize workflows through knowledge of industry-standard applications and database theory.Drive the interconnectivity between industrial automation data, MIS, and the data warehouse.Implement an end-to-end vision for the flow of production field data through the organization.Merge new systems or methods with existing data structures.Partner with Supply Chain to define, document, implement, and maintain business processes and data workflows.Develop and implement data visualizations and reporting solutions based on user needs and feedback.Collaborate with engineers and developers to create insightful data reports for business.Work closely with the manager, regional automation, or process engineers, depending on the site.RequirementsProven experience managing complex projects and collaborating with cross-functional teams.Strong knowledge of data governance concepts and implementation.Expertise in data access management, change management, and data historian/time series data.Proficiency in SQL, MSSQL, TSQL, and data modeling for maintaining data integrity between multiple schemas.Experience with ETL processes, with knowledge of Denodo/data virtualization being preferred.Proficiency in Python and query optimization.Hands-on experience working with big data sets.Familiarity with Tableau Server/Desktop/Prep setup and dashboard building.Strong data cleaning and wrangling skills.Initiative-driven, self-motivated, results-oriented, and able to work independently.Effective communication skills, both written and verbal, as this is a client-facing role.EducationBachelors would be a plus but not required; associates with relevant experience are welcome.Join our team and contribute to transforming the way we utilize data! This is a remote position, offering a collaborative and innovative work environment. We look forward to receiving your application.Employment Type: Full-Time"
Data Engineer,Garner Health,"Dallas, TX (Remote)",https://www.linkedin.com/jobs/view/3779646857/?eBP=JOB_SEARCH_ORGANIC&refId=BZF6grSNM9xRxSJxnO7rwQ%3D%3D&trackingId=37A9YonauSoQPycNw4qZ6A%3D%3D&trk=flagship3_search_srp_jobs,3779646857,"About the job
            
 
Garner's mission is to transform the healthcare economy, delivering high quality and affordable care for all. By helping employers restructure their healthcare benefit to provide clear incentives and data-driven insights, we direct employees to higher quality and lower cost healthcare providers. The result is that patients get better health outcomes while doctors are rewarded for practicing well, not performing more procedures. We are backed by top-tier venture capital firms, are growing rapidly and looking to expand our team.We are looking for a Data Engineer to support our technical teams by ensuring ease of access to data within our organization. The ideal candidate for this role will have strong technical skills, including Python, SQL, and AWS as well as a desire to be a hands-on contributor to building out a data platform from the ground up.Main Responsibilities: Build the data pipelines that power our businessCollaborate across disciplines to high-quality datasetsProtect our users' privacy and security through best practicesSupport data pipelines in production
Our Tools:Python, AWS, Snowflake, dbt, Terraform, PostgresThe ideal candidate has: 2+ years of experience building data pipelines in a fast-paced environmentStrong Python knowledgeExperience with Big Data technologies such as Snowflake, RedShift, BigQuery, or DataBricksAbility to think in principles and frameworks to understand and decompose abstract problemsAn aptitude to learn new technologies and tools quickly
Why You Should Join Our Team: You are mission-driven and want to work at a company that can change the healthcare systemYou want to be on a small, fast-paced team that nimbly moves to meet new challengesYou love ideating on new features and working with data to find new insightsYou're excited about researching and working with the latest tools and technologies
The target salary range for this position is $100,000 - $145,000. Individual compensation for this role will depend on a variety of factors including qualifications, skills and applicable laws. In addition to base compensation this role is eligible to participate in our equity incentive and competitive benefits plans.Garner Health is proud to be an Equal Employment Opportunity employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, reproductive health decisions, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, genetic information, political views or activity, or other applicable legally protected characteristics.Garner Health is committed to providing accommodations for qualified individuals with disabilities in our recruiting process. If you need assistance or an accommodation due to a disability, you may contact us at talent@getgarner.com."
Junior Data Backend Engineer (Clickstream Analytics) (Remote - Ireland),Yelp,United States (Remote),https://www.linkedin.com/jobs/view/3773569103/?eBP=JOB_SEARCH_ORGANIC&refId=BZF6grSNM9xRxSJxnO7rwQ%3D%3D&trackingId=BfYKjlFyP2Or2Z2lh4xcpA%3D%3D&trk=flagship3_search_srp_jobs,3773569103,"About the job
            
 
Interested in exploring data and finding innovative ways to collect it, curate it, and make it easily accessible for others to use? How about doing that at an enormous scale? Yelp’s datasets contain billions of interactions between users and local businesses around the globe. If working with our vast amount of data sounds exciting, come and join us!The Clickstream Analytics team is responsible for creating, managing, governing, and monitoring some key user journey metrics to support our product teams and also support offline analysis that aids in making long term company decisions. We also build tools that help us to better understand our data quality, and we strive to uncover data issues before they impact our consumers.As a Data Backend Engineer on this team, you will be responsible for building elegant and scalable data products that serve critical, up-to-date, structured information to support different types of analytics within Yelp. As a core contributor to our growing data modeling and data warehousing engineering efforts, you will help design and own mission-critical data flow pipelines and datastores to enable decisions including effective A/B testing and company investments.Yelp engineering culture is driven by our values: we’re a cooperative team that values individual authenticity and encourages creative solutions to problems. All new engineers deploy working code their first week, and we strive to broaden individual impact with support from managers, mentors, and teams. At the end of the day, we’re all about helping our users, growing as engineers, and having fun in a collaborative environment. This opportunity requires you to be located in the Republic of Ireland. We’d love to have you apply, even if you don’t feel you meet every single requirement in this posting. At Yelp, we’re looking for great people, not just those who simply check off all the boxes.  Build systems that can effectively store and crunch terabytes of data.Design and develop data models for efficient data storage, retrieval, and reporting.Create and maintain conceptual, logical, and physical data models using industry-standard modeling tools.Collaborate with cross-functional teams, including engineers, data analysts, business analysts, and data scientists, to understand data requirements and translate them into effective data models.Participate in data integration efforts, including ETL processes and data migration.Stay up-to-date with industry best practices, emerging technologies, and trends related to data warehousing.Support on-call rotations as needed to operate the team.
  Understanding of high performing and scalable data systems.Experience in building and orchestrating ETL pipelines.Experience with Data Lake or Data Warehouse landscape.A hunger for tracking down root causes and fixing them in systematic ways.Ability to communicate effectively to technical and non-technical cohorts alike.Exposure to some of the following technologies: Python, AWS Redshift, AWS Athena / Apache Presto, Big Data technologies (e.g S3, Hadoop, Hive, Spark, Flink, Kafka etc), DBT."
Senior AI/ML Engineer,Kraken Digital Asset Exchange,NAMER (Remote),https://www.linkedin.com/jobs/view/3756652785/?eBP=CwEAAAGMRKedbWogNFXJKVPCDb-0IY07Lxv92BSQhbZ0hDZ6xbQvfv_EBBS9PwQq3Krnd9G00sisyX86ESnABVQxAG9mNW35tzyVtqFtIaQ8Nrxz4TmVD44tP8R9eWsoVf-UlZw2y9FPAq2pYcfMUFnbbpnmX0gxLrsbDOPMYYx0SVHy2cxrnQiK16XEK5z5lKTaaMuajd_FGCIohtlq4mpZpwtkjmTM43_2rw633DHv1hGMPio00F6iYysFSTtmFl-HaCmaurd1-1lbmhz4iX03drji5ROzzHUs4o1PfymqkoMPv0Fftd9uo16oMJVzCiWwhIli1Nc4kmPz8qvgrVpdcT9Tt3rEWsqrDJNAupueYRBdb3m29j6VlRLDTm515RbYkYsG6G-6ifNTyWEr&refId=BZF6grSNM9xRxSJxnO7rwQ%3D%3D&trackingId=nycp5YH57o1XlhRRqlztgg%3D%3D&trk=flagship3_search_srp_jobs,3756652785,"About the job
            
 
Building the Future of Crypto Our Krakenites are a world-class team with crypto conviction, united by our desire to discover and unlock the potential of crypto and blockchain technology.What makes us different?Kraken is a mission-focused company rooted in crypto values. As a Krakenite, you’ll join us on our mission to accelerate the global adoption of crypto, so that everyone can achieve financial freedom and inclusion. For over a decade, Kraken’s focus on our mission and crypto ethos has attracted many of the most talented crypto experts in the world.Before you apply, please read the Kraken Culture page to learn more about our internal culture, values, and mission.As a fully remote company, we have Krakenites in 60+ countries who speak over 50 languages. Krakenites are industry pioneers who develop premium crypto products for experienced traders, institutions, and newcomers to the space. Kraken is committed to industry-leading security, crypto education, and world-class client support through our products like Kraken Pro, Kraken NFT, and Kraken Futures.Become a Krakenite and build the future of crypto!Proof of workThe teamKraken is looking for an experienced Machine Learning engineer to join our AI/ML Team in the centralized Data organization. In this role you will be applying cutting edge AI/ML technology to solving the most complex and exciting problems in the quickly growing and evolving crypto industry. We are looking for an extremely strong communicator and team-player, who is able to break down large complex problems into smaller more manageable problems-to-solve. You will take initiative to identify business problems, explore different ways to resolve issues, and systematically find the most efficient and effective way to deliver business impact.The Opportunity Design, implement and deploy Machine Learning solutions to solve complex problems and deliver real business value ie. revenue, engagement and customer satisfaction.Collaborate with data scientists, software engineers and business partners to identify AI/ML opportunities for improving operation scalability and efficiency.Develop production-grade ML models to power personalized customer experience, content recommendation, fraud detection/prevention and more.Monitor and improve model performance via data enhancement, feature engineering, experimentation and online/offline evaluation.Stay up-to-date with the latest in machine learning and artificial intelligence, and influence AI/ML for the Crypto industry.Mentor junior engineers, fostering a culture of continuous learning and improvement.
Skills You Should HODL Expertise in building, deploying, measuring, and maintaining machine learning models to address real-world problems.Thorough understanding of software development lifecycle, DevOps (build, continuous integration, deployment tools) and best practices.Strong programming skills in Python, Scala, Go, Rust or other languages.Excellent written and verbal communication skills and interpersonal skills.Experience with MLOps platforms, such as Kubeflow or MLFlow.Experience with ML frameworks, such as scikit-learn, Tensorflow, PyTorch.Experience with Big Data tools – Spark, S3, Athena/Trino.Experience with GenAI tools, such as Langchain, LlamaIndex, and open source Vector DBs.Advanced degree in Computer Science, Machine Learning or related field.A minimum of 5 years experience in AI/ML engineering, with a track record of handling increasingly complex projects.
Location Tagging: #US #EUKraken is powered by people from around the world and we celebrate all Krakenites for their diverse talents, backgrounds, contributions and unique perspectives. We hire strictly based on merit, meaning we seek out the candidates with the right abilities, knowledge, and skills considered the most suitable for the job. We encourage you to apply for roles where you don't fully meet the listed requirements, especially if you're passionate or knowledgable about crypto!As an equal opportunity employer, we don’t tolerate discrimination or harassment of any kind. Whether that’s based on race, ethnicity, age, gender identity, citizenship, religion, sexual orientation, disability, pregnancy, veteran status or any other protected characteristic as outlined by federal, state or local laws.Stay in the knowFollow us on TwitterLearn on the Kraken BlogConnect on LinkedIn"
Entry Level Data Scientist/Engineer - Remote,SynergisticIT,"Tampa, FL (Remote)",https://www.linkedin.com/jobs/view/3767597086/?eBP=JOB_SEARCH_ORGANIC&refId=BZF6grSNM9xRxSJxnO7rwQ%3D%3D&trackingId=OA2%2FNF9C1qm43xFqwcFMqw%3D%3D&trk=flagship3_search_srp_jobs,3767597086,"About the job
            
 
The Job Market is Challenging due to almost 300,000 Tech Layoffs since October 2022 due to which thousands of laid off Techies are competing with existing Jobseekers. Entry level Job seekers struggle to get responses to their applications forget about getting client interviews. As the Saying goes “when the Going gets tough the Tough get going” Candidates who want to make a tech career they need to differentiate themselves by ensuring they have exceptional skills and technologies to be noticed by clients.Since 2010 Synergisticit has helped Jobseekers differentiate themselves by providing candidates the requisite skills and experience to outperform at interviews and clients. Here at SynergisticIT We just don’t focus on getting you a Job we make careers.All Positions are open for all visas and US citizensWe are matchmakers we provide clients with candidates who can perform from day 1 of starting work. In this challenging economy every client wants to save $$$’s and they want the best value for their money. Jobseekers need to self-evaluate if they have the requisite skills to meet client requirements and needs. Clients now post covid can also hire remote workers which increases even more competition for jobseekers.We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000’s of candidates get jobs at technology clients like apple, google, Paypal, western union, bank of america, visa, walmart labs etc to name a few.We have an excellent reputation with the clients. Currently, We are looking for entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Machine Learning engineers for full time positions with clients.Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in IT IndustryWe assist in filing for STEM extension and also for H1b and Green card filing to Candidates We also offer optionally Skill and technology enhancement programs for candidates who are either missing skills or are lacking Industry/Client experience with Projects and skills. Candidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. If they are qualified with enough skills and have hands on project work at clients then they should be good to be submitted to clients. Shortlisting and selection is totally based on clients discretion not ours.Please check the below links to see success outcomes of our candidateshttps://www.synergisticit.com/candidate-outcomes/We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023Oracle CloudWorld Event (OCW) Las Vegas 2023/ 2022 | SynergisticIT - YouTubehttps://youtu.be/-HkNN1ag6Zkhttps://youtu.be/Rfn8Y0gnfL8https://www.youtube.com/watch?v=OAFOhcGy9Z8https://www.youtube.com/watch?v=EmO7NrWHkLMhttps://www.youtube.com/watch?v=NVBU9RYZ6UIhttps://www.youtube.com/watch?v=Yy74yvjatVgFor preparing for interviews please visit https://www.synergisticit.com/interview-questions/We are looking for the right matching candidates for our clientsPlease apply via the job postingREQUIRED SKILLS For Java /Full Stack/Software Programmer Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, ITHighly motivated, self-learner, and technically inquisitiveExperience in programming language Java and understanding of the software development life cycleProject work on the skillsKnowledge of Core Java , javascript , C++ or software programmingSpring boot, Microservices, Docker, Jenkins and REST API's experienceExcellent written and verbal communication skills
For data Science/Machine learning PositionsRequired Skills Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, ITProject work on the technologies neededHighly motivated, self-learner, and technically inquisitiveExperience in programming language Java and understanding of the software development life cycleKnowledge of Statistics, SAS, Python, Computer Vision, data visualization toolsExcellent written and verbal communication skills
Preferred skills: NLP, Text mining, Tableau, PowerBI, TensorflowIf you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team.No phone calls please. Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates"
"Data Engineer Seattle, WA (remote)","Conch Technologies, Inc",United States (Remote),https://www.linkedin.com/jobs/view/3765052211/?eBP=JOB_SEARCH_ORGANIC&refId=BZF6grSNM9xRxSJxnO7rwQ%3D%3D&trackingId=5q2wQxw4ZWsopgMAJn%2Bm7A%3D%3D&trk=flagship3_search_srp_jobs,3765052211,"About the job
            
 
Hi,Greetings from Conch Technologies IncData Engineering Lead – With Databricks and Unity CatalogLocation – Seattle, WA (remote)Duration – 12 monthsNeed: Design and implement data architecture and strategy, leveraging technologies such as Databricks, Unity Catalog, Privacera, and Collibra.As a Data Engineering Lead, you will play a pivotal role in defining and executing technical deliverables while assessing complexities and levels of effort. You'll engage with both vendor and internal teams to resolve technical challenges and ensure seamless collaboration. Your leadership will extend to planning and prioritization with the SCRUM master, facilitating effective coordination between onsite and offshore teams, and driving feature prioritization through collaboration with product managers.Key Responsibilities  Lead the definition and breakdown of technical deliverables, assessing complexities and levels of effort. Collaborate with vendor and internal teams to address technical issues. Drive planning and prioritization in conjunction with the SCRUM master. Coordinate with onsite and offshore teams for timely project delivery. Collaborate with product managers to prioritize features, communicate with stakeholders, and clarify requirements. Collect and manage user-reported issues, ensuring prompt resolution and communication. Define comprehensive test plans, oversee their execution, and document results.
--With Regards,Nagesh GMobile: 408-381-5645Desk: 901-313-3066Email: nagesh@conchtech.com Web: www.conchtech.com"
Data engineer with Python,FinTech LLC,United States (Remote),https://www.linkedin.com/jobs/view/3726513634/?eBP=JOB_SEARCH_ORGANIC&refId=BZF6grSNM9xRxSJxnO7rwQ%3D%3D&trackingId=JGprZ6EUZsqUx8MBRUP3ng%3D%3D&trk=flagship3_search_srp_jobs,3726513634,"About the job
            
 
About Client:The client provides information technology (IT) services, including business outsourcing, infrastructure technology, and application services. The application service offered by the company includes application development, maintenance, and support. The markets served by the company are financial services and insurance, healthcare, manufacturing, government, transportation, communications, and consumer and retail industries.Rate Range: $65-$70/hr On C2C without benefitsSalary Range: $125K - $130K with benefits Job Description: Full stack data engineer, working with any tech stack and data.Python Must.Django, bash, Pandas.
Responsibilities:  Analysing and translating business needs into long-term solution data models.Evaluating existing data systems.Experience with SQL Server is requiredExperience with MongoDB, Teradata, coding capabilities is highly preferred Working with the development team to create conceptual data models and data flows.Developing best practices for data coding to ensure consistency within the system.Reviewing modifications of existing systems for cross-compatibility.
Required Skill SetS:  PythonBashDjangoMongoDBTeradata
 About ApTask:Join ApTask, a global leader in workforce solutions and talent acquisition services, as we shape the future of work. We offer a comprehensive suite of offerings, including staffing and recruitment services, managed services, IT consulting, and project management, providing unparalleled opportunities for professional growth and development. As a member of our dynamic team, you'll have the chance to connect businesses with top-tier professionals, optimize workforce performance, and drive success for our clients across diverse industries. If you are passionate about excellence, collaboration, and innovation, and aspire to make a meaningful impact in the world of work, come join us at ApTask and be a part of our mission to empower organizations to thrive.Applicants may be required to attend interviews in person or by video conference. In addition, candidates may be required to present their current state or government issued ID during each interview.Candidate Data Collection Disclaimer:At ApTask, we prioritize safeguarding your privacy. As part of our recruitment process, certain Personally Identifiable Information (PII) may be requested by our clients for verification and application purposes. Rest assured, we strictly adhere to confidentiality standards and comply with all relevant data protection laws. Please note that we only collect the necessary information as specified by each client and do not request sensitive details during the initial stages of recruitment.If you have any concerns or queries about your personal information, please feel free to contact ourrecruitment team at businessexcellence@aptask.com ."
Data Engineer (Remote),SPAR Information Systems LLC,United States (Remote),https://www.linkedin.com/jobs/view/3647808248/?eBP=JOB_SEARCH_ORGANIC&refId=BZF6grSNM9xRxSJxnO7rwQ%3D%3D&trackingId=9zBjptI0ZP715pDb0%2FU87g%3D%3D&trk=flagship3_search_srp_jobs,3647808248,"About the job
            
 
Role: Data Engineer Location: 100% RemoteDuration: Full TimeRequirements4+ years' experience developing data centric applicationsKnowledge on Scala, DataBricks4+ years' experience creating data transformation and validation logicStrong understanding of streaming and batch data processing best practices.Advanced knowledge of data formats (JSON, XML, Parquet, etc.) and data modeling practices.Data modelling experience shaping and transforming data into third normal form and dimensional models.Advanced understanding of best practices for structuring and organizing Data Lake file systems for large volumes of data.SkillsNumber of years' experienceRating out of 5Developing data centric appsScala and DatabricksCreating data transformation and validation logicData formats (JSON, XML, Parquet)Data ModelingData Lake file systemsThanks & Regards,Arvind Kumar BindPH Number:- 469-750-0607Email : Arvind.B@sparinfosys.com"
Data Science Engineer - Freelance,Fantasy,"San Francisco, CA (Remote)",https://www.linkedin.com/jobs/view/3755004059/?eBP=JOB_SEARCH_ORGANIC&refId=BZF6grSNM9xRxSJxnO7rwQ%3D%3D&trackingId=0gDvkra8xeGgi5EmtHhUOw%3D%3D&trk=flagship3_search_srp_jobs,3755004059,"About the job
            
 
This role may sit anywhere as long as the individual is willing to work North America hours.Realize the unimaginedFantasy is a digital product and innovation company. We realize the unimagined for the world’s most ambitious organizations. Founded in 1999, our global reputation is based on our teams’ unique ability to think beyond the ordinary and launch ground-breaking products and services. We believe that our phenomenal team is the core reason for our success, our growth, and our thriving culture.The RoleAs a Data Science Engineer, your primary responsibility will be to build out exciting new AI-driven tools that our clients will love and use every day. Collaborating with the best design and research teams in the world, and world class client teams, your work will not only drive the future of Fantasy, but influence the future of the industry. Your work will frequently be on the bleeding edge of AI, incorporating the latest innovations in LLMs, machine learning, and data science.Responsibilities Prototype and build cutting edge, proof-of-concept AI productsCollaborate with product managers, designers, researchers, and executives to create successful and compelling solutionsWork with a large and globally distributed teamSolve challenging cutting-edge problems, using state-of-the-art techniques, services, and best practicesEvaluate and integrate new machine learning frameworks and tools, ensuring that Fantasy is always at the forefront of ML, LLMs, and Generative AIActively contribute to continuous improvement of AI at Fantasy, through educational presentations and thought partnership with internal teams
Experience / Key Personal Qualities Bachelor’s or Master’s degree in Data Science, Computer Science, Statistics, or a related fieldFluent in Python, C++, and Java3+ years of experience designing and building machine learning platforms and servicesDemonstrable expertise in LLMs, including fine-tuning and pre-trainingExperience with LLM specific packages and services (e.g. LangChain, vector databases)Experience with AI frameworks and libraries (e.g., TensorFlow, PyTorch, scikit-learn)Strong communication skills to convey complex concepts to non-technical stakeholdersFront end experience a plusData harvesting / scraping experience a plusExperience in design-related fields or understanding of design processes is a plusAbility to work collaboratively in a cross-functional team environment
Fantasy EOEFantasy is an Equal Opportunity Employer. Since 1999, diversity has been vital to our success and ability to create products and services used and loved by millions of people all over the world. We are committed to continually fostering a diverse, equitable, and inclusive workplace."
Data Engineer II - Remote,Net Health,"Alpharetta, GA (Remote)",https://www.linkedin.com/jobs/view/3779194501/?eBP=JOB_SEARCH_ORGANIC&refId=BZF6grSNM9xRxSJxnO7rwQ%3D%3D&trackingId=54h1iE%2FXjs%2FL324FKGMsbA%3D%3D&trk=flagship3_search_srp_jobs,3779194501,"About the job
            
 
Job DetailsDescriptionAbout Net HealthBelong. Thrive. Make a Difference.Are you looking for a meaningful and satisfying career where you have endless opportunities to grow and be financially rewarded? Net Health may be the perfect place for you.A high-growth and profitable company, we help caregivers harness data for human health. We also honor and respect the needs of our Net Health family and staff, which is why we offer a work-from-anywhere environment and unlimited PTO. Our welcoming and collaborative culture paired with progressive benefits makes Net Health the ultimate career home!As a leading-edge SaaS company in healthcare, we deliver solutions that help patients get better, faster, and live more fulfilling lives. Our software and predictive analytics cover the continuum of care, from hospital-to-home, across various medical specialties. Come join us and start the next chapter of your exciting career while helping others to live better lives.World-Class Benefits That Reflect Our World-Class Culture.Click Here to Learn More!:#WorkFromAnywhere #UnlimitedPTO #ComprehensiveBenefitsPackage #EmployeeResourceGroups #CasualDressCode #PrioritizedEmployeeWellness #DiversityAndInclusion #AVoice #NewHireSupport #CareerDevelopment #EducationalAssistance #EmployeeReferralBonus #ProgressiveParentalLeaveJob OverviewDesigns, models, documents, and guides the logical and conceptual relationship of data and database changes for complex applications. Analyzes needs and requirements of existing and proposed systems, and develops technical, structural, and organizational specifications. May also develop, implement, and maintain data systems to meet designs, models and specifications. This person will analyze client production issues and create SQL statements to fix the issue.Responsibilities And Duties Will work with development wen new features or changes happen to the schemaCreate and maintain internal procedures and documentation around reusable SQL scripts or new additions to the data schema due to newly deployed features and functionalityAssist support with triaging complex client issuesProvide ad-hoc reportsAnalyze data to spot anomalies, trend and correlate similar data setsAnalyze large, complex datasets to drive actionable insights and recommendationsEvaluate failures, defects, systemic problems and hardwareConfer with clients to identify problems, replicate the problems, and troubleshoot for root causeParticipate in Agile/Scrum process to refine, prioritize, and build solutions to meet customer needsCreate and maintain internal procedures and documentation around reusable SQL scripts or new additions to the data schema due to newly deployed features and functionalityPerform other related duties as required and assigned
Qualifications Bachelor’s degree or equivalent experience2-4 years of relevant experience
Required Software Experience MS Excel or Google Spreadsheets2-4 years SQL (both DML and DDL)Exposure to 1-2 relational Database Systems (Postgres, Oracle, SQl Server, Snowflake, RedShift, BigQuery etc.)2-4 years Data Visualization, Analysis, or reporting1-2 years with at least 1 scripting language (Javascript, Python, PHP, Bash, Powershell)0-2 years building and maintaining an enterprise Data Warehouse
Note: This job description is not intended to be all-inclusive. Employee may perform other related duties as requested to meet the ongoing needs of the organization.Colorado Pay Law: If you are a Colorado resident and this role is available in Colorado or remote, you may be eligible to receive additional information about the compensation and benefits for this role, which we will provide upon request. Please send an email to Recruiting@NetHealth.comIf you are a CA, CT, CO, IL, MD, NV, RI, WA or NY City resident and this role is available in one of those locales or remote, you may be eligible to receive additional information about the compensation and benefits for this role, which we will provide upon request. Please send an email to Recruiting@NetHealth.com"
Data Engineer,uConnect,"Cambridge, MA (Remote)",https://www.linkedin.com/jobs/view/3726901108/?eBP=JOB_SEARCH_ORGANIC&refId=BZF6grSNM9xRxSJxnO7rwQ%3D%3D&trackingId=DzdGPAZtbH6gbeR5QNYC8A%3D%3D&trk=flagship3_search_srp_jobs,3726901108,"About the job
            
 
uConnect is on a mission to help more people realize their potential by improving access to career resources and services - early in their lives and throughout their careers. Initially focused on the post-secondary higher education market, uConnect's All-In-One Virtual Career Center is used by schools like UCLA, MIT, and Baton Rouge Community College to radically increase student and alumni engagement with career services, and integrate career planning into the student experience, before, during and after college.We are committed to lowering barriers to opportunity for students and recent graduates from all backgrounds. Our customers are some of the most dedicated and capable career services professionals in the world and our software helps them be even more successful in their mission of helping students.We have created a working environment that is fun and collaborative and puts people first. We have a physical office in Cambridge, MA but operate as a remote-first organization, encouraging mobility for all employees, and to work from their chosen environment. uConnect is backed by leading education technology investors including Growth Street Partners, Strada Education and LearnLaunch.Summary:uConnect is dedicated to unleashing the full potential of data to drive innovation and impact in our industry. We are currently seeking a skilled Data Engineer to join our dynamic team and lead our data strategy to new heights. As a Data Engineer at uConnect, you'll be at the forefront, demonstrating the ROI of our products to both existing and prospective customers. You'll spearhead initiatives related to analytics tooling, guide scoping conversations, and contribute significantly to data programming. Your role is pivotal in ensuring seamless data flow between diverse teams and tools while upholding regulatory and ethical compliance in our data practices.Why We Need You:Data is at the core of our business strategy, and we require a dedicated owner. If you resonate with any of the following profiles, we want to hear from you: Experienced data engineer or analyst with programming skillsA startup enthusiast or someone seeking a cross-functional and self-directed roleSenior software or DevOps engineer with a passion for data engineeringAdvanced data analyst with a knack for programming
Primary Responsibilities:In this role, you will have a direct impact on our data-driven journey by: Discussing business objectives with stakeholdersCollaborating with product managers to collect and prioritize data collection needsBuilding data pipelines and an event-driven analytics modelCreating data visualizations and dashboardsUnderstanding and adhering to trust and safety constraintsOwning and iterating on our data systems in collaboration with the engineering teamResearching and evaluating potential vendorsValidating project outcomes across teamsEmpowering teams across uConnect to meet their data needsManaging and optimizing student product engagement dataDeveloping a strategic data roadmap aligning long-term plans with day-to-day decisions
Secondary Responsibilities: Conducting code reviews for other engineersResearching tools, techniques, and regulatory issuesWriting internal documentation about data availability, data modeling, and graph interpretationConsulting API documentation from vendors to evaluate data flow possibilitiesTesting new data workflows between first- and third-party tools and demonstrating proof-of-concept workEnsuring data security and compliance by enforcing access controls, encryption, monitoring, and regulatory adherence
A Typical Day:Your daily routine will be diverse and engaging, involving: Active participation in daily engineering standup meetingsCollaborative discussions with stakeholders and peers to prioritize data needsHands-on programming for data pipelines and code reviewsIn-depth research on data tools, techniques, and regulatory requirementsDocumentation of critical insights on data availability, modeling, and visualizationConsultation of API documentation to evaluate data flow possibilitiesRigorous testing of new data workflows and showcasing proof-of-concept resultsCrafting compelling data visualizations within dashboards
Required Skills, Background & Experiences:Required Skills: Proven experience in data engineering, including data pipeline development, ETL processes, and data warehousingStrong self-management and team leadership abilitiesProficiency in Python, Javascript, and/or PHP being idealMid-to-advanced level SQL skills in any flavorSound statistical analysis skills for meaningful data interpretationA knack for creating precise data visualizationsStrong communication skillsWorking remotely with a cross-functional team and collaborating with other engineers
Preferred Skills, Background & Experiences: Familiarity with BigQuery or similar data toolsExposure to Google AnalyticsStrong aptitude for self-directed learning and growthExperience in auditing application-level security with product engineersProficiency in code validation using test automationHands-on experience in DevOps/CI pipeline managementFamiliarity with event-driven frameworks like WordPressKnowledge of data collection policies and regulatory constraints
Why You Should Join Us:At uConnect, you will have the opportunity to: Contribute to our mission and make a meaningful impactLead and own data systems from discussions to deliveryGrow personally as an individual contributorShape the future of data collection and utilization in our organization
How to Apply:If you're ready to be part of a team that's transforming the way we use data and meet the requirements outlined above, we invite you to apply now. Help us shape the future of data at uConnect. Together, we'll achieve remarkable results!EEO StatementEqual Opportunity EmployeruConnect is proud to be an equal opportunity employer and is committed to maintaining a diverse and inclusive work environment. All qualified applicants will receive considerations for employment without regard to race, color, religion, sex, age, disability, marital status, familial status, sexual orientation, pregnancy, genetic information, gender identity, gender expression, national origin, ancestry, citizenship status, veteran status, and any other legally protected status under federal, state, or local anti-discrimination laws.View The EEO is the Law poster and its supplement.uConnect participates in E-Verify. View the E-Verify posters here.Disability AccommodationFor individuals with disabilities that need additional assistance at any point in the application and interview process, please contact our Manager of People Operations, Karen Lewis at karen.lewis@gouconnect.com.Annual salary range: $120K - $140K. Commensurate with experience."
Data Engineer -remote,"RIT Solutions, Inc.","Dallas, TX (Remote)",https://www.linkedin.com/jobs/view/3768004842/?eBP=JOB_SEARCH_ORGANIC&refId=BZF6grSNM9xRxSJxnO7rwQ%3D%3D&trackingId=z8s%2FSgpUqsaJA3kCKDt9Cw%3D%3D&trk=flagship3_search_srp_jobs,3768004842,"About the job
            
 
100% remote*CANDIDATES CANNOT LIVE IN - VT, RI, NY, NJ, NH, MI, ME, MA, DE, CO, CT, CA, and AZ*LinkedIn profile requiredPosition SummaryThe Senior Data Engineer for the Azure infrastructure will be responsible for the day to day operations of a large data warehouse, and will work closely with the business, product team, and the technical staff to ensure alignment to goals and objectives. Utilizing experience with Big Data, this position will drive consensus on designs of stable, reliable and effective dynamic ETL pipelines leveraging Azure Synapse Analytics Pipelines.Essential Job Functions Drive consensus on designs of stable, reliable and effective dynamic ETL pipelines leveraging Azure Synapse Analytics Pipelines.Perform root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.Design, implement, and document data load processes from disparate data sources into Azure Synapse Pipelines.Work with Continuous Integration/Delivery using Azure DevOps and Github.Provide data management, monitoring, troubleshooting and support to client successCreate various triggers to automate the pipeline in Azure Synapse Analytics Pipelines.Tune SQL queries in Azure SQL DB, Azure Synapse and solve complex data challenges and deliver insights that help our customers achieve their goals.Self-organize as part of a small-size scrum team and apply data engineering skills.Follows industry best practices and meets company's security and performance and requirements
Knowledge, Skills and Abilities Minimum three (3) years' experience with MS SQL/T-SQLMinimum three (3) years' experience with Azure SQLMinimum three (3) years' experience with Apache Spark (PySpark)Minimum of three (3) years' experience with Azure Data Factory or Azure Synapse building dynamic ETL pipelinesMinimum three (3) years' experience building dynamic Spark notebooks in Azure Synapse Spark or Azure DatabricksMinimum three (3) years' experience with PythonMinimum two (2) years' experience with a public cloud (AWS, Microsoft Azure, Google Cloud)Experience working with parquet, json , delta, avro and csv filesIn-depth understanding of data management (e.g. permissions, recovery, security and monitoring)Experience with Data Warehouse ArchitectureStrong analytic skills related to working with structured, semi-structured and unstructured datasets.Excellent analytical and organization skills requiredAbility to understand user requirementsClient service mindsetExcellent verbal and written communication skillsExcellent problem solving skillsFamiliarity with Agile frameworks a plus
Education and Experience Bachelor's degree in related discipline or combination of equivalent education and experience7-10 years of experience in similar field"
Data Engineer II - Remote,Net Health,"St Augustine, FL (Remote)",https://www.linkedin.com/jobs/view/3779194500/?eBP=JOB_SEARCH_ORGANIC&refId=BZF6grSNM9xRxSJxnO7rwQ%3D%3D&trackingId=7mvoF637iB7XJgwphN3s6Q%3D%3D&trk=flagship3_search_srp_jobs,3779194500,"About the job
            
 
Job DetailsDescriptionAbout Net HealthBelong. Thrive. Make a Difference.Are you looking for a meaningful and satisfying career where you have endless opportunities to grow and be financially rewarded? Net Health may be the perfect place for you.A high-growth and profitable company, we help caregivers harness data for human health. We also honor and respect the needs of our Net Health family and staff, which is why we offer a work-from-anywhere environment and unlimited PTO. Our welcoming and collaborative culture paired with progressive benefits makes Net Health the ultimate career home!As a leading-edge SaaS company in healthcare, we deliver solutions that help patients get better, faster, and live more fulfilling lives. Our software and predictive analytics cover the continuum of care, from hospital-to-home, across various medical specialties. Come join us and start the next chapter of your exciting career while helping others to live better lives.World-Class Benefits That Reflect Our World-Class Culture.Click Here to Learn More!:#WorkFromAnywhere #UnlimitedPTO #ComprehensiveBenefitsPackage #EmployeeResourceGroups #CasualDressCode #PrioritizedEmployeeWellness #DiversityAndInclusion #AVoice #NewHireSupport #CareerDevelopment #EducationalAssistance #EmployeeReferralBonus #ProgressiveParentalLeaveJob OverviewDesigns, models, documents, and guides the logical and conceptual relationship of data and database changes for complex applications. Analyzes needs and requirements of existing and proposed systems, and develops technical, structural, and organizational specifications. May also develop, implement, and maintain data systems to meet designs, models and specifications. This person will analyze client production issues and create SQL statements to fix the issue.Responsibilities And Duties Will work with development wen new features or changes happen to the schemaCreate and maintain internal procedures and documentation around reusable SQL scripts or new additions to the data schema due to newly deployed features and functionalityAssist support with triaging complex client issuesProvide ad-hoc reportsAnalyze data to spot anomalies, trend and correlate similar data setsAnalyze large, complex datasets to drive actionable insights and recommendationsEvaluate failures, defects, systemic problems and hardwareConfer with clients to identify problems, replicate the problems, and troubleshoot for root causeParticipate in Agile/Scrum process to refine, prioritize, and build solutions to meet customer needsCreate and maintain internal procedures and documentation around reusable SQL scripts or new additions to the data schema due to newly deployed features and functionalityPerform other related duties as required and assigned
Qualifications Bachelor’s degree or equivalent experience2-4 years of relevant experience
Required Software Experience MS Excel or Google Spreadsheets2-4 years SQL (both DML and DDL)Exposure to 1-2 relational Database Systems (Postgres, Oracle, SQl Server, Snowflake, RedShift, BigQuery etc.)2-4 years Data Visualization, Analysis, or reporting1-2 years with at least 1 scripting language (Javascript, Python, PHP, Bash, Powershell)0-2 years building and maintaining an enterprise Data Warehouse
Note: This job description is not intended to be all-inclusive. Employee may perform other related duties as requested to meet the ongoing needs of the organization.Colorado Pay Law: If you are a Colorado resident and this role is available in Colorado or remote, you may be eligible to receive additional information about the compensation and benefits for this role, which we will provide upon request. Please send an email to Recruiting@NetHealth.comIf you are a CA, CT, CO, IL, MD, NV, RI, WA or NY City resident and this role is available in one of those locales or remote, you may be eligible to receive additional information about the compensation and benefits for this role, which we will provide upon request. Please send an email to Recruiting@NetHealth.com"
Data Engineer,Analytica,"Washington, DC (Remote)",https://www.linkedin.com/jobs/view/3664083944/?eBP=JOB_SEARCH_ORGANIC&refId=BZF6grSNM9xRxSJxnO7rwQ%3D%3D&trackingId=YkueeE%2B0QJjM5t9AY8QpRA%3D%3D&trk=flagship3_search_srp_jobs,3664083944,"About the job
            
 
Analytica is seeking a remoteData Engineerto support one or more dynamic, long-term federal government enterprise big-data programs. The company works as a trusted advisor to U.S. federal government clients in health, civilian, and national security missions.The ideal candidate will be comfortable as a key member of a multi-disciplinary team working in an Agile environment to producebig data analytics solutions.Analytica has been recognized byInc. Magazineas one of the 250 fastest-growing US small businesses for three consecutive years.We work with U.S. government clients to build data-driven products and cultures that make an impact on our daily lives. Analytica offers competitive compensation with opportunities for bonuses, employer paid health care, unlimited training funds, and a 401k match.Requirements (may include, but not limited to): Collaborate with client stakeholders and technical employees to optimize data collection, storage, and usage to maximize the value of information within the organization.Research, design, build, optimize and maintain efficient and reliable data systems, data pipelines, and models.Align closely with operating user requirements on data science, architecture, governance, infrastructure, and security to apply standards and optimize production environments and practices.Translate business needs into data architecture solutions, designing and implementing in production environments within supported data systems.Implement data orchestration pipelines, data sourcing, cleansing, augmentation, and quality control processes within supported data systems.Deploy applications to production in partnership with business units.Develop, test, and integrate new data features and functionality as defined by the product owners and business teamsWork with a multi-disciplinary team of analysts, data engineers, data scientists, developers, and data consumers in a fast-paced Agile environment
Qualifications: Bachelor’s degree in Computer Science, Engineering, Science, or related field.5+ years of data engineering and data warehousing experience3+ years of experience building cloud-data pipeline solutions for data ingestion, data storage, real-time processing, and analyticsExperience working within Agile development environment, DevOps, and using version control platforms, e.g., GitHub)Have domain knowledge of standard data methodologies (DMBOK, NIST, etc.). Team player and the ability to work effectively within a group as well as self-motivated with minimal supervisionExcellent problem solving, collaboration, and communication skillsAWS verifiable certification is strongly preferred US Citizen with the ability to secure a US Federal Clearance
AboutAnalytica:Analytica is a leading consulting and information technology solutions provider to public sector organizations supporting health, civilian, and national security missions. Founded in 2009 and headquartered in Bethesda, MD, the company is an established8(a) small businessthat hasbeen recognized byInc. Magazineeach of the past three years as one of the 250 fastest-growingcompanies in theU.S. Analytica specializes in providing software and systems engineering, information management, analytics & visualization, agile project management, and management consulting services.The Software Engineering Institute (SEI) appraises the companyatCMMI® Maturity Level 3and is anISO 9001:2008 certifiedprovider.As a federal contractor, Analytica is required to verify that all employees are fully vaccinated against COVID-19. If you receive an offer and are unable to get vaccinated for religious or medical reasons, you may request a reasonable accommodation."
Data Engineer (Machine Learning),Jobot,"Virginia Beach, VA (Remote)",https://www.linkedin.com/jobs/view/3779800037/?eBP=JOB_SEARCH_ORGANIC&refId=BZF6grSNM9xRxSJxnO7rwQ%3D%3D&trackingId=3SfSIiOLXat6d8dAVh31OA%3D%3D&trk=flagship3_search_srp_jobs,3779800037,"About the job
            
 
Want to learn more about this role and Jobot? Click our Jobot logo and follow our LinkedIn page!Job details100% Remote / Direct Hire / Help build platforms that uncover game changing insights from data for a fast-growing SaaS platform company supporting veterinary medicineThis Jobot Job is hosted by Blake WilliamsAre you a fit? Easy Apply now by clicking the ""Easy Apply"" button and sending us your resume.Salary $90,000 - $135,000 per yearA Bit About UsVoted one of INC.’s Fastest Growing Companies in 2022, we are devoted to improving the lives of vets across the nation. Surprisingly, vets are overworked, burned out, and struggling to find balance between their work and personal lives. Often times they are also burdened with extraordinary student debt. We have created a whole new way to practice veterinary medicine that puts vets first to help them thrive in a job they love.Why join us?Our team is growing faster than a bullmastiff and we are looking for more amazing people to join our mission. Are you ready to join the movement? No matter where you live, we’re always looking for talented, nice people that believe in our mission of bringing joy to veterinary medicineJob DetailsWe are looking for a talented and ambitious Data Engineer with a strong background in Machine Learning to join our dynamic team. In this role, you will work closely with our data scientists, analysts, and other stakeholders to design, build, and maintain our data infrastructure. You will be responsible for developing and implementing machine learning models, as well as creating and managing robust data pipelines. This is an exciting opportunity to work on cutting-edge technologies, solve complex problems, and make a significant impact on our business.Job Duties  Design, construct, install, test, and maintain highly scalable data management systems. Collaborate with our data science team to design and develop machine learning models and algorithms. Build high-performance algorithms, predictive models, and prototypes. Create data tools that can provide data scientists with data from our central database. Ensure systems meet business requirements and industry practices. Integrate new data management technologies and software engineering tools into existing structures. Create custom software components and analytics applications. Research opportunities for data acquisition and new uses for existing data. Develop data set processes for data modeling, mining, and production. Employ a variety of languages and tools to marry systems together.
Ideal Background  4+ years of experience as a Data Engineer or in a similar role. Experience with data modeling, data warehousing, and building ETL pipelines. Knowledge of programming languages including Python and SQL. Familiarity with PostgreSQL, DBT, Machine Learning, or equivalent. Experience with machine learning frameworks such as TensorFlow or PyTorch. Strong analytical skills, attention to detail, and problem-solving aptitude. Excellent understanding of machine learning algorithms, processes, tools and platforms. Proven ability to drive business results with data-based insights. Bachelor’s degree in Computer Science, Engineering, or a related field.
If you are passionate about leveraging data to drive business results, and you meet the above qualifications, we would love to hear from you. This is your chance to contribute to an exciting, fast-paced organization, and we look forward to seeing what you can achieve as part of our team.Interested in hearing more? Easy Apply now by clicking the ""Easy Apply"" button.Want to learn more about this role and Jobot?Click our Jobot logo and follow our LinkedIn page!"
Data Engineer,Care.com,"Austin, TX (Remote)",https://www.linkedin.com/jobs/view/3765031801/?eBP=JOB_SEARCH_ORGANIC&refId=BZF6grSNM9xRxSJxnO7rwQ%3D%3D&trackingId=19AW6Q36t1p%2FbmD0sCeUtg%3D%3D&trk=flagship3_search_srp_jobs,3765031801,"About the job
            
 
About Care.comCare.com is a consumer tech company with heart. We're on a mission to solve a human challenge we all face: finding great care for the ones we love. We're moms and dads and pet parents. We have parents and grandparents, so we understand that everyone, at some point in their lives, could use a helping hand. Our culture and our products reflect that.Here, entrepreneurs, self-starters, team players, and big thinkers unite behind a common cause. Here, we're applying data analytics, AI, and the latest technologies to solve universal problems and connect people in new ways. If you like having autonomy, if you thrive on collaboration and building new things, and if you're all about using your talent for good, Care.com is the place for you.What Your Days Will be Like:The Data Engineer will be focusing on building out data feeds and tooling from our application platform and enable rapid ingestion into our centralized Data Lake/Data Warehouse. The Data Engineer will work across business areas and application teams at Care.com to rationalize data and design, build and maintain reusable data feeds which and ultimately empower analytics consumption at Care.com.The ideal candidate will have professional experience building data pipelines in a technical environment. S/he will have an understanding of application development, data warehousing, demonstrate strong business judgment, and be able to prioritize in a fast-paced environment.What You'll Be Working On:  Collaborate and partner with application teams to understand data collection/generation and design and partner to build and implement data feeds from our product tech stackDesign, develop, and build code for rapid feeds and ingestion into the Data WarehouseIdentify data sources used for building out data architecture diagrams/modelsEstablish engineering practices and setup frameworks for ""Data as a Service""Collaborate with relevant delivery teams, including infrastructure, operations, site reliability engineering, product development, and others to perform evaluations, POCs, and ultimately implement and operationalize new technology. Solve code level problems quickly and efficiently Participate in demos and code reviews Promote software best approach, standards, and processes Shape development processes to promote a high-quality output while continuing to iterate quickly Incorporate best practices for security, performance, and data privacy into data pipelines
What You'll Need to Succeed: BS or MS in Computer Science or relevant engineering experience5+ years work experience in Data Engineering/data pipelines3+ years SQL experience is a must1+ years Unix/batch scripting preferred1+ years Python experience is a plus1+ years Windows server admin experience is a plusExperience interfacing with business teams and turning requirements and vision into a technical realityMySQL & Vertica Experience a plus/preferredAWS experience is a plusAbility to drive efforts from start to finish as a self-motivatorKnowledge in Data Warehousing is a MUSTProven ability to maintain performance level in a fast-paced agile environment Pragmatic and realistic with solutions
For a list of our Perks + Benefits, click here!Care.com supports diverse families and communities and seeks employees who are just as diverse. As an equal opportunity employer, Care.com recognizes the power of a diverse and inclusive workforce and encourages applications from individuals with varied experiences, perspectives, and backgrounds. Care.com is committed to providing reasonable accommodations for qualified individuals with disabilities. If you need assistance or accommodation, please reach out to talent@care.com.Company Overview:Available in more than 20 countries, Care.com is the world's leading platform for finding and managing high-quality family care. Care.com is designed to meet the evolving needs of today's families and caregivers, offering everything from household tax and payroll services and customized corporate benefits packages covering the care needs of working families, to innovating new ways for caregivers to be paid and obtain professional benefits. Since 2007, families have relied on Care.com's industry-leading products—from child and elder care to pet care and home care. Care.com is an IAC company (NASDAQ: IAC).Salary Range: 110,000 to 135,000. The base salary range above represents the anticipated low and high end of the national salary range for this position. Actual salaries may vary and may be above or below the range based on various factors including but not limited to work location, experience, and performance. The range listed is just one component of Care.com's total compensation package for employees. Other rewards may include annual bonuses and short- and long-term incentives. In addition, Care.com provides a variety of benefits to employees, including health insurance coverage, life, and disability insurance, a generous 401K employer matching program, paid holidays, and paid time off (PTO)."
Senior Data Engineer,Nike,"Beaverton, OR (Remote)",https://www.linkedin.com/jobs/view/3775090168/?eBP=JOB_SEARCH_ORGANIC&refId=BZF6grSNM9xRxSJxnO7rwQ%3D%3D&trackingId=X3R31ZTjozOMVROgQibKmA%3D%3D&trk=flagship3_search_srp_jobs,3775090168,"About the job
            
 
Work options: RemoteTitle: Senior Data EngineerLocation: Remote, Beaverton, ORDuration: 6 Month ContractBecome a Part of the NIKE, Inc. TeamNIKE, Inc. does more than outfit the world's best athletes. It is a place to explore potential, obliterate boundaries and push out the edges of what can be. The company looks for people who can grow, think, dream and create. Its culture thrives by embracing diversity and rewarding imagination. The brand seeks achievers, leaders and visionaries. At Nike, it’sWhat You Will Work OnWe are looking for an experienced Senior Data Engineer to join our data team. You will be responsible for maintaining and improving our production Airflow pipelines, as well as developing new features using Python. This role requires strong Python programming skills, expertise with Airflow, and experience with SageMaker pipelines. Maintain and improve existing Airflow DAGs that move data through our analytics pipelineWork closely with data scientists and analysts to understand requirements for new pipelines and data transformation logicDevelop reusable Python modules and libraries for data transformation and integration Migrate existing Airflow pipelines to SageMaker pipelines utilizing our proprietary ETL libraryImplement new features for our Deployment Optimization platform using PythonMonitor pipeline health, troubleshoot issues, and resolve bugs in a timely mannerFollow best practices for version control, testing, and code reviews
What You Bring 5+ years of experience building and maintaining Airflow pipelinesExpert knowledge of Python, including modules like Pandas, NumPy, and SQLAlchemyExperience developing and deploying SageMaker pipelinesStrong understanding of data structures, algorithms, distributed computing, and data modeling Experience with Git, CI/CD, and automated testing frameworksExcellent communication and collaboration skills"
Data Engineer,Velir,United States (Remote),https://www.linkedin.com/jobs/view/3731818389/?eBP=JOB_SEARCH_ORGANIC&refId=BZF6grSNM9xRxSJxnO7rwQ%3D%3D&trackingId=dmclfVeYz1RpyR4%2BOyODnw%3D%3D&trk=flagship3_search_srp_jobs,3731818389,"About the job
            
 
Data Engineers understand the fundamentals of how to create a robust and efficient infrastructure that makes data accessible and ready for analysis for our clients.Because our clients are mostly US-based organizations, we look for the ability to communicate with professional proficiency in English, verbally and in writing.Functional Skills & Knowledge You are responsible for building the infrastructure to support the storing and movement of data, so that it can be prepared by analytics engineers to eventually be interpreted by analysts. Your job is data accessibility, enabling our clients to utilize data for performance evaluation and optimization. Designs and maintains data pipelines, cloud data warehouse, and storage solutions. Has the working knowledge and skills to ensure best practices in data quality, manage ETL (Extract, Transform, Load) / ELT processes, and establish data architecture.Has a solid understanding of discipline basics, wherein you can competently execute on smaller analytics tasks or product improvements with the guidance of senior engineers and management. You can consistently deliver small features and improvements that have an impact on clients.Provides constructive feedback on peer code or technical design document reviewsWith career development Participates in design and bug fixes under direct supervision. Writes, tests, and documents code as per BDC guidelines, writing and reviewing technical design documentation.Confidently contributes to internal analytics projects, eventually improving our in-house approach by regularly planning technical work with the team.Independently leads small- to medium-sized projects (4-8 analytics weeks) through the entire lifecycle from design to development and deployment.(Over time) Serves as a mentor for less advanced team members and onboarding new engineers onto the team.

Cross-Team Collaboration You are responsible for collaborating with peers and senior ICs in other functional departments—most often data solutions, analytics engineering, and data analytics—to develop and implement data engineering approaches that support our client engagement goals. Effectively collaborates across disciplines, including data analytics and analytics engineering.Collaborates with analytics engineers to design efficient data pipelines.Support other functions by building CI/CD workflows or other supporting components to enable them to deliver their work quickly and efficiently.Approaches problem-solving as a team effort, providing timely and helpful feedback to peers and leads. Works well with other members of the team and are comfortable challenging other people’s opinions and open to having their own opinions challenged, too.Regularly seeks feedback from other team members (other Engineers, Data Solutions Managers, and peers in other functions) and incorporates that feedback in day-to-day work.
Engagement Delivery You are responsible for ensuring that smaller and/or lower complexity data engineering projects are delivered on time, within scope, and within budget. Consistently seeks out, delivers and maintains small projects or surface-specific tasks with the help of more experienced Engineers.Capable of self-directed project delivery and can help the team to prioritize work that maximizes team impact, including avoiding getting stuck on unimportant details.Participates in the definition and delivery of work or projects within the team.Escalates matters when appropriate without getting into rabbit holes or blocked for long time periods. Exercises a good sense of timeboxing tasks and is able to proactively address issues before they become problematic.
Tools & Technologies Programming languages (e.g. SQL, Python)Data Processing (e.g. Apache Spark, dbt)Cloud-based data warehouses (e.g., Snowflake, Google BigQuery)
Technical Skills (Hard Skills)See the latest Data Engineering framework. Data Movement You can import large, assorted datasets from multiple sources into a single storage medium, wherein you understand event-driven architectures and asynchronous pub/sub design patterns.Data Warehousing You have a solid working knowledge of how to effectively collect, organize, and manage large sets of data from multiple sources toward generating insights and supporting decision-making. You can independently provision a warehouse and perform medium complexity administrative tasks (e.g. working with permissions).Programming You have a practical understanding and background in programming, as evidenced by the ability to maintain local development environments, contribute basic features (unit testing, following clear steps for completion), and write code thoughtfully to aid in testing and modularity.Domain Expertise You have a strong foundation of knowledge in domains in which you’re working. You are able to relate how the business works with the goals of the immediate team.
Bonus points for Data Modeling & Transformation You have a solid working knowledge of how to convert, cleanse, and structure data into usable formats/structures to prepare it for analysis. You can effectively transform raw data into analysis-ready models for different use cases.Data Infrastructure You have a working knowledge of how to design, build and maintain efficient systems using architecture components (DWH, VMs, storage buckets, etc.) as an infrastructure-as-code solution (Terraform, Pulumi), networking settings to appropriately secure access, and different storage tiers and life cycles of blob storage solutions.
Essential Skills (Intangible Skills) Organizational Accountability You can manage your own time in consultation with others, effectively delivering individual tasks and assignments.Curiosity & Versatility You are generally comfortable with new contexts and roles and can point to multiple occasions where you've changed approach or tools quickly and efficiently in response to a situation.Collaboration & Partnership You can reliably assist and support the facilitation of meetings and collaborative projects with team members and occasionally engage in discussions with external stakeholders, if needed.Effective Communication You usually communicate effectively, clearly, concisely and in an audience-oriented way, actively listening to others to ensure understanding.
Qualifications (Must Haves) Proven experience as a Data Engineer or related role, with a focus on designing and developing data pipelines.Strong programming skills in languages such as Python, Rust, Scala, or SQL.Deep knowledge of data warehousing and ETL/ELT processes.Hands-on experience with data warehouses like Snowflake, BigQuery, Databricks, or similar.Experience with streaming solutions such as Spark Streaming, Kafka, or Flink is desirable but not required.Familiarity with cloud platforms such as AWS, Azure, or Google Cloud.Familiarity with machine learning operations (MLOps) techniques and platforms is a plus but not required.Strong analytical and problem-solving skills.Excellent communication and collaboration skills
Physical Requirements Frequent sitting at a desk performing work on a computerReasonable accommodations may be made to enable individuals with disabilities to perform the essential functions
CompensationWe believe all team members should be rewarded competitively, using practices that are equitable and transparent. This philosophy ensures we’re able to find, grow, and retain exceptional talent from a variety of backgrounds.The pay range for this role is up to $115,000 if based in the USA and up to $58,000 if based in El Salvador.Please note that compensation packages are finalized after the interview process is concluded. We use a competency-based approach to base pay, which means it is based on the competencies and skills demonstrated for this role.Core Company ValuesVelir is an established mid-sized agency with a top-tier portfolio of clients, ranging from the world’s largest non-profits to Fortune 500 brands. We pride ourselves on our people-first culture and a low-ego workplace that embraces experimentation, collaboration and continuous improvement. We have a fun office environment located in Davis Square (Somerville, MA) and offer competitive pay and excellent benefits. Take the Long View - Ensure the company is built to lastBe Courageous - Make the right decisions even when they aren't the easiest decisionsBe Genuine - Bring honesty and authenticity to all that you doWork with Focus + Passion - Display purpose and pride in your work and never stop learning
As an equal opportunity employer, we are firmly committing to diversity, equity, and inclusion in our hiring efforts. We recognize that we need team members from all backgrounds and experiences to successfully shape a positive employee experience as well as deliver our product and service solutions. To that end, we actively seek candidates who can bring diverse experiences and backgrounds to our team. We know that complex factors and systemic bias can get in the way of us meeting strong candidates, so please don't hesitate to apply even if you're not 100% sure.At this time, Velir does not sponsor candidates and unfortunately cannot accept those on OPT or CPT."
Staff Data Engineer,"Nav Technologies, Inc.","Seattle, WA (Remote)",https://www.linkedin.com/jobs/view/3775330460/?eBP=JOB_SEARCH_ORGANIC&refId=BZF6grSNM9xRxSJxnO7rwQ%3D%3D&trackingId=UW2NAZReFIVZxtnrohpR1w%3D%3D&trk=flagship3_search_srp_jobs,3775330460,"About the job
            
 
Nav is democratizing small business financing. In other words, we give small business owners access and control. Yes, this challenges the norm, but it means working with curious, purpose driven, dedicated, and inquisitively smart people who push themselves, our company and the community to the next level (and every level after that). We are the people behind the tech. And when it's good, we look for better. We don't over think the value we bring nor spend time trying to revamp mantras. We also do not come up with some crafty way to tell you who we are and what we offer. We are Nav! Here, you'll gain a wealth of experience, learn the tricks of the trade, and work with winners. All companies say people are connected to their mission but in our case our mission and our people are one – it is a way of being not just a cause you are committed to. And since 2013, Nav has holistically and organically developed its own ideology because Nav can only be Nav.We are seeking an innovative and passionate Data Engineer to join our team. In this role you will be responsible for building the data and reporting infrastructure to power our systems, working with our team of engineers, product managers and designers; helping us create a better experience for the millions of Nav Small business users. You'll also work with our business intelligence and data science teams to improve the data platform, ensuring that the entire company is able to make better data-driven decisions.YOU WILL: Building cool stuff by maintaining and fulfilling a technical data engineering roadmap and vision. Creating data and reporting infrastructure by building and optimizing production-grade data pipelines through the use of continuous integration. Exploring data sets for insight and understanding that will help drive great product changes and priority. Communicating results and findings to company audiences of varying technical ability in a clear, engaging manner. Breaking down complex technical concepts into digestible tid-bits. Supporting self-service data pipeline management (ETL) and self-directed learning process. Addressing competing explanations in result validity, and use formal reasoning approaches to avoid bias and miscommunication. Collaboration with Engineering and Data Analysts to identify and design ways to solve critical business problems and ensure funnel optimization. Ability to design, develop, and maintain scalable, reusable code. 
WHO YOU ARE: You are a driven data engineer with experience working with big data technologies such as Docker, ECS, S3, Redshift, Kafka, and RDS. You have experience with ETL/ELT and data warehousing using tools such as dbt, Azure Data Factory, Matillion and/or Fivetran You love building data-driven products and have expertise in one or more programming languages (ideally Python). You have strong SQL experience, with expert level skill in Postgres, Snowflake and /or AWS Redshift. You're an Apache Airflow practitioner with experience managing cross-DAG dependencies in Airflow. You know how to implement, advocate, and teach agile practices, and adapt them to your people and circumstances, while working in accordance with data ethics & privacy standards. You can consistently evolve data models & data schema based on business and engineering needs. You've developed and extended design patterns, processes, standards, frameworks and reusable components for use in various data engineering functions and areas. Ability to implement systems for tracking data quality and consistency. You're comfortable working directly with all internal data consumers and are eager to share your work through calibrations and organizational product demos. You are committed to quality! And you're someone who's willing to tackle technical debt and constantly evolving our processes to help nav grow. We'd love it if you have a high level understanding of credit scores, ecommerce, and B2B financing. 
Inclusion at Nav:At Nav, we celebrate what makes our employees unique because the businesses we serve are progressively diverse and distinctly original. Navericks are diverse, side hustlers, immigrants, veterans, queer, and we push generational boundaries. We are college dropouts, PhDs, special needs parents, allies, pet owners and community leaders. Navericks are human. We are committed to upholding a safe, supportive environment where everyone matters. We are committed to making a better future for all of us. We have created a workplace where people of all backgrounds can express their identities authentically. To put it simply, we want you to be proud to be you.Our Compensation Philosophy is simple but powerful:At Nav, we are transparent about our total rewards, including pay, across all levels and roles. We believe great, enduring relationships are grounded in trust and transparency. Compensation shouldn't be a distraction, and employees should understand how pay and career advancement decisions are made. Providing equal pay for equal work is table stakes for being a great place to work. Gender and ethnic inequity should only be something that our children read about in history books. We believe providing Navricks with company ownership, competitive pay, and a range of meaningful benefits is the start of creating a culture where people want to give the best they've got — not because they're simply making money, but because they've fallen in love with our vision, mission, values, and team.During the interview process, your recruiter will be explaining how our rubrics work across all of our total rewards ( base, equity, bonus, perks, and benefit) offerings . The base salary for this role is targeted between $140K - $155K per year. Final offer amount is determined by your proficiencies within this level.Our impact on you:Competitive Pay. Company Ownership. Unlimited Vacation. Benefits Day One. 6 Weeks Paid Parental Leave. Work From Anywhere (yes we were distributed before it was cool). Flexible Work Arrangements. Free Telehealth and Telemental Health For All Employees. Employee Networking and Events. Community Network Groups (women's, PRIDE, culture). Meaningful Perks and Rewards. Learning and Development Opportunities. Pet Insurance.A Naverick's DNA:  We look at the future and say ""why not""; we see possibilities where others see problems or routines. We show the way ahead and are committed to achieving ambitious goals. We practice straight talk and listen generously to each other with empathy. We value different opinions and point of views. We ensure that we connect outside as well as inside to learn from others and inspire each other. We hold ourselves accountable for delivering results. We choose to not to be a victim of circumstance. We make decisions & take responsibility so that we can act & support each other, rather than adopting defensive, and ""finger pointing"" behaviors. As leaders we motivate & engage our teams to undertake beyond what they originally thought possible, by developing our teams & creating the conditions for people to grow and empower themselves through enabling & coaching. 
If you are based in California, we encourage you to read our privacy notice for California residents linked here."
Data Engineer,BLACKBIRD.AI,"New York, United States (Remote)",https://www.linkedin.com/jobs/view/3778950703/?eBP=JOB_SEARCH_ORGANIC&refId=BZF6grSNM9xRxSJxnO7rwQ%3D%3D&trackingId=eeqHV7LUe9I9WIaNbTot1g%3D%3D&trk=flagship3_search_srp_jobs,3778950703,"About the job
            
 
Blackbird.AI helps organizations discover emergent threats and stay one step ahead of real-world harm through our AI-powered Narrative and Risk Intelligence Platform. Our commitment is to prioritize safety and security, providing the tools to identify potential risks and ensure a safer environment proactively. No matter the job or where it’s located, we’re all connected by a shared vision: To lead and enhance the landscape of risk intelligence.Reporting to the Head of Data Engineering, the Data Engineer will optimize data pipelines for our cutting-edge real-time streaming cloud-hosted analytics platform. You will be the key driver of our collection, analysis, and visualization processes and will play a pivotal role in shaping the future of our platform.The Data Engineer will encompass writing ETL processes, designing database systems, and developing tools for query and analytic processing, all focusing on real-time streaming applications. Additionally, you will elevate performance tradeoffs, own the database architecture, and lead build automation, continuous integration, and deployment efforts while ensuring adherence to security requirements.As the Data Engineer, you’ll have the chance to: Write ETL processes to support the ingestion and normalization of various data formats from social media, news, and web sourcesDesign robust database systems and developing tools for query and analytic processing, especially for real-time streaming applicationsConduct performance analysis and empirical studies to evaluate tradeoffs (e.g., cost vs. throughput/latency)Develop, manage, and own the database architecture for our real-time streaming cloud-hosted analytics platformLead build automation, continuous integration, deployment, and performance optimization efforts while ensuring compliance with security requirementsLead design of test suites and inline instrumentation as needed to ensure data correctness
RequirementsWhat you’ll bring: Bachelor’s degree in Computer Science or related fieldProven success in deploying products in the cloud and SaaS model, with expertise in building optimized processing pipelines for streaming analytics applications and cloud-agnostic solutions (e.g., Kubernetes, Docker)Expertise in databases and query optimization, including PostgresSQL, ElasticSearch, MongoDB, Redis, and Druid, with additional experience in NoSQL and graph databases being advantageousExperience in horizontally scaling databasesProficiency in Kafka and Airflow, with a deep understanding of runtime profiling tools to optimize throughput and latency, and able to establish comparative performance benchmarksStrong skills in build automation, continuous integration, and deployment (CI/CD) tools like Webpack, Buddy, Jenkins, and DockerExpert-level Python coding skillsExperience collaborating with distributed teams
Helpful to have: Technical background in Artificial Intelligence (AI) and Machine Learning (ML)Experience designing and implementing interactive query-driven main-machine intelligence systemsSolid skills in Java
We’ve outlined specific skills, experience, and requirements for this position, but don’t stress if you don’t meet every single one. Our Talent Team is dedicated to discovering exceptional individuals, and they might identify a relevant aspect of your background that suits this role or another opportunity within Blackbird.AI.If you have passion for the role, please still apply.What’s in it for you:Blackbird.AI is embarking on an exciting growth journey with numerous opportunities for career development within the company. You will join a nurturing, inclusive, and experienced team.Join us as we soar to new heights!Values:At Blackbird.AI, our core values shape how we work and make decisions. Our values inspire us to be authentic and continue improving.We embrace a strong sense of responsibility to society, recognizing the vital role our services play in empowering governments, communities, and individuals to foster critical thinking and empowerment. We believe in integrating personal and professional lives with societal needs, emphasizing the importance of creating an environment that attracts top talent and provides substantial growth opportunities. We are motivated by the potential of science and technology to impact humanity positively.BenefitsWhy you’ll love working here: Competitive compensation package, 401(k), and equity - everyone has a stake in our growth! Comprehensive health benefits for you and your loved ones, including wellness days and monthly wellness reimbursements - an apple a day doesn’t always keep the doctor away! Generous vacation policy, encouraging you to take the time you need - we trust you to strike the right work/life balance! A flexible work environment with opportunities to collaborate with your team in person - you can have it all! Inclusion and Impact - soar to new heights! Bi-annual offsites - have fun with your colleagues! Professional development stipend - never stop learning! 
Pay Transparency:For individuals assigned and/or hired to work in New York, Blackbird.AI is required by law to include a reasonable estimate of the compensation range for this role. This compensation range is specific to New York. It takes into account the wide range of factors that are considered in making compensation decisions, including, but not limited to, skill sets, experience and training, licensure and certifications, and other business and organizational needs. At Blackbird.AI, it is not typical for an individual to be hired at or near the top of the range for their role, and compensation decisions are dependent on the facts and circumstances of each case. A reasonable estimate of the current compensation range for this position is expected to be $130,000-$170,000. This range may vary for positions outside of New York and as it has not been adjusted for the applicable geographic differential associated with the location where the position may be filled.Regardless of location, candidates can expect during the first few conversations with Blackbird.AI’s Talent Team and Hiring Managers to share any approved budget.Apply TodayEqual Opportunity Employer"
Data Engineer,Blue Onion,"New York, NY (Remote)",https://www.linkedin.com/jobs/view/3696157815/?eBP=JOB_SEARCH_ORGANIC&refId=BZF6grSNM9xRxSJxnO7rwQ%3D%3D&trackingId=AoUGJU6ehkVyd%2FgsmPedVQ%3D%3D&trk=flagship3_search_srp_jobs,3696157815,"About the job
            
 
How We’re Different  We’re a lean team so all of your work will have a direct and measurable impact on the business You’ll have the opportunity interact with some of our amazing beta customers who are constantly providing feedback and helping us make the product better You’ll have the opportunity to craft elegant, efficient, and (sometimes!) scrappy solutions to hard technical problems using the latest and greatest tools and technologies
Technology  Our application runs on a tech stack that is a mixture of Python and Ruby on Rails + React For our backend data processing, we use Apache Airflow.  For our web application, we use Ruby on Rails, with a Typescript/React single-page-app frontend, powered by a GraphQL API
What Your Day Would Look Like  Build scalable and fault-tolerant data pipelines in Google Cloud Platform using Apache Airflow Inspect, analyze, and transform data using SQL based tools like BigQuery and dbt Design, implement, and test features on our web application Build integrations with external services
What We Look For  You’re the best at what you do, take ownership for your work and are constantly looking to learn 3+ years of experience in Python and SQL You have experience with Apache Airflow and familiarity with AWS or GCP"
Data Engineer,Smile ID (formerly Smile Identity),"San Francisco, CA (Remote)",https://www.linkedin.com/jobs/view/3641780519/?eBP=JOB_SEARCH_ORGANIC&refId=YoSICnNo%2BWHRXbDyDF1PqQ%3D%3D&trackingId=FB01S6eBhD9P2odUnesqhQ%3D%3D&trk=flagship3_search_srp_jobs,3641780519,"About the job
            
 
Smile Identity builds trust.Smile Identity is Africa's leading identity verification, and digital Know Your Customer (KYC) provider. We help companies scale rapidly across Africa by confirming the true identity of their users in real time, using any smartphone or computer. Our technology is powered by proprietary Machine Learning algorithms designed specifically for African faces, devices and network connections.Our team is a diverse group of hardworking, truth-seeking, and fun-loving Smilers spanning 5 offices, 10 countries and 8 time zones. Our products are already making waves across many industries, from Banking to Fintech and Telecoms. We recently announced a $20M Series B raise and are backed by leading global investors, including Norsken22, Costanoa, CRE, Future Africa, Susa Ventures, Commerce Ventures, Courtside Ventures, Two Culture Capital, Latitude, Valuestream Ventures, Intercept Ventures and Vinod Khosla who are supporting us every step of the way.Do you like working alongside a team of intelligent individuals? Do you want to have fun while making a real difference? Here at Smile Identity, you'll get the freedom and autonomy you need to do your best work, the flexibility to be creative, and the opportunity to grow and put your unique stamp on our mission.What are you waiting for? Come with us on this amazing journey!The RoleWe are looking for a data engineer who loves bringing order to chaos. The individual in this role will maintain our data warehouse and associated data infrastructure and provide the entire organization with the data they need to be successful. This role is open to candidates across the globe. You will be working with colleagues ranging from the US West Coast to Eastern Africa, with that in mind candidates working in timezones between US Eastern and GMT are preferred.What You Will Do Work with our entire organization based in the US, London, Berlin, Lagos, Nairobi, and Cape Town to centralize our data and maintain our data warehouses/lakes. You will select the right tools and services to bring our data together and provide a solid foundation for all our product and business analytics. Your north star? Empowering the entire organization with data to make the best possible decisions. Design, build and launch extremely efficient and reliable data pipelines to move data. Architect, build and launch new data models that provide intuitive analytics. Manage the delivery of high impact dashboards, tools and data visualizationsBuild data expertise and own data quality, including defining and managing SLAs for data sets. Partner with leadership, engineers, commercial, and data scientists to understand data needs. Influence short- and long-term strategy with cross-functional teams to drive impact. Educate your partners: Use your data and analytics experience to discover opportunities, identifying and addressing gaps in existing logging and processes. 
Requirements 4+ years experience with data infrastructure, ETL design, data warehousing, schema design and dimensional data modeling2+ years of experience in SQL, Python, or similar languagesExperience with designing and implementing real-time pipelinesExperience with code management tooling such as Git, GithubExperience with data migrations in production settingsYou have a deep understanding of modern data tooling and infrastructureYou are comfortable working independently with periodic guidance from engineering & business teamsYou are a strong believer in scale and automationYou are entrepreneurial — you take initiative, solve problems and love to troubleshoot. You are a great collaborator and can communicate effectively. You enjoy teaching and learning from your colleaguesYou are not ideological about programming languages or tools. You have opinions but are open to discussion and tradeoffsYou are a pragmatistYou are a seeker of truth
Preferred Qualifications Experience querying big data using Spark, Presto, Hive, Impala, etc. Experience with data quality and validationExperience with SQL performance tuning and E2E process optimizationExperience creating reports and dashboards with modern business intelligence tools (Tableau, Metabase preferred)Experience working with Postgres, Hevo, and cloud or on-prem Big Data/MPP analytics platform (i.e. Snowflake, AWS Redshift, Google BigQuery, Azure Data Warehouse, Netezza, Teradata, or similar). Interest or experience in working in the African Fintech EcosystemExperience in a high-growth team and/or startup experienceAbility to communicate and prioritise effectively with a distributed team around the world
Compensation Salary commensurate with experienceStock options HealthcareOpportunities for travel (Post-Covid19)
Autonomy and a chance to work at a mission-driven company with purposeWhat Success Looks LikeSuccesses in your first 3 months include Take the time and learn the ins and outs of our data warehouse and dashboards. Investigate how data is being logged in our systems and what existing data pipelines exist across our two data warehouses (Postgres and Redshift). Evaluate existing dashboards, data quality, and pipelines and identify gaps. Get introduced to the Product and Engineering team and their bi-weekly sprint processes. At this point you are mostly observing the dynamics and taking on tasks by the team, while building a partnership and exploring support opportunities. 
In your first 6 months Based on your initial exposure to our data stack, you have already built several improvements based on the gaps you've identified. You are able to manage the flow of data across the stack. You have extended our system capabilities as needed and have improved efficiency and simplicity of shared tools and libraries. You are deeply embedded in how we set up data logging and are able to manage and successfully execute on data requirements from teams across the company (e.g. Engineering, Product, CVML, Data Science, Marketing, Commercial). You proactively develop technical methodologies or tools which can solve important classes of problems. You can evangelize these methodologies and tools to other data scientists and engineers to scale multiple people. 
In your first 12 months You are the company expert in our data, infrastructure, and technical architecture, and are actively involved in product and business operations to either improve existing data tools or suggest new methodologies to accelerate team execution, including influencing data best practices. You drive scalable solutions across teams. You are able to solve challenging technical problems faced by multiple teams and provide significant technical advice to newer or less-technical analysts."
4242 - Data Engineer,Mission Box Solutions,"Dayton, OH (Remote)",https://www.linkedin.com/jobs/view/3678271429/?eBP=JOB_SEARCH_ORGANIC&refId=YoSICnNo%2BWHRXbDyDF1PqQ%3D%3D&trackingId=WHXpj%2Fu2LTjcAiNjfeu9NA%3D%3D&trk=flagship3_search_srp_jobs,3678271429,"About the job
            
 
As a Data Engineer, you will work hands-on with challenging data engineering, data management, and analytics projects. You will collaborate with data scientists, analysts, business users, and IT teams to design, implement, and deploy data services and analytics.Must-Haves: Minimum of 6 years of experience as a data engineerStrong SQL skills in multiple data base platformsExperience with Snowflake, Databricks, Spark SQL, PySpark, and PythonCloud experience: Azure, AWS, or GCPDevelop and maintain ETL pipelinesDatabase design and principlesData modeling, schema development, and data-centric documentationExperience integrating data from a variety of data source typesRecommend and advise on optimal data models for data ingestion, integration, and visualizationExperience improving code performance and query optimization Use Continuous Integration/Continuous Delivery (CI/CD) concepts to engineer a standardized data environmentOutstanding problem-solving skillsExcellent verbal and written communication skills and the ability to interact professionally with a diverse group, including executives, managers, and subject matter experts
Salary: $100,000-$140,000Location: Varies by client: Columbus, OH, USCincinnati, OH, USDayton, OH, USRemote Considered
Benefits:Generous PTOMedical InsuranceDental & Vision Insurance401K MatchShort/Long term Disability InsuranceMission Box Solutions is an Equal Opportunity Employer. We value the benefits of diversity in our workforce. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity and expression, national origin, disability, protected Veteran status, or any other attribute or protected characteristic by law. Applicants selected may be subject to a government security investigation and must meet eligibility requirements for potential access to classified information. Accordingly, US Citizenship is required. Our strategic partner is committed to creating a diverse environment and is proud to be an equal-opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. The statements herein are intended to describe the general nature and level of work being performed by employees and are not to be construed as an exhaustive list of responsibilities, duties, and skills required of personnel so classified. Furthermore, they do not establish a contract of employment and are subject to change at the discretion of our strategic partner. Powered by JazzHRx62P458ugO"
Remote Entry Level Data Analyst/Engineer,SynergisticIT,"White Plains, NY (Remote)",https://www.linkedin.com/jobs/view/3774171423/?eBP=JOB_SEARCH_ORGANIC&refId=YoSICnNo%2BWHRXbDyDF1PqQ%3D%3D&trackingId=9wYi%2B7%2BwnFvwa6cf5hEvjw%3D%3D&trk=flagship3_search_srp_jobs,3774171423,"About the job
            
 
At SynergisticIT, we're all about making connections. Whatever IT goals you have, our software programmers can help achieve those. Our Software development teams can take up turnkey projects and execute them in an effective and efficient manner. If you are looking to source talent our recruiters will find the ideal IT talent for your company. What's the secret to our success? Well, it all starts with taking quality time to listen to each client's specific needs. After we have a thorough grasp of your IT goals, we can better customize our Developments as per your specific needs. We can also tailor make recruiting programs to exceed your expectations. Since our founding in 2010, SynergisticIT's strategies have earned the company an enviable position in the software development, IT staffing and IT skill enhancement fields. SynergisticIT continues to work with hundreds of satisfied American clients with our software programmers working on our projects and after gaining hands on experience on cutting edge technologies moving to contribute their skills to great clients like Apple, Google, Client, Ebay, Paypal, Kroger, the Walt Disney Company and hundreds more. If you are tired of working with inefficient programmers who take a lot of time to ramp up we want you to try us. Our software programmers can hit the ground running and get you the maximum return on your investment. You have already tried the rest its time you tried the best. SynergisticIT - Home of the Best Data Scientists and Software Programmers in the Bay Area. Why Us ? SynergisticIT has a proven track record of successfully skill enhancement and staffing IT employees for some of the world's most iconic brands. Our team takes the time to fully understand every client's needs so we could best meet your IT staffing requirements. The knowledgeable staff at SynergisticIT is always more than happy to work with clients to ensure they reach their software development goals. Besides staffing, SynergisticIT is also committed to helping young IT professionals advance their career with a robust upskill program . Everyone who goes through SynergisticIT's program learns all the skills necessary to succeed in many IT fields ranging from Java to Machine Learning. Additionally, everyone trained at SynergisticIT has been through extensive mock and technical interview screenings to bolster their career prospects. Last, but certainly not least, SynergisticIT takes great care to respect the privacy considerations for every client. All companies who work with SynergisticIT can rest assured their confidential data is protected using the most up-to-date encryption technologies. SynergisticIT also complies with all the latest NDA agreements. REQUIRED SKILLS For Java /Software Programmers   Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT  Highly motivated, self-learner, and technically inquisitive  Experience in programming language Java and understanding of the software development life cycle  Project work on the skills  Knowledge of Core Java , javascript , C++ or software programming  Spring boot, Microservices, Docker, Jenkins and REST API's experience  Excellent written and verbal communication skills 
 For data Science/Machine learning Required Skills  Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT  Project work on the technologies needed  Highly motivated, self-learner, and technically inquisitive  Experience in programming language Java and understanding of the software development life cycle  Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools  Excellent written and verbal communication skills 
 Preferred skills: NLP, Text mining, Tableau, PowerBI, Time series analysis  We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2022 and at Gartner Data Analytics Summit (Florida)-2023 Oracle CloudWorld Event (OCW) Las Vegas 2022 | SynergisticIT - YouTube https://www.youtube.com/watch?v=OAFOhcGy9Z8  https://www.youtube.com/watch?v=EmO7NrWHkLM  https://www.youtube.com/watch?v=NVBU9RYZ6UI  https://www.youtube.com/watch?v=Yy74yvjatVg SynergisticIT at Gartner Data and Analytics Summit 2023 - YouTubeFor preparing for interviews please visit  https://www.synergisticit.com/interview-questions/  We are looking for the right matching candidates for our clients  Please apply via the job posting  REQUIRED SKILLS For Java /Full Stack/Software Programmer   Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT  Highly motivated, self-learner, and technically inquisitive  Experience in programming language Java and understanding of the software development life cycle  Project work on the skills  Knowledge of Core Java , javascript , C++ or software programming  Spring boot, Microservices, Docker, Jenkins and REST API's experience  Excellent written and verbal communication skills 
 For data Science/Machine learning Positions Required Skills  Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT  Project work on the technologies needed  Highly motivated, self-learner, and technically inquisitive  Experience in programming language Java and understanding of the software development life cycle  Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools  Excellent written and verbal communication skills 
 Preferred skills: NLP, Text mining, Tableau, PowerBI, Tensorflow  If you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team.  No phone calls please.  Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates"
Data Engineer II - Remote,Net Health,"Pittsburgh, PA (Remote)",https://www.linkedin.com/jobs/view/3779192681/?eBP=JOB_SEARCH_ORGANIC&refId=YoSICnNo%2BWHRXbDyDF1PqQ%3D%3D&trackingId=O0azGx1Z88BOngibD8BYEQ%3D%3D&trk=flagship3_search_srp_jobs,3779192681,"About the job
            
 
Job DetailsDescriptionAbout Net HealthBelong. Thrive. Make a Difference.Are you looking for a meaningful and satisfying career where you have endless opportunities to grow and be financially rewarded? Net Health may be the perfect place for you.A high-growth and profitable company, we help caregivers harness data for human health. We also honor and respect the needs of our Net Health family and staff, which is why we offer a work-from-anywhere environment and unlimited PTO. Our welcoming and collaborative culture paired with progressive benefits makes Net Health the ultimate career home!As a leading-edge SaaS company in healthcare, we deliver solutions that help patients get better, faster, and live more fulfilling lives. Our software and predictive analytics cover the continuum of care, from hospital-to-home, across various medical specialties. Come join us and start the next chapter of your exciting career while helping others to live better lives.World-Class Benefits That Reflect Our World-Class Culture.Click Here to Learn More!:#WorkFromAnywhere #UnlimitedPTO #ComprehensiveBenefitsPackage #EmployeeResourceGroups #CasualDressCode #PrioritizedEmployeeWellness #DiversityAndInclusion #AVoice #NewHireSupport #CareerDevelopment #EducationalAssistance #EmployeeReferralBonus #ProgressiveParentalLeaveJob OverviewDesigns, models, documents, and guides the logical and conceptual relationship of data and database changes for complex applications. Analyzes needs and requirements of existing and proposed systems, and develops technical, structural, and organizational specifications. May also develop, implement, and maintain data systems to meet designs, models and specifications. This person will analyze client production issues and create SQL statements to fix the issue.Responsibilities And Duties Will work with development wen new features or changes happen to the schemaCreate and maintain internal procedures and documentation around reusable SQL scripts or new additions to the data schema due to newly deployed features and functionalityAssist support with triaging complex client issuesProvide ad-hoc reportsAnalyze data to spot anomalies, trend and correlate similar data setsAnalyze large, complex datasets to drive actionable insights and recommendationsEvaluate failures, defects, systemic problems and hardwareConfer with clients to identify problems, replicate the problems, and troubleshoot for root causeParticipate in Agile/Scrum process to refine, prioritize, and build solutions to meet customer needsCreate and maintain internal procedures and documentation around reusable SQL scripts or new additions to the data schema due to newly deployed features and functionalityPerform other related duties as required and assigned
Qualifications Bachelor’s degree or equivalent experience2-4 years of relevant experience
Required Software Experience MS Excel or Google Spreadsheets2-4 years SQL (both DML and DDL)Exposure to 1-2 relational Database Systems (Postgres, Oracle, SQl Server, Snowflake, RedShift, BigQuery etc.)2-4 years Data Visualization, Analysis, or reporting1-2 years with at least 1 scripting language (Javascript, Python, PHP, Bash, Powershell)0-2 years building and maintaining an enterprise Data Warehouse
Note: This job description is not intended to be all-inclusive. Employee may perform other related duties as requested to meet the ongoing needs of the organization.Colorado Pay Law: If you are a Colorado resident and this role is available in Colorado or remote, you may be eligible to receive additional information about the compensation and benefits for this role, which we will provide upon request. Please send an email to Recruiting@NetHealth.comIf you are a CA, CT, CO, IL, MD, NV, RI, WA or NY City resident and this role is available in one of those locales or remote, you may be eligible to receive additional information about the compensation and benefits for this role, which we will provide upon request. Please send an email to Recruiting@NetHealth.com"
Data Engineer (100% Remote),Praescient Analytics,"Fairfax, VA (Remote)",https://www.linkedin.com/jobs/view/3768491689/?eBP=JOB_SEARCH_ORGANIC&refId=YoSICnNo%2BWHRXbDyDF1PqQ%3D%3D&trackingId=TRuY12QlfMMC%2BIg1o4xqqA%3D%3D&trk=flagship3_search_srp_jobs,3768491689,"About the job
            
 
Praescient Analytics, LLC is a Veteran-led, certified Woman-Owned Small Business (WOSB) founded in 2011 which specializes in implementing advanced analytics solutions across the defense, intelligence, and law enforcement communities. Praescient has extensive experience designing, developing, and integrating solutions for customers including the US Army, Special Operations Command (SOCOM), US Navy (USN), US Marine Corps (USMC), US Coast Guard (USCG), Department of Justice (DOJ), Drug Enforcement Administration (DEA), and Federal Bureau of Investigation (FBI), Immigration and Customs Enforcement (ICE), intelligence community, and local law enforcement agencies, among others.Praescient Analytics is in search of a Data Engineer to help develop best practices and providing consultative advice across a wide range of big data technologies and data management tools for our client. The Data Engineer will be expected to perform duties such as evaluating the performance of current data integrations, data management tools, and associated processes and procedures within a cloud systems environment. This position is fully remote. US Citizenship and a Public Trust clearance will be REQUIRED.Primary Responsibilities Manage and maintain ADLS file storage directories to support large multi-stage ETL pipelinesImplement pipelines to transfer sever-based data sources into ADLS, or directly to Synapse where appropriate.Establish quality controls for any manual uploads of flat file extracts into ADLSImplement ETL/ELT process and architecture within Azure Synapse and Azure Data Factory, including creating lookup tables where necessary to improve performance or efficiencyImplement fraud flag tables by entity and individual, linking all identified fraud flags for easy access via SQL query.Implement aggregate views to quickly summarize key information and enable high performance queries by analysts and other usersDocument all transformation processes and data architecture, including any derived or aggregate fields created to support the architectural design.
Required Qualifications 5+ years of hands-on experience maintaining SQL databases, including expert level skills in SQL and T-SQL.5+ years of hands-on experience working in cloud-based environments with distributed SQL pools.Experience with designing and implementing ETL/ELT processes in support of cloud-based data analytics environments.
Desired Qualifications Experience with distributed systems utilizing tools such as Apache Hadoop and/or Spark.Experience with Azure cloud offerings.Experience with big data analytic tools such as Databricks or Azure Synapse.Experience working with graph databases such as Neo4j.Working experience in Agile Scrum environments.Experience with source control tools such as GitHub or Azure DevOpsBachelor’s degree in Data Science, Business Intelligence, Computer Science or related fields, or the equivalent combination of education, professional training, and work experience
If you are a self-starter who is passionate about data analysis and eager to make a meaningful impact, we invite you to apply for this exciting opportunity with Praescient Analytics! Very competitive salary based on qualifications and experience.Comprehensive, Company paid healthcare for you (We pay your premiums and deductibles)401(k) with company matchingTravel & performance incentives26 days of paid time off$5K annual training allowance$500 book allowanceTuition reimbursement program
Praescient Analytics is committed to providing equal employment opportunities for all. Employees and applicants without regard to race, color, age, religion, sex, sexual orientation, marital status, gender identity, national origin, legally protected physical or mental disability, genetic information, citizenship status, or status in the uniformed services of the United States, status as a disabled veteran or veteran of the Vietnam era, or on any other basis which is protected under applicable law. This includes a commitment to provide a work environment that is free from all forms of illegal harassment including sexual harassment.This covers all terms and conditions of employment including (but not limited to): Hiring, placement, promotion, transfer, or demotion of all job classifications;Recruiting, advertising, or solicitation for employment;Treatment during employment;Rates of pay or other forms of compensation;Benefits;Selection for training;Company sponsored social and recreational activities; andLayoff or termination
 Applicants selected will be subject to a government security investigation and must meet eligibility requirements for access to classified information. US Citizenship RequiredPraescient Analytics is an Equal Opportunity Employer.Interested Candidates: Please forward your resume to recruiting@praescientanalytics.com and please visit our website to apply online at www.praescientanalytics.applicantstack.com/x/openings."
Data Engineer,Care.com,"New York, NY (Remote)",https://www.linkedin.com/jobs/view/3743209378/?eBP=JOB_SEARCH_ORGANIC&refId=YoSICnNo%2BWHRXbDyDF1PqQ%3D%3D&trackingId=6%2B4v9owxTsxfpvA04Xu%2F4w%3D%3D&trk=flagship3_search_srp_jobs,3743209378,"About the job
            
 
About Care.comCare.com is a consumer tech company with heart. We're on a mission to solve a human challenge we all face: finding great care for the ones we love. We're moms and dads and pet parents. We have parents and grandparents, so we understand that everyone, at some point in their lives, could use a helping hand. Our culture and our products reflect that.Here, entrepreneurs, self-starters, team players, and big thinkers unite behind a common cause. Here, we're applying data analytics, AI, and the latest technologies to solve universal problems and connect people in new ways. If you like having autonomy, if you thrive on collaboration and building new things, and if you're all about using your talent for good, Care.com is the place for you.What Your Days Will be Like:The Data Engineer will be focusing on building out data feeds and tooling from our application platform and enable rapid ingestion into our centralized Data Lake/Data Warehouse. The Data Engineer will work across business areas and application teams at Care.com to rationalize data and design, build and maintain reusable data feeds which and ultimately empower analytics consumption at Care.com.The ideal candidate will have professional experience building data pipelines in a technical environment. S/he will have an understanding of application development, data warehousing, demonstrate strong business judgment, and be able to prioritize in a fast-paced environment.What You'll Be Working On:  Collaborate and partner with application teams to understand data collection/generation and design and partner to build and implement data feeds from our product tech stackDesign, develop, and build code for rapid feeds and ingestion into the Data WarehouseIdentify data sources used for building out data architecture diagrams/modelsEstablish engineering practices and setup frameworks for ""Data as a Service""Collaborate with relevant delivery teams, including infrastructure, operations, site reliability engineering, product development, and others to perform evaluations, POCs, and ultimately implement and operationalize new technology. Solve code level problems quickly and efficiently Participate in demos and code reviews Promote software best approach, standards, and processes Shape development processes to promote a high-quality output while continuing to iterate quickly Incorporate best practices for security, performance, and data privacy into data pipelines
What You'll Need to Succeed: BS or MS in Computer Science or relevant engineering experience5+ years work experience in Data Engineering/data pipelines3+ years SQL experience is a must1+ years Unix/batch scripting preferred1+ years Python experience is a plus1+ years Windows server admin experience is a plusExperience interfacing with business teams and turning requirements and vision into a technical realityMySQL & Vertica Experience a plus/preferredAWS experience is a plusAbility to drive efforts from start to finish as a self-motivatorKnowledge in Data Warehousing is a MUSTProven ability to maintain performance level in a fast-paced agile environment Pragmatic and realistic with solutions
For a list of our Perks + Benefits, click here!Care.com supports diverse families and communities and seeks employees who are just as diverse. As an equal opportunity employer, Care.com recognizes the power of a diverse and inclusive workforce and encourages applications from individuals with varied experiences, perspectives, and backgrounds. Care.com is committed to providing reasonable accommodations for qualified individuals with disabilities. If you need assistance or accommodation, please reach out to talent@care.com.Company Overview:Available in more than 20 countries, Care.com is the world's leading platform for finding and managing high-quality family care. Care.com is designed to meet the evolving needs of today's families and caregivers, offering everything from household tax and payroll services and customized corporate benefits packages covering the care needs of working families, to innovating new ways for caregivers to be paid and obtain professional benefits. Since 2007, families have relied on Care.com's industry-leading products—from child and elder care to pet care and home care. Care.com is an IAC company (NASDAQ: IAC).Salary Range: 110,000 to 145,000. The base salary range above represents the anticipated low and high end of the national salary range for this position. Actual salaries may vary and may be above or below the range based on various factors including but not limited to work location, experience, and performance. The range listed is just one component of Care.com's total compensation package for employees. Other rewards may include annual bonuses and short- and long-term incentives. In addition, Care.com provides a variety of benefits to employees, including health insurance coverage, life, and disability insurance, a generous 401K employer matching program, paid holidays, and paid time off (PTO)."
Data Engineer - Remote | WFH,Get It Recruit - Information Technology,"Jacksonville, FL (Remote)",https://www.linkedin.com/jobs/view/3766963082/?eBP=JOB_SEARCH_ORGANIC&refId=YoSICnNo%2BWHRXbDyDF1PqQ%3D%3D&trackingId=NaWUqeTqSoCTpirOW1PeBw%3D%3D&trk=flagship3_search_srp_jobs,3766963082,"About the job
            
 
Are you ready to embark on an exciting journey as an Associate Data Engineer? Join our dynamic team and be an integral part of our Business Intelligence initiatives, supporting one of the leading global pharmaceutical clients.Position OverviewAs an Associate Data Engineer, you'll play a pivotal role in crafting innovative data engineering solutions that drive business impact across the Americas Region. Your mission includes tackling complex data migration and transformation challenges, enabling the acceleration of end-to-end reporting and analytics, and managing data catalog for the business. Your problem-solving skills will be put to the test as you work in partnership with our IT team to ensure data integrity in our ""data lake.""Key ResponsibilitiesIdentify and resolve quality issues during complex data migration and transformation.Support BI analysts with fundamental data engineering capabilities to facilitate complex analytics.Be accountable for data engineering use cases, some of which may have ""limited time"" life cycles.Write queries and code real-time solutions for fast deployment within a sandbox environment.Engage in data engineering to support the acceleration of end-to-end reporting and analytics.Manage new data sources in compliance with HIPAA and HCC, including access protocols.Collaborate with the IT team to solve data issues in the data lake and answer urgent business priority questions.QualificationsBachelor's or advanced degree in Computer Science, Data Science, Business Administration, Engineering, or a related field with a focus on Information Technology.Minimum of two years of progressive experience in data warehousing and decision support/reporting environments.Minimum of two years of software development experience, preferably with data transformation scripting languages such as INFORMATICA, SQL, TALEND, or Python.Demonstrable expertise in data lifecycle management, including data ingestion, contextualization, and analytics.Familiarity with large datasets and understanding of data analysis workflows, experience with data visualization tools like Tableau is a plus.Strong attention to detail, organizational skills, and the ability to manage multiple tasks.Effective communication skills, both oral and written, for collaborating with business partners.BenefitsCompetitive salary in the range of $37.00 - $42.00 per hour.Comprehensive benefits package, including 401(k) with matching, dental and health insurance, paid time off, and vision insurance.ScheduleFull-time position, Monday to Friday.Equal Opportunity EmployerWe are an equal opportunity employer. We provide equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, national origin, age, sex, citizenship, disability, genetic information, gender, sexual orientation, gender identity, marital status, amnesty, or status as a covered veteran in accordance with applicable federal, state, and local laws.Join us in this exciting and meaningful role as an Associate Data Engineer and make a difference in the world of data and analytics!Employment Type: Full-Time"
Remote Work - Need Staff Data Engineer - USC only,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3746283980/?eBP=JOB_SEARCH_ORGANIC&refId=YoSICnNo%2BWHRXbDyDF1PqQ%3D%3D&trackingId=h5Uxd3bQTzd0COUR2M58Jw%3D%3D&trk=flagship3_search_srp_jobs,3746283980,"About the job
            
 
100% Remote, client is in NYLook for strong job history, Python, Machine Learning, SQL, 10/10 comms, etc. MUST be an Engineer SummaryWe're looking for an experienced Data Engineer to join our client's growing team of top notched IT professionals. You will work closely across multiple groups including Editorial and Audio, Marketing, Advertising, etc. to achieve their goals and objectives. The Data Engineering team builds tools throughout the stack while sharing knowledge across these departments.As the company grows, they're looking for Data Engineer who will help solidify and expand our pipelines and maintain their data warehouse. Their business model is based on performing complex and very detailed analyses of how our products perform with their subscribers. You'll be responsible for working alongside analysts, product, marketing, and other teams to define new data collection components and measurement schemes that support each new product and feature.How Can Add Value To Their Mission Create and maintain data pipelines to provide insights and drive business decisionsEstablish data warehousing strategy (ex. Kimball, Data Vault, etc.)Maintain data infrastructure on our AWS accountsCollaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.Write unit/integration tests, contributes to engineering wiki, and documents workImplement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it
What You'll Need To Succeed 7+ years of industry experience measuring product performance and user behaviorExperience working with a variety of data management technologies, including RedShift, Kinesis/Kafka, Glue, Spark, Postgres, Airflow, dbt, and others.Experience implementing BI reporting tools such as LookerExperience interfacing with engineers, product managers and analysts to understand data needsKnowledge of a variety of measurement beacons, SDKs, APIs including Google Analytics, Amplitude, Braze, Stripe, email service providersA commitment to building secure, resilient, fault-tolerant architectures, with clear documentation and procedures in place for supportA focus on accuracy and detail as you build, ensuring data pipelines produce clear, predictable resultsUnderstanding of the typical metrics a subscription and advertising-supported business needs to measure successExperience with Client techniques as applied to behavioral segmentation or anomaly identification is a definite plusFamiliarity using developer tools that increase productivity and facilitate the development of resilient code (eg. Docker, CircleCI, Serverless) is a plus"
Developer/Data Engineer || Remote,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3665291336/?eBP=JOB_SEARCH_ORGANIC&refId=YoSICnNo%2BWHRXbDyDF1PqQ%3D%3D&trackingId=vWy97AIBT%2FzvWFhe2UP55Q%3D%3D&trk=flagship3_search_srp_jobs,3665291336,"About the job
            
 
remote roleKey Experience(s)  Prior experience as a SQL or PL/SQL developer (5+ years) Previously developed in Power BI (5+ years) Previous experience working with Databricks Previous ETL data validation and reconciliation experience Previously participated in Data Conversion Understands how to iteratively develop Understanding of Agile (SCRUM) practices Previously run or contributed to discovery interviews Previous work on FISERV products, specifically DNA Previous financial services/banking experience is a plus
Key Skill(s)  Good communicator, both written and verbal Well organized Attention to detail Team Player
Key Technology /Certification(s)  PowerBI, SQL, Databricks"
Data Engineer,IVY TECH SOLUTIONS INC,United States (Remote),https://www.linkedin.com/jobs/view/3686025266/?eBP=JOB_SEARCH_ORGANIC&refId=YoSICnNo%2BWHRXbDyDF1PqQ%3D%3D&trackingId=dtqF9XmG4YslT7vDthyfBw%3D%3D&trk=flagship3_search_srp_jobs,3686025266,"About the job
            
 
Remote PositionData EngineerOnly w2Please send the resume to sowmya.g@ivytechsol.us Job Description:DataStage Data Engineer US/GC/Certain EADs – 6 month remote contract.Together we are building a culture that values diversity and creates a space of belonging for all our team members. We believe that investing in your success is an investment in our customers and our business. Our people are what sets us apart and make us great. As a Data Engineer, you’ll provide your talents in contributing to the success of the client’s team by delivering the following:  Serve in the goalie rotation to support the Production environment. Responsible for maintaining enterprise-grade platforms that enable data-driven solutions. Search for ways to automate and maintain scalable infrastructure. Ensure delivery of highly available and scalable systems. Monitor all systems and applications and ensure optimal performance. Analyzes and designs technical solutions to address production problems. Participate in troubleshooting applications and systems issues. Identifies, investigates, and proposes solutions to technical problems. While providing technical support for issues, develop, test, and modify software to improve efficiency of data platforms and applications. Monitors system performance to maintain consistent up time. Prepares and maintains necessary documentation. Participate in daily standups, team backlog grooming, and iteration retrospectives. Coordinate with data operations teams to deploy changes into production. Highest level may function as a lead. Other duties as assigned.
Qualifications:  Requires a Bachelor's in Computer Science, Computer Engineering or related field and experience with ETL development, SQL, UNIX/Linux scripting, Big Data distributed systems. Prefer experience with IBM DataStage. Various programming languages like Java and Python, orchestration tools and processes or other directly related experience. A combination of education and experience may meet qualifications. Excellent analytical, organizational, and problem-solving skills. Ability and desire to learn new technologies quickly. Ability to work independently and collaborate with others at all levels of technical understanding. Able to meet deadlines. Good judgment and project management skills. Ability to communicate both verbally and in writing with both technical and non-technical staff. Ability to work in a team environment and have good interpersonal skills. Ability to adapt to changing technology and priorities. Must be able to work independently, handle multiple concurrent tasks, with an ability to prioritize and manage tasks effectively
Powered by JazzHRgDX0uvPo7F"
(Remote) Data Engineer,SynergisticIT,"Anaheim, CA (Remote)",https://www.linkedin.com/jobs/view/3767593300/?eBP=JOB_SEARCH_ORGANIC&refId=YoSICnNo%2BWHRXbDyDF1PqQ%3D%3D&trackingId=3mYEod95uUPuid5CtCmAcw%3D%3D&trk=flagship3_search_srp_jobs,3767593300,"About the job
            
 
The Job Market is Challenging due to almost 300,000 Tech Layoffs since October 2022 due to which thousands of laid off Techies are competing with existing Jobseekers. Entry level Job seekers struggle to get responses to their applications forget about getting client interviews. As the Saying goes ""when the Going gets tough the Tough get going” Candidates who want to make a tech career they need to differentiate themselves by ensuring they have exceptional skills and technologies to be noticed by clients.Since 2010 Synergisticit has helped Jobseekers differentiate themselves by providing candidates the requisite skills and experience to outperform at interviews and clients. Here at SynergisticIT We just don't focus on getting you a Job we make careers.All Positions are open for all visas and US citizensWe are matchmakers we provide clients with candidates who can perform from day 1 of starting work. In this challenging economy every client wants to save $$$'s and they want the best value for their money. Jobseekers need to self-evaluate if they have the requisite skills to meet client requirements and needs. Clients now post covid can also hire remote workers which increases even more competition for jobseekers.We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like apple, google, PayPal, western union, Client, visa, Walmart labs etc. to name a few.We have an excellent reputation with the clients. Currently, We are looking for entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Machine Learning engineers for full time positions with clients.Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in IT IndustryWe assist in filing for STEM extension and also for H1b and Green card filing to Candidates We also offer optionally Skill and technology enhancement programs for candidates who are either missing skills or are lacking Industry/Client experience with Projects and skills. Candidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. If they are qualified with enough skills and have hands on project work at clients then they should be good to be submitted to clients. Shortlisting and selection is totally based on clients discretion not ours.please check the below links to see success outcomes of our candidateshttps://www.synergisticit.com/candidate-outcomes/We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023Oracle CloudWorld Event (OCW) Las Vegas 2023/ 2022 | SynergisticIT - YouTubehttps://youtu.be/Rfn8Y0gnfL8?si=p2V4KFv5HukJXTrnhttps://youtu.be/-HkNN1ag6Zk?si=1NRfgsvL_HJMVb6Qhttps://www.youtube.com/watch?v=OAFOhcGy9Z8 https://www.youtube.com/watch?v=EmO7NrWHkLM https://www.youtube.com/watch?v=NVBU9RYZ6UI https://www.youtube.com/watch?v=Yy74yvjatVgFor preparing for interviews please visit https://www.synergisticit.com/interview-questions/We are looking for the right matching candidates for our clientsPlease apply via the job postingREQUIRED SKILLS For Java /Full Stack/Software Programmer Bachelor's degree or Master's degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, ITHighly motivated, self-learner, and technically inquisitiveExperience in programming language Java and understanding of the software development life cycleProject work on the skillsKnowledge of Core Java, JavaScript, C++ or software programmingSpring boot, Microservices, Docker, Jenkins and REST API's experienceExcellent written and verbal communication skills
For data Science/Machine learning PositionsRequired Skills Bachelor's degree or Master's degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, ITProject work on the technologies neededHighly motivated, self-learner, and technically inquisitiveExperience in programming language Java and understanding of the software development life cycleKnowledge of Statistics, SAS, Python, Computer Vision, data visualization toolsExcellent written and verbal communication skills
Preferred skills: NLP, Text mining, Tableau, PowerBI, TensorFlowIf you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team.No phone calls please. Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates"
Data Engineer,NauWork,"San Diego, CA (Remote)",https://www.linkedin.com/jobs/view/3776572928/?eBP=JOB_SEARCH_ORGANIC&refId=YoSICnNo%2BWHRXbDyDF1PqQ%3D%3D&trackingId=APfnT4q%2Br2BTz9TnhRYSjg%3D%3D&trk=flagship3_search_srp_jobs,3776572928,"About the job
            
 
A NauWork client is seeking a Data Engineer to join their team. The position is fully remote or hybrid based in San Diego, California.This client is a leading medical staffing agency with a mission to help others live better by helping healthcare professionals and the patients they serve. They’ve received multiple awards and accolades for “Best Places to Work” from companies like Glassdoor and Modern Healthcare.As a Data Engineer II with a specialization in MS Power BI, you will lead the development and maintenance of data-driven solutions, creating compelling visualizations, and ensuring data integrity. Your deep understanding of data infrastructure and data visualization will help drive strategic decisions, streamline operations, and empower our team with actionable insights.Responsibilities:Data Pipeline Development & Management: Design, construct, install, and maintain large-scale processing systems and other infrastructure. Manage and optimize data pipelines, ensuring data availability, accuracy, and optimal performance. 
MS Power BI Development & Management: Develop, maintain, and optimize Power BI dashboards and reports tailored to business needs. Collaborate with stakeholders to identify opportunities for data-driven decision-making and to define metrics and KPIs. Ensure consistency and integrity of data visualizations across all Power BI reports. 
Data Analysis & Optimization: Work with cross-functional teams to gather requirements, understand business challenges, and provide data-driven solutions. Continuously analyze data processes and tools for improvement and scalability. 
Data Governance & Integrity: Collaborate with data governance teams to ensure data quality, compliance, and consistency. Develop and maintain documentation on data pipelines, data models, and data dictionaries. 
Team Collaboration & Leadership: Collaborate with IT, analytics, and business teams to ensure seamless integration of systems and tools. 
Required Experience: 5+ years of experience in data engineering with a strong emphasis on data visualization. Bachelor’s degree in computer science, engineering, information systems, or a related field. Proven expertise in MS Power BI development, including DAX, data modeling, and performance tuning. Strong experience in SQL, ETL processes, and data warehouse design. Familiarity with cloud platforms like Azure or AWS. 
Preferred Experience: Experience in the healthcare or recruiting industry. Familiarity with data governance principles and practices. Excellent communication skills, both written and verbal. Strong analytical and problem-solving skills with a keen attention to detail. 
To Learn More: 503-388-9585 833-NAU-WORK nauwork.com/careers 
Category: Technology - System SoftwarePosition: Data EngineerLocation: [Remote] San Diego, CaliforniaJob Type: Direct-Hire, Full-Time"
Data Engineer with centric application,SPAR Information Systems LLC,United States (Remote),https://www.linkedin.com/jobs/view/3652261474/?eBP=JOB_SEARCH_ORGANIC&refId=YoSICnNo%2BWHRXbDyDF1PqQ%3D%3D&trackingId=%2FZM22x%2FHoT4WhOYYOnStgw%3D%3D&trk=flagship3_search_srp_jobs,3652261474,"About the job
            
 
Hi TeamToday we have Urgent Need Data Engineer REMOTE)Location: RemoteLong Term Data EngineerRequirements4+ years' experience developing data centric applicationsKnowledge on Scala, DataBricks4+ years' experience creating data transformation and validation logicStrong understanding of streaming and batch data processing best practices.Advanced knowledge of data formats (JSON, XML, Parquet, etc.) and data modeling practices.Data modelling experience shaping and transforming data into third normal form and dimensional models.Advanced understanding of best practices for structuring and organizing Data Lake file systems for large volumes of data.Thanks & Regards,Ruchi VermaEmail : Ruchi.Verma@sparinfosys.com"
Data Engineer,HANDLE® Global,"Prospect, KY (Remote)",https://www.linkedin.com/jobs/view/3769118159/?eBP=JOB_SEARCH_ORGANIC&refId=YoSICnNo%2BWHRXbDyDF1PqQ%3D%3D&trackingId=Dg6gkrqHB%2B6O36JmwkLzDw%3D%3D&trk=flagship3_search_srp_jobs,3769118159,"About the job
            
 
Join Us as a Data Engineer - Shape the Future of Healthcare Analytics with HANDLE Global!Company Overview:HANDLE Global, a leading healthcare supply chain analytics and fulfillment solutions provider, is dedicated to revolutionizing the healthcare industry by providing cutting-edge software solutions. Our innovative products help healthcare providers optimize operational efficiency, enhance patient care, and drive business growth.What Sets You Apart: Adventure Awaits: Dive into the cutting-edge world of HANDLE Global. We stand at the vanguard of healthcare supply chain analytics, and we're looking for a Data Engineer who's eager to shape the industry with us.Data Maestro: Spearhead the creation and maintenance of our warehouse data models. As the primary architect and developer for our BI dashboards, you'll ensure our visualizations aren't just accurate, but also insightful, optimizing data extraction and interpretation at every turn.Project Trailblazer:.Embrace the responsibility for ad-hoc analytics and distinct project undertakings. As healthcare evolves, so do our business needs. Your innovative analytics solutions will be instrumental in steering our trajectory.Stakeholder Connector: Liaise seamlessly with internal and external partners, translating business requirements into actionable technical solutions. Your ability to bridge the gap between complex datasets and real-world needs will be invaluable.ELT Enthusiast: Dive into the depths of data source ELT development and maintenance. Your collaborations with our data engineering resources will help streamline customer data pipelines and ensure consistency in our analytics endeavors.Code Curator & Communicator: Own the documentation for your developed code, ensuring it's clear and easily referenced. Beyond that, articulate your findings and methodologies to a diverse audience, making the complex easily comprehensible.Opportunity Harbinger: As challenges arise, seize them. Whether they're on the radar or unforeseen, your ability to navigate, innovate, and drive us forward will set the pace for our growth.
Qualifications and skills:Essentials: Bachelor’s degree in Computer Science, IT, or equivalent combination of education, training, and experience.5+ years of experience with SQL. and 3+ years with Python / SparkIntrigued by the term “modern data stack"" and embody a lifelong learning attitude.Demonstrated prowess in problem-solving.Proficient with cloud data warehouses such as Databricks, Snowflake, BigQuery, etc.Knowledge of data quality standards and schema enforcement.Familiarity with version control platforms like GitLab or Github.Thrives when balancing both analyst and engineering responsibilities. Excellence in documentation - both in interpretation and creation/curation.Enjoys the journey of defining requirements, capturing business logic, and generating value.
Preferred: Agile or startup environment backgroundKnowledge in constructing medallion (bronze, silver, gold) style pipelines.Expertise in query optimizationExperience with dbt, Delta Live Tables, or BI semantics layers for data model building and maintenanceExperience with CI/CD pipelines and platforms like AWS/Azure/GCP.Track record of creating comprehensive data dictionaries.Familiarity with data orchestration tools such as Airflow, Prefect, Dagster, etc.A penchant for DAGS and Mermaid. 
HANDLE Global is an equal opportunity employer committed to valuing and promoting diversity. We base all employment decisions on merit, competence, and business needs, without discrimination based on protected statuses. All qualified applicants are encouraged to apply.Join HANDLE Global, where your expertise will not just shape our company but the future of healthcare analytics. Apply Now and be part of the revolution!Powered by JazzHRoKcqriYu96"
Data Engineer,Care.com,"Denver, CO (Remote)",https://www.linkedin.com/jobs/view/3743206753/?eBP=JOB_SEARCH_ORGANIC&refId=YoSICnNo%2BWHRXbDyDF1PqQ%3D%3D&trackingId=Oe4hg6e0AM2PIj7Es4RHbA%3D%3D&trk=flagship3_search_srp_jobs,3743206753,"About the job
            
 
About Care.comCare.com is a consumer tech company with heart. We're on a mission to solve a human challenge we all face: finding great care for the ones we love. We're moms and dads and pet parents. We have parents and grandparents, so we understand that everyone, at some point in their lives, could use a helping hand. Our culture and our products reflect that.Here, entrepreneurs, self-starters, team players, and big thinkers unite behind a common cause. Here, we're applying data analytics, AI, and the latest technologies to solve universal problems and connect people in new ways. If you like having autonomy, if you thrive on collaboration and building new things, and if you're all about using your talent for good, Care.com is the place for you.What Your Days Will be Like:The Data Engineer will be focusing on building out data feeds and tooling from our application platform and enable rapid ingestion into our centralized Data Lake/Data Warehouse. The Data Engineer will work across business areas and application teams at Care.com to rationalize data and design, build and maintain reusable data feeds which and ultimately empower analytics consumption at Care.com.The ideal candidate will have professional experience building data pipelines in a technical environment. S/he will have an understanding of application development, data warehousing, demonstrate strong business judgment, and be able to prioritize in a fast-paced environment.What You'll Be Working On:  Collaborate and partner with application teams to understand data collection/generation and design and partner to build and implement data feeds from our product tech stackDesign, develop, and build code for rapid feeds and ingestion into the Data WarehouseIdentify data sources used for building out data architecture diagrams/modelsEstablish engineering practices and setup frameworks for ""Data as a Service""Collaborate with relevant delivery teams, including infrastructure, operations, site reliability engineering, product development, and others to perform evaluations, POCs, and ultimately implement and operationalize new technology. Solve code level problems quickly and efficiently Participate in demos and code reviews Promote software best approach, standards, and processes Shape development processes to promote a high-quality output while continuing to iterate quickly Incorporate best practices for security, performance, and data privacy into data pipelines
What You'll Need to Succeed: BS or MS in Computer Science or relevant engineering experience5+ years work experience in Data Engineering/data pipelines3+ years SQL experience is a must1+ years Unix/batch scripting preferred1+ years Python experience is a plus1+ years Windows server admin experience is a plusExperience interfacing with business teams and turning requirements and vision into a technical realityMySQL & Vertica Experience a plus/preferredAWS experience is a plusAbility to drive efforts from start to finish as a self-motivatorKnowledge in Data Warehousing is a MUSTProven ability to maintain performance level in a fast-paced agile environment Pragmatic and realistic with solutions
For a list of our Perks + Benefits, click here!Care.com supports diverse families and communities and seeks employees who are just as diverse. As an equal opportunity employer, Care.com recognizes the power of a diverse and inclusive workforce and encourages applications from individuals with varied experiences, perspectives, and backgrounds. Care.com is committed to providing reasonable accommodations for qualified individuals with disabilities. If you need assistance or accommodation, please reach out to talent@care.com.Company Overview:Available in more than 20 countries, Care.com is the world's leading platform for finding and managing high-quality family care. Care.com is designed to meet the evolving needs of today's families and caregivers, offering everything from household tax and payroll services and customized corporate benefits packages covering the care needs of working families, to innovating new ways for caregivers to be paid and obtain professional benefits. Since 2007, families have relied on Care.com's industry-leading products—from child and elder care to pet care and home care. Care.com is an IAC company (NASDAQ: IAC).Salary Range: 110,000 to 145,000. The base salary range above represents the anticipated low and high end of the national salary range for this position. Actual salaries may vary and may be above or below the range based on various factors including but not limited to work location, experience, and performance. The range listed is just one component of Care.com's total compensation package for employees. Other rewards may include annual bonuses and short- and long-term incentives. In addition, Care.com provides a variety of benefits to employees, including health insurance coverage, life, and disability insurance, a generous 401K employer matching program, paid holidays, and paid time off (PTO)."
Data - Engineer/Scientist/Architect | $130K-$180K | Hybrid AND Remote Work Options,IT Pros,"Mt. Laurel, NJ (Remote)",https://www.linkedin.com/jobs/view/3779181275/?eBP=JOB_SEARCH_ORGANIC&refId=YoSICnNo%2BWHRXbDyDF1PqQ%3D%3D&trackingId=Q5TC5d06YiBsa%2BqBM7hMHQ%3D%3D&trk=flagship3_search_srp_jobs,3779181275,"About the job
            
 
Company DescriptionAccess opportunities with thousands of US-based companies seeking Data - Engineers/Scientists/Architects via IT Pros, a tech recruitment firm with a decade of experience since 2011.Work Location: Hybrid + 100% USA-Remote Work Schedule Options 100% USA-Remote1-2 Days In-Office2-3 Days In-Office3-4 Days In-Office5 Days In-Office
Company Locations: New York City, New YorkLos Angeles, CaliforniaChicago, IllinoisHouston, TexasPhoenix, ArizonaPhiladelphia, PennsylvaniaSan Antonio, TexasSan Diego, CaliforniaDallas, TexasSan Jose, CaliforniaAustin, TexasJacksonville, FloridaSan Francisco, CaliforniaColumbus, OhioIndianapolis, IndianaFort Worth, TexasCharlotte, North CarolinaSeattle, WashingtonDenver, ColoradoWashington, D.C. Boston, MassachusettsRaleigh-Durham, North CarolinaBoulder, ColoradoPortland, OregonAtlanta, Georgia
Compensation: Based on Years of Experience and Accomplishment 3-5 Years = $130,000 - $150,0006-8 Years = $150,000 to $170,0009+ Years = $170,000 to $180,000+
Benefits: Medical, Dental, and Vision InsuranceLife insurance, Long Term Disability, and Short Term DisabilityPaid Time Off (PTO)Plus more... 
Job DescriptionJob descriptions for Data - Engineers/Scientists/Architects positions will be provided upon a successful match.QualificationsSuccessful Data - Engineers/Scientists/Architects will meet the following criteria: Must be located and authorized to work in the United States without any work restrictions, now or in the future. Must have experience with one or more of the following: ETL, SQL, NoSQL, Hadoop, Spark, Kafka, Apache Nifi, Talend, Informatica, AWS, Azure, Google Cloud (GCP), Python, Pandas, R, Matplotlib, Seaborn, Tableau, TensorFlow, PyTorch, AWS Redshift, Azure Synapse. Must have a solid work track record of delivering resultsMust have excellent communication skills
Additional InformationYour application will be reviewed by a real human. Feedback will be provided. Your patience is appreciated. We look forward to having the opportunity to work with you."
Data Engineer,Care.com,"Austin, TX (Remote)",https://www.linkedin.com/jobs/view/3743207672/?eBP=JOB_SEARCH_ORGANIC&refId=YoSICnNo%2BWHRXbDyDF1PqQ%3D%3D&trackingId=NUBhV68cwo%2F%2B0cB49OKeKg%3D%3D&trk=flagship3_search_srp_jobs,3743207672,"About the job
            
 
About Care.comCare.com is a consumer tech company with heart. We're on a mission to solve a human challenge we all face: finding great care for the ones we love. We're moms and dads and pet parents. We have parents and grandparents, so we understand that everyone, at some point in their lives, could use a helping hand. Our culture and our products reflect that.Here, entrepreneurs, self-starters, team players, and big thinkers unite behind a common cause. Here, we're applying data analytics, AI, and the latest technologies to solve universal problems and connect people in new ways. If you like having autonomy, if you thrive on collaboration and building new things, and if you're all about using your talent for good, Care.com is the place for you.Office Locations: (This is a hybrid position) NY, NY 10011Austin, TX 78746Shelton, CT 06484 
What Your Days Will be Like:The Data Engineer will be focusing on building out data feeds and tooling from our application platform and enable rapid ingestion into our centralized Data Lake/Data Warehouse. The Data Engineer will work across business areas and application teams at Care.com to rationalize data and design, build and maintain reusable data feeds which and ultimately empower analytics consumption at Care.com.The ideal candidate will have professional experience building data pipelines in a technical environment. S/he will have an understanding of application development, data warehousing, demonstrate strong business judgment, and be able to prioritize in a fast-paced environment.What You'll Be Working On:  Collaborate and partner with application teams to understand data collection/generation and design and partner to build and implement data feeds from our product tech stackDesign, develop, and build code for rapid feeds and ingestion into the Data WarehouseIdentify data sources used for building out data architecture diagrams/modelsEstablish engineering practices and setup frameworks for ""Data as a Service""Collaborate with relevant delivery teams, including infrastructure, operations, site reliability engineering, product development, and others to perform evaluations, POCs, and ultimately implement and operationalize new technology. Solve code level problems quickly and efficiently Participate in demos and code reviews Promote software best approach, standards, and processes Shape development processes to promote a high-quality output while continuing to iterate quickly Incorporate best practices for security, performance, and data privacy into data pipelines
What You'll Need to Succeed: BS or MS in Computer Science or relevant engineering experience5+ years work experience in Data Engineering/data pipelines3+ years SQL experience is a must1+ years Unix/batch scripting preferred1+ years Python experience is a plus1+ years Windows server admin experience is a plusExperience interfacing with business teams and turning requirements and vision into a technical realityMySQL & Vertica Experience a plus/preferredAWS experience is a plusAbility to drive efforts from start to finish as a self-motivatorKnowledge in Data Warehousing is a MUSTProven ability to maintain performance level in a fast-paced agile environment Pragmatic and realistic with solutions
For a list of our Perks + Benefits, click here!Care.com supports diverse families and communities and seeks employees who are just as diverse. As an equal opportunity employer, Care.com recognizes the power of a diverse and inclusive workforce and encourages applications from individuals with varied experiences, perspectives, and backgrounds. Care.com is committed to providing reasonable accommodations for qualified individuals with disabilities. If you need assistance or accommodation, please reach out to talent@care.com.Company Overview:Available in more than 20 countries, Care.com is the world's leading platform for finding and managing high-quality family care. Care.com is designed to meet the evolving needs of today's families and caregivers, offering everything from household tax and payroll services and customized corporate benefits packages covering the care needs of working families, to innovating new ways for caregivers to be paid and obtain professional benefits. Since 2007, families have relied on Care.com's industry-leading products—from child and elder care to pet care and home care. Care.com is an IAC company (NASDAQ: IAC).Salary Range: 110,000 to 145,000. The base salary range above represents the anticipated low and high end of the national salary range for this position. Actual salaries may vary and may be above or below the range based on various factors including but not limited to work location, experience, and performance. The range listed is just one component of Care.com's total compensation package for employees. Other rewards may include annual bonuses and short- and long-term incentives. In addition, Care.com provides a variety of benefits to employees, including health insurance coverage, life, and disability insurance, a generous 401K employer matching program, paid holidays, and paid time off (PTO)."
Expression of Interest: Data Engineer,Fingerprint for Success (F4S),"San Diego, CA (Remote)",https://www.linkedin.com/jobs/view/3699994817/?eBP=JOB_SEARCH_ORGANIC&refId=YoSICnNo%2BWHRXbDyDF1PqQ%3D%3D&trackingId=nqt7QnhXQr3UBN97AaW0FQ%3D%3D&trk=flagship3_search_srp_jobs,3699994817,"About the job
            
 
We are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.The F4S Talent Pool is a pilot project designed to: Help job seekers get discovered by our partners based on their anticipated hiring needs.Provide optional support and resources for job seekers in their career endeavors.Help individuals understand, and bring out the best in themselves and each other.
The F4S Talent Pool process:Once you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.About Fingerprint For Success (F4S)Backed by 20+ years of research, F4S’s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high performing teams.Keep in mind, joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your F4S work style assessment to match the hiring needs of our partners.Your feedback is a gift! Write to us via:Powered by JazzHRzGyQ3I4owM"
Data Engineer,VitalSource,"Raleigh, NC (Remote)",https://www.linkedin.com/jobs/view/3755877673/?eBP=JOB_SEARCH_ORGANIC&refId=YoSICnNo%2BWHRXbDyDF1PqQ%3D%3D&trackingId=Ubqpk1ye9QG%2B1Zyfo9GjgA%3D%3D&trk=flagship3_search_srp_jobs,3755877673,"About the job
            
 
VitalSource®, is hiring a Data Engineer to contribute to our Engineering team located in Raleigh (hybrid) or remotely.VitalSource is looking to add two Data Engineers to our growing Engineering Department. In this role, you will focus on maintaining data architectures. The Data Engineer role is essential in ensuring the smooth flow and accessibility of data, as well as transforming this data into actionable insights, requiring deep analytical expertise. They manage and optimize data artifacts to handle and query data safely and efficiently, working with application engineers, data analysts, and data architects.VitalSource is a mission-driven company delivering affordable, impactful learning to anyone, anywhere.Required Qualifications:  Bachelor’s degree in Computer Science, Engineering, or related field.  5+ years of experience in a data engineering role with a focus on analytics.  Bachelor's or master’s degree in Computer Science, Data Science, or a related field.  Experience with relational SQL and NoSQL databases.  Strong proficiency with SQL, data modeling, and data engineering.  Strong analytical and statistical skills.  Experience with Cloud Platforms, in particular GCP.  Experience with Cloud Data Platforms, in particular Snowflake.  Strong problem-solving and analytical skills.  Excellent communication and collaboration skills.  Ability to communicate complex data insights in a clear and actionable manner to non-technical stakeholders. 
Preferred Skills:  Snowflake and GCP Experience with data governance and data management best practices Experience working with remote engineering teams Familiar with software development and project management frameworks
Key Responsibilities:  Develop, construct, test, and maintain data architectures emphasizing analytical and reporting capabilities. Design and implement systems that collect, transform, and store large amounts of structured and unstructured data. Collaborate with business stakeholders to understand their problems and design data-driven solutions. Optimize data retrieval processes for analysis and reporting. Work closely with business analysts to ensure data quality and timely delivery of insights. Ensure architecture supports the analytical requirements of the business, allowing for scalable analytics and machine learning use cases. Monitor data systems to ensure data integrity and quality. Stay current with industry best practices and technologies related to data engineering and analytics. 
Salary Range: $100,000 to $120,000What We Offer:  Culture: Collaborative, Inclusive, and Mission-driven  More in your pocket: Competitive base salary and a strong variable component  We take care of all aspects of our people: Generous, well-rounded benefits such as Medical, Vision, Dental, Life, Disability, Critical Illness, Accident, FSA, HSA, ID Protection, Pet and Legal Insurance  Retirement:401K match up to 5%  We support our families: 12 weeks of paid parental leave Continued education: Use our tuition reimbursement program The Importance of Balance: Start at 4 weeks’ vacation, 12 sick days, 10 company holidays, and 3 personal days,  Flexibility: Flexible work schedules and remote capabilities (by team) - feel free to skip the commute and hit your deadlines from home Casual Dress: Be as comfortable at work as you are in your own living room Wellness: Lots of opportunities for fitness challenges and rewards
Who We Are:VitalSource is the leading education technology solutions provider committed to helping partners create, deliver, and distribute affordable, accessible, and impactful learning experiences worldwide. As a recognized innovator in the digital course materials market, VitalSource is best known for partnering with more than 1,000 publishers and resellers to deliver extraordinary learning experiences to millions of active users globally—and today we’re also powering new, cutting-edge technologies designed to optimize teaching and learning for maximum results. Learn more at https://vitalsource.com and follow us on Twitter, LinkedIn, and Instagram.All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, work related mental or physical disability, veteran status, sexual orientation, gender identity, or genetic information. EEO/AA Employer/Vet/DisabledWe participate in EVerify. EEO Poster in English EEO Poster in Spanish"
Expression of Interest: Data Engineer,Fingerprint for Success (F4S),"New York, NY (Remote)",https://www.linkedin.com/jobs/view/3699917033/?eBP=JOB_SEARCH_ORGANIC&refId=YoSICnNo%2BWHRXbDyDF1PqQ%3D%3D&trackingId=gMbNssaKJxLeE3Z4%2FxMiHQ%3D%3D&trk=flagship3_search_srp_jobs,3699917033,"About the job
            
 
We are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.The F4S Talent Pool is a pilot project designed to: Help job seekers get discovered by our partners based on their anticipated hiring needs.Provide optional support and resources for job seekers in their career endeavors.Help individuals understand, and bring out the best in themselves and each other.
The F4S Talent Pool process:Once you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.About Fingerprint For Success (F4S)Backed by 20+ years of research, F4S’s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high performing teams.Keep in mind, joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your F4S work style assessment to match the hiring needs of our partners.Your feedback is a gift! Write to us via:Powered by JazzHRoWuPJ9lwq9"
Expression of Interest: Data Engineer,Fingerprint for Success (F4S),"San Jose, CA (Remote)",https://www.linkedin.com/jobs/view/3699913334/?eBP=JOB_SEARCH_ORGANIC&refId=YoSICnNo%2BWHRXbDyDF1PqQ%3D%3D&trackingId=5pR2Rykh7TehH6rnj01Iyg%3D%3D&trk=flagship3_search_srp_jobs,3699913334,"About the job
            
 
We are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.The F4S Talent Pool is a pilot project designed to: Help job seekers get discovered by our partners based on their anticipated hiring needs.Provide optional support and resources for job seekers in their career endeavors.Help individuals understand, and bring out the best in themselves and each other.
The F4S Talent Pool process:Once you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.About Fingerprint For Success (F4S)Backed by 20+ years of research, F4S’s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high performing teams.Keep in mind, joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your F4S work style assessment to match the hiring needs of our partners.Your feedback is a gift! Write to us via:Powered by JazzHRy5nDhBQ2nX"
Staff Data Engineer,Garner Health,"New York, NY (Remote)",https://www.linkedin.com/jobs/view/3779121987/?eBP=JOB_SEARCH_ORGANIC&refId=YoSICnNo%2BWHRXbDyDF1PqQ%3D%3D&trackingId=ik9xj8fqV55G1xwBsFS12w%3D%3D&trk=flagship3_search_srp_jobs,3779121987,"About the job
            
 
Garner's mission is to transform the healthcare economy, delivering high quality and affordable care for all. By helping employers restructure their healthcare benefit to provide clear incentives and data-driven insights, we direct employees to higher quality and lower cost healthcare providers. The result is that patients get better health outcomes while doctors are rewarded for practicing well, not performing more procedures. We are backed by top-tier venture capital firms, are growing rapidly and looking to expand our team.We are looking for a hands-on Staff Data Engineer to join our data engineering team and drive the implementation of pipelines which deliver the insights core to our business. The ideal candidate should have experience with a modern data stack and implementations of data pipelines using Python in AWS. They should be excited about collaborating with data scientists to bring their research to life.Main Responsibilities: Build and maintain data pipelines delivering core insightsCollaborate with data scientists to productionize research resultsProtect our users' privacy and security through best practicesDelivering insights via our data warehouseSupport pipelines in productionAssist in task planning, estimation, scheduling, and staffingMentor junior and mid-level engineersGrow engineering teams by interviewing, recruiting, and hiring 
Our Tools: Python, AWS, Terraform, Snowflake, Git
Ideal Qualifications: 7+ years hands-on work building data pipelines and internal applicationsExpertise in PythonComfortable leading research and development projects that produce new designs, products, and processesComfortable checking the team's work for technical accuracyAbility to coordinate work with managers and other staffExperience working with an orchestration tool such as Airflow, Argo, Prefect, or DagsterFamiliarity with healthcare or insuranceExperience with one or more database warehouses, especially Snowflake
Why You Should Join Our Team: You are mission-driven and want to work at a company that can change the healthcare systemYou want to be on a small, fast-paced team that nimbly moves to meet new challengesYou love ideating on new features and working with data to find new insightsYou're excited about researching and working with the latest tools and technologies
The target salary range for this position is $180,000 - $220,000 annually. Individual compensation for this role will depend on a variety of factors including qualifications, skills and applicable laws. In addition to base compensation, this role is eligible to participate in our equity incentive and competitive benefits plans.Garner Health is proud to be an Equal Employment Opportunity employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, reproductive health decisions, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, genetic information, political views or activity, or other applicable legally protected characteristics.Garner Health is committed to providing accommodations for qualified individuals with disabilities in our recruiting process. If you need assistance or an accommodation due to a disability, you may contact us at talent@getgarner.com."
Data Engineer - Remote | WFH,Get It Recruit - Information Technology,"New York, NY (Remote)",https://www.linkedin.com/jobs/view/3772288615/?eBP=JOB_SEARCH_ORGANIC&refId=YoSICnNo%2BWHRXbDyDF1PqQ%3D%3D&trackingId=kVWL4f5o58hIZmVMmEbJew%3D%3D&trk=flagship3_search_srp_jobs,3772288615,"About the job
            
 
We are a technology-enabled leader in providing flexible financing solutions to small and medium-sized businesses. Since our establishment in 2008, we've supported over 35,000 merchants nationwide, delivering more than $3 billion in capital for various purposes, including funding daily operating expenses, managing cash flow gaps, and facilitating business expansion. Our proprietary technology allows us to streamline the financing process, providing capital to our customers in as little as 24 hours, a significant improvement compared to the traditional weeks or even months it takes with traditional bank loans.Over the past fourteen years, our company has evolved from a small Manhattan workspace with two founders to a company of nearly 200 employees, with offices in New York and Miami. We take pride in being a partner-centric organization, offering a tailored funding approach and collaborating closely with our small-business customers to understand their needs in a dynamic economic environment.We're currently seeking an experienced Data Engineer to play a crucial role in our enterprise data initiative. In this role, you'll collaborate with cross-functional teams to ensure the availability, integrity, and reliability of our data, empowering our organization to extract valuable insights and drive business success.ResponsibilitiesDesigning and implementing data ingestion pipelines from multiple sources using Azure Databricks and Snowflake Snow-Pipes.Developing scalable and reusable frameworks for ingesting data sets.Integrating and maintaining the end-to-end data pipeline, ensuring data quality and consistency at all times.Maintaining data standardization and deduplication during the ingestion process.Evaluating the performance and applicability of multiple tools against business stakeholder requirements.Monitoring and tuning data pipelines for optimal performance and efficiency.Collaborating with cross-functional teams, including data scientists, analysts, and business stakeholders, to understand data requirements and provide technical solutions.Documenting data engineering processes, data flows, and architecture.What You Should Have8+ years of data engineering experience with a proven record of integrating and standardizing data for use in a centralized database.Expertise in designing and deploying data applications on cloud solutions, such as Azure or AWS.Expert knowledge of SQL.Experience with data modeling, ETL development, and data integration.Hands-on experience in performance tuning and optimizing code running in a Databricks environment.Proficiency in programming languages like Pyspark and Python or equivalent technologies used for data manipulation and transformation.Demonstrated analytical and problem-solving skills, particularly in a big data environment.Familiarity with data warehousing concepts and best practices.What We Would Love You To HaveFamiliarity with Snowflake and Snow-SQL.Exposure to data analytics tools and BI Tools, preferably Tableau.Familiarity with data lakes and storage solutions in Azure or AWS.NoSQL experience.Salesforce exposure.Why us?Competitive salary with growth potential.Casual yet professional hybrid work environment.Convenient midtown Manhattan location with easy access to parks, shows on Broadway, and other entertainment.Comprehensive benefits package, including medical, dental, vision, 401(k) with company match, gym reimbursement, life insurance, and more.Generous time-off plan.Flexible spending and commuter benefits.Hybrid working model, offering employees the option to work from home two days a week.Our company is an Equal Opportunity Employer.Job Type: Full-timePay: $110,000.00 - $160,000.00 per yearBenefits401(k) matchingDental insuranceEmployee assistance programEmployee discountHealth insuranceLife insurancePaid time offVision insuranceSchedule: Monday to FridayWork Location: Hybrid remoteIf you are passionate about leveraging data to drive business success and have expertise in Azure Databricks, we invite you to join our dynamic team. Apply now to be a key contributor to our data-driven journey!Employment Type: Full-Time"
Expression of Interest: Data Engineer,Fingerprint for Success (F4S),"Los Angeles, CA (Remote)",https://www.linkedin.com/jobs/view/3754141621/?eBP=JOB_SEARCH_ORGANIC&refId=YoSICnNo%2BWHRXbDyDF1PqQ%3D%3D&trackingId=iNovcOe%2FZ5FLv2eZVK9sNg%3D%3D&trk=flagship3_search_srp_jobs,3754141621,"About the job
            
 
We are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.The F4S Talent Pool is a pilot project designed to: Help job seekers get discovered by our partners based on their anticipated hiring needs.Provide optional support and resources for job seekers in their career endeavors.Help individuals understand, and bring out the best in themselves and each other.
The F4S Talent Pool process:Once you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.About Fingerprint For Success (F4S)Backed by 20+ years of research, F4S’s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high performing teams.Keep in mind, joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your F4S work style assessment to match the hiring needs of our partners.Your feedback is a gift! Write to us via:Powered by JazzHRZuHeAtwB0l"
Senior Data Engineer,Second Dinner,"Irvine, CA (Remote)",https://www.linkedin.com/jobs/view/3767052785/?eBP=JOB_SEARCH_ORGANIC&refId=YoSICnNo%2BWHRXbDyDF1PqQ%3D%3D&trackingId=hHZPBx5%2B%2FwLUlP%2F5CuVhQA%3D%3D&trk=flagship3_search_srp_jobs,3767052785,"About the job
            
 
Who We AreSecond Dinner is an award-winning independent game development studio that is here to make the most fun games in the world. Not super fun games. Not SUPER DUPER fun games. We mean the MOST fun games. In fact, our game MARVEL SNAP has earned multiple Mobile Game of the Year Awards (Game Awards, DICE), Best Strategy Game (IGN), and the Apple Design Award for Innovation!Second Dinner is a remote-first studio, so while we are headquartered in Irvine, California, most of our team is fully remote across the United States. We want the most talented teammates wherever they call home. A diverse team with varied perspectives makes us a better company and will help us make better games. If you can bring something new to the table and expand our point of view, that's a huge upside.Our Data TeamAt Second Dinner, data plays a crucial role in conveying the voices of our players and informing our decisions, which leads us to great games and player experiences. Our team enables decision-making with scientific and methodological rigor for Marvel SNAP and our new projects. We partner with teams across the studio to build data-powered player experiences directly into the games. We innovate in analytics tooling and data engineering capabilities to redefine what is possible in game development and operations.Your RoleYou will report to the Data Engineering Lead. You will play a critical role in shaping data-driven insights across our organization. You will develop and operate data pipelines to empower analytics, data science, marketing, product management, and design teams to create incredible player experiences. You will build and operate large-scale cloud data infrastructure. You will collaborate with cross-functional teams to ensure the collection and serving of timely high-quality data. Moreover, you will partner with the AI team to innovate in self-service analytics tools. If you are passionate about building high-impact core capabilities to help craft world-class game experiences and are excited to influence millions of players by leveraging your expertise in data engineering, then APPLY!!!What You'll Do: Develop and operate data infrastructure and pipelines to enable robust data for analytics and reporting in Marvel SNAPEmpower the SNAP Marketing team with high-quality data to improve user acquisition (UA)Enable SNAP Product Management, Analytics, Design, Engineering, and Production teams to gain insights from analytics quicklyPartner with data scientists, analysts, and software engineers to ensure high-quality and relevant data collectionPartner with the AI team to innovate, develop, and operate self-service analytics tooling
What You’ll Need: Extensive expertise with data infrastructure and data engineeringDemonstrated experience in large-scale distributed data systems (Spark, Flink)Deep expertise in analytical database technologies (SQL and NoSQL)Proficiency in SQL and PythonExperience with database technologies and ETL/ELTExperience with orchestration and automation tools (Airflow, Beam)Experience with Looker or TableauFamiliarity with Databricks and data/analytics solutions on AWSExperience with marketing platforms and data tools (Braze, AppsFlyer)Demonstrated success in a highly collaborative cross-functional work environmentPassionate player of mobile gamesMindset for serving a diverse and global player base
Nice to Have But Not Necessary: Experience working in online video games, preferably mobile free-to-play gamesExperience in .NET/C# and backend software developmentExperience in building and integrating data pipelines for customer-facing live services (e.g., recommenders)Experience in operational database and storage technologies (e.g., DynamoDB, Cassandra, and Redis)
The total compensation for this position includes a new hire offer base salary range of $130,000 - $210,000 USD + equity + comprehensive benefits + potential for discretionary performance bonuses.Individual pay within this salary range may span multiple levels within the discipline and is determined by assessed job-related skills, experience, relevant education or training. It also factors in market demands and business needs. The disclosed range is not adjusted based on location and may be subject to change or modification based on business needs in the future. Your recruiter can answer any questions about new hire total compensation during the hiring process.An overview of the benefits and perks at Second Dinner: Medical, Dental, and Vision insurance plans with Second Dinner paying 100% of premiums for employees and 75% for dependents for many plans401(k) contribution with no waiting period16 weeks paid parental leave with no waiting periodHome office improvement bonusPaid Vacation & Sick timeRemote-first with core overlap hours between 10AM and 4PM PTCompany Winter Holiday shutdown (Dec 25-Jan 1)Company Summer Holiday shutdown (week of July 4)Company Events - In-person Summer all-hands gathering, in-person holiday party, annual camping event, and virtual events throughout the year
We are an equal opportunity employer that places high value on diversity and inclusion. We do not discriminate on the basis of race, color, ancestry, national origin, religion, age, disability status, sex (including pregnancy), gender, gender identity, gender expression, sexual orientation, medical condition, genetic information, marital status, military status, or veteran status.You must be eligible to work in the United States to be considered for this position."
Founding Data Engineer,Rogo,"New York, NY (Remote)",https://www.linkedin.com/jobs/view/3749759981/?eBP=JOB_SEARCH_ORGANIC&refId=vqykNdXaS0b%2BjMByk%2BKfOg%3D%3D&trackingId=VMYxHk5TmIsbUbeVeWwg7A%3D%3D&trk=flagship3_search_srp_jobs,3749759981,"About the job
            
 
Why Rogo?Rogo will be the biggest Financial Services Artificial Intelligence company in the world. We're creating a category-defining AI company built on top of foundational AI models like GPT-4. Exceptional early users: high-paying contracts with the world's largest investment banks, hedge funds, private equity firms, and consultants. Massive demand: extensive waitlist of firms waiting for deployment. World-class team: we take talent density very seriously. We like working with incredibly smart, driven people. Cutting-edge technology: Work directly with the world's most advanced LLMs, AI, and RAG to build the future of generative AI and redefine finance. Top-of-market cash and equity compensation. 
Challenges:We are building systems that can automate the most complex knowledge work in the world, e.g., financial analysis, research, due diligence, and more. Creating financial research that's worth paying attention to: aggregating, analyzing, and producing insights from real-time information. Say goodbye to equity research. Dealing with the most sensitive data in the world: client data from the largest financial services companies on earth. Working past the edge of published AI research: tackling problems beyond the complexity of existing AI benchmarks. Unsolved product, architectural, and business problems: natural language interfaces, prohibitively expensive evaluation of models, massive marginal costs, versioning/training/segregating models per task, client, and so on. 
As a founding Data Engineer at Rogo, you will help build out our real-time data pipelines for millions of unstructured financial documents to feed our financial LLM. It’s cutting-edge data engineering at the AI frontier.Hard Requirements: 3+ years of industry experience as a data engineerHighly proficient with Python and SQL, and an intuitive understanding of multi-threading, multi-processing, asyncio, and other concurrency primitivesExperience with at least one of: Postgres, Snowflake or ElasticsearchExperience deploying and monitoring mission-critical ETL pipelines with large and heterogenous datasourcesExperience working with Apache AirflowExperience with AWS or other cloud environment
Bonus Requirements: Experience at a hypergrowth startupFinancial Services work experienceExperience with TypescriptExperience with stream processingKnowledge of Datadog and other Telemetry tooling
You'll fit in at Rogo if... You have fun solving hard problems: we're tackling tech/product/business problems that are unsolved. It's super exciting. You like to work hard: we feel lucky to work on these problems, and we enjoy pouring our all into solving them. You care deeply about talent density: we care deeply about working with people who are super smart and motivated. You have eclectic interests: whether you're a sci-fi aficionado, history buff, strategy game guru, policy wonk, or movie trivia expert, you'll find kindred spirits here. 
Compensation Range: $120K - $160K"
"Data Engineer at Phoenix, AZ",IVY TECH SOLUTIONS INC,"Chicago, IL (Remote)",https://www.linkedin.com/jobs/view/3667178222/?eBP=JOB_SEARCH_ORGANIC&refId=vqykNdXaS0b%2BjMByk%2BKfOg%3D%3D&trackingId=FBiBqLnVtpiJtf3aoEOMHQ%3D%3D&trk=flagship3_search_srp_jobs,3667178222,"About the job
            
 
HI,Kindly let me know if you have a suitable fit for the following positionThanksData Engineer Location: Phoenix, AZDuration: 12+MonthsInitially RemoteONLY W2Please send the resume to  or 847- 350-1008The data engineer is a critical role that will provide data engineering design, ETL development and technical expertise to an ETL scrum team. They will work on a team with other ETL developers, QA engineers and analysts to support data streams for payment, claim and customer service operations. The team is responsible for a large catalog of jobs that use a mixture of batch ETL architecture and real-time data streaming with API-integrated data services. The data engineer will spend their time doing hands on development, designing future data processes, conducting data analysis, consulting with other teams and interacting throughout the Agile process in a stable scrum team environment.Essential Functions / Principal Responsibilities Develops data pipelines in both batch ETL and real-time streaming architectures.Develops data models to define new or modify existing data structures in support of data integration initiatives.Provides expert technical knowledge of data solutions for business projects.Provides source system analysis, data discovery, complex transformation assessment and target system exploration to understand information data requirements and anticipate user needs.Contributes to data pipeline design, coding, and technical / functional reviews while collaborating with source system developers, data engineers and functional subject matter experts.Develops effective data pipeline solutions to deliver business features.Adheres to best practices for data movement, data quality, data profiling, data cleansing and other data pipeline related activities.Applies tuning and optimization for continuous improvement.Presents technical information in easily understood terms (written, verbal and visual).Communicates effectively within the Agile team and to external stakeholders and management.Follows Agile best practices and adheres to internal IT processes like change management and problem management.
Skills that will Ensure Success: Specialist in ETL development with a demonstrated understanding of transactional data processing, streaming data and data pipeline best practices.Experience in build, unit test, and deployment of Informatica ETL processes.Knowledgeable in making REST API calls within data processes.Familiar with real-time data pipeline platforms, preferably StreamSets, AWS Glue or similar platform.Hands on experience with data streaming in Apache Kafka.Able to interpret business needs and turn them into a technical plan of attack with pros and cons of various approaches to the data processing options.Demonstrates a solid understanding of technical standards and processes related to batch and real-time data pipeline development.Excellent team player, able to work with product owners, technical developers, DBAs, system administrators, BI professional services, data warehouse operations and functional experts.Expertise in SQL query transactions and optimization, especially T-SQL.Understand nulls, cardinality, joins, data types to develop technical ETL specifications and technical metadata.Ability to integrate an application solution into the broader business and IT ecosystem in which it will operate.Firm understanding of quality assurance activities and automation in data pipeline and ETL processing.Desire experience working with financial and/or claims data requiring compliance, balancing and integrity checks, especially payment-related data, PCI compliant data and banking industry formats such as NACHA.Desire a firm understanding of cloud data processing and data streaming architectures, especially in AWS.
Charan Kumar | IVY Tech Sols Inc.3403 N Kennecott Avenue, Suite B&C Arlington Heights, IL 60004( Direct: (847) 350-1008   |Gtalk : charan.ivytech|
Powered by JazzHR19EMA7VJlz"
Azure / Cosmos Data Engineer,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3769295575/?eBP=JOB_SEARCH_ORGANIC&refId=vqykNdXaS0b%2BjMByk%2BKfOg%3D%3D&trackingId=Z3gobhBEsmujpw6kbIUmgQ%3D%3D&trk=flagship3_search_srp_jobs,3769295575,"About the job
            
 
remote role, CST work hoursMust have healthcare domain expneed only 1 strong candidateneed Valid LinkedInTop Skill Sets/ Experiences Required Primary Skillset: Expert in Cosmos / Azure, recommend architecture changes to stay with changing system demandsSecondary Skillset: Data Engineering / API ArchitecturesIndustry Experience: Health Experience 
Your Impact Combine your technical expertise and problem-solving passion to work closely with clients, turning complex ideas into end-to-end solutions that transform our clients’ businessTranslate clients requirements to system design and develop a solution that delivers business valueLead, design, develop and deliver large-scale data systems, data processing and data transformation projectsAutomate data platform operations and manage the post-production system and processesConduct technical feasibility assessments and provide project estimates for the design and development of the solutionMentor, help and grow junior team members
QualificationsYour Skills & Experience: Demonstrable experience in data platforms involving implementation of end to end data pipelines Good communication and willingness to work as a teamHands-on experience with at least one of the leading public cloud data platforms (Amazon Web Services, Azure or Google Cloud) Implementation experience with column-oriented database technologies (i.e., Big Query, Redshift, Vertica), NoSQL database technologies (i.e., DynamoDB, BigTable, Cosmos DB, etc.) and traditional database systems (i.e., SQL Server, Oracle, MySQL)Experience in implementing data pipelines for both streaming and batch integrations using tools/frameworks like Glue ETL, Lambda, Google Cloud DataFlow, Azure Data Factory, Spark, Spark Streaming, etc. Ability to handle module or track level responsibilities and contributing to tasks “hands-on”Experience in data modeling, warehouse design and fact/dimension implementationsExperience working with code repositories and continuous integration"
Expression of Interest: Data Engineer,Fingerprint for Success (F4S),"San Antonio, TX (Remote)",https://www.linkedin.com/jobs/view/3699993860/?eBP=JOB_SEARCH_ORGANIC&refId=vqykNdXaS0b%2BjMByk%2BKfOg%3D%3D&trackingId=JA1P%2Fnf0h%2Bkvp3mr3swTPg%3D%3D&trk=flagship3_search_srp_jobs,3699993860,"About the job
            
 
We are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.The F4S Talent Pool is a pilot project designed to: Help job seekers get discovered by our partners based on their anticipated hiring needs.Provide optional support and resources for job seekers in their career endeavors.Help individuals understand, and bring out the best in themselves and each other.
The F4S Talent Pool process:Once you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.About Fingerprint For Success (F4S)Backed by 20+ years of research, F4S’s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high performing teams.Keep in mind, joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your F4S work style assessment to match the hiring needs of our partners.Your feedback is a gift! Write to us via:Powered by JazzHRo33Cn9hdYx"
Expression of Interest: Data Engineer,Fingerprint for Success (F4S),"Chicago, IL (Remote)",https://www.linkedin.com/jobs/view/3699993863/?eBP=JOB_SEARCH_ORGANIC&refId=vqykNdXaS0b%2BjMByk%2BKfOg%3D%3D&trackingId=Kjf%2BNEflZIlazdroxlqWLQ%3D%3D&trk=flagship3_search_srp_jobs,3699993863,"About the job
            
 
We are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.The F4S Talent Pool is a pilot project designed to: Help job seekers get discovered by our partners based on their anticipated hiring needs.Provide optional support and resources for job seekers in their career endeavors.Help individuals understand, and bring out the best in themselves and each other.
The F4S Talent Pool process:Once you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.About Fingerprint For Success (F4S)Backed by 20+ years of research, F4S’s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high performing teams.Keep in mind, joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your F4S work style assessment to match the hiring needs of our partners.Your feedback is a gift! Write to us via:Powered by JazzHRvZYkG83TOg"
Data Engineer - Remote,"RIT Solutions, Inc.","Phoenix, AZ (Remote)",https://www.linkedin.com/jobs/view/3768014589/?eBP=JOB_SEARCH_ORGANIC&refId=vqykNdXaS0b%2BjMByk%2BKfOg%3D%3D&trackingId=IsfXxsTiCVHcbA6tAYqloQ%3D%3D&trk=flagship3_search_srp_jobs,3768014589,"About the job
            
 
Duration: 6+ months CTHMust have a valid LinkedIn profileOur client is looking to hire for a Data Engineer in support of a migration to Snowflake. This person needs to have prior experience migrating to Snowflake and strong SSIS. Sr. Data EngineerRequirements: Strong with SSIS and Exposure to Snowflake PlatformPrior healthcare experience (Medicare, Medicaid)The position is remote but the candidate has to live in one of the following states. Preferably closer to the west coast due to the PST schedule.AZ, CO, NV, FL, ID, OR, WY"
Remote - Data Scientist/Analyst/Engineer(ENTRY LEVEL),SynergisticIT,"Indianapolis, IN (Remote)",https://www.linkedin.com/jobs/view/3774169587/?eBP=JOB_SEARCH_ORGANIC&refId=vqykNdXaS0b%2BjMByk%2BKfOg%3D%3D&trackingId=rNdZa4girEHlB0gGmxg%2FEA%3D%3D&trk=flagship3_search_srp_jobs,3774169587,"About the job
            
 
At SynergisticIT, we're all about making connections. Whatever IT goals you have, our software programmers can help achieve those. Our Software development teams can take up turnkey projects and execute them in an effective and efficient manner. If you are looking to source talent our recruiters will find the ideal IT talent for your company. What's the secret to our success? Well, it all starts with taking quality time to listen to each client's specific needs. After we have a thorough grasp of your IT goals, we can better customize our Developments as per your specific needs. We can also tailor make recruiting programs to exceed your expectations. Since our founding in 2010, SynergisticIT's strategies have earned the company an enviable position in the software development, IT staffing and IT skill enhancement fields. SynergisticIT continues to work with hundreds of satisfied American clients with our software programmers working on our projects and after gaining hands on experience on cutting edge technologies moving to contribute their skills to great clients like Apple, Google, Client, Ebay, Paypal, Kroger, the Walt Disney Company and hundreds more. If you are tired of working with inefficient programmers who take a lot of time to ramp up we want you to try us. Our software programmers can hit the ground running and get you the maximum return on your investment. You have already tried the rest its time you tried the best. SynergisticIT - Home of the Best Data Scientists and Software Programmers in the Bay Area.Why Us ? SynergisticIT has a proven track record of successfully skill enhancement and staffing IT employees for some of the world's most iconic brands. Our team takes the time to fully understand every client's needs so we could best meet your IT staffing requirements. The knowledgeable staff at SynergisticIT is always more than happy to work with clients to ensure they reach their software development goals. Besides staffing, SynergisticIT is also committed to helping young IT professionals advance their career with a robust upskill program . Everyone who goes through SynergisticIT's program learns all the skills necessary to succeed in many IT fields ranging from Java to Machine Learning. Additionally, everyone trained at SynergisticIT has been through extensive mock and technical interview screenings to bolster their career prospects. Last, but certainly not least, SynergisticIT takes great care to respect the privacy considerations for every client. All companies who work with SynergisticIT can rest assured their confidential data is protected using the most up-to-date encryption technologies. SynergisticIT also complies with all the latest NDA agreements.REQUIRED SKILLS For Java /Software Programmers  Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT Highly motivated, self-learner, and technically inquisitive Experience in programming language Java and understanding of the software development life cycle Project work on the skills Knowledge of Core Java , javascript , C++ or software programming Spring boot, Microservices, Docker, Jenkins and REST API's experience Excellent written and verbal communication skills 
For data Science/Machine learning Required Skills Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT Project work on the technologies needed Highly motivated, self-learner, and technically inquisitive Experience in programming language Java and understanding of the software development life cycle Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools Excellent written and verbal communication skills 
 Preferred skills: NLP, Text mining, Tableau, PowerBI, Time series analysis We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2022 and at Gartner Data Analytics Summit (Florida)-2023 Oracle CloudWorld Event (OCW) Las Vegas 2022 | SynergisticIT - YouTubehttps://www.youtube.com/watch?v=OAFOhcGy9Z8 https://www.youtube.com/watch?v=EmO7NrWHkLM https://www.youtube.com/watch?v=NVBU9RYZ6UI https://www.youtube.com/watch?v=Yy74yvjatVg SynergisticIT at Gartner Data and Analytics Summit 2023 - YouTubeFor preparing for interviews please visit  https://www.synergisticit.com/interview-questions/ We are looking for the right matching candidates for our clients Please apply via the job posting REQUIRED SKILLS For Java /Full Stack/Software Programmer  Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT Highly motivated, self-learner, and technically inquisitive Experience in programming language Java and understanding of the software development life cycle Project work on the skills Knowledge of Core Java , javascript , C++ or software programming Spring boot, Microservices, Docker, Jenkins and REST API's experience Excellent written and verbal communication skills 
For data Science/Machine learning Positions Required Skills Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT Project work on the technologies needed Highly motivated, self-learner, and technically inquisitive Experience in programming language Java and understanding of the software development life cycle Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools Excellent written and verbal communication skills 
 Preferred skills: NLP, Text mining, Tableau, PowerBI, Tensorflow If you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team. No phone calls please.  Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates"
BI Data Engineer,Publishing.com,"Austin, TX (Remote)",https://www.linkedin.com/jobs/view/3752272754/?eBP=JOB_SEARCH_ORGANIC&refId=vqykNdXaS0b%2BjMByk%2BKfOg%3D%3D&trackingId=DDiGusITHSP6BUOkTG3u%2Bw%3D%3D&trk=flagship3_search_srp_jobs,3752272754,"About the job
            
 
Reports toHead of EngineeringSummary:Publishing.com has helped thousands of normal everyday people to become successful self-published authors. And along the way, we also became one the most successful companies in the US (Want to be at the forefront of the AI revolution? Join us! We are building the most comprehensive AI-powered self-publishing platform and you get to join us at ground zero. Instead of just teaching people how to create successful books, we are going to help them do it.About you:You are a data engineer with strong analytical skills and hands-on experience with modern data warehousing and business intelligence solutions. You know how to use code and no-code to ingest data from various sources, how to interpret data and translate it into business insight, and how to prepare reports and dashboards that are easy to understand and digest.And you have a great attitude!About this role:As our first data engineer, you will have the opportunity to make important contributions to various aspects of our data platform. Your main responsibilities are: Build a highly scalable data warehousePropose, design, and implement data ingestion pipelines (ELT/ETL)Maintain our local and cloud data platformsUnderstand and interpret business intelligence requirements and translate them into technical solutionsBuild business analytics and dashboards to address sales and marketing needs
Required skills: Strong problem solving skillsStrong communication skillsStrong SQL skillsExpert in using data warehousing solutions such as BigQuery, Snowflake, or DatabricksExperience with data ingestion services such as Fivetran, Matilion, Segment, or similarExperience with Google SheetsExperience with business analytics for marketing and salesStrong programming skills in JavaScript and PythonExperience with HubSpotExperience with GitExperience with agile developmentExperience working with marketing and sales teamsStrong sense of ownership
Preferred Skills: Experience with Google CloudsExperience building CI/CD pipelinesExperience with AWS, Azure, or GCPExperience with Terraform or other IaC solutionsExperience with DevOps and SRE best practices
Why Publishing.com? People love working here and your peers are great - check out our We are growing (fast!) We are located all over the world with 60+ employees. We were remote before remote was a thing! And we will continue to be.Last year we hit $60M in revenue, and we are just getting started!We have all the fun perks you’d expect—flexible vacation policy, competitive vision, dental, and health benefits, 401k plans, and socials (yes, even remotely!)*We are proud of our culture and care about it deeply—we live by our team values and are always trying to make Publishing.com a better company today than it was yesterday.We encourage learning, growth, and continuous improvement so always looking for ways to help our staff grow. From monthly training to hiring mentors, we care about your personal growth!If you want to join a team on the ground floor, this is your chance. We have a grand vision for expanding beyond just an education company to become the one-stop shop for everything publishing related.some benefits are available to our US-based employees only.
Publishing.com is dedicated to building diverse teams that fuel an authentic workplace and sense of belonging for each and every employee. We know applying for a job can be intimidating, please don't hesitate to reach out - we encourage everyone interested in joining us to apply.Publishing.com LLC. provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, disability, genetics, gender, sexual orientation, age, marital status, veteran status. In addition to federal law requirements, Publishing.com LLC. complies with applicable state and local laws governing nondiscrimination in employment in every location we have employees in. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training.Powered by JazzHReUT95FbkXw"
Data Engineer,Fusemachines,"Texas, United States (Remote)",https://www.linkedin.com/jobs/view/3733596267/?eBP=JOB_SEARCH_ORGANIC&refId=vqykNdXaS0b%2BjMByk%2BKfOg%3D%3D&trackingId=RvUazjKGMe1PV05wKQ7cog%3D%3D&trk=flagship3_search_srp_jobs,3733596267,"About the job
            
 
About FusemachinesFusemachines is a leading AI strategy, talent, and education services provider. Founded by Sameer Maskey Ph.D., Adjunct Associate Professor at Columbia University, Fusemachines has a core mission of democratizing AI. With a presence in 4 countries (Nepal, United States, Canada, and Dominican Republic and more than 350 full-time employees) Fusemachines seeks to bring its global expertise in AI to transform companies around the world.About the role:This is a remote, 6 months contract role, with a possibility of extension, responsible for designing, building, and maintaining the infrastructure required for data integration, storage, processing, and analytics (BI, visualization and Advanced Analytics). Working in the Financial sector and implementing cyber-security use cases.Salary Range: US$7000/monthJob Type: 1099 ContractStart Date: The contract for this position is scheduled to begin in January 2024Qualification / Skill Set Requirement: 3+ years of real-world data engineering development experience in Snowflake and AWS (certifications preferred)Proven experience as a Snowflake Developer, with a strong understanding of Snowflake architecture and concepts.Proficient in snowflake services such as snowpipe, stages, stored procedures, views, materialized views, tasks and streams.Must have previous experience working with security datasetsStrong programming skills in SQL, with proficiency in writing efficient and optimized code for data integration, storage, processing, and manipulation.Robust understanding of data partitioning and other optimization techniques in Snowflake.Knowledge of data security measures in Snowflake, including role-based access control (RBAC) and data encryption.Highly skilled in one or more languages such as Python, Scala, and proficient in writing efficient and optimized code for data integration, storage, processing and manipulation.Strong knowledge of SDLC tools and technologies, including project management software (Jira or similar), source code management (GitHub or similar), CI/CD system (GitHub actions, AWS CodeBuild or similar) and binary repository manager (AWS CodeArtifact or similar).Skilled in Data Integration from different sources such as APIs, databases, flat files, event streaming.Good understanding of Data Modeling and Database Design Principles. Being able to design and implement efficient database schemas that meet the requirements of the data architecture to support data solutions.Strong experience in working with ELT and ETL tools and being able to develop custom integration solutions as needed.Strong experience with scalable and distributed Data Technologies such as Spark/PySpark, DBT and Kafka, to be able to handle large volumes of data.Strong experience in designing and implementing Data Warehousing solutions in AWS with RedShift. Demonstrated experience in designing and implementing efficient ELT/ETL processes that extract data from source systems, transform it (DBT), and load it into the data warehouse.Strong experience in Orchestration using Apache Airflow.Expert in Cloud Computing in AWS, including deep knowledge of a variety of AWS services like Lambda, Kinesis, S3, Lake Formation, EC2, ECS/ECR, IAM, CloudWatch, Redshift, etcGood understanding of Data Quality and Governance, including implementation of data quality checks and monitoring processes to ensure that data is accurate, complete, and consistent.Good Problem-Solving skills: being able to troubleshoot data processing pipelines and identify performance bottlenecks and other issues.
Responsibilities: Follow established design, constructed data architectures. Developing and maintaining data pipelines, ensuring data flows smoothly from source to destination. Handle ELT processes, including data extraction, loading, transformation and load data from various sources into Snowflake.Ensure the reliability, scalability, and efficiency of data systems are maintained at all timesAssist in the configuration and management of Snowflake data warehousing and data lake solutions, working under the guidance of senior team members.Collaborate closely with cross-functional teams including Product, Engineering, Data Scientists, and Analysts to thoroughly understand data requirements and provide data engineering support.Contribute to data quality assurance efforts, such as implementing data validation checks and tests.Evaluate and implement cutting-edge technologies and continue learning and expanding skills in data engineering and cloud platforms.Develop, design, and execute data governance strategies encompassing cataloging, lineage tracking, quality control, and data governance frameworks that align with current analytics demands and industry best practicesDocument data engineering processes and data flows.Care about architecture, observability, testing, and building reliable infrastructure and data pipelines.Takes ownership of storage layer, SQL database management tasks, including schema design, indexing, and performance tuning.Swiftly address and resolve complex data engineering issues, incidents and resolve bottlenecks in SQL queries and database operations.Assess best practices and design schemas that matches business needs for delivering a modern analytics solution (descriptive, diagnostic, predictive, prescriptive)Be an active member of our Agile team, participating in all ceremonies and continuous improvement activities.
Equal Opportunity Employer: Race, Color, Religion, Sex, Sexual Orientation, Gender Identity, National Origin, Age, Genetic Information, Disability, Protected Veteran Status, or any other legally protected group status.Powered by JazzHRshDrx201iX"
Remote Work - Need Data Engineer,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3774712978/?eBP=JOB_SEARCH_ORGANIC&refId=vqykNdXaS0b%2BjMByk%2BKfOg%3D%3D&trackingId=UxjcJEclfM1XBwd8EJUGWw%3D%3D&trk=flagship3_search_srp_jobs,3774712978,"About the job
            
 
Title : Data EngineerLocation : RemoteDuration : 6+ months to start (long term contract (work order are extended 2x year))Please Fill out the skill matrix: Technology Some Awareness (1) Novice (2) Intermediate (3) Advanced (4) Expert (5) SQL Tableau Desktop /Creator Tableau API Integration AWS - S3 Glue AWS - Athena Redshift AWS - Lambda AWS - SNS/SQS msExcelProfile: Data Analyst will work closely with Architects and leads in the Interflow team, key business stakeholders including program staff to support the organizations effort in the areas of data Analysis and information management. Role should have strong experience in writing SQL queries to extract data for reports. Candidate should have experience working with large set of data. As these reports are very critical for the company and we are looking for an expert who is capable of handling, consuming and extracting large sets of data for analysis and reports.Must Haves AWS - AthenaAWS - S3Gluehands-on development of reporting applicationsMS ExcelRedShiftSQLTableau Desktop /Creator
Nice AWS - LambdaAWS - SNS/SQSTableau API Integration
Skills BA/BS required (major in an analytical field desired) A minimum 10+ years work-related experience. Experience in hands-on development of reporting applications for the Web in a professional environment The Ethos of continuous improvement and interest in learning new things.Strong analytical thinking and structured problem-solving abilityAbility to handle multiple projects and assignments simultaneously and effectively in a cross-functional team environment. Versed on the agile methodology and best practices. Excellent interpersonal and collaboration skill with the ability to work with a diverse set of colleagues, across functions, from different organizations, disciplines, etc. Self-starter, ability to set priorities, work independently and attain goals"
Data Engineer,Act Digital Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3643165811/?eBP=JOB_SEARCH_ORGANIC&refId=vqykNdXaS0b%2BjMByk%2BKfOg%3D%3D&trackingId=lZelHfIzwOzoZw5rnGqkTA%3D%3D&trk=flagship3_search_srp_jobs,3643165811,"About the job
            
 
We are seeking an experienced Data Engineer to join our client's team. The ideal candidate will be responsible for designing, building, and maintaining our data architecture, infrastructure, and systems. The Data Engineer will work with data analysts, data scientists, and other stakeholders to ensure that data is efficiently collected, processed, and stored in a reliable and scalable way.Duties & ResponsibilitiesDevelop, implement and maintain scalable data pipelines and workflows that process large volumes of data efficiently and accurately.Collaborate with data analysts and scientists to design and implement data models that support business intelligence and analytics needs.Design, build and maintain data warehouses, data lakes, and other data storage and retrieval systems.Ensure data quality, reliability, and security by implementing appropriate data governance practices and monitoring systems.Develop and maintain automated testing, monitoring, and validation processes to ensure data accuracy and integrity.Continuously evaluate and recommend new data technologies and tools that can enhance our data infrastructure and architecture.Work with cross-functional teams to understand their data needs and provide solutions that meet their requirements.Technical SkillsDirect experience with at least one relational and one non-relational database technology.Expertise in ETL/ELT processes, data modeling and schema design.Experience with data automation tooling (e.g. Airflow, Databricks Workflows, MLFlow, etc.)Fluency in Python and SQL. Proficiency in Apache Spark. Comfortable on the Linux command line. Additional data or system languages (e.g. Java, Scala, Go, R) a plus.Hands on experience with one or more of the major cloud providers (GCP, AWS, Azure). Familiarity with infrastructure-as-code (e.g. Cloud Formation, Terraform) a plus.Familiarity with data visualization tools such as Tableau, Power BI, or LookerMinimum QualificationsBachelor's or Master's degree in Computer Science, Information Technology, or a related fieldMinimum of 3 years of experience in data engineering or a related fieldStrong problem-solving skills and ability to work in a fast-paced environmentAll positions with our client require an applicant who has accepted an offer to undergo a background check. The specific checks are based on the nature of the position. Background checks may include some or all of the following: SSN/SIN validation, education verification, employment verification, and criminal check, search against global sanctions and government watch lists, fingerprint verification, credit check, and/or drug test. By applying for a position with our client, you understand that you will be required to undergo a background check should you be made an offer. You also understand that the offer is contingent upon successful completion of the background check and results consistent with client's employment policies. You will be notified during the hiring process which checks are required for the position.Client is an Equal Employment Opportunity/Affirmative Action employer and well committed to a diverse workforce. We do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, veteran status, and basis of disability or any other federal, state or local protected class.Pay Transparency Non-Discrimination Notice Client will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor's legal duty to furnish information."
Data Engineer,Fusemachines,"North Carolina, United States (Remote)",https://www.linkedin.com/jobs/view/3733594441/?eBP=JOB_SEARCH_ORGANIC&refId=vqykNdXaS0b%2BjMByk%2BKfOg%3D%3D&trackingId=GR8%2BYFQ6cOaWv36kSEqXbg%3D%3D&trk=flagship3_search_srp_jobs,3733594441,"About the job
            
 
About FusemachinesFusemachines is a leading AI strategy, talent, and education services provider. Founded by Sameer Maskey Ph.D., Adjunct Associate Professor at Columbia University, Fusemachines has a core mission of democratizing AI. With a presence in 4 countries (Nepal, United States, Canada, and Dominican Republic and more than 350 full-time employees) Fusemachines seeks to bring its global expertise in AI to transform companies around the world.About the role:This is a remote, 6 months contract role, with a possibility of extension, responsible for designing, building, and maintaining the infrastructure required for data integration, storage, processing, and analytics (BI, visualization and Advanced Analytics). Working in the Financial sector and implementing cyber-security use cases.Salary Range: US$7000/monthJob Type: 1099 ContractStart Date: The contract for this position is scheduled to begin in January 2024Qualification / Skill Set Requirement: 3+ years of real-world data engineering development experience in Snowflake and AWS (certifications preferred)Proven experience as a Snowflake Developer, with a strong understanding of Snowflake architecture and concepts.Proficient in snowflake services such as snowpipe, stages, stored procedures, views, materialized views, tasks and streams.Must have previous experience working with security datasetsStrong programming skills in SQL, with proficiency in writing efficient and optimized code for data integration, storage, processing, and manipulation.Robust understanding of data partitioning and other optimization techniques in Snowflake.Knowledge of data security measures in Snowflake, including role-based access control (RBAC) and data encryption.Highly skilled in one or more languages such as Python, Scala, and proficient in writing efficient and optimized code for data integration, storage, processing and manipulation.Strong knowledge of SDLC tools and technologies, including project management software (Jira or similar), source code management (GitHub or similar), CI/CD system (GitHub actions, AWS CodeBuild or similar) and binary repository manager (AWS CodeArtifact or similar).Skilled in Data Integration from different sources such as APIs, databases, flat files, event streaming.Good understanding of Data Modeling and Database Design Principles. Being able to design and implement efficient database schemas that meet the requirements of the data architecture to support data solutions.Strong experience in working with ELT and ETL tools and being able to develop custom integration solutions as needed.Strong experience with scalable and distributed Data Technologies such as Spark/PySpark, DBT and Kafka, to be able to handle large volumes of data.Strong experience in designing and implementing Data Warehousing solutions in AWS with RedShift. Demonstrated experience in designing and implementing efficient ELT/ETL processes that extract data from source systems, transform it (DBT), and load it into the data warehouse.Strong experience in Orchestration using Apache Airflow.Expert in Cloud Computing in AWS, including deep knowledge of a variety of AWS services like Lambda, Kinesis, S3, Lake Formation, EC2, ECS/ECR, IAM, CloudWatch, Redshift, etcGood understanding of Data Quality and Governance, including implementation of data quality checks and monitoring processes to ensure that data is accurate, complete, and consistent.Good Problem-Solving skills: being able to troubleshoot data processing pipelines and identify performance bottlenecks and other issues.
Responsibilities: Follow established design, constructed data architectures. Developing and maintaining data pipelines, ensuring data flows smoothly from source to destination. Handle ELT processes, including data extraction, loading, transformation and load data from various sources into Snowflake.Ensure the reliability, scalability, and efficiency of data systems are maintained at all timesAssist in the configuration and management of Snowflake data warehousing and data lake solutions, working under the guidance of senior team members.Collaborate closely with cross-functional teams including Product, Engineering, Data Scientists, and Analysts to thoroughly understand data requirements and provide data engineering support.Contribute to data quality assurance efforts, such as implementing data validation checks and tests.Evaluate and implement cutting-edge technologies and continue learning and expanding skills in data engineering and cloud platforms.Develop, design, and execute data governance strategies encompassing cataloging, lineage tracking, quality control, and data governance frameworks that align with current analytics demands and industry best practicesDocument data engineering processes and data flows.Care about architecture, observability, testing, and building reliable infrastructure and data pipelines.Takes ownership of storage layer, SQL database management tasks, including schema design, indexing, and performance tuning.Swiftly address and resolve complex data engineering issues, incidents and resolve bottlenecks in SQL queries and database operations.Assess best practices and design schemas that matches business needs for delivering a modern analytics solution (descriptive, diagnostic, predictive, prescriptive)Be an active member of our Agile team, participating in all ceremonies and continuous improvement activities.
Equal Opportunity Employer: Race, Color, Religion, Sex, Sexual Orientation, Gender Identity, National Origin, Age, Genetic Information, Disability, Protected Veteran Status, or any other legally protected group status.Powered by JazzHR1ukLvPmZsT"
Senior Data Engineer,Tremendous,"New York, NY (Remote)",https://www.linkedin.com/jobs/view/3778692885/?eBP=JOB_SEARCH_ORGANIC&refId=vqykNdXaS0b%2BjMByk%2BKfOg%3D%3D&trackingId=UtRvy%2Fvt92wdJMG1EJ3qOg%3D%3D&trk=flagship3_search_srp_jobs,3778692885,"About the job
            
 
Tremendous is the fast, free, flexible way to send bulk payouts to people in over 200 countries. 10,000+ companies ranging from mom-and-pops to Google, MIT, and United Way have sent over $1 billion, saving 15 hours a month on average.In both our product and our workplace, we’re intentional about making work more efficient, flexible, and fulfilling. Tremendous is a fully remote, high-documentation, low-meeting culture, which means more time for what matters in both your professional and personal life.Our customers, who include marketers, UX researchers, HR teams and nonprofits, rave about how quick and easy it is to use Tremendous — check the ratings on G2. Yet there’s a lot of complexity under the hood, including over 2,000 redemption options and plenty of banking infrastructure. This duality makes working here a fun challenge.We’re about 90 people, highly profitable without any investors, and have been doubling our revenue for a few years and counting. Join us before the next international offsite!About The RoleWe're looking to add a senior data engineer to the Tremendous data team. The data team currently consists of our director of analytics (who has owned analytics engineering and data science in past roles) and two senior analysts.Tremendous values data as a first-class citizen and believes that insights unlock significant growth. You will own the data platform you build and work closely with the analysts on the team and stakeholders across the organization. As we continue to grow the business, you will scale our data infrastructure and develop new data applications throughout the business.You will Scale and contribute to a world-class data team. Lead infrastructure initiatives and contribute to analytics projects in a fast-paced, high-growth startup environment. You will be a driver of a responsible data-centric culture at Tremendous and champion data best practices within the company. Build and maintain enterprise-scale data infrastructure. Our data pipelines empower high-visibility projects and features, including payout analytics, executive reporting, FP&A, etc. You will manage and build out our data platform and suite of data tools. Develop and productize insights and impact. Own the development of data products end-to-end, including scoping, architecting, coding, testing, and rolling out insights across the company. You will scale the data platform for new marketing initiatives, data science models, experimentation, etc. 
You have 4+ years of experience in building and scaling data infrastructure at a growth-stage startup. Highly motivated self-starter who is eager to make an impact and unafraid of tackling large, complex problems while delivering high-quality results. Expertise in SQL and Python. Enjoy collaborating with business teams, product managers, and engineers. Comfortable taking on the role of a Project Manager to drive results. Previous experience in building real-time and batch data pipelines using tools such as dbt, Airflow, Spark, Kafka, S3, Airbyte, Census, etc. Also experienced in data transformation, event tracking frameworks, and experimentation frameworks. Experience in managing and working with Data Warehouses and Data Lakes such as Redshift, Big Query, Snowflake, Delta Lake, etc. Skilled in data modeling and developing core data sets to support analytics, reporting, and experimentation. Solid understanding of using data to inform the product roadmap. Strong written communication skills. We’re a documentation-first culture. Desire and ability to work autonomously and drive your work. Tremendous is not a great fit for people who default to waiting for instructions. High empathy. You care about your teammates and our users. You can put yourself in their shoes. 
Bonus Degree in a quantitative field (e.g., Economics, Statistics, Science, Engineering). Strong verbal and communication skills. You can articulate why something should be built a certain way and how it will impact the business. Experience with Ruby on Rails and Kubernetes. Experience with data privacy and security frameworks. 
What's Cool About The Role You'll get to work with a top-notch team at Tremendous. You'll work at a company growing quickly yet sustainably. We’re profitable with plenty more opportunity ahead. That’s good news for your career growth. Competitive pay, equity and benefits. The base salary for this role is (range). We're a remote company. Work from wherever you want in the Americas — we have collaboration hours from around 12-4 PM Eastern time. 
Compensation Range: $160K - $220K"
"Data Engineer, Remote","CorEvitas, LLC","Waltham, MA (Remote)",https://www.linkedin.com/jobs/view/3736699076/?eBP=JOB_SEARCH_ORGANIC&refId=vqykNdXaS0b%2BjMByk%2BKfOg%3D%3D&trackingId=w5oGZP%2BFCSXCutuscbdw0g%3D%3D&trk=flagship3_search_srp_jobs,3736699076,"About the job
            
 
Position SummaryThe Data Engineer will be responsible for designing and developing our data and data pipeline architecture, as well as optimizing data flow and collection for cross-functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure that optimal data delivery architecture is consistent throughout projects. The successful candidate will be self-motivated and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.This is an important, visible, roll-up-your-sleeves position that enables our biostatisticians, pharmacovigilance experts and epidemiologists to gain insights and critical knowledge, enabling them to make better decisions faster.Principle Duties And Responsibilities Work together with the Director, Data Engineering and the software development team to realize the data architecture and data processing pipelines on AWS.Create and maintain optimal data pipeline architecture.Assemble large, complex data sets that meet functional / non-functional business requirements.Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.Build analytics tools that utilize the data pipeline to provide actionable insights.Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.Keep our data separated and secure across national boundaries through multiple data centers and AWS regions.Create data tools for analytics and data scientist team members that assist them in building and optimizing our life sciences offerings.Promote data best-practices across the organization and help build a “culture of data”.
Skills/KnowledgeMINIMUM QUALIFICATIONS:Must have Strong SQL Server Database development, maintenance experience.Must have working knowledge SSRS reporting.Must have proficiency with SQL Server functions and stored procedures.Must be able to debug, profile, tune SQL queries, functions and stored procedures.NoSQL Database experience is a big plus.Proficiency in at least one high-level programming language, e.g., Python or Java.Demonstrated experience developing data pipelines/ETL/ELT processes.Excellent communication and organizational skills.Also, Cloud Platform Experience That Include AWS data storage and retrieval experience highly preferredAWS data services, e.g., AWS GlueAWS data processing and analytics services
Big Plus Familiarity working with sensitive data and with CFR Part 11, HIPAA, GDPR compliance.Working knowledge of clinical and pharma dataWorking knowledge of Clinical Data Standards such as CDISC
Experience5+ yeas overall experience, at least 3 in data management and/or data-centric software development. Life sciences background preferred.Experience in writing production-level SQL and working with various databases and reporting systems.Education/trainingBS/MS degree in Computer Science, Computer Engineering, or other technical discipline.Special RequirementsFully Remote, though some Travel may be required for this roleThis description is not intended to be a complete statement of the job, but rather to act as a guideAbout CorEvitasCorEvitas, now part of Thermo Fisher Scientific, is a science-led, real-world data intelligence company. Using syndicated registry data and analytic services to understand the post-approval comparative effectiveness and safety of approved therapies, CorEvitas provides biopharmaceutical companies with objective data and clinical insights to demonstrate the value of their products to clinicians, patients, payers, and regulators. The company operates nine major autoimmune and inflammatory registries across the U.S., Canada, and Japan, collecting data from over 400 participating investigator sites, including collection of biosamples linked to the deep clinical data. CorEvitas recently expanded its services to include Pregnancy Registries, through the acquisition of Pregistry. CorEvitas also conducts client-sponsored registries through its Patient Powered Registries business, employing a transformative patient-focused registry model to support research needs for patient-centered outcomes across all therapeutic areas. The company’s regulatory-grade registry data is complemented by its Patient Experience business, supporting evidence-based patient engagement initiatives across the product lifecycle, as well as its Specialty EMR Data business and retinal data set. CorEvitas is headquartered in Waltham, MA and is a portfolio company of Audax Private Equity. www.corevitas.comCorEvitas is proud to provide equal employment opportunities to all qualified individuals without regard to race, color, religion, sex, gender identity, sexual orientation, pregnancy, age, national origin, physical or mental disability, military or veteran status, genetic information, or any other protected classification. Minorities, women, LGBTQ candidates, Veterans, and individuals with disabilities are encouraged to apply.CorEvitas participate with E-Verify"
Data Engineer - Lead,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3750175286/?eBP=JOB_SEARCH_ORGANIC&refId=vqykNdXaS0b%2BjMByk%2BKfOg%3D%3D&trackingId=8siaT%2Bv6a9Ioewl3%2FLdHNA%3D%3D&trk=flagship3_search_srp_jobs,3750175286,"About the job
            
 
Remote role, someone from GA preferred, if not in GA, then must be from FL, NC, AL, CA, NV, IL, UT, MO, TX, PA, NH, PACandidates need to take a video screen with prime vendor prior to end client submittalNeed 2 strong candidates on this role  Need strong knowledge of PySprak. 80% of the role will be using Pyspark. They need to be able to be very hands-on and have experience coding within Pyspark not just enabling the tool. Candidates need to have experience with large-scale projects that take 1-2 years versus small projects or proof of concept. Candidates need to have more thought leadership skills. They have a lot of doers currently and need someone who can help guide the team. Top skills: Pyspark Azure Databricks and/or Data Factory Experience leading and managing offshore teams. ETL 

Must Have Azure CloudAzure Data BrickETLHigh volumes of dataPysparkPython Experience Leading or managing offshore teams
Data engineer has ownership of designing and developing large scale data solutions. You will have deep technical skills in a variety of technologies to play an important role in developing and delivering proof of concepts and product implementation.Responsibilities Design and develop high performant data ingestion pipelines from multiple sources using Azure Databricks and Azure Data FactoryWorking with event based streaming technologies to ingest and process dataDesign and Implement Data LakehouseWorking with other members of the project team to support delivery of additional project components (API interfaces, Search)Evaluating the performance and applicability of multiple tools against customer requirementsWorking within an Agile delivery / DevOps methodology to deliver proof of concept and production implementation in iterative sprints.Work with contractors to deliver products and services for the bottlersConduct Pilots with bottlers and measure valueTransition successful pilots into commercial applicationsEnsure design and implementation of technologies that comply with security standards and application architecture principles (in alignment with platform architects and respective review boards) and that consider state of the art data and analytics conceptsReview code/solutions developed by team prior to migration to production
What makes you a good fit? Strong knowledge of data management principlesExperience in building ETL pipelinesHands on experience designing and delivering solutions using the Azure including Azure Storage, Azure Data Factory, PySpark, Python, Databricks, Azure Data Lake, Azure Cosmos DB, Azure Stream AnalyticsExperience working with structured and unstructured dataExperience working in DevOps environment3+ years of relevant experienceAbility to balance high customer orientation and service attitude with business prioritiesAnalytic thinking, and problem-solving skillsHigh energy, with strong will and ambition to learn and work on new thingsOutstanding proven verbal, written and interpersonal communication skillsAbility to adapt quickly to changing product scope and priorities (demand driven)Proven ability to influence and collaborate effectively with cross-functional teamsDemonstrates noticeable commitment to foster and preserve a culture of diversity and inclusion by creating environments where people of diverse backgrounds are excited to bring all of who they are and do their best workConstant role model of culture: Integrity, Accountability, Passion, Collaboration, InnovationBachelor’s degree (or equivalent)"
Data Engineer - Remote,KNS IT GROUP,United States (Remote),https://www.linkedin.com/jobs/view/3731675404/?eBP=JOB_SEARCH_ORGANIC&refId=vqykNdXaS0b%2BjMByk%2BKfOg%3D%3D&trackingId=CIl81ypRctkUdsvNkERDmQ%3D%3D&trk=flagship3_search_srp_jobs,3731675404,"About the job
            
 
Job Description   Experience with big data tools: Hadoop, Spark, Kafka, etc.Experience with relational SQL and NoSQL databases, including Oracle, MS SQL Server, Postgres, Cassandra, etc.Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.Experience with data integration services solutions from vendors such as Informatica, MuleSoft, Talend, TIBCO, etc.Experience with cloud-based data services such as AWS (EC2, Glue, EMR, RDS, Redshift, etc.)Experience with stream-processing systems: Storm, Spark-Streaming, Kafka etc.Experience with object-oriented/object function scripting languages: Python, R, Java, C++, Scala, etc."
Data Engineer,IT Minds LLC,United States (Remote),https://www.linkedin.com/jobs/view/3717239257/?eBP=JOB_SEARCH_ORGANIC&refId=vqykNdXaS0b%2BjMByk%2BKfOg%3D%3D&trackingId=yUM%2Ba6tojGX6dEG1URQanA%3D%3D&trk=flagship3_search_srp_jobs,3717239257,"About the job
            
 
Data Engineer @ RemoteTop Skills Details  AWS services - Glue, S3, Lambdas, EMR, Redshift, EC2, RDS, sagemaker, etc.Need to have experience and knowledge around oracle and informatica Experience working with big data - have all structured data and currently do data compaction using hive They aren't sure if hive is the right approach, would want someone to help them figure out what the best approach would be
Strong ETL development experience - extract data from different sources and load it into data stores Want someone at an architect level, but need to be more hands on than a traditional architect - this person would work with architects to help implement the work they are doing Python as core language - using pyspark processing so need experience there as well Working with modeling and analytics users as their customers so knowledge in the data modeling / data analytics space would be a plus, not necessary though"
Entry Level Data Scientist/Analyst/Engineer - Remote,SynergisticIT,"Dallas, TX (Remote)",https://www.linkedin.com/jobs/view/3774168654/?eBP=JOB_SEARCH_ORGANIC&refId=vqykNdXaS0b%2BjMByk%2BKfOg%3D%3D&trackingId=hLyvI8YP7yYfLqKzoI7OEQ%3D%3D&trk=flagship3_search_srp_jobs,3774168654,"About the job
            
 
The Job Market is Challenging due to more than 150,000 Tech Layoffs in 2022 and in 2023 more than 240,000 layoffs so almost 3,90,00 tech employees have been laid off since 2022 and its still going on . The effect of this has led hundreds of thousands of laid off Tech employees competing with existing Jobseekers. Entry level Job seekers struggle to get responses to their applications, are getting ghosted after interviews. In such a scenario the Job seekers need  to differentiate themselves by ensuring to obtain exceptional skills and technologies to be hired by clients as its an employer's market presently and they have a lot of hiring choices.For more than 12+ years Synergisticit has helped Jobseekers differentiate themselves by providing candidates the requisite skills and experience to outperform at interviews and clients. Here at SynergisticIT We just don't focus on getting you a Job we make careers.All Positions are open for all visas and US citizensWe are matchmakers we provide clients with candidates who can perform from day 1 of starting work. In this challenging economy every client wants to save $$$'s and they want the best value for their money. Jobseekers need to self-evaluate if they have the requisite skills to meet client requirements and needs as Clients now post covid can also hire remote workers which increases even more competition for jobseekers.We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like  apple, google, Paypal, western union, Client, visa, walmart labs etc to name a few.We have an excellent reputation with the clients. Currently, We are looking for  entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Machine Learning engineers for full time positions with clients.Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in IT Industry We assist in filing for STEM extension and also for H1b and Green card filing to Candidates We also offer optionally Skill and technology enhancement programs for candidates who are either missing skills or are lacking Industry/Client experience with Projects and skills. Candidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. If they are qualified with enough skills and have hands on project work at clients then they should be good to be submitted to clients. Shortlisting and selection is totally based on clients discretion not ours. please check the below links to see success outcomes of our candidates and our participation at different Tech industry events and how we are different from other organizations in helping Jobseekers secure Tech careers https://www.synergisticit.com/candidate-outcomes/  We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023 Oracle CloudWorld Event (OCW) Las Vegas 2023/ 2022 | SynergisticIT - YouTube https://youtu.be/Rfn8Y0gnfL8?si=p2V4KFv5HukJXTrn  https://youtu.be/-HkNN1ag6Zk?si=1NRfgsvL_HJMVb6Q  https://www.youtube.com/watch?v=NVBU9RYZ6UI  https://www.youtube.com/watch?v=EmO7NrWHkLM  https://www.youtube.com/watch?v=NVBU9RYZ6UI  https://www.youtube.com/watch?v=OAFOhcGy9Z8  https://www.youtube.com/watch?v=Yy74yvjatVg For preparing for interviews please visit  https://www.synergisticit.com/interview-questions/  We are looking for the right matching candidates for our clients  Please apply via the job posting Required Skills REQUIRED SKILLS For Java /Full stack/Software Programmer   Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT  Highly motivated, self-learner, and technically inquisitive  Experience in programming language Java and understanding of the software development life cycle  Project work on the skills  Knowledge of Core Java , javascript , C++ or software programming  Spring boot, Microservices, Docker, Jenkins and REST API's experience  Excellent written and verbal communication skills 
 For data Science/Machine learning Positions Required Skills  Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT  Project work on the technologies needed  Highly motivated, self-learner, and technically inquisitive  Experience in programming language Java and understanding of the software development life cycle  Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools  Excellent written and verbal communication skills 
 Preferred skills: NLP, Text mining, Tableau, PowerBI, SAS, Tensorflow  If you get emails from our skill enhancement team please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team who only connect with candidates who are matching client requirements.  No phone calls please.  Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates"
Data Engineer - Lead,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3742097886/?eBP=JOB_SEARCH_ORGANIC&refId=vqykNdXaS0b%2BjMByk%2BKfOg%3D%3D&trackingId=so95oicm8J0csGAARdntgA%3D%3D&trk=flagship3_search_srp_jobs,3742097886,"About the job
            
 
Remote role, someone from GA preferred, if not in GA, then must be from FL, NC, AL, CA, NV, IL, UT, MO, TX, PA, NH, PACandidates need to take a video screen with prime vendor prior to end client submittalNeed 2 strong candidates on this role  Need strong knowledge of PySprak. 80% of the role will be using Pyspark. They need to be able to be very hands-on and have experience coding within Pyspark not just enabling the tool. Candidates need to have experience with large-scale projects that take 1-2 years versus small projects or proof of concept. Candidates need to have more thought leadership skills. They have a lot of doers currently and need someone who can help guide the team. Top skills: Pyspark Azure Databricks and/or Data Factory Experience leading and managing offshore teams. ETL 

Must Have Azure CloudAzure Data BrickETLHigh volumes of dataPysparkPython Experience Leading or managing offshore teams
Data engineer has ownership of designing and developing large scale data solutions. You will have deep technical skills in a variety of technologies to play an important role in developing and delivering proof of concepts and product implementation.Responsibilities Design and develop high performant data ingestion pipelines from multiple sources using Azure Databricks and Azure Data FactoryWorking with event based streaming technologies to ingest and process dataDesign and Implement Data LakehouseWorking with other members of the project team to support delivery of additional project components (API interfaces, Search)Evaluating the performance and applicability of multiple tools against customer requirementsWorking within an Agile delivery / DevOps methodology to deliver proof of concept and production implementation in iterative sprints.Work with contractors to deliver products and services for the bottlersConduct Pilots with bottlers and measure valueTransition successful pilots into commercial applicationsEnsure design and implementation of technologies that comply with security standards and application architecture principles (in alignment with platform architects and respective review boards) and that consider state of the art data and analytics conceptsReview code/solutions developed by team prior to migration to production
What makes you a good fit? Strong knowledge of data management principlesExperience in building ETL pipelinesHands on experience designing and delivering solutions using the Azure including Azure Storage, Azure Data Factory, PySpark, Python, Databricks, Azure Data Lake, Azure Cosmos DB, Azure Stream AnalyticsExperience working with structured and unstructured dataExperience working in DevOps environment3+ years of relevant experienceAbility to balance high customer orientation and service attitude with business prioritiesAnalytic thinking, and problem-solving skillsHigh energy, with strong will and ambition to learn and work on new thingsOutstanding proven verbal, written and interpersonal communication skillsAbility to adapt quickly to changing product scope and priorities (demand driven)Proven ability to influence and collaborate effectively with cross-functional teamsDemonstrates noticeable commitment to foster and preserve a culture of diversity and inclusion by creating environments where people of diverse backgrounds are excited to bring all of who they are and do their best workConstant role model of culture: Integrity, Accountability, Passion, Collaboration, InnovationBachelor’s degree (or equivalent)"
Data Analytics Engineer,Discovery Education,"Charlotte, NC (Remote)",https://www.linkedin.com/jobs/view/3726063539/?eBP=JOB_SEARCH_ORGANIC&refId=vqykNdXaS0b%2BjMByk%2BKfOg%3D%3D&trackingId=yDwkxRL6bvX7mFDD5bqTvQ%3D%3D&trk=flagship3_search_srp_jobs,3726063539,"About the job
            
 
Discovery Education is looking for an experienced Data Analytics Engineer to join our team to support various areas of the business such as marketing analytics, sales and revenue and operations. This role will support data-based decision making by modeling data from the data warehouse and cloud storage to support the development of insights for the business teams. The Analytics Engineer will become a specialist in our modern data stack, including Snowflake, dbt, Looker, and Hightouch.About The Role Work collaboratively with data engineers and data scientists to drive business value through turning data models into actionable insights for business stakeholders. Provide clean, transformed data ready for analysis. Apply engineering best practices to analytics code. Maintain data documentation and definitions. Collaborate with team members to collect business requirements, define successful analytics outcomes, and design data models. Craft code that meets our internal standards for style, maintainability, and best practices for a high-scale database environment. Maintain and advocate for these standards through code review. Ability to use Snowflake, dbt, Looker, and git. Ability to thrive and work with remote colleagues. Positive and solution-oriented mindset
Requirements A minimum of 2-3 years experience as an analyst, engineer, data scientist, or equivalent2+ years working with marketing, sales, and revenue dataDemonstrated ability to learn new systems quickly and master data integrity among multiple systems3+ years using SQL. Experience analyzing data with R or Python or other similar tools is a plusExperience working with data warehouses, data transformation tools, and writing technical documentation for your workExperience using Snowflake and dbt a plusWork collaboratively with the data management team; experience with API integrations a plusExperience working with large data volumes (millions of daily events)Must have strong interpersonal skills – collaboration across multiple businesses and technology. Functions, relationship building, and consensus building are a mustMust be able to work on and prioritize multiple tasks while working well under pressureAlways willing to learn something new, not rigid in beliefs or expectationsHave a demonstrated ability to understand, accept, and engage with people of diverse backgroundsLegal right to work in the United States
BenefitsWe are proud to offer employees and their families a comprehensive benefits package: Medical-Dental-VisionHealth Care Dependent CareShort & Long Term DisabilityLife Insurance401(k)FSA/HSAPaid Time OffVolunteer TimeEmployee Assistance Program7 Paid Holidays + Annual Winter Holiday Break (Typically the last week of December)
Discovery Education is an equal opportunity employer. Discovery Education is committed to being an employer of choice, not just a good place to work, but a great and inclusive place to work. To that end, we strive to recruit and maintain a workforce that meaningfully represents the diverse and culturally rich communities that we serve. Qualified applicants will receive consideration for employment without regard to their race, color, religion, national origin, sex, sexual orientation, gender identity, protected veteran status or disabled status or, genetic information."
Data Engineer,Advanced Hires,"Cedar Park, TX (Remote)",https://www.linkedin.com/jobs/view/3779442427/?eBP=JOB_SEARCH_ORGANIC&refId=vqykNdXaS0b%2BjMByk%2BKfOg%3D%3D&trackingId=4GaaWMzOIFJjVKoXGDQrVQ%3D%3D&trk=flagship3_search_srp_jobs,3779442427,"About the job
            
 
Hello,Please go through the job description below and let me know your interest in this opportunity. Kindly respond back with your latest resume and the best time to reach.Title: Data EngineerLocation: 100% RemoteDuration: Long Term contractThe selected candidate should be able to help support our Data and Process Governance Policies and Standards for data utilization and access. A strong business insight mindset and analytics background will help enable data visualizations, while helping maintain the technical foundation they are built on. Knowledge of industry standard applications and database theory will allow us to transform the business by optimizing our workflow and data processing to deliver data to the shop floor.Additional role info:  Interconnectivity between industrial automation data, MIS, and data warehouse. Helping influence and implement an end-to-end vision for how production field data will flow through the organization. Merge new systems or methods with existing data structures. Partner with Supply Chain to define, document, implement and maintain business processes and data workflows. Implement data visualizations and drive reporting solutions based on Engineers, PCIT Developers and End Users needs and feedback. Help our engineers and PCIT developers create data reports for business Will work with the manager, regional automation or process engineers depending on the site. 50% Data engineering, 25% visualization 25% maintenance Team Dynamic/Culture: Currently 1 FT and 2 contractors. The current team that’s been working on this project for about 1yr, there is an existing automation team that has been stood up for a while that this team integrates with.
Skill Set Requirements: Experience in managing and communicating complex projects and collaborating with cross functional teams to accomplish project goals within expected timelines.Knowledgeable on data governance concepts and implementationData Access ManagementChange ManagementData Historian/ Time Series DataSQL, MSSQL, TSQLData modeling for creating/maintaining data integrity between multiple schemasETL experienceDenodo/Data virtualization experience preferredKnowledgeable on deployment process flow conceptsPython experienceQuery optimizationExperience working with big data setsTableau Server/Desktop/Prep setup and dashboard buildingStrong data cleaning and wrangling skillsStrong initiative, self-motivated, results orientated, and able to work independently with minimal directionWorkflow managementDemonstrated ability to see differing perspectives and work cross-functionally.This is a client facing role so good communication skills, written and verbal, is required.
Education: Bachelors would be nice but is not required, associates with experience is acceptable.This is a remote position."
Data/Backend Engineer,Loaded,Los Angeles Metropolitan Area (Remote),https://www.linkedin.com/jobs/view/3768083232/?eBP=JOB_SEARCH_ORGANIC&refId=vqykNdXaS0b%2BjMByk%2BKfOg%3D%3D&trackingId=CYocJJJKJQdCR%2FNR1emPlQ%3D%3D&trk=flagship3_search_srp_jobs,3768083232,"About the job
            
 
Loaded and Open World are market leaders in gaming talent management and strategic brand consultancy – in any of our business endeavors we aim to provide high-value services to support the development, marketing, and growth of the video gaming industry and community.Over the past 7 years, we have facilitated partnerships with world-leading brands such as Netflix, Universal Music Group, Pepsi, Activision, Amazon, Spotify, Gillette, Google, Journey's, and Adidas to either build or deepen their connection to gamers.We believe that gaming is one of the great connectors, and the communities that grow here change lives, move the culture forward, and bring tremendous value to the brands, creators, and fans that participate in it.It's time for our team to grow, and we're looking for creative minds, committed hearts, and forward-thinking humans to join us and make the work we do the most impactful it can be.If that ignites or intrigues you, keep reading...We're looking for Data/Backend EngineerThe Loaded Product team is looking for a talented Engineer who will work alongside our pipeline engineer to warehouse and make available data to our various analytics products.Reporting to an engineer turned VP of Product, the ideal candidate must be very pragmatic, productive, open-minded, and passionate about the space and the innovations they are making to it.On any given day you might... Design and develop sophisticated warehousing schemas, storage systems, and processesWork closely with the data science team to programmatically generate and make available insights and calculationsArchitect and provision cloud-based infrastructure in an AWS environmentAssist in the develop APIs that support our product initiativesCollaborate with coworkers and stakeholders to design and scope new features and roadmap - acting as a gatekeeper for feasibility as it relates to data and its availability as well as being a resource for determining efficient pathingAssisting in efforts to hire and build out our engineering team and processResearching and making long-lasting architectural and design decisions for company-wide tech offerings
Ideally, you have... A pragmatic, productive, and creative development and design philosophyA passion and pride for the work they do and the problems that they solveThe ability to work autonomously, taking ownership of projects and confidently moving them forwardOpinionated but open-minded and established views about development solutions/processes/architecture/patterns and the ability to express them productivelyExpertise in building scalable solutions in a modern development environmentExperience contributing to and designing data-heavy productsThe ability to work on a very lean and high-impact team. There are no NPCs at Loaded, only main characters. 
You earn bonus points if you... Have strong expertise in the storage and gathering of user data and the privacy considerations that must be madeAre able to meaningfully contribute to existing Laravel + Python backend servicesHave experience building applications and tools in the Discord/live streaming/gaming ecosystemAre an avid Discord userHave a strong understanding of live streaming content and/or consume it oftenYou have worked in or around marketing/management/gaming and understand the nuances of the industriesBuild side projects in your spare timeAre a LIRIK sub. 
The most important qualities that a candidate can have are passion, enthusiasm, and most importantly, curiosity. If you feel like you would do a great job, do not let self-doubt or imposter syndrome deter you from applying.Benefits Medical, Dental, and Vision InsuranceCompany-paid life insurance, short term and long term disability insurance401k plan with 4% company matchingFlexible work scheduleUnlimited PTODog-Friendly OfficeCell Phone, and Internet Service coverageTeam Events, expensable Lunches & Coffee
Loaded Holdings, Inc. is an Equal Opportunity / Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, national origin, disability, or protected Veteran status"
Entry Level Data Scientist/ML Engineer - Remote,SynergisticIT,"Pittsburgh, PA (Remote)",https://www.linkedin.com/jobs/view/3767590737/?eBP=JOB_SEARCH_ORGANIC&refId=vqykNdXaS0b%2BjMByk%2BKfOg%3D%3D&trackingId=6b7ntBIxbAcENI%2FpMRjakg%3D%3D&trk=flagship3_search_srp_jobs,3767590737,"About the job
            
 
The Job Market is Challenging due to almost 300,000 Tech Layoffs since October 2022 due to which thousands of laid off Techies are competing with existing Jobseekers. Entry level Job seekers struggle to get responses to their applications forget about getting client interviews. As the Saying goes ""when the Going gets tough the Tough get going”  Candidates who want to make a tech career they need to differentiate themselves by ensuring they have exceptional skills and technologies to be noticed by clients.Since 2010 Synergisticit has helped Jobseekers differentiate themselves by providing candidates the requisite skills and experience to outperform at interviews and clients. Here at SynergisticIT We just don't focus on getting you a Job we make careers.All Positions are open for all visas and US citizensWe are matchmakers we provide clients with candidates who can perform from day 1 of starting work. In this challenging economy every client wants to save $$$'s and they want the best value for their money. Jobseekers need to self-evaluate if they have the requisite skills to meet client requirements and needs.  Clients now post covid can also hire remote workers which increases even more competition for jobseekers.We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like  apple, google, Paypal, western union, Client, visa, walmart labs etc to name a few.We have an excellent reputation with the clients. Currently, We are looking for  entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Machine Learning engineers for full time positions with clients.Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in IT IndustryWe assist in filing for STEM extension and also for H1b and Green card filing to Candidates We also offer optionally Skill and technology enhancement programs for candidates who are either missing skills or are lacking Industry/Client experience with Projects and skills. Candidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. If they are qualified with enough skills and have hands on project work at clients then they should be good to be submitted to clients. Shortlisting and selection is totally based on clients discretion not ours.please check the below links to see success outcomes of our candidateshttps://www.synergisticit.com/candidate-outcomes/ We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023 Oracle CloudWorld Event (OCW) Las Vegas 2023/ 2022 | SynergisticIT - YouTubehttps://youtu.be/Rfn8Y0gnfL8?si=p2V4KFv5HukJXTrn https://youtu.be/-HkNN1ag6Zk?si=1NRfgsvL_HJMVb6Q https://www.youtube.com/watch?v=OAFOhcGy9Z8 https://www.youtube.com/watch?v=EmO7NrWHkLM https://www.youtube.com/watch?v=NVBU9RYZ6UI https://www.youtube.com/watch?v=Yy74yvjatVgFor preparing for interviews please visit  https://www.synergisticit.com/interview-questions/ We are looking for the right matching candidates for our clients Please apply via the job posting REQUIRED SKILLS For Java /Full Stack/Software Programmer Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT Highly motivated, self-learner, and technically inquisitive Experience in programming language Java and understanding of the software development life cycle Project work on the skills Knowledge of Core Java , javascript , C++ or software programming Spring boot, Microservices, Docker, Jenkins and REST API's experience Excellent written and verbal communication skills 
For data Science/Machine learning Positions Required Skills Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT Project work on the technologies needed Highly motivated, self-learner, and technically inquisitive Experience in programming language Java and understanding of the software development life cycle Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools Excellent written and verbal communication skills 
 Preferred skills: NLP, Text mining, Tableau, PowerBI, Tensorflow If you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team. No phone calls please.  Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates"
Remote - Job Opportunity - Data Engineer,"Donato Technologies, Inc.",United States (Remote),https://www.linkedin.com/jobs/view/3690943882/?eBP=JOB_SEARCH_ORGANIC&refId=vqykNdXaS0b%2BjMByk%2BKfOg%3D%3D&trackingId=4enO9KyZkN%2FQCgRHZWU9fA%3D%3D&trk=flagship3_search_srp_jobs,3690943882,"About the job
            
 
Job Title: Data EngineerLocation: RemoteDuration: Long TermRequired Experience Data Engineer with Enterprise Data warehouse/Datamart and ETL background.Experience in Property & Casualty (P&C) Insurance domain is required.Strong Python (DataFrame, APIs, Batch processing, Data pipelines) experience.Experience in Azure platform including Azure Data Lake, Data Bricks, Delta Lake (Bronze, Silver, Gold), Data Factory, Azure SQL DatabaseExperience in interpretation of insurance data to identify trends, patterns, and anomalies that can impact business performance and profitability.Experience in data analysis and predictive modeling to support decision-making processes such as policy performance, claims data, customer behavior etc.Strong SQL experience with excellent Excel skills (i.e. Pivot Tables).Understanding of Data modeling for Enterprise Data Warehouse ecosystemsKnowledge in Spark, PySpark preferable.Familiarity working in a data lake environment, leveraging data streaming, and developing data pipelines driven by events/queues.Working knowledge on different file formats such as JSON, Parquet, CSV, etc.Familiarity with data encryption, data maskingDatabase experience in SQL Server
QualificationsBachelor's degree in Information Science, Computer Science, Mathematics, Statistics or a quantitative discipline in science, business, or social science"
Data Engineer - Lead,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3742583023/?eBP=JOB_SEARCH_ORGANIC&refId=vqykNdXaS0b%2BjMByk%2BKfOg%3D%3D&trackingId=cSjcsCZcQdclTkz%2BLVepKg%3D%3D&trk=flagship3_search_srp_jobs,3742583023,"About the job
            
 
Remote role, someone from GA preferred, if not in GA, then must be from FL, NC, AL, CA, NV, IL, UT, MO, TX, PA, NH, PACandidates need to take a video screen with prime vendor prior to end client submittalNeed 2 strong candidates on this role  Need strong knowledge of PySprak. 80% of the role will be using Pyspark. They need to be able to be very hands-on and have experience coding within Pyspark not just enabling the tool. Candidates need to have experience with large-scale projects that take 1-2 years versus small projects or proof of concept. Candidates need to have more thought leadership skills. They have a lot of doers currently and need someone who can help guide the team. Top skills: Pyspark Azure Databricks and/or Data Factory Experience leading and managing offshore teams. ETL 

Must Have Azure CloudAzure Data BrickETLHigh volumes of dataPysparkPython Experience Leading or managing offshore teams
Data engineer has ownership of designing and developing large scale data solutions. You will have deep technical skills in a variety of technologies to play an important role in developing and delivering proof of concepts and product implementation.Responsibilities Design and develop high performant data ingestion pipelines from multiple sources using Azure Databricks and Azure Data FactoryWorking with event based streaming technologies to ingest and process dataDesign and Implement Data LakehouseWorking with other members of the project team to support delivery of additional project components (API interfaces, Search)Evaluating the performance and applicability of multiple tools against customer requirementsWorking within an Agile delivery / DevOps methodology to deliver proof of concept and production implementation in iterative sprints.Work with contractors to deliver products and services for the bottlersConduct Pilots with bottlers and measure valueTransition successful pilots into commercial applicationsEnsure design and implementation of technologies that comply with security standards and application architecture principles (in alignment with platform architects and respective review boards) and that consider state of the art data and analytics conceptsReview code/solutions developed by team prior to migration to production
What makes you a good fit? Strong knowledge of data management principlesExperience in building ETL pipelinesHands on experience designing and delivering solutions using the Azure including Azure Storage, Azure Data Factory, PySpark, Python, Databricks, Azure Data Lake, Azure Cosmos DB, Azure Stream AnalyticsExperience working with structured and unstructured dataExperience working in DevOps environment3+ years of relevant experienceAbility to balance high customer orientation and service attitude with business prioritiesAnalytic thinking, and problem-solving skillsHigh energy, with strong will and ambition to learn and work on new thingsOutstanding proven verbal, written and interpersonal communication skillsAbility to adapt quickly to changing product scope and priorities (demand driven)Proven ability to influence and collaborate effectively with cross-functional teamsDemonstrates noticeable commitment to foster and preserve a culture of diversity and inclusion by creating environments where people of diverse backgrounds are excited to bring all of who they are and do their best workConstant role model of culture: Integrity, Accountability, Passion, Collaboration, InnovationBachelor’s degree (or equivalent)"
Data Engineer - Full Time/Hybrid,PSI (Proteam Solutions),United States (Remote),https://www.linkedin.com/jobs/view/3774847684/?eBP=JOB_SEARCH_ORGANIC&refId=r5m9CV5iULHiwwerT09nWg%3D%3D&trackingId=WXLHpvs8bkZH5MSKcgFtGQ%3D%3D&trk=flagship3_search_srp_jobs,3774847684,"About the job
            
 
Seeking a mid-level Data Engineer to perform ETL and Data Modeling. The position is either Direct Hire or Contract-To-Hire.Candidates must be eligible to work in the United States without visa sponsorship now OR in the future.  Must be U.S. Citizens or Permanent Residents.Essential functions and responsibilities: Conduct complex data mapping efforts between systems in a clear and well defined mannerPerform data analysis and develop the data models to support ETL developmentComplete complex data migrations (ETL) processes to support development effortsCollaborate with Software Architects, Software Developers, and BI Developers to design appropriate solutions for our internal team and external clients.Mentor software developers in data architecture and database design best practicesPerform and assign database development tasks of medium complexity across multiple projects.Work directly with third party solutions to design, document, and develop data integrations.Create, contribute to, review and collaborate on data and database designs and implementations.Work with IT to deploy solutionsInnovate and contribute to improving development standards, techniques, tools, and processesTrain and test for industry certificationsIdentify, set, monitor, and achieve individual goalsLeverage mentorship and peer relationships to increase proficiency in developmentProvide support to team membersParticipate in the development personnel interview processCollaborate with project managers, business systems analysts, UX designers, and application developers to deliver high-quality deliverablesSupport production systems as urgent and critical issues arise; including non-business hours support on a rotating basis.
 Qualifications required: Bachelor's Degree in computer science, Information Technology, or related fields3+ years of experience developing data/database systems and integrating with third-party solutions.3+ years of Extract, Transform, and Load (ETL) experience2+ years of experience architecting data/database solutions and designing integrations with third-party solutions.At least 1 year of Data Modeling experienceExperience with Data Bricks (preferably with Azure)Experience with cloud data warehouse technologies, Azure preferredExpertise with SQL3+ years of experience working in a hybrid agile development methodologyExcellent verbal and written communication skillsThe ability to work as a full-time employee without requiring visa sponsorship now or in the future is required.
Qualifications preferred: Ability to work on a Hybrid schedule in Columbus, OH, Baltimore, MD, or Philadelphia, PA.
Working Conditions:Work is performed within a general hybrid office environment. Work is generally sedentary in nature, but may require occasional standing and walking. Lighting and temperature are adequate and there are no hazardous or unpleasant conditions caused by noise, dust, etc. within the office environment."
Data Engineer,JavanTech Inc,United States (Remote),https://www.linkedin.com/jobs/view/3665673821/?eBP=JOB_SEARCH_ORGANIC&refId=r5m9CV5iULHiwwerT09nWg%3D%3D&trackingId=RbX%2BDXclZQuoWDtLFoCU%2BQ%3D%3D&trk=flagship3_search_srp_jobs,3665673821,"About the job
            
 
Locals to United StatesOnly W2Python with pyspark data engineer with GIS background preferably"
Remote Job Opportunity - Data Engineer – (Need: Core Data Engineer),"Donato Technologies, Inc.",United States (Remote),https://www.linkedin.com/jobs/view/3731654219/?eBP=JOB_SEARCH_ORGANIC&refId=r5m9CV5iULHiwwerT09nWg%3D%3D&trackingId=NzcPR%2BnXA3H9hxBjKLjGxw%3D%3D&trk=flagship3_search_srp_jobs,3731654219,"About the job
            
 
Job Title: Data Engineer – (Need: Core Data Engineer)Location: RemoteDuration: Long TermRequired Experience Data Engineer with Enterprise Data warehouse/Datamart and ETL background.Experience in Property & Casualty (P&C) Insurance domain is required.Strong Python (DataFrame, APIs, Batch processing, Data pipelines) experience.Experience in Azure platform including Azure Data Lake, Data Bricks, Delta Lake (Bronze, Silver, Gold), Data Factory, Azure SQL DatabaseExperience in interpretation of insurance data to identify trends, patterns, and anomalies that can impact business performance and profitability.Experience in data analysis and predictive modeling to support decision-making processes such as policy performance, claims data, customer behavior etc.Strong SQL experience with excellent Excel skills (i.e. Pivot Tables).Understanding of Data modeling for Enterprise Data Warehouse ecosystemsKnowledge in Spark, PySpark preferable.Familiarity working in a data lake environment, leveraging data streaming, and developing data pipelines driven by events/queues.Working knowledge on different file formats such as JSON, Parquet, CSV, etc.Familiarity with data encryption, data maskingDatabase experience in SQL Server
QualificationsBachelor's degree in Information Science, Computer Science, Mathematics, Statistics or a quantitative discipline in science, business, or social science"
Snowflake Data Engineer + paid remote training,Jefferson Frank,United States (Remote),https://www.linkedin.com/jobs/view/3763959101/?eBP=JOB_SEARCH_ORGANIC&refId=r5m9CV5iULHiwwerT09nWg%3D%3D&trackingId=U1%2B4daMLls7nltbRWGYOHw%3D%3D&trk=flagship3_search_srp_jobs,3763959101,"About the job
            
 
Snowflake Data Engineer - paid remote training and placement opportunityBecome a Snowflake Data Engineer with this paid career programWant to use your programming skills in an exciting new sector? Interested in using your SQL knowledge and communication skills to cross-train as a specialist in the world's leading cloud-based data warehouse? Then, this Snowflake Data Engineer career program is for you!We're looking for professionals with SQL experience and basic Python programming knowledge who want to cross-train as Snowflake Data Engineers on a two-year program that includes paid training, certifications, and a remote work placement with our market-leading client, based in Wisconsin.The opportunityYou don't need any prior experience with Snowflake to apply.As a member of the Snowflake Partner Network, we'll get you up to speed on Snowflake, supporting you in gaining your industry certifications to become a certified Snowflake Data Engineer.We'll then place you with our client in a paid full-time role, to give you that all-important work experience.RoleOnce you're trained and certified, you'll work with the client for a period of up to two years. Your role as a Snowflake Data Engineer could include:  Developing, testing, and maintaining data pipelines Using these to convert raw data into usable formats and organize it for efficient use Ensuring that your company's data strategies are aligned to support its business goals Designing, building, and deploying custom functional solutions using Salesforce products Collaborating with a wide range of stakeholders across the business, from data analysts to the C-suite
Who we're looking forWe're looking for intrinsically motivated individuals who are passionate about learning a new cloud technology. This role will suit a self-starter, who has lots of motivation to work independently and also as part of a team.You'll join a fast-growing business where change takes place quickly, so adaptability and flexibility are key. You'll be asked to speak up and share ideas to problem-solve, so we're looking for someone comfortable providing and receiving honest feedback from a wide range of stakeholders.Since the role is client-facing, strong communication and interpersonal skills are a must.To Apply For This Program, You'll Also Need  2+ years of SQL experience at least 1 year of basic Python programming knowledge
Bonus Points If You Have  a Financial Services background ETL, ELT, and basic data modeling experience prior experience using cloud technology
To Apply For This Program, You'll Also Need To  Commit to a period of up to two years working in a paid work placement, post-training Be based anywhere in the USA (East Coast, NY tristate area, or Boston preferred but not essential) Available to start training in January 2024
Although this is a remote role, there might be some occasional travel from time-to-time, which will be discussed with you with plenty of notice.We are offering a salary range of $60,000 to $80,000 USD, depending on your skills and experience. We also offer a full range of company benefits including health benefits and retirement/401k plan.If you take this opportunity, you will also receive 8 weeks of fully-funded comprehensive training to become a Snowflake Data Engineer, before you start working on the client's site.SkillsThe training will primarily use Snowflake, dbt, and Fivetran with plenty of hands-on exercises and real-world projects to put your new skills into practice before you're deployed on your work placement. The training will also include:  Advanced Snowflake SQL skills SnowPro Core Certification dbt Analytics Engineering Certification Extensive consultancy skills training Support to gain further certifications while you're completing your work placement Access to mentoring and support programs and networking opportunities
Want to find out more about Revolent? Visit our website or check out career stories from others who have launched amazing careers in the cloud by cross-training with us.Please also feel free to email me directly with your resume to m.ramirez@jeffersonfrank.com"
Data Engineer,IVY TECH SOLUTIONS INC,United States (Remote),https://www.linkedin.com/jobs/view/3686022567/?eBP=JOB_SEARCH_ORGANIC&refId=r5m9CV5iULHiwwerT09nWg%3D%3D&trackingId=3AmJ%2B2RsHkHTdavT1XtqVA%3D%3D&trk=flagship3_search_srp_jobs,3686022567,"About the job
            
 
HI,Kindly let me know if you have a suitable fit for the following positionThanksJob Title: Data EngineerRemoteC2C PositionPlease send the resume to supriya.g@ivytechsol.usJob Description:Data EngineerPosition can be 100% remoteSkillshive; KAFKA; Python; SPARK; UNIX; Hadoop platformRequired (Individual Role)  Extensive experience in engineering and designing data management solutions using Hadoop platform tools and technologies such as Apache HDFS, Sqoop, Spark, Hive, Impala, HBase, and Kafka Significant experience in Python programming Proficient in the data ingestion pipeline process, exception handling, and metadata management on Hadoop platforms. Hands-on experience creating automated data integration applications using data models, data mappings and business rules specifications to load data warehouses, operational data stores, data marts, and data lakes while programmatically handling exceptions including late arriving, missing, or erroneous data. Experience in UNIX shell scripting Demonstrated experience developing/expanding automated technology controls (e.g., Data Quality checks, Data Movement controls) Demonstrated experience enabling access to data by way of databases or dashboards. Demonstrated experience cleaning, filtering, transforming data, and/or enriching data
Thanks & Regards,Supriya/ Recruiter224-348-8788Ivytech Solutions IncPowered by JazzHRg3o7gDvWps"
Junior Data Engineer - US/Canada Only,Phoenix Recruitment LLC,"Tampa, FL (Remote)",https://www.linkedin.com/jobs/view/3781564382/?eBP=JOB_SEARCH_ORGANIC&refId=r5m9CV5iULHiwwerT09nWg%3D%3D&trackingId=F677MKTHNNYQB8uk4I%2FOxA%3D%3D&trk=flagship3_search_srp_jobs,3781564382,"About the job
            
 
This is a remote position.Junior Data Engineer - US/Canada Only, 1 year of project experienceEmployment Type: Full-timeBase Salary: $60K-$74KPhoenix Recruitment offers a variety of recruiting services to assist both employers and employees. They specialize in marketing open positions, recruiting, and helping employers to find qualified candidates across various industries. Phoenix Recruitment has expertise in streamlining the hiring process. They can help ensure that the process is efficient, well organized, and compliant with relevant regulations.Position SummaryJoin the fast-paced, innovative, and collaborative environment focused on providing an AIOps platform that enhances the intelligence of the CVS Health infrastructure. Work closely with subject matter experts and colleagues to build and scale out machine learning and AI solutions that will detect, predict, and recommend solutions to correct issues before system impact and enhance the efficiency, reliability, and performance of CVS Health’s IT operations.Key Responsibilities include: Data pipeline development: Designed, implemented, and managed data pipelines for extracting, transforming, and loading data from various sources into data lakes for processing, analytics, and correlation. Data modeling: Create and maintain data models ensuring data quality, scalability, and efficiency Develop and automate processes to clean, transform, and prepare data for analytics, ensuring data accuracy and consistency Data Integration: Integrate data from disparate sources, both structured and unstructured to provide a unified view of key infrastructure platform and application data Utilize big data technologies such as Kafka to process and analyze large volumes of data efficiently Implement data security measures to protect sensitive information and ensure compliance with data and privacy regulation Create/maintain documentation for data processes, data flows, and system configurations Performance Optimization- Monitor and optimize data pipelines and systems for performance, scalability and cost-effectiveness 
Characteristics of this role: Team Player: Willing to teach, share knowledge, and work with others to make the team successful. Communication: Exceptional verbal, written, organizational, presentation, and communication skills. Creativity: Ability to take written and verbal requirements and come up with other innovative ideas. Attention to detail: Systematically and accurately research future solutions and current problems. Strong work ethic: The innate drive to do work extremely well. Passion: A drive to deliver better products and services than expected to customers. 
Required Qualifications 2+ years of programming experience in languages such as Python, Java, SQL 2+ years of experience with ETL tools and database management (relational, non-relational) 2+ years of experience in data modeling techniques and tools to design efficient scalable data structures Skills in data quality assessment, data cleansing, and data validation 
Preferred Qualifications Knowledge of big data technologies and cloud platforms Experience with technologies like PySpark, Databricks, and Azure Synapse. 
EducationBachelor’s degree in Computer Science, Information Technology, or related field, or equivalent working experienceWhy Phoenix Recruitment LLC?Phoenix Recruitment often has an extensive network of employers and candidates. This network allows them to tap into a pool of qualified candidates and connect them with suitable job opportunities. They can also leverage their connections to help employers find the right talent efficiently. Outsourcing the recruitment process to a specialized agency can save your time and resources, avoid delays, reduce administrative burdens, and increase the chances of finding the right fit for your organization."
Big Data Engineer - W2,Megan Soft Inc,United States (Remote),https://www.linkedin.com/jobs/view/3674845652/?eBP=JOB_SEARCH_ORGANIC&refId=r5m9CV5iULHiwwerT09nWg%3D%3D&trackingId=HJdT0N7tLM9kbqsGfomurQ%3D%3D&trk=flagship3_search_srp_jobs,3674845652,"About the job
            
 
Title: Big Data Engineer - W2Location: Dearborn, MI (Remote)Duration: 12+ MonthsDirect Client: Ford MotorsMust have skills: Should be able to handle GCP requirements""Translates business requirements to build repeatable, sustainable, efficient, coded processes that can be productionized by Software Engineers and readily modified by other Data Engineer Creates POC processes in Dev/QA and works with Software Engineers to productionize and align best coding and process practices that maximize efficiency, speed, stability, system resources and capabilities Awareness of and compliance with data privacy, security, legal and contractual guidelines Incorporates data quality and privacy checks/alerts to minimize bad data being consumed by end users, models and dashboards, and to protect customer data; revises checks for new data issues Validates data products and pipelines are functioning as expected following system or application upgrades, source changes, etc. Works closely with Architects if required to align systems, tools and applications being utilized with business use case and performance requirements Provides data product support and maintenance Establishes, tracks and monitors KPIs related to specific data products and deliverable Participate in iteration cadence activities (Iteration Planning, task estimation, daily scrum, demos, etc.). Will be a part of on-call team to resolve critical production support tickets during business hours and off hours.Skills RequiredExperience with Big Data Technologies (Hadoop , ETL Tool, Spark. Hive etc..) Someone who understands Cloud and has experience working with Cloud Platforms (preferably GCP) Should be able to handle GCP requirements - ingesting into GCS/ Big Query, working with GCP components Hands-on in Data Flow, Cloud Storage (GCS), Big Query and GitHub. Proficient in Python and SQL.Skills PreferredManages ETL: uses programming and tools for data ingestion, configures pipelines, applies transformations and decoding, integrates and fuses data, moves and securely deliver Experience NoSQL databasesAdditional InformationIf the candidate is remote only, please indicate """"100% Remote"""" under candidate's name on resume. Other similar schedule notations can be """"Local Candidate"""" or """"Hybrid Candidate."""" ***HRA Test Required***"""
Data Engineer,IVY TECH SOLUTIONS INC,United States (Remote),https://www.linkedin.com/jobs/view/3667192142/?eBP=JOB_SEARCH_ORGANIC&refId=r5m9CV5iULHiwwerT09nWg%3D%3D&trackingId=57f4Xj7R6QlfSonsBPCSZA%3D%3D&trk=flagship3_search_srp_jobs,3667192142,"About the job
            
 
HI,Kindly let me know if you have a suitable fit for the following positionThanksJob Title: Data EngineerRemoteC2C PositionPlease send the resume to sowmya.g@ivytechsol.us OR CALL: 2243488595Job Description:Data EngineerPosition can be 100% remoteSkillshive; KAFKA; Python; SPARK; UNIX; Hadoop platformRequired (Individual Role)  Extensive experience in engineering and designing data management solutions using Hadoop platform tools and technologies such as Apache HDFS, Sqoop, Spark, Hive, Impala, HBase, and Kafka Significant experience in Python programming Proficient in the data ingestion pipeline process, exception handling, and metadata management on Hadoop platforms. Hands-on experience creating automated data integration applications using data models, data mappings and business rules specifications to load data warehouses, operational data stores, data marts, and data lakes while programmatically handling exceptions including late arriving, missing, or erroneous data. Experience in UNIX shell scripting Demonstrated experience developing/expanding automated technology controls (e.g., Data Quality checks, Data Movement controls) Demonstrated experience enabling access to data by way of databases or dashboards. Demonstrated experience cleaning, filtering, transforming data, and/or enriching data
Thanks & Regards,Sowmya/ Recruiter2243488595IVYTECH solutions incPowered by JazzHRoeUP0YiVgP"
Sr. Azure Data Engineer,Volitiion IIT - Putting Intelligence in IT,United States (Remote),https://www.linkedin.com/jobs/view/3779272126/?eBP=JOB_SEARCH_ORGANIC&refId=r5m9CV5iULHiwwerT09nWg%3D%3D&trackingId=eDiRbbxw42z29n2PEqI17A%3D%3D&trk=flagship3_search_srp_jobs,3779272126,"About the job
            
 
Volitiion IIT Inc. is an IT Service and Staffing firm based of Leesburg VA. We have hiring Top Talents to add to our growing team of Technology Professionals.We are looking for a Sr. Azure Data Engineer to add to our team.Job OverviewPosition is remoteJob DescriptionYou will be working with all levels of technology from backend data processing technologies (Databricks/Apache Spark) to other Cloud computing technologies / Azure Data Platform. You should be a strong analytical thinker, detail-oriented and love working with data with a strong background in data engineering and application development. Must be a hand-on technologist passionate about learning new technologies and help improve the ways we can better leverage Advanced Analytics and Machine Learning.Responsibilities  Build end-to-end direct capabilities. Create and maintain optimal data pipeline architecture. Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources. Use analytics for capitalizing on the data for making decisions and achieving better outcomes for the business. Derive insights to differentiate member and team member experiences. Collaborate with cross-functional teams. Analyze and define with product teams the data migration and data integration strategies. Apply experience in analytics, data visualization and modeling to find solutions for a variety of business and technical problems. Querying and analyzing small and large data sets to discover patterns and deliver meaningful insights. Integrate source systems with information management solutions and target systems for automated migration processes. Create proof-of-concepts to demonstrate viability of solutions under consideration.
Qualifications  Strong hands-on experience leading design thinking as well as the ability to translate ideas to clearly articulate technical solutions. Bachelor’s degree in computer science, information systems, or other technology-related field or equivalent number of years of experience. Advanced hands-on experience implementing and supporting large scale data processing pipelines and migrations using technologies (eg. Azure Services, Python programming) Significant hands-on experience guiding technical teams as a mentor while leading collaboration with multiple teams across the organization. Significant hands-on experience with Azure services such as Azure Data Factory, Azure Databricks, Azure Data Lake Storage (ADLS Gen2), Azure SQL, and other data sources. Significant hands-on experience designing and implementing reusable frameworks using Apache Spark (PySpark preferred or Java/Scala) Solid foundation in data structures, algorithms, design patterns and strong analytical and problem-solving skills. Experience with any of the following Analytics and Information Management competencies: Data Management and Architecture, Performance Management, Information Delivery and Advanced Analytics
Desired  Proficiency in collaborative coding practices, such as pair programming, and ability to thrive in a team-oriented environment The following certifications:Microsoft Certified Azure Data EngineerMicrosoft Certified Azure Solutions ArchitectDatabricks Certified Associate Developer for Apache 2.4/3.0
4 Reasons To Join Volitiion IIT, Inc.  Our Commitment to You - We offer competitive pay, multi-year projects, and a list of exciting clients. Work-Life Balance - We work hard; we work smart and have quality time for family and ""life."" Our Mantra - We treat our consultants the way we want to be treated: with integrity, professionalism, and trust. Career Development - We help you meet your career goals and continuously support your efforts to build your skillset.
Check out our Referral Program!Volitiion IIT Inc will pay you up to $1000 for every qualified professional that you refer and we place. If you see a position posted by Volitiion IIT Inc. and know the perfect person for the job, please send us your referral.Volitiion IIT Inc. is an Equal Opportunity/Affirmative Action Employer."
#610 - Azure Data Engineer,Venon Solutions,United States (Remote),https://www.linkedin.com/jobs/view/3750802003/?eBP=JOB_SEARCH_ORGANIC&refId=r5m9CV5iULHiwwerT09nWg%3D%3D&trackingId=d3QZ1OyCkLC4q%2FP8oy7%2BbA%3D%3D&trk=flagship3_search_srp_jobs,3750802003,"About the job
            
 
Job opportunity only available for Latin America residents.Qualifications B2 English level (Upper-intermediate) or higher3+ years of experience in data integration and database development.Proficiency in Core SQL and Azure SQL.Extensive experience with Azure Data Factory and Databricks.Strong expertise in designing and implementing complex stored procedures.Experience with SSAS.Excellent problem-solving skills and attention to detail.
What do we offer? Type of contract: Independent Contractor with Venon Solutions LLCContract duration: Long-termBenefits: 2 weeks of PTO (paid time off)Holidays: from the North American calendar
Working hours: Full-time PST, fully committed"
Data Services Engineer,Catalist,"Washington, DC (Remote)",https://www.linkedin.com/jobs/view/3744911304/?eBP=JOB_SEARCH_ORGANIC&refId=r5m9CV5iULHiwwerT09nWg%3D%3D&trackingId=40T9K95fZqr4BwNCqC9IKQ%3D%3D&trk=flagship3_search_srp_jobs,3744911304,"About the job
            
 
For over 17 years, Catalist has been a leader in civic data and data science innovation. Our mission is to provide progressive organizations with the data, software, and services needed to better identify, understand, and communicate with the people they need to engage and mobilize. Our clients include the largest, most influential organizations in the U.S. active in civic engagement, advocacy, and political campaigns.Catalist is home to a dedicated, creative team of technologists, data scientists, and campaign experts committed to using our talents and technology to nurture a vibrant and growing progressive community.As a Data Services Engineer (DSE) at Catalist, you will have a leading role in providing support and creating solutions for a wide array of Catalist clients and partners as well assisting or leading efforts to manage cross-departmental projects. The DSE supports the Client Services team by providing reports, data for visualizations, lists, and appends that the Catalist suite of tools cannot readily provide.The ideal candidate will be a highly motivated individual with excellent technical skills, a strong desire to learn new skills, and an interest in progressive politics. Catalist values creativity and problem-solving. Our work is on the cutting edge of data-driven politics, and your support will help Democratic candidates and progressive organizations conduct successful advocacy and electoral campaigns.This position reports to the Deputy Chief Data Officer. While Data Services Engineers work closely with the Client Services, Analytics, and Tech teams, they are a part of the growing Data team that supports all underlying work at Catalist.This position is included in our CWA bargaining unit.Principal Duties & Responsibilities Acts as an advanced user of all internal and external Catalist tools, data, and products for the purpose of supporting client requests and internal projects as neededGains an understanding of client needs, political implications, and scope by interacting with the Client Services, Analytics, and Technology departments on a regular basisOversees all projects assigned from start to finish; successfully delivers reports, lists, and other material requests on time, accurately, and in a client-friendly format with actionable insightsExecutes requests for new business development support from Marketing teamEscalates issues and concerns to the Deputy Chief Data Officer as neededCreates and maintains documentation to support all deliverables for future replication
Requirements BS or BA or relevant experienceWillingness to be a problem solver and produce results in a fast paced environmentAbility to focus on details and make productive suggestions on ways to streamline and improve processesAbility to be creative and personable, and articulate ideas clearlyAbility and willingness to learn new skills quicklyAbility to become an internal subject matter expert on various datasets and support other Catalist departments/teams on usage of those datasetsBackground check required
Preferred Skills & Abilities Interest, familiarity or experience with SQL, Python or other relational database programming languageExperience in the following: Terminal, R, or using a Unix command line interfaceExperience managing projectsFamiliarity with Catalist data, progressive politics, voter files, and/or commercial data
BenefitsMedical, Dental, Vision, Prescription DrugCatalist offers Medical, Dental, Vision, and Prescription Drug coverage for eligible staff and their eligible dependents. Catalist’s Medical plan is a comprehensive PPO program including Prescription Drug coverage with 85% of the premium paid by Catalist. Dental and Vision coverage is provided at no cost to employees.Group Term Life Insurance and Long-Term & Short-Term Disability CoverageGroup Term Life Insurance and Long-Term and Short-Term Disability coverage is available for eligible staff. These benefits are provided at no cost to Catalist employees.401(k) Safe Harbor PlanA 401(k) Safe Harbor Plan is available to eligible staff with a 3% contribution from Catalist from the date of hire. Employees may contribute pre-tax or post-tax from their salary up to the legal limits set forth by the IRS.Medical and Dependent Care Flexible Spending Accounts (FSAs)Catalist offers an FSA Program that gives eligible staff the ability to pay out-of-pocket medical/dental/vision/child care expenses from pre-tax earnings.Transit BenefitsCatalist also makes available a Transit benefit FSA program to eligible employees using pre-tax contributions with a company match.Professional Development and Remote Work ExpensesEligible employees may be reimbursed up to $750 each year for professional development / education and remote work expenses.Student Loan PayDown or SaveUpCatalist offers a Student Loan PayDown and College SaveUp benefit for eligible staff.Vacation, Personal Leave, Sick Leave BenefitsCatalist offers generous vacation benefits to all eligible staff. Eligible employees also receive:  14 Paid Holidays Personal Days Sick Leave Parental Leave
Hybrid Office/Remote WorkCertain positions at Catalist are eligible for Office/Remote Hybrid or full Remote status."
Data Engineer - Remote,"The Dignify Solutions, LLC","Edison, NJ (Remote)",https://www.linkedin.com/jobs/view/3768002998/?eBP=JOB_SEARCH_ORGANIC&refId=r5m9CV5iULHiwwerT09nWg%3D%3D&trackingId=BW46b3xo3koUxW%2F1VQ%2F2yQ%3D%3D&trk=flagship3_search_srp_jobs,3768002998,"About the job
            
 
7+ Years of Experience2+ years of hands-on experience in ETL development for a data warehouse2+ years of hands-on experience in development, maintenance, and enhancements of Informatica Mappings, Workflows, and processesProficient in programming against large data assets with a working knowledge of SQLExpertise with integration technologies and processesHands-on experience or Knowledge about Airflow, or automated job scheduling tools.Experience with API/REST, JSONExperience working on an Agile Development team and delivering features incrementally.Experience with Git repositoriesExperience developing in PythonExperience working with Pytest framework, or other testing frameworksExperience with cloud platforms (AWS, Azure) with strong preference towards AWS.Experience in Database design practicesExperience building data automation pipelinesExperience working with Azure DevOps, JIRA or similar project tracking software.Experience with both Windows and Linux."
Data Engineer,Mudflap,United States (Remote),https://www.linkedin.com/jobs/view/3767199474/?eBP=JOB_SEARCH_ORGANIC&refId=r5m9CV5iULHiwwerT09nWg%3D%3D&trackingId=5wZJbLRndMbpBAZhCIu%2FMQ%3D%3D&trk=flagship3_search_srp_jobs,3767199474,"About the job
            
 
The Mudflap mobile app connects professional truck drivers with fuel stops in the $800B trucking industry. We help truckers save thousands of dollars on diesel fuel (their #1 business expense), while providing our truck stop partners with access to new, hard-to-reach customers.We are extremely product and customer centric, and engineers play a key role in major company wide projects. Data Engineers strengthen the existing capabilities of our Data Warehouse and support transformation of the company to make data driven decisions. We use a modern, cloud based data warehouse as well as other modern data technology.This is an opportunity to build at a rapid pace and continue creating a platform that hundreds of thousands of SMB trucking companies use to run their business - and love!In this role, you will: Assist in the development, testing, and deployment of data pipelines using SQL and/or PythonContribute to and maintain documentation of data flows and schema designCollaborate with senior developers to troubleshoot issues and optimize performanceCollaborate with business stakeholders to understand needs and collect feedback on solutionsParticipate in code reviews to uphold high-quality standardsProvide support to end users in building reporting and analyticsAssist with the estimation of development tasksTake ownership of your projects from beginning to endExhibit enthusiasm about sharing and adopting best practices with your teamBe flexible--you understand the needs of a startupBe detail-oriented, a fast learner, and scrappy 
You have: 3-7 years of Data Engineering experience with a focus on large data setsExperience with at least one cloud data platform (Snowflake, Redshift, BigQuery)Advanced SQL knowledge and Python experience is a plusKnowledge of Data Warehouse methodologies and modelingExperience with source control tools such as Git, SVN, and TFS
Perks and Benefits (What we offer):  A flexible, remote-first companyA team of talented individuals on a mission to change a $1 trillion industry for the betterA high bar for quality and commitment to self-improvement An open mind to new ideas and methodologiesCompetitive salary and benefit optionsOpportunities and support for massive career growth
The salary range for this role is $143,000 - $193,500. This information reflects a base salary range for this position based on current market data, which may be subject to change as new market data becomes available. The candidate's skills, experience, and other relevant factors will determine the exact compensation. This position may also be eligible for additional incentives such as equity awards or short-term incentives. Our benefits include medical/dental/vision insurance, 401(k) with company match, WFH stipend and PTO.Company overview (Who we are):Mudflap is well-funded by top-tier venture investors, including QED, Matrix, Commerce Ventures, NFX and 500 Startups, and our core team hails from Disney, Pandora, Meta/Facebook, Postmates, HomeAway (VRBO), Procore, DoorDash, Capital One, Uber, and Zillow.Here are the core values that we believe in and look for in new teammates: Put Customers First: We put our customers and partners at the center of everything we do.Be Curious: Have a learning mindset. Ask, wonder, dig, investigate. Share the joy of discovery.Sweat The Details: We create magical experiences by obsessing over every detail.Do The Right Thing: We operate with integrity and exercise good judgment, even if no one is watching.Find A Way: We push past roadblocks to get the best outcomes for our customers and our teammates.Own It: Show up prepared, be present, and make it count."
Snowflake Data Engineer + paid remote training,Jefferson Frank,United States (Remote),https://www.linkedin.com/jobs/view/3776182195/?eBP=JOB_SEARCH_ORGANIC&refId=r5m9CV5iULHiwwerT09nWg%3D%3D&trackingId=Cz77I3C9MVYM5QAX9Z2hFw%3D%3D&trk=flagship3_search_srp_jobs,3776182195,"About the job
            
 
Snowflake Data Engineer - paid remote training and placement opportunityBecome a Snowflake Data Engineer with this paid career programWant to use your programming skills in an exciting new sector? Interested in using your SQL knowledge and communication skills to cross-train as a specialist in the world's leading cloud-based data warehouse? Then, this Snowflake Data Engineer career program is for you!We're looking for professionals with SQL experience and basic Python programming knowledge who want to cross-train as Snowflake Data Engineers on a two-year program that includes paid training, certifications, and a remote work placement with our market-leading client, based in Wisconsin.The opportunityYou don't need any prior experience with Snowflake to apply.As a member of the Snowflake Partner Network, we'll get you up to speed on Snowflake, supporting you in gaining your industry certifications to become a certified Snowflake Data Engineer.We'll then place you with our client in a paid full-time role, to give you that all-important work experience.RoleOnce you're trained and certified, you'll work with the client for a period of up to two years. Your role as a Snowflake Data Engineer could include:  Developing, testing, and maintaining data pipelines Using these to convert raw data into usable formats and organize it for efficient use Ensuring that your company's data strategies are aligned to support its business goals Designing, building, and deploying custom functional solutions using Salesforce products Collaborating with a wide range of stakeholders across the business, from data analysts to the C-suite
Who we're looking forWe're looking for intrinsically motivated individuals who are passionate about learning a new cloud technology. This role will suit a self-starter, who has lots of motivation to work independently and also as part of a team.You'll join a fast-growing business where change takes place quickly, so adaptability and flexibility are key. You'll be asked to speak up and share ideas to problem-solve, so we're looking for someone comfortable providing and receiving honest feedback from a wide range of stakeholders.Since the role is client-facing, strong communication and interpersonal skills are a must.To Apply For This Program, You'll Also Need  2+ years of SQL experience at least 1 year of basic Python programming knowledge
Bonus Points If You Have  a Financial Services background ETL, ELT, and basic data modeling experience prior experience using cloud technology
To Apply For This Program, You'll Also Need To  Commit to a period of up to two years working in a paid work placement, post-training Be based anywhere in the USA (East Coast, NY tristate area, or Boston preferred but not essential) Available to start training in January 2024
Although this is a remote role, there might be some occasional travel from time-to-time, which will be discussed with you with plenty of notice.We are offering a salary range of $60,000 to $80,000 USD, depending on your skills and experience. We also offer a full range of company benefits including health benefits and retirement/401k plan.If you take this opportunity, you will also receive 8 weeks of fully-funded comprehensive training to become a Snowflake Data Engineer, before you start working on the client's site.SkillsThe training will primarily use Snowflake, dbt, and Fivetran with plenty of hands-on exercises and real-world projects to put your new skills into practice before you're deployed on your work placement. The training will also include:  Advanced Snowflake SQL skills SnowPro Core Certification dbt Analytics Engineering Certification Extensive consultancy skills training Support to gain further certifications while you're completing your work placement Access to mentoring and support programs and networking opportunities
Want to find out more about Revolent? Visit our website or check out career stories from others who have launched amazing careers in the cloud by cross-training with us.Please also feel free to email me directly with your resume to m.ramirez@jeffersonfrank.com"
AWS Data Engineer with Abinitio,PSRTEK,United States (Remote),https://www.linkedin.com/jobs/view/3657164816/?eBP=JOB_SEARCH_ORGANIC&refId=r5m9CV5iULHiwwerT09nWg%3D%3D&trackingId=j5CNbRf8wIOqj1FqcA9ebQ%3D%3D&trk=flagship3_search_srp_jobs,3657164816,"About the job
            
 
Hi,Hope you are doing good today.A short description of the job can be found below. If you are interested, kindly respond with an updated copy of your resume.Role: AWS Data EngineerLocation: St Louis, MO (Remote Opportunity)Duration: Long Term Contract Mandatory Skills: AWS, Bigdata, Databricks, Ab InitioJob DescriptionEngineers can articulate clear business objectives aligned to technical specifications and work in an iterative, agile pattern to deliver value. They have ownership over their work tasks, and embrace interacting with all levels of the team and raise challenges when necessary.We aim to be cutting-edge data and software engineers, not institutionalized developers.Candidate Should Have A passion for healthcare and the potential for technology to improve people's lives.The aptitude to deliver enterprise or customer-facing applications in a variety of scenarios.A deep understanding of environmental constraints to implementing and deploying differing application solutions.Proven track record of quality software development and an ability to innovate outside of traditional architecture/software patterns when needed.A desire to collaborate in a high-performing team environment, and an ability to influence and be influenced by others.Experience in helping to distil complex ideas into an architectural and implementation plan, then building them outThe ability to critically evaluate the trade-offs and implementation impacts of emerging vs. more established technologies.
Skills, Qualifications, AndCertifications (not all are required): Bachelor's degree or equivalent experience required; a degree in a quantitative discipline: statistics, applied mathematics, computer science, data mining, machine learning, or some other empirical science.Expertise in Big Data and related technologiesCloud certifications, and/or experience developing applications and infrastructure in AWS.Knowledge of Apache Spark, distributed application development on Spark using PySpark or Scala, and familiarity with other Hadoop ecosystem such as HDFS, Hive, Oozie, ImpalaExperience with distributed computing and vendor solutions such as Databricks and Domino operationalizing predictive models, developing data pipelines, and general data engineering tasks.Experience operationalizing predictive and heuristic models.Experience designing and building resilient data pipelines and orchestrating data flow using tools such as Airflow and DbtExperience in development using scripting languages such as Python and BashExperience in development using object-oriented languages such as Python, Java, or ScalaExperience writing and maintaining cloud Infrastructure as Code using Terraform or Amazon Cloud Development Kit (AWS CDK)Experience with containerization and container orchestration using tools like Docker and KubernetesComfortable configuring and using multiple Unix operating systems such as MacOS and Red Hat Linux, both locally and via SSHDesign and architecture expertise with AWS, Hadoop, and TeradataExperience with disciplined agile software development lifecycleAbility to communicate at various levels within large organizations.Knowledge of Big Data tool performance/tuning/measurement and usage patterns to drive appropriate tool selections and use.Knowledge and/or experience with Health care information domains a plusDocumented experience working on advanced Big Data solutions is a plus.Some expertise in designing business intelligence systems, dashboard reporting, and analytical reporting is a plus.DevOps and general CI/CD development experience is a plus (Jenkins/Ansible/CloudBees)
Job Duties Develop predictive and heuristic models and data pipelines with Spark (both PySpark and Scala)Understand business requirements and provide design, analysis, and performance improvements for all models.Use and maintain CI/CD pipeline for seamless deployment of all models.Conduct code reviews to maintain highest quality of code.Conduct knowledge transfer sessions and responsible for providing technical support on entire software development lifecycle.Involved in identifying manual processes and automate them using Bash, Python, and ScalaPerform quality analysis to monitor and refine models and model metrics such as accuracy scores, updating business specifications, as necessary.Write unit tests to capture business requirements and usage patterns with high code coverage.
Improve overall performance by writing efficient Scala/Python code and identifying and fixing bugs/inefficiencies found in legacy systems.Regards,B. LokeshAccount Manager- Recruitment and OperationsPSRTEK Inc.Princeton, NJ 08540lokesh@psrtek.comhttps://www.linkedin.com/in/lokesh-bala-93469abb/""Culture of excellence""PSRTEK is a reputed technology recruitment and IT staffing brand with a global footprint and an admired client base. As an ideas and innovation powerhouse with a culture of excellence, we bring remarkable expertise and deliver powerfully transformative results."
Azure Data Engineer,Amsive,"Greenville, SC (Remote)",https://www.linkedin.com/jobs/view/3778951526/?eBP=JOB_SEARCH_ORGANIC&refId=r5m9CV5iULHiwwerT09nWg%3D%3D&trackingId=AkpQueXR7voFpOwHCDehRQ%3D%3D&trk=flagship3_search_srp_jobs,3778951526,"About the job
            
 
Amsive is a data-driven performance marketing agency that enhances marketing ROI through innovative customer acquisition and engagement solutions. As a full-service agency, Amsive enables growth by leveraging digital and direct-native expertise, with dedicated teams shaping powerful strategies, creative executions, with direct and digital solutions, including SEO, paid search, media, and performance measurement. At Amsive, our people are our first priority. With approximately 1,000 hybrid employees across 6 regional offices, we bring together the brightest minds in direct and digital marketing to challenge the status quo as a new kind of agency. With a focus on collaboration and innovation, we deliver measurable results and drive growth for our clients while providing a supportive environment where you can focus on your career growth. We offer competitive wages, excellent benefits, and a positive work environment designed around the commitment to mutual respect and the challenge of contributing to the continued success of our organization. Explore our culture.  This is a REMOTE position. Candidates can be located anywhere in US Time Zones but will work in Eastern/CT**
Job SummaryAs an Azure Data Engineer will work with clients, internal business stakeholders, and other data engineers to translate business requirements into modern and innovative data integration routines to move, cleanse, and transform large data sets in an efficient and scalable manner.Job Responsibilities Data hygiene, ETL/ELT pipeline development and administration, computations, aggregations, analytics, ad-hoc queries, studies, and maintenance in accordance with enterprise data governance standardsMonitor and maintain data pipelines proactively to ensure high service availabilityTroubleshoot incidents, identify root causes, resolve, and document problems, and implement preventive measures including alerting and notificationsDocument data flows and mappings as neededCollaborate with the Architecture team to modify and improve existing data management systemsWork with Data Scientists to understand mathematical models and optimize data solutions accordinglyWork with BI teams to understand reporting requirements and optimize data solutions accordingly
Job Requirements Deep experience in developing robust, scalable, and resilient data management systemsBachelor’s degree from an accredited university or college and/or demonstrated technical knowledge and equivalent work experience3+ years of experience developing queries and ELT/ETL solutions using Microsoft tools (SQL, ADF, ADLS, Azure Synapse), working with structured, unstructured, and semi-structured datasets2+ years of experience in architecting, designing, developing, implementing, and optimizing cloud solutions leveraging Azure services1+ years of experience working with one or more languages commonly used for data operations including SQL, Python, Scala, and R1+ years of experience in Big Data Distributed systems such as Databricks, Apache Spark, etc.Knowledge of integrating data pipelines with RESTful web servicesStrong problem-solving, communication, and interpersonal skillsMust be creative, organized, detail oriented, and able to assimilate information quicklyAbility to work in a team as well as independently
Preferred Competencies Experience with marketing or campaign management systems is a plusAzure certifications: Azure Data Engineer Associate, Azure Data FundamentalsExperience with CI/CD systems (Azure DevOps preferred)Understanding of Master Data Management conceptsWorking experience in Agile Scrum environmentsWorking knowledge of BI Reporting tools (PowerBI, Tableau)Experience with container management frameworks such as Docker, Kubernetes, ECR etc.NoSQL experience (particularly GraphQL/neo4j) is a plus
If you need any assistance seeking a job opportunity at Amsive, or if you need reasonable accommodation with the application process, please call (331) 318-7800.Amsive is proud to be an Equal Opportunity Employer. We are committed to building a supportive and inclusive environment for all employees. It is Amsive’s continuing policy to provide equal employment opportunity and not to discriminate on the basis of race, color, religion, pregnancy or childbirth, marital status, national origin, ancestry or citizenship status, age, disability, sex, sexual orientation, gender identity, veteran status or any other characteristic protected by applicable federal, state or local laws. This policy applies to all aspects of employment, including (but not limited to) application for employment, recruiting, hiring, compensation, benefits, promotions and transfers, training, layoffs, rehires, termination of employment and all other terms and conditions of employment.As part of the Company's equal employment opportunity policy, Amsive will also take affirmative action as called for by applicable laws and Executive Orders to ensure that minority group individuals, females, disabled veterans, recently separated veterans, other protected veterans, Armed Forces service medal veterans, and qualified disabled persons are introduced into our workforce and considered for promotional opportunities.Powered by JazzHRLrBX35cWwW"
Data Integration Engineer - REMOTE --WORK FROM HOME!,SYNQ3 Restaurant Solutions,"Colorado Springs, CO (Remote)",https://www.linkedin.com/jobs/view/3751660153/?eBP=JOB_SEARCH_ORGANIC&refId=r5m9CV5iULHiwwerT09nWg%3D%3D&trackingId=Az7vI4GnjJVhEQCFgeOtMA%3D%3D&trk=flagship3_search_srp_jobs,3751660153,"About the job
            
 
Job DetailsDescriptionWho is SYNQ3?SYNQ3 is the restaurant industry’s leading provider of conversational ordering solutions. SYNQ3’s combination of highly skilled team members and a proprietary conversational AI platform enables restaurants to enhance their order processing, provide better customer service for to-go and catering orders, and increase their sales and profits. SYNQ3 provides ordering solutions to thousands of restaurants for many of largest chains and has processed billions of dollars of orders for our clients.Role DescriptionREMOTE - WORK FROM HOME!The Data Integration Engineer is primarily responsible for helping to build and maintain our core data integration platforms. Work primarily involves cleaning, parsing, storing, enriching, and validating data from a growing number of systems and services at varying velocities. Tasks will vary in complexity and scope, with examples ranging from service endpoint integrations to collaborating on larger team projects designed for Machine Learning and Analytics consumption. The successful candidate must work well in a team environment, enjoy a dynamic fast-paced atmosphere, possess excellent problem-solving and critical thinking skills, and display a passion for learning new methods and technologies.Responsibilities Ensure parsed, validated data is reliably processed, stored, and enrichedShape and transform data from various structured, semi-structured, and unstructured sourcesOptimize complex database queries and authoring large Stored Procedures, Views and Triggers for data centric enterprise business solutionsMonitor availability, performance, and utilization of all core and edge integration pointsIdentify and resolve data pipeline and endpoint performance and utilization issuesCollaborate with other technical resources and cross-department staff on various projectsMaintain relevant data stewardship documentationProvide Tier-3 support for data ingress/egress service and availability incidentsAs this position supports critical business functions, On-Call work is requiredApplication development and maintenance across Microsoft Technology stack with a focus on Microsoft Azure Synapse, Web API, SQL Server, Spark, and Databricks.App Service, Logic Apps, Web Jobs, Azure Functions in an integrated development & test environment involving Azure SDK, Visual Studio and Azure EmulatorAll other duties as assigned
Qualifications Degree (AS/BS) desired but may be substituted with relevant experienceExperience with MS Azure (ADF, Storage/Warehousing, App Svcs)Experience with PythonExperience with ingesting, processing, and storing various data source formats (JSON, XML, CSV)Experience with DML/DDL T-SQL
Benefits And Compensation An annual salary of $90k - $115k based on experience.Accrued paid vacation time in accordance with our policy.Medical, dental, vision, and life insurance benefits are available on the 1st of the month following 60 days of employment.Optional group rate long-term and short-term disability will also be available on the same schedule.Direct deposit."
Data Engineer Lead(with QE Exp.) - Remote,HireKeyz Inc,United States (Remote),https://www.linkedin.com/jobs/view/3728527548/?eBP=JOB_SEARCH_ORGANIC&refId=r5m9CV5iULHiwwerT09nWg%3D%3D&trackingId=g2%2FLV5LiTCQSgP1zIKLOhg%3D%3D&trk=flagship3_search_srp_jobs,3728527548,"About the job
            
 
Job Role: Data Engineer Lead(with QE Exp.)Location: RemoteEmployment: ContractJob DescriptionLooking for a Data Engineer with knowledge in Python programming, DWH, Strong in PL/SQL programming, Optimization of stored Procedures and knowledge in Test Automation ( using Python) ."
Data Engineer/Data Scientist,iTech Solutions,United States (Remote),https://www.linkedin.com/jobs/view/3714396620/?eBP=JOB_SEARCH_ORGANIC&refId=r5m9CV5iULHiwwerT09nWg%3D%3D&trackingId=YO9a3Wy8cCS1ZEWsPcPfkQ%3D%3D&trk=flagship3_search_srp_jobs,3714396620,"About the job
            
 
Qualifications:Skills associated with being a data engineer/data scientist Must be well versed in data manipulation, data organization, and process improvementMust have extensive knowledge of Tableau, creating metrics reports, and creating dashboards with proper data visualizationMust be proficient in Python, R, Spark and/or Power Bi Some experience using Git, GitHub and GitLab for code repositorySome knowledge of Hadoop and connecting/setting up live Tableau dashboards Some knowledge of AI/Machine LearningSome knowledge of RPA and how to automate processes, including automation of existing dashboard and reporting processesSome knowledge of agile working processes, Scrumban and product management processesResponsibilities:Contractor would be responsible for accomplishing the following objectives:Review and validate the current process, data management practices and data organization to ensure data is appropriately obtained, combined and protected within data repositories, published dashboards and reporting processesAutomate the process where possible – suggest using Python and/or Tableau but are open to other tools as long as they are consistent with tools used in the FR; many data sources and/or data refresh processes are manual – looking to reduce the level of manual intervention to support analytic solutionsRefine and modernize the current reports by creating a new look and feel that is sustainable and allows for the expansion of customers to be reported on.The idea is to utilize productivity gains from automation to offset the expansion of customer base and ability to use dashboards are the main reporting tool for our usersCreate documentation that describes the new processes end-to-endCreate user access processes to support expansion of customer base, and ensure data is appropriately secured and accessible only by those who have a business need to the dataContractor must be able to work in small teams and be comfortable with knowledge sharing Risks Existing data sources are moving to new solutions which may impact the existing data collection, analysis and reporting processes.These processes will need to be updated as new solutions are implemented, and we will need to ensure updates enable automation as much as possible to further support expansion of analytic capabilities.

Desired Skills and Experience
                TABLEAU"
Data Engineer - Azure (Scala/Kafka),Tiger Analytics,United States (Remote),https://www.linkedin.com/jobs/view/3719413249/?eBP=JOB_SEARCH_ORGANIC&refId=r5m9CV5iULHiwwerT09nWg%3D%3D&trackingId=eQf3lIN9c4ZE0ni2WarrZw%3D%3D&trk=flagship3_search_srp_jobs,3719413249,"About the job
            
 
DescriptionTiger Analytics is pioneering what AI and analytics can do to solve some of the toughest problems faced by organizations globally. We develop bespoke solutions powered by data and technology for several Fortune 100 companies. We have offices in multiple cities across the US, UK, India, and Singapore, and a substantial remote global workforce.We are expanding our Data Engineering practice and looking for Sr. Azure Data Engineers to join our growing team of analytics experts. The right candidate will have strong analytical skills and the ability to combine data from different sources and will strive for efficiency by aligning data systems with business goals.This is a remote role for applicants based in USA.Requirements Bachelor’s degree in Computer Science or similar field8+ years experience in Data Engineering + several years in Analytics spaceStrong Proficiency in Scala - coding experience a mustStrong Proficiency in Kafka and ADF for data pipelines /migration experience a must (Azure Synapses) Experience with real time streaming, Kafka, and API Integration Experience in PySparkStrong Proficiency in Python programmingStrong Proficiency in SQL queriesExperience building data pipelines using Azure stackExperience using Apache sparkGood working experience on Delta Lake and ETL processingPrior experience of working in a Unix environmentExperience in harmonizing raw data into a consumer-friendly format using Azure DatabricksExperience extracting/querying/joining large data sets at scaleExperience building data ingestion pipelines using Azure Data Factory to ingest structured and unstructured dataExperience in data wrangling, advanced analytic modeling is preferredStrong communication and organizational skills
BenefitsThis position offers an excellent opportunity for significant career development in a fast-growing and challenging entrepreneurial environment with a high degree of individual responsibility."
Data Engineer | Contract to Hire  | Remote,Spanco Solutions,United States (Remote),https://www.linkedin.com/jobs/view/3728263830/?eBP=JOB_SEARCH_ORGANIC&refId=r5m9CV5iULHiwwerT09nWg%3D%3D&trackingId=qbgWrDqiZZRMiN19fGcPsg%3D%3D&trk=flagship3_search_srp_jobs,3728263830,"About the job
            
 
Title: Data EngineerJob Type: Contract to Hire RemoteTop 3 Skills: Data Manipulation (DBX, Spark, SQL) Job DescriptionSupply Chain Data Strategy and Cloud Operation are core to the clients operations. With sales of over $100 B, the capability of delivering critical data and building cloud first solutions our teams offer are one of the most critical capabilities of the company. We’re looking for individuals who can bring their core set of knowledge as well as learn new tools to provide new data and capabilities to support growing Supply Chain. Accountable for developing and delivering technological responses to targeted business outcomes. Analyze, design and develop enterprise data and information architecture deliverables, focusing on data as an asset for Supply Chain and the overall enterprise. Understand and follow reusable standards, design patterns, guidelines, and configurations to deliver valuable data and information across the enterprise, including direct collaboration with 84.51, where needed. Demonstrate the company’s core values of respect, honesty, integrity, diversity, inclusion and safety. 
Key Responsibilities Create and leverage Databricks notebooks to source, shape and store data using DBX, SQL, Python, PySpark Utilize enterprise standards for data domains and data solutions, focusing on simplified integration and streamlined operational and analytical uses Ensure there is clarity between ongoing projects, escalating when necessary, including direct collaboration with 84.51 Define high-level migration plans to address the gaps between the current and future state Analyze technology environments to detect critical deficiencies and recommend solutions for improvement Promote the reuse of data assets, including the management of the data catalog for reference"
Data Engineer,"Enterra Solutions, LLC","Princeton, NJ (Remote)",https://www.linkedin.com/jobs/view/3681509550/?eBP=JOB_SEARCH_ORGANIC&refId=r5m9CV5iULHiwwerT09nWg%3D%3D&trackingId=cxCrTEIbgnwL8g8ca2KwbA%3D%3D&trk=flagship3_search_srp_jobs,3681509550,"About the job
            
 
LOCATION: U.S. Eastern Time ZoneMust reside in the US – preferably in the Eastern Time Zone. Remote working permitted. Must be eligible to work in the US without sponsorship now or in the future. This is a full-time position with benefits. Contractors will not be considered for this position.Who we are:Enterra provides solutions that leverage sophisticated machine learning, artificial intelligence (ontologies, inference engines and rules) and natural language processing to provide highly actionable insights and recommendations to business users. Today, our solutions impact just about every aspect of the products you buy at your local store – from what is available to how it is priced and even where it is placed on the shelf. Our SolaaS (Solution as a Service) solutions are deployed within private clouds – principally on Azure. We help transform market-leading companies into true data-driven digital enterprises.What you will do: The ideal candidate must be collaborative, and deadline driven. Because of the nature of our work and our technology, successful candidates must take a growth mindset and be comfortable with ambiguity, with the ability to take a proactive, structured approach to achieve results. Results-orientation and deadline driven are critical in our fast-paced environment.The successful candidate will join a diverse team to: Build unique high-impact business solutions utilizing advanced technologies for use by world class clients. Create and maintain the underlying data pipeline architecture for the solution offerings from raw client data to final solution output. Create, populate, and maintain data structures for machine learning and other analytics. Use quantitative and statistical methods to derive insights from data. Guide the data technology stack used to build Enterra's solution offerings. Combine machine learning, artificial intelligence (ontologies, inference engines and rules) and natural language processing under a holistic vision to scale and transform businesses — across multiple functions and processes. 
Responsibilities Include:  Work with other Enterra personnel to develop and enhance commercial quality solution offerings Create and maintain optimal data pipeline architecture, incorporating data wrangling and Extract-Transform-Load (ETL) flows. Assemble large, complex data sets to meet analytical requirements – analytics tables, feature-engineering etc. Build the infrastructure required for optimal, automated extraction, transformation, and loading of data from a wide variety of data sources using SQL and other 'big data' technologies such as Databricks. Build automated analytics tools that utilize the data pipeline to derive actionable insights. Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. Design and develop data integrations and data quality frameworkDevelop appropriate testing strategies and reports for the solution as well as data from external sources. Evaluate new technology for use within Enterra. 
Work with other Enterra and client personnel to administer and operate client-specific instances of the Enterra solution offeringsConfigure the data pipelines to accommodate client-specific requirements to onboard new clients. Perform regular operations tasks to ingest new and changing data – implement automation where possible. Implement processes and tools to monitor data quality - investigate and remedy any data-related issues in daily solution operations. 
Requirements: Bachelor's degree in Computer Science or a STEM (Science, Technology, Engineering or Math) field requiredMinimum of 3 years hands on experience as a data engineer or similar position. Minimum of 3 years commercial experience with Python or Scala Programming LanguageMinimum of 3 years SQL and experience working with relational databases (Postgres preferred). Experience with at least one of the following – Databricks, Spark, Hadoop or KafkaDemonstratable knowledge and experience developing data pipelines to automate data processing workflowsDemonstratable experience in data modeling Demonstratable knowledge of data warehousing, business intelligence, and application data integration solutionsDemonstratable experience in developing applications and services that run on a cloud infrastructure Azure preferredExcellent problem-solving and communication skillsAbility to thrive in a fast-paced, remote environment. Comfortable with ambiguity with the ability to build structure and take a proactive approach to drive results. Attention to detail – quality and accuracy in work is essential. 
The following additional skills would be beneficial: Knowledge of one or more of the following technologies: Data Science, Machine Learning, Natural Language Processing, Business Intelligence, and Data Visualization. Knowledge of statistics and experience using statistical or BI packages for analyzing large datasets (Excel, R, Python, Power BI, Tableau etc.). Experience with container management and deployment, e.g., Docker and Kubernetes"
Data Engineer,"Changeis, Inc.","Arlington, VA (Remote)",https://www.linkedin.com/jobs/view/3778337444/?eBP=JOB_SEARCH_ORGANIC&refId=r5m9CV5iULHiwwerT09nWg%3D%3D&trackingId=1i3m6sEu8lvdgyw3GV2llA%3D%3D&trk=flagship3_search_srp_jobs,3778337444,"About the job
            
 
Changeis, Inc. is an award-winning 8(a) certified, woman-owned small business that provides management consulting and engineering services to the public sector. Changeis' work has resulted in the successful execution of numerous programmatic initiatives, development of acquisition-sensitive deliverables, and establishment of a variety of long-term innovative strategic priorities for its customers. Changeis focuses on delivering unparalleled expertise in the areas of strategy and transformation management, investment analysis and acquisition management, governance, and innovation management. Inc. magazine has ranked the management consulting firm, Changeis Inc., among the top 1000 firms on its 35th annual Inc. 5000, the most prestigious ranking of the nation's fastest-growing private companies. Changeis offers a full benefit package that includes medical, dental, and vision, short and long term disability, retirement plan with immediate vesting and company match, and a generous annual leave plan.The Data Engineer will partner with a Federal Agency Office of Human Resources, focusing on essential areas such as business management, strategic planning, and decision-making. By developing and maintaining data architectures, engaging in acquisition/contract management, and applying expertise in information technology, data analytics, and knowledge management, the Data Engineer will significantly contribute to the optimization and innovation of organizational processes. The Data Engineer will collaborate with product design and engineering teams to understand their needs, and then research and devise innovative statistical models for data analysis. By communicating findings to all stakeholders and using analytics for meaningful insights, they will enable smarter business processes and stay abreast of current technical and industry developments.Roles And Responsibilities  Collaborating with clients and end-users to understand their mission, architecture, and security requirements.  With a focus on the client's goals, build a design that will scale to meet their evolving needs.  Recommend tools and capabilities based on your research of the environment and new technology.  Design the standard for future development, so you'll craft an architecture that smoothly works with existing infrastructure without compromising security.  Identify new opportunities to build platform-based solutions to help your customers meet their toughest challenges.  Developing and running ETL (Extract, Transform, Load) processes to manage data flow and ensure data quality. 
Requirements  8+ years of experience with designing, developing, operationalizing, and maintaining complex data applications at enterprise scale.  5+ years of experience with creating software for retrieving, parsing, and processing structured and unstructured data.  5+ years of experience with developing scalable ETL workflows for reporting and analytics.  5+ years of experience with using Python, SQL, Scala, or Java  3+ years of experience with AWS.  3+ years of experience with using Docker, Kubernetes, and Helm.  3+ years of experience with Distributed data and computing tools, including Spark, Databricks, Hadoop, Nifi, Kafka, Scala, or Java.  Experience with developing scripts and programs for converting several types of data into usable formats, supporting project team to scale, and monitoring and operating data platforms.  Ability to supervise others and lead projects and deliverables in a collaborative, cross-functional team environment.  Prefer candidates holding active AWS certifications."
REMOTE DATA ENGINEER,Skiltrek,"Sandy, UT (Remote)",https://www.linkedin.com/jobs/view/3768034278/?eBP=JOB_SEARCH_ORGANIC&refId=r5m9CV5iULHiwwerT09nWg%3D%3D&trackingId=FAEejyXWG9NMm%2B0ALhVnNw%3D%3D&trk=flagship3_search_srp_jobs,3768034278,"About the job
            
 
Job DescriptionSelling points  Great Benefits Tons of room for growth Gym Allowance
Minimum RequirementsMust-haves  4+ years as a Data Engineer or related work experience or 6+ years if they do not have a bachelor's degree in information technology or a related field Proven work experience as an ETL or data engineer (3+ years of experience preferred) Proven experience being able to analyze data and data sources (clarifying data requirements, concepts, data types and structures) Hands on experience identifying patterns, relationships, and flows, and troubleshoot data inconsistencies Experience developing and maintaining code in the ETL processes Using stored procedures, T-SQL, and SSIS, against a variety of data sources such as PostgreSQL, Microsoft SQL Server, flat files, etc. 2+ Years of experience with SQL Server, including creating tables, stored procedures, views, setting up jobs, and troubleshooting code and data issues Working experience developing SSIS packages (1+ years of experience preferred) Working experience designing and developing data warehouse dimensional modeling Experience documenting business rules and cases to be applied to the ETL processes Experience Implementing quality automated testing for data completion, accuracy, integrity, and consistency
Desired SkillsPlusses  Experience building data warehouse from scratch Insurance industry experience"
Staff Data Engineer - Remote | WFH,Get It Recruit - Information Technology,"New York, NY (Remote)",https://www.linkedin.com/jobs/view/3769033681/?eBP=JOB_SEARCH_ORGANIC&refId=r5m9CV5iULHiwwerT09nWg%3D%3D&trackingId=V9c0kBTEMRLQHPKQc3AsNQ%3D%3D&trk=flagship3_search_srp_jobs,3769033681,"About the job
            
 
We are on a mission to simplify the purchasing experience for businesses. We believe that it's not just about what you buy, but how you buy it. The current purchase-to-pay process is complex and often misses opportunities for leverage. Our goal is to highlight that leverage through our innovative platform.Our company streamlines manual purchasing and payment tasks, providing your team with a centralized hub for purchasing, approval, tracking, and payment of all the physical goods your business needs. Our customizable budgets and reporting empower operations and finance teams to regain control over the buying process, saving time, money, and gaining clarity in procurement.Founded in 2016 and headquartered in New York City, our company oversees nearly half a billion dollars in annualized spend across a diverse customer base, including companies like WeWork, SoulCycle, and Lume. We've secured $50 million in funding from esteemed investors such as MIT, Stage 2 Capital, Rally Ventures, 645 Ventures, and more. Our company has been recognized as a ""50 to Watch"" by Spend Matters and honored as a ""Best Place to Work"" by BuiltIn and Inc. Magazine.Job Description: Staff Data EngineerAs a Staff Data Engineer at [Company], you will play a pivotal role in designing, building, and maintaining our data infrastructure. Your expertise in data modeling, ETL pipelines, and data warehousing will be crucial for data-driven decision-making throughout the organization. Join us in shaping the future of our organization by unlocking the power of our data and transforming it into actionable insights.ResponsibilitiesContribute to the data roadmap and OKRs, ensuring the identification, prioritization, and delivery of data infrastructure initiatives.Plan epics, ensuring they are well understood, appropriately broken down, and prioritized by the team.Design and build simple services and systems with a focus on iterative development, reliability, and minimizing the cost of future changes.Continually optimize the data architecture to provide a reliable and adaptive infrastructure that scales with the business.Support existing data pipelines and build new integrations based on business needs.Develop and implement monitoring processes.Deploy and monitor products on cloud platforms.Strive for continuous learning and improvement, providing technical mentorship wherever possible.Participate in rotating on-call duties, including incident management.QualificationsStrong leadership mindset with a focus on accountability.Results-oriented and passionate about helping team members grow and improve.Proficient in writing tests as an integral part of the development process.Experienced in designing and building software incrementally.Natural ability to rally people around achieving a common goal.Collaborative, open-minded, and committed to continuous professional development.Technical SkillsExpertise in Python and SQL.Hands-on experience with data orchestration tools (preferably Airflow, Dagster, AWS Step Functions).Proven track record of implementing AWS cloud services (Lambda, SQS, ECS).Skills in infrastructure as code (preferably Terraform).Experience with big data platforms in production (Spark/PySpark, AWS Glue, EMR).Knowledge of data lake tools on AWS (S3, Glue Catalog, LakeFormation).Understanding of business intelligence and data visualization tools (Metabase or Tableau).Familiarity with CI/CD and supporting tools like GitHub Actions and CircleCI.Strong understanding of data security and experience in data protection.Nice To HaveExperience with streaming data architecture (AWS Kinesis, Spark Stream, Kafka Stream, Flink, Storm).Experience with MLOps model deployment and pipelines using AWS services (Sagemaker, S3, ECR, Redshift).Additional InformationWhat You'll Receive:Competitive compensation package, including stock options.Robust medical, dental, vision, and wellness benefits.Flexible time off and remote work policies.Employer-sponsored 401(k).Note: The anticipated annual base salary range for this role is $210,000 - $220,000. Actual compensation and title will be commensurate with experience, qualifications, knowledge, and skills.We are an equal opportunity employer. Applicant qualifications are considered without regard to race, color, religion, sex, national origin, age, disability, veteran status, genetic information, or any other basis prohibited by law.#BI-RemoteEmployment Type: Full-Time"
Data Scientist Engineer,Team Remotely Inc,"Phoenix, AZ (Remote)",https://www.linkedin.com/jobs/view/3780154126/?eBP=JOB_SEARCH_ORGANIC&refId=sjQPSlxFFJAXxmDgKKgqXg%3D%3D&trackingId=Uvq8lOFX%2BSC%2B6mnZb8jOsw%3D%3D&trk=flagship3_search_srp_jobs,3780154126,"About the job
            
 
This is a remote position.Data Scientist Engineer (US/Canada, 1 Year Experience, Remote)Team Remotely Inc. is a staffing and recruitment agency that offers a comprehensive solution for talent acquisition, including sourcing, vetting, pay rolling, and managing talent. Whether you need contract staffing, direct hire, direct sourcing, talent pools, or diversity initiatives, our model can support your hiring strategy. Hiring Type: Full-Time Base Salary: $67K-$78K Per Annum. How to Apply: Please visit teamremotely.com to learn more & apply.Skills and Abilities: Strong knowledge of R or Python for data analysis and modeling. Proficiency in statistical programs such as R, SAS, MATLAB, or Python. Familiarity with spreadsheets (VBA) and database applications (Access, Oracle, SQL, or equivalent technology). Basic understanding of SQL, Javascript, XML, JSON, and HTML. Ability to learn new methods quickly and work under deadlines. Excellent teamwork and communication skills. Strong analytical and problem-solving abilities. Basic understanding of SQL, Javascript, XML, JSON, and HTML. 
Preferred: Knowledge of actuarial concepts and life, health, and/or annuity products. Experience with statistical modeling techniques such as GLM, Decision Trees, Time Series, Regression, etc. Familiarity with Microsoft DeployR. Exposure to insurance risk analysis. Basic experience in computational finance, econometrics, statistics, and math. Knowledge of SQL and VBA. Familiarity with R or Python for predictive modeling 
 Why work with Team Remotely?Team Remotely Inc. is a staffing platform offering a seamless experience for employers and candidates. Employers can post job openings and specify their requirements, while candidates can create profiles and upload resumes.The team of Team Remotely continuously learns and adapts based on previous successful placements, constantly improving its matching capabilities. This ensures that the recommendations provided by Team Remotely are tailored and accurate, increasing the likelihood of a successful match between employers and candidates. By providing intelligent and data-driven solutions, they strive to enhance the efficiency and effectiveness of the hiring process, ultimately helping companies find the best talent and individuals find their dream jobs."
Data Engineer - Remote,Get It Recruit - Information Technology,"Herndon, VA (Remote)",https://www.linkedin.com/jobs/view/3770166914/?eBP=JOB_SEARCH_ORGANIC&refId=sjQPSlxFFJAXxmDgKKgqXg%3D%3D&trackingId=xxunX8BwgrfSwrf8aYPh3A%3D%3D&trk=flagship3_search_srp_jobs,3770166914,"About the job
            
 
Are you passionate about working with data and eager to make a meaningful impact on government services? We're looking for a Data Engineer to join our dynamic team dedicated to delivering Data Warehouse and Business Intelligence services to our government clients in an Agile environment.About The ProgramThis program is committed to enhancing the services provided to U.S. Citizenship and Immigration Services (USCIS). Our focus is on the development, security, and modernization of the Agency's Enterprise Data Warehouse/Data Lake. We harness open-source, AWS Cloud, and Big Data technologies, employing agile project management practices and modern DevSecOps delivery to empower USCIS leadership, data/business analysts, data scientists, and decision-makers with essential business intelligence support systems.Your ResponsibilitiesCollaborate with product owners, system owners, and source system business owners to understand transactional system data models, elicit requirements, and logic for ETL processes.Develop ETL workflows and data pipelines to ingest data from various sources into our Data Warehouse and Data Lake, utilizing AWS Data Migration Service (DMS), Scala, Kafka, Restful APIs, and other relevant technologies.Tackle data discrepancies and missing data issues resulting from daily ETL loads.Assist the operations team in deploying ETL jobs in integration and production environments, while providing support for debugging and troubleshooting critical production issues.Actively participate in Agile development activities and ceremonies, including sprint planning, sprint grooming, artifact creation, testing, demonstrations, retrospectives, and solution releases.Document ETL logic and mappings concisely for future reference in development and maintenance.Execute various development-related activities, such as attending meetings, delivering briefings and presentations, and providing support materials to promote the program and explain technical concepts to non-technical audiences.Required SkillsMinimum of 5 years of experience in ETL development, working with diverse and extensive data sources.Proficiency in programming languages like Java, Scala, Python, R, and familiarity with JSON Schema.A minimum of 2 years of experience in producing and consuming Rest APIs.Solid understanding of relational databases used for BI analytics.Demonstrated experience in Data Warehouse/Data Lake and Business Intelligence environments.Ability to write complex SQL queries and scripts.Strong teamwork, coordination, planning, and influencing skills.Self-driven and adaptable, capable of thriving in a challenging, fast-paced, cross-functional team environment.Experience with Agile development practices, including Scrum and Kanban, as well as management tools such as Jira and Confluence.Familiarity with GIT and branching strategies.Proficiency in engineering/DevOps tools, such as Jenkins.Excellent analytical, communication, and organizational skills.Experience working in an AWS Cloud environment.Competency with Microsoft Office Suite, including Excel, PowerPoint, and Visio.Desired SkillsExperience with AWS Database Migration Service (DMS), Databricks/Apache Spark, and/or Kafka.Familiarity with Postgres and Oracle databases.Exposure to the Scaled Agile Framework SAFe.EducationA Bachelor's degree in a technical discipline, preferably in Computer Science, Mathematics, or an equivalent technical field, or an equivalent combination of education, professional training, and work experience.Location:Reston, VA - Currently fully remote due to COVID-19.Employment Type: Full-Time"
Contract || Databricks Data Engineer || Remote,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3712328856/?eBP=JOB_SEARCH_ORGANIC&refId=sjQPSlxFFJAXxmDgKKgqXg%3D%3D&trackingId=906%2BBRCoLJaUfzsDtYMSBw%3D%3D&trk=flagship3_search_srp_jobs,3712328856,"About the job
            
 
Hi,Hope you are doing great,I am Kumar Harshit, a recruitment lead with Steneral Consulting. I have an urgent requirement mentioned below. If you find yourself comfortable with the requirement please reply back with your updated resume and I will get back to you or I would really appreciate if you can give me a call back at my contact number 302-549-3240.Job Title: Databricks Data EngineerLocation: RemoteDuration: 6+ MonthsJob DescriptionMUST HAVE: 3 years' experience with Databricks, Python and Azure Development (all 3)Job DescriptionAs a Databricks data engineer, your main role is to design, develop, and manage the data infrastructure on the Databricks platform within the Azure cloud environment. This involves tasks like configuring the data lake (ADLS Gen2), create and optimizing data pipelines, and closely monitoring them to ensure data quality and scalability.Your responsibilities also extend to integrating data from different sources, conducting data transformations, configuring security data sharing, and ensuring data cleanliness. To achieve success, effective collaboration with various internal and client teams, including product owners and developers, is essential. Understanding their data requirements and providing appropriate solutions will be an integral part of your work. By doing so, you'll contribute significantly to client's digital transformation initiatives and facilitate data-driven decision-making while advancing AI/ML journey.Job ResponsibilitiesThe Azure Databricks Developer, will be responsible for designing, developing, and maintaining data processing workflows and analytics solutions using Azure DatabricksUse business requirements to drive the design of data solutions/applications and technical architecture. Create technical, functional, and operational documentation for data pipelines and applicationsDevelop and maintain ETL (Extract, Transform, Load) pipelines using Databricks to process and transform large datasets.Collaborate with data engineers and data scientists to design and implement scalable and efficient data processing workflows.Build and optimize Apache Spark jobs and clusters on the Databricks platform.Develop and maintain data ingestion processes to acquire data from various sources and systems.Implement data quality checks and validation procedures to ensure accuracy and integrity of data.Perform data analysis and exploratory data mining to derive insights from complex datasets.Design and implement machine learning workflows using Databricks for predictive analytics and model training.Coordinate and participate in structured peer reviews / walkthroughs / code reviews.Work effectively in an Agile Scrum environment (JIRA / Azure DevOps)Stay updated with the latest advancements in big data technologies and contribute to the improvement of existing systems and processes.Required QualificationsB.S. in Computer Science/Engineering or relevant field8+ years of experience in the IT industry3+ years of hands-on experience in data engineering/ETL using Databricks Notebook programming on Azure or any cloud infrastructure and functionsSolid Databricks development experience with significant Python, PySpark, Spark SQL, Pandas, NumPy in Azure environment.Hands on experience of building data pipelines using Databricks and Apache Spark.Hands on experience designing and delivering solutions using Terraform and Azure DevOps agents.Creating mount points for ADLS Gen2 storage in DBFS to implement RBAC for end users.Strong understanding of distributed computing principles and experience with large-scale data processing frameworks.Experience with CI/CD on Databricks using tools such as AZDO Git, and Databricks CLIExperience working with structured and unstructured data.Strong understanding of Data Management principles (quality, governance, security, privacy, life cycle management, cataloguing). Unity Catalog experience desirable.Experience with Delta Lake, Unity Catalog, Delta Sharing, Delta Live Tables (DLT)Able to work independentlyExcellent oral and written communication skillsExperienceDatabricks: 3 years (Required)Azure: 3 years (Required)Cloud development: 5 years (Required)Python: 3 years (Required)Nice to have: Azure Synapse, Databricks Lakehouse Architecture, Azure Data Factory (ADF), PowerBI, Predictive Analytics, AI/ML, Medallion architecture Nice to have: Microsoft Azure Databricks and Azure Data Engineer certifications.Kumar HarshitAssociate Team Lead-Talent Acquisition -North AmericaDesk: 302-549-3240kumar.harshit@steneral.com"
Data Engineer Lead - ALT-006,Five Cubes,United States (Remote),https://www.linkedin.com/jobs/view/3728089548/?eBP=JOB_SEARCH_ORGANIC&refId=sjQPSlxFFJAXxmDgKKgqXg%3D%3D&trackingId=yyZi%2FsWpGB28WUQvtFNZ8A%3D%3D&trk=flagship3_search_srp_jobs,3728089548,"About the job
            
 
The ideal candidate will have a strong background in Python programming, Data Warehousing (DWH), proficient in PL/SQL programming, expertise in optimizing stored procedures, and knowledge in Test Automation using Python. As a Data Engineer Lead, you will play a pivotal role in designing, developing, and maintaining robust data pipelines, ensuring the integrity, availability, and security of our data assets.RequirementsQualifications: Bachelor's degree in Computer Science, Engineering, or a related field. Master's degree is a plusGood years of experience in data engineering or a related roleProficiency in Python programming for data manipulation, automation, and testingStrong expertise in PL/SQL programming and optimizing stored proceduresExtensive knowledge of Data Warehousing concepts and best practicesExperience with ETL tools and processesHands-on experience with database technologies such as SQL Server, Oracle, or PostgreSQLFamiliarity with cloud platforms (e.g., AWS, Azure, GCP) and associated data servicesExcellent problem-solving skills and attention to detailStrong communication and teamwork skills, with the ability to lead and mentor a team of Data Engineers
BenefitsRate: $55-$65 on C2C and $41.50 - $51.50 on W2RemoteSubcontracting or C2C is allowed.Any Visa."
Data Engineer,APN Consulting Inc.,United States (Remote),https://www.linkedin.com/jobs/view/3775902091/?eBP=JOB_SEARCH_ORGANIC&refId=sjQPSlxFFJAXxmDgKKgqXg%3D%3D&trackingId=QWUVcu37U8L2CXFPW45xnQ%3D%3D&trk=flagship3_search_srp_jobs,3775902091,"About the job
            
 
APN Consulting has an immediate need for a direct client requirement: Data Engineer   Duration: Full Time/Permanent Remote Role As a Senior Data Engineer on the client IT Data and Analytics Team, we will count on you to develop and maintain scalable data pipelines, enhance data models, and provide subject matter expertise in data acquisition and consumption pipelines on Azure, Databricks and Confluent. It involves developing best practices, reusable code, libraries, and frameworks for cloud-based data warehousing and ETL, and uses multi-cloud, programming languages like Java, Scala, Python, RDBMS, NoSQL databases, and design enterprise data warehouse platforms. By fostering collaboration and aligning with business objectives, you will have the opportunity to elevate data models, drive data-driven decision-making, and enhance data accessibility throughout the organization. Join us in championing data-driven decision-making and becoming an essential contributor to our data-driven success.   Our future colleague.   We'd love to meet you if your professional track record includes these skills:    7+ years of hands-on experience designing and implementing data engineering solutions using Azure cloud offerings including Data Factory, Azure Synapse, ADLS, and Azure Functions and Logic Apps.  ​ In-depth understanding and experience in data engineering and data warehousing best practices, data modeling, data security, and governance principles throughout the data lifecycle.  Hands-on development experience with Relational DBs (MS SQL Server, PostgreSQL, Oracle), as well as complex stored procedures and functions using SQL and optimization of SSIS packages and Redshift database queries.  Experience designing and building data pipelines using APIs, BigQuery and Streaming ingestion methods. Proficiency in Spark, Python and ability work with telemetry using Azure monitor, App insights.  Solid understanding of DevSecOps, Git integration, Code deployment using CI/CD pipelines, managing work in an agile environment using Azure Boards, Confluence. Ability to troubleshoot and deploy using Redgate tools and Visual Studio is a must. 
   These additional qualifications are a plus, but not required to apply:    Familiarity with Modern Data Platforms technologies like Snowflake, Matillion, Bryte Flow, and Tableau is a plus.  ​ Knowledge in Azure IoT, Azure HDInsight + Spark, Kafka and Azure Stream Analytics is a bonus.  Experience with Informatica - IICS task flows development and maintenance.  Data bricks, Microsoft Azure Data Engineer, and/or other cloud certifications..

Desired Skills and Experience
                DATA FACTORY"
"Tech Lead, Data Engineer",Lovelytics,"Arlington, VA (Remote)",https://www.linkedin.com/jobs/view/3759813731/?eBP=JOB_SEARCH_ORGANIC&refId=sjQPSlxFFJAXxmDgKKgqXg%3D%3D&trackingId=7b8KFijgDsHEZkj0SD9J2Q%3D%3D&trk=flagship3_search_srp_jobs,3759813731,"About the job
            
 
Lovelytics is seeking a Technical Lead (Level 4) Consultant with experience delivering strategic Databricks client engagements to join our Data & AI practice!As a Tech Lead, you will gain people management skills in order to develop to the next level. In addition, you will play a key role, often as an engagement lead, on client engagements related to data warehousing, ETL development, data integrations, and data modeling. This is a client-facing and stakeholder management role, focused on using and migrating to our partner technologies; Databricks, AWS, and Azure to name a few. In addition to the technical capabilities for this role, we are looking for someone who wants to work in a collaborative, dynamic, and inclusive environment and has a passion for bringing meaning to data.Role Location: Arlington, VA, or Remote in the US (MD, DC, CA, IA, ID, IN, MA, NC, SC, TX, TN, GA, CO, NY, NJ, VA, FL, PA)This role is not eligible for sponsorship at this time.Primary Responsibilities: Utilize consulting and technical skills to be able to work in a client-facing project environment independently.Be responsible for your own execution and often others' on client projects, communicating directly with internal and external stakeholders on status updates and potential roadblocks.Collaborate with other team members to successfully deliver on projects.Work effectively and directly communicate with both internal and client and/or partner teams.Develop full ownership of your execution on client engagements, play a role in the project planning and solution stages of engagements as well.Lead the end-to-end design and implementation of multiple ETL/ELT pipelines, demonstrating efficient data transformation.Mentor junior data engineers, and their growth is evident in their project contributionsSuccessfully lead small data warehousing projects with measurable performance enhancements under the management of an engagement lead- may also play the role of an engagement lead.Contribute to real-time data processing solutions and managed streaming data.Implement security and compliance measures for data pipelines.Design and implement version control and branching strategies and integrate them into CI/CD for promoting and testing in higher environments.
Our Ideal Candidate's Skills and Experiences: B.S. in Computer Science or equivalent4-6 years' experience in data engineering and big data. 2 years' of professional services experience interacting directly with clients.Extensive knowledge of data warehousing concepts and hands-on experience deploying pipelines using Databricks and/or SparkDatabricks Solution Architect certification a plus.Data modeling and database design skills and knowledge of version controlExcellent verbal and written communication skillsExperience architecting scalable and fault-tolerant data solutions across Azure, AWS, and DatabricksUnderstands and utilizes Lovelytics tools and client tools 
What We Promise You: Exciting projects with great clients in varying departments and verticals across the worldThe ability to work closely with experienced data engineers and quickly grow and expand your skillsetThe ability to work closely with all sizes of companies, ranging from Fortune 100 to small local businessesA workplace where you are encouraged to challenge the status quo and develop new technologies, methodologies, and processesA diverse team consisting of data gurus, experience seekers, and entrepreneurial minds that are always pushing to be better
Lovelytics is an Equal Opportunity Employer. This means you don’t have to worry about whether your application process will be fair. We consider all applicants without regard to race, color, religion, age, ancestry, ethnicity, gender, gender identity, gender expression, sexual orientation, veteran status, or disability. US Salary for this position is $125,000-160,000, however, actual salary is based on a number of various factors including skillset, experience, credentials, etc. Powered by JazzHRw2mSVnuYhb"
Data Engineer - Remote | WFH,Get It Recruit - Information Technology,"Washington, DC (Remote)",https://www.linkedin.com/jobs/view/3761431771/?eBP=JOB_SEARCH_ORGANIC&refId=sjQPSlxFFJAXxmDgKKgqXg%3D%3D&trackingId=Opuo%2FAxju3xS%2FJ9GmE4q9Q%3D%3D&trk=flagship3_search_srp_jobs,3761431771,"About the job
            
 
Are you passionate about leveraging data to make a positive impact on government initiatives? Join our team at a leading consulting firm, currently seeking a Remote Data Engineer to contribute to dynamic, long-term federal government enterprise big-data programs. As a trusted advisor to U.S. federal government clients in health, civilian, and national security missions, we pride ourselves on creating a collaborative and innovative work environment.ResponsibilitiesCollaborate with client stakeholders and technical teams to optimize data collection, storage, and usage, maximizing the value of information within the organization.Research, design, build, optimize, and maintain efficient and reliable data systems, pipelines, and models.Align closely with operating user requirements on data science, architecture, governance, infrastructure, and security, applying standards to optimize production environments.Translate business needs into data architecture solutions, implementing and designing in production environments within supported data systems.Implement data orchestration pipelines, data sourcing, cleansing, augmentation, and quality control processes within supported data systems.Deploy applications to production in partnership with business units.Develop, test, and integrate new data features and functionality as defined by product owners and business teams.Collaborate with a multi-disciplinary team of analysts, data engineers, data scientists, developers, and data consumers in a fast-paced Agile environment.QualificationsBachelor’s degree in Computer Science, Engineering, Science, or related field.5+ years of data engineering and data warehousing experience.3+ years of experience building cloud-data pipeline solutions for data ingestion, storage, real-time processing, and analytics.Experience working within an Agile development environment, DevOps, and using version control platforms (e.g., GitHub).Domain knowledge of standard data methodologies (DMBOK, NIST, etc.).Team player with the ability to work effectively within a group as well as self-motivated with minimal supervision.Excellent problem-solving, collaboration, and communication skills.AWS verifiable certification is strongly preferred.U.S. Citizen with the ability to secure a U.S. Federal Clearance.About UsWe are a leading consulting and information technology solutions provider to public sector organizations, supporting health, civilian, and national security missions. Recognized by Inc. Magazine as one of the 250 fastest-growing companies in the U.S. for three consecutive years, we specialize in providing software and systems engineering, information management, analytics & visualization, agile project management, and management consulting services. As a federal contractor, we are fully committed to fostering a diverse and inclusive workplace.BenefitsCompetitive compensation with opportunities for bonuses.Employer-paid health care.Unlimited training funds.401k match.Note: As a federal contractor, we are required to verify that all employees are fully vaccinated against COVID-19. If you receive an offer and are unable to get vaccinated for religious or medical reasons, you may request a reasonable accommodation.Employment Type: Full-Time"
Data Engineer,VitalSource,United States (Remote),https://www.linkedin.com/jobs/view/3755874888/?eBP=JOB_SEARCH_ORGANIC&refId=sjQPSlxFFJAXxmDgKKgqXg%3D%3D&trackingId=zeoWGkD6Cm62zgzzWF2OOQ%3D%3D&trk=flagship3_search_srp_jobs,3755874888,"About the job
            
 
VitalSource®, is hiring a Data Engineer to contribute to our Engineering team located in Raleigh (hybrid) or remotely.VitalSource is looking to add two Data Engineers to our growing Engineering Department. In this role, you will focus on maintaining data architectures. The Data Engineer role is essential in ensuring the smooth flow and accessibility of data, as well as transforming this data into actionable insights, requiring deep analytical expertise. They manage and optimize data artifacts to handle and query data safely and efficiently, working with application engineers, data analysts, and data architects.VitalSource is a mission-driven company delivering affordable, impactful learning to anyone, anywhere.Required Qualifications:  Bachelor’s degree in Computer Science, Engineering, or related field.  5+ years of experience in a data engineering role with a focus on analytics.  Bachelor's or master’s degree in Computer Science, Data Science, or a related field.  Experience with relational SQL and NoSQL databases.  Strong proficiency with SQL, data modeling, and data engineering.  Strong analytical and statistical skills.  Experience with Cloud Platforms, in particular GCP.  Experience with Cloud Data Platforms, in particular Snowflake.  Strong problem-solving and analytical skills.  Excellent communication and collaboration skills.  Ability to communicate complex data insights in a clear and actionable manner to non-technical stakeholders. 
Preferred Skills:  Snowflake and GCP Experience with data governance and data management best practices Experience working with remote engineering teams Familiar with software development and project management frameworks
Key Responsibilities:  Develop, construct, test, and maintain data architectures emphasizing analytical and reporting capabilities. Design and implement systems that collect, transform, and store large amounts of structured and unstructured data. Collaborate with business stakeholders to understand their problems and design data-driven solutions. Optimize data retrieval processes for analysis and reporting. Work closely with business analysts to ensure data quality and timely delivery of insights. Ensure architecture supports the analytical requirements of the business, allowing for scalable analytics and machine learning use cases. Monitor data systems to ensure data integrity and quality. Stay current with industry best practices and technologies related to data engineering and analytics. 
Salary Range: $100,000 to $120,000What We Offer:  Culture: Collaborative, Inclusive, and Mission-driven  More in your pocket: Competitive base salary and a strong variable component  We take care of all aspects of our people: Generous, well-rounded benefits such as Medical, Vision, Dental, Life, Disability, Critical Illness, Accident, FSA, HSA, ID Protection, Pet and Legal Insurance  Retirement:401K match up to 5%  We support our families: 12 weeks of paid parental leave Continued education: Use our tuition reimbursement program The Importance of Balance: Start at 4 weeks’ vacation, 12 sick days, 10 company holidays, and 3 personal days,  Flexibility: Flexible work schedules and remote capabilities (by team) - feel free to skip the commute and hit your deadlines from home Casual Dress: Be as comfortable at work as you are in your own living room Wellness: Lots of opportunities for fitness challenges and rewards
Who We Are:VitalSource is the leading education technology solutions provider committed to helping partners create, deliver, and distribute affordable, accessible, and impactful learning experiences worldwide. As a recognized innovator in the digital course materials market, VitalSource is best known for partnering with more than 1,000 publishers and resellers to deliver extraordinary learning experiences to millions of active users globally—and today we’re also powering new, cutting-edge technologies designed to optimize teaching and learning for maximum results. Learn more at https://vitalsource.com and follow us on Twitter, LinkedIn, and Instagram.All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, work related mental or physical disability, veteran status, sexual orientation, gender identity, or genetic information. EEO/AA Employer/Vet/DisabledWe participate in EVerify. EEO Poster in English EEO Poster in Spanish"
GCP Data Engineer,Accroid Inc,United States (Remote),https://www.linkedin.com/jobs/view/3774786009/?eBP=JOB_SEARCH_ORGANIC&refId=sjQPSlxFFJAXxmDgKKgqXg%3D%3D&trackingId=HfD43eAANbodv1Zw46iv3w%3D%3D&trk=flagship3_search_srp_jobs,3774786009,"About the job
            
 
GCP Data EngineerGCP ExperienceMust have Banking and Payments experienceMust have good communication"
DataStage Data Engineer,IVY TECH SOLUTIONS INC,"Chicago, IL (Remote)",https://www.linkedin.com/jobs/view/3667180270/?eBP=JOB_SEARCH_ORGANIC&refId=sjQPSlxFFJAXxmDgKKgqXg%3D%3D&trackingId=vDkxKM1PgUhmUJ%2Fl17j5yA%3D%3D&trk=flagship3_search_srp_jobs,3667180270,"About the job
            
 
RemoteDataStage Data EngineerDuration: 6+MonthsOnly w2 or 1099Please send the resume to  or 847- 350-1008 together we are building a culture that values diversity and creates a space of belonging for all our team members. We believe that investing in your success is an investment in our customers and our business. Our people are what sets us apart and make us great. As a Data Engineer, you’ll provide your talents in contributing to the success of the client’s team by delivering the following:  Serve in the goalie rotation to support the Production environment. Responsible for maintaining enterprise-grade platforms that enable data-driven solutions. Search for ways to automate and maintain scalable infrastructure. Ensure delivery of highly available and scalable systems. Monitor all systems and applications and ensure optimal performance. Analyzes and designs technical solutions to address production problems. Participate in troubleshooting applications and systems issues. Identifies, investigates, and proposes solutions to technical problems. While providing technical support for issues, develop, test, and modify software to improve efficiency of data platforms and applications. Monitors system performance to maintain consistent up time. Prepares and maintains necessary documentation. Participate in daily standups, team backlog grooming, and iteration retrospectives. Coordinate with data operations teams to deploy changes into production. Highest level may function as a lead. Other duties as assigned.
Qualifications:  Requires a Bachelor's in Computer Science, Computer Engineering or related field and experience with ETL development, SQL, UNIX/Linux scripting, Big Data distributed systems. Prefer experience with IBM DataStage. Various programming languages like Java and Python, orchestration tools and processes or other directly related experience. A combination of education and experience may meet qualifications. Excellent analytical, organizational, and problem-solving skills. Ability and desire to learn new technologies quickly. Ability to work independently and collaborate with others at all levels of technical understanding. Able to meet deadlines. Good judgment and project management skills. Ability to communicate both verbally and in writing with both technical and naon-technical staff. Ability to work in a team environment and have good interpersonal skills. Ability to adapt to changing technology and priorities. Must be able to work independently, handle multiple concurrent tasks, with an ability to prioritize and manage tasks effectively.
Warm Regards,Charan Kumar | IVY Tech Sols Inc.3403 N Kennecott Avenue, Suite B&C Arlington Heights, IL 60004PH.( Direct: (847) 350-1008   |Gtalk : charan.ivytech|
Powered by JazzHRf1Hgh1ZRAW"
Data Engineer,"Resource Informatics Group, Inc",United States (Remote),https://www.linkedin.com/jobs/view/3720358162/?eBP=JOB_SEARCH_ORGANIC&refId=sjQPSlxFFJAXxmDgKKgqXg%3D%3D&trackingId=gVRFOGw5Uf%2FAOaM%2FXy7ZSQ%3D%3D&trk=flagship3_search_srp_jobs,3720358162,"About the job
            
 
Required SkillsJob DescriptionAs a contract Data Engineer at Bezos Academy, you will be a critical member of our data engineering team. You will help build and maintain the data infrastructure that is essential to Bezos Academy having a positive impact across the country and will help us make decisions that will shape our organization for years to come.You are an experienced data engineer who operates autonomously, loves to dig into data and learn new domains, knows how to keep things simple, and finds creative, cost-effective and scalable ways to solve problems. You will help us establish best-in-class data security and work closely with stakeholders to understand use cases, build data pipelines and warehouses, and ensure data quality. You will monitor data for bias and work to eliminate it. You may also have the opportunity to lend your skills to other exciting data and analytic needs as we continue to grow in size and complexity. Above all, you share our passion for expanding access to high quality, Montessori-inspired preschool within underserved communities as we scale to hundreds of schools nationwide."
ETL Data Engineer,PSRTEK,United States (Remote),https://www.linkedin.com/jobs/view/3645665395/?eBP=JOB_SEARCH_ORGANIC&refId=sjQPSlxFFJAXxmDgKKgqXg%3D%3D&trackingId=bJNVMcW5eY5bpbJv5N9fpQ%3D%3D&trk=flagship3_search_srp_jobs,3645665395,"About the job
            
 
Hi,Hope you are doing good today.A short description of the job can be found below. If you are interested, kindly respond with an updated copy of your resume.Role: ETL Data EngineerLocation: Philadelphia, PA- Remote OpportunityDuration: Long Term Contract Job DescriptionManages data activities such as data requirements gathering, data analysis/modeling, and data issues resolution using standard approved technologyManages standardization, migration, transformation, validation, and quality assurance of data within multi-database platformsCreates and maintains code through GitHub repository for change controlBuilds Jenkins pipelines using groovyLeverages internal and external ETL tools for data processing and publishingIdentifies and maintains company databases, including data sources, data structures, data organization, and data optimizationIdentifies complex issues proactively and is responsible to see them through resolution, including identifying trends through data analysis and manipulationResponsible for specific client data life cycles from discovery to implementation to maintenanceFormulates and monitors policies, procedures, and standards relating to database managementResponds to production defects and relays information back to the Operations Manager to communicate to clientsContribute to all phases of the data and software development lifecycle when neededAbility to support off hours data processing and emergency requestsExperience, Qualifications, Knowledge, And SkillsBachelor's degree (B. A. / B. S.) from four-year college or university; and two to four years related experience and/or training; or equivalent combination of education and experience.2+ years Healthcare industry experience preferred3+ years of experience with SQL, database design, optimization, and tuning 3+ years of experience with open source relational (e.g., PostgreSQL)3+ years of experience using GitHub 3+ years of experience in Shell Scripting and one other object-oriented language such as Python, or Php.3+ years of experience in continuous integration and development methodologies tools such as Jenkins3+ years of experience in an Agile development environmentTime management skillsProfessionalismProgramming skills particularly SQL, Shell Scripting, and PythonDetail orientedConscientiousTeam playerOral and written communication skillsRegards,B. LokeshAccount Manager- Recruitment and OperationsPSRTEK Inc.Princeton, NJ 08540lokesh@psrtek.comhttps://www.linkedin.com/in/lokesh-bala-93469abb/""Culture of excellence""PSRTEK is a reputed technology recruitment and IT staffing brand with a global footprint and an admired client base. As an ideas and innovation powerhouse with a culture of excellence, we bring remarkable expertise and deliver powerfully transformative results."
Expression of Interest: Data Engineer,Fingerprint for Success (F4S),"Philadelphia, PA (Remote)",https://www.linkedin.com/jobs/view/3699995810/?eBP=JOB_SEARCH_ORGANIC&refId=sjQPSlxFFJAXxmDgKKgqXg%3D%3D&trackingId=VwL1aHPUsqBdxMj6ywuOxw%3D%3D&trk=flagship3_search_srp_jobs,3699995810,"About the job
            
 
We are inviting professionals in high-growth industries who are thinking about their next move or looking for a new opportunity to join our expanding talent pool.The F4S Talent Pool is a pilot project designed to: Help job seekers get discovered by our partners based on their anticipated hiring needs.Provide optional support and resources for job seekers in their career endeavors.Help individuals understand, and bring out the best in themselves and each other.
The F4S Talent Pool process:Once you express your interest, you will be asked to complete the F4S work style assessment which measures 48 key attitudes and motivations in the context of work. On completion, you will be automatically added to our growing talent pool and contacted as new opportunities arise.About Fingerprint For Success (F4S)Backed by 20+ years of research, F4S’s revolutionary predictive analytics have achieved over 90% reliability in forecasting personal and team motivations, behaviors, and performance. Ultimately, we help people find purpose and fulfillment at work, and help build and scale high performing teams.Keep in mind, joining our talent pool does not guarantee a job offer. We aim to balance your technical skills with the results of your F4S work style assessment to match the hiring needs of our partners.Your feedback is a gift! Write to us via:Powered by JazzHR7aarLxO3rm"
Data Engineer GCP,I2B Tech,"America, SD (Remote)",https://www.linkedin.com/jobs/view/3781238393/?eBP=JOB_SEARCH_ORGANIC&refId=sjQPSlxFFJAXxmDgKKgqXg%3D%3D&trackingId=OY3JShB4%2FKA5ObY3ub%2BxfQ%3D%3D&trk=flagship3_search_srp_jobs,3781238393,"About the job
            
 
Hola estimad@ postulante,Actualmente nos encontramos en búsqueda de nuevos horizontes y búsqueda de ampliar nuestra red de profesionales en Data Engineer GCP, quieres ser parte de nuestro team DOER? Pero... ¿Qué es un Doer?👀Los Doers somos la comunidad dentro de I2B Technologies. Somos pioneros en consultoría de estrategias digitales, desarrollo de software y proyectos de inteligencia artificial 🤖. Los Doers rompemos fronteras, estamos en Chile, Colombia y Perú.En I2B Technologies somos cercanos, nos encanta brindar el apoyo a nuestros clientes para su mejora, nos preocupamos de la eficiencia y la calidad de vida de sus usuariosSi buscas un lugar de trabajo donde puedas aprender, trabajar en equipo y mostrar tus ideas y llevarlas a cabo sin dejar de lado tu espacio personal ¡Este es tu momento! Funciones del cargoEl Data Engineer estaría encargado de diseñar, desarrollar y mantener sistemas para procesar grandes volúmenes de datos, garantizando su disponibilidad para otros especialistas y analistas en ingeniería de datos. Requerimientos del cargo Carreras de: Ingeniero comercial, estadista, Ingeniero civil, Ingeniero en informático y afinesOptimización de rendimiento y escalabilidad de bases de datos.Garantía de calidad de datos.Colaboración con equipos de análisis y científicos de datos.Mantenimiento y monitoreo de la infraestructura de datos.Implementación de herramientas y tecnologías de Big Data
Conocimientos Técnicos En  Google Cloud Platform BigQuery  Composer  Gestión de proyectos  SQL  ETL  Data Studio, Power BI o Herramientas de Visualización  Arquitectura de datos  Modelos de datos  Spark, bases de datos NoSQL y soluciones de datos basadas en la nube  Dominio de lenguajes de programación como Python, R o SQL, así como herramientas de visualización de datos 
 Condiciones"
GCP Data Engineer,FinTech LLC,United States (Remote),https://www.linkedin.com/jobs/view/3752335269/?eBP=JOB_SEARCH_ORGANIC&refId=sjQPSlxFFJAXxmDgKKgqXg%3D%3D&trackingId=Uou%2FsT4%2B3XYqbk8Df5QxeA%3D%3D&trk=flagship3_search_srp_jobs,3752335269,"About the job
            
 
About Client:Client is a company that helps other businesses with technology and consulting services. They offer various services like developing software, managing computer systems, and providing advice on how to improve business processes. Client works with different industries like healthcare, finance, retail, and more. They help these businesses by using technology to make their operations smoother and more efficient. For example, they might create a mobile app for a retail store to make it easier for customers to shop online, or they might help a hospital manage patient records using computer systems.Overall, Client is like a trusted partner that businesses can rely on to solve their technology-related challenges and make their operations better. They have experts who understand the business needs and use technology to find the right solutions.Rate Range: $40-$50/HrJob Description:Responsibilities: Sr engineer- development experiences - Python (5 ~ 7 years)CI/CD (>2 years)Cloud experiences, GCP preferred (>2 years)Data engineering pipeline development experiences with following preferredDataflow (Apache beam),Cloud function + cloud composerPub/Sub (streaming)SQL development (>3 years)Cloud data warehousing/BigQuery (>2 years)Data analysis (>2 years)
About ApTask:Join ApTask, a global leader in workforce solutions and talent acquisition services, as we shape the future of work. We offer a comprehensive suite of offerings, including staffing and recruitment services, managed services, IT consulting, and project management, providing unparalleled opportunities for professional growth and development. As a member of our dynamic team, you'll have the chance to connect businesses with top-tier professionals, optimize workforce performance, and drive success for our clients across diverse industries. If you are passionate about excellence, collaboration, and innovation, and aspire to make a meaningful impact in the world of work, come join us at ApTask and be a part of our mission to empower organizations to thrive.Applicants may be required to attend interviews in person or by video conference. In addition, candidates may be required to present their current state or government issued ID during each interview.Candidate Data Collection Disclaimer:At ApTask, we prioritize safeguarding your privacy. As part of our recruitment process, certain Personally Identifiable Information (PII) may be requested by our clients for verification and application purposes. Rest assured, we strictly adhere to confidentiality standards and comply with all relevant data protection laws. Please note that we only collect the necessary information as specified by each client and do not request sensitive details during the initial stages of recruitment.If you have any concerns or queries about your personal information, please feel free to contact our compliance team at businessexcellence@aptask.com. 

Desired Skills and Experience
                RANGE: $"
GCP Data Engineer -- 1 year contract -- Remote,Lorven Technologies Inc.,United States (Remote),https://www.linkedin.com/jobs/view/3734718572/?eBP=JOB_SEARCH_ORGANIC&refId=sjQPSlxFFJAXxmDgKKgqXg%3D%3D&trackingId=n9njiI5j6G0Zf%2FLw%2Bkwlhw%3D%3D&trk=flagship3_search_srp_jobs,3734718572,"About the job
            
 
Job Title: GCP Data EngineerLocation: RemoteDuration: 1 year contractMust Have 7+ years of overall data engineer experienceGoogle CloudDataprocBigqueryDB2Airflow experience is nice to have."
Remote Work - Need Lead AWS Data Engineer,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3714161104/?eBP=JOB_SEARCH_ORGANIC&refId=sjQPSlxFFJAXxmDgKKgqXg%3D%3D&trackingId=YzoqX4JVRcz%2BglOSQRmymQ%3D%3D&trk=flagship3_search_srp_jobs,3714161104,"About the job
            
 
remote roleRequired Skills Must have lead exp Candidates will be dealing with confidential information that align with government backed mortgage loans and financial data. Skill CategorySkillDescriptionAWS ServicesAWS ServicesSkilled in Amazon Web Services (AWS) offerings, development, and networking platformsApplication IntegrationJenkinsSkilled in JenkinsData AnalysisAWS AnalyticsSkilled in AWS Analytics such as Athena, EMR, or GlueAWS ComputeSkilled in AWS Compute such as EC2, Lambda, Beanstalk, or ECSDatabase ManagementAWS DatabaseSkilled in AWS Database products specifically Redshift and AuroraProgramming SkillsSQLExpert SQL SkillsPythonSkilled in Python object-oriented programmingProject ManagementJIRAExperience using JIRANice To Have SkillsSkill CategorySkillDescriptionApplication IntegrationContinuous Integration/Continuous Delivery (CI/CD)Skilled in the continuous integration and continuous delivery (CI/CD) process within the DevOps SDLC methodologyData AnalysisCleaning and Transforming DataSkilled in cleaning and transforming data into a desired format to prepare for analysis or storageData AnalysisExperience in the process of analyzing data to identify trends or relationships to inform conclusions about the dataData ManipulationSkilled in the process of modifying data to make it easier to read or be more organizedData ValidationExperience in assessing the accuracy and quality of source data before using, importing, or otherwise processing dataData VisualizationData VisualizationSkilled in the graphical representation of information in the form of a charts, diagrams, pictures, and dashboards with programs and tools such as Excel, Tableau, or Power BIProgrammingDbtSkilled in creating models using Dbt open-source framework.Product Development SkillsDesign Requirements TranslationExperience translating business requirements into design requirements for product developmentProduct Development SkillsThe group of skills related to Product Development including designing products, developing product roadmaps, translating design requirements, prototyping, etc.Software DevelopmentExperience using the SDLC process to successfully develop software using programming language(s) to write interrelated programming codes that achieve desired business outcomesProduct Testing SkillsProduct Testing SkillsThe group of skills related to Product Testing including testing and evaluating software, usability testing, UAT, and using relevant product testing technologySoftware Evaluation and TestingExperience comparing products or systems components, against business requirements and specifications, through testing and evaluating results to assess design, performance, and/or supportabilityProgramming SkillsThe group of skills related to Programming including coding, debugging, and using relevant programming languagesRisk MitigationExperience anticipating risks and designing solutions to mitigate them"
Data Engineer - Remote | WFH,Get It Recruit - Finance,"Washington, DC (Remote)",https://www.linkedin.com/jobs/view/3752359334/?eBP=JOB_SEARCH_ORGANIC&refId=sjQPSlxFFJAXxmDgKKgqXg%3D%3D&trackingId=YC3TwGJGqo4Wee6RN9We0w%3D%3D&trk=flagship3_search_srp_jobs,3752359334,"About the job
            
 
Job DescriptionPosition Title: Senior Data/ETL Engineer (Pentaho)About UsWe are a dynamic organization looking for a Senior Data Engineer to join our team in Washington, DC. As a Senior Data Engineer, you will play a pivotal role in implementing business intelligence solutions across all divisions. Your expertise will guide the team in designing and implementing solutions while fostering improvements in technical standards, development processes, governance, and technologies.Key Responsibilities  Technical Leadership: Lead the development of business intelligence solutions. Collaboration: Coordinate with scrum masters and project managers to optimize scheduling, work allocations, and story assignments. Data Management: Manage the reporting environment, including data sources, connections, security, subscriptions, and metadata. Data Access Control: Contribute to initiatives to enhance data access control and integrity across the enterprise. Data Modeling: Create data models and schemas for reliable and high-performance data marts and data warehouses. ETL Development: Develop complex ETL (extract, transform, and load) SQL scripts to collect data from various databases and systems for enterprise analytics and reporting. Software Management: Evaluate, test, and implement new or upgraded software. Problem Solving: Troubleshoot reported problems related to reports and the reporting database environment, collaborating with IT team members to identify causes and design resolutions. Change Management: Assess the impact of changes and updates to source production systems, planning and implementing resulting changes to reports.
Position Description  Continuous Improvement: Identify areas for improvement within the organization, related to architecture, design, performance optimization, development processes, workflows, governance, and tools/technologies. End User Training: Conduct end-user training on data solutions.
Education, Experience, And Skills  Qualifications: Bachelor's degree and/or relevant technical training with no preferred certification. Experience: Over 8-10 years of experience in Information Technology, with proficiency in ETL design/development and Data Warehouse Implementation/development. Industry Experience: Experienced in designing, developing, and implementing large-scale projects in Financial, Shipping, and Retail industries using Data Warehousing ETL tools (Pentaho). Methodologies: Knowledge about Software Development Lifecycle (SDLC), Agile, CI/CD deployment. Pentaho Expertise: Experience in architecting and building Data Warehouse systems and Business Intelligence systems, including ETL using Pentaho BI Suite (Pentaho Data Integration Designer / Kettle). ETL Skills: Hands-on experience with the entire ETL (Extract Transformation & Load) process. Data Integration: Experience in creating ETL transformations and jobs using Pentaho Kettle Spoon designer and Pentaho Data Integration Designer and scheduling. Data Sources: Used various steps in Pentaho transformations for various data sources, including Tables, Access, Text File, Excel, and CSV files. Hadoop Integration: Integrating Kettle (ETL) with Hadoop and other NoSQL data sources and loading unstructured data into Hadoop File System (HDFS). Data Security: Experience in performing Data Masking/Protection using Pentaho Data Integration (Kettle). Scripting: Experience in writing shell scripts for various ETL needs. Analytical Skills: Exceptional data analytical abilities. Database Experience: Strong development experience in writing PL/SQL Stored Procedures, Functions, packages, and debugging the code. Database Systems: Strong expertise in Developing and Debugging MSSQL or PostgreSQL SQL scripts, functions, stored procedures, views, complex SQL queries, cursors, and triggers. Performance Optimization: Expertise in Performance tuning, Query optimization, and exception handling.
If you are a passionate Data Engineer looking to make a significant impact in a dynamic environment, we encourage you to apply for this position. Join our team and be a part of our journey towards creating innovative solutions and driving success.Employment Type: Full-Time"
Data Engineer,ASCENDING Inc.,"Rockville, MD (Remote)",https://www.linkedin.com/jobs/view/3776467012/?eBP=JOB_SEARCH_ORGANIC&refId=sjQPSlxFFJAXxmDgKKgqXg%3D%3D&trackingId=jKOujP5dDQcbGAyPRz129w%3D%3D&trk=flagship3_search_srp_jobs,3776467012,"About the job
            
 
Our client, one of the largest Amazon Web Services (AWS) partners for data services, is looking for a true Mid level Big Data Engineer to contribute to join their team of technologists to build and contribute to large-scale, innovative projects. Technological and career growth opportunities are a natural and every day part of the working environment.  This role is only available for W2 or individual contracts. Please no C2C.  100% Remote Work.
Responsibilities: Analyze system requirements and design responsive algorithms and solutions.Use big data and cloud technologies to produce production quality code.Engage in performance tuning and scalability engineering.Work with team, peers and management to identify objectives and set priorities.Perform related SDLC engineering activities like sprint planning and estimation.Work effectively in small agile teams.Provide creative solutions to problems.Identify opportunities for improvement and execute.
Requirements: Minimum 5 years of proven professional experience working in the IT industry.Degree in Computer Science or related domains.Experience with cloud based Big Data technologies.Experience with big data technologies like Hadoop, Spark and Hive.AWS experience is a big plus. Proficiency in Hive / Spark SQL / SQL. Experience with Spark.Experience with one or more programming languages like Scala & Python & Java.Ability to push the frontier of technology and independently pursue better alternatives.Kubernetes or AWS EKS experience will be a plus.
Thanks for applying!Powered by JazzHRU3GJMKlbkr"
Remote Work - Need Data Science Engineer,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3689026924/?eBP=JOB_SEARCH_ORGANIC&refId=sjQPSlxFFJAXxmDgKKgqXg%3D%3D&trackingId=K3sQjgRdM1ZnN9H2hvlcPQ%3D%3D&trk=flagship3_search_srp_jobs,3689026924,"About the job
            
 
Data Science EngineerRemote- EST and CST Candidates only VERY strong communication.OK to work 7 a.m. to 7 p.m.
InsightProject overall This is a new team and they are trying out new tools, it is ok if candidates don't have all the tools, but would be willing to learn and grow. Try it and fail fast approach. They are experimenting with different vector databases MS and Google.Candidate needs to demonstrate that they are willing to learn and grow with the new technologies. (so West Coast is fine)Data Science Engineer High Jr. to mid level with python, tensorflow, generative AICan have post doc academic backgroundPrefer one resource to have healthcare background
Putting together a strategy for AI and ML for the CCHMC.Responsible for Advance Analytics, Power BI, Cloud Migration, Cloud Architecture.3-5 years experience.Client is using Azure, but any cloud experience OK (Google, AWS or Azure).Understanding of Architecture.Background in academia or healthcare is super helpful to understand the processes and core principals.They are working in Agile (but really more at a task/use case level).Natural Language Processing would be help."
Data Scientist Engineer (Junior),HireMeFast LLC - Career Accelerator - Land A Job,"Albuquerque, NM (Remote)",https://www.linkedin.com/jobs/view/3779421409/?eBP=JOB_SEARCH_ORGANIC&refId=sjQPSlxFFJAXxmDgKKgqXg%3D%3D&trackingId=lQ%2FgQ3GYV%2Ba0vPIjznomRw%3D%3D&trk=flagship3_search_srp_jobs,3779421409,"About the job
            
 
This is a remote position. Job Title: Data Scientist Engineer (Junior) Employment Type: Full-Time Salary: $55,000 - $65,000 per annum Experience Required:  Minimum 1 year of project experience How to Apply: visit hiremefast.net  to learn more & apply.About us: HireMeFast is a leading staffing and recruitment agency specializing in connecting businesses with top-tier talent across various industries. Our mission is to bridge the gap between exceptional candidates and organizations needing their skills, expertise, and unique qualities. Our team of experienced and dedicated recruitment specialists utilizes innovative sourcing strategies, and a vast network to identify and attract top talent. We conduct comprehensive procedures to ensure that only the most qualified candidates are presented to our clients.Skills and Abilities: Strong knowledge of R or Python for data analysis and modeling. Proficiency in statistical programs such as R, SAS, MATLAB, or Python. Familiarity with spreadsheets (VBA) and database applications (Access, Oracle, SQL, or equivalent technology). Basic understanding of SQL, Javascript, XML, JSON, and HTML. Ability to learn new methods quickly and work under deadlines. Excellent teamwork and communication skills. Strong analytical and problem-solving abilities. Basic understanding of SQL, Javascript, XML, JSON, and HTML. 
Preferred: Knowledge of actuarial concepts and life, health, and/or annuity products. Experience with statistical modeling techniques such as GLM, Decision Trees, Time Series, Regression, etc. Familiarity with Microsoft DeployR. Exposure to insurance risk analysis. Basic experience in computational finance, econometrics, statistics, and math. Knowledge of SQL and VBA. 
Why HireMeFast LLC?At HireMeFast, we understand that finding the right individuals to join your team is crucial for success and growth of your organization. We are committed to streamlining the hiring process for our clients, ensuring they have access to a diverse pool of highly qualified candidates who are a perfect fit for their specific needs."
Data Engineer | REMOTE 100%,eStaffing Inc.,United States (Remote),https://www.linkedin.com/jobs/view/3617898159/?eBP=JOB_SEARCH_ORGANIC&refId=sjQPSlxFFJAXxmDgKKgqXg%3D%3D&trackingId=QqM9GisV7REf6rGrtGQGyg%3D%3D&trk=flagship3_search_srp_jobs,3617898159,"About the job
            
 
Title: Data EngineerDuration: C2H for 6monthsLocation: RemoteClient: Health care servicesRequirementJob DescriptionMust-Haves Experience working with Kafka, Spark Streams, Snowflake, Grafana, Prometheus, and AWS services (S3, Kinesis, TimeStream, Redshift, CloudWatch, EKS)Experience working with HL7 dataExperience coding with Python/JavaStrong SQL, Data Warehousing, and Data Lake fundamentalsHands-on experience with Linux (RHEL/Debian) operating systemKnowledge of version control systems such as GitExperience consuming and building APIsExperience utilizing Agile methodology for developmentExperience evaluating and implementing technologies in a production capacityExperience building a streaming data platform from ground upAbility to manage one's tasks and work with stakeholders to understand the requirements as well provide solutions."
Data Engineer II,Definitive Logic a ManTech Company,"Washington, DC (Remote)",https://www.linkedin.com/jobs/view/3686393964/?eBP=JOB_SEARCH_ORGANIC&refId=sjQPSlxFFJAXxmDgKKgqXg%3D%3D&trackingId=8HTQREuTgmC0QvUtR7VNxg%3D%3D&trk=flagship3_search_srp_jobs,3686393964,"About the job
            
 
Definitive Logic is seeking a Data Engineer to join our team.  This position is contingent upon award.Roles and Responsibilities:  Create database models and components Assist in the design of data pipelines, data models, profiling/cleansing the data, and performance tuning Develop and maintain data pipelines, data workflows, ETL/ELT scripts or packages Conducts tests Tests data pipelines Provides test results and recommends more complex corrections to senior developers Implements full lifecycle of services/solution delivery for projects
Required Qualifications Ability to obtain a U.S. Government security clearance 2+ years of Relevant Experience Bachelor’s Degree, preferably in Engineering, Mathematics, or Business 
Desired Qualifications Experience engineering Disparate Data Sets from multiple systems Experience with moving data across multiple platforms Security+ certification 
About Definitive LogicDefinitive Logic (DL) is a management and technology consulting firm known for delivering outcomes and ROI for agencies’ most complex business challenges.  DL delivers performance-based and outcome-driven technology consulting solutions that directly support the strategic intent of our Defense, Homeland Security, Emergency Management, Federal Civilian and Commercial clients. We’re the preferred technology integration partner for Federal agencies to apply the best of data science, app dev, DevSecOps, cyber and cloud solutions to improve decision support, empower front-line employees and enhance back-office operations. We serve as trusted advisors providing objective, fact-based, vendor & technology-neutral consulting services.Definitive Logic is ultimately a team of problem solvers — thought leaders, domain experts, coders, data enthusiasts, and technophiles.  Our exciting projects and learning and sharing culture have consistently resulted in validation as a Great Place to Work: 2023 Washington Post Top Workplaces (8-time winner) | 2023 Virginia Best Places to Work (10 years running, #1 midsize in 2019).Definitive Logic is committed to hiring and retaining a diverse workforce. We are proud to be an Equal Opportunity/Affirmative Action Employer, making decisions without regard to race, color, religion, creed, sex, sexual orientation, gender identity, marital status, national origin, age, veteran status, disability, or any other protected class.If you are a qualified individual with a disability or a disabled veteran, you have the right to request a reasonable accommodation if you are unable, or limited in your ability, to use or access our Careers page: https://www.definitivelogic.com/careers/open-opportunities/ as a result of your disability. You can request a reasonable accommodation by sending an e-mail to Recruiting@DefinitiveLogic.com or via phone: 703-955-4186. In order to quickly respond to your request, please use the words ""Accommodation Request"" as your e-mail subject line.DL BenefitsHealthDentalVisionLife/AD&D: Company paidSTD/LTD:Company paidSupplemental Plans: TriCare Supplement, Pet Insurance through Nationwide, Legal Resources and hospital/accidental indemnity plans and Wellness initiatives.Compensation Benefits:Competitive Base SalaryAnnual performance based bonus401(k) & Roth option: You are fully (100%) vested on day 1 and DL matches up to 5%Spot BonusesReferral BonusesAdditional Benefits:Flexible Time Off (FTO): Under our FTO plan, there is no cap in the amount of leave you choose to take, with proper coordination and prior approval.Volunteer Hours: DL allocates up to 8 hours for you to use every year to volunteer for a 501c3 organization of your choice and DL will donate to that charity based on how many hours you volunteer.Cell Phone Reimbursement: $80/monthLocation Specific Metro/ParkingTuition ReimbursementTraining & Certifications"
Data Engineer II,Vibrant Emotional Health,United States (Remote),https://www.linkedin.com/jobs/view/3754513086/?eBP=JOB_SEARCH_ORGANIC&refId=sjQPSlxFFJAXxmDgKKgqXg%3D%3D&trackingId=gm6tu90j%2FejwZh2%2BqGpZuw%3D%3D&trk=flagship3_search_srp_jobs,3754513086,"About the job
            
 
Position Title: Data Engineer IISalary Range: $92,000-$105,800*Department: Information TechnologyReports to: Manager, Data EngineeringLocation: RemoteSchedule: M-F, 9-5 ET New hires are typically brought into the organization between the minimum to midpoint of the salary range posted depending on qualifications, internal equity, and the budgeted amount for the role.
Formerly the Mental Health Association of New York City (MHA-NYC), Vibrant Emotional Health’s groundbreaking solutions have delivered high quality services and support, when, where and how people need it for over 50 years. Through our state-of-the-art technology-enabled services, community wellness programs, and advocacy and education work, we are building a society in which emotional wellness can be a reality for everyone.Position SummaryThe Data Engineer II is responsible for the development of data pipelines, the orchestration and planning of data transformations and the development and support of data automations. This role interfaces with the Analytics and Research team around the design of data points, measures, and the implementation of models; with Data Governance roles around the alignment of policy and definitions with their implementation; and with Dev Ops, Security and Software Development teams around the broader organization’s use of data infrastructure.Duties/Responsibilities Develop patterns for data ingestion using Fivetran, AWS Lambda, and related technologies.Orchestrate data transformations using DBT, database functions, materialized views and similar patterns.Write tests, integrity checks, conduct performance monitoring and tuning of data systems.Write anomaly detection routines to support alerting and monitoring.Design and develop secure, high performance, API’s to support data requests. Establish patterns for automation of forecasts, scoring, and support of ML/AI implementations.Establish patterns for publishing and distributing data sets.Establish and maintain archiving and retention schedules.Ensure data systems conform with data governance and data security best practices.
Required Skills/Abilities  Extensive experience building and supporting data infrastructure. Direct experience working with Snowflake, DBT and Fivetran. Strong experience with AWS services such as Data Migration Services, RDS, Lambda, API Gateway, S3, etc. Expert knowledge of SQL / PSQL. Ability to code in Python and R (additional languages a +) Strong track record of managing high availability systems. Initiative to solve complex problems; takes an outside in perspective to identify innovative solutions. We value candidates who have demonstrated commitment to the goal of working with people to achieve mental and emotional wellbeing with dignity and respect.
Required Qualifications Bachelors’ or Masters’ Degree in an analytics or engineering focused discipline or equivalent experience and knowledge.1+ years of data engineering experience developing and maintaining high availability systems.
Excellent comprehensive benefits, including medical, dental, vision, supplemental income insurance, pre-tax transit/parking, pre-tax FSA for medical and dependent care, and 401K available. 4 weeks’ vacation, plum benefits, etc.Studies have shown that women and people of color are less likely to apply for jobs unless they believe they are able to perform every task in the job description. We are most interested in finding the best candidate for the job, and that candidate may be one who come from a less traditional background. Vibrant will consider any equivalent combination of knowledge, skills, education and experience to meet minimum qualifications. If you are interested in applying, we encourage you to think broadly about your background and skill set for the role.Vibrant Emotional Health is an equal opportunity employer. Applicants are considered for positions without regard to veteran status, uniformed service member status, race, creed, color, religion, gender, gender identity, sex, sexual orientation, citizenship status, national origin, marital status, age, physical or mental disability, genetic information, caregiver status or any other category protected by applicable federal, state or local laws.""Please be aware that fictitious job openings, consulting engagements, solicitations, or employment offers may be circulated on the Internet in an attempt to obtain privileged information, or to induce you to pay a fee for services related to recruitment or training. Vibrant does NOT charge any application, processing, or training fee at any stage of the recruitment or hiring process. All genuine job openings will be posted on our careers page and all communications from the Vibrant recruiting team and/or hiring managers will be from an @vibrant.org email address"""
Data Engineer,"Donato Technologies, Inc.",United States (Remote),https://www.linkedin.com/jobs/view/3688011143/?eBP=JOB_SEARCH_ORGANIC&refId=sjQPSlxFFJAXxmDgKKgqXg%3D%3D&trackingId=jv3Ms4jM1FMnSN0HSEWW9g%3D%3D&trk=flagship3_search_srp_jobs,3688011143,"About the job
            
 
Job Title: Data EngineerClient : Insurance DomainLocation: RemotePay rate : $60/hour on C2C.Required Experience Data Engineer with Enterprise Data warehouse/Datamart and ETL background.Experience in Property & Casualty (P&C) Insurance domain is required.Strong Python (DataFrame, APIs, Batch processing, Data pipelines) experience.Experience in Azure platform including Azure Data Lake, Data Bricks, Delta Lake (Bronze, Silver, Gold), Data Factory, Azure SQL DatabaseExperience in interpretation of insurance data to identify trends, patterns, and anomalies that can impact business performance and profitability.Experience in data analysis and predictive modeling to support decision-making processes such as policy performance, claims data, customer behavior etc.Strong SQL experience with excellent Excel skills (i.e. Pivot Tables).Understanding of Data modeling for Enterprise Data Warehouse ecosystemsKnowledge in Spark, PySpark preferable.Familiarity working in a data lake environment, leveraging data streaming, and developing data pipelines driven by events/queues.Working knowledge on different file formats such as JSON, Parquet, CSV, etc.Familiarity with data encryption, data maskingDatabase experience in SQL Server
QualificationsBachelor's degree in Information Science, Computer Science, Mathematics, Statistics or a quantitative discipline in science, business, or social science"
Data Engineer______________remote(PST or MST),Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3713013303/?eBP=JOB_SEARCH_ORGANIC&refId=jOJ4bUxjdJ4hWHUjgMBOEA%3D%3D&trackingId=THfMNkDuMi%2B0KxdPsmNJIQ%3D%3D&trk=flagship3_search_srp_jobs,3713013303,"About the job
            
 
Hi,Please find attached Job Description. If you are interested please do share with me your updated resume or call me on ""+1 3026017375"".Job Title:- Data EngineerWork Location:- remote(PST/MST)Duration: 3-6 monthWork Authorization:- Citizen Interview : video/skypeData EngineerRemotebut mist reside in MST or PST time zones 3-6 monthsData EngineerEnd Client: Kaiser PermanteDuration: 3-6 monthsWork schedule: 40 hrs/weekEstimated/Targeted Start: ASAPOn-site/Remote: RemotePotential for client hire: NoAdditional requirements: US Citizens only,Candidates must be PST or MSTInterview Process: 2-3 video interviewsSkillsMinimum of 5-10 years of IT/IS experienceNeed To Have Excellent Communication Skills5+ years working with Advanced Structured Query Language (SQL) & PL/SQL5+ years experience with at least Oracle relational database5+ years experience in software development lifecycle activities5+ years experience with data loading (ETL, ELT)5+ years working with data at scale 50+ TBExperience With Data Warehouses, Operational Data Stores, Data HubsExperience in end-to-end design of near-real-time and batch data pipelinesExperience Working In An Agile EnvironmentExperience developing detailed systems design and written test plansExperience Preparing Installation Instructions And Coordinating Installation ProceduresExperience documenting data audits, archiving, and restoration processesExperience With Version Control Systems GitExperience with tracking and ticket software (Jira, Confluence, etc.)Familiarity with data architecture, data integration, data governance, and data lineage conceptsEDUCATIONBachelor's Degree in computer science or a related disciplineExperienceMinimum of 5-10 years of IT/IS experienceNeed to have excellent communication skills5+ years working with Advanced Structured Query Language (SQL) & PL/SQL5+ years experience with at least Oracle relational database5+ years experience in software development lifecycle activities5+ years experience with data loading (ETL, ELT)5+ years working with data at scale 50+ TBExperience with data warehouses, operational data stores, data hubsExperience in end-to-end design of near-real-time and batch data pipelinesExperience working in an Agile environmentExperience developing detailed systems design and written test plansExperience preparing installation instructions and coordinating installation proceduresExperience documenting data audits, archiving, and restoration processesExperience with version control systems GitExperience with tracking and ticket software (Jira, Confluence, etc.)Familiarity with data architecture, data integration, data governance, and data lineage conceptsNICE TO HAVELeading a teamMaster's DegreeHealth care experienceProfessional certificationsOther Programming Language ExperienceExperience in Machine learning or Artificial IntelligenceFamiliarity with microservicesFamiliarity with DevSecOps2+ years working with one NoSQL database (Hadoop)Experience with MongoDB, Redshift, Synapse, or others is a plus.Experience working in one public cloud environment (Azure, AWS, etc.)Snowflake familiarityExperience with containerization (Docker, Kubernetes, etc.)Familiarity with agile and lean methodologiesFamiliarity with JSON, XMLPERSONALWelcomes new approaches and innovative thinkingSelf-organized And Responsible With Experience In a Distributed TeamAble to multitask and be responsive/flexible to support customersAbility to work with others from diverse skill sets and backgroundsTakes ownership of a situation and sees it through the completionAble to switch context and complete work processesKirti RaniAssociate Talent Acquisition -North AmericaDesk: +1 3026017375kirti@steneral.comIn my absence please reach out to Mr. Harish Sharma at harish@steneral.com & 3027216151"
Data Engineer - GTM & Finance,Pinecone,"San Francisco, CA (Remote)",https://www.linkedin.com/jobs/view/3748536344/?eBP=JOB_SEARCH_ORGANIC&refId=jOJ4bUxjdJ4hWHUjgMBOEA%3D%3D&trackingId=H6aIGdBCwtrIA9yH%2BZ1i7A%3D%3D&trk=flagship3_search_srp_jobs,3748536344,"About the job
            
 
About PineconePinecone is on a mission to build the search and database technology to power AI applications for the next decade and beyond. Our fully managed vector database makes it easy to add vector search to AI applications. Since creating the “vector database” category, demand has grown incredibly fast and it shows in our user base.We are a distributed team with clusters in New York, San Francisco, Tel-Aviv, and Manchester.About The RolePinecone is seeking a skilled and highly motivated Senior Data Engineer to lay the foundation of our data practice across GTM and Finance organizations. As a Senior Data Engineer, you will collaborate with cross-functional teams to develop and deliver board reports and operational data that provide meaningful insights to guide business decisions and strategies.You will work in a fast-paced and rewarding environment that demands the highest quality work with minimal supervision. And as we all do a little bit of everything, you will also be a strong generalist, work directly with executive leadership, and mentor new data engineers and scientists.ResponsibilitiesOptimize Data Delivery and ScalabilityStreamline and enhance data delivery pipelines to improve efficiency and scalability, ensuring seamless access to data for various teams within the organization.Enable Insights and ReportingCollaborate with cross-functional teams to develop and deliver board reports and operational data that provide meaningful insights to guide business decisions and strategies.Enhance Operational EfficiencyImplement measures to improve the efficiency and effectiveness of data engineering operations, enabling smoother data processing and analysis.What we look for: A passion for technology 5+ years of experience with SQL and Python5+ years of experience in the Data Eng/Science spaceBS in Computer Science, Math, a related technical field or equivalent experienceStrong foundations in databases, warehousing, data infrastructure, ELT/ETLExperience working cross functionally to ground business strategy and process in data
Bonus Points: Experience with orchestration platforms Experience with metric and features storesExpertise working with cloud-based data warehouse solutions (BigQuery, Snowflake)Knowledge and experience in the GTM and Finance space for B2B products"
Data Engineer,EPSoft,United States (Remote),https://www.linkedin.com/jobs/view/3689428796/?eBP=JOB_SEARCH_ORGANIC&refId=jOJ4bUxjdJ4hWHUjgMBOEA%3D%3D&trackingId=0VroNv3jet8qEHxmLF1I0g%3D%3D&trk=flagship3_search_srp_jobs,3689428796,"About the job
            
 
Requirements Data Engineer with Integration (ETL/Informatica), Database (SQL Server/Oracle) and Automation (API, Python scripting etc.)experienceExperienced in large-scale application development testing Cloud/ On Prem Data warehouse, Data Lake, Data scienceExperience with multi-year, large-scale projectsExpert technical skills with hands-on testing experience using SQL queries.Extensive experience with both data migration and data transformation testingExtensive experience DBMS like Oracle, Teradata, SQL Server, DB2, Redshift, Postgres and Sybase.Extensive testing Experience with SQL/Unix/Linux.Extensive experience using Python scripting and Cloud Technologies.API/RESTAssured automation, building reusable frameworks, and good technical expertise/acumenJava/Java Script - Implement core Java, Integration, Core Java and API. Functional/UI/ Selenium - BDD/Cucumber, Specflow, Data Validation/Kafka, Big Data, also automation experience using Cypress.API/Rest API - Rest API and Micro Services using JSON, SoapUI.Extensive experience in Map reduce using tools like Hadoop, Hive, Pig, Kafka, S4, Map R.Experience in testing storage tools like S3, HDFSExperience with one or more industry-standard defect or Test Case management ToolsGreat communication skills (regularly interacts with cross functional team members)"
Azure Data Engineer,Analytica,"Washington, DC (Remote)",https://www.linkedin.com/jobs/view/3705613492/?eBP=JOB_SEARCH_ORGANIC&refId=jOJ4bUxjdJ4hWHUjgMBOEA%3D%3D&trackingId=DJi4E0nGvdBHJXD1r7XC%2BA%3D%3D&trk=flagship3_search_srp_jobs,3705613492,"About the job
            
 
Analytica is seeking a talented remoteAzure Data Engineer to join its growing Analytics practice in support of our federal clients. The ideal candidate will be comfortable working in an agile, multi-faceted team that involves direct, collaborative client engagement to design and develop software products.Analytica has been recognized by Inc. Magazine as one of the fastest-growing 250 businesses in the US for 3 years. We work with U.S. government clients in health, civilian, and national security missions to build better technology products that impact our day-to-day lives. The company offers competitive compensation with opportunities for bonuses, employer-paid health care, training and development funds, and 401k match. Responsibilities include (but not limited to):  Lead initiatives to create and maintain optimal data pipeline architecture.Design, build and maintain large, complex data processing pipelines using Apache Spark on Databricks in Azure tomeet enterprise requirements for reporting and analytics.Provide operational and functional support on creating, storing, managing, and maintaining enterprise data, including the ability to incorporate policies and procedures for centrally managing and sharing data through the data life cycle.Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, and re-designing for flexibility and scalability.Build the infrastructure required for extraction, transformation, and loading of data from a wide variety of cloud and on-premises data sources.
Work with stakeholders including to assist with data-related technical issues and support their data infrastructure needs.Work closely with data architecture, data governance and data analytics teams to ensure pipelines adhere to enterprise standards, usability, and performanceGuide and mentor team members on best practices, effective design patterns and data maturity.Ensure safeguards and security are effectively implemented to protect UMA enterprise data.Ensure code is well structured, sufficiently documented, and is easy to maintain and reuse.
Implement DataOps and DevSecOps for data initiatives using UMA’s preferred processes and tools. Participate as team member / leader in Agile/Scrum teamsWork on continuous improvement initiatives and effectively address root cause analysis outcomes to deliver sustainable excellence.Mentor, coach, and support team members
Basic Qualifications: Bachelor’s degree in Computer Science, Engineering, IT, or other scientific or quantitative fields7+ years of experience in a Data Engineer or equivalent role in enterprise initiatives.3+ years of experience with Azure: ADF, ADLS Gen 2, Azure Synapse, Databricks, Streamsets or equivalent technologies,3+ years of experience in working with SQL databases, Transaction replication and Change data capture technologies.Experience building processes to support data transformation, data structures, metadata, dependencies, and workload management.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.Experience with Data Ops , DevSecOps, CI/CD workflowsStrong communication, interpersonal, and collaboration skills with both business and technical resourcesExperience supporting and working with cross-functional teams in a dynamic environment.Demonstrated team player with proactive, can-do attitude with a demonstrated ability to handle multiple priorities.
Able to support a diverse and inclusive work environment.Ability to secure a US Federal Clearance.
About Analytica: Analytica is a leading consulting and information technology solutions provider to public sector organizations supporting health, civilian, and national security missions. The company is an award-winning SBA certified 8(a) small business that has been recognized by Inc. Magazine each of the past three years as one of the 250 fastest-growing companies in the U.S. Analytica specializes in providing software and systems engineering, information management, analytics & visualization, agile project management, and management consulting services. The company is appraised by the Software Engineering Institute (SEI) at CMMI® Maturity Level 3 and is an ISO 9001:2008 certified provider.As a federal contractor, Analytica is required to verify that all employees are fully vaccinated against COVID-19. If you receive an offer and are unable to get vaccinated for religious or medical reasons, you may request a reasonable accommodation."
Data Analytics Engineer____________________remote,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3647011796/?eBP=JOB_SEARCH_ORGANIC&refId=jOJ4bUxjdJ4hWHUjgMBOEA%3D%3D&trackingId=XSNPxjH3YGS0xmEJLzebKg%3D%3D&trk=flagship3_search_srp_jobs,3647011796,"About the job
            
 
Hi,Please find attached Job Description. If you are interested please do share with me your updated resume or call me on ""+1 3024402487"".Job Title:- Data Analytics Engineer/Python/Tableau/CloudWork Location:- 100% RemoteDuration: 6month contractWork Authorization:- citizen/gcInterview: videoTitle- Data Analytics Engineer/Python/Tableau/CloudLinkedin must.Jd-We need a senior (10+ Years) Data Analytics engineer with heavy Python and data visualization (Tableau or Power BI) and working with Large Cloud Data (BigQuery) sets as well as heavy ETL experience. Matlab is a big plus. This is a high-level resource.. Statistical and Matlab experienceCandidates Must Have PythonTableau or Power BICloud DatabasesETL Development
As an Analytics Engineer you will play a critical role in the execution of the global analytics strategy and raising the technical bar for junior data analysts. You will develop, maintain, and continuously improve business-focused reporting and visualization suites to promote data-driven decision making. You will write efficient, clear code as well as comprehensive, accurate technical documents to ensure deliverables meet customer needs and team goals. You will collaborate with an interdisciplinary team of engineers, scientists, and product managers to translate business and functional requirements into concrete deliverables, including the design, development, testing, and deployment of scalable solutions. You will provide essential training to existing business analysts to empower them with the technical skill necessary to meaningfully contribute to a data science & analytics team. You will bring engineering maturity and structure to ambiguous business problems and address the challenges of product creation, development, and improvement with an appreciation for the behaviors and needs of our consumers. You will partner with analysts and business leaders to create compelling visualization environments. You will acquire data by building the necessary queries and pipelines. You will automate data transformation, validation, monitoring, and alerting, with emphasis on maintainability, robustness, and scalability. You will develop ETL/ELT processes that comply with the computational demands, accuracy, and reliability of the relevant processes at various stages of production. You are an effective communicator capable of independently driving issues to resolution and communicating insights to non-technical audiences. You tackle intrinsically difficult problems; you are interested in learning and will acquire skills and expertise as needed.Responsibilities Utilize code (SQL, python, etc.) and apply engineering, reporting, and visualization expertise to solve business problems. Develop scalable tools to drive automation and optimize business operations.Develop and lead a team of junior data analyst, providing mentorship, training, and coaching to increase technical acumen across the team.Develop scalable systems to expand reporting capabilities, facilitate ad hoc analysis, and improve data-driven decision making at all levels of the business.Work with large, complex data sets. Solve difficult data transformation problems with efficiency, applying advanced analytical methods as needed. Conduct analysis that includes data gathering and requirements specification, processing, analysis, ongoing deliverables, and presentations.Contribute to the establishment of standards for writing clean, organized code and documentation to streamline data and analytics workflow.Drive and promote a culture of testing, observability, and scalability with a data-driven mindsetManage and continuously improve business user's experience with data and reporting, including KPI development, data visualization, and communication of applicable insights to audiences at varying levels of technical sophistication. Increase overall data literacy across through education and advocacy.Develop partnerships with engineering, data science, and product teams to deliver on major cross-functional reporting, measurement, and testing efforts.Actively participate in diversity and inclusion agenda.
Requirements BS/BA in a quantitative field; a plus if MS degree in a quantitative discipline (e.g., Statistics, Operations Research, Economics, Computer Science, Mathematics, Engineering) or equivalent practical experience.10+ years of experience (2+ with a graduate degree) in Analytics, Data Science, or Software Engineering, both as an individual contributor and managing teams; including expertise with descriptive statistical analysis and data transformationExperience with analytics software such as Looker, PowerBI, Tableau, or similar open-source tools6+ years of experience with database languages (SQL, etc...). 3+ year of experience with data scripting languages or statistical/mathematical software (Python, R, Matlab, etc.) is a plusTrack record of delivering data-driven products and insights as well as influencing product and engineering decisions.Significant experience working with large-scale cloud database systems (e.g., BigQuery). Broad Knowledge of best practices in large- and small-scale data processingExperience in analyzing, validating, and transforming large datasets.Proficiency in deploying data-intensive solutions. Solid understanding of the software development lifecycle.4+ years of hands-on experience in crafting compelling data visualizations to inform business decisions
Thanks & Regards,Apurva AnandTalent Acquisition - North AmericaDirect: +13024402487apurva@steneral.comIn my absence please reach out to Mr. Harish Sharma at harish@steneral.com & 3027216151"
Sr. Data Engineer,Blueprint,"Dallas, TX (Remote)",https://www.linkedin.com/jobs/view/3736679720/?eBP=JOB_SEARCH_ORGANIC&refId=jOJ4bUxjdJ4hWHUjgMBOEA%3D%3D&trackingId=SLGYOTkVV0TJ%2BOIMJv966A%3D%3D&trk=flagship3_search_srp_jobs,3736679720,"About the job
            
 
Senior Data Engineer, AzureSenior Data EngineerRemote-US OnlyWho is Blueprint?We are a technology solutions firm headquartered in Bellevue, Washington, with a strong presence across the United States. Unified by a shared passion for solving complicated problems, our people are our greatest asset. We use technology as a tool to bridge the gap between strategy and execution, powered by the knowledge, skills, and the expertise of our teams, who all have unique perspectives and years of experience across multiple industries. We're bold, smart, agile, and fun.What does Blueprint do?Blueprint helps organizations unlock value from existing assets by leveraging cutting-edge technology to create additional revenue streams and new lines of business. We connect strategy, business solutions, products, and services to transform and grow companies.Why Blueprint?At Blueprint, we believe in the power of possibility and are passionate about bringing it to life. Whether you join our bustling product division, our multifaceted services team or you want to grow your career in human resources, your ability to make an impact is amplified when you join one of our teams. You'll focus on solving unique business problems while gaining hands-on experience with the world's best technology. We believe in unique perspectives and build teams of people with diverse skillsets and backgrounds. At Blueprint, you'll have the opportunity to work with multiple clients and teams, such as data science and product development, all while learning, growing, and developing new solutions. We guarantee you won't find a better place to work and thrive than at Blueprint.What will I be doing?Blueprint is looking for an Azure, Senior Data Engineer to join us as we build cutting-edge technology solutions! This is a fast-paced role that needs a dedicated and passionate individual focused on team and client satisfaction! The ideal candidate will have a solid background in consulting, with demonstrating experience in leading clients through the process of building modern data estates. You will also be responsible for mentoring any junior developers on the engagement.Responsibilities: Develop and implement effective data architecture solutions using Databricks and LakehouseOptimize and tune data pipelines for performance and scalabilityMonitor and troubleshoot data pipelines to ensure data availability and reliabilityCollaborate with data scientists, analysts, and other stakeholders to understand their data needs and build solutions that enable them to extract insights from dataImplement best practices for data governance, data security, and data quality to ensure data integrity across all data sourcesCreate and maintain documentation related to data architecture, data pipelines, and data modelsStay up to date with emerging technologies and best practices in data engineering and big data processingMentor and train other data engineers on best practices for data engineering and Databricks usageProvide thought leadership in the Databricks and Lakehouse space, both within the organization and externally
Qualifications: Bachelor's degree in computer science or equivalent experienceAt least 5+ -years of experience as a data engineerAt least 5+ years of experience with SQL Development (ETL transformations, stored procedure)Data Ingestion experience from inception to Gold MedallionStrong understanding of data engineering, data warehousing, data modeling, data governance, and data security best practicesAt least 3-5 -years of experience programming with PySpark performing various transformationsDesign, build, implement and maintain our data infrastructure to power analytics and MLPartner with engineers, producers and designers to deliver data insights that impact our playersContribute to our investments into various open-source and 3rd party tools to build a system that scales with the companyCollaborate with our Data Scientists and Analytics Engineers to make pipeline implementation faster, more straightforward, and more trustworthy driven decisions that will shape our business2-5+ years building large scale data infrastructure on Spark/Databricks or similar3+ years experience working with real-time data ingestion/processingWorking knowledge of Databricks DLT(Delta Live Table) and Unity Catalog a plusExperience with relational and non-relational database technologies (i.e., NoSQL, blob storage, etc.)Experience with data wrangling skills with csv, tsv, parquet, and jsonExperience designing, building and optimizing Big Data platforms that are robust, scalable and reliableExcellent problem-solving and troubleshooting skills
Salary RangePay ranges vary based on multiple factors including, without limitation, skill sets, education, responsibilities, experience, and geographical market. The pay range for this position reflects geographic based ranges for Washington state: $146,400 to $175,100 USD/annually. The salary/wage and job title for this opening will be based on the selected candidate's qualifications and experience and may be outside this range.Equal Opportunity EmployerBlueprint Technologies, LLC is an equal employment opportunity employer. Qualified applicants are considered without regard to race, color, age, disability, sex, gender identity or expression, orientation, veteran/military status, religion, national origin, ancestry, marital, or familial status, genetic information, citizenship, or any other status protected by law.If you need assistance or a reasonable accommodation to complete the application process, please reach out to: recruiting@bpcs.comBlueprint believe in the importance of a healthy and happy team, which is why our comprehensive benefits package includes: Medical, dental, and vision coverageFlexible Spending Account401k programCompetitive PTO offeringsParental LeaveOpportunities for professional growth and development"
Data Engineer,Barbaricum,United States (Remote),https://www.linkedin.com/jobs/view/3768668695/?eBP=JOB_SEARCH_ORGANIC&refId=jOJ4bUxjdJ4hWHUjgMBOEA%3D%3D&trackingId=csj8uvfSS0KWbGuwgNFHUg%3D%3D&trk=flagship3_search_srp_jobs,3768668695,"About the job
            
 
Barbaricum is a rapidly growing government contractor providing leading-edge support to federal customers, with a particular focus on Defense and National Security mission sets. We leverage more than 15 years of support to stakeholders across the federal government, with established and growing capabilities across Intelligence, Analytics, Engineering, Mission Support, and Communications disciplines. Founded in 2008, our mission is to transform the way our customers approach constantly changing and complex problem sets by bringing to bear the latest in technology and the highest caliber of talent. Headquartered in Washington, DC's historic Dupont Circle neighborhood, Barbaricum also has a corporate presence in Tampa, FL and Dayton, OH, with team members across the United States and around the world. As a leader in our space, we partner with firms in the private sector, academic institutions, and industry associations with a goal of continually building our expertise and capabilities for the benefit of our employees and the customers we support. Through all of this, we have built a vibrant corporate culture diverse in expertise and perspectives with a focus on collaboration and innovation. Our teams are at the frontier of the Nation's most complex and rewarding challenges. Join us. Barbaricum is seeking a Data Engineer to support a Naval Surface Warfare Center (NSWC) Crane Division initiative to stand up a hybrid cloud-based System Integration Lab (SIL). This SIL will aggregate, store, and further process ingested vehicle data to increase maintenance availability. This data will then be transmitted to external analytics organizations for predictive analysis. This initiative will support the United States Marine Corps (USMC) Condition Based Maintenance (CBM+) capability, which is a key component of USMC’s strategy to reduce maintenance workloads, costs, and downtime, improve supply chain visibility and management and overall military readiness, and sustain lethality.In order to realize the CBM+ capability, data will be collected from USMC ground vehicles transmitted through electronic means so it can be aggregated, stored, and further processed and analyzed using diagnostic, reliability, predictive trend, prognostic, and condition monitoring analysis. This analysis will be leveraged to reevaluate maintenance strategies, capture demand, conductive preemptive maintenance, and create decision support tools.This position may be performed remotely. Limited travel to government customer site may be requested, but proximity to NSWC-Crane is not required for this role.Responsibilities Responsible for establishment and operation of a data pipeline to support transfer of USMC ground vehicle Controller Area Network (CAN bus) data from USMC vehicle fleet to a System Integration Lab (SIL) hybrid cloud environment hosted at NSWC Crane.Utilize data engineering tools and generate scripts to enable transformation of raw CAN bus data into processed data ready for ingestion into SIL cloud environment.Develop automation processes for ingestion of processed data into SIL cloud environment and export of data to external predictive analytics organizations.Identify methods to improve data collection, data integrity, data security, analysis, validation, scalability, and reporting.Design and implement data solutions using industry best practices.Maintain and monitor data pipelines to ensure high service availability.Work with Data Scientists to understand analysis needs and mathematical models and optimize data solutions accordingly.
Qualifications Active DoD Secret clearance requiredExperience writing re-usable code in R, Python, Java, Scala, SQL, or other languagesBachelor’s degree in Computer or Electrical Engineering, Computer Science, Data Science or Information Systems5+ years of demonstrated experience working in a data engineering / data scientist roleExperience designing, managing, and developing Artificial Intelligence (AI) and/or Machine Learning (ML) data models spanning multiple disparate sourcesDemonstrated experience with DOD data systems (Qlik, Databricks, AWS SageMaker)Demonstrated experience with relational databases (e.g., Oracle, SQL Server, PostgreSQL)Demonstrated experience with NoSQL databases (e.g., MongoDB, InFluxDB, Redis)Demonstrated knowledge of extract, transform, and load (ETL) best practices in an enterprise environmentKnowledge of software engineering best practices (Agile principles, code standards, code reviews, source code management, build processes, testing, and operations)Ability to communicate effectively with individuals in both technical and leadership roles
Preferred Qualifications Familiarity in ASAM MDF (Measurement Data Format), NASA Common Data format (CDF), and Army Bulk CBM+ Data (ABCD) formats for logging of CAN Bus dataKnowledge of Controller Area Network (CAN bus) standards including: ISO 11898 / SAE J1939 / SAE 1587 / SAE J1708Current or previous work experience supporting USMC or other Navy customers would be a plusDemonstrated experience working with data logging devices to capture CAN bus data from heavy-duty vehiclesCompTIA Security+ Certification or other DoD 8570 IAT II-level certification strongly preferred prior to start date
Additional InformationFor more information about Barbaricum, please visit our website at www.barbaricum.com . We will contact candidates directly to schedule interviews. No phone calls please."
Data QA Automation Engineer II,GOBankingRates,"Los Angeles, CA (Remote)",https://www.linkedin.com/jobs/view/3735382947/?eBP=JOB_SEARCH_ORGANIC&refId=jOJ4bUxjdJ4hWHUjgMBOEA%3D%3D&trackingId=t4OIO1KzlY46j0kPYZGzTg%3D%3D&trk=flagship3_search_srp_jobs,3735382947,"About the job
            
 
GOBankingRates™ is unique in the digital marketing and media industry - we combine marketing, digital, content and fintech. Our performance based approach increases brand awareness and generates targeted audience engagement on our internal web properties and partner sites.Learn More About What We DoHow Will You Make an Impact? Participate in Agile ceremonies, help analyze requirements, develop testing strategies, and write manual and automated test scripts for new and existing functionality. Monitor all development cycles, prepare test data, design and execute test plans, and evaluate test resultsMentor fellow Test Engineers on the team on automation concepts and contribute towards maintaining full test coverageCollaborate with product owners, project managers, data analysts, data scientists, and developers to refine user stories and acceptance criteriaAct as a key point of contact for all QA aspects of releases, providing QA services, UAT guidance, and coordinating QA resources internally and externally for the squad. Suggest process improvements that enable efficient delivery and maintenanceIdentify areas for cross functional testing to improve overall quality and, with peers or others, implement initiatives improving testing capability and efficiencyHave a leadership mentality when it comes to research and implementation of current and new technologies to help expand our current processes and time-to-market
What Do You Bring to Us? Bachelors in Computer Science or Data Science or Information Systems or Mathematics or Statistics or equivalent years of relevant work experience4+ years of experience with agile software development testing, reviewing user stories, acceptance criteria, and other available information in order to develop test plans and test scenarios, both manual and automated4+ years of experience in database testing for relational databases( preferred Snowflake and MS SQL Server), ETL/ELT data solutions, and reporting and analytics toolsKnowledge of dimensional modeling and data warehouse concepts, such as star schemas, snowflakes, dimensions, factsExperience in writing complex SQL queries, ability to determine the types of testing that must be conducted (i.e., data validation, regression, etc.), including evaluating the testability of requirements and create a comprehensive test plan that supports the business and technological solutions being delivered3+ years of experience creating automated scripts using pytest for data validation Expertise building test architecture and framework from ground up 1+ years of experience with API testing; manual (using tools like Postman/swagger) & automated (REST Assured/CURL). Experience working with Cloud services and CI/CD tools like JenkinsAbility to multi-task and adapt quickly to changes while maintaining urgency in completing assigned tasks
The salary range for this role is $125,000-$135,000 per year. Pay offered may vary based on a number of factors including but not limited to job-related knowledge, skills, experience, and location.Benefits Competitive salary with excellent growth opportunity; we pride ourselves in having a team that exudes leadership, high initiative, creativity and passion. Awesome medical, dental and vision plans with heavy employer contributionPaid maternity leave and paternity leave programsPaid vacation, sick days and holidaysCompany funding for outside classes and conferences to help you improve your skillsContribution to student loan debt payments after the first year of employment401(k) -- employees can start contributing immediately. After the first year, GOBankingRates matches your contribution up to 4% of your salary
A note about our new norm: The world has changed and we know it's important to adapt and to do our part to do what's best for our team. Our number one priority is to have our team feel safe, balanced and connected. We're committed to providing our teams with the best resources and tools to navigate this new virtual world that we're living in. We've also reinvented the ways in which we recognize, celebrate, and engage with each other to keep our culture strong!Here's a peek into our world at GOBankingRates -  Our teams are working remotely 100% for the foreseeable future. We're in the digital media space, so we're mobile and flexible! *Option to work from an office (if you need to get away!)
Tools & resources are available to keep our team connected across North America. (JIRA, Trello, Slack, Zoom and so much more!)To keep our community engaged and connected, virtual team building events are held weekly and monthly. For wellness and balance, weekly virtual fitness classes such as yoga are available. To care for the local communities that we're a part of across the U.S our team members host socially distanced philanthropic events every quarter. And most importantly, we've committed to consistent and transparent communication to help us all stay informed, engaged and to keep us on our path to success and #greatness. 
We are an equal-opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law."
Senior Data Engineer,BambooHR,"Lindon, UT (Remote)",https://www.linkedin.com/jobs/view/3760292962/?eBP=JOB_SEARCH_ORGANIC&refId=jOJ4bUxjdJ4hWHUjgMBOEA%3D%3D&trackingId=ivAByxhmPVw1sH04Ae9irA%3D%3D&trk=flagship3_search_srp_jobs,3760292962,"About the job
            
 
About UsOur mission is simple: we want to set people free to do meaningful work. People love our software—and it turns out that people love working here too. We've been recognized as a ""Best Company to Work For"" and we're proud of our team for creating software that makes an impact in the lives of HR pros and employees all over the world.What You'll DoAs a Senior Data Engineer on the data platform team, we'll rely on your expertise across multiple disciplines to develop, deploy and support data systems, data pipelines, data lakes, and lakehouses. Your ability to automate, performance tune, and scale the data platform will be key to your success.Your initial areas of focus will include: Collaborate with stakeholders to make effective use of core data assetsWith Spark and Pyspark libraries, load both streaming and batched dataEngineer lakehouse models to support defined data patterns and use casesLeverage a combination of tools, engines, libraries, and code to build scalable data pipelinesWork within an IT managed AWS account and VPC to stand up and maintain data platform development, staging, and production environmentsDocumentation of data pipelines, cloud infrastructure, and standard operating proceduresExpress data platform cloud infrastructure, services, and configuration as codeAutomate load, scaling, and performance testing of data platform pipelines and infrastructureMonitor, operate, and optimize data pipelines and distributed applicationsHelp ensure appropriate data privacy and securityAutomate continuous upgrades and testing of data platform infrastructure and servicesBuild data pipeline unit, integration, quality, and performance testsParticipate in peer code reviews, code approvals, and pull requestsIdentify, recommend, and implement opportunities for improvement in efficiency, resilience, scale, security, and performance
What You Need to Get the Job Done (if you don't have all, apply anyway!) Experience developing, scaling, and tuning data pipelines in Spark with PySparkUnderstanding of data lake, lakehouse, and data warehouse systems, and related technologiesKnowledge and understanding of data formats, data patterns, models, and methodologiesExperience storing data objects in hadoop or hadoop like environments such as S3Demonstrated ability to deploy, configure, secure, performance tune, and scale EMR and Spark Experience working with streaming technologies such as Kafka and KinesisExperience with the administration, configuration, performance tuning, and security of database engines like Snowflake, Databricks, Redshift, Vertica, or GreenplumAbility to work with cloud infrastructure including resource scaling, S3, RDS, IAM, security groups, AMIs, cloudwatch, cloudtrail, and secrets managerUnderstanding of security around cloud infrastructure and data systemsGit-based team coding workflows
Bonus Skills (Not Required, So Apply Anyway!) Experience deploying and implementing lakehouse technologies such as Hudi, Iceberg, and DeltaExperience with Flink, Presto, Dremio, Databricks, or KubernetesExperience with expressing infrastructure as code leveraging tools like TerraformExperience and understanding of a zero trust security frameworkExperience developing CI/CD pipelines for automated testing and code deploymentExperience with QA and test automationExposure to visualization tools like Tableau
Beyond the technical skills, we're looking for individuals who are: Clear communicators with team members and stakeholdersAnalytical and perceptive of patternsCreative in codingDetail-oriented and persistentProductive in a dynamic setting
If you love to learn, you'll be in good company. You'll likely have a Bachelor's degree in computer science, information systems, or equivalent working experience.Schedule 9AM-5PM MST, Monday-Friday
Work Environment Remote
What You'll Love About Us Great Company Culture. We've been recognized by multiple organizations like Inc, Salt Lake Tribune, Glassdoor, & Comparably for our great workplace cultureMake an Impact. We care about your individuality by giving you freedom to grow and create within the company, regardless of your positionRest and Relaxation. 4 weeks paid time off, 11 paid holidays, and we pay you to go on vacation (ask us about this)!Health Benefits. Medical with HSA and FSA options, dental, and visionPrepare for the Future. 401(k) with a generous company match, access to a personal financial planner, and both legal and life insuranceFinancial Peace University. We pay for a one year subscription and you walk away with financial savvy and a bonusGive back. Get paid to give your time to the community: ask us about this!Educational Benefits. Whether you are a previous student, or currently enrolled in higher education, we can help cover some of those expensesFlexible Work Models. In-office, work-from-home, or hybrid, depending on position and location
An Equal Opportunity Employer--M/F/D/VBecause our team members are trusted to handle sensitive information, we require all candidates that receive and accept employment offers to complete a background check before being hired.For information on our Privacy Policy, click here."
TEST DATA ENGINEER-REMOTE-EST Or CST Only || Remote,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3673754570/?eBP=JOB_SEARCH_ORGANIC&refId=jOJ4bUxjdJ4hWHUjgMBOEA%3D%3D&trackingId=Dl%2BAd8oAcxd13NRHDn0B9w%3D%3D&trk=flagship3_search_srp_jobs,3673754570,"About the job
            
 
GenRocket is MUST have. Additionally, I would like to see minimum of 1+ yr. of implementation experience.  Must required: GenRocket, scripting, PostGres/DB ability, API work Determine best practice(s) around synthetic, masked data in DCE/Platform Analyze and understand how data flows in data hub Determine best way to manage and create synthetic data in Data Hub; deliver solution Figure out a solution to mock massive accounts coming through APIs Understanding data models, table structures and dependencies. Guide teams in generating new synthetic data, creating more, and maintaining over time. Create PoC for syntheitc data use cases working with Product team Agile Mindset, familiarity with Azure Cloud ADO experience, python or powershell scripting experience
Must required: GenRocket, scripting, PostGres/DB ability, API workDetermine best practice(s) around synthetic, masked data in DCE/PlatformAnalyze and understand how data flows in data hubDetermine best way to manage and create synthetic data in Data Hub; deliver solutionFigure out a solution to mock massive accounts coming through APIsUnderstanding data models, table structures and dependencies.Guide teams in generating new synthetic data, creating more, and maintaining over time.Create PoC for syntheitc data use cases working with Product teamAgile Mindset, familiarity with Azure CloudADO experience, python or powershell scripting experience"
Sr. Data Engineer,Otter Products,United States (Remote),https://www.linkedin.com/jobs/view/3694381629/?eBP=JOB_SEARCH_ORGANIC&refId=jOJ4bUxjdJ4hWHUjgMBOEA%3D%3D&trackingId=13ARQfmtPM2M1%2FZDQ4tSlQ%3D%3D&trk=flagship3_search_srp_jobs,3694381629,"About the job
            
 
Overview Otter Products is currently recruiting for a Sr. Data Engineer. You can be based in our Fort Collins, CO office with flexibility to work a portion of your time remotely, or 100% Remote in the U.S.The Sr. Data Engineer will play a pivotal role in building and operationalizing the minimally inclusive data necessary for the enterprise data initiatives following industry standard practices and tools. The bulk of the Sr. Data Engineer’s work would be in designing, managing and optimizing data pipelines and then moving these data pipelines effectively into production for key data and analytics consumers like business/data analysts, data scientists or any persona that needs curated data for data and analytics use cases across the enterprise. The Sr. Data Engineer also needs to guarantee compliance with data governance and data security requirements while creating, improving and operationalizing these integrated and reusable data pipelines. This would enable faster data access, integrated data reuse and vastly improved time-to-solution for data and analytics initiatives. Additionally, the Sr. Data Engineers will also be expected to collaborate with data scientists, data architects, data analysts and other data consumers and work on the models and algorithms developed by them in order to optimize them for data quality, security and governance and put them into production leading to potentially large productivity gains. About Otter Products At Otter Products, we grow to give. From our founder’s garage in 1998 to the global technology leader we are today, Otter Products continues to drive growth through innovation.Through our industry-leading brands — OtterBox, Liviri and OtterCares — we provide our partners the number-one selling and most trusted products in our categories. Our philanthropic spirit is the foundation on which we foster our partner relationships, allowing us to grow and to give — together.By way of our charitable arm, the OtterCares Foundation, we support our communities and invest in our future through education that inspires kids to change the world.And even as our global community of Otters continues to grow, our founder’s core values are still at the heart of everything we do. We measure our success by our ability to give back to our communities and strengthen opportunities for all.For more information visit otterproducts.com Responsibilities  Responsible for the verification and validation of data moving into or out of systems providing information to identify issues or inaccuracies in ETL pipelines from internal and external systemsManage Azure Data Catalog and Business Glossary Application ensuring linage and data sources are mapped within the applicationPrimary lead working with data architects to define the development of data systems, creating data pipelines and optimizing ETL processes for ingest of data internally and externallyPrimary resources to collaborate with a cross-functional team to determine requirements for data needs and requirementsProvide technical guidance and coaching to members of the data teamSet the standards on Power BI Dashboard/Reporting creationDefine and manage standards, guidelines, and processes to ensure data quality.Oversee the creation and maintenance of Data Quality Metrics that drive improvement in Otter Products data qualityDevelop and maintain the standards on ETL development within OtterDefine and own the release management process for ETL code deployment, including version controlEvaluate and recommend emerging technologies for data management, storage, and analyticsSupport and maintain a positive safety culture by following all safety policies and procedures and actively contributing to a safe working environmentOther duties as required
 Qualifications  Bachelor’s degree required. Degree in Computer Science or Mathematics preferred.Minimum of six years of experience in an IT or analytical role required. Experience in database development, report writing and/or statistics preferred.
 EEO Otter Products, LLC is an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, age, marital status, pregnancy, sex, sexual orientation, gender, gender identity or expression, national origin, disability, veteran status, or any other characteristic or status protected by law. For US Based Roles Only - Compensation Range Minimum USD $110,000.00/Yr. For US Based Roles Only - Compensation Range Maximum USD $135,000.00/Yr. Additional Total Rewards Benefits Eligible - Full Time- check out otterproducts.com/careers/why for more info, Profit Sharing Program Eligible"
Data Engineer (Snowflake),Workforce Connections,United States (Remote),https://www.linkedin.com/jobs/view/3766667239/?eBP=JOB_SEARCH_ORGANIC&refId=jOJ4bUxjdJ4hWHUjgMBOEA%3D%3D&trackingId=1M3UMiNyEopfZa4bUKWMkg%3D%3D&trk=flagship3_search_srp_jobs,3766667239,"About the job
            
 
Data Engineer II (Snowflake)Remote - Must reside in USPosition Purpose: Develops and operationalizes data pipelines to make data available for consumption (reports and advanced analytics), including data ingestion, data transformation, data validation / quality, data pipeline optimization, and orchestration. Engages with the DevSecOps Engineer during continuous integration and continuous deployment. Developing and maintaining data processing solutions. Designs and implements standardized data management procedures around data staging, data ingestion, data preparation, data provisioning, and data destruction (scripts, programs, automation, assisted by automation, etc.)Designs, develops, implements, tests, documents, and operates large-scale, high-volume, high-performance data structures for business intelligence analyticsDesigns, develops, and maintains real-time processing applications and real-time data pipelinesEnsure quality of technical solutions as data moves across Company’s environmentsProvides insight into the changing data environment, data processing, data storage, and utilization requirements for the company and offers suggestions for solutionsDevelops, constructs, tests, and maintains architectures using programming language and toolsIdentifies ways to improve data reliability, efficiency, and quality; use data to discover tasks that can be automatedPerforms other duties as assignedComplies with all policies and standards
Education/Experience: A Bachelor's degree in a quantitative or business field (e.g., statistics, mathematics, engineering, computer science).Requires 2 – 4 years of related experience.Or equivalent experience acquired through accomplishments of applicable knowledge, duties, scope and skill reflective of the level of this position.Technical Skills One or more of the following skills are desiredExperience developing stored procedures in Snowflake (Programming Language)Experience with SQL (Programming Language)Experience with AWSExperience with Healthcare (specifically HEDIS)Experience with Big Data; Data ProcessingExperience with diagnosing system issues, engaging in data validation, and providing quality assurance testingExperience with Data Manipulation; Data MiningExperience working in a production cloud infrastructureKnowledge of Microsoft SQL Servers
Soft Skills Strong Communication and Organizational skillsQuick learner willing to work with others to understand applications and processes"
Principal Data Engineer,Blueprint,"Bellevue, WA (Remote)",https://www.linkedin.com/jobs/view/3766037114/?eBP=JOB_SEARCH_ORGANIC&refId=jOJ4bUxjdJ4hWHUjgMBOEA%3D%3D&trackingId=C2lqOxt67hfbeiRLpunb%2Fw%3D%3D&trk=flagship3_search_srp_jobs,3766037114,"About the job
            
 
Remote (U.S. Only)Who is Blueprint?We are a technology solutions firm headquartered in Bellevue, Washington, with a strong presence across the United States. Unified by a shared passion for solving complicated problems, our people are our greatest asset. We use technology as a tool to bridge the gap between strategy and execution, powered by the knowledge, skills, and the expertise of our teams, who all have unique perspectives and years of experience across multiple industries. We're bold, smart, agile, and fun.What does Blueprint do?Blueprint helps organizations unlock value from existing assets by leveraging cutting-edge technology to create additional revenue streams and new lines of business. We connect strategy, business solutions, products, and services to transform and grow companies.Why Blueprint?At Blueprint, we believe in the power of possibility and are passionate about bringing it to life. Whether you join our bustling product division, our multifaceted services team or you want to grow your career in human resources, your ability to make an impact is amplified when you join one of our teams. You'll focus on solving unique business problems while gaining hands-on experience with the world's best technology. We believe in unique perspectives and build teams of people with diverse skillsets and backgrounds. At Blueprint, you'll have the opportunity to work with multiple clients and teams, such as data science and product development, all while learning, growing, and developing new solutions. We guarantee you won't find a better place to work and thrive than at Blueprint.What will I be doing?Blueprint is looking for a Principal Data Engineer to join us as we build cutting-edge technology solutions! The ideal candidate will have a solid background in consulting, with demonstrated experience leading clients through the process of building modern data estates. As a Principal Data Engineer, you will spend a majority of your time working directly with clients to develop their advanced modern data estates, warehouses, and analytical environments. You will also be responsible for overseeing and mentoring junior developers within the organization.Responsibilities: Develop and implement effective data architecture solutions using Databricks and LakehouseDevelop Real Time Data Processing using KafkaOptimize and tune data pipelines for performance and scalabilityMonitor and troubleshoot data pipelines to ensure data availability and reliabilityCollaborate with data scientists, analysts, and other stakeholders to understand their data needs and build solutions that enable them to extract insights from dataImplement best practices for data governance, data security, and data quality to ensure data integrity across all data sourcesCreate and maintain documentation related to data architecture, data pipelines, and data modelsStay up to date with emerging technologies and best practices in data engineering and big data processingMentor and train other data engineers on best practices for data engineering and Databricks usageProvide thought leadership in the Databricks and Lakehouse space, both within the organization and externally
Qualifications: Bachelor's or Master's degree in Computer Science, Computer Engineering, or a related field8+ years of experience in data engineering3+ years of experience working with Databricks and PySpark2+ Years of Batch Procesing and Real Time Procesing using Kafka6-8+ years of experience with SQLAppreciation for the Lakehouse medallion data architecture – bronze, silver, gold – and how those data stages are usedKnowledge of ADLS Gen1/Gen2 (Azure Data Lake Stroage)Working knowledge of DLT(Delta Live Tables) and Unity Catalog a plusStrong understanding of ETL and ELT data ingestion, acquisition, and data processing patternsExperience with cloud-based data warehousing platforms such as Synapse, AWS Redshift, Google BigQuery, or SnowflakeKnowledge of Airflow a +Strong understanding of data engineering, data warehousing, data modeling, data governance, and data security best practicesExcellent problem-solving and troubleshooting skillsStrong communication and collaboration skills, with the ability to work effectively in a team environmentExperience mentoring and training other data engineers
Salary RangePay ranges vary based on multiple factors including, without limitation, skill sets, education, responsibilities, experience, and geographical market. The pay range for this position reflects geographic based ranges for Washington state: $146,400 to $175,100 USD/annually. The salary/wage and job title for this opening will be based on the selected candidate's qualifications and experience and may be outside this range.Equal Opportunity EmployerBlueprint Technologies, LLC is an equal employment opportunity employer. Qualified applicants are considered without regard to race, color, age, disability, sex, gender identity or expression, orientation, veteran/military status, religion, national origin, ancestry, marital, or familial status, genetic information, citizenship, or any other status protected by law.If you need assistance or a reasonable accommodation to complete the application process, please reach out to: recruiting@bpcs.comBlueprint believe in the importance of a healthy and happy team, which is why our comprehensive benefits package includes: Medical, dental, and vision coverageFlexible Spending Account401k programCompetitive PTO offeringsParental LeaveOpportunities for professional growth and development
Location: Remote"
Data Engineer,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3733209006/?eBP=JOB_SEARCH_ORGANIC&refId=jOJ4bUxjdJ4hWHUjgMBOEA%3D%3D&trackingId=HX5DUk7NQrV7moxjuGrLlQ%3D%3D&trk=flagship3_search_srp_jobs,3733209006,"About the job
            
 
Location: Remote ( Candidates from Atlanta, GA or neighboring states)Duration: 6 MonthsClient is looking for an engineer who specializes in Databricks and event-driven data collection and streaming services. This engineer will play an essential role by helping our clients build-out robust data pipelines to enable their data science and analytics activities. This position comes with the potential of future career development at a fast-growing data science consultancy.Experience  3~~@~~ years of experience in data engineering, with exposure to Databricks/Delta Lake/Lakehouse architecture and technologies Experience with event-driven data collection and streaming services Experience with pipeline orchestration tools such as Airflow or others
Skills  Strong programming skills in Python and Scala Experience with Apache Spark and SQL Experience with cloud computing platforms, such as AWS, Azure, or GCP Knowledge of data engineering best practices
Additional Qualifications  Databricks Certified Data Engineer Associate or similar professional certification Experience building analytics-oriented data solutions and services
Responsibilities  Design, build, and maintain data pipelines using Databricks Collect and stream data from event-driven sources Orchestrate data pipelines using pipeline orchestration tools Test and monitor data pipelines Collaborate with data scientists and analysts to ensure that data pipelines meet their needs
Education  Master’s degree in computer science, data science, or a related field preferred"
Data Engineer,Mavinsys,United States (Remote),https://www.linkedin.com/jobs/view/3737942766/?eBP=JOB_SEARCH_ORGANIC&refId=jOJ4bUxjdJ4hWHUjgMBOEA%3D%3D&trackingId=vU5mwPPz83ucNsLylJzJiw%3D%3D&trk=flagship3_search_srp_jobs,3737942766,"About the job
            
 
Hello,We are from Mavinsys Talent Acquisition team based on One World Trade center, New York. We are specializing in IT services and staffing majorly in lateral hiring/contract. Below is one of our requirement to fill immediately, if you're interested, please share your candidature to joinus@mavinsys.com Job Title : Data Engineer Location : Remote Duration : 12months   Job Description; 1) Previous experience as a data engineer or in a similar role 2) Technical expertise with data models, data mining, and segmentation techniques 3) Knowledge of programming languages (e.g. Java and Python) 4) Hands-on experience with SQL database design 5) Great numerical and analytical skills 6) Degree in Computer Science, IT, or similar field; a Master's is a plus 7) Data engineering certification (e.g IBM Certified Data Engineer) is a plus

Desired Skills and Experience
                Knowledge of programming languages (e.g. Java and Python),Hands-on experience with SQL database design"
Data Engineer,XO Health Inc.,"Stamford, CT (Remote)",https://www.linkedin.com/jobs/view/3779643937/?eBP=JOB_SEARCH_ORGANIC&refId=jOJ4bUxjdJ4hWHUjgMBOEA%3D%3D&trackingId=EmqxP6AA%2B9zFttCL8pHeQA%3D%3D&trk=flagship3_search_srp_jobs,3779643937,"About the job
            
 
XO Health believes healthcare is fixable. Become part of the community changing the face of the industry.XO Health is the first health plan designed by and for self-insured employers that delivers a more unified health experience for everyone – from those who receive care, to those who deliver it, to those who pay for it.We are growing a multi-disciplinary team of diverse and digitally empowered employees ready to rebuild trust in healthcare through comprehensive and unified transformation.About the Role:The Data Engineer is a member of our Data Engineering team who will focus on data analytics, data engineering and report development to meet business requirements. A successful candidate has experience building healthcare measures, efficiently retrieving data from a data lake, build and troubleshooting data pipelines and developing automated data set processes. The candidate will act as a subject matter expert in implementing the best practices of coding in implementing the projects. In this role, you'll influence technology strategies, ensure that the technological solutions are aligned with the company's business needs and bring to life data and how it can impact positive healthcare outcomes.The ideal candidate should have a good understanding of health plan data to design and build data solutions using data engineering techniques to provide unparalleled experience to our clients, members and providers. The candidate should be passionate about continuously collaborating with other teams to provide optimal analytical insights and reporting frameworks.In This Role, You will: Design & implement big data ingestion & transformation pipelines in order to publish data for consumption. Develop & champion best practices for large scale information extraction from structured/semi-structured data sources. Understand data sources in-depth; design & implement data extraction & processing pipelines that work around imperfections of data to improve data quality & coverage. Deliver high-quality, scalable code with automated test coverage. Drive data quality across the product vertical and related business areas. Support the delivery of high impact dashboards and data visualizations. Define and manage SLAs for all data sets and processes running in production. 
We're Looking for People Who Have: A bachelor's degree in a technical or business discipline, or equivalent experience. 4 years of related data engineering, software engineering and/or business intelligence experience. Minimum of 4 years of experience in creating reports using SAS, SQL languages. 3 years of hands-on experience with big data technologies & event driven architecture. Experience working in building NCQA/HEDIS quality measures to measure quality performance rates is preferred. Ability to develop highly scalable cloud-based data platforms. Preferably using AWS Stack and Snowflake. Hands on data modeling / data architecture experience. 2+ years development experience in at least one object-oriented language (Python, R, Java, etc.). Progressive experience with SQL and related data base technologies. Progressive experience with a variety of data management tools and technologies, and related tools, data visualization and data extraction and transformation tools. Must have strong problem-solving, analytical and in-depth research skills. Possess the ability to communicate effectively with internal and external partners, both orally and written. Experience using DBT is preferred. Experience with PowerBI or Tableau preferred. 
Full compensation packages are based on candidate experience and relevant certifications.$135,000—$140,000 USDXO Health is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. XO Health promotes a drug-free workplace."
Senior/Staff Data Engineer,EvenUp,"Miami, FL (Remote)",https://www.linkedin.com/jobs/view/3728163073/?eBP=JOB_SEARCH_ORGANIC&refId=jOJ4bUxjdJ4hWHUjgMBOEA%3D%3D&trackingId=m7FOtoPA8fEeElcnXQhKHg%3D%3D&trk=flagship3_search_srp_jobs,3728163073,"About the job
            
 
EvenUp is a venture-backed generative AI startup that ensures injury victims are awarded the full value of their claims, expanding the $100B+ in awards granted to injury victims every year. Every year, the legal system has made it difficult for millions of ordinary people to seek justice, especially for folks without means or who come from underrepresented backgrounds. Our vision is to help these injury victims get the justice they deserve, irrespective of their income, demographics, or the quality of their legal representation.EvenUp operates across all types of injury cases, from police brutality and child abuse to California wildfires and motor vehicle accidents. Our ML-driven software empowers attorneys to accurately assess the value of these cases by doing a core part of their workflow (legal drafting), enabling them to secure larger settlements in record time. As EvenUp evaluates more cases, our proprietary data grows, enhancing the precision of our predictions and delivering more value to both attorneys and victims alike.As one of the fastest growing startups ($0 to $10M in ARR in <2 years), we raised $65M in investment from some of the best investors in the world (Bessemer, Bain Capital, Signalfire, DCM, NFX, Tribe Capital), seasoned tech executives (i.e. founder of Quora, SVP at Google, former CPO at Uber), and public figures that care about our social mission (Nas, Jared Leto, Byron Jones). Our team comes from top tech, legal, and investing backgrounds including Waymo, Google, Amazon, Uber, Quora, Blizzard, Norton Rose, Warburg Pincus, Bain, and McKinsey.Why we are hiring a Senior/Staff Data Engineer now? We have experienced unprecedented growth and need to scale out our data warehousing, data tooling and internal analytics. We need to architect the future of our data infrastructure at EvenUp and we’re seeking engineering leaders to help drive that vision. We will need to 10x our pipeline processing throughput over the next 12 months. We’ll need to rethink and rebuild how we extract, process and model our ingestion to enable our organization with precise and actionable data. We need to design & build data warehousing that democratizes data for our entire organization. We will invest in identifying and integrating tools and services that empower our teams to build on top of our data and analytics. 
What you’ll do: Democratize data at EvenUp. Ensure our organization can scale with consistent, standardized access to our data stores and accelerate our ability to build and experiment with data productsArchitect and build out the future of data warehousing at EvenUpEnable and empower our Data Science team to rapidly iterate on model experimentationDesign, organize and refine data storage strategies that reduce development friction for our tech organizationCollaborate with cross functional teams to solve critical data problemsHelp grow our nascent Data Insights team and define a “data first” mentality across our organization
What we are seeking: 8+ years of data engineering experiencePrevious experience building out data warehousing, data pipelines, and internal analyticsStrong understanding and practical experience with data tooling, BI tools, and systems such as DBT, BigQuery, ElasticsearchThe ability to communicate cross-functionally with various stakeholders to derive requirements and architect scalable solutionsHave several years of industry experience building high-quality software, shipping production-ready code and infrastructureYou enjoy owning a project from start to finish and love to drive a project across the finish lineInterest in making the world a fairer place (we don’t get paid unless we’re helping injured victims and/or their attorneys)
Nice to haves: Have previously built out a Data Insights team at a data-oriented startupHave previously planned and architected data migrations at scaleHave stood up analytics tooling to enable cross-functional teamsDomain expertise in legal technology, medical records, and working with unstructured data
A successful first year may look like: 75% doing system design and contributing code, starting with shipping code within 2 weeks!25% collaborating with stakeholders and mentoring, lunch and learns, and moreLeverage a self-starter mindset by taking a product concept and building the feature end to end (whether it’s a component of the system or a significant piece of functionality). Collaborate with the team to scale the tech stack based on our rapidly growing user base!
Benefits & Perks:We seek to empower all of our team members to fulfill our mission of making the world a more just place, regardless of our team’s function, geography, or experience level. To that end, we offer:  Fully remote setup - work from wherever you feel is best (Plus a stipend to upgrade your home office!) Flexible working hours to match your style Offsites - get to meet your coworkers on a fully-expensed trip every 6-12 months! Choice of great medical, dental, and vision insurance plan options Flexible paid time off A variety of virtual team events such as game nights & happy hours
EvenUp is an equal-opportunity employer. We are committed to diversity and inclusion in our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status."
Data Engineer,Therapy Brands,"Birmingham, AL (Remote)",https://www.linkedin.com/jobs/view/3711695981/?eBP=JOB_SEARCH_ORGANIC&refId=jOJ4bUxjdJ4hWHUjgMBOEA%3D%3D&trackingId=1kpwBtLuhX193YFZIW6GwA%3D%3D&trk=flagship3_search_srp_jobs,3711695981,"About the job
            
 
Company DescriptionTherapy Brands is the leading healthcare technology partner for mental, behavioral, and rehabilitative therapy. Our purpose-built and all-in-one practice management, data, and billing solutions drive exceptional clinical and financial outcomes.Thousands of therapy practices rely on us as a trusted partner, to make their lives simpler and more efficient, improve revenue, and enable them to focus on patient care.For more information, explore our solutions at therapybrands.com.Job DescriptionWe are seeking a talented and motivated Data Engineer with a strong background in handling large volumes of data and expertise in Master Data Management (MDM). As a Data Engineer, you will play a critical role in designing, developing, and maintaining our data infrastructure to support our data-driven decision-making processes. You will work with a team of dedicated professionals and utilize your expertise in Azure Data Factory, Databricks, Python, and MDM to optimize data pipelines, ensure data quality and reliability, and manage data repositories.Key Responsibilities: Design, develop, and maintain data pipelines for the ingestion, transformation, and storage of large datasets.Collaborate with analysts, and other stakeholders to understand data requirements and ensure data availability.Implement data integration solutions using Azure Data Factory, Databricks, and Python.Optimize and fine-tune data pipelines for performance, scalability, and cost-efficiency.Implement data security and compliance best practices.Develop and maintain documentation for data engineering processes and pipelines.Design, develop, and maintain Enterprise AnalyticsImplement and maintain Master Data Management (MDM) processes, including the management of master data repositories.Ensure data quality and consistency across the organization by defining and enforcing MDM standards.Monitor and troubleshoot data pipelines and MDM processes to ensure data accuracy and reliability.Stay up-to-date with industry best practices and emerging technologies in data engineering and MDM.
QualificationsQualifications: Proven experience as a Data Engineer, preferably with experience working on large-scale data projects.Strong proficiency in Azure Data Factory and Databricks for data integration and processing.Proficiency in Python for data manipulation and scripting.Familiarity with data warehousing concepts and technologies.Experience with SQL and NoSQL databases.Strong problem-solving skills and attention to detail.Excellent communication and collaboration skills.Experience with Azure Data LakeAbility to work independently and as part of a team.Azure certifications (e.g., Azure Data Engineer) a plus.Experience with Master Data Management (MDM) tools and practices.
Preferred Skills: Experience with data orchestration and scheduling toolsKnowledge of data modeling and ETL (Extract, Transform, Load) processes.Familiarity with big data technologies.Experience with version control systems (e.g., Git).Understanding of data governance and data quality principles.
Additional InformationWhile we've outlined some key qualities we typically seek, it's essential to remember that there might be additional unique strengths and talents you possess that would make you an exceptional match for us, even if they're not explicitly mentioned. Studies have consistently highlighted the significance of this principle, particularly for individuals from disenfranchised backgrounds, including women and other marginalized groups. These individuals often hesitate to apply unless they meet every single requirement, unlike their male counterparts who are more inclined to apply when they meet around 60% of the criteria.The message we want to convey is that taking a leap of faith and applying can be incredibly rewarding. Your distinct abilities and perspectives could be exactly what we need to create a more diverse and inclusive team. So, don't hesitate—apply today and let's explore the exciting possibilities together!All your information will be kept confidential according to EEO guidelines.At Therapy Brands, Diversity, Equity, Inclusion, and Belonging aren’t just words. We celebrate what makes us unique, foster an ecosystem of inclusion for all and harness our talents to promote diversity of thought and action in everything we do.We instill Diversity, Equity, Inclusion, and Belonging into the fabric of our CARING culture and business, as we strive to be recognized not only as the leader in healthcare technology, but also for our intentional efforts to promote a diverse community.We will champion non-discriminatory practices throughout the employee and customer lifecycle; caring for every person regardless of race, national origin, color, religion, disability, sex, orientation, or familial status.Therapy Brands is an equal opportunity employer."
Data Engineer II,Idelic,United States (Remote),https://www.linkedin.com/jobs/view/3729741105/?eBP=JOB_SEARCH_ORGANIC&refId=jOJ4bUxjdJ4hWHUjgMBOEA%3D%3D&trackingId=lWKAMvflrUGPGDNivYnGNw%3D%3D&trk=flagship3_search_srp_jobs,3729741105,"About the job
            
 
About IdelicIdelic is focused on driving the best possible insurance outcomes for the transportation industry through the combination of process and technology. The Idelic program combines the Driver Safety Playbook with Safety Suite®, the first end-to-end driver performance management platform, to help fleets consolidate their most relevant driver data into one platform, analyze it with proprietary machine learning models, and take action on new insights. Empowering fleets with a data-driven view of fleet safety through Idelic’s advanced AI-based Driver Watch List, broad integration network and proven driver professional development plans (PDPs), the Idelic Program is the most proactive and effective way to prevent crashes, reduce losses and produce better insurance outcomes.Overview Of The RoleCollaborate with our product, customer success, and technical teams to research, design, and enhance Idelic's Data Services Platform. Innovate in an industry full of data and experience and create invaluable solutions to our customer’s problems. Your focus will be on developing new features, fixing bugs, and optimizing our Data Services Platform.What You'll Do Build new integrations, features, and support our Data Services PlatformDevelop and maintain efficient and scalable data pipelines and ETL/ELT processes to acquire and transform data from various sources into usable formatsIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etcBuild processes supporting data quality, data transformation, data structures, metadata, dependency, and workload managementOwning existing processes running in production, problem solving and optimizationLead & contribute to the development of tools, infrastructure, and architecture to scale our business and our customersParticipate in internal reviews of code, software components, and systems and make data-driven decisions on how they should evolveCommunicate effectively and participate with team members in an Agile environmentWork on any task and help solve problems when needed
WHAT YOU'LL NEED TO SUCCEED 6+ years of experience as a software developer in an ETL environmentExperience with ETL tools (Informatica, Talend, etc.)Strong proficiency with Python, Google Go (Golang), or similar languageExperience with AWS cloud services: EC2, EMR, RDS, Redshift, Glue, lambda, etcExperience with relational SQL and NoSQL databases such as Postgres or CassandraExperience reviewing code and mentoring less experienced developersStrong quantitative and analytical background & processKnowledge of streaming technologies (Kafka, RabbitMQ)Knowledge of Jenkins, Github and Reporting toolsExperience writing unit, integration, and end-to-end test codeProven ability to work in a collaborative and fast-paced environment
WHAT WILL SET YOU APART Experience in the Logistics / Transportation industryExperience with a variety of data sources (API, CSV, SFTP, RDBMS, etc.)Experience with distributed technologies like KubernetesExperience working in an entrepreneurial or enterprise environment
WHAT MAKES IDELIC A GREAT PLACE TO WORK Competitive Compensation Package Including OptionsUnlimited Paid Time Off (PTO)Medical, Dental, Vision, Disability, and Life Insurance401(k) with Company Matching FundsPaid Parental Leave, Adoption Assistance, and Paid Military LeavePaid Volunteer Time to Support Your Local Community
If you’ve made it this far, we hope you're feeling excited about this role. Even if you don't feel you meet every single requirement, we still encourage you to apply. You may be surprised! We're eager to meet people who are passionate, believe in Idelic’s mission, and can contribute to our team in a variety of ways—not just candidates who check all the boxes.We do not and shall not discriminate on the basis of race, color, religion (creed), gender, gender expression, age, national origin (ancestry), disability, marital status, sexual orientation, or military status in any of its activities or operations. We at Idelic are committed to providing an inclusive and welcoming environment for all members of our team, contractors, vendors, and clients. Idelic is an equal-opportunity employer. Our success depends heavily on the effective utilization of qualified people, regardless of their race, ancestry, religion, color, sex, age, national origin, sexual orientation, gender, identity, disability, veteran status, or any characteristic protected by law.TYPICAL PHYSICAL DEMANDS & WORKING CONDITIONSThe physical demands that are described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. While performing the duties of this job, the employee is regularly required to hear and see. The employee is regularly required to stand and sit/ The employee is regularly required to practice manual dexterity sufficient to operate standard office equipment. Specific vision abilities required by this job include close vision and distant vision. While performing the duties of this job, the employee is exposed to standard office equipment. Occasionally called upon to work hours in excess of your normal daily schedule.The salary range for this position is $86,000 to $100,000. Actual compensation within that range will depend on the candidate's experience, skills, qualifications, and all applicable state laws."
Principal Data Software Engineer,Atlassian,"Austin, TX (Remote)",https://www.linkedin.com/jobs/view/3748161134/?eBP=JOB_SEARCH_ORGANIC&refId=jOJ4bUxjdJ4hWHUjgMBOEA%3D%3D&trackingId=ouIZSVIaXDGhcguE4A9O%2BA%3D%3D&trk=flagship3_search_srp_jobs,3748161134,"About the job
            
 
Working at AtlassianAtlassians can choose where they work - whether in an office, from home, or a combination of the two. That way, Atlassians have more control over supporting their family, personal goals, and other priorities. We can hire people in any country where we have a legal entity. Interviews and onboarding are conducted virtually, a part of being a distributed-first company.  Apply for the Principal Data Software Engineer role at Atlassian and give yourself a chance to join one of our great US Based Product Development Teams working on our Enterprise Agility tools such as Jira Align. The Jira Align Data Team owns multiple production services that transform the team level data into enterprise decision making tools.  Have you heard of the Fortune 500 list? Yup, these are our Enterprise Agility Customers! They are BIG, with development teams reaching 50k users. Our tools allow data analysis of the largest software portfolios in the world. You will end up with your code helping millions of people all around the Globe! You will report into the Senior Engineer Manager within the Jira Align team. In this role: You will work with multiple software engineering teams and product squads to improve Jira AlignYou will refine and govern our logical data models which track team data for the largest software organizations in the world.You will guide designs and decisions for our data architecture through statistical analysis of production dataYou will meet and work with our enterprise scale customers to put the personal touch on our engineering plansWe are also considering multiple database technologies to improve the storage and retrieval for our end-users, including new solutions for data ingestion and translation. This exciting opportunity will engage you with our Site Reliability Engineering and Security teams as well the application development technical leadsYou will work with design and product management team to translate our customer reporting requirements into improvements into our current mechanismsYou will work through our support and field organization to understand how our customers use our data today, to lead future changes as efficiently as possible
This role would be a great fit for an inclusive coach, someone with a willingness to oversee the data architecture from every angle. You will develop and implement solutions that operate at scale - seeing your own technology efforts directly improve the product.  Your background: 15+ years of experience in Software DevelopmentPublic cloud experience in AWS or AzureTechnical leader, able to pitch ideas and lead programs to trigger across multiple teams and departmentsProficiency in SQL, python, Java, C# or another JVM-based languageSuccess with designing and maintenance of systems with multiple dependenciesRelational and NoSQL database design and best practicesPrevious experience in data ingestion and translation toolsExperience managing customer data for Security and PrivacyData modeling and API designExperience in cross-team collaboration in large-scale projects
  Compensation At Atlassian, we strive to design equitable and explainable compensation programs. To support this goal, the baseline of our range is higher than that of the typical market range, but in turn we expect to hire most candidates near this baseline. Base pay within the range is ultimately determined by a candidate's skills, expertise, or experience. In the United States, we have three geographic pay zones. For this role, our current base pay ranges for new hires in each zone are: Zone A: $205,800 - $274,400Zone B: $185,200 - $246,900Zone C: $170,800 - $227,700 This role may also be eligible for benefits, bonuses, commissions, and equity. Please visit go.atlassian.com/payzones for more information on which locations are included in each of our geographic pay zones. However, please confirm the zone for your specific location with your recruiter.   Our perks & benefitsAtlassian offers a variety of perks and benefits to support you, your family and to help you engage with your local community. Our offerings include health coverage, paid volunteer days, wellness resources, and so much more. Visit go.atlassian.com/perksandbenefits to learn more.  About AtlassianAt Atlassian, we're motivated by a common goal: to unleash the potential of every team. Our software products help teams all over the planet and our solutions are designed for all types of work. Team collaboration through our tools makes what may be impossible alone, possible together. We believe that the unique contributions of all Atlassians create our success. To ensure that our products and culture continue to incorporate everyone's perspectives and experience, we never discriminate based on race, religion, national origin, gender identity or expression, sexual orientation, age, or marital, veteran, or disability status. All your information will be kept confidential according to EEO guidelines. To provide you the best experience, we can support with accommodations or adjustments at any stage of the recruitment process. Simply inform our Recruitment team during your conversation with them. To learn more about our culture and hiring process, visit go.atlassian.com/crh."
AWS + Data bricks engineer : Remote,Diverse Lynx,United States (Remote),https://www.linkedin.com/jobs/view/3764421873/?eBP=JOB_SEARCH_ORGANIC&refId=jOJ4bUxjdJ4hWHUjgMBOEA%3D%3D&trackingId=37EUiPmjuJ%2FaCIeWNxxFRw%3D%3D&trk=flagship3_search_srp_jobs,3764421873,"About the job
            
 
Primary skills will be  AWS Native services , AWS Glue , Lambda functions , Step functions  and also on ETL and  Data bricks.Mandatory SkillDataBricks - Data EngineeringAdditional SkillAWS Native application services Diverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company."
Senior Data Engineer,GameChanger,"New York, NY (Remote)",https://www.linkedin.com/jobs/view/3741580803/?eBP=JOB_SEARCH_ORGANIC&refId=jOJ4bUxjdJ4hWHUjgMBOEA%3D%3D&trackingId=XMD2gVcPzwZlp2MWqqkBmA%3D%3D&trk=flagship3_search_srp_jobs,3741580803,"About the job
            
 
About GameChanger:We believe in the life changing impact youth sports have on and off the field. Sports encourage leadership, teamwork, responsibility, and confidence – important life lessons that have the power to propel our youth toward meaningful futures. We recognize that without coaches, parents, and volunteers, organized youth sports could not exist. By building the first and best place to experience the youth sports moments important to our community, we are helping families elevate the next generation through youth sports.So if you love sports and their community building potential, or building cool products is your sport, GameChanger is the team for you. We are a remote first, dynamic tech company based in New York City, and we are solving some of the biggest challenges in youth sports today.The Position:We are looking for a Senior Data Engineer to help evolve our data pipelines, warehouse, and workflow systems to be more resilient, extensible, and maintainable. This role sits on the Core Data Team, which is focused on delivering high-quality data and tooling on a reliable and scalable platform. You’ll work closely with stakeholders across the company to understand their needs and build systems to unlock operational efficiencies or analytic insights. Our solutions benefit everyone including Analysts, Product Managers, Marketers, Developers, Executives, Coaches, Players, and Parents!What You’ll Do: Develop and maintain scalable, robust and efficient data platform serving analytic and operational needs across the organization. Architect and implement high volume, high load data pipelines. Improve existing systems' performance, observability, reliability, and scalability. Collaborate with data producers and consumers across the organization to drive efficiencies for their work. Develop an understanding of our core data model and optimize our data pipelines for scale. Debug and troubleshoot complex problems across multiple systems and services. Ensure system security and data privacy compliance. Share your knowledge through technical documentation, code reviews, and mentoring. 
Who You Are: 5+ years of software development experience, preferably as a data or backend engineer. Expertise in Python for the processing of data. Expertise with SQL and cloud data warehousing in Snowflake or BigQuery. Experience with containers and orchestration tools. Familiarity with cloud computing services such as AWS, Google Cloud, or Azure. Outstanding communication and problem-solving skills. Drive to help others learn and improve themselves as engineers. 
Bonus Points: Experience working with Kafka, Scala, Typescript and Node.js. Experience with IaC tools like Terraform. Experience with dbt and Airflow. Experience with data governance and privacy. 
Perks: Work remotely or from our well-furnished, modern office in Manhattan, NY. Unlimited vacation policy. Technology stipend - $4,000 every 2 years to purchase new technology. WFH stipend - $500 annually to make your WFH situation comfortable. Snack stipend - $60 monthly to have snacks shipped to your home office. Full health benefits - medical, dental, vision, prescription, FSA/HRA., and coverage for family/dependents. Life insurance - basic life, supplemental life, and dependent life. Disability leave - short-term disability and long-term disability. Retirement savings - 401K plan offered through Vanguard, with a company match. Company paid access to a wellness platform to support mental, financial and physical wellbeing. Generous parental leave. DICK’S Sporting Goods Teammate Discount. 
We are an equal opportunity employer and value diversity in our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.The target salary range for this position is between $145,000 and $200,000. This is part of a competitive total rewards package that includes incentive, equity, and benefits for eligible roles. Individual pay may vary from the target range and is determined by a number of factors including experience, internal pay equity, and other relevant business considerations. We review all teammate pay regularly to ensure competitive and fair pay.GameChanger (a subsidiary of DICK’S Sporting Goods) has company-wide practices to monitor and protect the company from significant compliance and monetary implications as it pertains to employer state tax liabilities. Due to said guidelines put in place, we are unable to hire in AK, DE, HI, IA, LA, MS, MT, OK, and SC.Compensation Range: $145K - $200K"
Collibra Data Governance Engineer,IVY TECH SOLUTIONS INC,United States (Remote),https://www.linkedin.com/jobs/view/3667181190/?eBP=JOB_SEARCH_ORGANIC&refId=jOJ4bUxjdJ4hWHUjgMBOEA%3D%3D&trackingId=3TkymrTgTcj9zLwLes2oRQ%3D%3D&trk=flagship3_search_srp_jobs,3667181190,"About the job
            
 
I do need a Collibra Data Governance Engineer. 6 month CTH that is remotePowered by JazzHRf2oKiInTll"
DataStage Data Engineer,IVY TECH SOLUTIONS INC,United States (Remote),https://www.linkedin.com/jobs/view/3667182157/?eBP=JOB_SEARCH_ORGANIC&refId=jOJ4bUxjdJ4hWHUjgMBOEA%3D%3D&trackingId=MITMZkM0Z6O%2FPZB9REfatg%3D%3D&trk=flagship3_search_srp_jobs,3667182157,"About the job
            
 
DataStage Data Engineer  US/GC/Certain EADs – 6 month remote contract. Together we are building a culture that values diversity and creates a space of belonging for all our team members. We believe that investing in your success is an investment in our customers and our business. Our people are what sets us apart and make us great. As a Data Engineer, you’ll provide your talents in contributing to the success of the client’s team by delivering the following:  Serve in the goalie rotation to support the Production environment. Responsible for maintaining enterprise-grade platforms that enable data-driven solutions. Search for ways to automate and maintain scalable infrastructure. Ensure delivery of highly available and scalable systems. Monitor all systems and applications and ensure optimal performance. Analyzes and designs technical solutions to address production problems. Participate in troubleshooting applications and systems issues. Identifies, investigates, and proposes solutions to technical problems. While providing technical support for issues, develop, test, and modify software to improve efficiency of data platforms and applications. Monitors system performance to maintain consistent up time. Prepares and maintains necessary documentation. Participate in daily standups, team backlog grooming, and iteration retrospectives. Coordinate with data operations teams to deploy changes into production. Highest level may function as a lead. Other duties as assigned.
Qualifications:  Requires a Bachelor's in Computer Science, Computer Engineering or related field and experience with ETL development, SQL, UNIX/Linux scripting, Big Data distributed systems. Prefer experience with IBM DataStage. Various programming languages like Java and Python, orchestration tools and processes or other directly related experience. A combination of education and experience may meet qualifications. Excellent analytical, organizational, and problem-solving skills. Ability and desire to learn new technologies quickly. Ability to work independently and collaborate with others at all levels of technical understanding. Able to meet deadlines. Good judgment and project management skills. Ability to communicate both verbally and in writing with both technical and non-technical staff. Ability to work in a team environment and have good interpersonal skills. Ability to adapt to changing technology and priorities. Must be able to work independently, handle multiple concurrent tasks, with an ability to prioritize and manage tasks effectively.
Powered by JazzHRodgGvyPQIz"
TEST DATA ENGINEER-REMOTE-EST or CST only,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3661805618/?eBP=JOB_SEARCH_ORGANIC&refId=F%2F8e1MFOoG6d9fc37hW0nw%3D%3D&trackingId=5DLHr0FsK7TryI%2BjRghBUg%3D%3D&trk=flagship3_search_srp_jobs,3661805618,"About the job
            
 
GenRocket is MUST have. Additionally, I would like to see minimum of 1+ yr. of implementation experience.  Must required: GenRocket, scripting, PostGres/DB ability, API work Determine best practice(s) around synthetic, masked data in DCE/Platform Analyze and understand how data flows in data hub Determine best way to manage and create synthetic data in Data Hub; deliver solution Figure out a solution to mock massive accounts coming through APIs Understanding data models, table structures and dependencies. Guide teams in generating new synthetic data, creating more, and maintaining over time. Create PoC for syntheitc data use cases working with Product team Agile Mindset, familiarity with Azure Cloud ADO experience, python or powershell scripting experience
Must required: GenRocket, scripting, PostGres/DB ability, API workDetermine best practice(s) around synthetic, masked data in DCE/PlatformAnalyze and understand how data flows in data hubDetermine best way to manage and create synthetic data in Data Hub; deliver solutionFigure out a solution to mock massive accounts coming through APIsUnderstanding data models, table structures and dependencies.Guide teams in generating new synthetic data, creating more, and maintaining over time.Create PoC for syntheitc data use cases working with Product teamAgile Mindset, familiarity with Azure CloudADO experience, python or powershell scripting experience"
Senior/Staff Data Engineer,EvenUp,"San Francisco, CA (Remote)",https://www.linkedin.com/jobs/view/3728158599/?eBP=JOB_SEARCH_ORGANIC&refId=F%2F8e1MFOoG6d9fc37hW0nw%3D%3D&trackingId=V9HVEpPP3QU2b0X%2F4PS6fA%3D%3D&trk=flagship3_search_srp_jobs,3728158599,"About the job
            
 
EvenUp is a venture-backed generative AI startup that ensures injury victims are awarded the full value of their claims, expanding the $100B+ in awards granted to injury victims every year. Every year, the legal system has made it difficult for millions of ordinary people to seek justice, especially for folks without means or who come from underrepresented backgrounds. Our vision is to help these injury victims get the justice they deserve, irrespective of their income, demographics, or the quality of their legal representation.EvenUp operates across all types of injury cases, from police brutality and child abuse to California wildfires and motor vehicle accidents. Our ML-driven software empowers attorneys to accurately assess the value of these cases by doing a core part of their workflow (legal drafting), enabling them to secure larger settlements in record time. As EvenUp evaluates more cases, our proprietary data grows, enhancing the precision of our predictions and delivering more value to both attorneys and victims alike.As one of the fastest growing startups ($0 to $10M in ARR in <2 years), we raised $65M in investment from some of the best investors in the world (Bessemer, Bain Capital, Signalfire, DCM, NFX, Tribe Capital), seasoned tech executives (i.e. founder of Quora, SVP at Google, former CPO at Uber), and public figures that care about our social mission (Nas, Jared Leto, Byron Jones). Our team comes from top tech, legal, and investing backgrounds including Waymo, Google, Amazon, Uber, Quora, Blizzard, Norton Rose, Warburg Pincus, Bain, and McKinsey.Why we are hiring a Senior/Staff Data Engineer now? We have experienced unprecedented growth and need to scale out our data warehousing, data tooling and internal analytics. We need to architect the future of our data infrastructure at EvenUp and we’re seeking engineering leaders to help drive that vision. We will need to 10x our pipeline processing throughput over the next 12 months. We’ll need to rethink and rebuild how we extract, process and model our ingestion to enable our organization with precise and actionable data. We need to design & build data warehousing that democratizes data for our entire organization. We will invest in identifying and integrating tools and services that empower our teams to build on top of our data and analytics. 
What you’ll do: Democratize data at EvenUp. Ensure our organization can scale with consistent, standardized access to our data stores and accelerate our ability to build and experiment with data productsArchitect and build out the future of data warehousing at EvenUpEnable and empower our Data Science team to rapidly iterate on model experimentationDesign, organize and refine data storage strategies that reduce development friction for our tech organizationCollaborate with cross functional teams to solve critical data problemsHelp grow our nascent Data Insights team and define a “data first” mentality across our organization
What we are seeking: 8+ years of data engineering experiencePrevious experience building out data warehousing, data pipelines, and internal analyticsStrong understanding and practical experience with data tooling, BI tools, and systems such as DBT, BigQuery, ElasticsearchThe ability to communicate cross-functionally with various stakeholders to derive requirements and architect scalable solutionsHave several years of industry experience building high-quality software, shipping production-ready code and infrastructureYou enjoy owning a project from start to finish and love to drive a project across the finish lineInterest in making the world a fairer place (we don’t get paid unless we’re helping injured victims and/or their attorneys)
Nice to haves: Have previously built out a Data Insights team at a data-oriented startupHave previously planned and architected data migrations at scaleHave stood up analytics tooling to enable cross-functional teamsDomain expertise in legal technology, medical records, and working with unstructured data
A successful first year may look like: 75% doing system design and contributing code, starting with shipping code within 2 weeks!25% collaborating with stakeholders and mentoring, lunch and learns, and moreLeverage a self-starter mindset by taking a product concept and building the feature end to end (whether it’s a component of the system or a significant piece of functionality). Collaborate with the team to scale the tech stack based on our rapidly growing user base!
Benefits & Perks:We seek to empower all of our team members to fulfill our mission of making the world a more just place, regardless of our team’s function, geography, or experience level. To that end, we offer:  Fully remote setup - work from wherever you feel is best (Plus a stipend to upgrade your home office!) Flexible working hours to match your style Offsites - get to meet your coworkers on a fully-expensed trip every 6-12 months! Choice of great medical, dental, and vision insurance plan options Flexible paid time off A variety of virtual team events such as game nights & happy hours
EvenUp is an equal-opportunity employer. We are committed to diversity and inclusion in our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status."
Staff Data Engineer,theSkimm,"New York, NY (Remote)",https://www.linkedin.com/jobs/view/3657168160/?eBP=JOB_SEARCH_ORGANIC&refId=F%2F8e1MFOoG6d9fc37hW0nw%3D%3D&trackingId=vvNKJMV3X12nFhXlJyPVfQ%3D%3D&trk=flagship3_search_srp_jobs,3657168160,"About the job
            
 
theSkimm'We're hiring a Staff Data Engineer.About Our Team And What We'll Build TogetherWe're looking for an experienced Staff Data Engineer and mentor to join theSkimm's Tech team. Our mission is to enable our partners across theSkimm, from Editorial and Audio to Marketing and Advertising, to achieve their goals with the best systems and processes we can offer. We build tools throughout the stack, share knowledge across departments, and learn quickly so we can take best advantage of what's coming.As we grow, we're looking for a Staff Data Engineer who will help solidify and expand our pipelines and maintain our data warehouse. Our business is run on detailed analysis of how our products perform with our members, which features they love and which can be improved, and which product and marketing campaigns are bringing in the best quality users. You'll be responsible for working alongside analysts, product, marketing, and other teams to define new data collection components and measurement schemes that support each new product and feature.How You'll Contribute To Our Mission Create and maintain data pipelines to provide insights and drive business decisionsEstablish theSkimm's data warehousing strategy (ex. Kimball, Data Vault, etc.)Maintain theSkimm's data infrastructure on our AWS accountsCollaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization. Write unit/integration tests, contributes to engineering wiki, and documents workImplement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it
You're ready for this! Here's a bit more about what we're looking for 7+ years of industry experience measuring product performance and user behaviorExperience working with a variety of data management technologies, including RedShift, Kinesis/Kafka, Glue, Spark, Postgres, Airflow, dbt, and others. Experience implementing BI reporting tools such as LookerExperience interfacing with engineers, product managers and analysts to understand data needsKnowledge of a variety of measurement beacons, SDKs, APIs including Google Analytics, Amplitude, Braze, Stripe, email service providersA commitment to building secure, resilient, fault-tolerant architectures, with clear documentation and procedures in place for supportA focus on accuracy and detail as you build, ensuring data pipelines produce clear, predictable resultsAbility to thrive in a dynamic, fast-paced, collaborative, and high-growth environmentFacility in presenting and discussing the trade-offs in employing different engineering solutions to a problem, valuing pragmatism over idealismAn empathetic leadership style that encourages open communication and trustUnderstanding of the typical metrics a subscription and advertising-supported business needs to measure successExperience with ML techniques as applied to behavioral segmentation or anomaly identification is a plusFamiliarity using developer tools that increase productivity and facilitate the development of resilient code (eg. Docker, CircleCI, Serverless) is a plusFamiliarity and enthusiasm for theSkimm: a passion for our audience and mission is a plus
The expected annual base salary for this role is $145,000-$180,000. We'll consider a variety of factors when determining the offered base salary including an evaluation of a candidate's skills, abilities, experience, location, market demands, and internal parity.Why our employees love working hereWe are mission driven and values driven:We are collectively building an impactful brand to empower generations of informed, confident women. Our vision is for every woman to have the info they need to navigate a complex world and the decisions in it. Every role has a purpose and allows us to provide useful information succinctly and contextualized. We tee her up to take action - whichever action is right for her, wherever she is.We are values driven at every point - how we work, the way we work and our culture come straight from our core values. Each of us supports this incredible brand and takes part in its continued growth, fulfilling the needs of our audience each and every day.We put you and your personal lives first - we have a generous benefits policy that demonstrates we are listening to what our employees need: Unlimited vacation policy and generous holiday observancesEncouraged time off on your birthday and Skimm'versary A hybrid working model with flexibility for remote workComprehensive insurance plans and commuter benefitsAuto-enrollment into an Empower 401(k) plan starting on your first day and employer contribution available after one year of employment Rewards for every work anniversaryOne month paid sabbatical after your five year Skimm'versary as a full-time employee
We support our parents at all points on their journey:  18 weeks of paid parental leave (adoption, fostering and surrogacy included)Bereavement leave for pregnancy lossPhased return to work schedule availabilityFlexibility based on parental schedulesAccess to family building and fertility benefitsFlexible and broad-scale child support program
Your well being is our priority:  We honor Sacred Time in our workplaceWe have aligned company ""time off"" during the summer to truly allow for us all to break from our work at the same timeWe offer fitness membership reimbursementOne Medical Membership is included with our benefits
We have a vibrant, collaborative, and supportive culture (we celebrate and have fun!): Weekly company updates led by our executive teamEmployee Resource GroupsAnnual employee award ceremony to celebrate individual accomplishmentsClubs and activities designed to meet other employees like book clubs, new hire buddy programs, off-sites, and wellness focused classesNumerous culture events that enable our workforce, in the office and remote, to connect and have fun
Your career and development are a priority:  Annual learning and development stipendLEAD@theSkimm, our leadership development programDEI initiatives to ensure we are the best partners and colleagues to each otherCareer development guidance and planning
And more… Competitive salary and equity packagesThe opportunity to be part of a values-driven, hardworking, and diverse group of people building a media company that makes it easier to live smarter
Our story, Skimm'dWe are a digital media company, dedicated to succinctly giving women the information they need to make confident decisions. We make it easier to live smarter.At our core, we are writers, editors, producers, designers, marketers, engineers, analysts, sellers, creatives, and strategists all working together to achieve this goal.Every day we're breaking down the news, trends, policies, and politics that impact women so that they can navigate their daily lives and futures – from managing their paychecks to casting their ballots – with confidence. We provide our dedicated audience of millions with reliable, non-partisan, information, informing and empowering them while fitting into their daily routines.Since disrupting the media landscape and defining a new category a decade ago, we have become a trusted source for our audience of millions by seamlessly integrating into their existing routines, fundamentally changing the way they consume news and make decisions. Today theSkimm ecosystem includes the Daily Skimm, the Daily Skimm: Weekend, Skimm Money and Skimm Your Life newsletters, B2B marketer's newsletter The SKM Report, ""9 to 5ish with theSkimm"" podcast, theSkimm mobile app. We also house Skimm Studios which creates innovative in-house video and audio content, and our in-house creative agency SKM Lab, which conceptualizes, develops, and produces innovative solutions and content for brands to engage with generations of informed women. Our first book, How to Skimm Your Life debuted at #1 on The New York Times Best Seller list. Through Skimm Impact, our purpose-driven platform, we are proud to support get-out-the-vote efforts with Skimm Your Ballot, which has spurred more than two million voting-related actions across the last four election cycles. We have mobilized hundreds of companies to join our #ShowUsYourLeave movement, which has created transparency and pushed for progress for Paid Family Leave in the U.S. And we're empowering women to take agency of their lives and control of their futures through our State of Women initiative, grounded in a study conducted by The Harris Poll.Come join us!"
Collibra Data Governance Engineer,IVY TECH SOLUTIONS INC,United States (Remote),https://www.linkedin.com/jobs/view/3686019980/?eBP=JOB_SEARCH_ORGANIC&refId=F%2F8e1MFOoG6d9fc37hW0nw%3D%3D&trackingId=XrJv6YqpppFuKADK8tZSbQ%3D%3D&trk=flagship3_search_srp_jobs,3686019980,"About the job
            
 
HI,Kindly let me know if you have a suitable fit for the following positionThanks..PositionCollibra Data Governance EngineerRemote PoistionPlease send the resume to sowmya.g@ivytechsol.us OR CALL: 2243488595Job DescriptionI do need a Collibra Data Governance Engineer. 6 month CTH that is remotePowered by JazzHR81Z7dtGAIl"
Principal Data Engineer (Azure),Tiger Analytics,"Jersey City, NJ (Remote)",https://www.linkedin.com/jobs/view/3739548031/?eBP=JOB_SEARCH_ORGANIC&refId=F%2F8e1MFOoG6d9fc37hW0nw%3D%3D&trackingId=LlyWuagRMwb2LrSHn2ph7Q%3D%3D&trk=flagship3_search_srp_jobs,3739548031,"About the job
            
 
Tiger Analytics is a global AI and analytics consulting firm. With data and technology at the core of our solutions, we are solving problems that eventually impact the lives of millions globally. Our culture is modeled around expertise and respect with a team-first mindset. Headquartered in Silicon Valley, you’ll find our delivery centers across the globe and offices in multiple cities across India, the US, UK, Canada, and Singapore, including asubstantial remote global workforce.We’re Great Place to Work-Certified™. Working at Tiger Analytics, you’ll be at the heart of an AI revolution. You’ll work with teams that push the boundaries of what is possible and build solutions that energize and inspire.RequirementsCurious about the role? What your typical day would look like?As a Principal Data Engineer (Azure), you would have hands on experience working on Azure as cloud, Databricks and some exposure/experience on Data Modelling. You will build and learn about a variety of analytics solutions & platforms, data lakes, modern data platforms, data fabric solutions, etc. using different Open Source, Big Data, and Cloud technologies on Microsoft Azure. Design and build scalable & metadata-driven data ingestion pipelines (For Batch and Streaming Datasets)Conceptualize and execute high-performance data processing for structured and unstructured data, and data
harmonization Schedule, orchestrate, and validate pipelinesDesign exception handling and log monitoring for debuggingIdeate with your peers to make tech stack and tools-related decisionsInteract and collaborate with multiple teams (Consulting/Data Science & App Dev) and various stakeholders to meet deadlines, to bring Analytical Solutions to life
What do we expect? Experience in implementing Data Lake with technologies like Azure Data Factory (ADF), PySpark, Databricks, ADLS,
Azure SQL Database A comprehensive foundation with working knowledge of Azure Synapse Analytics, Event Hub & Streaming
Analytics, Cosmos DB, and Purview A passion for writing high-quality code and the code should be modular, scalable, and free of bugs (debugging
skills in SQL, Python, or Scala/Java). Enthuse to collaborate with various stakeholders across the organization and take complete ownership of
deliverables. Experience in using big data technologies like Hadoop, Spark, Airflow, NiFi, Kafka, Hive, Neo4J, Elastic SearchAdept understanding of different file formats like Delta Lake, Avro, Parquet, JSON, and CSVGood knowledge of building and designing REST APIs with real-time experience working on Data Lake or
Lakehouse projects. Experience in supporting BI and Data Science teams in consuming the data in a secure and governed mannerCertifications like Data Engineering on Microsoft Azure (DP-203) or Databricks Certified Developer (DE) are
valuable addition.Note: The designation will be commensurate with expertise and experience. Compensation packages are among the best in the industry.Job Requirement Mandatory: Azure Data Factory (ADF), PySpark, Databricks, ADLS, Azure SQL DatabaseOptional: Azure Synapse Analytics, Event Hub & Streaming Analytics, Cosmos DB and PurviewStrong programming, unit testing & debugging skills in SQL, Python or Scala/JavaSome experience of using big data technologies like Hadoop, Spark, Airflow, NiFi, Kafka, Hive, Neo4J, Elastic SearchGood Understanding of different file formats like Delta Lake, Avro, Parquet, JSON and CSVExperience of working in Agile projects and following DevOps processes with technologies like Git, Jenkins & Azure DevOpsGood to have:Experience of working on Data Lake & Lakehouse projectsExperience of building REST services and implementing service-oriented architecturesExperience of supporting BI and Data Science teams in consuming the data in a secure and governed mannerCertifications like Data Engineering on Microsoft Azure (DP-203) or Databricks Certified Developer (DE)
BenefitsThis position offers an excellent opportunity for significant career development in a fast-growing and challenging entrepreneurial environment with a high degree of individual responsibility."
Urgent Role - Business Systems Analyst /Data Engineer (AZURE) || Remote,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3710624819/?eBP=JOB_SEARCH_ORGANIC&refId=F%2F8e1MFOoG6d9fc37hW0nw%3D%3D&trackingId=25b8yH6nmqy5aA57yjBfwQ%3D%3D&trk=flagship3_search_srp_jobs,3710624819,"About the job
            
 
Job DescriptionTitle: Business Systems Analyst /Data Engineer (AZURE)Location: Irving Texas (Remote)Duration: 6+ Months CTHPosition DetailsMust Have Excellent Communication SkillsMust have valid LinkedInNeed DL or Visa CopSkills: 3 main requirements are SQL, Azure Data Factory, Azure(very heavy Azure role), SQL stored procedures. Functional requirement documents, Must have experience creating User stories.Will be working with 2 Data Engineers Ideal candidate will have experience as a Data Engineer Azure space.Gathering requirements from stakeholders in business units-creating data and requirements gathering- Azure environmentHeavy data focused and heavy integration focused. Must work in an Agile environmentMust write BRD'sMust write FRD'sMust have experience in creating user storiesMust have experience in data mapping and workflow diagrams.Must have experience with SQL Queries/Joins/Stored procedures.Must have heavy requirements gathering experience. Must have experience building pipelines
Work closely with stakeholders, customer experience, and product management to identify and document business problems and will lead efforts to gather business, functional, and non-functional requirements in the form of features and/or user stories. This role is a vital link between the scrum teams and business counterparts and will work in close partnership with the team's scrum master and product owner to refine and prioritize the scrum team's backlog.Responsibilities Function as the liaison between the application development (scrum team), product owners, and customers (internal and/or external).Act as an information source and authority on business for the team, especially in the absence of the Product Owner.Convert roadmap features into smaller user stories.Write clear and well-structured business requirements/documents.Produce workflow diagrams, data mapping documents, and other relevant supporting documentation.Communicate and validate requirements with relevant stakeholders.Collaborate with product owner on roadmap planning and prioritization.Identify specific business opportunities.Test business processes and recommend improvements.Identify automation opportunities.Perform user acceptance testing.Develop applicable user documentation in support of training."
Principal Data Engineer,Blueprint,"Seattle, WA (Remote)",https://www.linkedin.com/jobs/view/3716423540/?eBP=JOB_SEARCH_ORGANIC&refId=F%2F8e1MFOoG6d9fc37hW0nw%3D%3D&trackingId=7G1HzWpedi23UDjZ00nZLQ%3D%3D&trk=flagship3_search_srp_jobs,3716423540,"About the job
            
 
Principal Date Engineer RemoteWho is Blueprint?We are a technology solutions firm headquartered in Bellevue, Washington, with a strong presence across the United States. Unified by a shared passion for solving complicated problems, our people are our greatest asset. We use technology as a tool to bridge the gap between strategy and execution, powered by the knowledge, skills, and the expertise of our teams, who all have unique perspectives and years of experience across multiple industries. We're bold, smart, agile, and fun.What does Blueprint do?Blueprint helps organizations unlock value from existing assets by leveraging cutting-edge technology to create additional revenue streams and new lines of business. We connect strategy, business solutions, products, and services to transform and grow companies.Why Blueprint?At Blueprint, we believe in the power of possibility and are passionate about bringing it to life. Whether you join our bustling product division, our multifaceted services team or you want to grow your career in human resources, your ability to make an impact is amplified when you join one of our teams. You'll focus on solving unique business problems while gaining hands-on experience with the world's best technology. We believe in unique perspectives and build teams of people with diverse skillsets and backgrounds. At Blueprint, you'll have the opportunity to work with multiple clients and teams, such as data science and product development, all while learning, growing, and developing new solutions. We guarantee you won't find a better place to work and thrive than at Blueprint.What will I be doing?Blueprint is looking for a Principal Data Engineer to join us as we build cutting-edge technology solutions! The ideal candidate will have a solid background in consulting, with demonstrated experience leading clients through the process of building modern data estates. As a Principal Data Engineer, you will spend a majority of your time working directly with clients to develop their advanced modern data estates, warehouses, and analytical environments. You will also be responsible for overseeing and mentoring junior developers within the organization.Responsibilities: Develop and implement effective data architecture solutions using Databricks and LakehouseOptimize and tune data pipelines for performance and scalabilityMonitor and troubleshoot data pipelines to ensure data availability and reliabilityCollaborate with data scientists, analysts, and other stakeholders to understand their data needs and build solutions that enable them to extract insights from dataImplement best practices for data governance, data security, and data quality to ensure data integrity across all data sourcesCreate and maintain documentation related to data architecture, data pipelines, and data modelsStay up to date with emerging technologies and best practices in data engineering and big data processingMentor and train other data engineers on best practices for data engineering and Databricks usageProvide thought leadership in the Databricks and Lakehouse space, both within the organization and externally
Qualifications: Bachelor's or Master's degree in Computer Science, Computer Engineering, or a related field8+ years of experience in data engineering3+ years of experience working with Databricks and PySpark6-8+ years of experience with SQLAppreciation for the Lakehouse medallion data architecture – bronze, silver, gold – and how those data stages are usedWorking knowledge of DLT(Delta Live Tables) and Unity Catalog a plusStrong understanding of ETL and ELT data ingestion, acquisition, and data processing patternsExperience with cloud-based data warehousing platforms such as Synapse, AWS Redshift, Google BigQuery, or SnowflakeStrong understanding of data engineering, data warehousing, data modeling, data governance, and data security best practicesExcellent problem-solving and troubleshooting skillsStrong communication and collaboration skills, with the ability to work effectively in a team environmentExperience mentoring and training other data engineers
Salary RangePay ranges vary based on multiple factors including, without limitation, skill sets, education, responsibilities, experience, and geographical market. The pay range for this position reflects geographic based ranges for Washington state: $127,000 to $211,600 USD/annually. The salary/wage and job title for this opening will be based on the selected candidate's qualifications and experience and may be outside this range.Equal Opportunity EmployerBlueprint Technologies, LLC is an equal employment opportunity employer. Qualified applicants are considered without regard to race, color, age, disability, sex, gender identity or expression, orientation, veteran/military status, religion, national origin, ancestry, marital, or familial status, genetic information, citizenship, or any other status protected by law.If you need assistance or a reasonable accommodation to complete the application process, please reach out to: recruiting@bpcs.comBlueprint believe in the importance of a healthy and happy team, which is why our comprehensive benefits package includes: Medical, dental, and vision coverageFlexible Spending Account401k programCompetitive PTO offeringsParental LeaveOpportunities for professional growth and development
Location: Remote"
Remote work - Need GCP certified Data Engineer,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3691192389/?eBP=JOB_SEARCH_ORGANIC&refId=F%2F8e1MFOoG6d9fc37hW0nw%3D%3D&trackingId=Z6of%2BZFhcARbUXRYwiNSRg%3D%3D&trk=flagship3_search_srp_jobs,3691192389,"About the job
            
 
Must have LinkedInNeed strong experience need senior consultants DBT experience needs to be strong as wellTitle : GCP Data Engineer Location : Remote ( need to work in Est timezone ) Duration : till end of this year and will have more extensions GCP ACTIVE CERTIFICAION IS MUST HAVEWhat You'll Do Design, develop, and optimize data pipelines, architectures, and data sets in Google Cloud using tools such as BigQuery and DBT.Use SQL and DBT for data transformation and manipulation, developing complex SQL queries and implementing optimization techniques for improved performance.Utilize your expertise in Python to automate data pipeline processes, ensuring accuracy and efficiency.Collaborate with our team of data analysts and data scientists, providing them with clean, reliable data for their analytical work.Ensure data governance and security practices are applied across all data initiatives.Conduct regular reviews and tests of our data systems to ensure data integrity and quality.Stay up-to-date with industry trends and innovations, continuously improving and expanding your knowledge and skills.
What You Have Google Cloud Certification (Professional Data Engineer)Deep expertise in DBT (Data Build Tool) for defining, testing, and documenting data transformations in BigQuery.Proficiency in creating and maintaining DBT models, understanding the use of macros, sources, and snapshots.Understanding of advanced DBT concepts like incremental models, custom schemas, and the use of variables.Experience with DBT Cloud, including setting up deployments, automated tests, and version control.Outstanding proficiency in SQL, with the ability to write complex, efficient queries for data extraction and transformation.Extensive experience with SQL optimization techniques, aiming to improve query performance and database scalability.Knowledge of advanced SQL concepts like window functions, common table expressions (CTEs), and stored procedures.Extensive experience in Google Cloud, including hands-on experience with Google Cloud data services such as BigQuery, Pub/Sub, Dataflow, and Dataproc.Proven ability to design and implement Google Cloud-based data architectures, adhering to best practices for security, performance, and reliability.Solid understanding of data warehousing concepts and data modeling principles.Proficient in Python for data automation tasks.Understanding of data governance and security practices.Google Cloud Certification - Professional Data EngineerStrong communication skills, with the ability to explain complex concepts to non-technical stakeholders.Excellent problem-solving skills and attention to detail."
Data Engineer,Arrow Search Partners,"New York, NY (Remote)",https://www.linkedin.com/jobs/view/3781532035/?eBP=JOB_SEARCH_ORGANIC&refId=F%2F8e1MFOoG6d9fc37hW0nw%3D%3D&trackingId=%2FZYBtosqS4J4coB5E%2BF%2FOQ%3D%3D&trk=flagship3_search_srp_jobs,3781532035,"About the job
            
 
About The CompanyOur client is a private equity real estate firm and is hiring a Data Engineer who can sit fully remote.Responsibilities Develop, construct, test and maintain architecturesDevelop data set processesBuild data systems and pipelinesBuild algorithms and prototypesEvaluate business needs and objectivesData acquisition and ingestion – Identify data sources and build pipelines using various ETL tools such as but not limited to, SSIS, and AlteryxAssist in defining the data architecture framework, standards and principles, including modeling, metadata, security and reference dataCreate and optimize data models to support various business applicationsAutomate and support workflows to ensure timely delivery
Requirements 5+ years data engineering experienceProficient programming capability, Python preferredExperience designing data models and data warehouses and using SQL database management systems along with data processing using traditional and distributed systemsExperience with data modeling, data warehousing, and building ETL pipelinesExperience working within financial services industry and/or familiarity of different asset classes is preferred
Salary Range$150,000-$250,000"
REMOTE:  Data Engineer Scala Spark,"Conch Technologies, Inc",United States (Remote),https://www.linkedin.com/jobs/view/3766017679/?eBP=JOB_SEARCH_ORGANIC&refId=F%2F8e1MFOoG6d9fc37hW0nw%3D%3D&trackingId=t30WpW6LhAeCFXsAxHBySQ%3D%3D&trk=flagship3_search_srp_jobs,3766017679,"About the job
            
 
Hi,Greetings from Conch Technologies IncWe have a few openings for our client in NC. The roles are remote.Data Engineer (scala/spark and AWS).Remote and long-term.Must work est time.Data Engineer Requirements  3+ years of experience building scalable data pipelines with Scala and Spark Strong Scala programming skills and knowledge of functional programming Experience with Spark Scala, DataFrames, Datasets, and Hadoop Filesystem Knowledge of AWS services like EMR, S3, OpenSearch etc. Familiarity with CI/CD best practices and experience with Jenkins, Git, and Azure DevOps Ability to optimize Spark jobs for performance and cost efficiency Experience supporting production data pipelines and ETL processes Excellent communication skills and ability to quickly ramp up on our tech stack
--With Regards,Teja MaripetiDesk: 901-317-3455Email: teja@conchtech.comWeb: www.conchtech.com"
Senior/Staff Data Engineer,EvenUp,"Orlando, FL (Remote)",https://www.linkedin.com/jobs/view/3728157943/?eBP=JOB_SEARCH_ORGANIC&refId=F%2F8e1MFOoG6d9fc37hW0nw%3D%3D&trackingId=zZ1u9psOapK77NgBJNm9yg%3D%3D&trk=flagship3_search_srp_jobs,3728157943,"About the job
            
 
EvenUp is a venture-backed generative AI startup that ensures injury victims are awarded the full value of their claims, expanding the $100B+ in awards granted to injury victims every year. Every year, the legal system has made it difficult for millions of ordinary people to seek justice, especially for folks without means or who come from underrepresented backgrounds. Our vision is to help these injury victims get the justice they deserve, irrespective of their income, demographics, or the quality of their legal representation.EvenUp operates across all types of injury cases, from police brutality and child abuse to California wildfires and motor vehicle accidents. Our ML-driven software empowers attorneys to accurately assess the value of these cases by doing a core part of their workflow (legal drafting), enabling them to secure larger settlements in record time. As EvenUp evaluates more cases, our proprietary data grows, enhancing the precision of our predictions and delivering more value to both attorneys and victims alike.As one of the fastest growing startups ($0 to $10M in ARR in <2 years), we raised $65M in investment from some of the best investors in the world (Bessemer, Bain Capital, Signalfire, DCM, NFX, Tribe Capital), seasoned tech executives (i.e. founder of Quora, SVP at Google, former CPO at Uber), and public figures that care about our social mission (Nas, Jared Leto, Byron Jones). Our team comes from top tech, legal, and investing backgrounds including Waymo, Google, Amazon, Uber, Quora, Blizzard, Norton Rose, Warburg Pincus, Bain, and McKinsey.Why we are hiring a Senior/Staff Data Engineer now? We have experienced unprecedented growth and need to scale out our data warehousing, data tooling and internal analytics. We need to architect the future of our data infrastructure at EvenUp and we’re seeking engineering leaders to help drive that vision. We will need to 10x our pipeline processing throughput over the next 12 months. We’ll need to rethink and rebuild how we extract, process and model our ingestion to enable our organization with precise and actionable data. We need to design & build data warehousing that democratizes data for our entire organization. We will invest in identifying and integrating tools and services that empower our teams to build on top of our data and analytics. 
What you’ll do: Democratize data at EvenUp. Ensure our organization can scale with consistent, standardized access to our data stores and accelerate our ability to build and experiment with data productsArchitect and build out the future of data warehousing at EvenUpEnable and empower our Data Science team to rapidly iterate on model experimentationDesign, organize and refine data storage strategies that reduce development friction for our tech organizationCollaborate with cross functional teams to solve critical data problemsHelp grow our nascent Data Insights team and define a “data first” mentality across our organization
What we are seeking: 8+ years of data engineering experiencePrevious experience building out data warehousing, data pipelines, and internal analyticsStrong understanding and practical experience with data tooling, BI tools, and systems such as DBT, BigQuery, ElasticsearchThe ability to communicate cross-functionally with various stakeholders to derive requirements and architect scalable solutionsHave several years of industry experience building high-quality software, shipping production-ready code and infrastructureYou enjoy owning a project from start to finish and love to drive a project across the finish lineInterest in making the world a fairer place (we don’t get paid unless we’re helping injured victims and/or their attorneys)
Nice to haves: Have previously built out a Data Insights team at a data-oriented startupHave previously planned and architected data migrations at scaleHave stood up analytics tooling to enable cross-functional teamsDomain expertise in legal technology, medical records, and working with unstructured data
A successful first year may look like: 75% doing system design and contributing code, starting with shipping code within 2 weeks!25% collaborating with stakeholders and mentoring, lunch and learns, and moreLeverage a self-starter mindset by taking a product concept and building the feature end to end (whether it’s a component of the system or a significant piece of functionality). Collaborate with the team to scale the tech stack based on our rapidly growing user base!
Benefits & Perks:We seek to empower all of our team members to fulfill our mission of making the world a more just place, regardless of our team’s function, geography, or experience level. To that end, we offer:  Fully remote setup - work from wherever you feel is best (Plus a stipend to upgrade your home office!) Flexible working hours to match your style Offsites - get to meet your coworkers on a fully-expensed trip every 6-12 months! Choice of great medical, dental, and vision insurance plan options Flexible paid time off A variety of virtual team events such as game nights & happy hours
EvenUp is an equal-opportunity employer. We are committed to diversity and inclusion in our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status."
"Quantitative Data Engineer, Research Signals REMOTE","Software Guidance & Assistance, Inc. (SGA, Inc.)",United States (Remote),https://www.linkedin.com/jobs/view/3766969839/?eBP=JOB_SEARCH_ORGANIC&refId=F%2F8e1MFOoG6d9fc37hW0nw%3D%3D&trackingId=4UZ418qT%2F0vE7CwhTTFkZA%3D%3D&trk=flagship3_search_srp_jobs,3766969839,"About the job
            
 
Software Guidance & Assistance, Inc., (SGA), is searching for a  Quantitative Data Engineer, Research Signals for a  Contract assignment with one of our premier  Financial Services clients in  a 100% Remote role. The Research Signals group provides independent research and investment consulting services to the institutional asset management community. The group researches and develops individual factors and multi-factor models across a range of asset classes by applying a systematic evaluation process to a variety of fundamental, technical, and industry-specific data sources. The Quantitative Data Engineer develops and supports new and existing quantitative products and datasets. The Research Signals team's research and datasets are widely used across the investment management industry. This position will partner with stakeholders in technology, research and operations and the broader product management team to build a data platform, deliver robust production systems and ensure high data integrity of our quantitative products which include alpha factors, risk models and quantitative investing software solutions. Responsibilities  :  Develop both on-premises and cloud-based data ingestion processes using SQL, Python Engineer data models and infrastructure for a wide variety of market and alternative dataset Author tests to validate data quality and measure the stability of the data acquisition processes Work directly with Analysts, Product Specialists, and the technology team to understand requirements and provide end-to-end data solutions Communicate with data providers to onboard new datasets and troubleshoot technical issues Investigate and defuse time-sensitive data incidents 
Required   Skills:  Bachelor/Master's degree in Computer Science, Information Systems, or related field Strong analytical, data and programming skills (Python/SQL/NoSQL/JavaScript) 3+years of experience with large data sets ETL and techniques to architect them for performance, experience using alternative unstructured data is a plus 1+ year of experience with cloud computing services, AWS preferred Aptitude for designing infrastructure, and data products for Quant/Data Scientists is a plus Ability to work effectively in an agile environment with numerous stakeholders on complex research and new development projects A genuine interest in investment strategies, equities, and fixed income. Asset management industry experience is a plus. Strong verbal and written communication skill, must be a team player 
SGA is a technology and resource solutions provider driven to stand out. We are a women-owned business. Our mission: to solve big IT problems with a more personal, boutique approach. Each year, we match consultants like you to more than 1,000 engagements. When we say let's work better together, we mean it. You'll join a diverse team built on these core values: customer service, employee development, and quality and integrity in everything we do. Be yourself, love what you do and find your passion at work. Please find us at     https://sgainc.com    . EEO Employer: Race, Color, Sex, Sexual Orientation, Gender Identity, Religion, National Origin, Disability, Veteran Status, Age, Marital Status, Pregnancy, Genetic Information, or Other Legally Protected Status.

Desired Skills and Experience
                SQL"
Data Integration Engineer - REMOTE --WORK FROM HOME!,SYNQ3 Restaurant Solutions,"Oklahoma City, OK (Remote)",https://www.linkedin.com/jobs/view/3751658219/?eBP=JOB_SEARCH_ORGANIC&refId=F%2F8e1MFOoG6d9fc37hW0nw%3D%3D&trackingId=JBbinDy5dW1AT9rtrVsiIw%3D%3D&trk=flagship3_search_srp_jobs,3751658219,"About the job
            
 
Job DetailsDescriptionWho is SYNQ3?SYNQ3 is the restaurant industry’s leading provider of conversational ordering solutions. SYNQ3’s combination of highly skilled team members and a proprietary conversational AI platform enables restaurants to enhance their order processing, provide better customer service for to-go and catering orders, and increase their sales and profits. SYNQ3 provides ordering solutions to thousands of restaurants for many of largest chains and has processed billions of dollars of orders for our clients.Role DescriptionREMOTE - WORK FROM HOME!The Data Integration Engineer is primarily responsible for helping to build and maintain our core data integration platforms. Work primarily involves cleaning, parsing, storing, enriching, and validating data from a growing number of systems and services at varying velocities. Tasks will vary in complexity and scope, with examples ranging from service endpoint integrations to collaborating on larger team projects designed for Machine Learning and Analytics consumption. The successful candidate must work well in a team environment, enjoy a dynamic fast-paced atmosphere, possess excellent problem-solving and critical thinking skills, and display a passion for learning new methods and technologies.Responsibilities Ensure parsed, validated data is reliably processed, stored, and enrichedShape and transform data from various structured, semi-structured, and unstructured sourcesOptimize complex database queries and authoring large Stored Procedures, Views and Triggers for data centric enterprise business solutionsMonitor availability, performance, and utilization of all core and edge integration pointsIdentify and resolve data pipeline and endpoint performance and utilization issuesCollaborate with other technical resources and cross-department staff on various projectsMaintain relevant data stewardship documentationProvide Tier-3 support for data ingress/egress service and availability incidentsAs this position supports critical business functions, On-Call work is requiredApplication development and maintenance across Microsoft Technology stack with a focus on Microsoft Azure Synapse, Web API, SQL Server, Spark, and Databricks.App Service, Logic Apps, Web Jobs, Azure Functions in an integrated development & test environment involving Azure SDK, Visual Studio and Azure EmulatorAll other duties as assigned
Qualifications Degree (AS/BS) desired but may be substituted with relevant experienceExperience with MS Azure (ADF, Storage/Warehousing, App Svcs)Experience with PythonExperience with ingesting, processing, and storing various data source formats (JSON, XML, CSV)Experience with DML/DDL T-SQL
Benefits And Compensation An annual salary of $90k - $115k based on experience.Accrued paid vacation time in accordance with our policy.Medical, dental, vision, and life insurance benefits are available on the 1st of the month following 60 days of employment.Optional group rate long-term and short-term disability will also be available on the same schedule.Direct deposit."
Job Opening for Test Data Engineer  - Remote,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3647023329/?eBP=JOB_SEARCH_ORGANIC&refId=F%2F8e1MFOoG6d9fc37hW0nw%3D%3D&trackingId=ISC%2FbjQzTG1Fj4mleM56ZA%3D%3D&trk=flagship3_search_srp_jobs,3647023329,"About the job
            
 
Hi,Please find attached Job Description. If you are interested please do share with me your updated resume or call me on ""3025492448"".Title:- Test Data EngineerLocation:- RemoteDuration:- 6+ MonthsVisa:- Citizen, GC, GC-EADInterview Mode:- Video Est Or Cst OnlyDescription  Must required: GenRocket, scripting, PostGres/DB ability, API work Determine best practice(s) around synthetic, masked data in DCE/Platform Analyze and understand how data flows in data hub Determine best way to manage and create synthetic data in Data Hub; deliver solution Figure out a solution to mock massive accounts coming through APIs Understanding data models, table structures and dependencies. Guide teams in generating new synthetic data, creating more, and maintaining over time. Create PoC for syntheitc data use cases working with Product team Agile Mindset, familiarity with Azure Cloud ADO experience, python or powershell scripting experience
Must required: GenRocket, scripting, PostGres/DB ability, API workDetermine best practice(s) around synthetic, masked data in DCE/PlatformAnalyze and understand how data flows in data hubDetermine best way to manage and create synthetic data in Data Hub; deliver solutionFigure out a solution to mock massive accounts coming through APIsUnderstanding data models, table structures and dependencies.Guide teams in generating new synthetic data, creating more, and maintaining over time.Create PoC for syntheitc data use cases working with Product teamAgile Mindset, familiarity with Azure CloudADO experience, python or powershell scripting experienceGaurav VermaTalent Acquisition -North AmericaDirect:+1 3025492448gaurav.verma@steneral.comIn my absence please reach out to Mr. Harish Sharma at harish@steneral.com & 302-721-6151"
Data Engineer,Tail Wind Informatics Corporation,"Minnesota, United States (Remote)",https://www.linkedin.com/jobs/view/3732071591/?eBP=JOB_SEARCH_ORGANIC&refId=F%2F8e1MFOoG6d9fc37hW0nw%3D%3D&trackingId=dH7UN3RBQiQTgYO1tUoOUw%3D%3D&trk=flagship3_search_srp_jobs,3732071591,"About the job
            
 
About Us:Tail Wind Informatics Corporation -Microsoft Solutions Partner- is a dynamic and innovative IT consulting services company that specializes in delivering Data Architecture and Business Intelligence solutions. We are currently seeking a talented and experienced Data Engineer to join our team and play a crucial role in managing and optimizing data infrastructure. If you are passionate about data, have a strong background in Microsoft technologies, and enjoy solving complex data challenges, we want to hear from you!Position Overview:As a Data Engineer at Tail Wind, you will be responsible for designing, developing, and maintaining data pipelines and systems on the Azure cloud platform. You will work closely with cross-functional teams to ensure that data is accurate, accessible, and supports data-driven decision-making. The ideal candidate has hands-on experience with Azure Data Factory, SQL, Stored Procedures, Python, Databricks, Snowflake, Azure Synapse Analytics, and Power BI.Key Responsibilities: Design, build, and maintain data pipelines and ETL processes using Azure Data FactoryDevelop and optimize SQL queries, stored procedures, and scripts for data transformation and extractionCollaborate with data scientists and analysts to understand data requirements and ensure data availabilityImplement data quality checks and data validation processes to ensure data accuracy and consistencyUtilize Databricks for advanced data processing, transformation, and analyticsManage and optimize data storage, including Azure Data Lake Storage, Azure Blob Storage, and SnowflakeBuild and maintain data warehouses and analytics solutionsCreate interactive reports and dashboards using Power BI for data visualization and insightsMonitor and troubleshoot data pipelines, addressing any issues in a timely mannerStay up-to-date with the latest Azure data technologies and best practices
Qualifications: Bachelor's degree in Computer Science, Information Technology, or a related field (or equivalent work experience)2+ years of experience as a Data Engineer with a focus on Microsoft technologiesStrong proficiency in Azure Data Factory, or related product, for ETL processes and data orchestrationProficiency in SQL, including the ability to write complex queries and stored proceduresExperience with Python for data manipulation and automationKnowledge of Databricks for big data processing and analyticsFamiliarity with Snowflake or Azure Synapse Analytics for data warehousingProficiency in Power BI for data visualization and reportingStrong problem-solving skills and attention to detailExcellent communication and teamwork abilities
Why Join Tail Wind? Competitive salary and benefits packageOpportunity to work with cutting-edge technologies and solve challenging data problemsCollaborative and innovative work environmentProfessional development opportunities and support for various certifications
Application Process:To apply for this position, please click ""apply for this job"" at the top or bottom of the page, then upload and submit a current copy of your resume. One of the members of our Talent Acquisition team will reach out to schedule a phone interview if you meet the qualifications listed above.If you are a passionate Senior Data Engineer with a strong background in Azure Data Factory, SQL, Python, Databricks, Snowflake, Synapse, and Power BI, we encourage you to apply and be a part of our incredible team.Tail Wind Informatics Corp., is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.No agencies please. Salary range can vary depending on experience and will be determined based on the results of a Technical Interview"
Senior Data Engineer,BambooHR,"Phoenix, AZ (Remote)",https://www.linkedin.com/jobs/view/3760295698/?eBP=JOB_SEARCH_ORGANIC&refId=F%2F8e1MFOoG6d9fc37hW0nw%3D%3D&trackingId=BPhzx9Vd3zYEh8j%2FniCTqg%3D%3D&trk=flagship3_search_srp_jobs,3760295698,"About the job
            
 
About UsOur mission is simple: we want to set people free to do meaningful work. People love our software—and it turns out that people love working here too. We've been recognized as a ""Best Company to Work For"" and we're proud of our team for creating software that makes an impact in the lives of HR pros and employees all over the world.What You'll DoAs a Senior Data Engineer on the data platform team, we'll rely on your expertise across multiple disciplines to develop, deploy and support data systems, data pipelines, data lakes, and lakehouses. Your ability to automate, performance tune, and scale the data platform will be key to your success.Your initial areas of focus will include: Collaborate with stakeholders to make effective use of core data assetsWith Spark and Pyspark libraries, load both streaming and batched dataEngineer lakehouse models to support defined data patterns and use casesLeverage a combination of tools, engines, libraries, and code to build scalable data pipelinesWork within an IT managed AWS account and VPC to stand up and maintain data platform development, staging, and production environmentsDocumentation of data pipelines, cloud infrastructure, and standard operating proceduresExpress data platform cloud infrastructure, services, and configuration as codeAutomate load, scaling, and performance testing of data platform pipelines and infrastructureMonitor, operate, and optimize data pipelines and distributed applicationsHelp ensure appropriate data privacy and securityAutomate continuous upgrades and testing of data platform infrastructure and servicesBuild data pipeline unit, integration, quality, and performance testsParticipate in peer code reviews, code approvals, and pull requestsIdentify, recommend, and implement opportunities for improvement in efficiency, resilience, scale, security, and performance
What You Need to Get the Job Done (if you don't have all, apply anyway!) Experience developing, scaling, and tuning data pipelines in Spark with PySparkUnderstanding of data lake, lakehouse, and data warehouse systems, and related technologiesKnowledge and understanding of data formats, data patterns, models, and methodologiesExperience storing data objects in hadoop or hadoop like environments such as S3Demonstrated ability to deploy, configure, secure, performance tune, and scale EMR and Spark Experience working with streaming technologies such as Kafka and KinesisExperience with the administration, configuration, performance tuning, and security of database engines like Snowflake, Databricks, Redshift, Vertica, or GreenplumAbility to work with cloud infrastructure including resource scaling, S3, RDS, IAM, security groups, AMIs, cloudwatch, cloudtrail, and secrets managerUnderstanding of security around cloud infrastructure and data systemsGit-based team coding workflows
Bonus Skills (Not Required, So Apply Anyway!) Experience deploying and implementing lakehouse technologies such as Hudi, Iceberg, and DeltaExperience with Flink, Presto, Dremio, Databricks, or KubernetesExperience with expressing infrastructure as code leveraging tools like TerraformExperience and understanding of a zero trust security frameworkExperience developing CI/CD pipelines for automated testing and code deploymentExperience with QA and test automationExposure to visualization tools like Tableau
Beyond the technical skills, we're looking for individuals who are: Clear communicators with team members and stakeholdersAnalytical and perceptive of patternsCreative in codingDetail-oriented and persistentProductive in a dynamic setting
If you love to learn, you'll be in good company. You'll likely have a Bachelor's degree in computer science, information systems, or equivalent working experience.An Equal Opportunity Employer--M/F/D/VBecause our team members are trusted to handle sensitive information, we require all candidates that receive and accept employment offers to complete a background check before being hired.For information on our Privacy Policy, click here."
Remote Work - Need Data Quality Engineer (Databricks),Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3731640859/?eBP=JOB_SEARCH_ORGANIC&refId=F%2F8e1MFOoG6d9fc37hW0nw%3D%3D&trackingId=uB6%2BjBcUv0Nq%2BYZVlmiICQ%3D%3D&trk=flagship3_search_srp_jobs,3731640859,"About the job
            
 
Candidate must have Selenium and Databricks experience to be considered.Job: Data Quality Engineer (Databricks)Location: Remote 100%Term: 8+ months. Likely to extend to 12+ months. Start date is targeted for 11/13. Please confirm this with your candidate 
SummaryMcLaren has been hired by a Fortune 500 Consumer Packaged Goods (CPG) company to assist with implementing data mesh within Databricks. The client has been hoping to deploy this initiative and struggling so McLaren is coming in to assist. They will be moving the primary source data from SAP and Snowflake into Databricks.They have 120 data product they are building out. The goal is to do 2-week sprints for 2-4 data product deployments at a time.Responsibilities Implement data validation routines to ensure data accuracy.Monitor and improve data quality through validation and cleansing.Collaborate with data developers to address data-related issues.
Requirements 5+ years QA experiencePrevious experience working on a data mesh or databricks implementationExperience as main POC for all QA activities on a projectQuality Assurance Methodologies (e.g., Agile testing, Test Driven Development)Test Planning and StrategyTest Automation (e.g., Selenium, JUnit)Defect Management Tools (e.g., Jira, Bugzilla)Performance Testing"
Data and Artificial Intelligence Research Engineer (DARE),Citrine Informatics,United States (Remote),https://www.linkedin.com/jobs/view/3719550442/?eBP=JOB_SEARCH_ORGANIC&refId=F%2F8e1MFOoG6d9fc37hW0nw%3D%3D&trackingId=9YCv3kQYNV9bQGYzq%2BMgQg%3D%3D&trk=flagship3_search_srp_jobs,3719550442,"About the job
            
 
Design and build the core scientific libraries that enable our users to apply machine learning to solve the world’s most important materials science problems at an industrial scale.About CitrineAt Citrine, we’re using generative Artificial Intelligence to enable our customers to usher in the next generation of sustainable, high-performing materials and chemicals.We’re the leader in AI for materials and chemicals. Our no-code SaaS platform gives product developers, researchers, and engineers access to domain-specific data management and AI tools. Our models propose new ways to combine ingredients and processes that are most likely to outperform, helping companies discover and deploy innovative, sustainable, high-performing materials and chemicals up to 98% faster than traditional R&D approaches.Citrine regularly wins awards for innovation and sustainability, earning spots on the CB Insights AI 100 list and the Inc. 5000 list of fastest-growing private companies in the US. Our customers include leading organizations such as LyondellBasell, Braskem, Grace, Synthomer, and Solvay. We also collaborate with researchers from world-renowned institutions on cutting-edge research at the intersection of AI and the physical sciences. Our team is ambitious with our goals, passionate about our vision, driven by our sustainability mission, and eager to grow and learn from each other. We’re headquartered in California, with team members throughout North America and Europe. We’re growing quickly, and we’re looking for the best to join us!About The RoleData & AI Research Engineering is a uniquely interdisciplinary team working at the intersection of materials science, applied mathematics, and software engineering. We are responsible for researching, developing, testing, implementing, and maintaining scientific methods that form the core materials-aware machine learning functionality that powers the Citrine Platform. We collaborate extensively across the company — other teams rely on us throughout the product lifecycle to translate ideas to math to code and back again.Responsibilities include:  Writing and maintaining Scala ML code, with some additional Python Collaborate closely with other engineers, frequently reviewing code and best practices Mentor other developers Design high-performance, scalable systems Test & analyze the impact and performance of your software Work in a multi-functional team on features from concept to delivery
Example projects:  Improve and maintain our core Scala libraries that enable our users to apply machine learning to solve the world’s most important materials science problems at an industrial scale Collaborate with our External Research and Development (ERD) team to write and publish papers related to the work we’re doing Improve the interpretability of machine learning models and develop tools to communicate that information to users Develop ways to efficiently explore the complex parameter spaces that characterize real-world materials synthesis problems Collaborate with our Product team to shape the future of our AI capabilities and support our customers’ unique use cases Develop new methods to quantify uncertainty in machine learning predictions
Skills And Experience 6+ years of professional experience or MS/PhD in mathematics, information science, or computer scienceExtensive knowledge of Scala in a production environmentProven history of implementing AI solutions for customers, with quantifiable resultsExperience solving scientific problems with computational techniques Ability to communicate complex technical concepts and design choices to any audienceIn-depth knowledge of how core ML algorithms work and their design (random forest, k-nearest neighbors, linear and logistic regression, backpropagation, etc.) Python (Sklearn, Numpy, Pandas, Scipy, etc.) Advanced developer-testing skills: unit, integration, property-based testing, etcContainers (Docker, ECS) for development, testing, and production
Nice-to-haves Experience with and/or a degree in Materials SciencePrior projects where you took materials manufacturing data and built ML models to detect anomalies or quality defectsExtensive knowledge of statisticsExperience with LLMs such as the GPT models or BERT, etcBonus points if you’ve taken one of these models and created a customer-facing product with it!Published papers that establish you as a domain expert in AI/ML, NLP, or scientific computingPinecone (vector database)Relational databases (Advanced PostgreSQL is an asset) Close integration with various AWS services (S3, RDS, SQS, etc) 
Equal OpportunityAll qualified applicants will receive consideration for employment without regard to race, creed, color, or national origin.AccommodationsCitrine is an inclusive work environment, and we are committed to ensuring equal opportunity in employment for qualified persons with disabilities. Please email us at hello@citrine.io or inform your recruiter if you require any reasonable accommodations throughout the recruiting process.Our Core ValuesCitrine Informatics recognizes that its most valuable asset is its people. We have created our set of Core Values to encourage, support, and invest in our team as they work to innovate and support a more sustainable world. Our Core Values reflect our ongoing commitment to continuously invest in nurturing our talent and our people-first approach to conducting business.  We take pride in and recognize the successes and growth of ourselves and our colleagues. We support each other in our growth We prototype and collect data to make good decisions. We question that data and are constantly iterating to find the best solution We are all owners of Citrine and make decisions like owners. We work autonomously with personal and organizational accountability We commit to building a diverse and inclusive community within Citrine and actively promote equity and belonging We are tirelessly committed to creating value for our customers We exist to help our customers accelerate the development of sustainable products that are critical to the future of both our planet and our industry
Compensation and Pay TransparencyAt Citrine, we want your path to career growth to be transparent, straightforward, fair, and easily accessible -- starting with your application and interview process. The annual salary range listed below reflects the level we are considering for this position (please note that there may be unique situations where you may fall outside of this range). Where you fall within the range will depend on how your experience and skills align to our internal leveling system as we learn more about you throughout the interview process.$145,000 USD - $175,000 USD Range(s) listed are for full-time employees based in the United States onlyColorado only: disclosure above meets the requirement by sb19-085(8-5-20)
Our Benefits (for exempt, full-time employees based within the United States)401k with matching up to 4%Medical, vision, dental insurance (we pay 100% of your premium and 75% of your dependents)Life and Disability insuranceFSA and HSA plansEquity options within the company12 weeks of paid parental leaveFlexible PTO on top of our 15 paid company holidays (includes your birthday!)Free financial counseling$600 tech allowanceMonthly $75 phone reimbursement$5,000 annual continuing educational allowance"
Data Engineer,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3632565777/?eBP=JOB_SEARCH_ORGANIC&refId=F%2F8e1MFOoG6d9fc37hW0nw%3D%3D&trackingId=C3weYaJK9tx1zLzij9Z90Q%3D%3D&trk=flagship3_search_srp_jobs,3632565777,"About the job
            
 
Title: Data EngineerLocation: Remote (EST hours)LinkedIn is requiredPlease send the candidate's resume with their contact information included. If their background looks like a fit I will give them a quick call to discuss the position. Include a link to their LinkedIn profile with your submission.Job Description We have flexibility in terms of hours but we are aligning to EST as that's where our HQ is located. We work with offshore and we do tend to start earlier than some with stand ups at 8am. We need a Cloudera expert who will help us with the migration so Cloudera CDP expertise & hands-on experience is a must.
Job SummaryThis position is responsible for being an expert with Cloudera Data Platform and related technologies including Hadoop, Hive, PostgreSQL, Impala, Amazon Web Services (AWS), Linux, database management and data migrations. The position is responsible for architecting and building a next generation data platform using reusable, scalable code with the ability to scale very large data volumes. This position requires the ability to assess the current project's challenges and offer solutions leading to the launch of the next generation product.Demonstrated hands-on experience with Cloudera Data Platform.Demonstrated work experience with distributed, scalable Big Data programming model and technologies such as Hadoop, Hive, Pig, etc.Deep technical Expertise in the Cloudera Data Platform ecosystem components.Experience automating via Terraform, Python, CDP CLI, AWS CLI, etc.Experience designing and configuring cluster right sizing strategies.Cloudera CDP Certification(s)Hortonworks Hadoop Certification(s)8+ years' experience in dimensional data modeling, ETL development, and Data Warehousing.5+ years' experience in Business Consulting-ETL Processes.4-6 years' experience using BIG Data (BD)-Apache Hadoop (HDFS)Base/Hive/Pig/Mahout/Flume/Scoop/MapReduce/Yarn.1+ year experience in Cloud Dev and Migration-AWS-Analytics/DW/Redshift."
Data Integration Engineer (REMOTE),Sears,"Hoffman Estates, IL (Remote)",https://www.linkedin.com/jobs/view/3771222423/?eBP=JOB_SEARCH_ORGANIC&refId=F%2F8e1MFOoG6d9fc37hW0nw%3D%3D&trackingId=AUWKvud1wJ2AXzAQjg8tgg%3D%3D&trk=flagship3_search_srp_jobs,3771222423,"About the job
            
 
Job DescriptionJOB SCOPE:This position will report directly to the Sr Dir, Technology for Sears Home Services and will support financial platform technology integrations across the Sears Home Services organization.Job SummaryThe Data Integration Engineer is responsible for designing, developing, testing, and deploying data integrations across multiple platforms both within middleware and through API connections. They will work with stakeholders to understand data requirements and develop data models. They will also select and implement the appropriate data integration tools and technologies.Responsibilities/Skills/Experience RequirementsJOB DUTIES/RESPONSIBILITIES: Designs, develops, tests, and deploys data integration solutions with a focus on financial data accuracy and integrityCollaborates with stakeholders and cross-functional teams in the Finance Business Unit to understand financial data requirements and develops financial data models that can be used for further analysisEvaluates, selects, and implements appropriate data integration tools and technologies, with a focus on financial data security and efficiencyDevelops and executes comprehensive testing plans to ensure the accuracy and reliability of financial data integration solutions;Continuously monitors and troubleshoots data integrations if issues ariseCreates and maintains documentation for financial data integration processes, ensuring compliance with regulatory requirements and facilitating knowledge transfer within the teamStays updated on industry best practices and emerging technologies in financial data integrationEnsures compliance with financial data regulations and industry standardsCommunicates effectively with both technical and non-technical team members to ensure successful implementation of financial data integration projects
REQUIRED SKILLS: Bachelor's degree in Computer Science, Information Systems, or a related field3+ years of experience in data integrationExperience with SQL, NoSQL, and data warehousing technologiesExperience with data modeling and data qualityExperience with cloud computing platforms (AWS, Azure, GCP)Strong programming skills (Python, Java, etc.)Excellent problem-solving and analytical skillsAbility to work independently and as part of a teamExperience with cloud-based data integration platforms such as AWS Glue or Azure Data FactoryExperience with machine learning and artificial intelligence
PREFERRED SKILLS: Experience with data integration tools such as Boomi, Informatica, or MuleSoftExperience with big data technologies such as Hadoop and Spark
Years Experience2 - 5 Years ExperienceTravel RequirementsOn Occasion (Less than 5%)CountryUnited StatesWork-In Address 1REMOTEWork-In Address 2REMOTEWork-In CityREMOTEWork-In StateREMOTEWork-In Postal CodeREMOTEBusinessTransformco Home Services - SupportJob FunctionInformation TechnologyEmployment CategoryRegular, Full-timeCompensation RangeNAAdditional Compensation ExplanationN/AEEO/EOE FooterEqual Opportunity Employer / Disability / Vet.Posting Tags#HSCorporateCompany BrandSears Home ServicesLocation CityHOFFMAN ESTATES"
Data Engineer With Scala/Spark,Tror - AI for everyone,United States (Remote),https://www.linkedin.com/jobs/view/3639540196/?eBP=JOB_SEARCH_ORGANIC&refId=F%2F8e1MFOoG6d9fc37hW0nw%3D%3D&trackingId=mTHdEIV%2FkUKnatWRQ69thQ%3D%3D&trk=flagship3_search_srp_jobs,3639540196,"About the job
            
 
Job DescriptionJob Title : Data Engineer With Scala/SparkLocation : RemoteVISA : Only H1BContract Type: C2C or W2Job Description Need a senior engineer with 10+ years of experience. Must be able to design, build and deploy data transformation and ETL jobs in Azure or AWS cloud. Work with large scale datasets Work with/use various external APIs to enhance data Setup database tables for analytics users to consume the data collected by the Data Engineering team Work with big data technologies to improve data availability and data quality in the cloud (Azure or AWS) 
Skills Required  Strong technical experience in Scala and Spark for designing , creating and maintaining applications. Experience with Cloud Data Platforms like Databricks. Experience with Azure cloud (or similar cloud like AWS). Strong SQL skills with commensurate experience in a large database platform. Experience implementing ETL routines with 100M+ datasets in a performant and scalable manner Experience with relational database design (MySQL, Postgres etc) Experience in complete SDLC process and Agile Methodology Strong oral and written communication skills"
Senior/Staff Data Engineer,EvenUp,"Jacksonville, FL (Remote)",https://www.linkedin.com/jobs/view/3728162152/?eBP=JOB_SEARCH_ORGANIC&refId=vkk%2FjwP03Cxiw9XTXse90w%3D%3D&trackingId=Szx4mR19OWydoIkRcr12Jg%3D%3D&trk=flagship3_search_srp_jobs,3728162152,"About the job
            
 
EvenUp is a venture-backed generative AI startup that ensures injury victims are awarded the full value of their claims, expanding the $100B+ in awards granted to injury victims every year. Every year, the legal system has made it difficult for millions of ordinary people to seek justice, especially for folks without means or who come from underrepresented backgrounds. Our vision is to help these injury victims get the justice they deserve, irrespective of their income, demographics, or the quality of their legal representation.EvenUp operates across all types of injury cases, from police brutality and child abuse to California wildfires and motor vehicle accidents. Our ML-driven software empowers attorneys to accurately assess the value of these cases by doing a core part of their workflow (legal drafting), enabling them to secure larger settlements in record time. As EvenUp evaluates more cases, our proprietary data grows, enhancing the precision of our predictions and delivering more value to both attorneys and victims alike.As one of the fastest growing startups ($0 to $10M in ARR in <2 years), we raised $65M in investment from some of the best investors in the world (Bessemer, Bain Capital, Signalfire, DCM, NFX, Tribe Capital), seasoned tech executives (i.e. founder of Quora, SVP at Google, former CPO at Uber), and public figures that care about our social mission (Nas, Jared Leto, Byron Jones). Our team comes from top tech, legal, and investing backgrounds including Waymo, Google, Amazon, Uber, Quora, Blizzard, Norton Rose, Warburg Pincus, Bain, and McKinsey.Why we are hiring a Senior/Staff Data Engineer now? We have experienced unprecedented growth and need to scale out our data warehousing, data tooling and internal analytics. We need to architect the future of our data infrastructure at EvenUp and we’re seeking engineering leaders to help drive that vision. We will need to 10x our pipeline processing throughput over the next 12 months. We’ll need to rethink and rebuild how we extract, process and model our ingestion to enable our organization with precise and actionable data. We need to design & build data warehousing that democratizes data for our entire organization. We will invest in identifying and integrating tools and services that empower our teams to build on top of our data and analytics. 
What you’ll do: Democratize data at EvenUp. Ensure our organization can scale with consistent, standardized access to our data stores and accelerate our ability to build and experiment with data productsArchitect and build out the future of data warehousing at EvenUpEnable and empower our Data Science team to rapidly iterate on model experimentationDesign, organize and refine data storage strategies that reduce development friction for our tech organizationCollaborate with cross functional teams to solve critical data problemsHelp grow our nascent Data Insights team and define a “data first” mentality across our organization
What we are seeking: 8+ years of data engineering experiencePrevious experience building out data warehousing, data pipelines, and internal analyticsStrong understanding and practical experience with data tooling, BI tools, and systems such as DBT, BigQuery, ElasticsearchThe ability to communicate cross-functionally with various stakeholders to derive requirements and architect scalable solutionsHave several years of industry experience building high-quality software, shipping production-ready code and infrastructureYou enjoy owning a project from start to finish and love to drive a project across the finish lineInterest in making the world a fairer place (we don’t get paid unless we’re helping injured victims and/or their attorneys)
Nice to haves: Have previously built out a Data Insights team at a data-oriented startupHave previously planned and architected data migrations at scaleHave stood up analytics tooling to enable cross-functional teamsDomain expertise in legal technology, medical records, and working with unstructured data
A successful first year may look like: 75% doing system design and contributing code, starting with shipping code within 2 weeks!25% collaborating with stakeholders and mentoring, lunch and learns, and moreLeverage a self-starter mindset by taking a product concept and building the feature end to end (whether it’s a component of the system or a significant piece of functionality). Collaborate with the team to scale the tech stack based on our rapidly growing user base!
Benefits & Perks:We seek to empower all of our team members to fulfill our mission of making the world a more just place, regardless of our team’s function, geography, or experience level. To that end, we offer:  Fully remote setup - work from wherever you feel is best (Plus a stipend to upgrade your home office!) Flexible working hours to match your style Offsites - get to meet your coworkers on a fully-expensed trip every 6-12 months! Choice of great medical, dental, and vision insurance plan options Flexible paid time off A variety of virtual team events such as game nights & happy hours
EvenUp is an equal-opportunity employer. We are committed to diversity and inclusion in our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status."
Data Integration Engineer - REMOTE --WORK FROM HOME!,SYNQ3 Restaurant Solutions,United States (Remote),https://www.linkedin.com/jobs/view/3756093311/?eBP=JOB_SEARCH_ORGANIC&refId=vkk%2FjwP03Cxiw9XTXse90w%3D%3D&trackingId=ElIRncmqyBEEyFur1OJArw%3D%3D&trk=flagship3_search_srp_jobs,3756093311,"About the job
            
 
Job DetailsDescriptionWho is SYNQ3?SYNQ3 is the restaurant industry’s leading provider of conversational ordering solutions. SYNQ3’s combination of highly skilled team members and a proprietary conversational AI platform enables restaurants to enhance their order processing, provide better customer service for to-go and catering orders, and increase their sales and profits. SYNQ3 provides ordering solutions to thousands of restaurants for many of largest chains and has processed billions of dollars of orders for our clients.Role DescriptionREMOTE - WORK FROM HOME!The Data Integration Engineer is primarily responsible for helping to build and maintain our core data integration platforms. Work primarily involves cleaning, parsing, storing, enriching, and validating data from a growing number of systems and services at varying velocities. Tasks will vary in complexity and scope, with examples ranging from service endpoint integrations to collaborating on larger team projects designed for Machine Learning and Analytics consumption. The successful candidate must work well in a team environment, enjoy a dynamic fast-paced atmosphere, possess excellent problem-solving and critical thinking skills, and display a passion for learning new methods and technologies.Responsibilities Ensure parsed, validated data is reliably processed, stored, and enrichedShape and transform data from various structured, semi-structured, and unstructured sourcesOptimize complex database queries and authoring large Stored Procedures, Views and Triggers for data centric enterprise business solutionsMonitor availability, performance, and utilization of all core and edge integration pointsIdentify and resolve data pipeline and endpoint performance and utilization issuesCollaborate with other technical resources and cross-department staff on various projectsMaintain relevant data stewardship documentationProvide Tier-3 support for data ingress/egress service and availability incidentsAs this position supports critical business functions, On-Call work is requiredApplication development and maintenance across Microsoft Technology stack with a focus on Microsoft Azure Synapse, Web API, SQL Server, Spark, and Databricks.App Service, Logic Apps, Web Jobs, Azure Functions in an integrated development & test environment involving Azure SDK, Visual Studio and Azure EmulatorAll other duties as assigned
Qualifications Degree (AS/BS) desired but may be substituted with relevant experienceExperience with MS Azure (ADF, Storage/Warehousing, App Svcs)Experience with PythonExperience with ingesting, processing, and storing various data source formats (JSON, XML, CSV)Experience with DML/DDL T-SQL
Benefits And Compensation An annual salary of $90k - $115k based on experience.Accrued paid vacation time in accordance with our policy.Medical, dental, vision, and life insurance benefits are available on the 1st of the month following 60 days of employment.Optional group rate long-term and short-term disability will also be available on the same schedule.Direct deposit."
Sr. Data Engineer,Blueprint,"Bellevue, WA (Remote)",https://www.linkedin.com/jobs/view/3766037108/?eBP=JOB_SEARCH_ORGANIC&refId=vkk%2FjwP03Cxiw9XTXse90w%3D%3D&trackingId=El55qC%2B6KBKh0HRwZCuyHA%3D%3D&trk=flagship3_search_srp_jobs,3766037108,"About the job
            
 
Remote (U.S. Only)Who is Blueprint?We are a technology solutions firm headquartered in Bellevue, Washington, with a strong presence across the United States. Unified by a shared passion for solving complicated problems, our people are our greatest asset. We use technology as a tool to bridge the gap between strategy and execution, powered by the knowledge, skills, and the expertise of our teams, who all have unique perspectives and years of experience across multiple industries. We're bold, smart, agile, and fun.What does Blueprint do?Blueprint helps organizations unlock value from existing assets by leveraging cutting-edge technology to create additional revenue streams and new lines of business. We connect strategy, business solutions, products, and services to transform and grow companies.Why Blueprint?At Blueprint, we believe in the power of possibility and are passionate about bringing it to life. Whether you join our bustling product division, our multifaceted services team or you want to grow your career in human resources, your ability to make an impact is amplified when you join one of our teams. You'll focus on solving unique business problems while gaining hands-on experience with the world's best technology. We believe in unique perspectives and build teams of people with diverse skillsets and backgrounds. At Blueprint, you'll have the opportunity to work with multiple clients and teams, such as data science and product development, all while learning, growing, and developing new solutions. We guarantee you won't find a better place to work and thrive than at Blueprint.What will I be doing?Blueprint is looking for a Sr. Data Engineer to join us as we build cutting-edge technology solutions! The ideal candidate will have a solid background in consulting, with demonstrated experience leading clients through the process of building modern data estates. As a Principal Data Engineer, you will spend a majority of your time working directly with clients to develop their advanced modern data estates, warehouses, and analytical environments. You will also be responsible for overseeing and mentoring junior developers within the organization.Responsibilities: Develop and implement effective data architecture solutions using Databricks and LakehouseOptimize and tune data pipelines for performance and scalabilityMonitor and troubleshoot data pipelines to ensure data availability and reliabilityImplement best practices for data governance, data security, and data quality to ensure data integrity across all data sourcesCreate and maintain documentation related to data architecture, data pipelines, and data modelsStay up to date with emerging technologies and best practices in data engineering and big data processingMentor and train other data engineers on best practices for data engineering and Databricks usageProvide thought leadership in the Databricks and Lakehouse space, both within the organization and externallyDesign, build, implement and maintain our data infrastructure to power analytics and MLContribute to our investments into various open-source and 3rd party tools to build a system that scales with the companyCollaborate with our Data Scientists and Analytics Engineers to make pipeline implementation faster, more straightforward, and more trustworthy driven decisions that will shape our business
Qualifications: Bachelor's or Master's degree in Computer Science, Computer Engineering, or a related field8+ years of experience in data engineering3+ years of experience working with Databricks and PySpark6-8+ years of experience with SQLAppreciation for the Lakehouse medallion data architecture – bronze, silver, gold – and how those data stages are usedWorking knowledge of DLT(Delta Live Tables) and Unity Catalog a plusStrong understanding of ETL and ELT data ingestion, acquisition, and data processing patternsExperience with cloud-based data warehousing platforms such as Synapse, AWS Redshift, Google BigQuery, or SnowflakeStrong understanding of data engineering, data warehousing, data modeling, data governance, and data security best practicesExcellent problem-solving and troubleshooting skillsStrong communication and collaboration skills, with the ability to work effectively in a team environmentExperience mentoring and training other data engineers
Salary RangePay ranges vary based on multiple factors including, without limitation, skill sets, education, responsibilities, experience, and geographical market. The pay range for this position reflects geographic based ranges for Washington state: $146,400 to $175,100 USD/annually. The salary/wage and job title for this opening will be based on the selected candidate's qualifications and experience and may be outside this range.Equal Opportunity EmployerBlueprint Technologies, LLC is an equal employment opportunity employer. Qualified applicants are considered without regard to race, color, age, disability, sex, gender identity or expression, orientation, veteran/military status, religion, national origin, ancestry, marital, or familial status, genetic information, citizenship, or any other status protected by law.If you need assistance or a reasonable accommodation to complete the application process, please reach out to: recruiting@bpcs.comBlueprint believe in the importance of a healthy and happy team, which is why our comprehensive benefits package includes: Medical, dental, and vision coverageFlexible Spending Account401k programCompetitive PTO offeringsParental LeavePersonal paid Volunteer time to support our communityOpportunities for professional growth and development
Location: Remote"
"Data Engineer//Pittsburgh, PA (remote) no h1b",TekIntegral,"Pittsburgh, PA (Remote)",https://www.linkedin.com/jobs/view/3667476473/?eBP=JOB_SEARCH_ORGANIC&refId=vkk%2FjwP03Cxiw9XTXse90w%3D%3D&trackingId=nmNyY42jdOWd16KsVEEKCg%3D%3D&trk=flagship3_search_srp_jobs,3667476473,"About the job
            
 
Hi ,Hope you are doing greatI have very urgent need Data Engineer role let me know if you are interested.Title: Data Engineer IILocation: Pittsburgh, PADuration: 6+ MonthThis role will also be charged with understanding and interpreting requirements to contribute to the technical architecture and the associated design documents.Please send resume with LinkedIn Profile (this is required), contact information, location, work status and rate.A Data Engineer III is responsible for coding and continuous testing of complex modules and applications in support of the Client's platform.PRIMARY DUTIES AND RESPONSIBILITIESWriting, debugging, unit testing, and performance test code in the data access layer in accordance with Client's standards.As an agile team member, participate in code reviews, design reviews, etc.Utilize domain driven techniques and design patterns to build and contribute to technical design.Develop and maintain strong knowledge of implemented requirements and detailed application behaviors.Assists in the development and training of SE I.JOB SUMMARYA Data Engineer II is responsible for coding and continuous testing of complex modules and applications in support of the Client's platform. This role will also be charged with understanding and interpreting requirements to contribute to the technical architecture and the associated design documents.PRIMARY DUTIES AND RESPONSIBILITIESWriting, debugging, unit testing, and performance test code in the data access layer in accordance with Client's standards.As an agile team member, participate in code reviews, design reviews, etc.Utilize domain driven techniques and design patterns to build and contribute to technical design.Develop and maintain strong knowledge of implemented requirements and detailed application behaviors.Assists in the development and training of SE I.EDUCATIONBachelor's computer information technology, computer science, management requiredMaster's preferredStrong understanding and familiarity working in the Linux operating environment.Familiarity and experience executing several software development methodologies and life cycles preferred.5+ years of developing software using object-oriented or functional language experience5+ years of SQL2+ years working with open source Big Data technology stacks (Apache Nifi, Spark, Kafka, HBase, Hadoop/HDFS, Hive, Drill, Pig, etc.) or commercial open source Big Data technology stacks (Hortonworks, Cloudera, etc.)3+ years with document databases (e.g. MongoDB, Accumulo, etc.)3+ years of experience using Agile development processes (e.g. developing and estimating user stories, sprint planning, sprint retrospectives, etc.)2+ years of distributed version control system (e.g. git)3+ years of experience in cloud-based development and deliveryFamiliarity with distributed computing patterns, techniques, and technologies (e.g. ESB)Familiarity with continuous delivery technologies (e.g. Puppet, Chef, Ansible, Docker, Vagrant, etc.)Familiarity with build automation and continuous integration tools (e.g. Maven, Jenkins, Bamboo, etc.)Familiarity with Agile process management tools (e.g.? Atlassian Jira)Familiarity with test automation (Selenium, SoapUI, etc.)Good software development and Object Oriented programming skills.Strong analytical skills and the ability to work with end users to transform requests into robust solutions.Excellent oral and written communication skills.Initiative and self-motivation to work independently on projects.Mohit RajputTechnical RecruiterTekIntegral Inc.500 N Central Expwy #500GPlano, TX USA 75074mrajput www.tekintegral.com"
Mostly Remote: Data Engineer,Stellar Professionals,United States (Remote),https://www.linkedin.com/jobs/view/3718987119/?eBP=JOB_SEARCH_ORGANIC&refId=vkk%2FjwP03Cxiw9XTXse90w%3D%3D&trackingId=yU0tbdBlzCa5GErHY6NeSw%3D%3D&trk=flagship3_search_srp_jobs,3718987119,"About the job
            
 
Skills Required Strong knowledge of SQL to aid in data visualizationWorking knowledge of Hadoop tools, such as Spark, Impala, Hue, and Kafka, to create, query, and manage ETL data flowsworking knowledge of data connectors for embedding data visualizations from Tableau Server or PowerBI into SharePointKnowledge of Tableau Server and/or Microsoft PowerBIPython experience is a bonus"
"Job Opportunity :: Sr. Data Engineer :: 12 Months Contract to hire :: Fully Remote (Hybrid to Houston, TX)",Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3718993789/?eBP=JOB_SEARCH_ORGANIC&refId=vkk%2FjwP03Cxiw9XTXse90w%3D%3D&trackingId=y3UBwgT9Hja%2B%2Fqt9mCT1gg%3D%3D&trk=flagship3_search_srp_jobs,3718993789,"About the job
            
 
Hi, Hope you are doing well, Please find the requirement below , If you find yourself comfortable with the requirement please reply with your Updated Resume and I will get back to you or I would really appreciate if you can give me a call back at my contact number 302-721-5174Position: Sr. Data EngineerLocation: 100% Remote or Hybrid (Houston, TX)Duration: 12 Months Contract to hire Work Authorization: USC, GC Only Interview: Skype Job DescriptionThe Client has expressed a desire for the candidates to be able to work a hybrid 3days a week on-site, but they are willing to consider a remote candidate that has ""stellar"" skill set . This means , EOG would not have to cross train them into any of the tools being used in their environment. They would also need to have the ability of handling development independently as an individual contributor. As discussed before , this Client does not use large teams on big apps, they use data engineers on individual application issues.If you find a candidate who needs to stay remote like Sai, they have to have everything requested in the description andOver and above what the description requests , candidates would need to handle :Experience in PL/SQL development work experience with Oracle 19Discuss working experience with tools like Airflow, GitTell me about your python coding experience and REST API developmentIn your career , do you have any experience with accounting and financial dataData Engineer Consultant for Houston Client, and they can be under a VISA , Green Card or USC.""Typically working on big data and cloud technologies is not going to be helpful to us  such as; "" Hadoop ecosystem, Bigdata, PySpark, Spark, AWS, GCP, Big Query, Data Warehousing""Please note the following preferred skills as they truly need these to get the Manager's attention:Expert In PL/SQL Development Work Experience With Oracle 19Must have working experience with tools like Airflow, GitMust have python coding experience and REST API development experiencePlus Would Be Experience With Accounting And Financial DataClient would also like to see 2 code samples of their best code to gauge their coding style, and they can just show it to Client during the interview, They don't need copies of the code. Interview will be Teams based.They have commented , they don't want to see resumes just with cloud technology experience (like with Azure, AWS, GCS etc), they are not using those at site.Candidates need to have the drive to learn new technologies, and can pick up new technologies rapidly, able to self-manage a growing list of tasks and priorities.Remote or Hybrid: We prefer someone local to Houston, who is willing to come to the office at least 3 days a week. But remote option is ok, if you are able to find great candidates.Thank you and RegardsGaurav RajSr. Talent Acquisition Specialist -North AmericaDirect: +1 302-721-5174gaurav@steneral.com1007, N Orange St, 4th FL 329, Wilmington, DE 19801"
Data Engineer [Remote],SmartIPlace,United States (Remote),https://www.linkedin.com/jobs/view/3713002852/?eBP=JOB_SEARCH_ORGANIC&refId=vkk%2FjwP03Cxiw9XTXse90w%3D%3D&trackingId=8NvM%2FyKnXOEP9SMOHVZVVQ%3D%3D&trk=flagship3_search_srp_jobs,3713002852,"About the job
            
 
Title: Data Engineer [Remote]Experience: 8+ yearsVisa: USC OnlySkills Minimum of 5-10 years of IT/IS experienceNeed to have excellent communication skills5+ years working with Advanced Structured Query Language (SQL) & PL/SQL5+ years experience with at least Oracle relational database5+ years experience in software development lifecycle activities5+ years experience with data loading (ETL, ELT)5+ years working with data at scale 50+ TBExperience with data warehouses, operational data stores, data hubsExperience in end-to-end design of near-real-time and batch data pipelinesExperience working in an Agile environmentExperience developing detailed systems design and written test plansExperience preparing installation instructions and coordinating installation proceduresExperience documenting data audits, archiving, and restoration processesExperience with version control systems GitExperience with tracking and ticket software (Jira, Confluence, etc.)Familiarity with data architecture, data integration, data governance, and data lineage concepts
EDUCATIONBachelor's Degree in computer science or a related disciplineExperienceMinimum of 5-10 years of IT/IS experienceNeed to have excellent communication skills5+ years working with Advanced Structured Query Language (SQL) & PL/SQL5+ years experience with at least Oracle relational database5+ years experience in software development lifecycle activities5+ years experience with data loading (ETL, ELT)5+ years working with data at scale 50+ TBExperience with data warehouses, operational data stores, data hubsExperience in end-to-end design of near-real-time and batch data pipelinesExperience working in an Agile environmentExperience developing detailed systems design and written test plansExperience preparing installation instructions and coordinating installation proceduresExperience documenting data audits, archiving, and restoration processesExperience with version control systems GitExperience with tracking and ticket software (Jira, Confluence, etc.)Familiarity with data architecture, data integration, data governance, and data lineage conceptsNICE TO HAVELeading a teamMaster's DegreeHealth care experienceProfessional certificationsOther Programming Language ExperienceExperience in Machine learning or Artificial IntelligenceFamiliarity with microservicesFamiliarity with DevSecOps2+ years working with one NoSQL database (Hadoop)Experience with MongoDB, Redshift, Synapse, or others is a plus.Experience working in one public cloud environment (Azure, AWS, etc.)Snowflake familiarityExperience with containerization (Docker, Kubernetes, etc.)Familiarity with agile and lean methodologiesFamiliarity with JSON, XMLPERSONALWelcomes new approaches and innovative thinkingSelf-organized and responsible with experience in a distributed teamAble to multitask and be responsive/flexible to support customersAbility to work with others from diverse skill sets and backgroundsTakes ownership of a situation and sees it through the completionAble to switch context and complete work processes"
Staff Data Engineer,Begin,"Ontario, CA (Remote)",https://www.linkedin.com/jobs/view/3767886685/?eBP=JOB_SEARCH_ORGANIC&refId=vkk%2FjwP03Cxiw9XTXse90w%3D%3D&trackingId=FJBozPJWFPJYwGjtZssfJA%3D%3D&trk=flagship3_search_srp_jobs,3767886685,"About the job
            
 
BEGiN has an exciting opportunity for a Staff Data Engineer to join our growing team! This role will be remote in Ontario, Canada.BEGiN is an award-winning educational technology company with world-wide impact. With products that are as effective as they are fun, BEGiN’s family of brands builds critical skills for school and life.We’re a diverse team of talented people passionate about creating educational content kids love. At BEGiN, we have the rare opportunity to make a dent in the universe by bringing high-quality at-home learning to kids globally!Reporting into our Director, Data Engineering, the Staff Data Engineer will lead the design and implementation of the enterprise data warehouse models (i.e. data vault, data mart etc) to advance the data platform architecture and implement reliable data pipelines upholding the best practices that are pivotal to analytics, data science, and reporting across the organization.You will: Work as an architect for the enterprise data warehouse and ensure all deliverables according to the data platform roadmap.Own efficacy and quality of data pipelines and ETL processes that bring data into the enterprise data warehouse.Develop, maintain, and improve tools to enable team members to rapidly consume and understand data.Design and architect scalable infrastructure to build, train, and deploy machine learning models, ETL, and CI/CD with an eye on efficiency.Work hands on with multiple cloud technologies and tools (Python, PySpark, AWS, GCP, Databricks, etc.).
Responsibilities: Work closely with the business stakeholders, data scientists/analysts, and our engineering team to translate requirements into deliverable products.Execute the strategy for the data platform to support the business while optimizing performance and minimizing cost.Partner with stakeholders and engineering teams to deliver solutions in an iterative and incremental manner, leveraging lean and agile principles, fostering an environment of learning and collaboration.Ensure that our applications and operational data remain in sync and all integrations are flowing with no data errors.Lead root cause analysis, prioritize and manage data quality and remediation, and ensure data integrity to all downstream data systems.You will be an expert on understanding how data is collected, maintained, and interpreted and be knowledgeable on the official sources of data in scope to address use case requirements and business needs.
Must Haves: Bachelor degree in Computer Science or related field.Deep understanding of Spark (Databricks) and expertise on Data Warehousing approaches in the Databricks Lakehouse. Expert in Python/Scala and follow/evolve established SDLC, coding best practices, version control etc. Data Platform Architecture experience in AWS and/or GCP.Previous hands-on experience with developing data warehouses.5+ years of experience in data architecture and engineering.Excellent communication skills tailored for target audience.Strong data management skills with a focus on data warehouse (lakehouse) design, data quality management, and data analysis of large datasets, including hands-on-experience with SQL, no-SQL, and ETL software.Experience in BI tools (i.e. Looker).
Nice-to-Haves: Graduate degree in Computer Science or related field.Understanding of Analytics use cases (i.e. customer360, marketing channel optimization etc).Prior experience with AWS Infrastructure (Networking, VPCs etc).Prior experience with tools such as Fivetran, Airflow, Metarouter, Terraform etc.
We like people who: Are open to suggestions, collaborative, and thrive in team environments.Love and are willing to learn new technologies and styles.Are scrappy, entrepreneurial with the ability to turnaround high-quality projects quickly without depending on a large team.
What you’ll get: BEGiN offers competitive compensation including equity and benefits.Smart, passionate, and engaged co-workers.Paid time off. Including Holiday/Summer break.Unlimited sick time off.
The salary range for this role is $150,000 - $165,000 CAD.BEGiN is a proud equal opportunity employer. All qualified applicants will be considered without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status.At BEGiN, we are committed to building a diverse team of talented people who are passionate about creating educational content kids love. We believe in fostering a culture where productivity can flourish, one that is empathetic, respectful, and inclusive. At BEGiN, we know that diversity, equity, and inclusion aren’t just an idea, a one-time initiative, or phrases to throw into a job post: they’re a daily practice and an ongoing conversation. We survey our team about inclusivity, run training on DEI topics, and have a committee to ensure we are all continuing to learn and grow."
Senior Data Engineer,BambooHR,"Orlando, FL (Remote)",https://www.linkedin.com/jobs/view/3760295690/?eBP=JOB_SEARCH_ORGANIC&refId=vkk%2FjwP03Cxiw9XTXse90w%3D%3D&trackingId=44O3TgPXsl9N%2F7wr5o%2Be7A%3D%3D&trk=flagship3_search_srp_jobs,3760295690,"About the job
            
 
About UsOur mission is simple: we want to set people free to do meaningful work. People love our software—and it turns out that people love working here too. We've been recognized as a ""Best Company to Work For"" and we're proud of our team for creating software that makes an impact in the lives of HR pros and employees all over the world.What You'll DoAs a Senior Data Engineer on the data platform team, we'll rely on your expertise across multiple disciplines to develop, deploy and support data systems, data pipelines, data lakes, and lakehouses. Your ability to automate, performance tune, and scale the data platform will be key to your success.Your initial areas of focus will include: Collaborate with stakeholders to make effective use of core data assetsWith Spark and Pyspark libraries, load both streaming and batched dataEngineer lakehouse models to support defined data patterns and use casesLeverage a combination of tools, engines, libraries, and code to build scalable data pipelinesWork within an IT managed AWS account and VPC to stand up and maintain data platform development, staging, and production environmentsDocumentation of data pipelines, cloud infrastructure, and standard operating proceduresExpress data platform cloud infrastructure, services, and configuration as codeAutomate load, scaling, and performance testing of data platform pipelines and infrastructureMonitor, operate, and optimize data pipelines and distributed applicationsHelp ensure appropriate data privacy and securityAutomate continuous upgrades and testing of data platform infrastructure and servicesBuild data pipeline unit, integration, quality, and performance testsParticipate in peer code reviews, code approvals, and pull requestsIdentify, recommend, and implement opportunities for improvement in efficiency, resilience, scale, security, and performance
What You Need to Get the Job Done (if you don't have all, apply anyway!) Experience developing, scaling, and tuning data pipelines in Spark with PySparkUnderstanding of data lake, lakehouse, and data warehouse systems, and related technologiesKnowledge and understanding of data formats, data patterns, models, and methodologiesExperience storing data objects in hadoop or hadoop like environments such as S3Demonstrated ability to deploy, configure, secure, performance tune, and scale EMR and Spark Experience working with streaming technologies such as Kafka and KinesisExperience with the administration, configuration, performance tuning, and security of database engines like Snowflake, Databricks, Redshift, Vertica, or GreenplumAbility to work with cloud infrastructure including resource scaling, S3, RDS, IAM, security groups, AMIs, cloudwatch, cloudtrail, and secrets managerUnderstanding of security around cloud infrastructure and data systemsGit-based team coding workflows
Bonus Skills (Not Required, So Apply Anyway!) Experience deploying and implementing lakehouse technologies such as Hudi, Iceberg, and DeltaExperience with Flink, Presto, Dremio, Databricks, or KubernetesExperience with expressing infrastructure as code leveraging tools like TerraformExperience and understanding of a zero trust security frameworkExperience developing CI/CD pipelines for automated testing and code deploymentExperience with QA and test automationExposure to visualization tools like Tableau
Beyond the technical skills, we're looking for individuals who are: Clear communicators with team members and stakeholdersAnalytical and perceptive of patternsCreative in codingDetail-oriented and persistentProductive in a dynamic setting
If you love to learn, you'll be in good company. You'll likely have a Bachelor's degree in computer science, information systems, or equivalent working experience.An Equal Opportunity Employer--M/F/D/VBecause our team members are trusted to handle sensitive information, we require all candidates that receive and accept employment offers to complete a background check before being hired.For information on our Privacy Policy, click here."
Lead Data Engineer,"Dynatron Software, Inc.","Richardson, TX (Remote)",https://www.linkedin.com/jobs/view/3768749551/?eBP=JOB_SEARCH_ORGANIC&refId=vkk%2FjwP03Cxiw9XTXse90w%3D%3D&trackingId=ndY7%2BVcrdSWSo6%2FhVETMCQ%3D%3D&trk=flagship3_search_srp_jobs,3768749551,"About the job
            
 
Lead Data Engineer100% Remote | Full TimeWe are seeking a dynamic and experienced Lead Data Engineer to join our newly formed Data Platform Team. As a Lead Data Engineer, you will have the opportunity to collaborate with a team of data scientists, engineers, and analysts to create and maintain scalable data pipelines. You will be responsible for writing big data pipelines in Python and DBT transformations in SQL. The ideal candidate will have a strong technical background, a passion for data processing, and the ability to think creatively and strategically to solve complex challenges.Key Responsibilities: Write, test, and maintain robust, high-quality code in Python and DBT data models on Snowflake.Work with data and analytics experts to strive for greater functionality in our data systems.Collaborate with cross-functional teams to gather requirements and develop software solutions.Design, construct, install, test, and maintain highly scalable data management systems.Provide debugging and troubleshooting support for existing systems.Participate in code reviews to ensure software quality and adherence to standards.Assist in database design and the development of our Snowflake data warehouse.Leverage AWS and other cloud technologies for efficient software deployment and scalability.Use Python to process and clean data, manage ETL pipelines, and create automated workflows.
Minimum Qualifications: Bachelor's degree in Computer Science, Engineering, or a related field.5+ years of experience in designing, implementing, and maintaining relational/data warehousing environments.Strong proficiency in programming languages commonly used in data engineering, such as Python, SQL, and big data technologies like Snowflake, Spark, Kafka, and distributed computing frameworks.Experience working with data warehouses and relational databases.Familiarity with AWS and/or other cloud-based technologies.Experience with Big Data technologies such as Hadoop, Spark, Beam, Flink, or similar technologies is a plus.Excellent problem-solving skills with a strong attention to detail.Ability to work both independently and as part of a team.Excellent verbal and written communication skills.
Preferred Qualifications: Experience working on a project that involved multi-classificationExperience with hybrid cloud workflows
In Return for Your Expertise, You Will Receive: Excellent benefits including health, dental, and vision insurance, stock options, work from home and flexible scheduling depending on job requirements, professional development opportunities, 9 paid holidays, and 15 days PTO.Home office setup support for remote employees.A welcome “swag bag” with branded clothing as an official welcome to the team.The chance to work for an organization that puts people first and fosters a culture of teamwork, integrity, communication, accountability, and positive attitude!Dynatron Software is an Equal Opportunity Employer and encourages all qualified individuals to apply.
Compensation: $130,000 - $160,000 + Bonus Potential"
Data Engineer,ClifyX,United States (Remote),https://www.linkedin.com/jobs/view/3746299241/?eBP=JOB_SEARCH_ORGANIC&refId=vkk%2FjwP03Cxiw9XTXse90w%3D%3D&trackingId=hBudD4rYVdtBfkigQ%2F13gg%3D%3D&trk=flagship3_search_srp_jobs,3746299241,"About the job
            
 
For all the below Job Title : AI & GenAI (Generative Artificial Intelligence) Exp Mandatory --- Contract or FTE Any Location Open to Travel or Relocate Plz Check all the options... Client ---Client   4.Data Engineer: Proficiency in data warehousing, ETL (Extract, Transform, Load) processes, and data modeling. Strong knowledge of databases (SQL and NoSQL). Experience with big data technologies like Hadoop and Spark."
Data Engineer - GTM & Finance,Pinecone,United States (Remote),https://www.linkedin.com/jobs/view/3748536347/?eBP=JOB_SEARCH_ORGANIC&refId=vkk%2FjwP03Cxiw9XTXse90w%3D%3D&trackingId=EBfal62LNx4f60eD3tCTXg%3D%3D&trk=flagship3_search_srp_jobs,3748536347,"About the job
            
 
About PineconePinecone is on a mission to build the search and database technology to power AI applications for the next decade and beyond. Our fully managed vector database makes it easy to add vector search to AI applications. Since creating the “vector database” category, demand has grown incredibly fast and it shows in our user base.We are a distributed team with clusters in New York, San Francisco, Tel-Aviv, and Manchester.About The RolePinecone is seeking a skilled and highly motivated Senior Data Engineer to lay the foundation of our data practice across GTM and Finance organizations. As a Senior Data Engineer, you will collaborate with cross-functional teams to develop and deliver board reports and operational data that provide meaningful insights to guide business decisions and strategies.You will work in a fast-paced and rewarding environment that demands the highest quality work with minimal supervision. And as we all do a little bit of everything, you will also be a strong generalist, work directly with executive leadership, and mentor new data engineers and scientists.ResponsibilitiesOptimize Data Delivery and ScalabilityStreamline and enhance data delivery pipelines to improve efficiency and scalability, ensuring seamless access to data for various teams within the organization.Enable Insights and ReportingCollaborate with cross-functional teams to develop and deliver board reports and operational data that provide meaningful insights to guide business decisions and strategies.Enhance Operational EfficiencyImplement measures to improve the efficiency and effectiveness of data engineering operations, enabling smoother data processing and analysis.What we look for: A passion for technology 5+ years of experience with SQL and Python5+ years of experience in the Data Eng/Science spaceBS in Computer Science, Math, a related technical field or equivalent experienceStrong foundations in databases, warehousing, data infrastructure, ELT/ETLExperience working cross functionally to ground business strategy and process in data
Bonus Points: Experience with orchestration platforms Experience with metric and features storesExpertise working with cloud-based data warehouse solutions (BigQuery, Snowflake)Knowledge and experience in the GTM and Finance space for B2B products"
Data Engineer,PharmaLogic Holdings Corp.,United States (Remote),https://www.linkedin.com/jobs/view/3777118247/?eBP=JOB_SEARCH_ORGANIC&refId=vkk%2FjwP03Cxiw9XTXse90w%3D%3D&trackingId=alx4m52UsTU%2Bgh%2BA7ZP%2BmQ%3D%3D&trk=flagship3_search_srp_jobs,3777118247,"About the job
            
 
Our company is looking to fill the role of data engineer. This role will work closely with our operations and finance team to support our data warehouse and transform the way we read our data. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing our company’s data architecture to support our next generation of products and data initiatives.Responsibilities for data engineer Create and maintain optimal data pipeline architecture,Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. Help build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources. Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics. Work with stakeholders to assist with data-related technical issues and support their data infrastructure needs. 
The ideal candidate must have: A minimum of 2 years of relevant work experienceAdvanced working SQL, and MySQL development experienceExperience with PowerBi, Domo, or related BI productsExperience with BI / data visualization toolsWorking experience with Azure preferred and / or AWS. Experience in the fields of data warehousing, and business intelligence. Excellent computer science fundamentals and problem-solving skillsExperience in creating logical and physical data modelsExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. Experience with multiple programming languages. Strong analytic skills related to working with unstructured datasets. Data engineer certifications are a plus."
Data Engineer,Mattress Firm,United States (Remote),https://www.linkedin.com/jobs/view/3751156893/?eBP=JOB_SEARCH_ORGANIC&refId=vkk%2FjwP03Cxiw9XTXse90w%3D%3D&trackingId=kL1DhWhm%2FrsPCWHdU7Rz0w%3D%3D&trk=flagship3_search_srp_jobs,3751156893,"About the job
            
 
Mattress Fir is excited to announce the opening of the (Remote) Data Engineer role! The Data Engineer implements methods to improve data reliability and quality. The individual in this role combines raw information from different sources to create consistent and machine-readable formats. This role also develops and tests architectures that enable data extraction and transformation for predictive or prescriptive modeling.Essential Responsibilities Analyze and organize raw data Developing and maintaining datasets Improving data quality and efficiency Interpret trends and patterns Conduct complex data analysis and report on results Prepare data for prescriptive and predictive modeling Build algorithms and prototypes Combine raw information from different sources Explore ways to enhance data quality and reliability Identify opportunities for data acquisition Develop analytical tools and programs Complete all required training modules and certifications prior to the due date Ensure all safety policies and procedures are followed to ensure a safe work environment for all Communicate professionally with all internal and external contacts Follow all Company policies and execute Company standards on appearance and functionality as well as appropriate brand representation Communicates any concerns or issues to leadership to ensure proper efficiency of department and company operations 
Non-Essential Responsibilities Collaborate with data scientists and architects on several projects Execute company initiatives and other activities requested by supervisor Updates job knowledge by participating in educational opportunities. Contributes ideas on ways to optimize or improve the team, the department, and the Company 
Education Bachelor's Degree Degree in Computer Science, IT, or similar field required Master's Degree Degree in Computer Science, IT, or similar field preferred 
Professional Experience 3+ Years Previous experience as a data engineer or in a similar role required 3+ Years Hands-on experience with SQL database design required 
Skills List IT Expertise: Technical expertise with data models, data mining, and segmentation techniques Analytical : Great numerical and analytical skills 
Competencies Results Driven - Competitive Awareness Stays informed of industry trends and technology of the competitive landscape. Analyzes the information to improve overall team performance and future planning. Looks for ways to win in the local marketplace or in business unit. Surfaces ideas to improve competitiveness and identify obstacles. Communication - Listening Ability to accurately receive and interpret messages. Practices attentive, active listening with co-workers and/or guests with patience and can accurately restate opinions of others. Interpersonal Skills - Build Trust Ability to build a common identity with others that aligns with company goals and fosters mutual respect. Demonstrates integrity by living the company values in their relationships. Shows respect for other’s contributions and gives honest feedback that is positive and constructive. Seeks to understand before being understood. Mattress Firm Core Values Customer First, Integrity Always, Teams Win, Results Matter, Data Driven, Give Back Core Competencies - Technical Expertise, job skills and technical knowledge are up to date. Communication - Written Expressing yourself clearly to the organization in a variety of styles such as constructing a logical argument, electronic mail, note taking or summarizing. Reviews and edits materials to improve clarity while ensuring messages are targeted for the intended audience. Technical Proficiency - Initiative Maintains a high-level of energy and self-motivation to achieve goals. Looks beyond his/her job requirement to make a contribution and offer support to the overall organization. Technical Proficiency - Personal Development Quickly identifies when there is a need to change personal or interpersonal behavior, as well as leadership style. Is self-aware and knows how others respond to his/her influence and performance. Is sensitive to personal demands and requirements and is willing to adjust, accordingly. Technical Proficiency - Knowledge Understands the job requirements of the role and demonstrates both soft and hard skills needed to be proficient. Results Driven - Time Utilization Uses time effectively while concentrating on more important priorities. Quickly modifies behavior to deal effectively with change in the work environment. Efficiently manages shifting priorities to drive the best outcome for the business while supporting the team. Results Driven - Takes Measured Risks Willing to make difficult decisions while understanding when to gain consensus. Ability to calculate risk factors in measuring potential obstacles and assess possible outcomes. Consistently seeks information to support informed decision making. 
Knowledge SQL High Python High Cloud Platforms High 
Licenses and Certifications Data engineering certification (e.g GCP Data Engineer) is a plus Upon Hire preferred 
Physical DemandsOffice Environment Sitting for up to 8+ hours Standing occasionally Walking occasionally Talking frequently Hearing Frequently Usage of hands and fingers Reaching with hands and arms 
*Must be a US Citizen without the need for sponsorship*Pay Range$110,000-130,000Now don’t fall asleep out there… the sooner that we receive your application, the closer you are to the career of your dreams!DIVERSE CANDIDATES ARE ENCOURAGED TO APPLYMattress Firm is an equal employment opportunity employer and is committed to maintaining a non-discriminatory work environment, and does not discriminate against any applicant or employee for employment on the basis of race, color, religion, sex, national origin, age, disability, veteran status, marital status, sexual orientation, gender identity, or any other characteristic protected by applicable law. Mattress Firm is committed to creating a dynamic work environment that values diversity and inclusion, respect and integrity, customer focus, and innovation."
MDM Data Engineer,Apptad Inc.,United States (Remote),https://www.linkedin.com/jobs/view/3679206061/?eBP=JOB_SEARCH_ORGANIC&refId=vkk%2FjwP03Cxiw9XTXse90w%3D%3D&trackingId=2F3xCs51vAu0NJi1rXsZRQ%3D%3D&trk=flagship3_search_srp_jobs,3679206061,"About the job
            
 
Job Title: MDM Data EngineerJob Location: Canada (Hybrid/Remote)Job Duration: Long-TermJob Description Core skill set: Infirmatica MDM and Informatic ETL toolExperience in ETL tools with strong SQL skills, minimum 5+ yearsExperience in creating pipelines for Data warehouse and should be able to read/write data to/from Snowflake and MDMCreate and maintain data model standards, including master data management (MDM)Capture, validate, and publish metadata by enterprise data governance policies and MDM taxonomiesExperience in backend programming including schema and table design, stored procedures, Triggers, Views, and IndexesConduct data analysis, mapping transformation, data modeling and data-warehouse conceptsStrong working Experience with Agile, Scrum, Kanban, and Waterfall methodologies
Responsibilities Work on the Enterprise Metadata repositories for updating the metadata and was involved in Master Data Management [MDM]Work with the MDM systems team concerning technical aspects and generating reports
About UsApptad offers strategic consulting, enterprise information management and digital transformation services. With globally connected offices in US and India along with a team of trained and certified IT resources, Apptad ensures quick and effective delivery to its customers.Apptad is relentlessly reinventing the outlook of how companies leverage data.With an effort to enable our customers the ability to solve biggest problems within their organization.We perceive our clients’ problems and respond with custom solutions instead of handing over boilerplate responses.OUR MISSIONCustomer Focus: We listen carefully to the needs of our clients so that we know what’s important for their business and can design a customized solution for their business.Innovation: As a firm, we believe in constantly upgrading ourselves and improving our solutions to adapt to the changing landscape of technology.Accountability and Ethics: We believe in taking our commitments as seriously as our customers and living up to them while building trust for a long term business relationship."
Data Engineer/Data Intelligence #37690,INSPYR Solutions,United States (Remote),https://www.linkedin.com/jobs/view/3640964099/?eBP=JOB_SEARCH_ORGANIC&refId=vkk%2FjwP03Cxiw9XTXse90w%3D%3D&trackingId=NcW%2BZHFKgv%2B6OwZBjnGyug%3D%3D&trk=flagship3_search_srp_jobs,3640964099,"About the job
            
 
Healthcare Client Date Engineer Contract to Hire  Remote- Local to DMV Applies data extraction, transformation and loading techniques in order to connect large data sets from a variety of sources. Creates data collection frameworks for structured and unstructured data. Develops and maintains infrastructure systems (e.g., data warehouses, data lakes) including data access APIs. Prepares and manipulates data using Hadoop or equivalent. Employee is recognized as an expert within the organization and has in-depth and/or breadth of expertise in own discipline and broad knowledge of other disciplines within the function. Anticipates internal and/or external business challenges and/or regulatory issues; recommends process, product or service improvements. Solves unique and complex problems that have a broad impact on the business. Contributes to the development of functional strategy. Leads project teams to achieve milestones and objectives.  ESSENTIAL FUNCTIONS: 20% Leads project teams to achieve milestones and objectives. Creates data collection frameworks for structured and unstructured data. Applies data extraction, transformation and loading techniques in order to connect large data sets from a variety of sources. Develops data models by studying existing data warehouse architecture; evaluating alternative logical data models including planning and execution tables; applying metadata and modeling standards, guidelines, conventions, and procedures; planning data classes and sub-classes, indexes, directories, repositories, messages, sharing, hiding, replication, back-up, retention, and recovery.  20% Solves the most complex problems; takes a new perspective on existing solutions. Develops and maintains infrastructure systems (e.g., data warehouses, data lakes) including data access APIs. Recommends process, product or service improvements. Solves unique and complex problems that have a broad impact on the business. Contributes to the development of functional strategy. Acts as a mentor for colleagues with less experience.  20% Manage the data collection process providing interpretation and recommendations to management. Executes quantitative analyses that translate data into actionable insights. Provides analytical and data-driven decision-making support for key projects. Designs, manages, and conducts quality control procedures for data sets using data from multiple systems.  15% Develops and maintains infrastructure systems (e.g., data warehouses, data lakes) including data access APIs. Prepares and manipulates data using various technologies (e.g. Hadoop or equivalent MapReduce platform). Defines, designs and build dimensional databases and data pipelines to support analytics projects.  15% Oversees the delivery of engineering data initiatives and projects. Supports long term data initiatives as well as Ad-Hoc analysis and ETL activities. Creates data collection frameworks for structured and unstructured data. Applies data extraction, transformation and loading techniques in order to connect large data sets from a variety of sources.  10% Improves data delivery engineering job knowledge by attending educational workshops; reviewing professional publications; establishing personal networks; benchmarking state-of-the-art practices; participating in professional societies. Provide technical guidance and support to developers, data engineers and data administrators. Develop strategies for data acquisitions, recovery and implementations.  Qualifications To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The requirements listed below are representative of the knowledge, skill, and/or ability required. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.  Education Level: Bachelor's Degree  Education Details: Information Technology or Computer Science  Experience: 10 years Experience leading database design and developing modeling tools. Experience in leading data engineering and cross functional teams to implement scalable and fine tuned ETL/ELT solutions for optimal performance. Experience developing and updating ETL/ELT scripts. Hands-on experience with application development, relational database layout, development, data modeling.  In Lieu of Education In lieu of a Bachelor's degree, an additional 4 years of relevant work experience is required in addition to the required work experience.  Preferred Qualifications Knowledge, Skills and Abilities (KSAs) Expert in at least one programming language (i.e., SQL, NoSQL, Python)., Expert Knowledge of multiple database technologies - structured and un-structured., Expert Ability to quick learn new technology and take direction., Expert Strong customer service orientation., Expert Provide direction to and lead technical teams., Expert Requires strong organizational with the ability to handle multiple priorities., Expert Excellent communication skills both written and verbal., Expert  The incumbent is required to immediately disclose any debarment, exclusion, or other event that makes them ineligible to perform work directly or indirectly on Federal health care programs. Must be able to effectively work in a fast-paced environment with frequently changing priorities, deadlines, and workloads that can be variable for long periods of time. Must be able to meet established deadlines and handle multiple customer service demands from internal and external customers, within set expectations for service excellence. Must be able to effectively communicate and provide positive customer service to every internal and external customer, including customers who may be demanding or otherwise challenging.  Licenses/Certifications Data Management\Certified Analytics Professional (CAP) Upon Hire"
Lead Data Engineer _ Remote,Ekodus INC.,"Cincinnati, OH (Remote)",https://www.linkedin.com/jobs/view/3730306480/?eBP=JOB_SEARCH_ORGANIC&refId=vkk%2FjwP03Cxiw9XTXse90w%3D%3D&trackingId=tcRGULz4AOc8UBODjAYfYw%3D%3D&trk=flagship3_search_srp_jobs,3730306480,"About the job
            
 
Title: Lead Data Engineer Location: Cincinnati,OHDuration: 6 MonthsThe Data Engineer will be part of our Debit Modernization workstream joining our one of our Debit squads and assisting with updating how the bank ingests and publishes our Debit product data as part of our larger platform modernization. The individual will be responsible for building mechanisms to receive data from an external partner (file batch and real time events via kafka) and route to our downstream teams for reporting and operational processing.General FunctionDesigns and implements software & support solutions as a member of an agile squad. Software in scope are 3rd party software applications which support finance and regulatory reporting business functions. Technical support functions include ETL, file transfer services, job scheduling, multiple environment support, data quality, upgrades, incident & problem resolution, etc. Being assigned to an agile squad means this role also participates in all agile ceremonies driving activities from design to delivery. Follows best practices and standards and participates in communities of practice to continuously refine and document these standards, following any required compliance & governance requirements.Responsible and accountable for risk by openly exchanging ideas and opinions, elevating concerns, and personally following policies and procedures as defined. Accountable for always doing the right thing for customers and colleagues, and ensures that actions and behaviors drive a positive customer experience. While operating within the Bank's risk appetite, achieves results by consistently identifying, assessing, managing, monitoring, and reporting risks of all types.Essential Duties And Responsibilities Provide technical knowledge, leadership and collaboration as an ETL developer & designerDevelop and maintain data transfer jobs & schedulers, as required, including to identify & execute opportunities to automateAchieve operational excellence by automating processes and writing maintainable, supportable, and testable codeAssist with problem resolution for end users and customers, including incident & problem managementSupport both application and environment upgrades, new implementations & patches, as required.Develop software meeting code quality standards and metricsParticipate in communities of practice by contributing to and following standards, test driven development, reviewing others code, and sharing knowledgeMaintain effective partnerships with operations and engineering teams to drive service improvementRemain current on IT trends pertaining to their area of practiceContribute to the definition of operational procedures for software developmentMaintain appropriate controls and documentation to ensure compliance of audit requirements
Minimum Knowledge, Skills And Abilities Required Bachelor's degree in Computer Science/Information SystemsUnderstanding of Object-Oriented Programming LanguagesUnderstanding of Software Development LifecyleStrong SQL Skills with ability to perform ETLFamiliarity with relational database architecture techniques like EDW, Postgrese, etcDemonstrated practice for scripting languages, like Python, Java, PowershellStrong Windows Server Experience and/or Application Support ExperiencePrior experience with Alation, Snowflake, NIFI/KAFKA, PowerBI, Tableau, & Git is a plusUnderstanding of Agile Software Development methodologiesUnderstanding of data management and info security best practicesDemonstrated problem solving skillsDemonstrated collaboration skillsExcellent verbal and written communication skillsPrior experience working for a financial institution or with Debit Cards a plus
Technical SkillsMust Have Ability to write complex procedures/views/SQLApache KafkaApache NIFI & SQLCloud Data Warehousing - Snowflake preferredDataStage, SQL, understanding of dimensionally-designed databases, and experience interacting with a relational database (DB2, Oracle, etc.). Mainframe experience would be a bonus, but not required. Ability to analyze complex data flows and understand existing jobs.
Please share your resume to career@ekodusinc.com"
100% remote - Azure data Engineer with facets experience,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3638461482/?eBP=JOB_SEARCH_ORGANIC&refId=vkk%2FjwP03Cxiw9XTXse90w%3D%3D&trackingId=VcDjg7SWBj%2FhvhalMk6hNA%3D%3D&trk=flagship3_search_srp_jobs,3638461482,"About the job
            
 
They have a claims data warehouse on prem that they are migrating to Azure. Make sure they have experience with this in their background and on the resume.  Need to have claims processing experience (Payer claim) . Facets integration with the Azure platform. Azure ADF, Engineering Experience ( from an engineering standpoint) Building of a Relational Database for the datasets. 
**Job Description: Azure Claims Processing and Database EngineeringWe are seeking a skilled professional to join our team as a Claims Processing and Database Engineer with expertise in Azure, Facet integration, relational database engineering, Azure Data Factory, and on-premises migration to a data warehouse using Azure. In this role, you will be responsible for designing, developing, and maintaining efficient and scalable data processing systems, specifically focused on claims processing within the healthcare industry.ResponsibilitiesFacet Integration with Azure: Collaborate with cross-functional teams to integrate the Facet claims processing system with Azure cloud services. Ensure seamless data flow, security, and performance optimization.Relational Database Engineering: Design and implement relational database models to support efficient data storage and retrieval. Optimize database performance and ensure data integrity through effective indexing, partitioning, and normalization techniques.Azure Data Factory: Utilize Azure Data Factory to orchestrate data pipelines, extract, transform, and load (ETL) processes, and data integration across various data sources. Create and manage data workflows to enable efficient data processing and data movement within the Azure ecosystem.On-premises Migration to Data Warehouse using Azure: Collaborate with stakeholders to plan and execute the migration of on-premises data to a cloud-based data warehouse using Azure services. Ensure a smooth transition while maintaining data integrity, security, and performance.Data Processing and Transformation: Develop data processing logic and algorithms to transform and cleanse claims data for analysis and reporting purposes. Implement data quality checks and validations to ensure accuracy and reliability.Performance Optimization: Identify performance bottlenecks and optimize data processing workflows, database queries, and data storage mechanisms to improve system performance and efficiency.Security and Compliance: Implement appropriate security measures, data access controls, and encryption techniques to protect sensitive data. Ensure compliance with industry regulations and data privacy standards.RequirementsBachelor's degree in Computer Science, Information Systems, or a related field.Strong experience with Azure cloud services, including Azure Data Factory, Azure SQL Database, and Azure Storage.Proficiency in relational database concepts, database design, and SQL programming.Experience with Facet claims processing system integration is highly desirable.Hands-on experience with data migration projects, specifically involving on-premises to cloud migration using Azure.Strong problem-solving skills and the ability to analyze complex data-related issues.Knowledge of data warehousing concepts and best practices.Familiarity with healthcare claims processing and related industry standards (e.g., HIPAA) is a plus.Excellent communication and collaboration skills to work effectively with cross-functional teams and stakeholders.Join our team and contribute to the development of cutting-edge claims processing and data engineering solutions using Azure. Help us streamline processes, enhance data accuracy, and drive actionable insights for improved business outcomes."
Data Engineer,"Changeis, Inc.","Arlington, VA (Remote)",https://www.linkedin.com/jobs/view/3759561188/?eBP=JOB_SEARCH_ORGANIC&refId=vkk%2FjwP03Cxiw9XTXse90w%3D%3D&trackingId=bQIYgKysEdX2UJWli2GYPg%3D%3D&trk=flagship3_search_srp_jobs,3759561188,"About the job
            
 
Changeis, Inc. is an award-winning 8(a) certified, woman-owned small business that provides management consulting and engineering services to the public sector. Changeis' work has resulted in the successful execution of numerous programmatic initiatives, development of acquisition-sensitive deliverables, and establishment of a variety of long-term innovative strategic priorities for its customers. Changeis focuses on delivering unparalleled expertise in the areas of strategy and transformation management, investment analysis and acquisition management, governance, and innovation management. Inc. magazine has ranked the management consulting firm, Changeis Inc., among the top 1000 firms on its 35th annual Inc. 5000, the most prestigious ranking of the nation's fastest-growing private companies. Changeis offers a full benefit package that includes medical, dental, and vision, short and long term disability, retirement plan with immediate vesting and company match, and a generous annual leave plan.The Data Engineer will partner with a Federal Agency Office of Human Resources, focusing on essential areas such as business management, strategic planning, and decision-making. By developing and maintaining data architectures, engaging in acquisition/contract management, and applying expertise in information technology, data analytics, and knowledge management, the Data Engineer will significantly contribute to the optimization and innovation of organizational processes. The Data Engineer will collaborate with product design and engineering teams to understand their needs, and then research and devise innovative statistical models for data analysis. By communicating findings to all stakeholders and using analytics for meaningful insights, they will enable smarter business processes and stay abreast of current technical and industry developments.Roles And Responsibilities  Collaborating with clients and end-users to understand their mission, architecture, and security requirements.  With a focus on the client's goals, build a design that will scale to meet their evolving needs.  Recommend tools and capabilities based on your research of the environment and new technology.  Design the standard for future development, so you'll craft an architecture that smoothly works with existing infrastructure without compromising security.  Identify new opportunities to build platform-based solutions to help your customers meet their toughest challenges.  Developing and running ETL (Extract, Transform, Load) processes to manage data flow and ensure data quality. 
Requirements  8+ years of experience with designing, developing, operationalizing, and maintaining complex data applications at enterprise scale.  5+ years of experience with creating software for retrieving, parsing, and processing structured and unstructured data.  5+ years of experience with developing scalable ETL workflows for reporting and analytics.  5+ years of experience with using Python, SQL, Scala, or Java  3+ years of experience with AWS.  3+ years of experience with using Docker, Kubernetes, and Helm.  3+ years of experience with Distributed data and computing tools, including Spark, Databricks, Hadoop, Nifi, Kafka, Scala, or Java.  Experience with developing scripts and programs for converting several types of data into usable formats, supporting project team to scale, and monitoring and operating data platforms.  Ability to supervise others and lead projects and deliverables in a collaborative, cross-functional team environment.  Prefer candidates holding active AWS certifications."
REMOTE - SR DATA ENGINEER,Skiltrek,"Paradise Valley, AZ (Remote)",https://www.linkedin.com/jobs/view/3768036451/?eBP=JOB_SEARCH_ORGANIC&refId=vkk%2FjwP03Cxiw9XTXse90w%3D%3D&trackingId=sxmxjuqfWgSpghNqzeTMKg%3D%3D&trk=flagship3_search_srp_jobs,3768036451,"About the job
            
 
Job DescriptionSkiltrek is looking for a Senior Data Engineer to join one of the largest digital credential service companies in the nation. They support institutions and students share and verify credentials in a secure way. Data has become a primary focus at this organization. They're seeking a highly motivated data engineer for a key role in maintaining and developing data pipelines for data products and analytics. This position will report to the Data Analytics and Science Manager. The data stack includes both our application supporting databases as well as our data analytics infrastructure. Some of the technologies we use include PostgreSQL, MySQL, Redshift, S3, Meltano, Airflow, Python, and Kubernetes.You will work with the Data Engineering team to develop and maintain data pipelines. You will be responsible for setting a high level of excellence and expectations when it comes to how engineering teams access and use production databases. You will support data analysts and Product Managers with data related needs.Other Responsibilities IncludeDesign and build data pipelines using TDD best practices, and a Quality First approachDevelop solutions that can be implemented via CI/CD pipelinesRecommend changes to to existing databases to improve performance or resolve issuesWork towards expanding our Data Lake, and moving more into cloud based solutionsWork with a team of engineers and analysts to upskill everyone, including yourselfMinimum Requirements  5 years of experience with ELT/ETL data pipeline development and maintenance Proven experience and expertise using Python, SQL, and AWS cloud storage such as AWS S3 Experience with developing in a multi environment (Dev, QA, Prod, etc.) and DevOps procedures for code deployment/promotion. Strong understanding of OLAP database design and proficiency utilizing a database such as Redshift or Snowflake Experience with Data Lake design and proficiency utilizing HUDI or Parquet Experience managing and deploying code using a source control product such as GitLab/GitHub
Desired SkillsInsight Global is looking for a Senior Data Engineer to join one of the largest digital credential service companies in the nation. They support institutions and students share and verify credentials in a secure way. Data has become a primary focus at this organization. They're seeking a highly motivated data engineer for a key role in maintaining and developing data pipelines for data products and analytics. This position will report to the Data Analytics and Science Manager. The data stack includes both our application supporting databases as well as our data analytics infrastructure. Some of the technologies we use include PostgreSQL, MySQL, Redshift, S3, Meltano, Airflow, Python, and Kubernetes.You will work with the Data Engineering team to develop and maintain data pipelines. You will be responsible for setting a high level of excellence and expectations when it comes to how engineering teams access and use production databases. You will support data analysts and Product Managers with data related needs."
Data Engineer,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3729200430/?eBP=JOB_SEARCH_ORGANIC&refId=vkk%2FjwP03Cxiw9XTXse90w%3D%3D&trackingId=9lht2avkVbldXYfnKMPu2A%3D%3D&trk=flagship3_search_srp_jobs,3729200430,"About the job
            
 
Data EngineerRemote1 to 5 years experienceEast Coast PreferredMust have skills:  Design and develop an end-to-end Camunda application. Have experience with business rules and decision tables. Have experience in Java technologies.
Additional Skills  Ability to understand Business Problem and provide BPM centric solution through digitized process, automations, User Interventions, and well-defined User Experience. Conversant with latest versions and features of Camunda (ver 8) and be able to maintain application with reusable components for rapid application building in future
BPM Solution Developer with Camunda Experience Camunda Developer with 3+ years of experience in BPM space and with minimum 1+ years of working experience on Camunda BPM. Roles and Responsibilities Integrate and maintain Camunda BPM solutions through the systems development lifecycle. Hands-on development, coding, debugging of Camunda BPM applications. Skills: Camunda BPM Development, Java/J2EE.  Required Skills: Design and develop an end-to-end Camunda 8 application. Have experience with business rules and decision tables. Have experience in Java technologies. Ability to understand Business Problem and provide BPM centric solution through digitized process, automations, User Interventions, and well-defined User Experience. Conversant with latest versions and features of Camunda (ver 8) and be able to maintain application with reusable components for rapid application building in future business cases.  Preferred Skills Hands on experience with BPMN, J2EE technologies, REST, open-source products, database, and ability to review variety of code. Have experience integrating with an external content management system. Have experience building dashboards for Camunda Operate. Behavioral Skills: Team player and a good communicator. Result oriented with ability to work with virtual/distributed teams."
Data Engineer II,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3757573233/?eBP=JOB_SEARCH_ORGANIC&refId=vkk%2FjwP03Cxiw9XTXse90w%3D%3D&trackingId=lxlU9xG3tXbBerb87RiG8Q%3D%3D&trk=flagship3_search_srp_jobs,3757573233,"About the job
            
 
Remote roleNeed Valid LinkedInNeed photo ID [ no info must be hidden/blacked out in the IDs]Need 1-2 strong candidates only on this. Must be able to obtain a clearance7+ years of development experience building data pipelines using Cloud technologies.5+ years of experience in architecture of modern data warehousing 5+ platforms using technologies such as Snowflake or RedshiftCloud experience – S3, step functions, Glue, Step functions and Airflow .Good Python development for data transfers and extractions (ELT and ETL)."
GCP Data Engineer,United Consulting Hub,United States (Remote),https://www.linkedin.com/jobs/view/3689044361/?eBP=JOB_SEARCH_ORGANIC&refId=vkk%2FjwP03Cxiw9XTXse90w%3D%3D&trackingId=45OQRZXBoAToIJ55U%2FCVJA%3D%3D&trk=flagship3_search_srp_jobs,3689044361,"About the job
            
 
Role: GCP Data EngineerLocation: Pleasanton, USA (Hybrid) or RemoteDuraction : long termRequired Past ExperienceExp: 4-6Years 1+ years of overall experience in architecting, developing, testing & implementing Data Platform projects using GCP Components (e.g. BigQuery, Dataflow, Dataproc,DLP,BigTable,Pub/Sub,Compos etc..).Good Understanding of Data Structures.Worked with large datasets and solving difficult analytical problems.Experience working with GIT for Source Code ManagementWorked with Structured and Unstructured dataE2E Data Engineering and Lifecycle (including non-functional requirements and operations) management.Worked with client teams to design and implement modern, scalable data solutions using a range of new and emerging technologies from the Google Cloud PlatformAutomating manual processes to speed up delivery.Good Understanding of Data Pipeline (Batch and Streaming) and Data GovernanceExperience in code deployment from lower environment to production. Good communication skills to understand business requirements.Work closely with offshore team in India
Required Skills And AbilitiesMandatory Skills - BigQuery ,Composer, Python, GCP Fundamentals.Secondary Skills Snowflake, DLP, Pub/Sub, Dataflow,Shell Scripting,SQL, Security(Platform & Data) concepts. Knowledge of ETL Migration from On-Premises to GCP CloudSQL Performance TuningBatch/Streaming Data ProcessingFundamentals of Kafka,Pub/Sub to handle real-time data feeds.Good To Have - Certifications in any of the following: GCP Professional Cloud Architect, GCP Professional Data EngineerAbility to communicate with customers, developers, and other stakeholders.Mentor and guide team membersGood Presentation skillsStrong Team Player"
Data Engineer,Treez Inc.,United States (Remote),https://www.linkedin.com/jobs/view/3748420103/?eBP=JOB_SEARCH_ORGANIC&refId=vkk%2FjwP03Cxiw9XTXse90w%3D%3D&trackingId=my7zv8Z5k5jMRQJSH2X1Nw%3D%3D&trk=flagship3_search_srp_jobs,3748420103,"About the job
            
 
Treez is not for everyone. Are you right for Treez?The Company Values At Treez Are Stay CuriousPresent SolutionsEmbrace SimplicityEncourage CandorDrive to Outcomes
With SPEED, Treez team members are making an impact at our growing startup. We are motivated by our mission and do what it takes to get the job done. We need you to be as passionate as we are about making Treez the leader in powering the global cannabis economy. We are looking for hustlers, doers, people who see the goal and drive to it.We are seeking a skilled and experienced Data Engineer / Architect to join our dynamic team.What You Will DoAs a Data Engineer at Treez you will be responsible for designing the data architecture for our organization. You will collaborate with and mentor cross-functional teams, including engineers, analysts, and business stakeholders, to ensure the integrity, availability, and security of our data assets. Your expertise in data modeling, database design, and data integration will play a vital role in driving data-driven decision-making and supporting our business objectives..How You Will Do It Develop and maintain an enterprise-wide data architecture strategy that aligns with the organization's business goals and objectives. Define and implement data standards, principles, and guidelines for data modeling, database design, and data integration.Design and create logical and physical data models that meet the requirements of various business functions. Oversee the defining of data entities, relationships, attributes, and hierarchies to ensure data integrity and consistency across different systems and applications.Identify and design efficient data integration solutions to facilitate seamless near real time data flow between different systems and platforms. Develop strategies for data extraction, transformation, and loading, ensuring high data quality and reliability.Establish and enforce data governance policies and procedures to ensure compliance with regulatory requirements and industry best practices. Define data ownership, access controls, data retention policies, and data quality standards.Provide mentorship and guidance to engineers, and other team members. Share your expertise and knowledge to foster their professional growth and enhance their understanding of database design, SQL, query evaluation and performance tuning.Collaborate with cross-functional teams, including engineers, analysts, and business stakeholders, to understand data requirements and translate them into actionable data architecture solutions. Effectively communicate complex technical concepts to non-technical stakeholders.Stay abreast of industry trends, emerging technologies, and best practices in data architecture and management. Evaluate and recommend new tools and technologies to enhance data processing, analytics, and visualization capabilities.Create and maintain comprehensive documentation of data models, data flows, data dictionaries, and technical specifications. Ensure that documentation is up to date and accessible to relevant stakeholders.
What You Will Need Minimum 5+ years of software engineering experiencePreferred 3+ as Data Engineer or Architect preferably at a SaaS software company.Strong understanding of data architecture principles, methodologies, and best practices.Proficiency in relational database management systems, particularly AWS RDS, Postgres, and MySQL.Experience with data integration technologies and ETL tools a plusIn-depth knowledge of AWS data services such as AWS Redshift, AWS Athena, and AWS S3 a plusUnderstanding of data governance frameworks, data quality management, and data security practices.Strong analytical, problem-solving, and critical-thinking skills.Excellent communication and interpersonal skills to effectively collaborate with stakeholders at all levels.Ability to work independently and manage multiple priorities in a fast-paced environment.Bachelor's or Master's degree in Computer Science, Information Systems, or a related field.
The anticipated compensation for this role is base salary of $155,000 to $185,000 USD depending upon a number of factors such as a candidate's qualifications, skills, competencies, work experience, geographic location, business needs and market demands. The base pay range is subject to change and may be modified in the future. Benefits That Treeple Enjoy A remote first work environmentMedical, dental, vision and 401(K) - no match yet, we're a startupEquity
COME AS YOU ARETreez continually strives to create a diverse and inclusive environment. Treez provides equal employment opportunities to all job applicants and employees and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.About TreezTreez is the leading enterprise cloud commerce platform that streamlines retail and supply chain operations within the cannabis market. Through its innovative technology for retailers and brands, the company provides a robust breadth and depth of software solutions required to operate a successful modern dispensary.Solutions include point of sale, dispensary inventory management, omnichannel sales capabilities and multiple cashless payment options all on a mission-critical platform that ensures regulatory compliance across every supply chain transaction. The innovative platform also connects essential brands with their retailers through a centralized brand catalog with real-time market insights. The extensible open API platform provides smooth integration into a variety of best-of-breed solutions, including CRM, marketplace, cashless payments and data analytics across the partner ecosystem, giving retailers everything they need to grow their business."
GCP Data Engineer (W2 Position - Remote),Megan Soft Inc,United States (Remote),https://www.linkedin.com/jobs/view/3717097161/?eBP=JOB_SEARCH_ORGANIC&refId=vkk%2FjwP03Cxiw9XTXse90w%3D%3D&trackingId=YpjaZ2E4fOcXvgvHoiundA%3D%3D&trk=flagship3_search_srp_jobs,3717097161,"About the job
            
 
Role: Sr GCP Data Engineer Location: Remote / W2 Position )Direct Client : FORDNote: HRA TEST RequiredPosition DescriptionCloud Software Engineer Position Overview At Ford Motor Company, we believe freedom of movement drives human progress. We also believe in providing you with the freedom to define and realize your dreams. With our incredible plans for the future of mobility, we have a wide variety of opportunities for you to accelerate your career potential as you help us define tomorrow's transportation. Global Data Insight & Analytics organization is looking for a Machine Learning Engineer focused on building and driving the strategy forward for our internal Data Science / AI/ML platform. This role will work in a small, cross-functional team. The position will collaborate directly and continuously with other engineers, business partners, product managers and designers, and will release early and often. The team you will be working on is focused on building Mach1ML platform an AI/ML enablement platform to democratize Machine Learning across Ford enterprise (similar to Uber's Michelangelo, Facebook's FBLearner, etc). Position Responsibilities Work closely with Tech Anchor, Product Manager and Product Owner to deliver MLOPs platform in GCP using Python and other tools for the data scientists across the company. Work with software and ML engineers/Data Scientist to tackle challenging AIOps problems. Maintain and mange current CI/CD ecosystem and tools Find ways to automate and continually improve current CI/CD processes and release processes Examine, inspect codes / scripts and resolve issues Help innovate standardize machine learning development practices. Experiment, innovate and share knowledge with the team. Lead by example in use of Paired Programming for cross training/upskilling, problem solving, and speed to delivery. Leverage latest ML/NLP/ GCP/AIOPs/Kubernetes technologiesSkills RequiredA Bachelor's degree in Computer Science / Computer Engineering or similar technical discipline. 2+ years of experience with Cloud Engineering / Services 3+ years of work experience as a backend software engineer in Python with exceptional software engineering knowledge. Advanced working knowledge of object-oriented/object function programming languages: Python, C/C++, Julia Experience in DevOps: Jenkins/Tekton etc. Experience with ML workflow orchestration tools: Airflow, Kubeflow etc. Experience with cloud services, preferably GCP Services like Vertex AI, Cloud Function, BigQuery etc. Experience in container management solution: Kubernetes, Docker Experience in scripting language: Bash, PowerShell etc. Experience with Infrastructure as code: Terraform etcSkills PreferredMaster focused in Computer Science / Machine Learning (highly preferred). Experience working with Google Cloud platform(GCP) specifically Google Kubernetes engine, Terraform, and infrastructure Experience in delivering cloud engineering product Experience in programming concepts such as Paired Programming, Test Driven Development, etc. Understanding of MLOPs/Machine Learning Life Cycle and common machine learning frameworks: sklearn, tensorflow, pytorch etc. is a big plus Must be a quick learner and open to learning new technology. Experience applying agile practices to solution delivery. Experience in all phases of the development lifecycle. Must be team-oriented and have excellent oral and written communication skills. Good organizational and time-management skills. Must be a self-starter to understand existing bottlenecks and come up with innovative solutions. Knowledge of coding and software craftsmanship practices.Experience RequiredMaster focused in Computer Science / Machine Learning (highly preferred). Experience working with Google Cloud platform(GCP) specifically Google Kubernetes engine, Terraform, and infrastructure Experience in delivering cloud engineering product Experience in programming concepts such as Paired Programming, Test Driven Development, etc. Understanding of MLOPs/Machine Learning Life Cycle and common machine learning frameworks: sklearn, tensorflow, pytorch etc. is a big plus Must be a quick learner and open to learning new technology. Experience applying agile practices to solution delivery. Experience in all phases of the development lifecycle. Must be team-oriented and have excellent oral and written communication skills. Good organizational and time-management skills. Must be a self-starter to understand existing bottlenecks and come up with innovative solutions. Knowledge of coding and software craftsmanship practices.Experience PreferredExperience and good understanding of GCP processing /DevOPs/ Machine LearningEducation RequiredA Bachelor's degree in Computer Science / Computer Engineering or similar technical discipline.Education PreferredAdditional Safety Training/Licensing/Personal Protection Requirements:Additional InformationIf the candidate is remote only, please indicate ""100% Remote"" under candidate's name on resume. Other similar schedule notations can be ""Local Candidate"" or ""Hybrid Candidate."" HRA TEST Required"
Data Engineer,ASCENDING Inc.,United States (Remote),https://www.linkedin.com/jobs/view/3776517687/?eBP=JOB_SEARCH_ORGANIC&refId=Ly4j4gR%2BFb0O2prgwdRS2Q%3D%3D&trackingId=xY3dyhvnjeDpcdwLdsGm1A%3D%3D&trk=flagship3_search_srp_jobs,3776517687,"About the job
            
 
Our client, one of the largest Amazon Web Services (AWS) partners for data services, is looking for a true Mid level Big Data Engineer to contribute to join their team of technologists to build and contribute to large-scale, innovative projects. Technological and career growth opportunities are a natural and every day part of the working environment.  This role is only available for W2 or individual contracts. Please no C2C.  100% Remote Work.
Responsibilities Analyze system requirements and design responsive algorithms and solutions.Use big data and cloud technologies to produce production quality code.Engage in performance tuning and scalability engineering.Work with team, peers and management to identify objectives and set priorities.Perform related SDLC engineering activities like sprint planning and estimation.Work effectively in small agile teams.Provide creative solutions to problems.Identify opportunities for improvement and execute.
Requirements Minimum 5 years of proven professional experience working in the IT industry.Degree in Computer Science or related domains.Experience with cloud based Big Data technologies.Experience with big data technologies like Hadoop, Spark and Hive.AWS experience is a big plus. Proficiency in Hive / Spark SQL / SQL. Experience with Spark.Experience with one or more programming languages like Scala & Python & Java.Ability to push the frontier of technology and independently pursue better alternatives.Kubernetes or AWS EKS experience will be a plus.
Thanks for applying!"
Senior Data Engineer,TrustEngine,United States (Remote),https://www.linkedin.com/jobs/view/3722210170/?eBP=JOB_SEARCH_ORGANIC&refId=Ly4j4gR%2BFb0O2prgwdRS2Q%3D%3D&trackingId=WDcs1VdjEzfmDIY2aXEehw%3D%3D&trk=flagship3_search_srp_jobs,3722210170,"About the job
            
 
Job Summary:TrustEngine is on a mission to revolutionize the mortgage tech industry and empower borrowers to achieve their financial goals. With our Borrower Intelligence Platform and the ethical use of data and machine learning, we equip lenders with the tools for meaningful conversations that support borrowers in pursuing their life goals. We're transforming the industry, fostering trust, making a positive impact on lives, and creating a future of #NoBorrowerLeftBehind. Join us on this thrilling journey. Unlock the full potential of our organization and accelerate our growth!Bring your deep expertise in building out data pipelines and implementing data lake architecture. As a senior data engineer, you will elevate your team, foster collaboration, and drive continuous improvement to deliver industry-transforming products with technical excellence. You will contribute to the success of the company by designing and implementing a scalable data platform as the foundation of our tech stack. Collaborating with cross-functional teams, you will serve as a subject matter expert to drive data-driven decision-making and create innovative solutions for our customers.Join us and shape the future of mortgage tech!Responsibilities: Design and develop robust data pipelines and ETL processes for ingesting, transforming, and integrating data from various sources into the data lakeOptimize data infrastructure for performance, reliability, and scalability, considering factors such as data volume, velocity, and varietyEnsure data quality, integrity, and security across the data lake, implementing data governance practices and monitoring mechanismsCollaborate with cross-functional teams, including data scientists, analysts, and software engineers, to understand data requirements and develop solutions that meet business needsChampion engineering best practices and drive improvements in data engineering methodologies, processes, and technologiesStay updated with emerging trends and advancements in data engineering, sharing knowledge and insights with the team and the broader organizationEffectively communicate technical concepts, solutions, and recommendations to both technical and non-technical stakeholders
RequirementsRequired Qualifications: 4+ years of professional experience in data engineering, building scalable data pipelines and ETL processesStrong proficiency in programming languages commonly used in data engineering, such as Java, Scala, or PythonIn-depth knowledge of distributed systems, data processing frameworks (such as Hadoop, Spark, or Flink), data lake, and cloud computing platforms (e.g., AWS, Google Cloud, Azure)Proficient in SQL and relational database technologies (e.g., MySQL, PostgreSQL)Strong knowledge of data modeling, and data warehousing conceptsFamiliarity with data governance, security, access controls, and compliance principles and solutionsDemonstrated ability to optimize data pipelines for performance, scalability, and reliabilityExcellent problem-solving and analytical skills, with a passion for tackling complex data engineering challengesExcellent communication skills, with the ability to contribute to a shared vision
Preferred Qualifications: Experience with real-time data processing and streaming frameworks (e.g., Kafka, AWS Kinesis)Knowledge of modern data lakehouse technologies such as Apache Hudi, Delta Lake, or Apache IcebergExperience with NoSQL databases (e.g., Redis, Cassandra)Experience with data visualization tools (e.g., Tableau, Metabase) and data exploration techniquesContributions to open-source data engineering projects or active participation in the data engineering community
Education and Experience: Bachelor's degree in Computer Science or a related technical field. Advanced degrees are preferred but not required. Equivalent practical experience will also be considered
BenefitsOur benefits include but are not limited to the following: 100% company paid medical; company matching 401(k), paid maternity and paternity leave, unlimited FTO package, ongoing professional development and certification opportunities, competitive salary, special employee discounts and health and wellness perks."
Data Quality Engineer,Vibrant Emotional Health,United States (Remote),https://www.linkedin.com/jobs/view/3748678730/?eBP=JOB_SEARCH_ORGANIC&refId=Ly4j4gR%2BFb0O2prgwdRS2Q%3D%3D&trackingId=4WZ1cf7oTPfdj5xRq8rOMA%3D%3D&trk=flagship3_search_srp_jobs,3748678730,"About the job
            
 
Job Title: Data Quality Engineer Department: Information TechnologyReports to: Director, Data Governance & PolicySalary Range: $80,000-$92,000 Location: RemoteSchedule: M-F, 9-5 ETFormerly the Mental Health Association of New York City (MHA-NYC), Vibrant Emotional Health’s groundbreaking solutions have delivered high quality services and support, when, where and how people need it for over 50 years. Through our state-of-the-art technology-enabled services, community wellness programs, and advocacy and education work, we are building a society in which emotional wellness can be a reality for everyone.Position OverviewThe Data Quality (DQ) Engineer designs, develops, documents and performs data quality checks across all data assets. The DQ Engineer stays informed of leading practices, emerging tools and technologies and enterprise initiatives to provide ongoing improvements and recommendations for data quality maintenance, including data profiling and cleansing.This role is the “first line of defense” protecting the quality of data and serves as a liaison between the business and technical stakeholders to ensure functional data needs are supported by a technical quality framework.Duties/Responsibilities Establish metrics and build Data Quality dashboardsCollaborate with stakeholders to understand use cases and recommend proactive/ reactive business rules to address data integrity issuesRecommend technology and process updates to improve quality and system performance.Develop technical design for data profiling, cleansing and quality maintenance.Develop models to monitor data quality and recommend remediations steps.Create alerts for various levels of data integrity disruptions.Perform assessments and model updates for new data sources and pipelines.Monitor Data Quality dashboards.Resolve service tickets related to data quality.Perform root cause analysis, anomaly detection.Monitor inbound data for new data elements to map.Participate in mapping and migration testing.
Required Skills/Abilities SQL.R, Python.Collibra or other Governance / Metadata management toolTableau, and one other BI analytics tool.Strong data analysis background.Experience with industry data quality dimensions and best practices.Can operate effectively within ambiguity.Familiar with Agile development.Ideally has ML/AI experience
Required Qualifications 1+ year of experience in a data quality role in a medium to large size organization.Experience working on data governance issues concerning health data and with government partners a plus.Bachelors’ or Masters’ Degree in analytics focused discipline or equivalent experience and knowledge.
Excellent comprehensive benefits, including medical, dental, vision, supplemental income insurance, pre-tax transit/parking, pre-tax FSA for medical and dependent care, and 401K available. 4 weeks’ vacation, plum benefits, etc.Studies have shown that women and people of color are less likely to apply for jobs unless they believe they are able to perform every task in the job description. We are most interested in finding the best candidate for the job, and that candidate may be one who come from a less traditional background. Vibrant will consider any equivalent combination of knowledge, skills, education and experience to meet minimum qualifications. If you are interested in applying, we encourage you to think broadly about your background and skill set for the role.Vibrant Emotional Health is an equal opportunity employer. Applicants are considered for positions without regard to veteran status, uniformed service member status, race, creed, color, religion, gender, gender identity, sex, sexual orientation, citizenship status, national origin, marital status, age, physical or mental disability, genetic information, caregiver status or any other category protected by applicable federal, state or local laws.""Please be aware that fictitious job openings, consulting engagements, solicitations, or employment offers may be circulated on the Internet in an attempt to obtain privileged information, or to induce you to pay a fee for services related to recruitment or training. Vibrant does NOT charge any application, processing, or training fee at any stage of the recruitment or hiring process. All genuine job openings will be posted on our careers page and all communications from the Vibrant recruiting team and/or hiring managers will be from an @vibrant.org email address"""
Healthcare Claims Data Engineer (Contract),Vivante Health,"Chicago, IL (Remote)",https://www.linkedin.com/jobs/view/3780555436/?eBP=JOB_SEARCH_ORGANIC&refId=Ly4j4gR%2BFb0O2prgwdRS2Q%3D%3D&trackingId=ELW2Tky8D9%2FFpAsftShBbQ%3D%3D&trk=flagship3_search_srp_jobs,3780555436,"About the job
            
 
Job Title: Healthcare Claims Data Engineer (Contract)Remote – US and British Columbia OnlyAbout VivanteVivante Health is an innovative startup reinventing the way chronic conditions are managed. We’re filling the unmet needs of people with chronic conditions that are invisible, neglected or stigmatized, starting with digestive disease.Why digestive? Because an astonishing 70 million people in the US are affected—that’s twice the number with diabetes and more than many other chronic conditions combined. Unlike other chronic conditions, though, digestive diseases often go untreated or misdiagnosed…because of stigma.At Vivante, we think it’s time to bring digestive health to the forefront while providing REAL solutions to the millions who are struggling and don’t know where to turn. Our health management ecosystem, GIThrive, empowers people to spend less time worrying about their digestive symptoms and more time living life.GIThrive works with our members (patients) to help identify and manage their digestive health conditions with personalized insights & recommendations. Our platform also facilitates interactions with our world-class clinical team to blend our technology with real person-to-person relationships that support each patient throughout their individual journeys.With a remote-first workforce, backed by leading digital health investors, we’re changing the way healthcare is delivered.Position SummaryWe're looking for an experienced, solution-focused Claims Data Engineer to support our data insights and analytics efforts. If you like designing and building scalable data solutions while collaborating with other great engineers in a test-driven environment, please read on!This engineer would help us integrate various external and internal data sources, with a special focus on medical and prescription claims data ingestion. This work is pivotal to enabling data analytics and data science efforts to assess the impact of our program on clinical outcomes, as well as personalize the program experience for our members. There will also be close collaboration with our Strategy and Operations team, who will be among the end consumers of the claims data products built.The ideal candidate is someone who can reason through the interactions of a distributed system and deliver solutions that emphasize simplicity, reliability and supportability.Why is this a great opportunity for a data engineer? The problem domain and our approach to solving it is super compelling. We're bringing together data sources that have never been joined, building models of a GI patient that have never been built, and discovering new GI health insights that help real people to manage and support their real medical conditions. It's still early enough that there’s a lot of decisions and new discoveries to be made, so this is your chance to get in early and help shape our future. 
Other things about our environment you should be aware of: We're still a small company, which means that everyone still wears a lot of hats and we need engineers who are comfortable with ambiguity and working across a wide variety of challenges. That said, our existing engineering team has built an extremely efficient working environment based on best-in-class hosted SaaS frameworks that greatly minimize the overhead in deploying and supporting software in production. We're all really passionate about improving clinical outcomes for our members and truly advancing the GI health space. Most of us also have personal (direct/family/friend) connections to GI health problems. Whatever it is that fuels you, we're on a mission to build something much larger than ourselves here and looking for another partner in driving that mission forward. 
Responsibilities Include: Expand our capabilities for ingesting and organizing medical and pharmacy claims data, along with other forms of healthcare and/or population health dataDevelop the kind of software and pipelines that you’d want to inherit from another developer (documentation, test coverage, logging, metrics, etc.) Collaborate with the rest of your engineering team on design, planning and code reviewsPartner with product stakeholders on ideation, feedback and refinement of solutions that meet our business needs Keep learning
You'll be a good fit here if you are: A team player. You like collaborating closely with other engineers, often through pair programming, design & code reviewsEmpathetic to our members’ GI health conditions and are driven to improve their outcomesSecurity oriented. We take the stewardship of our customer's healthcare data seriously, and take no shortcuts to protect itComfortable with a distributed workforce. We interact with each other via video chat, Slack and PR commentsA self motivated, creative problem solverAt least partially obsessed with automating everything 
We are proud of the team culture that we foster today, which is extremely friendly and supportive while constantly reaching to raise our own standards of engineering. We're really excited about what we're building, and usually having fun building it together. If all of that sounds fun to you too, we'd love to meet you.Desired Qualifications Degree in Computer Science or a related field, or equivalent experienceExperience in medical, healthcare and/or population health data, including expertise in medical and pharmacy claims data (e.g., medical coding systems, nuances specific to interpreting claims data)3+ years of demonstrated experience building and supporting data pipelines,SaaS solutions or other related technologiesProficiency with SQL, Python and some form of ELT/ETL solutionsOutstanding communication and interpersonal abilities with colleagues, business partners, and vendors alikeBonus: Familiarity with Google Cloud Platform (GCP) frameworks, including, BigQuery, IAM (fundamentals), Pub/Sub, Dataflow, Cloud Composer, DLP, or any othersExperience with Terraform and/or other infrastructure-as-code frameworksExperience working in a HIPAA regulated environment (or other regulated industry) and supporting data security and privacy controlsExperience analyzing and optimizing system performance

Vivante Health is an equal opportunity employer.We believe safe spaces where everyone can be their authentic selves is the key to a successful team so we welcome and embrace all identities, cultures, and backgrounds.Interested? Apply here(Link to Rippling ATS posting as a back-stop)https://vivante-health.rippling-ats.com/job/529412/data-engineer"
REMOTE: Data Engineer //Pay rate: $43.35/hr,Stellar Professionals,United States (Remote),https://www.linkedin.com/jobs/view/3704005282/?eBP=JOB_SEARCH_ORGANIC&refId=Ly4j4gR%2BFb0O2prgwdRS2Q%3D%3D&trackingId=3irz68g%2B7lXMm41qcrfhMQ%3D%3D&trk=flagship3_search_srp_jobs,3704005282,"About the job
            
 
Applicant must have 2 years of relevant experience with the following:  Hands-on experience with data engineering design and implementationExperience with data modeling design and implementationHands-on industry experience programming in SQL on relational database platforms (T-SQL and PL/SQL preferred)Hands-on industry experience working with enterprise ETL/ELT tools (Azure Data Factory and Databricks preferred)Hands-on experience with modern programing languages like Python, C#, JavaScript, etc (Python preferred)Hands-on experience with Azure, AWS, and/or GCP cloud platforms (Azure preferred)Bachelor's degree in Computer Science, Data Science, Software Engineering, Information Technology or a similar fieldProgressive mindset particularly around deployment models and emerging technologiesCollaborative team player who is detailed oriented, focused on solution quality and executionExperience with Docker for containerization and Kubernetes for orchestration"
Senior Data Engineer,HireMeFast LLC - Career Accelerator - Land A Job,"Dallas, TX (Remote)",https://www.linkedin.com/jobs/view/3779421402/?eBP=JOB_SEARCH_ORGANIC&refId=Ly4j4gR%2BFb0O2prgwdRS2Q%3D%3D&trackingId=EtXUI7y6pYmjmlWvWhBdxA%3D%3D&trk=flagship3_search_srp_jobs,3779421402,"About the job
            
 
This is a remote position. Job Title: Senior Data Engineer Employment Type: Full-Time Salary: $118,000 - $128,000 per annum Experience Required:  Minimum 3-5+ years of project experience How to Apply: visit hiremefast.net  to learn more & apply.About us: HireMeFast is a leading staffing and recruitment agency specializing in connecting businesses with top-tier talent across various industries. Our mission is to bridge the gap between exceptional candidates and organizations needing their skills, expertise, and unique qualities. Our team of experienced and dedicated recruitment specialists utilizes innovative sourcing strategies, and a vast network to identify and attract top talent. We conduct comprehensive procedures to ensure that only the most qualified candidates are presented to our clients.Responsibilities Work collaboratively with other engineers, data scientists, analytics teams, and business product owners in an agile environment. Build and support the operation of Cloud and On-Premises enterprise data infrastructure and tools. Design robust, reusable and scalable data driven solutions, and data pipeline frameworks to automate the ingestion, processing and delivery of structured and unstructured batch and real-time streaming data. Build data APIs and data delivery services to support critical operational processes, analytical models and machine learning applications. Assist in selection and integration of data related tools, frameworks, and applications required to expand platform capabilities. Understand and implement best practices in management of enterprise data, including master data, reference data, metadata, data quality and lineage. 
Qualifications Bachelor’s degree in computer science, math, engineering, or relevant technical field Two years of collective experience in the application of data engineering, data analytics, data warehousing, business intelligence, database administration and data integration concepts and methodologies One year of experience developing and supporting modern data architectures One year of experience developing and supporting large-scale distributed applications One year of experience with Linux operations and development, including basic commands and shell scripting One year of experience with execution of DevOps methodologies and continuous integration/continuous delivery One or more years of experience and proficiency in AWS Cloud Service, Python and Sql for data profiling, analysis and extraction. Results oriented with a strong customer focus Ability to work within a team environment Ability to prioritize and meet tight deadlines Ability to learn and keep pace with the latest technology advances and quickly grasp new technologies to support the environment and contribute to project deliverables 
Preferred Qualifications Master’s degree in a technical field (e.g. computer science, math, engineering) Understanding of big data and real time streaming analytics processing architecture and ecosystems Understanding of data warehousing architecture and implementation, including source to target mappings and ETL Experience with advanced analytics and machine learning concepts and technology implementations Experience with data analysis and using data visualization tools to describe data Relevant technology or platform certification (AWS, Microsoft, etc.) Software development experience in relevant programming languages (i.e. Java, Python, Scala, Node.js) 
Benefits Package 401k Match Tuition Reimbursement Disability Insurance Medical Insurance Dental Insurance Vision Insurance Employee Discounts Career Training & Development Opportunities Parental Leave PTO for Volunteer Hours Employee Resource Groups Inclusion and Diversity Programs Employee Recognition Program 
Why HireMeFast LLC?At HireMeFast, we understand that finding the right individuals to join your team is crucial for success and growth of your organization. We are committed to streamlining the hiring process for our clients, ensuring they have access to a diverse pool of highly qualified candidates who are a perfect fit for their specific needs."
Data Engineer,ADLIB Recruitment | B Corp™,United States (Remote),https://www.linkedin.com/jobs/view/3734928117/?eBP=JOB_SEARCH_ORGANIC&refId=Ly4j4gR%2BFb0O2prgwdRS2Q%3D%3D&trackingId=mqV87sSoFGOH8WSKf%2BNE8w%3D%3D&trk=flagship3_search_srp_jobs,3734928117,"About the job
            
 
Support the creation of an impactful data platform, shaping the future of the business.Work with a mix of advanced technologies in a hybrid data estate.Work remotely with an employee-owned company that encourages continuous learning with substantial training time and support for skill enhancement.A wonderful opportunity for a Data Engineer to join an industry-leading company that’s committed to making a difference. This role is perfect for those who want to make their mark in a well-established yet forward-thinking business.This employee-owned company is passionate about its work and offers an array of unique benefits. They believe in co-ownership, promoting work-life balance and giving back to their team members.Their goal is to provide the best service to our customers and improve their lives through their product offering. In this role, you will be vital to that mission, building the data foundation for their new customer platform, and enabling data-driven decisions across the organisation.What You’ll Be DoingAs a Data Engineer, you’ll be a part of a driven and innovative engineering team. Your primary responsibility will be to helpbuild and develop the new customer platform. You will work with a mix of software and data engineers, using Python, SQL and other tools, to construct and manage data pipelines.You will play a crucial role in getting data to the customer service teams, enabling them to serve their clients better. You’ll be at the forefront of defining a vision for their data estate, which is currently hybrid on-prem and Cloud, view a view to move to a data lakehouse architecture in the future.Your skills and expertise will contribute to debugging, defect correction, testing, and maintaining the highest quality throughout the development process.What Experience You’ll Need To Apply Strong proficiency in data transformation using SQL and Python, especially with Pandas Data Frames.Hands-on experience with source data, including operational relational data models, business files and log streams.Experience in managing scheduled ETL tools, and a history of working with Cloud Data Warehouses/Data Lakes in AWS.Proven ability in commercial data analytics development, coupled with a knack for problem-solving and outstanding communication skills.Bonus: Familiarity with tools like Power BI, AWS Glue, PySpark, Databricks, Snowflake, Redshift, DBT, and version control using git.
What You’ll Get In Return For Your ExperienceAs part of our team, you will enjoy a competitive salary of up to £70,000 and an array of unique benefits that truly set them apart.This role is offered remote, with the expectation to go in to the office 1 day p/quarter and comes with an annual profit share, a robust company pension scheme, along with some one-of-a-kind perks like discounted meals and produce.They support flexible working arrangements to promote a healthy work-life balance. They are committed to personal and professional development, offering mentorship and learning opportunities to fuel your growth too.What’s next?Are you eager to be a part of a company that values you not just as an employee, but as a co-owner? Are you ready to contribute to a mission that makes a tangible difference? Don’t wait – apply today to take your career to new heights!"
Lead Data Engineer,XO Health Inc.,"Stamford, CT (Remote)",https://www.linkedin.com/jobs/view/3779647775/?eBP=JOB_SEARCH_ORGANIC&refId=Ly4j4gR%2BFb0O2prgwdRS2Q%3D%3D&trackingId=4KsV4071SuJiEiH3CuB8xQ%3D%3D&trk=flagship3_search_srp_jobs,3779647775,"About the job
            
 
XO Health believes healthcare is fixable. Become part of the community changing the face of the industry.XO Health is the first health plan designed by and for self-insured employers that delivers a more unified health experience for everyone – from those who receive care, to those who deliver it, to those who pay for it.We are growing a multi-disciplinary team of diverse and digitally empowered employees ready to rebuild trust in healthcare through comprehensive and unified transformation.About the Role:The Lead Data Engineer is a critical role in our Data Engineering team who will work closely with Product Managers, Data Scientists and Software Engineers to support product launches and roadmaps by building the data architecture that informs and drives insight. In this role, you'll influence technology strategies, ensure that the technological solutions are aligned with the company's business needs and bring to life data and how it can impact positive healthcare outcomes.The perfect candidate will have strong data infrastructure and data architecture skills, great understanding and experience with health plan data, a proven track record of technical leadership, strong operational skills to drive efficiency and speed, strong project management skills, and a vision for how data can be an enabler.In This Role, You will: Possess a hardworking ""can-do"" mindset with focus on the collective success of the scrum team to iteratively deliver high quality products to enable the XO Health business architecture, intelligence, and creation of intellectual property. Collaborates with the Product Owner, Project Manager / Business Analyst, Subject Matter Experts and Development team to define and analyze user stories tracked in Jira. Collaborate in design, development, and implementation of software/data solutions for using modern cloud native, API first technology stack. Build cross-functional relationships with Data Scientists, Product Managers and Software Engineers, and PMO to understand data needs and deliver on those needs. Drive the design, building, and launching of new data models and data pipelines in production. Continually enhance full delivery pipeline through automation, expanded yet increasingly efficient test coverage, ultimately optimizing time-to-market and quality. Participate in innovation in the team through continuous learning and building prototyping complex, cross platform business solutions. Drive data quality across the product vertical and related business areas. Support the delivery of high impact dashboards and data visualizations. Define and manage SLAs for all data sets and processes running in production. Mentor new or junior team members and participate in code reviews. 
We're Looking for People Who Have: A bachelor's or master's degree in a technical or business discipline, or equivalent experience. 5+ years of related data engineering, data science and/or business intelligence experience. Extensive knowledge of contemporary frameworks, data/software engineering languages, diverse and emerging technologies. 3+ years data architecture experience. 3+ years development experience in at least one object-oriented language (Python, R, Java, etc.). Hands on experience in building cloud native applications using AWS platform. Strong experience building processes and architecture in AWS Stack and Snowflake. Progressive experience with SQL and related database technologies. Progressive experience with a variety of data management tools and technologies, and related tools, data visualization and data extraction and transformation tools. Hands on experience in Python (ETL tools - Pandas/NumPy, Python unit testing etc.). Experience with business intelligence tools and reporting solutions (PowerBI, Tableau). Experience with build/deploy automation & DevOps frameworks (CI/CD, Bamboo, GitHub Actions, pipeline-as-code, AWS CDK). Familiarity and experience in building and consuming APIs (REST, API Gateway, etc.) in an environment that uses multiple external applications. Strong testing and version control methodology. Must have strong problem-solving, analytical and in-depth research skills. Possess the ability to communicate effectively with internal and external partners, both orally and written. Experience using DBT is preferred. 
Full compensation packages are based on candidate experience and relevant certifications.$140,000—$160,000 USDXO Health is an equal opportunity employer committed to diversity and inclusion in the workplace. All qualified applicants will receive consideration for employment without regard to sex (including pregnancy, childbirth or related medical conditions), race, color, age, national origin, religion, disability, genetic information, marital status, sexual orientation, gender identity, gender reassignment, citizenship, immigration status, protected veteran status, or any other basis prohibited under applicable federal, state or local law. XO Health promotes a drug-free workplace."
Data Engineer (REMOTE),eStaffing Inc.,United States (Remote),https://www.linkedin.com/jobs/view/3645146791/?eBP=JOB_SEARCH_ORGANIC&refId=Ly4j4gR%2BFb0O2prgwdRS2Q%3D%3D&trackingId=P144V9N%2BGKOR2PnCvuwPiA%3D%3D&trk=flagship3_search_srp_jobs,3645146791,"About the job
            
 
Title: Sr. Software EngineerDuration: full time employmentLocation: RemoteClient: Workforce Communication application RequirementJob DescriptionMust-Haves10+ years of experience developing and delivering scalable, customer-facing enterprise software. Candidates with previous heavy work experience in contract positions are not preferred.Not married to any technology (has breadth of coding experience & enjoys honing their craft) but also understands how to leverage latest tech to solve problems in a better way. Ruby and Node.js preferred.5+ years of experience designing, implementing and managing advanced architectures in a SaaS application domain or platformTrack record of delivering timely, high-quality features and functionality within an agile sprint environment.Experience with large scaling challenges and solving challenging problems.Nice-To-Haves1 Good energy and communication skills2 A leadership mindset3 Comfort in ambiguous environments"
Data Engineer with Graph databases experience,TekIntegral,United States (Remote),https://www.linkedin.com/jobs/view/3714137963/?eBP=JOB_SEARCH_ORGANIC&refId=Ly4j4gR%2BFb0O2prgwdRS2Q%3D%3D&trackingId=yxGWWvXM%2B8Cd9foybpQ8eQ%3D%3D&trk=flagship3_search_srp_jobs,3714137963,"About the job
            
 
POSITIONData Engineer With Graph Databases ExperienceLOCATIONRemoteDURATION4+ monthsINTERVIEW TYPEVideoVISA RESTRICTIONSUSC/GC OnlyRequired Skills Hands-on Data Engineer to code ETL ingestion pipelines using AWS tools-for ETL they are using API calls-Glue, LambdaExperience with Engineering functions like estimating, procurement, constructionA big plus is experience with Graph databases such as Neptune"
Senior/Staff Data Engineer,EvenUp,"Tampa, FL (Remote)",https://www.linkedin.com/jobs/view/3728157945/?eBP=JOB_SEARCH_ORGANIC&refId=Ly4j4gR%2BFb0O2prgwdRS2Q%3D%3D&trackingId=nWPMTl6If8MMKwgK6auoLg%3D%3D&trk=flagship3_search_srp_jobs,3728157945,"About the job
            
 
EvenUp is a venture-backed generative AI startup that ensures injury victims are awarded the full value of their claims, expanding the $100B+ in awards granted to injury victims every year. Every year, the legal system has made it difficult for millions of ordinary people to seek justice, especially for folks without means or who come from underrepresented backgrounds. Our vision is to help these injury victims get the justice they deserve, irrespective of their income, demographics, or the quality of their legal representation.EvenUp operates across all types of injury cases, from police brutality and child abuse to California wildfires and motor vehicle accidents. Our ML-driven software empowers attorneys to accurately assess the value of these cases by doing a core part of their workflow (legal drafting), enabling them to secure larger settlements in record time. As EvenUp evaluates more cases, our proprietary data grows, enhancing the precision of our predictions and delivering more value to both attorneys and victims alike.As one of the fastest growing startups ($0 to $10M in ARR in <2 years), we raised $65M in investment from some of the best investors in the world (Bessemer, Bain Capital, Signalfire, DCM, NFX, Tribe Capital), seasoned tech executives (i.e. founder of Quora, SVP at Google, former CPO at Uber), and public figures that care about our social mission (Nas, Jared Leto, Byron Jones). Our team comes from top tech, legal, and investing backgrounds including Waymo, Google, Amazon, Uber, Quora, Blizzard, Norton Rose, Warburg Pincus, Bain, and McKinsey.Why we are hiring a Senior/Staff Data Engineer now? We have experienced unprecedented growth and need to scale out our data warehousing, data tooling and internal analytics. We need to architect the future of our data infrastructure at EvenUp and we’re seeking engineering leaders to help drive that vision. We will need to 10x our pipeline processing throughput over the next 12 months. We’ll need to rethink and rebuild how we extract, process and model our ingestion to enable our organization with precise and actionable data. We need to design & build data warehousing that democratizes data for our entire organization. We will invest in identifying and integrating tools and services that empower our teams to build on top of our data and analytics. 
What you’ll do: Democratize data at EvenUp. Ensure our organization can scale with consistent, standardized access to our data stores and accelerate our ability to build and experiment with data productsArchitect and build out the future of data warehousing at EvenUpEnable and empower our Data Science team to rapidly iterate on model experimentationDesign, organize and refine data storage strategies that reduce development friction for our tech organizationCollaborate with cross functional teams to solve critical data problemsHelp grow our nascent Data Insights team and define a “data first” mentality across our organization
What we are seeking: 8+ years of data engineering experiencePrevious experience building out data warehousing, data pipelines, and internal analyticsStrong understanding and practical experience with data tooling, BI tools, and systems such as DBT, BigQuery, ElasticsearchThe ability to communicate cross-functionally with various stakeholders to derive requirements and architect scalable solutionsHave several years of industry experience building high-quality software, shipping production-ready code and infrastructureYou enjoy owning a project from start to finish and love to drive a project across the finish lineInterest in making the world a fairer place (we don’t get paid unless we’re helping injured victims and/or their attorneys)
Nice to haves: Have previously built out a Data Insights team at a data-oriented startupHave previously planned and architected data migrations at scaleHave stood up analytics tooling to enable cross-functional teamsDomain expertise in legal technology, medical records, and working with unstructured data
A successful first year may look like: 75% doing system design and contributing code, starting with shipping code within 2 weeks!25% collaborating with stakeholders and mentoring, lunch and learns, and moreLeverage a self-starter mindset by taking a product concept and building the feature end to end (whether it’s a component of the system or a significant piece of functionality). Collaborate with the team to scale the tech stack based on our rapidly growing user base!
Benefits & Perks:We seek to empower all of our team members to fulfill our mission of making the world a more just place, regardless of our team’s function, geography, or experience level. To that end, we offer:  Fully remote setup - work from wherever you feel is best (Plus a stipend to upgrade your home office!) Flexible working hours to match your style Offsites - get to meet your coworkers on a fully-expensed trip every 6-12 months! Choice of great medical, dental, and vision insurance plan options Flexible paid time off A variety of virtual team events such as game nights & happy hours
EvenUp is an equal-opportunity employer. We are committed to diversity and inclusion in our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status."
Data Science Engineer - remote,Hour Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3726912321/?eBP=JOB_SEARCH_ORGANIC&refId=Ly4j4gR%2BFb0O2prgwdRS2Q%3D%3D&trackingId=8TKy9HZHGX48yTw3ufh%2FHw%3D%3D&trk=flagship3_search_srp_jobs,3726912321,"About the job
            
 
Our client is an established growing part of the Higher Education technology industry. They are a Global SaaS Company with over 300 campus partners and 2 million students actively using their platform.   They help higher education institutions better engage their students, improve the student life experience on campus, and student success. They are committed to improving student success and college graduation rates worldwide by crafting digital experiences that build communities and increase student engagement. They have a diverse and world-class global team poised for their next phase of rapid growth.     Understanding students and institutions and their behaviours through data lie at the foundation of their work to improve student success. Every data point in their systems is important to helping them achieve their main goal. They are looking for people with a strong background in data engineering and analytics to help them design, build, scale, and maintain their data pipelines and models.     As a Data Science Engineer, you will be working with various internal teams across engineering, product, and business to help solve their data needs. Your work will directly and tangibly impact the success of millions of students across the world.     In terms of the role and responsibilities, you will:       Identify the data needs of the engineering, product, and business teams, understand their specific requirements for metrics and analysis, then build efficient, scalable, accurate, and complete data pipelines to enable data-informed decisions across the company.   Architect data pipelines and models that power internal analytics for teams, as well as customer-facing data visualization product features   Be the subject matter expert on data-driven processes, visualization workflows, and tools with which to analyze data   Build processes supporting data transformation, data structures, metadata, dependency and workload management.   Drive the collection of new data and the evolution of existing data sources, collaborate with the engineering teams to manage the product instrumentation strategies and data structures   Help the product and engineering teams understand and generalize statistical models from research efforts and help build data systems that would allow these models to be used directly in product to drive student success.   Work with the Product Manager on the squad to ensure productive, fast-moving sprints that deliver the maximum value to their customers.   Work with the other senior engineers and architects to ensure that the integration stack is reliable, flexible, and scalable.   Help to improve the team processes of the engineering team continuously. 
    You should:     Have at least 4 years of experience in a Data Science or Data Engineering role, with a focus on instrumenting data collection, building data pipelines, data modelling and driving insights from complex data   Have a strong engineering background and are interested in data   Care deeply about the integrity of data, have a good nose for inconsistencies in data, and be able to pinpoint the issue to ensure that the team is not making decisions based on inaccurate or incomplete data   Working experience designing data systems   Experience with data related tools such as Athena, Apache Airflow or Google Composer   Strong analytical skills to pull insights from quantitative and qualitative data sets.   Have extensive experience in a scientific computing language - Python   Experience and understanding of APIs   A successful history of manipulating, processing and extracting value from large disconnected datasets   Advanced hands-on SQL knowledge and experience working with relational databases, query authoring   Have experience building systems that process data across multiple data stores and technologies, including MySQL, Redis, Elasticsearch   The ability to set up and manage a data warehouse is a plus   Know the best practices of how different types of data should be visualized in different contexts   Have good writing and verbal communication skills   
  This is a remote role based anywhere in North America.

Desired Skills and Experience
                DATA SCIENCE"
Data Engineer with HL7/FHIR experience,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3670744900/?eBP=JOB_SEARCH_ORGANIC&refId=Ly4j4gR%2BFb0O2prgwdRS2Q%3D%3D&trackingId=vf7UTYr7GdKE0Bx0NwnHow%3D%3D&trk=flagship3_search_srp_jobs,3670744900,"About the job
            
 
Job: Data EngineerLocation: RemoteDuration: 6 month CTH (USC/GC only)Top Skills SQLAzureData FactoryDBTETL workProven ability to complete projects in a timely manner while clearly measuring progressStrong software engineering fundamentals (data structures, algorithms, async programming patterns, object-oriented design, parallel programming) Strong understanding and demonstrated experience with at least one popular programming language (.NET or Java) and SQL constructs.Experience writing and maintaining frontend client applications, Angular preferredStrong experience with revision control (Git)Experience with cloud-based systems (Azure / AWS / GCP).High level understanding of big data design (data lake, data mesh, data warehouse) and data normalization patternsDemonstrated experience with Queuing technologies (Kafka / SNS / RabbitMQ etc)Demonstrated experience with Metrics, Logging, Monitoring and Alerting toolsStrong communication skillsStrong experience with use of RESTful APIsHigh level understanding of HL7 V2.x / FHIR based interface messages.High level understanding of system deployment tasks and technologies. (CI/CD Pipeline, K8s, Terraform).
Project/Day To Day Communicate with business leaders to help translate requirements into functional specificationDevelop broad understanding of business logic and functionality of current systemsAnalyze and manipulate data by writing and running SQL queriesAnalyze logs to identify and prevent potential issues from occurringDeliver clean and functional code in accordance with business requirementsConsume data from any source, such a flat files, streaming systems, or RESTful APIs Interface with Electronic Health RecordsEngineer scalable, reliable, and performant systems to manage dataCollaborate closely with other Engineers, QA, Scrum master, Product Manager in your team as well as across the organizationBuild quality systems while expanding offerings to dependent teamsComfortable in multiple roles, from Design and Development to Code Deployment to and monitoring and investigating in production systems."
Senior Data Engineer,BambooHR,"Oklahoma City, OK (Remote)",https://www.linkedin.com/jobs/view/3760295697/?eBP=JOB_SEARCH_ORGANIC&refId=Ly4j4gR%2BFb0O2prgwdRS2Q%3D%3D&trackingId=3F3SBbeuOZaIKz2UB1fzTw%3D%3D&trk=flagship3_search_srp_jobs,3760295697,"About the job
            
 
About UsOur mission is simple: we want to set people free to do meaningful work. People love our software—and it turns out that people love working here too. We've been recognized as a ""Best Company to Work For"" and we're proud of our team for creating software that makes an impact in the lives of HR pros and employees all over the world.What You'll DoAs a Senior Data Engineer on the data platform team, we'll rely on your expertise across multiple disciplines to develop, deploy and support data systems, data pipelines, data lakes, and lakehouses. Your ability to automate, performance tune, and scale the data platform will be key to your success.Your initial areas of focus will include: Collaborate with stakeholders to make effective use of core data assetsWith Spark and Pyspark libraries, load both streaming and batched dataEngineer lakehouse models to support defined data patterns and use casesLeverage a combination of tools, engines, libraries, and code to build scalable data pipelinesWork within an IT managed AWS account and VPC to stand up and maintain data platform development, staging, and production environmentsDocumentation of data pipelines, cloud infrastructure, and standard operating proceduresExpress data platform cloud infrastructure, services, and configuration as codeAutomate load, scaling, and performance testing of data platform pipelines and infrastructureMonitor, operate, and optimize data pipelines and distributed applicationsHelp ensure appropriate data privacy and securityAutomate continuous upgrades and testing of data platform infrastructure and servicesBuild data pipeline unit, integration, quality, and performance testsParticipate in peer code reviews, code approvals, and pull requestsIdentify, recommend, and implement opportunities for improvement in efficiency, resilience, scale, security, and performance
What You Need to Get the Job Done (if you don't have all, apply anyway!) Experience developing, scaling, and tuning data pipelines in Spark with PySparkUnderstanding of data lake, lakehouse, and data warehouse systems, and related technologiesKnowledge and understanding of data formats, data patterns, models, and methodologiesExperience storing data objects in hadoop or hadoop like environments such as S3Demonstrated ability to deploy, configure, secure, performance tune, and scale EMR and Spark Experience working with streaming technologies such as Kafka and KinesisExperience with the administration, configuration, performance tuning, and security of database engines like Snowflake, Databricks, Redshift, Vertica, or GreenplumAbility to work with cloud infrastructure including resource scaling, S3, RDS, IAM, security groups, AMIs, cloudwatch, cloudtrail, and secrets managerUnderstanding of security around cloud infrastructure and data systemsGit-based team coding workflows
Bonus Skills (Not Required, So Apply Anyway!) Experience deploying and implementing lakehouse technologies such as Hudi, Iceberg, and DeltaExperience with Flink, Presto, Dremio, Databricks, or KubernetesExperience with expressing infrastructure as code leveraging tools like TerraformExperience and understanding of a zero trust security frameworkExperience developing CI/CD pipelines for automated testing and code deploymentExperience with QA and test automationExposure to visualization tools like Tableau
Beyond the technical skills, we're looking for individuals who are: Clear communicators with team members and stakeholdersAnalytical and perceptive of patternsCreative in codingDetail-oriented and persistentProductive in a dynamic setting
If you love to learn, you'll be in good company. You'll likely have a Bachelor's degree in computer science, information systems, or equivalent working experience.An Equal Opportunity Employer--M/F/D/VBecause our team members are trusted to handle sensitive information, we require all candidates that receive and accept employment offers to complete a background check before being hired.For information on our Privacy Policy, click here."
LEAD DATA ENGINEER - REMOTE - $150-170K SALARY,Jefferson Frank,"Virginia, United States (Remote)",https://www.linkedin.com/jobs/view/3723945088/?eBP=JOB_SEARCH_ORGANIC&refId=Ly4j4gR%2BFb0O2prgwdRS2Q%3D%3D&trackingId=O%2FTotoAlTc%2FIsW3CZcxu1A%3D%3D&trk=flagship3_search_srp_jobs,3723945088,"About the job
            
 
Job Description: As a lead data engineer you will be responsible for creating a new data pipeline, ETL process, and architecture for 2.0 of our client's platform. This could include multi-model databases (including PostgreSQL and graph databases).Role & Responsibilities  Creating and maintaining a scalable ETL data pipeline that ingests multiple data sets both structured and unstructured data Creating and maintaining a multi-model data storage system including PostgresSQL, AWS Architecture and graph databases Working with Data Science team to enable ML Ops Ability to efficiently query and obtain data via SQL Assist Dev and Data Science teams with processing and integrating data analysis Clearly documenting processes, methodologies and tools used
Skills & Qualifications  Experience as Data Engineer in the AWS Ecosystem Usage and experience writing complex SQL queries and analysis of data correlations Heavy experience with Redshift, Glue, and other tools Experience with multi-model DB and ETL Pipelines"
Senior Data Engineer,"Nav Technologies, Inc.","Chicago, IL (Remote)",https://www.linkedin.com/jobs/view/3775324854/?eBP=JOB_SEARCH_ORGANIC&refId=Ly4j4gR%2BFb0O2prgwdRS2Q%3D%3D&trackingId=aVRTdnNcvzvmdV%2FnN%2BJihg%3D%3D&trk=flagship3_search_srp_jobs,3775324854,"About the job
            
 
Nav is democratizing small business financing. In other words, we give small business owners access and control. Yes, this challenges the norm, but it means working with curious, purpose driven, dedicated, and inquisitively smart people who push themselves, our company and the community to the next level (and every level after that). We are the people behind the tech. And when it's good, we look for better. We don't over think the value we bring nor spend time trying to revamp mantras. We also do not come up with some crafty way to tell you who we are and what we offer. We are Nav! Here, you'll gain a wealth of experience, learn the tricks of the trade, and work with winners. All companies say people are connected to their mission but in our case our mission and our people are one – it is a way of being not just a cause you are committed to. And since 2013, Nav has holistically and organically developed its own ideology because Nav can only be Nav.We are seeking an innovative and passionate Data Engineer to join our team. In this role you will be responsible for building the data and reporting infrastructure to power our systems, working with our team of engineers, product managers and designers; helping us create a better experience for the millions of Nav Small business users. You'll also work with our business intelligence and data science teams to improve the data platform, ensuring that the entire company is able to make better data-driven decisions.YOU WILL: Building cool stuff by maintaining and fulfilling a technical data engineering roadmap and vision. Creating data and reporting infrastructure by building and optimizing production-grade data pipelines through the use of continuous integration. Exploring data sets for insight and understanding that will help drive great product changes and priority. Communicating results and findings to company audiences of varying technical ability in a clear, engaging manner. Breaking down complex technical concepts into digestible tid-bits. Supporting self-service data pipeline management (ETL) and self-directed learning process. Addressing competing explanations in result validity, and use formal reasoning approaches to avoid bias and miscommunication. Collaboration with Engineering and Data Analysts to identify and design ways to solve critical business problems and ensure funnel optimization. Ability to design, develop, and maintain scalable, reusable code. 
WHO YOU ARE: You are a driven data engineer with experience working with big data technologies such as Docker, ECS, S3, Redshift, Kafka, and RDS. You have experience with ETL/ELT and data warehousing using tools such as dbt, Azure Data Factory, Matillion and/or Fivetran You love building data-driven products and have expertise in one or more programming languages (ideally Python). You have strong SQL experience, with expert level skill in Postgres, Snowflake and /or AWS Redshift. You're an Apache Airflow practitioner with experience managing cross-DAG dependencies in Airflow. You know how to implement, advocate, and teach agile practices, and adapt them to your people and circumstances, while working in accordance with data ethics & privacy standards. You can consistently evolve data models & data schema based on business and engineering needs. You've developed and extended design patterns, processes, standards, frameworks and reusable components for use in various data engineering functions and areas. Ability to implement systems for tracking data quality and consistency. You're comfortable working directly with all internal data consumers and are eager to share your work through calibrations and organizational product demos. You are committed to quality! And you're someone who's willing to tackle technical debt and constantly evolving our processes to help nav grow. We'd love it if you have a high level understanding of credit scores, ecommerce, and B2B financing. 
Inclusion at Nav:At Nav, we celebrate what makes our employees unique because the businesses we serve are progressively diverse and distinctly original. Navericks are diverse, side hustlers, immigrants, veterans, queer, and we push generational boundaries. We are college dropouts, PhDs, special needs parents, allies, pet owners and community leaders. Navericks are human. We are committed to upholding a safe, supportive environment where everyone matters. We are committed to making a better future for all of us. We have created a workplace where people of all backgrounds can express their identities authentically. To put it simply, we want you to be proud to be you.Our Compensation Philosophy is simple but powerful:At Nav, we are transparent about our total rewards, including pay, across all levels and roles. We believe great, enduring relationships are grounded in trust and transparency. Compensation shouldn't be a distraction, and employees should understand how pay and career advancement decisions are made. Providing equal pay for equal work is table stakes for being a great place to work. Gender and ethnic inequity should only be something that our children read about in history books. We believe providing Navricks with company ownership, competitive pay, and a range of meaningful benefits is the start of creating a culture where people want to give the best they've got — not because they're simply making money, but because they've fallen in love with our vision, mission, values, and team.During the interview process, your recruiter will be explaining how our rubrics work across all of our total rewards ( base, equity, bonus, perks, and benefit) offerings . The base salary for this role is targeted between $120K - $140K per year. Final offer amount is determined by your proficiencies within this level.Our impact on you:Competitive Pay. Company Ownership. Unlimited Vacation. Benefits Day One. 6 Weeks Paid Parental Leave. Work From Anywhere (yes we were distributed before it was cool). Flexible Work Arrangements. Free Telehealth and Telemental Health For All Employees. Employee Networking and Events. Community Network Groups (women's, PRIDE, culture). Meaningful Perks and Rewards. Learning and Development Opportunities. Pet Insurance.A Naverick's DNA:  We look at the future and say ""why not""; we see possibilities where others see problems or routines. We show the way ahead and are committed to achieving ambitious goals. We practice straight talk and listen generously to each other with empathy. We value different opinions and point of views. We ensure that we connect outside as well as inside to learn from others and inspire each other. We hold ourselves accountable for delivering results. We choose to not to be a victim of circumstance. We make decisions & take responsibility so that we can act & support each other, rather than adopting defensive, and ""finger pointing"" behaviors. As leaders we motivate & engage our teams to undertake beyond what they originally thought possible, by developing our teams & creating the conditions for people to grow and empower themselves through enabling & coaching. 
If you are based in California, we encourage you to read our privacy notice for California residents linked here."
Data Engineer,Vevo,United States (Remote),https://www.linkedin.com/jobs/view/3764348307/?eBP=JOB_SEARCH_ORGANIC&refId=Ly4j4gR%2BFb0O2prgwdRS2Q%3D%3D&trackingId=lvS72rpDFxxBQrTu6dH8mw%3D%3D&trk=flagship3_search_srp_jobs,3764348307,"About the job
            
 
Vevo is the world's leading music video network, connecting an ever-growing global audience to high quality music video content for more than a decade. Founded by Universal Music Group and Sony Music Entertainment in 2009, Vevo offers fans worldwide a vast array of premium content to choose from, showcasing official music videos alongside a constantly developing lineup of live performances and innovative original programming. From top superstars to rising new talents, Vevo brings incomparable cross-promotional support to artists across the musical spectrum, at every stage of their careers.Vevo has consistently evolved over the past decade to lead within today's ever-changing media landscape, embracing partnerships with a number of leading distribution platforms to deliver extraordinary content within ad-supported environments. With more than 25B views across television, desktop and mobile devices each month, Vevo brings music videos to the world – when, where, and how fans want them.As a Data Engineer at Vevo, you will extend and maintain the data pipelines that feed our ever growing data lake. Join a small autonomous team responsible for this data lake and its ingress and egress pipelines. Through this data lake and its data pipelines you will be providing immensely important data to internal business analysts, data scientists, Vevo leadership, as well as content partners in a multi-billion dollar industry.As a Member Of Our Team, You Will Be responsible for designing, building and supporting components that compose the Vevo data lake and it’s pipelinesHelp build and extend our data lake by designing and implementing: data pipeline libraries and systems, internal analytics tooling / dashboards, and monitoring and alerting dashboardsProvide support for the data pipelines including after-hours support on a rotational basisWork in a collaborative environment with other data engineers, data scientists, and software engineers to achieve important goals for Vevo
This Describes You You believe in values like effectiveness over efficiency and quality over quantityYou desire to continuously improve your team, your product and yourselfYou have pragmatic communication and problem-solving skillsYou are opinionated yet humble
Requirements BS/MS in computer science or equivalent experience in data engineeringYou love different types of data. i.e. content metadata, viewership metrics, etc.You love to solve difficult and interesting problems using data from various systemsYou have experience developing and maintaining software in PythonYou have experience with data pipelines that process large data sets via streams and/or batchesYou have experience in building services, capable of handling large amounts of data.You have experience building and maintaining tests (unit, integration, etc.) that provide necessary quality checks. TDD experience a plusYou have experience with modern persistence stores, primarily SQL; however NoSQL experience is a plusYou embrace best practices via pair programming, constructive code reviews, and thorough testing.You thrive in an environment with rapid iterations on platform featuresYou're a team player and work well in a highly collaborative environment, which includes staff in remote locations
Interested? Great! You might like to know: We are a tight community of fun, upbeat people with a real passion for music and technologyWe have premier access to music content and new releases of original media contentWe provide excellent benefits and competitive compensation packages"
IA- Remote Role-AWS DATA ENGINEER,NextRow Digital,United States (Remote),https://www.linkedin.com/jobs/view/3661832656/?eBP=JOB_SEARCH_ORGANIC&refId=Ly4j4gR%2BFb0O2prgwdRS2Q%3D%3D&trackingId=PianK6NaDCfuoNJ8VFEj%2Fg%3D%3D&trk=flagship3_search_srp_jobs,3661832656,"About the job
            
 
This is a remote position for an AWS Data Engineer with ETL and Analytical Reporting experience for our Iowa office. This position requires in-depth knowledge of AWS Data Integration Services such as Glue, as well as experience with Microsoft SQL Server, Microsoft SQL Server Integration Services, and MySQL.Role ExpectationsThe successful candidate will spend a good portion of their time in transitioning already developed AWS data pipelines and procedures that are built for Health and Human Services department. The candidate is also expected to work in concert with resident Data Engineers, Data Analysts and Report Developers to enhance, develop and automate recurring data requests and troubleshooting related issues.You will be primarily focused on backend development with AWS Data Integration and Storage Services tech stack (AWS Glue, AWS Lambda, AWS Spark, AWS Data Migration Services, AWS RDS, Amazon S3, Amazon Redshift, Amazon Dynamo).The successful candidate will be required to follow standard practices for migrating changes to the test and production environments and provide postproduction support. When not working on enhancement requests or problem reports, you would concentrate on performance tuning.Individual should work well in a team and independently as needed.ResponsibilitiesDesign and implement scalable and efficient data pipelines and ETL processes using AWS services such as AWS Glue, AWS Lambda, and Apache Spark.Develop and maintain data models, schemas, and data transformation logic to support data integration, data warehousing, and analytics needs.Collaborate with stakeholders to understand business requirements and translate them into technical data solutions.Implement data ingestion processes from various data sources such as databases, APIs, and streaming platforms into AWS data storage services like Amazon S3 or Amazon Redshift.Optimize data pipelines for performance, scalability, and cost-efficiency, utilizing AWS services like Amazon EMR, AWS Glue, and AWS Athena.Ensure data quality, integrity, and security by implementing appropriate data governance practices, data validation rules, and access controls.Monitor and troubleshoot data pipelines, identifying and resolving issues related to data processing, data consistency, and performance bottlenecks.Collaborate with data scientists, analysts, and other stakeholders to support data-driven initiatives and provide them with the necessary datasets and infrastructure.Stay updated with the latest AWS data engineering trends, best practices, and technologies, and proactively identify opportunities for improvement.Mentor and provide guidance to junior members of the data engineering team, fostering a culture of knowledge sharing and continuous learning.Key RequirementsBachelor's or master's degree in computer science, Data Engineering, or a related field.Minimum of 5 years of professional experience as a Data Engineer, with a focus on AWS data services and technologies.Strong expertise in designing and implementing ETL processes using AWS Glue, AWS Lambda, Apache Spark, or similar technologies.Proficient in programming languages such as Python, Scala, or Java, with experience in writing efficient and maintainable code for data processing and transformation.Hands-on experience with AWS data storage services like Amazon S3, Amazon Redshift, or Amazon DynamoDB.In-depth understanding of data modeling, data warehousing, and data integration concepts and best practices.Familiarity with big data technologies such as Hadoop, Hive, or Presto is a plus.Solid understanding of SQL and experience with database technologies like PostgreSQL, MySQL, or Oracle.Excellent problem-solving skills, with the ability to analyze complex data requirements and design appropriate solutions.Strong communication and collaboration skills, with the ability to work effectively in a team-oriented environment.Required/Desired Skills Skill Required /Desired Amount of Experience Bachelor's or master's degree in computer science, Data Engineering, or a related field. Required Professional experience as a Data Engineer, with a focus on AWS data services and technologies. Required 5 Years Strong expertise in designing and implementing ETL processes using AWS Glue, AWS Lambda, Apache Spark, or similar technologies. Required 5 Years Proficient in programming languages such as Python, Scala, or Java, with experience in writing efficient and maintainable code for data processing a Required 5 Years Hands-on experience with AWS data storage services like Amazon S3, Amazon Redshift, or Amazon DynamoDB. Required 5 Years In-depth understanding of data modeling, data warehousing, and data integration concepts and best practices. Required 5 Years Familiarity with big data technologies such as Hadoop, Hive, or Presto is a plus. Desired 0 Solid understanding of SQL and experience with database technologies like PostgreSQL, MySQL, or Oracle. Required 5 Years Excellent problem-solving skills, with the ability to analyze complex data requirements and design appropriate solutions. Required 0 Strong communication and collaboration skills, with the ability to work effectively in a team-oriented environment Required 0 *INTERVIEW DATES: Interviews will be conducted on [July 19 and 21] ; candidates needs to be available for interviews on the date(s) provided."
Epic Data Engineer,Prominence Advisors,"San Francisco, CA (Remote)",https://www.linkedin.com/jobs/view/3676184717/?eBP=JOB_SEARCH_ORGANIC&refId=Ly4j4gR%2BFb0O2prgwdRS2Q%3D%3D&trackingId=DN8Xf%2Fvvkot%2FdysU5Cak4Q%3D%3D&trk=flagship3_search_srp_jobs,3676184717,"About the job
            
 
Prominence is looking for a Data Engineer to assist with dimensional data modeling development for healthcare data architecture project.Who We AreProminence is a healthcare technology strategy and implementation firm, focused on helping the nation’s leading healthcare organizations to do more with their data. Founded by former Epic managers, we understand the technology landscape in healthcare and provide IT staffing, advisory services, and analytics solutions to create robust data ecosystems that support clinical workflows, automate operational processes, and expedite research. Whether it’s guiding a technology implementation, establishing governance principles, or developing leading edge analytics, we help our customers make sense out of the mountain of data at their fingertips in order to deliver higher quality care at a lower cost.Ranked as a best place to work over 27 times (and counting!), Prominence’s culture provides consultants with a supportive environment that allows you to innovate and grow your career in healthcare IT. Additional information is available on our website.Your RoleOur consultants guide our customers through complex technology requirements to summit the challenge at hand. You will need to be able to create order out of chaos, and compile ambiguous information into tactical action plans.Our ideal team members are humble, smart, and driven to ensure our customer’s success. This includes a passion to deliver high-quality results, while teaching our counterparts how to fish and grow the skills needed to support and expand upon the deliverables of our projects.If this sounds like you, and you meet the requirements below, we encourage you to apply. If you know of someone else how would be a great fit, let us know!RequirementsAs a member of our Epic Consulting team, you’ll work closely with our customers to implement and optimize their Epic workflows. In addition to your Epic project work, you will help mentor and grow our customer’s teams, escalate issues, and guide projects to a successful outcome.Key ResponsibilitiesPerform Epic-related consulting and advisory services, including but not limited to the following: Apply technical expertise to implement and optimize EMR workflows and data captureMentor customers to up-level their system knowledge and analyst skillsAnalyze operational and business requirements, and translate into system configurationCreate build documentation and workflow diagramsTrack and resolve project risks and issuesLead meetings and participate in ongoing work-product coordination. Transparently report on project status and deliverables. Develop robust knowledge transfer documentation to hand-off deliverables to customer teams. Additional duties as may be required to successfully deliver a projectMay be invited to participate in corporate functions, events, and meetings
Desired Qualifications Active Certification(s): Cogito Data Model (Clinical or Revenue Cycle), Cogito Tools, Caboodle Developer5+ years of experience as an Epic BI Developer or Data EngineerDimensional data modeling experienceCaboodle development experience preferredSSIS, Azure Data Factory, or Data Lake development preferredDemonstrated ability to deliver successful projects remotely
Success CriteriaSuccessful team members at Prominence display the following: High degree of professionalism; treats others with respect, keeps commitments, builds trust within a team, works with integrity, and upholds organizational values. Highly organized; able to manage multi-faceted work streamsSelf-motivated; able to maintain schedule, meet deadlines, and monitor your personal work productHighly adaptable; able to acclimate quickly to new project assignments and work environments. Creative; not paralyzed by problems and able to work collaboratively to find novel solutionsClear communication skills; ability to clearly convey messaging that resonates with your audience, in clear and concise written and verbal communicationsCan smell smoke and anticipate issues before they arise, ability to escalate effectivelyPassion to mentor and guide others
BenefitsProminence is dedicated to hiring the best and brightest minds in healthcare and maintaining a culture that rewards our employees for following their passion. We are excited to offer the following benefits for this position: Competitive Salaried and Hybrid Compensation PlansHealth Care Plan (Medical, HSAs, Dental & Vision)Retirement Plan (401k)Life Insurance (Basic, Voluntary & AD&D)Dependent & Health Savings AccountsShort Term & Long Term DisabilityPaid Time Off (Vacation/Sick & Public Holidays)Training & Development FundTechnology Stipends (for Qualifying Roles)Work From HomeCharitable Giving to Causes You Believe In
Employment EligibilityMust be legally authorized to work in the United States without sponsorship.Commitment to Equal OpportunityThe world’s most talented professionals come from every background. All applicants will be considered for employment without attention to age, race, color, religion, gender identity and/or expression, sexual orientation, national origin, marital status, veteran or disability status, or any other characteristic protected by law. In addition, Prominence will provide reasonable accommodations for qualified individuals with disabilities.If you are smart and good at what you do, come as you are. All qualified candidates are encouraged to apply.Partnership EligibilityOur partnerships are extremely important to us. This online application is not intended for anyone who is currently under a non-compete agreement or has an arrangement that precludes employment at Prominence. We appreciate your help in respecting our partners.Interested in learning more? Apply below to connect with our Talent team about immediate openings and future consulting projects."
Test Data Engineer  with GenRocket,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3645968687/?eBP=JOB_SEARCH_ORGANIC&refId=Ly4j4gR%2BFb0O2prgwdRS2Q%3D%3D&trackingId=TQOoSXlym%2BPCS8Tr%2BJrbdg%3D%3D&trk=flagship3_search_srp_jobs,3645968687,"About the job
            
 
Job DescriptionTitle: TEST DATA ENGINEERLocation: RemoteDuration: 6+ MonthsPosition DetailsMust required: GenRocket, scripting, PostGres/DB ability, API work  Determine best practice(s) around synthetic, masked data in DCE/Platform Analyze and understand how data flows in data hub Determine best way to manage and create synthetic data in Data Hub; deliver solution Figure out a solution to mock massive accounts coming through APIs Understanding data models, table structures and dependencies. Guide teams in generating new synthetic data, creating more, and maintaining over time. Create PoC for syntheitc data use cases working with Product team Agile Mindset, familiarity with Azure Cloud ADO experience, python or powershell scripting experience
Must required: GenRocket, scripting, PostGres/DB ability, API workDetermine best practice(s) around synthetic, masked data in DCE/PlatformAnalyze and understand how data flows in data hubDetermine best way to manage and create synthetic data in Data Hub; deliver solutionFigure out a solution to mock massive accounts coming through APIsUnderstanding data models, table structures and dependencies.Guide teams in generating new synthetic data, creating more, and maintaining over time.Create PoC for syntheitc data use cases working with Product teamAgile Mindset, familiarity with Azure CloudADO experience, python or powershell scripting experience"
REMOTE SENIOR DATA ENGINEER,Skiltrek,"Paradise Valley, AZ (Remote)",https://www.linkedin.com/jobs/view/3768026934/?eBP=JOB_SEARCH_ORGANIC&refId=Ly4j4gR%2BFb0O2prgwdRS2Q%3D%3D&trackingId=E%2Baxb%2BRDD3FEYEksxreaHw%3D%3D&trk=flagship3_search_srp_jobs,3768026934,"About the job
            
 
Job DescriptionThe Senior Data Engineer will oversee the department's data integration work, including developing a data model, maintaining a data warehouse and analytics environment, and writing scripts for data integration and analysis.Minimum Requirements  Python, SQL, OLAP, OLTP, Datalakes, Data Pipelining, Cloud Development (AWS) AWS certification desired Agile or Scrum experience
Relevant Experience  3-5 years Data Engineering with BA in Computer Science"
"Data Engineer with DBT, Snowflake, Python and Asset Management",Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3770675727/?eBP=JOB_SEARCH_ORGANIC&refId=Ly4j4gR%2BFb0O2prgwdRS2Q%3D%3D&trackingId=09kT1CJEb1ln4yFpD%2FM%2FYQ%3D%3D&trk=flagship3_search_srp_jobs,3770675727,"About the job
            
 
Remote role, CST work hoursNeed only 1 strong candidate on thisAsset Management domain exp neededmust have strong and extensive exp with DBTResponsibilities  Play a key role in delivering data-driven interactive experiences to our clients Work closely with our clients in understanding their needs and translating them to technology solutions Provide expertise as a technical resource to solve complex business issues that translate into data integration and database systems designs Problem solving to resolve issues and remove barriers throughout the lifecycle of client engagements Ensuring all deliverables are high quality by setting development standards, adhering to the standards and participating in code reviews Participate in integrated validation and analysis sessions of components and subsystems on production servers Mentor, support and manage team members
Skills And Experience  Experience in the implementation, execution, and maintenance of Data Integration technology solutions Experience advancing and supporting information management practices within business processes, applications and technology that underpin the data discipline (e.g. establishing data quality processes, performing data analysis, participating in technology implementation planning, implementing data integration processes, etc) Expertise in Snowflake data modelling, ELT using snowpipe, implementing stored procedures and standard DWH and ETL concepts. Strong experience in DBT (data build tool), Snowflake and Python Expertise in Snowflake concepts like setting up resource monitors, RBAC controls, virtual warehouse, query performance tuning, Zero copy clone, time travel and understand how to use these features Experience in Data Migration from RDBMS to Snowflake cloud data warehouse Experience with enterprise cloud economics. Understanding of enterprise data management concepts (Data Governance, Data Engineering, Data Science, Data Lake, Data Warehouse, Data Sharing, Data Applications) Hands-on expertise with SQL and SQL analytics Industry benchmarking experience in major industries such as: Financial Services and Retail
Good-to-Have Skills  Certifications for any of the cloud services like AWS, Snowflake, GCP or Azure Experience working with code repositories and continuous integration Understanding of development and project methodologies"
Azure Data Engineer,Epik Solutions,"Charlotte, NC (Remote)",https://www.linkedin.com/jobs/view/3545094907/?eBP=JOB_SEARCH_ORGANIC&refId=Ly4j4gR%2BFb0O2prgwdRS2Q%3D%3D&trackingId=ujElM%2BsCNRpUffrB4khntA%3D%3D&trk=flagship3_search_srp_jobs,3545094907,"About the job
            
 
Experience Level: 10 YearsMust have skills: Azure, Data Vault, Data Modeling, Data Analysis, SQL, Data WarehouseNotes: LinkedIn Profile is a mustRole Description  Transforming business needs into technical solutions Mapping data and analytics Data profiling, cataloguing and mapping to enable the design and build of technical data flows Use proven methods to solve business problems using Azure Data and Analytics services in combination with building data pipelines, data streams and system integration Knowledge of multiple Azure data applications including Azure Databricks Experience in preparing data for and building pipelines and architecture
If Interested please reply with below details  Updated Resume Best time to talk Number to connect"
Data Engineer - Digital Experience Platform - Remote | WFH,Get It Recruit - Information Technology,"Atlanta, GA (Remote)",https://www.linkedin.com/jobs/view/3775425444/?eBP=JOB_SEARCH_ORGANIC&refId=Ly4j4gR%2BFb0O2prgwdRS2Q%3D%3D&trackingId=QPle1Ez8iLbo2AYTc1NFkg%3D%3D&trk=flagship3_search_srp_jobs,3775425444,"About the job
            
 
At our company, our technology empowers the world, and our people make it happen. We move swiftly because the world can't wait, and we innovate uniquely for our customers and communities. By joining us, you become a valued member of a dynamic team of change-makers driven by curiosity and ingenuity. We understand that your best work comes from living your best life and sharing your unique talents, so we do everything we can to facilitate that. Together, we dream big, supporting each other to turn individual and collective dreams into reality. The future is ours, and it starts with you. With a vast clientele of over 7,700 customers, including approximately 85% of the Fortune 500®, we take pride in being recognized as one of FORTUNE 100 Best Companies to Work For® and World's Most Admired Companies™. Discover more on Life at Our Company's blog and hear firsthand experiences from our employees. Not sure if you meet all the qualifications? We encourage you to apply anyway. At our company, we are dedicated to fostering an inclusive environment where every voice is heard, valued, and respected. We welcome candidates from diverse backgrounds, including those with non-traditional paths to this role, as we believe skills and experience are transferable, and the desire to dream big makes for outstanding candidates.Job DescriptionWhat you get to do in this role:Ensure adherence to company data policies and procedures for enhanced data quality and reduced discrepancies.Obtain approvals for data access based on business needs.Develop scalable automated ETL/ELT jobs and ensure their maintenance.Identify and address data integrity issues, performing deep dives to determine root causes.Utilize PL/SQL queries, Python, and API calls to establish master data sets and merge datasets across different systems.Provide training for analysts and data scientists on available data sources.Optimize the operation of very large databases and compute clusters.Implement and maintain database structures and governance.Develop and maintain documentation on databases and production tables.Collaborate with multiple data teams across the company to meet analytics deliverables.QualificationsTo be successful in this role, you have:5 years of experience in consulting, business intelligence, analytics, or an equivalent analyst position, with expertise in PL/SQL and an additional object-oriented programming language (e.g., Python, Java, JavaScript).Effective problem-solving and analytical skills, managing multiple projects and reporting simultaneously across different stakeholders.Structured thinking with the ability to break down ambiguous problems and propose impactful data modeling designs.Passion for analyzing large and complex data sets to derive information driving business decisions.Attention to detail, organizational skills, and effective verbal/written communication skills.Experience in big data instances, such as Cloudera, Azure, Snowflake, etc.A proven track record in rolling out self-service analytics solutions (e.g., Tableau Server Ask Data).Solid decision-making, negotiation, and persuasion skills, often in ambiguous situations.Ability to work in a fast-paced environment and adapt to changing requirements.Understanding of technology development projects and the full technology development lifecycle.Employment Type: Full-Time"
Job Opening for Informatica Cloud Data Engineer - Remote,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3645943188/?eBP=JOB_SEARCH_ORGANIC&refId=UigNF7FNZGveMVVZmDkKLQ%3D%3D&trackingId=I8haGN2nIL5GsdoaotBwcg%3D%3D&trk=flagship3_search_srp_jobs,3645943188,"About the job
            
 
Hi,Please find attached Job Description. If you are interested please do share with me your updated resume or call me on ""+1 3026017375"".Job Title:- INFORMATICA CLOUD DATA ENGINEERWork Location:- 100% Remote((will work CST Time zone))Duration: 12 month contractWork Authorization:- GC, USCInterview Mode: videoInformatica Cloud Data Engineer - w2 candidates onlyLocation: REMOTE (will work CST Time zone)Work Status: USC / Green CardPerfect English/Communication skillsRequired Skills 9+ years Informatica - Axon, EDC, IDQ, MDM5+ Linux8+ years Data integration, data quality management and data governance solutionsapplication log files (Informatica, Splunk, Azure, etc.), monitoring backups, disaster recovery planningInformatica Cloud, Azure, AWS, Google,IT Data Security, LDAP , configure and manage access
We have an exciting opportunity for an Engineer 2, Data in the Cloud Data Engineering department! The Cloud Data Engineering department is responsible for a suite of Data Governance applications and establishing enterprise-wide Data Governance Framework that will help mature how data are managed as a corporate asset across Enterprise Holdings. EHI expects data to provide our competitive edge in making effective data-driven decisions across our lines of business.As an Engineer 2, you will routinely collaborate with team members and contribute to the development of our data strategy. You will be responsible for holding yourself accountable and will be required to deliver well-tested, secure, interoperable technologies that support the company's vision and strategy. You must be able to maintain effective relationships with all areas of company management, as well as with internal and external customers. You will also be required to establish and monitor key performance metrics to ensure compliance of established standards, processes, and procedures. You will be expected to represent your team in a positive manner both within and outside of your department. You will collaborate with other engineers and architects to provide recommendations for implementing new solutions and capabilities, ensuring security standards are met. You will be responsible for ensuring your tasks are completed and deadlines are met. You will also be required to meet key performance metrics to ensure compliance of established standards, processes, and procedures. You must be able to work in a fast paced, production environment with the ability to handle multiple tasks.This team works in a fun, collaborative virtual environment and is responsible for high-priority efforts. We are looking for a self-starter that is motivated to solve problems and implement technical solutions. In this role you will support our Data Governance and Data Engineering teams.You will also be required to adhere to our established Corporate Security Policies and Frameworks, Global Data Privacy Framework to establish and key performance metrics to ensure compliance of established standards, processes, and procedures. In addition, you will be expected to mentor others to develop and advance their own careers.In partnership with our IT Organizations, this role also helps define and support continuous use and adoption of Data Governance Tools, Data Governance Framework, as well as, other supporting standard operating procedures, measures, standards, and requirements, to foster awareness, trust and strengthen ownership and accountability for the data across Enterprise Holdings.Additional Responsibilities  Install, upgrade, patch, configure and administer the Data Governance tools: Informatica Axon, EDC, IDQ and MDM Identify, recommend, and implement process improvements, procedures, and controls to ensure data accuracy, security, and legal and regulatory compliance Optimize system performance to enhance throughput and gain efficiencies Support releases, migration of items from one environment to another and adherence to change control standards Provide options and recommendations related to resolving issues, mitigating risks, and resolving escalated items Establish and maintain processes to meet SLAs Collaborate in planning and process improvement efforts Develop and maintains productive working relationships with other Information Technology teams Work with outside vendors/consultants on related projects Implement and perform regular monitoring and proactively engage in mitigating concerns Implement and provide metrics on system and tool performance Create maintenance and monitoring scripts for Informatica server Manage all backup, restore, and disaster recovery processes and procedures Perform system recovery in the event of system failures Creating connections, maintaining repositories for Informatica tools Configuration and management of Users, Groups, and Roles (including security levels and user access) Perform Server maintenance activities like configuration changes, server reboots, and issue resolutions Provide on call support Create and maintain appropriate documentation Ability to handle multiple tasks, set priorities, schedule, and meet deadlines.
Required  5+ year of related Informatica administration experience administering any or all of these Informatica on premise products Axon, EDC, IDQ, or MDM 5+ Linux experience
5+ years of experience supporting data integration, data quality management and data governance solutions  Informatica experience installing, configuring, and maintaining Informatica applications on Linux Experience in system recovery in the event of system failures Experience working with Informatica support Ability to read and understand system, database, and application log files (Informatica, Splunk, Azure, etc.) Experience setting up and monitoring backups Experience with disaster recovery planning Experience creating connections and resources in EDC Experience monitoring IDQ workflow and providing recommendations for optimization Experience scripting informatica stop and start automation Proactively work on continuous improvement by identifying cost reductions and productivity improvements Experience with cloud-based technologies (Azure, Informatica, AWS, Google, etc.) Must have a clear understanding of IT Data Security Strong troubleshooting skills and ability to methodical approach an issue to identify necessary steps to resolve an issue Must have strong communication skills including the ability to effectively communicate with various levels of leadership Ability to safeguard proprietary and sensitive information exercising good judgment while working with confidential information Ability to work on medium to large, moderately complex assignments that may require research and advanced skill application Ability to articulate complex technical concepts to non-technical audiences Established, effective, existing relationships within all levels of the organization Excellent analytical, organizational and critical thinking skills Must be committed to incorporating security into all decisions and daily job responsibilities Experience with LDAP and ability to configure and manage access
Preferred  Bachelor's degree in Computer Science, Computer Information Systems, Management Information Systems, or related field preferred Knowledge of underlying infrastructure for Big Data Solutions such as Databricks Knowledge of Informatica Cloud Experience with Atlassian tools, Jira, Confluence, Bitbucket Ability to create automation using scripting languages such as Python, Perl, PowerShell, or other scripting language Experience with SQL Experience with APM solutions such as Dynatrace Experience with ADLS Experience with SSL Site Certificates Familiarity with Imperva and CyberArk
Kirti RaniAssociate Talent Acquisition -North AmericaDesk: +1 3026017375kirti@steneral.comIn my absence please reach out to Mr. Harish Sharma at harish@steneral.com & 3027216151"
Senior Data Engineer,Tiger Analytics,"Jersey City, NJ (Remote)",https://www.linkedin.com/jobs/view/3780467078/?eBP=JOB_SEARCH_ORGANIC&refId=UigNF7FNZGveMVVZmDkKLQ%3D%3D&trackingId=BmKKqFXcMQxJq7Y9em9W8Q%3D%3D&trk=flagship3_search_srp_jobs,3780467078,"About the job
            
 
Tiger Analytics is a fast-growing advanced analytics consulting firm. Our consultants bring deep expertise in Data Science, Machine Learning and AI. We are the trusted analytics partner for several Fortune 100 companies, enabling them to generate business value from data. Our business value and leadership has been recognized by various market research firms, including Forrester and Gartner. We are looking for top-notch talent as we continue to build the best analytics global consulting team in the world.We are seeking an experienced Data Engineer to join our data team. As a Data Engineer, you will be responsible for designing, building, and maintaining data pipelines, data integration processes, and data infrastructure using Dataiku. You will collaborate closely with data scientists, analysts, and other stakeholders to ensure efficient data flow and support data-driven decision making across the organization.Requirements 8+ years of overall industry experience specifically in data engineeringStrong knowledge of data engineering principles, data integration, and data warehousing conceptsStrong understanding of the pharmaceutical domain, including knowledge of clinical data, drug development processes, regulatory requirements, and healthcare dataProficiency in data engineering technologies and tools, such as SQL, Python, ETL frameworks, data integration platforms, and data warehousing solutionsExperience with data modeling, database design, and data architecture principlesFamiliarity with big data technologies (e.g., Hadoop, Spark) and cloud platforms - AWS, AzureStrong analytical and problem-solving skills, with the ability to work with large and complex datasetsStrong communication and collaboration abilitiesAttention to detail and a focus on delivering high-quality work
BenefitsSignificant career development opportunities exist as the company grows. The position offers a unique opportunity to be part of a small, challenging, and entrepreneurial environment, with a high degree of individual responsibility."
GCP Data Engineer with AI/ML (W2-Remote),Megan Soft Inc,United States (Remote),https://www.linkedin.com/jobs/view/3699604820/?eBP=JOB_SEARCH_ORGANIC&refId=UigNF7FNZGveMVVZmDkKLQ%3D%3D&trackingId=kCqdRTQjqxxFgYpJkLfihg%3D%3D&trk=flagship3_search_srp_jobs,3699604820,"About the job
            
 
Position: GCP Data Engineer with AI/ML (W2 Position)Location: Dearborn, MI (Remote)Duration: 12+ monthsMOI: Phone & WebExDirect Client: FORD MOTORSNote: Hacker Rank Test MandatoryNote: 1. U.S. Citizens and those authorized to work in the U.S. are encouraged to apply. We are NOTABLE to sponsor H1-B at this time.  H1B Consultant who are willing to WORK ON OUR W2 (H1B TRANSFER) are welcome. 
""Cloud Software Engineer Position Overview At Ford Motor Company, we believe freedom of movement drives human progress.We also believe in providing you with the freedom to define and realize your dreams. With our incredible plans for the future of mobility, we have a wide variety of opportunities for you to accelerate your career potential as you help us define tomorrow's transportation.Global Data Insight & Analytics organization is looking for a Machine Learning Engineer focused on building and driving the strategy forward for our internal Data Science / AI/ML platform.This role will work in a small, cross-functional team.The position will collaborate directly and continuously with other engineers, business partners, product managers and designers, and will release early and often.The team you will be working on is focused on building Mach1ML platform an AI/ML enablement platform to democratize Machine Learning across Ford enterprise (similar to Uber's Michelangelo, Facebook's FBLearner, etc).Position ResponsibilitiesWork closely with Tech Anchor, Product Manager and Product Owner to deliver MLOPs platform in GCP using Python and other tools for the data scientists across the company.Work with software and ML engineers/Data Scientist to tackle challenging AIOps problems.Maintain and mange current CI/CD ecosystem and toolsFind ways to automate and continually improve current CI/CD processes and release processesExamine, inspect codes / scripts and resolve issuesHelp innovate standardize machine learning development practices.Experiment, innovate and share knowledge with the team.Lead by example in use of Paired Programming for cross training/upskilling, problem solving, and speed to delivery.Leverage latest ML/NLP/ GCP/AIOPs/Kubernetes technologiesSkills RequiredA Bachelor's degree in Computer Science / Computer Engineering or similar technical discipline.2+ years of experience with Cloud Engineering / Services3+ years of work experience as a backend software engineer in Python with exceptional software engineering knowledge.Experience with ML workflow orchestration tools: Airflow, Kubeflow etc.Advanced working knowledge of object-oriented/object function programming languages: Python, C/C++, JuliaExperience in DevOps: Jenkins/Tekton etc.Experience with cloud services, preferably GCP Services like Vertex AI, Cloud Function, BigQuery etc.Experience in container management solution: Kubernetes, DockerExperience in scripting language: Bash, PowerShell etc.Experience with Infrastructure as code: Terraform etcSkills PreferredMaster focused in Computer Science / Machine Learning (highly preferred).Experience working with Google Cloud platform(GCP) specifically Google Kubernetes engine, Terraform, and infrastructureExperience in delivering cloud engineering productExperience in programming concepts such as Paired Programming, Test Driven Development, etc.Understanding of MLOPs/Machine Learning Life Cycle and common machine learning frameworks: sklearn, tensorflow, pytorch etc. is a big plusMust be a quick learner and open to learning new technology.Experience applying agile practices to solution delivery.Experience in all phases of the development lifecycle.Must be team-oriented and have excellent oral and written communication skills.Good organizational and time-management skills.Must be a self-starter to understand existing bottlenecks and come up with innovative solutions.Knowledge of coding and software craftsmanship practices.Experience RequiredA Bachelor's degree in Computer Science / Computer Engineering or similar technical discipline.2+ years of experience with Cloud Engineering / Services3+ years of work experience as a backend software engineer in Python with exceptional software engineering knowledge.Experience with ML workflow orchestration tools: Airflow, Kubeflow etc.Advanced working knowledge of object-oriented/object function programming languages: Python, C/C++, JuliaExperience in DevOps: Jenkins/Tekton etc.Experience with cloud services, preferably GCP Services like Vertex AI, Cloud Function, BigQuery etc.Experience in container management solution: Kubernetes, DockerExperience in scripting language: Bash, PowerShell etc.Experience with Infrastructure as code: Terraform etcExperience PreferredExperience and good understanding of GCP processing /DevOPs/ Machine LearningEducation RequiredA Bachelor's degree in Computer Science / Computer Engineering or similar technical discipline."
Data Engineer,CannonDesign,United States (Remote),https://www.linkedin.com/jobs/view/3774894116/?eBP=JOB_SEARCH_ORGANIC&refId=UigNF7FNZGveMVVZmDkKLQ%3D%3D&trackingId=Xj1Ap6Aohe%2B8X20MR8SIdQ%3D%3D&trk=flagship3_search_srp_jobs,3774894116,"About the job
            
 
CannonDesign has an exciting remote opportunity for a Data Engineer. Take a look at the description below. If you think your skills and experience, make you a good match for this position, we’d love to hear from you. We’re looking for creative, curious, empathetic people to join our Living-Centered Design movement.About Our FirmCannonDesign is an integrated design solutions firm focused on helping people continuously flourish. Whether designing for innovations in health, new scientific discoveries, equity in education or the next big idea in business, we use our Living-Centered Design approach to help organizations realize widespread change. Our ability to design transformational places, plans and strategies is why Fast Company named us a World Changing Company and one of the most innovative design firms in the world.About The RoleAs a member of the Data Visualization team, you will report to the Data Visualization Leader. You will work with data solutions including data warehouses and data lakes to support data analytics, dashboard development, and data science efforts. You will work with legacy company data platforms including ERP, CRM, and HRMS systems to migrate data to best-in-class platforms.What You Will Do Develop integrated database solutions to store and retrieve company information for BI reporting and other analytical needsMigrate data from legacy systems to new solutionsInstall and configure information systems to ensure functionalityAnalyze structural requirements for new software and applicationsDesign conceptual and logical data models and flowchartsImprove system performance by conducting tests, troubleshooting, and integrating new elementsOptimize new and current database systems
About Your Qualifications Bachelor's degree in Information Technology, Computer Science, or a related field is preferred. A minimum of 5 years of related experience is required, 10 years is preferred. Preferred certifications: Microsoft Azure administration, Microsoft Windows desktop and Server certifications, VMware VCP-DTM, and/or VCP-DCV (Horizon and VCenter). Strong customer service, communication skills, documentation, attention to detail, and innovative thinking are essential. Ability to provide technical software, hardware, and network problem resolutions in a user-friendly professional manner. Must be a self-starter capable of working well independently, managing multiple technology issues, and prioritizing deadlines. Strong knowledge of MS Windows 10/11 desktop, Microsoft 365, and VMware vSphere and Horizon is required. Knowledge of various software applications (Adobe CS Suite, Autodesk Products, Bluebeam, Rhino, Sketchup, Jive, V-Ray, MS Teams, Argos, Azure, Microsoft 365 suite of products, PDQ Deploy, and Deltek Vision) is a plus. 
The salary range for this position is $90,000 - $100,000 annually. This salary range is the range we believe is the anticipated range of possible base compensation at the timing of the posting. We may ultimately pay more or less than the anticipated salary range for the position. Employees may be eligible for discretionary bonuses. We offer a full benefit package including medical, dental and vision coverage and flexible spending account options and voluntary insurances. We have paid time off, flex-time summer hours, remote work options and a 401k plan and employee perk programs. For a general overview of our benefits, please visit our careers page at www.cannondesign.com/careers/. Actual compensation may vary from posting based on geographic location, work experience, education and/or skill level.About Working Here We are relentless in our pursuit of client adoration (not simply satisfaction). Consistent delivery of the best service is what we are about. We are committed to ensuring our practice is equitable for all employees, representative of the communities around us – and focused on the future of design. We advocate for equity, diversity, and inclusion efforts through the leadership of our DEI Council, Employee Resource Groups and other community advocacy initiatives. We’re about communication and transparency here. If you want to talk to someone about an idea you have, or a challenge that needs addressing, we’re ready for you. 
For a general overview of our benefits, please visit our careers page at www.cannondesign.com/careers/Please note that candidates can only apply to our positions on our company Careers site. It's not uncommon for scammers to create positions that look legitimate on other sites; never enter your information or apply for CannonDesign positions on any platform. Should an issue arise that you feel we should be aware of, please contact us. Please provide your resume and portfolio when applying.As a condition of employment, all employees are expected to complete mandatory training, including compliance training, within required timeframes and adhere to our internal policies and our Code of Conduct.CannonDesign recognizes the value of diversity in our workforce. We are committed to equal opportunity. We consider all qualified employment applicants without regard to race, religion, color, gender, age, national origin, sexual orientation, gender identity, partnership status, protected veteran status, disability, or any other status protected by federal, state, or local law. Individuals who hold legal work authorization applicable to employment at CannonDesign in the United States will be considered without regard to citizenship/alienage."
Databricks Developer (Data Engineer—Databricks/ Azure),Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3774716308/?eBP=JOB_SEARCH_ORGANIC&refId=UigNF7FNZGveMVVZmDkKLQ%3D%3D&trackingId=UEyjpGwH0Gnq0ik1IKXVVw%3D%3D&trk=flagship3_search_srp_jobs,3774716308,"About the job
            
 
Job: Databricks Developer (Data Engineer—Databricks/ Azure)Location: Remote 100%Term: 8+ months contract with extensionsNew Start Date expectation: January 10, 2024Need to have hands-on design experience developing a core framework for data quality. Need to design a foundational pipeline that can then be used to build reusable pipelines on top of this. True product developers.Hands on architect that developed a core framework. Written core code. Not just scripters but app centric developers.Most candidate will not have done this before on DataBricks but need to be dynamic enough to get it done.SummaryMcLaren has been hired by a Fortune 500 Consumer Packaged Goods (CPG) company to assist with implementing data mesh within Databricks. The client has been hoping to deploy this initiative and struggling so McLaren is coming in to assist. They will be moving the primary source data from SAP and Snowflake into Databricks.They have 120 data product they are building out. The goal is to do 2-week sprints for 2-4 data product deployments at a time.Responsibilities Develop and implement data pipelines and transformations on Databricks.Integrate data from various sources into the Databricks environment.Ensure code quality, optimization, and adherence to best practices.Collaborate with architects and business analysts to validate solutions.
Requirements 5+ years of experience in Databricks development and data engineering (AZURE)Apache SparkDatabricks NotebooksScala/Python ProgrammingData Transformation and ProcessingGit/GitHubData Pipeline Development"
Senior Data Engineer,Attune,"New York, NY (Remote)",https://www.linkedin.com/jobs/view/3711772375/?eBP=JOB_SEARCH_ORGANIC&refId=UigNF7FNZGveMVVZmDkKLQ%3D%3D&trackingId=OUX3qs9B7bp%2Bq0ggNXLNbA%3D%3D&trk=flagship3_search_srp_jobs,3711772375,"About the job
            
 
Attune is making it easier, faster, and more reliable for small businesses to access insurance (think, pizza place down the street or main street store). They all need insurance to protect their businesses, but when it comes time to get a policy, they're bogged down by hundreds of questions, lots of paperwork, and a seemingly never-ending timeline stretching for weeks or months.We are changing all that. By using data and technology expertise to understand risk, automating legacy processes, and creating a better user experience for brokers, we're able to provide policies that are more applicable and more transparent in just minutes.Simply put, we're pushing insurance into the future, focusing on accuracy, speed, and ease.Our mission and our team are growing. In staying true to our values, we've earned our spot on many workplace award lists. Attune is dedicated to creating a diverse team of curious, focused, and motivated people who are excited about changing the future of an entire industry.Job DescriptionAs a Senior Data Engineer joining our growing Data team, you will serve as a leader for data engineering - helping maintain our existing data pipelines and internal web applications while also providing guidance and support to other team members and looking for ways to improve our code and architecture to meet our growing data needs. You will also work with team members across the organization to develop and test new pipelines as we launch new products and integrate with third-party data sources. We work with various tools including Python/Pandas, Postgresql, Bash, AWS EC2, and S3. We are always open to using whatever tool is best for the job.Responsibilities: Independently drive improvements and optimization in ETLs and internal apps with a focus on future scalability and performanceServe as a leader/mentor for data engineering, employing and encouraging good software and pipeline development practicesDevelop automated checks to ensure data quality and integrityMaintain and improve batch ETL jobs - including SQL query optimization, bug fixes and code improvementsWork with our data engineers, BI, and other teams across the business to develop/refine ETL processesUnderstand and answer questions on the data our team maintains & influence business decisions in the process, in partnership with Revenue, Marketing, and Underwriting
Qualifications:  5+ years experience in analytics, data science, or data engineering role building & maintaining ETL pipelinesExpert Python and Postgresql skillsSolid understanding of relational database design and basic query optimization techniquesExperience working with/optimizing for medium to large datasets in SQLExperience working with git, and Gitlab or GithubNice to have: Experience with Linux CLI, shell programmingUnderstanding of CI/CDExperience with AWS EC2 and S3

What we offer you: 150-190k per yearFlexible PTOGenerous parental and caregiver leave401K matchExcellent medical, dental, and vision plansRemote-first cultureAnd more!"
"Data Engineer (MarTech, AdTech, Data Profiling)_____________remote",Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3697172445/?eBP=JOB_SEARCH_ORGANIC&refId=UigNF7FNZGveMVVZmDkKLQ%3D%3D&trackingId=4qr%2BgEoqOXrO9B9OrISlxA%3D%3D&trk=flagship3_search_srp_jobs,3697172445,"About the job
            
 
Hi,Please find attached Job Description. If you are interested please do share with me your updated resume or call me on ""+1 3026017375"".Job Title:- DATA ENGINEER (MARTECH, ADTECH, DATA PROFILING)Work Location:- remoteDuration: 18 monthWork Authorization:- Citizen, GCInterview : ZoomJob Title: Data Engineer (MarTech, AdTech, Data Profiling)Location: REMOTE or Memphis, TNI am looking for candidates who have HANDS ON experience w/ MarTech and/or AdTech and IT SHOWS ON THE RESUME. If your candidate does NOT have this experience, do NOT send them to me for review.Job DescriptionData Engineer excited about marketing tech, ad-tech, and customer data profiling. Responsible for working with large data sets and developing data pipelines that move data from source systems to segmentation systems, advertising platforms, data warehouses, data lakes, and other data storage and processing systems. The data engineer will prepare data for synthesis and analysis by CDP and other marketing tech systems.Requirements Solid programming skills (J2EE, Python), statistics knowledge, analytical skills, and an understanding of big data technologiesStrong knowledge of software engineering principles and techniquesData Warehousing & ETLData architecture & pipeliningUnderstanding of Salesforce and Adobe Marketing stacksExperience with third party databases, libraries, interfaces, and internet protocolsKnowledge of LINUX, J2EE, Relational and Document Databases, JSON, Shell Scripting, automation
Kirti RaniAssociate Talent Acquisition -North AmericaDesk: +1 3026017375kirti@steneral.comIn my absence please reach out to Mr. Harish Sharma at harish@steneral.com & 3027216151"
Data Engineer,Gentis Solutions,"Cincinnati, OH (Remote)",https://www.linkedin.com/jobs/view/3744003851/?eBP=JOB_SEARCH_ORGANIC&refId=UigNF7FNZGveMVVZmDkKLQ%3D%3D&trackingId=Nbd9gbpDUJVvY0TTKqrHVg%3D%3D&trk=flagship3_search_srp_jobs,3744003851,"About the job
            
 
Gentis Solutions is seeking a Data Engineer to join our team. This full-time contract position is with an industry-leading data analytics firm interested in full-time flex/remote consultants. The ideal candidates will have the required skills listed below and will be eligible and open to being hired by the data firm at the end of the project's duration. This position will work alongside an existing team and leverage enterprise-level technologies and processes.Requirements Bachelors Degree (Computer Science, Management Information Systems, Mathematics, Business Analytics, or STEM)5+ years proven ability of professional Data Development experience3+ years proven ability to develop with Azure and SQL (Oracle, SQL Server)3+ years of experience with PySpark/Spark2+ years of experience in Azure Data Factory and/or Azure DatabricksExperience in working with large-scale data sets and distributed systemsFull understanding of ETL concepts and Data Warehousing conceptsExposure to version control software (Git, GitHub SaaS)Strong understanding of Agile Principles (Scrum)
Desirable Skills Proficient with Relational Data ModelingUnderstanding of Data Mesh PrinciplesExperience with CI/CD - Continuous Integration/Continuous DeliveryExperience with Python Library DevelopmentExperience with Structured Streaming (Spark or otherwise)Experience with Kafka and/or Azure Event HubExperience with Github SaaS / Github ActionsExperience with SnowflakeExposure to Service Oriented ArchitectureExposure to BI Tooling (Tableau, Power BI, Cognos, etc.)
Typical Duties Design and develop Azure solutionsImplement automated unit and integration testingCollaborate with architecture and lead engineers to ensure consistent development practicesParticipate in retrospective reviewsParticipate in the estimation process for new work and releasesCollaborate with other engineers to solve and bring new perspectives to complex problemsDrive improvements in data engineering practices, procedures, and ways of workingEmbrace new technologies and an ever-changing environment"
Data Visualization Engineer,VanderHouwen,"Salem, OR (Remote)",https://www.linkedin.com/jobs/view/3779713515/?eBP=JOB_SEARCH_ORGANIC&refId=UigNF7FNZGveMVVZmDkKLQ%3D%3D&trackingId=7Msixvzihnp7RoQCXO0Pvw%3D%3D&trk=flagship3_search_srp_jobs,3779713515,"About the job
            
 
Job ID: 63958Data Visualization EngineerOur client is looking for a Data Visualization Engineer to join their team. This is a 100% remote position.The Data Visualization Engineer will be responsible for transforming raw data into visually compelling and informative dashboards, reports, and visualizations.By leveraging data visualization tools and techniques, the Data Visualization Engineer will help stakeholders across the organization gain valuable insights, make data-driven decisions, and effectively communicate complex information.Data Visualization Engineer ResponsibilitiesCollaborate with cross-functional teams to understand business requirements and identify key insights to be visualized.Design and develop visually appealing and intuitive dashboards, reports, and interactive visualizations to present complex data in a clear and concise manner.Utilize data visualization tools (strongly preferred tools: Power-BI/Tableau) to create dynamic and interactive visualizations that allow users to explore data and uncover patterns and trends.Work closely with data engineers and other analysts to ensure accurate and reliable data sources for visualization purposes.Interpret data and provide insights to stakeholders through visualizations, enabling them to make informed decisions and take appropriate actions.Conduct data analysis and validation to ensure the accuracy and integrity of data used for visualization.Stay updated on industry best practices and emerging trends in data visualization and provide recommendations for continuous improvement.Data Visualization Engineer Qualifications5 or more years of experience in data analytics and data visualization.Strong analytical and problem-solving skills, with a track record of delivering valuable data-driven insights.Expertise in data visualization tools, such as Tableau, Power BI, or similar tools.Understanding of data modeling and database design principles.Proficient in SQL and programming languages such as Python or similar.Excellent communication and collaboration skills, with the ability to work effectively in cross-functional teams.Ability to work independently and prioritize tasks in a fast-paced environment.Strong attention to detail and commitment to data accuracy and completeness.Salary: $50-$70/hr. (DOE)BenefitsBenefits are available to eligible VanderHouwen contractors and include coverage for medical, dental, vision, life insurance, short and long term disability, and matching 401k.About VanderHouwenVanderHouwen is an award-winning, Women-Owned, WBENC certified professional staffing firm. Founded in 1987, VanderHouwen has been successfully placing experienced professionals throughout the Pacific Northwest and nationwide. Our recruitment teams are highly specialized in either Technology and IT, Engineering, or Accounting and Finance career markets. Our recruiters value building meaningful, professional relationships with each candidate as well as developing honed knowledge of companies' staffing needs and workplaces. Partner with us to land your next exciting career.VanderHouwen is an Equal Opportunity Employer and participates in E-Verify. VanderHouwen does not discriminate on the basis of race, color, religion, sex, national origin, age, disability, or any other characteristic protected by applicable local, state or federal civil rights laws."
Big Data Application Engineer,Binance.US,"Seattle, WA (Remote)",https://www.linkedin.com/jobs/view/3620308341/?eBP=JOB_SEARCH_ORGANIC&refId=UigNF7FNZGveMVVZmDkKLQ%3D%3D&trackingId=hjScrFFFXH0n6T6969sRwQ%3D%3D&trk=flagship3_search_srp_jobs,3620308341,"About the job
            
 
Launched in 2019, Binance.US is the fastest growing and most integrated digital asset marketplace in the United States, powered by matching engine and wallet technologies license from the world’s largest cryptocurrency exchange - Binance. Our mission is to provide liquidity, transparency, and efficiency to financial markets by creating products that leverage crypto to unlock the power of everything. We build bridges between traditional finance and digital markets that enable growth for all—empowering the future of finance. Binance.US is operated by BAM Trading Services.Responsibilities Be responsible for leveraging existing data assets and platform components to build a flexible and reliable service layer for business systems. After joining our team, you need to get familiar with our core business logic, and then use the big data tool stack to develop services and applications to satisfy product requirements.
Requirements Bachelor’s degree or higher in Computer Science, Software Engineering or a related field, or equivalent functional experience in the area. 2+ years experience in a big data related area. Decent understanding in big data ecosystem and familiar with computation engines(Spark/Flink) and storage engines(Hive/Hudi/Doris). Strong Java/Scala backend development skills. Strong SQL development and optimization skills. Familiar with Shell/Python or any other scripting languages. Experiences with monitoring, optimizing and troubleshooting large scale big data applications is a plus.
Binance.US is an Equal Opportunity Employer. Our mission is to give Americans access to a broad array of digital assets, and we thrive because of the diverse and inclusive team that we are building. We do not discriminate against qualified employees or applicants because of race, color, religion, gender identity, sex, sexual preference, sexual identity, pregnancy, national origin, ancestry, citizenship, age, marital status physical disability, mental disability, medical condition, military status, or any other characteristic protected by local law or ordinance.Binance.US complies with Federal Transparency in Coverage regulations by providing this link to machine readable files related to the health plans offered to our employees. The machine-readable files are formatted to allow researchers, regulators, and application developers to more easily access and analyze data including negotiated service rates, and out-of-network allowed amounts between health plans and healthcare providers.Kaiser Permanentewww.aetna.com"
Azure Data Engineer - REMOTE ( No Sponsorship ; No c2c ),"LINQM, Inc.",United States (Remote),https://www.linkedin.com/jobs/view/3621075225/?eBP=JOB_SEARCH_ORGANIC&refId=UigNF7FNZGveMVVZmDkKLQ%3D%3D&trackingId=s%2F3t%2BwBvBmr46AOIGEBYeQ%3D%3D&trk=flagship3_search_srp_jobs,3621075225,"About the job
            
 
This role is part of a Data Management team that is responsible for modernizing and transforming data and reporting capabilities across our products by implementing a new modernized data architecture. The position will be responsible for day-to-day data collection, transportation, maintenance/curation, and access to the corporate data asset. The Cloud Data Engineer will work cross-functionally across the enterprise to centralize data and standardize it for use by business reporting, machine learning, data science or other stakeholders. This position plays a critical role in increasing the awareness about available data and democratizing access across and our data partners.Responsibilities Crafting and maintaining efficient data pipeline architecture. Assembling large, complex data sets that meet functional / non-functional business requirements. Design and maintain schemas. Create and maintain optimal data pipeline/flow architecture. Identifying, crafting, and implementing internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. Working with technical and non-technical stakeholders to assist with data-related technical issues and support their data infrastructure needs. Working with the team to strive for clean and meaningful data, and greater functionality and flexibility within the team’s data systems. Design processes supporting data transformation, data structures, metadata, dependency, and workload management. 
Qualifications Bachelor’s degree or equivalent experience. Building data engineering pipelines in Azure (ADF, Databricks, or Synapse). Strong cloud data engineering experience in Azure. 2+ years of experience with SQL Server. Advanced working knowledge of SQL and NoSQL query authoring. Experience working with streams such as Event Hubs and Event Driven Architectures. Experience with Microsoft Fabric, Power Platform including Power BI and Power Apps. Proficiency with object-oriented/object function scripting languages: Python, C#, etc. Experience with big data tools: Spark, etc. Building, maintaining, and optimizing ‘big data’ data pipelines, architectures, and data sets. Experience deploying and maintaining always-on data services Experience cleaning, testing, and evaluating data quality from a wide variety of ingestible data sources. Strong project management, communication, and organizational skills."
Data Engineer,Crossover Health,United States (Remote),https://www.linkedin.com/jobs/view/3745230782/?eBP=JOB_SEARCH_ORGANIC&refId=UigNF7FNZGveMVVZmDkKLQ%3D%3D&trackingId=z5IwQzcKAGxTlRY5aQMeQg%3D%3D&trk=flagship3_search_srp_jobs,3745230782,"About the job
            
 
DescriptionAbout Crossover HealthCrossover Health is creating the future of health as it should be. A national, team-based medical group with a focus on wellbeing and prevention that extends beyond traditional sick care, the company delivers an entirely new model of healthcare—Primary Health—built on the foundation of trusted relationships, an interdisciplinary care team approach, and outcomes-based payment. Crossover’s Primary Health model integrates primary care, physical medicine, mental health, health coaching, care navigation and more, and delivers care in surround-sound—in-person, virtually and via asynchronous messaging. Together we are building a community of members that embraces healthcare as a proactive part of their lifestyle.Job SummaryCrossover’s Data Engineer is responsible for managing and developing data sources for analytics at scale. This is a critical role for supporting Crossover’s growing analytics team, and serves as the connection between Crossover’s Product and Technology teams, Data Science team, and data infrastructure vendors. In this role, the successful candidate will build out new data sources within the enterprise data warehouse, guide data modeling efforts for new and existing projects, and manage data ingress and egress between Crossover teams, clients, partners, and vendors. The ideal candidate will have experience with both clinical healthcare data as well as healthcare claims data.Job Responsibilities Crossover’s Data Engineer is responsible for managing and developing data sources for analytics at scale. This is a critical role for supporting Crossover’s growing analytics team, and serves as the connection between Crossover’s Product and Technology teams, Data Science team, and data infrastructure vendors. In this role, the successful candidate will build out new data sources within the enterprise data warehouse, guide data modeling efforts for new and existing projects, and manage data ingress and egress between Crossover teams, clients, partners, and vendors. The ideal candidate will have experience with both clinical healthcare data as well as healthcare claims data.  Develop and maintain data sources within Crossover’s enterprise data warehouse (inclusive of our current vendor and/or future data infrastructure) Assist with recommendations for data architecture, data storage, data integration, data quality, and data models Contribute to design sessions based on technical requirements, and build data models to clean and transform datasets for use by Crossover’s Data Science and Analytics teams Assist with ETL, ELT, and reverse-ETL design and development initiatives including data analysis, source-target mapping, data profiling, change data capture, QA testing, and performance tuning to guarantee quality and repeatability of data model results Create and maintain data model standards, including MDM (Master Data Management) and codebase standardization Migrate Enterprise Workloads to Snowflake using industry standard methodologies Automate and deploy as well as build CI/CD pipelines to support cloud based workload Design, deliver cloud native, hybrid, and multi-cloud Workloads Invest in documentation, including all system design, architecture and ongoing changes Design and support production job schedules, including alerting, monitoring, break fixes, and performance tuning Build solutions that are automated, scalable, and sustainable while minimizing defects and technical debt Assist stakeholders including analytics, design, product, and executive teams with data-related technical issues Ability to work independently with little instruction or direct oversight Perform other duties as assigned 
 Minimum Qualifications Bachelors in Computer Science or Data Engineering, related degree, or equivalent professional experience 3+ years relevant work experience within a complex, dynamic environment, with preference for experience with clinical healthcare data 3+ years architecting , implementing, and supporting data infrastructure and topologies Experience building and operating highly available, distributed systems of extraction, ingestion, and processing of large data sets across a variety of applications (OLTP, OLAP and DSS) 3+ years Experience with Data warehousing, methodologies, modeling techniques, design patterns, and technologies. Experience with data migration tools and deploying cloudbase solutions Experience in writing advanced SQL (DML & DDL), including Stored Procedures, Indexes, user defined functions, windows functions, correlated subqueries and CTE's, and related data query and management technology Coding ability in R, Python, and Shell Scripting to build and deploy Pipelines Working knowledge of Git, or similar collaborative code management software Experience with data integration tools such as Matillion, FiveTran, DBT, or similar ETL/ELT tools Experience with Snowflake’s data platform 
 Preferred Qualifications Masters in Computer Science, Data Engineering, or related degree Healthcare data acquisition, ingestion, processing, and analytics knowledge highly preferred Previous experience with health informatics, taxonomies, terminologies, and code sets Knowledge and understanding of product features: IAAS, PAAS and SAAS solutions Experience with healthcare claims data, formats, and analytics Experience with Health Catalyst’s data and analytics platform Experience with Tableau Cloud administration Experience with Master Data Management Understand Cloud Ecosystem 
Crossover Health is committed to Equal Employment Opportunity regardless of race, color, national origin, gender, sexual orientation, age, religion, veteran status, disability, history of disability or perceived disability. If you need assistance or an accommodation due to a disability, you may email us at [email protected].To all recruitment agencies: We do not accept unsolicited agency resumes and are not responsible for any fees related to unsolicited resumes."
Remote work - Need AWS Data Engineer,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3661740035/?eBP=JOB_SEARCH_ORGANIC&refId=UigNF7FNZGveMVVZmDkKLQ%3D%3D&trackingId=T%2BqFUuVhKX0xh2rcH5bb6g%3D%3D&trk=flagship3_search_srp_jobs,3661740035,"About the job
            
 
100% Remote Need LinkedInTOP NEEDS (MUST HAVE ALL ) (Experience with): (the years listed is the minimum # of years) AWS Integration Services GLUE 5 years TOP NEED!! AWS (AWS Glue, AWS Lambda, AWS Apache Spark, AWS Data Migration Services, AWS RDS, Amazon S3, Amazon Redshift, Amazon Dynamo) 5 yearsETL Analytical Reporting & data pipelines 5 yearsDevelop Data models/Schemas 5 yearsWith programming languages (such as Python or Scala or Java) 5 yearsCI/CD 3 yearsSQL SERVER 5 yearsBig Data (Hadoop, Hive or Presto) 3 years Microsoft SQL SERVER Integration Services 5 yearsMYSQL 5 yearsData Pipelines (build, optimize, monitor, troubleshoot) 5 yearsCollaborate cross functionally 5 yearsMentor junior developers 6 months minimum
Full Job DescriptionThis position is for an AWS Data Engineer with ETL and Analytical Reporting experience. This position requires in-depth knowledge of AWS Data Integration Services, such as Glue, as well as experience with Microsoft SQL Server, Microsoft SQL Server Integration Services, and MySQL.The successful candidate will spend a good portion of their time in transitioning already developed AWS data pipelines and procedures that are built for Department of Health and Human Services. The candidate is also expected to work in concert with resident Data Engineers, Data Analysts and Report Developers to enhance, develop and automate recurring data requests and troubleshooting related issues.This role will be primarily focused on backend development with AWS Data Integration and Storage Services tech stack (AWS Glue, AWS Lambda, AWS Spark, AWS Data Migration Services, AWS RDS, Amazon S3, Amazon Redshift, Amazon Dynamo).The successful candidate will be required to follow standard practices for migrating changes to the test and production environments and provide postproduction support. When not working on enhancement requests or problem reports, the candidate would concentrate on performance tuning.Individual should work well in a team and independently as needed.Responsibilities Design and implement scalable and efficient data pipelines and ETL processes using AWS services such as AWS Glue, AWS Lambda, and Apache Spark.Develop and maintain data models, schemas, and data transformation logic to support data integration, data warehousing, and analytics needs.Collaborate with stakeholders to understand business requirements and translate them into technical data solutions.Implement data ingestion processes from various data sources such as databases, APIs, and streaming platforms into AWS data storage services like Amazon S3 or Amazon Redshift.Optimize data pipelines for performance, scalability, and cost-efficiency, utilizing AWS services like Amazon EMR, AWS Glue, and AWS Athena.Ensure data quality, integrity, and security by implementing appropriate data governance practices, data validation rules, and access controls.Monitor and troubleshoot data pipelines, identifying and resolving issues related to data processing, data consistency, and performance bottlenecks.Collaborate with data scientists, analysts, and other stakeholders to support data-driven initiatives and provide them with the necessary datasets and infrastructure.Stay updated with the latest AWS data engineering trends, best practices, and technologies, and proactively identify opportunities for improvement.Mentor and provide guidance to junior members of the data engineering team, fostering a culture of knowledge sharing and continuous learning.
Requirements Bachelor's or master's degree in computer science, Data Engineering, or a related field.Minimum of 5 years of professional experience as a Data Engineer, with a focus on AWS data services and technologies.Strong expertise in designing and implementing ETL processes using AWS Glue, AWS Lambda, Apache Spark, or similar technologies.Proficient in programming languages such as Python, Scala, or Java, with experience in writing efficient and maintainable code for data processing and transformation.Hands-on experience with AWS data storage services like Amazon S3, Amazon Redshift, or Amazon DynamoDB.In-depth understanding of data modeling, data warehousing, and data integration concepts and best practices.Familiarity with big data technologies such as Hadoop, Hive, or Presto is a plus.Solid understanding of SQL and experience with database technologies like PostgreSQL, MySQL, or Oracle.Excellent problem-solving skills, with the ability to analyze complex data requirements and design appropriate solutions.Strong communication and collaboration skills"
Senior Data Engineer - Azure Data Factory,"Data Ideology, LLC","Pittsburgh, PA (Remote)",https://www.linkedin.com/jobs/view/3775481241/?eBP=JOB_SEARCH_ORGANIC&refId=UigNF7FNZGveMVVZmDkKLQ%3D%3D&trackingId=nE8tnI2tFOsiRHm0vBSMsw%3D%3D&trk=flagship3_search_srp_jobs,3775481241,"About the job
            
 
Data IdeologyAt DI, we provide Data & Analytics expertise to drive measurable business outcomes, often solving complex business problems for our clients. Our data analytics advisory services enable our customers to transform data into insights by driving a culture of empowerment and ownership of results. Our team consists of highly motivated individuals passionate about learning, understanding, collaborating, and intellectually curious.For more information about Data Ideology, visitwww.dataideology.comSenior Data Engineer - Full-timeWe are looking for a Sr. Data Engineerto join our growing team. Sr. Data Engineer will leverage their business and technical knowledge to develop production-ready data models by integrating multiple data sources while working with business and technical teams to understand business strategy and objectives, gather information, and ensure business requirements are being fulfilled throughout the entire data & analytics lifecycle.Key ResponsibilitiesTo perform in this position successfully, an individual must be able to perform each essential duty satisfactorily. Reasonable accommodations may be made to enable individuals with disabilities to perform essential functions. Other duties may be assigned to meet business needs. Ability to collect and understand business requirements and translate those requirements into an actionable data warehouse plan.Knowledge of multi-dimensional and tabular design patterns and ability to identify solutions that leverage these modeling techniques.Ability to work within the SDLC framework in multiple environments and understand the complexities and dependencies of the data warehouse built within those constraints.Ability to define and implement best practices across database design and ETL.Ability to direct the work of others, including but not limited to directing ETL development, demonstrating an understanding of key concepts of ETL/ELT, including best practices for optimization and scheduling.
Supervisory Responsibilities: NoneQualificationsEducation and Experience: Proven understanding of Data Warehousing, Data Architecture, and BI.Experience with data pipelines and architecture/engineering.Knowledge of modern apps and data platforms.Cloud-based project implementation.Azure Data Factory experience is requiredSnowflake experience is a plusPython or Java experience is a plus
Knowledge, Skills, and Abilities: BI/Data Warehousing (5+ years)Cloud platforms (2+ years)ETL (3+ years)SQL (3+ years)Data ModelingData Vault ModelingHealthcare experience a plusConsulting experience a plus
Work Environment: Remote work from home.Hours of work and days are generally Monday through Friday. Specific business hours will depend on client needs.
Physical Demands: Must be able to remain in a stationary position 50% of the time.The person in this position must occasionally move about inside the office to access file cabinets, library stacks, office machinery, etc.Constantly operates a computer and other office productivity machinery, such as a calculator, copy machine, and printer.The person in this position frequently communicates with clients and coworkers. Must be able to exchange accurate information in these situations.
Benefits: Unlimited Discretionary Time Off PolicyInsurance (medical, dental, vision) for employees100% company paid - short and long-term disability insurance for employees100% company paid - life insurance and AD&D insurance for employees100% company paid – employee assistance programRetirement plans with company matchTraining and Certification Reimbursement annuallyPerformance-based incentive programCommission incentive programProfit Sharing PlanReferral Bonuses
Data Ideology is an EEO Employer"
Senior Data Engineer,Chord,"New York, NY (Remote)",https://www.linkedin.com/jobs/view/3760354231/?eBP=JOB_SEARCH_ORGANIC&refId=UigNF7FNZGveMVVZmDkKLQ%3D%3D&trackingId=blBadKB9mUkXiUdblYJJ1g%3D%3D&trk=flagship3_search_srp_jobs,3760354231,"About the job
            
 
The opportunityWe’re looking for a Senior Data Engineer to elevate Chord as the leader in headless commerce and data. You’ll join a growing team that is building a platform to empower modern and omnichannel brands with smart technology, unique customer experiences, and insightful data.In this role, you will build foundational infrastructure and associated data models that fuel best-in-class insights and a reporting toolkit intended to empower our customers with solid insights to drive their business. As our next Sr. Data Engineer, you will be a key member of the Data team and will report directly to our Chief of Data.Chord offers advanced data and ecommerce infrastructure for modern brands that want to have cutting-edge technology and unified data without a large engineering team. We like to think of our product as the tech engine for high-velocity growth. Backed by top investors, Chord is a SaaS company led by industry veterans, and we’re seeking smart, focused, creative talent to help us realize our vision.Our company is proudly remote-first, distributed across North America.As a Senior Data Engineer, you will: Chart the course for our data pipelines and data warehouse, optimizing for accuracy, performance, and accessibilityBuild best-in-class data architecture and frameworks that enable a modern, scalable data product. Lead architecture frameworks and the development of data, experimentation, and analytics solutions in collaboration with cross-functional partners in the Product and Engineering organizationsProvide production support for pipeline and data warehouse to tackle issues that arise with data load, data transformation, and reporting. Test and clearly document data assets and warehouse implementations to enable others to understand the implementation and definition of data methodologies easily. Focus on monitoring, CICD methodologies, and pipeline security. 
To be successful in this role, you’ll need: Previous experience with distributed systems at scale. Demonstrated ability to build, manage, and optimize core data infrastructureExperience building and maintaining custom ingestion pipelinesExperience gathering requirements and creating solutions for high data accessibility and usabilityComfortability defining and improving standards and frameworks for maintainable best practices for a high-scale data infrastructure -- you'll maintain and advocate for these standardsTo be excited by the opportunity to optimize and automate our pipelinesExperience with DBT, SQL, Snowflake, and Fivetran. Experience with cloud-based AWS ecosystems and productsExperience with real-time data streams, cohorting, data classification, and data lineage is a plus. 
Working at Chord, you can expect: An investment in your physical and mental well-being; we offer 100% employee Medical Benefits coverage and a percentage towards dependant coverage. Flexible PTO; we encourage you to take the time you need to be your best self at work. An onboarding package and annual work-from-home stipend to ensure you have everything you need to be successful while working remotely. Generous Parental Leave with a customizable transition back to work programTo make an impact! We’re an early-stage company, which means there is space to champion ideas and create and lead initiatives at any level in the organization. 
About your application and the interview processChord is an equal opportunity employer, and we value diversity. We do not discriminate based on race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. If you’re unsure about your qualifications for this position, we still encourage you to apply.Our interview process for the role begins with an exploratory conversation with the Hiring Manager. After that, we’ll invite you to a Zoom session with various stakeholders from across our organization. We aim to get to know you and allow you to learn more about our team and product while being respectful of your time.This is the expected salary range for US-based employment. This is a full-time, salaried position that includes Equity. We set standard ranges for all roles based on function, level, and geographic location, benchmarked against similar-stage growth companies in our market. This salary range represents the full salary range for the position. The starting base pay offered may vary depending on factors including experience, expertise, market demands, and internal parity."
Senior Data Engineer,BambooHR,"Boise, ID (Remote)",https://www.linkedin.com/jobs/view/3760292963/?eBP=JOB_SEARCH_ORGANIC&refId=UigNF7FNZGveMVVZmDkKLQ%3D%3D&trackingId=swIdgL07TnZ2QYp9be7hAQ%3D%3D&trk=flagship3_search_srp_jobs,3760292963,"About the job
            
 
About UsOur mission is simple: we want to set people free to do meaningful work. People love our software—and it turns out that people love working here too. We've been recognized as a ""Best Company to Work For"" and we're proud of our team for creating software that makes an impact in the lives of HR pros and employees all over the world.What You'll DoAs a Senior Data Engineer on the data platform team, we'll rely on your expertise across multiple disciplines to develop, deploy and support data systems, data pipelines, data lakes, and lakehouses. Your ability to automate, performance tune, and scale the data platform will be key to your success.Your initial areas of focus will include: Collaborate with stakeholders to make effective use of core data assetsWith Spark and Pyspark libraries, load both streaming and batched dataEngineer lakehouse models to support defined data patterns and use casesLeverage a combination of tools, engines, libraries, and code to build scalable data pipelinesWork within an IT managed AWS account and VPC to stand up and maintain data platform development, staging, and production environmentsDocumentation of data pipelines, cloud infrastructure, and standard operating proceduresExpress data platform cloud infrastructure, services, and configuration as codeAutomate load, scaling, and performance testing of data platform pipelines and infrastructureMonitor, operate, and optimize data pipelines and distributed applicationsHelp ensure appropriate data privacy and securityAutomate continuous upgrades and testing of data platform infrastructure and servicesBuild data pipeline unit, integration, quality, and performance testsParticipate in peer code reviews, code approvals, and pull requestsIdentify, recommend, and implement opportunities for improvement in efficiency, resilience, scale, security, and performance
What You Need to Get the Job Done (if you don't have all, apply anyway!) Experience developing, scaling, and tuning data pipelines in Spark with PySparkUnderstanding of data lake, lakehouse, and data warehouse systems, and related technologiesKnowledge and understanding of data formats, data patterns, models, and methodologiesExperience storing data objects in hadoop or hadoop like environments such as S3Demonstrated ability to deploy, configure, secure, performance tune, and scale EMR and Spark Experience working with streaming technologies such as Kafka and KinesisExperience with the administration, configuration, performance tuning, and security of database engines like Snowflake, Databricks, Redshift, Vertica, or GreenplumAbility to work with cloud infrastructure including resource scaling, S3, RDS, IAM, security groups, AMIs, cloudwatch, cloudtrail, and secrets managerUnderstanding of security around cloud infrastructure and data systemsGit-based team coding workflows
Bonus Skills (Not Required, So Apply Anyway!) Experience deploying and implementing lakehouse technologies such as Hudi, Iceberg, and DeltaExperience with Flink, Presto, Dremio, Databricks, or KubernetesExperience with expressing infrastructure as code leveraging tools like TerraformExperience and understanding of a zero trust security frameworkExperience developing CI/CD pipelines for automated testing and code deploymentExperience with QA and test automationExposure to visualization tools like Tableau
Beyond the technical skills, we're looking for individuals who are: Clear communicators with team members and stakeholdersAnalytical and perceptive of patternsCreative in codingDetail-oriented and persistentProductive in a dynamic setting
If you love to learn, you'll be in good company. You'll likely have a Bachelor's degree in computer science, information systems, or equivalent working experience.An Equal Opportunity Employer--M/F/D/VBecause our team members are trusted to handle sensitive information, we require all candidates that receive and accept employment offers to complete a background check before being hired.For information on our Privacy Policy, click here."
Azure Data Engineer,FinTech LLC,United States (Remote),https://www.linkedin.com/jobs/view/3739469271/?eBP=JOB_SEARCH_ORGANIC&refId=UigNF7FNZGveMVVZmDkKLQ%3D%3D&trackingId=f97S%2FrW3Z5%2B%2Fa58LHqqczw%3D%3D&trk=flagship3_search_srp_jobs,3739469271,"About the job
            
 
About Client:Client is a company that helps other businesses with technology and consulting services. They offer various services like developing software, managing computer systems, and providing advice on how to improve business processes. Client works with different industries like healthcare, finance, retail, and more. They help these businesses by using technology to make their operations smoother and more efficient. For example, they might create a mobile app for a retail store to make it easier for customers to shop online, or they might help a hospital manage patient records using computer systems.Overall, Client is like a trusted partner that businesses can rely on to solve their technology-related challenges and make their operations better. They have experts who understand the business needs and use technology to find the right solutions.Salary Range: $100K-$130K/Annum MaxJob Description: Understand business requirements.Understand source systems, source data and source data formats that are available on-prem / cloud.Design and build data ingestion pipeline.Design and build complex data processing pipelines.Work with relevant stakeholders to assist with data-related technical issues and support their data needs.Build programs for data quality checks.Provide operational support.Work with data architecture, data governance and data analytics. teams to ensure pipelines adhere to enterprise standards, usability, and performance.Involve in System Testing, UAT, code deployment activities.Coordinate with offshore team on regular basis
Mandatory skills: 4-6 years of working experience in Azure, Databricks, Snowflake, PySpark, Scala.Overall, 8-10 years in ETL/Data Engineering.
About ApTask:Join ApTask, a global leader in workforce solutions and talent acquisition services, as we shape the future of work. We offer a comprehensive suite of offerings, including staffing and recruitment services, managed services, IT consulting, and project management, providing unparalleled opportunities for professional growth and development. As a member of our dynamic team, you'll have the chance to connect businesses with top-tier professionals, optimize workforce performance, and drive success for our clients across diverse industries. If you are passionate about excellence, collaboration, and innovation, and aspire to make a meaningful impact in the world of work, come join us at ApTask and be a part of our mission to empower organizations to thrive.Applicants may be required to attend interviews in person or by video conference. In addition, candidates may be required to present their current state or government issued ID during each interview.Candidate Data Collection Disclaimer:At ApTask, we prioritize safeguarding your privacy. As part of our recruitment process, certain Personally Identifiable Information (PII) may be requested by our clients for verification and application purposes. Rest assured, we strictly adhere to confidentiality standards and comply with all relevant data protection laws. Please note that we only collect the necessary information as specified by each client and do not request sensitive details during the initial stages of recruitment.​​​​​​​If you have any concerns or queries about your personal information, please feel free to contact our recruitment team at businessexcellence@aptask.com."
DATA ENGINEER/ Dollar Universe Consultant,TekIntegral,United States (Remote),https://www.linkedin.com/jobs/view/3736649509/?eBP=JOB_SEARCH_ORGANIC&refId=UigNF7FNZGveMVVZmDkKLQ%3D%3D&trackingId=YOODjbQpWXXAs8Y06oPy9Q%3D%3D&trk=flagship3_search_srp_jobs,3736649509,"About the job
            
 
Role: Dollar Universe ConsultantType: 6+ months on-going contract to potential hire or renewalStart: 2 weeks from offerLocation: 100% remoteResponsibilitiesBasic Qualifications  Dollar Universe AdministrationOwn the Dollar Universe Automation tool for the teamHands on experience with BMC / Control M 
What is Dollar Universe scheduler tool?Dollar Universe Workload Automation optimizes IT workloads in today's high volume, hybrid, heterogeneous environments. The peer-to-peer architecture of Dollar Universe Workload Automation makes it easy to deploy and easy to scale software, while limiting the risk of a single point of catastrophic failure."
Remote Work - Need Lead Big Data Engineer,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3728848468/?eBP=JOB_SEARCH_ORGANIC&refId=UigNF7FNZGveMVVZmDkKLQ%3D%3D&trackingId=cDX%2Fkrao%2FmtVqZeWqPXOiQ%3D%3D&trk=flagship3_search_srp_jobs,3728848468,"About the job
            
 
Big Data Engineer: Various levels (Principal, Lead,)Duration: 1 year ++Location: Hybrid/Midtown Atlanta (may consider Remote for the right candidate)Work Location:650 W Peachtree St NWAtlantaGAUSA30308Big Data Engineers are responsible for architecture, design and build of Big Data applications to support business strategies and deliver business value. A data engineer participates in all phases of the Data Engineering life cycle and will independently and collaboratively write project requirements, architect solutions, and perform data ingestion development and support duties. Building data ingestion pipelines is highly technical and requires advanced skills with computers and proficiency with Big Data tools and technologies such as SQL, Python/Scala, Spark, Spark Structured Streaming, Spark SQL, Kafka, Sqoop, Hive, Kudu, HBASE, Impala, S3, HDFS, or Cloud platforms such as AWS, GCP, Azure, etc. Data Engineers require ability to communicate effectively, both written and verbal, and have extensive experience working with business areas to translate their business data needs and data questions into project requirements.QualificationBachelor’s Degree required. Preferably in Information Systems, Computer Science, Electrical Engineering, Computer Information Systems or related fieldFrom The ManagerFor Big Data Engineer position:   Must have hands-on experience with high-velocity high-volume stream processing: Apache Kafka and Spark Streaming a. Experience with real-time data processing and streaming techniques using Spark structured streaming and Kafka b. Deep knowledge of troubleshooting and tuning Spark applications Must have hands-on experience with Python and/or Scala i.e. PySpark/Scala-Spark Must have experience with Databricks Must have hands-on experience building, testing, and optimizing ‘Big Data’ data ingestion pipelines, architectures and data sets Experience in successfully building and deploying a new data platform on Azure/ AWS Experience in Azure / AWS Serverless technologies, like, S3, Kinesis/MSK, lambda, and Glue Strong knowledge of Messaging Platforms like Kafka, Amazon MSK & TIBCO EMS or IBM MQ Series Experience with Databricks UI, Managing Databricks Notebooks, Delta Lake with Python, Delta Lake with Spark SQL, Delta Live Tables, Unity Catalog Experience with data ingestion of different file formats across like JSON, XML, CSV Experience with NoSQL databases, including HBASE and/or Cassandra Knowledge of Unix/Linux platform and shell scripting is a must Experience with database solutions like Kudu/Impala, or Delta Lake"
"ETL Data Engineer, Remote",Stellent IT,United States (Remote),https://www.linkedin.com/jobs/view/3645145612/?eBP=JOB_SEARCH_ORGANIC&refId=UigNF7FNZGveMVVZmDkKLQ%3D%3D&trackingId=t0m6qpQMlVrfWwUeCDL6sQ%3D%3D&trk=flagship3_search_srp_jobs,3645145612,"About the job
            
 
ETL Data EngineerRemotePhone + SkypeCritical SkillsDesign, develop, and maintain scaled ETL process to deliver meaningful insights from large and complicated data sets.Support existing ETL processes written in SQL, troubleshoot and resolve production issues.Additional ResponsibilitiesWork as part of a team to build out and support Data Lake, implement solutions using Python to process structured and unstructured data.Partner with business users, architects and cloud engineers to develop, implement, and automated data pipelines.Collaborate with Engineering teams to discovery and leverage new data being introduced into the environmentCreate and maintain report specifications and process documentations as part of the required data deliverables.Serve as liaison with business and technical teams to achieve project objectives, delivering cross functional reporting solutions.Communicating with business partners, other technical teams and management to collect requirements, articulate data deliverables, and provide technical designs.Ability to multitask and prioritize an evolving workload in a fast pace environment."
AWS Data Engineer( W2 Only),JavanTech Inc,United States (Remote),https://www.linkedin.com/jobs/view/3714626224/?eBP=JOB_SEARCH_ORGANIC&refId=UigNF7FNZGveMVVZmDkKLQ%3D%3D&trackingId=%2BtL8GVJUztyIYi%2BoziWIyA%3D%3D&trk=flagship3_search_srp_jobs,3714626224,"About the job
            
 
Role: AWS Data Engineer(W2 Only)Candidates must have all of the AWS data services below and be senior (minimum 10+ years) with strong communicationLocation: 100% RemoteDuration: 12+ monthsRequired Skills  Data Engineering Very strong in AWS technologies (Lambda, STEP Functions, SQS/SNS, S3, CloudWatch, EventBridge, EMR, EC@, Redshift, Airflow, Appflow etc.).Expert in Python, PySpark.Very Strong Database and SQL skills.Strong Python, Strong working AWS experience, Data engineering (EMR, PySpark, Redshift, Glue), Serverless experience (Lambda, step functions), containerization (ECS with Fargate).Good to have: SAS knowledge, DevOps knowledge(Jenkins, Bitbucket, Terraform/UCD/CloudFormation), Testing Automation.
AWS Services Compute: Glue, Lambda, AWS Batch.Storage: S3.Database: Redshift, Aurora, RDS.Strong SAS/R/Python Scripting Experience.Familiarity Domino Data Lab.Sagemaker.
PLease contact us at arun@javantech.com , +1 T: +1 13363447715"
Senior Data Engineer - Databricks,"Data Ideology, LLC","Pittsburgh, PA (Remote)",https://www.linkedin.com/jobs/view/3775478550/?eBP=JOB_SEARCH_ORGANIC&refId=UigNF7FNZGveMVVZmDkKLQ%3D%3D&trackingId=8o3BRcDv6MVMTwoZYqajjw%3D%3D&trk=flagship3_search_srp_jobs,3775478550,"About the job
            
 
Data IdeologyAre you ready to embark on a transformative journey in the realm of data engineering? We are seeking a seasoned professional to join our dynamic team as a Senior Data Engineer. In this role, you will play a pivotal part in designing, optimizing, and maintaining data pipelines that propel our clients to new heights. If you're passionate about leveraging cutting-edge technologies and have a knack for turning complex data challenges into streamlined solutions, this might be the perfect opportunity for you.At DI, we provide Data & Analytics expertise to drive measurable business outcomes, often solving complex business problems for our clients. Our data analytics advisory services enable our customers to transform data into insights by driving a culture of empowerment and ownership of results. Our team consists of highly motivated individuals who are passionate about learning, understanding, collaborating, and who are intellectually curious.  For more information about Data Ideology visit www.dataideology.com .Senior Data Engineer - Full-timeKey Responsibilities System Architecture: Develop and maintain scalable and efficient data pipelines for ETL processes, utilizing AWS, Databricks, Python, and SQL technologies.Cross-functional Collaboration: Collaborate with various teams to grasp data requirements and implement solutions aligning with business needs.Performance Optimization: Enhance and troubleshoot existing data pipelines to improve performance and reliability.Data Quality Assurance: Implement processes for data quality and validation to ensure accuracy and consistency.Continuous Improvement: Stay informed on industry trends and best practices in data engineering for ongoing enhancement of our data infrastructure.Stakeholder Support: Work closely with data scientists, analysts, and other stakeholders, providing infrastructure and support for data-related initiatives.Data Assembly: Compile large, complex data sets that meet business requirements effectively.Process Enhancement: Identify and implement internal process improvements, optimizing infrastructure for scalability and automating manual processes.Migration Expertise: Proven experience in executing a data migration.
Requirements: Education: Bachelor's degree or experience in Data Science and Management.Experience: Proven experience as a Data Engineer or in a similar role, showcasing proficiency in data manipulation.Technical Skills: Strong SQL proficiency, experience with relational databases, and competency in at least one programming language (e.g., Python, Java, Scala) for data processing and scripting.Data Modeling: Knowledge of data modeling and database design principles.Cloud Familiarity: Understanding of cloud platforms such as AWS, Azure, or Google Cloud.Problem-solving and Communication: Excellent problem-solving skills and effective communication abilities.
Preferred Qualifications: Education: Bachelor’s degree in Computer Science, Engineering, Mathematics, or a related field.Stream Processing: Experience with stream processing technologies.Data Governance: Understanding of data governance and security best practices.Certifications: Experience and certifications in AWS and Databricks technologies.
Work Environment: Remote work from home.Hours of work and days are generally Monday through Friday. Specific business hours will depend on client needs.
Physical Demands: Must be able to remain in a stationary position 50% of the time.The person in this position must occasionally move about inside the office to access file cabinets, library stacks, office machinery, etc.Constantly operates a computer and other office productivity machinery, such as a calculator, copy machine, and printer.The person in this position frequently communicates with clients and coworkers. Must be able to exchange accurate information in these situations.
Benefits: Unlimited Discretionary Time Off PolicyInsurance (medical, dental, vision) for employees100% company paid - short and long-term disability insurance for employees100% company paid - life insurance and AD&D insurance for employees100% company paid – employee assistance programRetirement plans with company matchTraining and Certification Reimbursement annuallyPerformance-based incentive programCommission incentive programProfit Sharing PlanReferral Bonuses
Data Ideology is an EEO Employer"
SQL DBA/Data Engineer,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3712839182/?eBP=JOB_SEARCH_ORGANIC&refId=UigNF7FNZGveMVVZmDkKLQ%3D%3D&trackingId=RIjEsi6XnXOy2tOjwX2uqQ%3D%3D&trk=flagship3_search_srp_jobs,3712839182,"About the job
            
 
They will consider remote candidates, but prefer onsite and will give preference to those candidates. Location preference:Baton RougeCedar RapidsRosemountDes MoinesRemote
Need With SubmissionOfficial Photo IDSSN last 5 digitDob-month and dateComplete education details.This is a perm position not a contract, so only USC or GC candidates please.This position prefers onsite/hybrid work in Cedar Rapids or Des Moines, Iowa.Candidates can relocate, but must be onsite from day one and the client would prefer someone local to the Midwest.They will consider remote candidates, but prefer onsite and will give preference to those candidates.Must Have's We are looking for a Senior-level DBA who can perform Data Engineering work. DBA is the primary skill we are looking to fill with the ability to perform Data Engineering work.Someone with both SQL Server on-prem and Azure SQL experience with performance tuning.Must be an excellent communicator and collaborator.Experience administrating and managing the organization's data infrastructure. Experience monitoring performance and optimize databases to ensure efficiency.Assist with the design and development of data storage/integration/transformation/distribution strategies that can handle large volumes of varying types of data.Experience monitoring and maintaining database security by implementing access controls, authentication, audit trails, and other security measures.Experience integrating data from multiple sources such as legacy systems, cloud applications, proprietary file formats, APIs, event sourcing, and other databases. Experience developing, implementing, maintaining, and managing Data Quality Services ensuring the best possible data accuracy.Experience with both relational and dimensional data models, such as in Azure SQL Server.Experience with ETL/ELT pipelines including SSIS, Azure Data Factory, Function Apps, Logic Apps, Databricks, SPARK and Azure Synapse Analytics.Experience with data mesh and event streaming architectures e.g. Azure Event Hub / Kafka.Experience with structured, semi-structured and unstructured data forms e.g. SQL, JSON, YAML.
Job DescriptionSr. DBA / Data Engineer (Direct Hire): We are looking for a Senior-level DBA who can perform Data Engineering work. DBA is the primary skill we are looking to fill with the ability to perform Data Engineering work.Someone with both SQL Server on-prem and Azure SQL experience with performance tuning.
SummaryThe Data Engineer is responsible for the movement and capture of data across varied platforms and data stores.Non-Technical Professionalism required. (must be a team player)Flexibility.Strong analysis and problem-solving skills.Team Player, able to evaluate technical solutions against objective requirements and develop consensus as a team; rather than leaning to individual preference or organizational authority.High level verbal and written communication skills.Knowledge of insurance processing and terminology (preferred).
Technical -  We are looking for a Senior-level DBA who can perform Data Engineering work. DBA is the primary skill we are looking to fill with the ability to perform Data Engineering work.Someone with both SQL Server on-prem and Azure SQL experience with performance tuning.Experience administrating and managing the organization's data infrastructure. Experience monitoring performance and optimize databases to ensure efficiency.Assist with the design and development of data storage/integration/transformation/distribution strategies that can handle large volumes of varying types of data.Experience monitoring and maintaining database security by implementing access controls, authentication, audit trails, and other security measures.Experience integrating data from multiple sources such as legacy systems, cloud applications, proprietary file formats, APIs, event sourcing, and other databases. Experience developing, implementing, maintaining, and managing Data Quality Services ensuring the best possible data accuracy.Experience with both relational and dimensional data models, such as in Azure SQL Server.Experience with ETL/ELT pipelines including SSIS, Azure Data Factory, Function Apps, Logic Apps, Databricks, SPARK and Azure Synapse Analytics.Experience with data mesh and event streaming architectures e.g. Azure Event Hub / Kafka.Experience with structured, semi-structured and unstructured data forms e.g. SQL, JSON, YAML.Experience with Microsoft Visual Studio and Azure DevOps (preferred).Familiarity with software development life cycle."
Data Engineer - Web Crawling,Sayari | Commercial Risk Intelligence,United States (Remote),https://www.linkedin.com/jobs/view/3767219961/?eBP=JOB_SEARCH_ORGANIC&refId=UigNF7FNZGveMVVZmDkKLQ%3D%3D&trackingId=kACwZT409WWKft108rh5VQ%3D%3D&trk=flagship3_search_srp_jobs,3767219961,"About the job
            
 
About Sayari:Sayari is a venture-backed and founder-led global corporate data provider and commercial intelligence platform, serving financial institutions, legal and advisory service providers, multinationals, journalists, and governments. Thousands of analysts and investigators in over 30 countries rely on our products to safely conduct cross-border trade, research front-page news stories, confidently enter new markets, and prevent financial crimes such as corruption and money laundering. Our company culture is defined by a dedication to our mission of using open data to prevent illicit commercial and financial activity, a passion for finding novel approaches to complex problems, and an understanding that diverse perspectives create optimal outcomes. We embrace cross-team collaboration, encourage training and learning opportunities, and reward initiative and innovation. If you like working with supportive, high-performing, and curious teams, Sayari is the place for you.Position Description:Sayari is looking for a Data Engineer specializing in web crawling to join its Data Engineering team! Sayari has developed a robust web crawling project that collects hundreds of millions of documents every year from a diverse set of sources around the world. These documents serve as source records for Sayari’s flagship graph product, which is a global network of corporate and trade entities and relationships. As a member of Sayari's data team your primary objective will be to work on maintaining and improving Sayari’s web crawling framework, with an emphasis on scalability and reliability. You will work with our Product and Software Engineering teams to ensure our crawling deployment meets product requirements and integrates efficiently with our ETL pipelines.Job Responsibilities: Investigate and implement web crawls for new sourcesMaintain and improve existing crawling infrastructureImprove metrics and reporting for web crawlingHelp improve and maintain ETL processesContribute to development and design of Sayari’s data product
Required Skills & Experience: Experience with PythonExperience managing web crawling at scale, any framework, Scrapy is a plusExperience working with Kubernetes Experience working collaboratively with git
Desired Skills & Experiences: Experience with Apache projects such as Spark, Avro, Nifi, and AirflowExperience with datastores Postgres Experience working on a cloud platform like GCP, AWS, or AzureWorking knowledge of API frameworks, primarily REST Understanding of or interest in knowledge graphs
Benefits:  Limitless growth and learning opportunities A collaborative and positive culture - your team will be as smart and driven as you A strong commitment to diversity, equity & inclusion Exceedingly generous vacation leave, parental leave, floating holidays, flexible schedule, & other remarkable benefits Outstanding competitive compensation & commission package Comprehensive family-friendly health benefits, including full healthcare coverage plans, commuter benefits, & 401K matching
Sayari is an equal opportunity employer and strongly encourages diverse candidates to apply. We believe diversity and inclusion mean our team members should reflect the diversity of the United States. No employee or applicant will face discrimination or harassment based on race, color, ethnicity, religion, age, gender, gender identity or expression, sexual orientation, disability status, veteran status, genetics, or political affiliation. We strongly encourage applicants of all backgrounds to apply."
REMOTE SENIOR DATA ENGINEER,Skiltrek,"New York, NY (Remote)",https://www.linkedin.com/jobs/view/3768028709/?eBP=JOB_SEARCH_ORGANIC&refId=UigNF7FNZGveMVVZmDkKLQ%3D%3D&trackingId=TXbivQN%2FqKT0J8KuO4cgFA%3D%3D&trk=flagship3_search_srp_jobs,3768028709,"About the job
            
 
Job DescriptionThe Senior Data Engineer is responsible for expanding, troubleshooting, and updating the organizations data pipeline architecture to facilitate data collection and integration into our analytical data stores. This individual will play a key role in re-designing the company's data architecture to support our digital transformation data initiatives. The ideal candidate is an experienced data pipeline builder who enjoys optimizing data systems and building them from the ground up. The Senior Data Engineer will work closely with database engineers, database architects, and data analysts to ensure that performant, flexible, scalable, and sustainable data delivery mechanisms are in place for enterprise use.Minimum RequirementsRemote but must sit out of:  Alabama Arizona Arkansas Connecticut Delaware Florida Georgia Idaho Indiana Iowa Kansas Kentucky Massachusetts Mississippi Missouri Nebraska Nevada New Hampshire New Jersey New Mexico New York North Carolina Ohio Oregon South Carolina South Dakota Texas Utah Vermont Virginia West Virginia Wisconsin Wyoming
Bachelor's degree in engineering, computer science or similar discipline from an accredited institution7+ years of experience in a Data Engineer or ETL Engineering or ETL Development or equivalent role in enterprise initiatives.3+ years of experience with Azure: ADF, ADLS Gen 2, Azure Synapse, Databricks, Snowflake or equivalent technologies,3+ years of experience in working with SQL databases, Transaction replication and Change data capture technologies.Experience building processes to support data transformation, data structures, metadata, dependencies, and workload management.Experience building and optimizing data pipelines, architectures, and data sets.Desired SkillsExperience with Python, Scala, Spark-SQL or equivalent.Experience working with Data Catalog, Data Quality tools, Data Replication tools, Metadata Management, Data Governance and Master Data Management.Experience with Data Modeling tools and cloud data architectures.Experience With Regulatory And Compliance Requirements For Educational InstitutionsBenefit packages for this role will start on the 31st day of employment and include medical, dental, and vision insurance, as well as HSA, FSA, and DCFSA account options, and 401k retirement account access with employer matching. Employees in this role are also entitled to paid sick leave and/or other paid time off as provided by applicable law."
Data Engineer (Mortgage experience required) Independent Candidates Only -- Remote | WFH,Get It Recruit - Real Estate,"Christiana, DE (Remote)",https://www.linkedin.com/jobs/view/3758982743/?eBP=JOB_SEARCH_ORGANIC&refId=LrRO5gXX%2Fb1L%2BdZGZl1WXw%3D%3D&trackingId=FbvjFBi%2FfgTJVxE%2FjpLiOw%3D%3D&trk=flagship3_search_srp_jobs,3758982743,"About the job
            
 
Job DescriptionJob Overview:Are you a skilled Data Engineer with a passion for innovation and a solid background in mortgage banking? Join our dynamic IT team and play a key role in shaping and maintaining our cutting-edge mortgage banking data architecture. Your expertise will drive the optimization of data flow and collection, ultimately supporting data analysis and decision-making processes.Responsibilities  Design and Construct: Develop, install, test, and maintain highly scalable data management systems. Business Alignment: Ensure systems align with business requirements and adhere to industry practices in mortgage banking. Innovation: Build high-performance algorithms, prototypes, predictive models, and proof of concepts. Translation: Translate complex functional and technical requirements into detailed architecture, design, and high-performing software. Integration: Integrate new data management technologies and software engineering tools into existing structures. Tool Development: Create data tools for analytics and line of business to optimize our product and drive innovation. Collaboration: Work closely with data and analytics experts to enhance functionality in our data systems. Security and Compliance: Maintain a secure and compliant data processing environment in accordance with industry regulations.
Qualifications  Education: Bachelor’s or Master’s degree in Computer Science, Engineering, or a related field. Experience: Proven experience as a Data Engineer, preferably in the mortgage banking industry. Technical Skills: Strong knowledge of data warehousing solutions, relational SQL and NoSQL databases (Snowflake, MS SQL Server a plus). Cloud Expertise: Experience with AWS cloud services such as EC2, RDS, MSK, and Lambda. Scripting Languages: Experience with object-oriented/object function scripting languages is a plus. Analytical Skills: Solid analytical skills with the ability to understand complex business requirements. Tool Proficiency: Familiarity with data pipeline and workflow management tools (dbt, Apache Kafka, Snowflake data pipeline/streams). Industry Knowledge: Understanding of financial and mortgage banking principles. Soft Skills: Strong organizational and interpersonal skills, with effective task and timeline management.
Must HaveJAVA 8/11, Jenkins, GitHub, SQL, Embedded Tomcat, Spring Boot, Swagger, Microservices Kafka MSPNice To HaveCOBOL, LoanServ MSPJob Type: ContractPay: $60.00 - $65.00 per hourWork Location: RemoteIf you are a forward-thinking Data Engineer ready to make an impact in the dynamic world of mortgage banking, we invite you to join our innovative team. Embrace the opportunity to contribute to industry-leading solutions while enjoying the flexibility of a remote work environment. We look forward to welcoming you aboard!Employment Type: Full-Time"
Data Platform/ DevOps Engineer - Remote | WFH,Get It Recruit - Information Technology,"Mojave, CA (Remote)",https://www.linkedin.com/jobs/view/3774102181/?eBP=JOB_SEARCH_ORGANIC&refId=LrRO5gXX%2Fb1L%2BdZGZl1WXw%3D%3D&trackingId=gZB9BCg87B0wjGSUxdByeg%3D%3D&trk=flagship3_search_srp_jobs,3774102181,"About the job
            
 
Mastech Digital is currently hiring for the position of Data Platform/DevOps Engineer for our client in the Consulting domain. We take pride in fostering a work environment that values our professionals, offering comprehensive benefits and ample opportunities for growth. This is a contract position with an immediate start date.Duration: 6+ Months ContractLocation: Remote, CaliforniaRole: Data Platform/DevOps EngineerPrimary Skills: MS AzureRole DescriptionAs a Data Platform/DevOps Engineer, you'll play a crucial role with a minimum of 5+ years of experience.Required Experience And SkillsIn-depth technical knowledge of Databricks deployment in Azure, covering both administrative and consultative aspects.Proven expertise in seamlessly integrating Databricks into the Azure ecosystem, including WebApp, GitHub Actions, and AKS.Extensive hands-on experience with Azure Monitor, Sysdig, Splunk, and Dynatrace. Develop procedures for monitoring dashboards and alerts.A solid understanding of big data use cases and best practices, with experience in managing large data and compute clusters.Working knowledge of DevOps CI/CD tools (Jenkins, GitHub, Artifactory, docker images).Strong working experience with monitoring tools such as Splunk and Dynatrace.Bonus points for experience in creating a base view, Derived View, and Data source connections using Informatica (IICS).Familiarity with Fivetran-HVR, Snowflake, Collibra, and Apigee is a plus.Flexibility in work schedule to collaborate and guide the offshore development team.Education: Any/Bachelor’s degree in Computer Science, Electrical/Electronic Engineering, Information Technology, or another related field or EquivalentExperience: Minimum 5+ years of experienceRelocation: This position does not cover relocation expensesTravel: NoLocal Preferred: YesNote: Must be able to work on a W2 basis (No C2C).We invite qualified candidates to join our dynamic team and contribute to exciting projects in the ever-evolving digital landscape. If you are passionate about data platforms, DevOps, and continuous improvement, we would love to hear from you. Apply now and be a part of our collaborative and innovative work environment!Employment Type: Full-Time"
Arity - Lead Data Analytics Engineer - Remote,Allstate,"Chicago, IL (Remote)",https://www.linkedin.com/jobs/view/3504277392/?eBP=JOB_SEARCH_ORGANIC&refId=LrRO5gXX%2Fb1L%2BdZGZl1WXw%3D%3D&trackingId=IFaHd%2BHEjUbwxXza7f2VfQ%3D%3D&trk=flagship3_search_srp_jobs,3504277392,"About the job
            
 
Founded by The Allstate Corporation in 2016, Arity is a data and analytics company focused on improving transportation. We collect and analyze enormous amounts of data, using predictive analytics to build solutions with a single goal in mind: to make transportation smarter, safer and more useful for everyone.At the heart of that mission are the people that work here—the dreamers, doers and difference-makers that call this place home. As part of that team, your work will showcase both your intelligence and your creativity as you tackle real problems and put your talents towards transforming transportation.That’s because at Arity, we believe work and life shouldn’t be at odds with one another. After all, we know that your unique qualities give you a unique perspective. We don’t just want you to see yourself here. We want you to be yourself here.The TeamOur Data Science team is fueled by a passion to impact the future of mobility. We love to find the meaning within the 100’s of billions of miles of driving data we collect each year. We push the boundaries of telematics and transportation by building data and machine learning solutions to power our groundbreaking products.We are a team of data scientists, data engineers, and analysts that take full ownership of our data pipelines and feature generation and use AWS Managed Services as the basis of our technology stack. As a part of the team, in collaboration with product owners and other experts, your work will showcase your intelligence and creativity as you solve real problems that will improve transportation and even save lives. Personally, we think that’s game-changing stuff.The RoleWe are seeking a highly motivated Senior Data Analytics Engineer to join our Analytics Data Engineering team inside of Data Science. You’ll help us continue to build out an analytics data ecosystem that organizes our data for research, merges disparate sources together, and ensures data privacy best practices. You will work with a team of data engineers dedicated to creating the best telematics data and insights platform in the market.You will help us build the next generation of deep mobility insights by extracting relevant behavioral and geospatial patterns from users’ trip data. Your team will help us find new sources of telematics data and figure out how to ingest, normalize, and process it at a scale of over 500 trips per second with full system observability.Responsibilities Build pipelines that source data from operational systems and then process and organize it to optimize for R&D efforts across Arity Apply appropriate methodologies for the problem using a variety of tool sets and writing code in Scala or Python.Create highly reusable and reliable code and leverage CI/CD principles to create robust data applications Explore data across the company and work cross-functionally to find opportunities for new data sets that can support our products Ensure projects have appropriate measures of success that support data-driven development Create reusable validations of data pipelines and guide the implementation of monitoring Support a culture of reproducibility via peer review, code review, and documentation Drive continuous improvement of data science and data engineering practices to create world-class capabilities Influence analytics strategy and roadmap with your combination of data engineering expertise and domain experience Build the Arity technical brand by engaging in conferences, meetups, blogs, and other external public engagements Drive recruiting by building relationships in the industry and supporting our interviewing efforts 
Qualifications Bachelor’s degree in Geospatial Science, Mathematics, Statistics, Physics, Computer Science, Engineering, or related quantitative field. A Master's degree is preferred. At least five years of industry experience in data engineering or related roles such as Software EngineeringRemote employment experienceExperience with several of EMR, Spark, Kinesis, Athena, and AirflowRespected by peers for technical prowess in Scala. Python is a plusAbility to translate business problems into well-defined data and analytics problems with quantitative success measures Experience driving end-to-end data engineering projects to generate measurable business value, including ideation, development, deployment, and maintenance & monitoring Ability to envision and articulate radical change through highly innovative thought leadership Inspires, mentors, and enables team to be bold and deliver high quality work Comfortable in a fast-paced environment with high ambiguity; inspires others to embrace these conditions 
Nice to Have Geospatial, sensor, or telematics experience is quite valuable, but not required Docker, Kubernetes, CI/CD, TerraformData Science and machine learning (Pandas, Scikit learn) You write code to transform data between data models and formats, preferably in Scala / Spark, Python or PySpark.Experience moving trained machine learning models
Compensation offered for this role is $130,400.00-$179,675.00 per year and is based on experience and qualifications.The candidate(s) offered this position will be required to submit to a background investigation, which includes a drug screen.That’s the day-to-day, now let’s talk about the rest of it. As we mentioned, Arity was founded by The Allstate Corporation. But you’ll be working for—and at—Arity. It’s the best of both worlds. You’ll get access to the full suite of Allstate benefits and work in a fast-paced startup culture. That’s more than just free breakfasts and brain breaks. It’s a culture that encourages you to be you.Sound like a fit? Apply now! We can’t wait to meet you.Arity.com Instagram Twitter LinkedInAllstate generally does not sponsor individuals for employment-based visas for this position.Effective July 1, 2014, under Indiana House Enrolled Act (HEA) 1242, it is against public policy of the State of Indiana and a discriminatory practice for an employer to discriminate against a prospective employee on the basis of status as a veteran by refusing to employ an applicant on the basis that they are a veteran of the armed forces of the United States, a member of the Indiana National Guard or a member of a reserve component.For jobs in San Francisco, please click ""here"" for information regarding the San Francisco Fair Chance Ordinance.For jobs in Los Angeles, please click ""here"" for information regarding the Los Angeles Fair Chance Initiative for Hiring Ordinance.To view the “EEO is the Law” poster click “here”. This poster provides information concerning the laws and procedures for filing complaints of violations of the laws with the Office of Federal Contract Compliance ProgramsTo view the FMLA poster, click “here”. This poster summarizing the major provisions of the Family and Medical Leave Act (FMLA) and telling employees how to file a complaint.It is the Company’s policy to employ the best qualified individuals available for all jobs. Therefore, any discriminatory action taken on account of an employee’s ancestry, age, color, disability, genetic information, gender, gender identity, gender expression, sexual and reproductive health decision, marital status, medical condition, military or veteran status, national origin, race (include traits historically associated with race, including, but not limited to, hair texture and protective hairstyles), religion (including religious dress), sex, or sexual orientation that adversely affects an employee's terms or conditions of employment is prohibited. This policy applies to all aspects of the employment relationship, including, but not limited to, hiring, training, salary administration, promotion, job assignment, benefits, discipline, and separation of employment"
"Data Engineer (Redshift) -- Fulltime / 12 Months Contract -- Los Angeles, CA - Onsite",Lorven Technologies Inc.,United States (Remote),https://www.linkedin.com/jobs/view/3681083408/?eBP=JOB_SEARCH_ORGANIC&refId=LrRO5gXX%2Fb1L%2BdZGZl1WXw%3D%3D&trackingId=YBY02oLmJEXPMOWMeE10pA%3D%3D&trk=flagship3_search_srp_jobs,3681083408,"About the job
            
 
Job Title: Data Engineer(Redshift)Job Location: Los Angeles, CA - OnsiteDuration : Fulltime / 12 Months ContractJob DescriptionNeed Strong experience in below skill sets. InformaticaPythonSQLRedshift"
4242 - Data Engineer - Remote | WFH,Get It Recruit - Information Technology,"Dayton, OH (Remote)",https://www.linkedin.com/jobs/view/3769084688/?eBP=JOB_SEARCH_ORGANIC&refId=LrRO5gXX%2Fb1L%2BdZGZl1WXw%3D%3D&trackingId=RvUwGE3lEqg%2FDMqhE6U%2F%2FQ%3D%3D&trk=flagship3_search_srp_jobs,3769084688,"About the job
            
 
We are a dynamic and innovative company at the forefront of data engineering, data management, and analytics. As a Data Engineer with us, you'll embark on exciting projects, collaborating with a diverse team of data scientists, analysts, business users, and IT professionals. Join us in designing, implementing, and deploying cutting-edge data services and analytics solutions.ResponsibilitiesLeverage your expertise with a minimum of 6 years in data engineering.Showcase Your Strong SQL Skills Across Multiple Database Platforms.Work with tools such as Snowflake, Databricks, Spark SQL, PySpark, and Python.Utilize cloud platforms like Azure, AWS, or GCP.Develop and maintain ETL pipelines for seamless data flow.Apply your knowledge of database design principles.Engage in data modeling, schema development, and comprehensive data-centric documentation.Experience in integrating data from diverse sources.Provide recommendations on optimal data models for ingestion, integration, and visualization.Enhance code performance and optimize queries for efficiency.Implement Continuous Integration/Continuous Delivery (CI/CD) concepts to engineer a standardized data environment.Demonstrate outstanding problem-solving skills.Communicate effectively with a diverse group, including executives, managers, and subject matter experts.RequirementsMinimum 6 years of data engineering experience.Proficient in SQL across multiple database platforms.Familiarity with Snowflake, Databricks, Spark SQL, PySpark, and Python.Cloud experience with Azure, AWS, or GCP.Proven experience in developing and maintaining ETL pipelines.Strong foundation in database design principles.Expertise in data modeling, schema development, and data-centric documentation.Ability to integrate data from various source types.Knowledge of code performance improvement and query optimization.Experience with CI/CD concepts for a standardized data environment.Exceptional problem-solving skills.Excellent verbal and written communication skills.Location: Varies by client: Columbus, OH, US | Cincinnati, OH, US | Dayton, OH, US | Remote ConsideredSalary Range: $100,000-$140,000BenefitsGenerous PTOMedical InsuranceDental & Vision Insurance401K MatchShort/Long Term Disability InsuranceJoin us in shaping the future of data engineering and analytics! Apply now and become an integral part of our innovative team.Employment Type: Full-Time"
Data Engineer (Streaming Platforms )@ Remote,Diverse Lynx,United States (Remote),https://www.linkedin.com/jobs/view/3764421953/?eBP=JOB_SEARCH_ORGANIC&refId=LrRO5gXX%2Fb1L%2BdZGZl1WXw%3D%3D&trackingId=CCLV2H4pNPQ4OXp5KA1tUw%3D%3D&trk=flagship3_search_srp_jobs,3764421953,"About the job
            
 
Title  Data Engineer (Streaming Platforms )  Duration  9&plus; Months   Location  Remote customer is doing near  real-time data integration. They do streaming analytics and they emphasize on  Live Table Structured streaming, Kafka, Event Hub, Apache Spark  . These technologies are definitely required to do near real-time data integration.Job Description  Streaming Platforms Mandatory BS, MS, or PhD in Computer Science, Information Technology, Management Information Systems (MIS), Data Science or related field 2&plus; years of experience in Apache Spark (PySpark / Spark SQL) 2&plus; years of experience in Python Experience with large scale streaming platforms (e.g. Kafka, Event Hub, Databricks Live Table / Structured Streaming), processing frameworks (e.g. Spark, Databricks) and storage engines stream analytics 3&plus; years of experience in data engineering, data integration, data modeling, data architecture, and ETL/ELT processes to provide quality data and analytics solutions 3&plus; years of experience in SQL with designing complex data schemas and query performance optimization Highly proficient working in Azure cloud environment (e.g. Blob / ADLS, Databricks, Azure Data Factory, Event Hub) Excellent communication skills ability to communicate technical concepts to both technical and non-technical audiences Experience in Regular Expression, CI/CD technology, Terraform and Git"
Data Engineer,Adame Services,United States (Remote),https://www.linkedin.com/jobs/view/3773322808/?eBP=JOB_SEARCH_ORGANIC&refId=LrRO5gXX%2Fb1L%2BdZGZl1WXw%3D%3D&trackingId=K4ajY37ozClDWNvC1mn1tA%3D%3D&trk=flagship3_search_srp_jobs,3773322808,"About the job
            
 
Need a full submittal to consider any candidate. Spell and grammar checked resume without bolded skills on it, completed information sheet by the candidate with a good summary of their experience relative to the job description (not a general summary), hand held copy of the DL and EAD/GC or H1B. Need a solid LinkedIn profile with headshot photo and resume details. LinkedIn references strongly preferred.All 4 documents need to be send in one email. Job Title: Data Engineer (MarTech, AdTech, Data Profiling)Location: Memphis, TN or REMOTEDuration: 18 Months + Extensions (Long term contract. Could possibly go perm if candidate desires)Compensation: – Up to $55-58/hr c2cStart Date: ASAPClient : ALSACNumber of Openings: 1Job DescriptionData Engineer excited about marketing tech, ad-tech, and customer data profiling. Responsible for working with large data sets and developing data pipelines that move data from source systems to segmentation systems, advertising platforms, data warehouses, data lakes, and other data storage and processing systems. The data engineer will prepare data for synthesis and analysis by CDP and other marketing tech systems.Requirements Solid programming skills (J2EE, Python), statistics knowledge, analytical skills, and an understanding of big data technologies Strong knowledge of software engineering principles and techniques Data Warehousing & ETL Data architecture & pipelining Understanding of Salesforce and Adobe Marketing stacks Experience with third party databases, libraries, interfaces, and internet protocols Knowledge of LINUX, J2EE, Relational and Document Databases, JSON, Shell Scripting, automation 
Relevant SkillsSoftware/TechnologyYears Of ExperienceDate Last Used(MM/YYYY)Total IT Exp.Total Data Engineer Exp.Marketing Technology (MarTech) Exp.Advertising Technology (AdTech) Exp.Exp. w/ Customer Data ProfilingExp. w/ J2EE, Python, statistics knowledge, analytical skills, and an understanding of big data technologiesData Warehousing and ETLData architecture and pipeliningExp. w/ Salesforce and Adobe Marketing stacksExperience with third party databases, libraries, interfaces, and internet protocolsLINUXRelational and Document DatabasesJSONShell ScriptingAutomation"
"Cloud Data Engineer, GCP",Damco Solutions,United States (Remote),https://www.linkedin.com/jobs/view/3775485391/?eBP=JOB_SEARCH_ORGANIC&refId=LrRO5gXX%2Fb1L%2BdZGZl1WXw%3D%3D&trackingId=3ghKgK%2BqbW9xPtMcBNEIpg%3D%3D&trk=flagship3_search_srp_jobs,3775485391,"About the job
            
 
SummaryOur client is seeking a skilled GCP DevOps / DBA Engineer to join their team. In this role, you will be responsible for ensuring the stability, performance, and security of their cloud-based infrastructure and databases. You will also work closely with developers to design and implement efficient database architectures and optimize database performance. Responsibilities Build and maintain our cloud-based infrastructure using Google Cloud Platform (GCP).Design, deploy, and manage databases, ensuring high availability and scalability.Develop and implement backup and disaster recovery strategies.Monitor and optimize database performance to ensure maximum efficiency and uptime.Collaborate with developers to design and implement efficient database architectures.Troubleshoot and resolve database issues in a timely manner.Automate tasks using Terraform and other automation tools.Ensure security of databases and infrastructure by implementing best practices and staying up to date with security trends and threats.Provide guidance and support to other team members on database and infrastructure-related issues.
Requirements Bachelor's degree in computer science or a related field.Direct experience as a DevOps engineer working with GCP.Strong experience with database administration, particularly with PostgreSQL.Direct experience with Terraform and Snowflake.Experience with AWS is a plus.Strong analytical and problem-solving skills.Excellent communication and collaboration skills.
 Top Skills:1. Strong experience as a GCP DevOps Engineer and Database Administrator. They are looking for a blend of DevOps skills and database / data warehouse skills. 2. For DevOps they are looking for direct experience with GCP, Terraform, and Bamboo (can have experience with other CI/CD tools instead).3. On the Database side, they are looking for a resource with extensive Postgres and Snowflake experience."
Senior Data Engineer,BambooHR,"Charlotte, NC (Remote)",https://www.linkedin.com/jobs/view/3760296665/?eBP=JOB_SEARCH_ORGANIC&refId=LrRO5gXX%2Fb1L%2BdZGZl1WXw%3D%3D&trackingId=R0F%2BybCHd5Z01EeJx9ToHg%3D%3D&trk=flagship3_search_srp_jobs,3760296665,"About the job
            
 
About UsOur mission is simple: we want to set people free to do meaningful work. People love our software—and it turns out that people love working here too. We've been recognized as a ""Best Company to Work For"" and we're proud of our team for creating software that makes an impact in the lives of HR pros and employees all over the world.What You'll DoAs a Senior Data Engineer on the data platform team, we'll rely on your expertise across multiple disciplines to develop, deploy and support data systems, data pipelines, data lakes, and lakehouses. Your ability to automate, performance tune, and scale the data platform will be key to your success.Your initial areas of focus will include: Collaborate with stakeholders to make effective use of core data assetsWith Spark and Pyspark libraries, load both streaming and batched dataEngineer lakehouse models to support defined data patterns and use casesLeverage a combination of tools, engines, libraries, and code to build scalable data pipelinesWork within an IT managed AWS account and VPC to stand up and maintain data platform development, staging, and production environmentsDocumentation of data pipelines, cloud infrastructure, and standard operating proceduresExpress data platform cloud infrastructure, services, and configuration as codeAutomate load, scaling, and performance testing of data platform pipelines and infrastructureMonitor, operate, and optimize data pipelines and distributed applicationsHelp ensure appropriate data privacy and securityAutomate continuous upgrades and testing of data platform infrastructure and servicesBuild data pipeline unit, integration, quality, and performance testsParticipate in peer code reviews, code approvals, and pull requestsIdentify, recommend, and implement opportunities for improvement in efficiency, resilience, scale, security, and performance
What You Need to Get the Job Done (if you don't have all, apply anyway!) Experience developing, scaling, and tuning data pipelines in Spark with PySparkUnderstanding of data lake, lakehouse, and data warehouse systems, and related technologiesKnowledge and understanding of data formats, data patterns, models, and methodologiesExperience storing data objects in hadoop or hadoop like environments such as S3Demonstrated ability to deploy, configure, secure, performance tune, and scale EMR and Spark Experience working with streaming technologies such as Kafka and KinesisExperience with the administration, configuration, performance tuning, and security of database engines like Snowflake, Databricks, Redshift, Vertica, or GreenplumAbility to work with cloud infrastructure including resource scaling, S3, RDS, IAM, security groups, AMIs, cloudwatch, cloudtrail, and secrets managerUnderstanding of security around cloud infrastructure and data systemsGit-based team coding workflows
Bonus Skills (Not Required, So Apply Anyway!) Experience deploying and implementing lakehouse technologies such as Hudi, Iceberg, and DeltaExperience with Flink, Presto, Dremio, Databricks, or KubernetesExperience with expressing infrastructure as code leveraging tools like TerraformExperience and understanding of a zero trust security frameworkExperience developing CI/CD pipelines for automated testing and code deploymentExperience with QA and test automationExposure to visualization tools like Tableau
Beyond the technical skills, we're looking for individuals who are: Clear communicators with team members and stakeholdersAnalytical and perceptive of patternsCreative in codingDetail-oriented and persistentProductive in a dynamic setting
If you love to learn, you'll be in good company. You'll likely have a Bachelor's degree in computer science, information systems, or equivalent working experience.An Equal Opportunity Employer--M/F/D/VBecause our team members are trusted to handle sensitive information, we require all candidates that receive and accept employment offers to complete a background check before being hired.For information on our Privacy Policy, click here."
Data Platform Engineer – 100% Remote- Full time,IBU Consulting,"Michigan, ND (Remote)",https://www.linkedin.com/jobs/view/3667478492/?eBP=JOB_SEARCH_ORGANIC&refId=LrRO5gXX%2Fb1L%2BdZGZl1WXw%3D%3D&trackingId=yuuVF2LdYHHwG19o9BiVoA%3D%3D&trk=flagship3_search_srp_jobs,3667478492,"About the job
            
 
QualificationsJOB DESCRIPTION: Significant experience implementing advanced data and analytics solutions on a modern data platform (e.g., Dremio, Databricks, Snowflake)Experience working with Kubernetes and dealing with networking policies and configurations.Experience working with and configuring enterprise data visualization tools (e.g., Qlik, Tableau, Power BI)Ability to effortlessly build rapport and maintain close working relationship with technical colleagues.5+ years' experience with SQL5+ years' experience writing Scala, Python, or JavaYou will be familiar with Git and DevOps tooling.You will be familiar with containerization and the surrounding ecosystem, including secrets management.Experience working with Agile methodologies.
Additional Qualifications Experience configuring and working with Kafka.Experience working with data masking and anonymization techniques.Experience working across both modern and legacy data sources and structures.Experience working with DBT or other modern data development tools.Good understanding of Front/Backend developmentExposure to Workflow and Decision automation (e.g., Camunda, Airflow)"
IBM CloudPak for Data/Watson Engineer,IVY TECH SOLUTIONS INC,"Woonsocket, RI (Remote)",https://www.linkedin.com/jobs/view/3667175683/?eBP=JOB_SEARCH_ORGANIC&refId=LrRO5gXX%2Fb1L%2BdZGZl1WXw%3D%3D&trackingId=QGWU%2FvBoomh%2F0Cvsxdwe1Q%3D%3D&trk=flagship3_search_srp_jobs,3667175683,"About the job
            
 
HI,Kindly let me know if you have a suitable fit for the following positionThanksTitle: IBM CloudPak for Data/Watson EngineerLocation: 100% remoteDuration: 6 month contract + extensionsPlease send the resume to  or 847- 350-1008Must haves:  7+ years as an infrastructure engineer Expertise with IBM products (CloudPak 4 Data + IBM Watson) Knowledge in the following areas: CP4D Admin - Openshift, PortWorx, CP4D patching, installation, operational management Sub Products – Watson Knowledge Studio, Watson Discovery, WAVI Assistant Voice & Voice Gateway
Charan Kumar | IVY Tech Sols Inc.3403 N Kennecott Avenue, Suite B&C Arlington Heights, IL 60004( Direct: (847) 350-1008   |Gtalk : charan.ivytech|
Powered by JazzHRmCjms8v4dV"
TEST DATA ENGINEER with GenRocket experience -REMOTE-EST Or CST Only,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3670706223/?eBP=JOB_SEARCH_ORGANIC&refId=LrRO5gXX%2Fb1L%2BdZGZl1WXw%3D%3D&trackingId=lYToIoY%2Fl%2FwkkYz3CT8owQ%3D%3D&trk=flagship3_search_srp_jobs,3670706223,"About the job
            
 
GenRocket is MUST have. Additionally, I would like to see minimum of 1+ yr. of implementation experience.  Must required: GenRocket, scripting, PostGres/DB ability, API work Determine best practice(s) around synthetic, masked data in DCE/Platform Analyze and understand how data flows in data hub Determine best way to manage and create synthetic data in Data Hub; deliver solution Figure out a solution to mock massive accounts coming through APIs Understanding data models, table structures and dependencies. Guide teams in generating new synthetic data, creating more, and maintaining over time. Create PoC for syntheitc data use cases working with Product team Agile Mindset, familiarity with Azure Cloud ADO experience, python or powershell scripting experience
Must required: GenRocket, scripting, PostGres/DB ability, API workDetermine best practice(s) around synthetic, masked data in DCE/PlatformAnalyze and understand how data flows in data hubDetermine best way to manage and create synthetic data in Data Hub; deliver solutionFigure out a solution to mock massive accounts coming through APIsUnderstanding data models, table structures and dependencies.Guide teams in generating new synthetic data, creating more, and maintaining over time.Create PoC for syntheitc data use cases working with Product teamAgile Mindset, familiarity with Azure CloudADO experience, python or powershell scripting experience"
Data Engineer | W2 only | REMOTE,Sciprex,United States (Remote),https://www.linkedin.com/jobs/view/3674898191/?eBP=JOB_SEARCH_ORGANIC&refId=LrRO5gXX%2Fb1L%2BdZGZl1WXw%3D%3D&trackingId=OIGxCWmglfmmhzQ0Q%2Bn45A%3D%3D&trk=flagship3_search_srp_jobs,3674898191,"About the job
            
 
Title: Data EngineerLocation: RemoteDuration: 2-YearsMust-HavesStrong Data Engineering background (prefer someone with 10+ years of experience)Very Strong AWS Hands-on ExperienceEC2LambdaStep FunctionsEMRSQS/SNSExpert skills with Python & PySparkExperience in Tableau (nice-to-have: Tableau Prep)Nice-to-HavesDatabase skills and SQL skills are highly preferred.AWS certification(s)."
Data Quality Engineer (Databricks),Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3774724574/?eBP=JOB_SEARCH_ORGANIC&refId=LrRO5gXX%2Fb1L%2BdZGZl1WXw%3D%3D&trackingId=LTOwEh%2FzfMCFd7Xh7lQ9lw%3D%3D&trk=flagship3_search_srp_jobs,3774724574,"About the job
            
 
Candidate must have Selenium and Databricks experience to be considered.Job: Data Quality Engineer (Databricks)Location: Remote 100%Term: 8+ months. Likely to extend to 12+ months. Start date is targeted for 11/13. Please confirm this with your candidate 
SummaryMcLaren has been hired by a Fortune 500 Consumer Packaged Goods (CPG) company to assist with implementing data mesh within Databricks. The client has been hoping to deploy this initiative and struggling so McLaren is coming in to assist. They will be moving the primary source data from SAP and Snowflake into Databricks.They have 120 data product they are building out. The goal is to do 2-week sprints for 2-4 data product deployments at a time.Responsibilities Implement data validation routines to ensure data accuracy.Monitor and improve data quality through validation and cleansing.Collaborate with data developers to address data-related issues.
Requirements 5+ years QA experiencePrevious experience working on a data mesh or databricks implementationExperience as main POC for all QA activities on a projectQuality Assurance Methodologies (e.g., Agile testing, Test Driven Development)Test Planning and StrategyTest Automation (e.g., Selenium, JUnit)Defect Management Tools (e.g., Jira, Bugzilla)Performance Testing"
Senior Python Engineer | Data Engineer.,IVY TECH SOLUTIONS INC,"Chicago, IL (Remote)",https://www.linkedin.com/jobs/view/3667177169/?eBP=JOB_SEARCH_ORGANIC&refId=LrRO5gXX%2Fb1L%2BdZGZl1WXw%3D%3D&trackingId=Ml451fIGksJzfRx5Jkd8Ow%3D%3D&trk=flagship3_search_srp_jobs,3667177169,"About the job
            
 
HI,Kindly let me know if you have a suitable fit for the following positionThanksSenior Python Engineer | Data Engineer. Location: Chicago, ILDuration: Long-termInitially Remote(only w2)Please send the resume to  or 847- 350-1008WORK TO BE PERFORMED: My client does not use any ETL tools – All pipelines are custom built from scratch with Python and SQL. Participate in Requirements Gathering: work with key business partner groups and other Data Engineering personnel to understand department-level data requirements for the analytics platform. Design Data Pipelines: work with other Data Engineering personnel on an overall design for flowing data from various internal and external sources into the Analytics platform. Build Data Pipelines from scratch with Python and SQL: leverage standard toolset and develop ETL/ELT code to move data from various internal and external sources into the analytics platform. Support Data Quality Program: work with Data QA Engineer to identify automated QA checks and associated monitoring. Key contributor to defining, implementing, and supporting: Data Services Data Dictionary 
SKILL AND EXPERIENCE REQUIRED:  Strong ELT/ ETL designer/developer Strong SQL & Python Expert level Performance tuning with SQL Structured & unstructured data expertise Cloud environment development & operations experience (GCP, AWS) Preference for candidates experienced with: Google Cloud Platform (GCP) and associated services; e.g., Big Query, GCS, Cloud Composer, Dataproc, Dataflow, Dataprep, Cloud Pub/Sub, Metadata DB, Data Studio, Datalab, other tools Apache Airflow (scheduler), Bitbucket Real-time data replication/streaming tools Data Modeling 
Charan Kumar | IVY Tech Sols Inc.3403 N Kennecott Avenue, Suite B&C Arlington Heights, IL 60004( Direct: (847) 350-1008   |Gtalk : charan.ivytech|
Powered by JazzHRQ702cvVvwR"
Workplace Analytics Engineer,Canonical,"Spokane County, WA (Remote)",https://www.linkedin.com/jobs/view/3735322324/?eBP=JOB_SEARCH_ORGANIC&refId=LrRO5gXX%2Fb1L%2BdZGZl1WXw%3D%3D&trackingId=JUE4BuwnWBI4itHRYXXw%2FQ%3D%3D&trk=flagship3_search_srp_jobs,3735322324,"About the job
            
 
Bring your people analytics, social science research and data mining/science skills to a unique team seeking to understand, and shape, the future of the digital workplace. We are interested in technology, of course, but we are also interested in the human mission of enabling the world's brightest and hardest working people to live where they want and work from anywhere. Most of our colleagues could move to a tech hub but they choose Canonical because of our mission and our approach to the workplace.We'd like to understand what really makes a distributed, remote-first workplace work. We think we're pretty good at this (being remote first for almost 20 years), but we know there is a lot still to understand, and the frontier of possibility continues to move outward. We'd like to invest in research, analytics and tooling which raises the bar even further for remote collaboration and organisation.If we are able to build tools that meaningfully improve our cooperation and our satisfaction, then we intend to share our insights with the world, both as a narrative and as SAAS or open source that helps other companies follow in our footsteps.The role of a Workplace Analytics Engineer at CanonicalSupport analytics and data mining in a cross-disciplinary team of organisational psychologists, web front end engineers, back end engineers and statistics / analytics experts to help us build a new definition for the 21st century digital workplace. Collaborate to figure out what really drives productivity, effectiveness and happiness in a remote-first globally distributed company.In addition to your existing people analytics work experience, this role will combine your skills in psychology, data analytics and visualisation, to help create a more effective workplace.Location: This role will be based remotely in the AMER region.All applicants applying must be legally authorized to work in the United States, as we cannot offer visa sponsorship for this job position.What your day will look like Utilize advanced data analytics to understand how we hire and how we work (productivity, happiness and effectiveness) across a global, remote first organisationFocus on quantitative and qualitative data analytics to find insights and meaningful business outcomesTell the story from the insights through dashboards, visualizations and presentationsDesign and conduct research into trends shaping talent science and remote workCollaborate with stakeholder teams (ex., engineering, information systems, etc) to improve the data and tool ecosystem supporting our digital workplace
What we are looking for in you Background in data science, mathematics, actuarial science, or engineeringFirst work experience in People Analytics Knowledge in advanced statistics, data sciences, coding/scripting languages (Python, R, etc), and databases (SQL, etc)Strength in data analytics and visualization (Looker Studio, Tableau, etc)Ability to translate business questions to key research objectivesAbility to identify the best methodology to execute research, synthesize and analyse findingsExcellent writing and communication skillsWillingness to examine the status quo and resilient in the face of challenges
What we offer youYour base pay will depend on various factors including your geographical location, level of experience, knowledge and skills. In addition to the benefits above, certain roles are also eligible for additional benefits and rewards including annual bonuses and sales incentives based on revenue or utilization. Our compensation philosophy is to ensure equity right across our global workforce.In addition to a competitive base pay, we provide all team members with additional benefits, which reflect our values and ideals. Please note that additional benefits may apply depending on the work location and, for more information on these, you can ask in the later stages of the recruitment process.🏠 Fully remote working environment - we've been working remotely since 2004!📚 Personal learning and development budget of 2,000USD per annum💰 Annual compensation review🏆 Recognition rewards🏝 Annual holiday leave👶 Parental Leave🧑‍💼 Employee Assistance Programme🧳 Opportunity to travel to new locations to meet colleagues at 'sprints'✈️ Priority Pass for travel and travel upgrades for long haul company eventsAbout CanonicalCanonical is a pioneering tech firm that is at the forefront of the global move to open source. As the company that publishes Ubuntu, one of the most important open source projects and the platform for AI, IoT and the cloud, we are changing the world on a daily basis. We recruit on a global basis and set a very high standard for people joining the company. We expect excellence - in order to succeed, we need to be the best at what we do.Canonical has been a remote-first company since its inception in 2004. Work at Canonical is a step into the future, and will challenge you to think differently, work smarter, learn new skills, and raise your game. Canonical provides a unique window into the world of 21st-century digital business.Canonical is an equal opportunity employerWe are proud to foster a workplace free from discrimination. Diversity of experience, perspectives, and background create a better work environment and better products. Whatever your identity, we will give your application fair consideration."
Data Engineer - 10+ Years,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3641969074/?eBP=JOB_SEARCH_ORGANIC&refId=LrRO5gXX%2Fb1L%2BdZGZl1WXw%3D%3D&trackingId=iqltSVNlK6BMiaGR4FZ%2Fhg%3D%3D&trk=flagship3_search_srp_jobs,3641969074,"About the job
            
 
Job Description  10+ years of overall experience5+ years of experience working on any cloud platform Experience enabling new tools in cloud platforms and making it enterprise readyWorked with large multi tenancy environment Hands on experience with the following IAM Provisioning VPC Networks and VPC Subnets.Cloud compute(cloud function, Kubernetes and VMs)"
Databricks Developer (Data Engineer—Databricks/ Azure),Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3731636871/?eBP=JOB_SEARCH_ORGANIC&refId=LrRO5gXX%2Fb1L%2BdZGZl1WXw%3D%3D&trackingId=91U6CseeMi82paCZJp5jPA%3D%3D&trk=flagship3_search_srp_jobs,3731636871,"About the job
            
 
Job: Databricks Developer (Data Engineer—Databricks/ Azure)Location: Remote 100%Term: 8+ months contract with extensionsStart date is targeted for 11/13. Please confirm this with the candidate.SummaryMcLaren has been hired by a Fortune 500 Consumer Packaged Goods (CPG) company to assist with implementing data mesh within Databricks. The client has been hoping to deploy this initiative and struggling so McLaren is coming in to assist. They will be moving the primary source data from SAP and Snowflake into Databricks.They have 120 data product they are building out. The goal is to do 2-week sprints for 2-4 data product deployments at a time.Responsibilities Develop and implement data pipelines and transformations on Databricks.Integrate data from various sources into the Databricks environment.Ensure code quality, optimization, and adherence to best practices.Collaborate with architects and business analysts to validate solutions.
Requirements 5+ years of experience in Databricks development and data engineering (AZURE)Apache SparkDatabricks NotebooksScala/Python ProgrammingData Transformation and ProcessingGit/GitHubData Pipeline Development"
Workplace Analytics Engineer,Canonical,"Phoenix, AZ (Remote)",https://www.linkedin.com/jobs/view/3738249120/?eBP=JOB_SEARCH_ORGANIC&refId=LrRO5gXX%2Fb1L%2BdZGZl1WXw%3D%3D&trackingId=ZXN5A0eUsKi0weXvATfMRg%3D%3D&trk=flagship3_search_srp_jobs,3738249120,"About the job
            
 
Bring your people analytics, social science research and data mining/science skills to a unique team seeking to understand, and shape, the future of the digital workplace. We are interested in technology, of course, but we are also interested in the human mission of enabling the world's brightest and hardest working people to live where they want and work from anywhere. Most of our colleagues could move to a tech hub but they choose Canonical because of our mission and our approach to the workplace.We'd like to understand what really makes a distributed, remote-first workplace work. We think we're pretty good at this (being remote first for almost 20 years), but we know there is a lot still to understand, and the frontier of possibility continues to move outward. We'd like to invest in research, analytics and tooling which raises the bar even further for remote collaboration and organisation.If we are able to build tools that meaningfully improve our cooperation and our satisfaction, then we intend to share our insights with the world, both as a narrative and as SAAS or open source that helps other companies follow in our footsteps.The role of a Workplace Analytics Engineer at CanonicalSupport analytics and data mining in a cross-disciplinary team of organisational psychologists, web front end engineers, back end engineers and statistics / analytics experts to help us build a new definition for the 21st century digital workplace. Collaborate to figure out what really drives productivity, effectiveness and happiness in a remote-first globally distributed company.In addition to your existing people analytics work experience, this role will combine your skills in psychology, data analytics and visualisation, to help create a more effective workplace.Location: This role will be based remotely in the AMER region.All applicants applying must be legally authorized to work in the United States, as we cannot offer visa sponsorship for this job position.What your day will look like Utilize advanced data analytics to understand how we hire and how we work (productivity, happiness and effectiveness) across a global, remote first organisationFocus on quantitative and qualitative data analytics to find insights and meaningful business outcomesTell the story from the insights through dashboards, visualizations and presentationsDesign and conduct research into trends shaping talent science and remote workCollaborate with stakeholder teams (ex., engineering, information systems, etc) to improve the data and tool ecosystem supporting our digital workplace
What we are looking for in you Background in data science, mathematics, actuarial science, or engineeringFirst work experience in People Analytics Knowledge in advanced statistics, data sciences, coding/scripting languages (Python, R, etc), and databases (SQL, etc)Strength in data analytics and visualization (Looker Studio, Tableau, etc)Ability to translate business questions to key research objectivesAbility to identify the best methodology to execute research, synthesize and analyse findingsExcellent writing and communication skillsWillingness to examine the status quo and resilient in the face of challenges
What we offer youYour base pay will depend on various factors including your geographical location, level of experience, knowledge and skills. In addition to the benefits above, certain roles are also eligible for additional benefits and rewards including annual bonuses and sales incentives based on revenue or utilization. Our compensation philosophy is to ensure equity right across our global workforce.In addition to a competitive base pay, we provide all team members with additional benefits, which reflect our values and ideals. Please note that additional benefits may apply depending on the work location and, for more information on these, you can ask in the later stages of the recruitment process.🏠 Fully remote working environment - we've been working remotely since 2004!📚 Personal learning and development budget of 2,000USD per annum💰 Annual compensation review🏆 Recognition rewards🏝 Annual holiday leave👶 Parental Leave🧑‍💼 Employee Assistance Programme🧳 Opportunity to travel to new locations to meet colleagues at 'sprints'✈️ Priority Pass for travel and travel upgrades for long haul company eventsAbout CanonicalCanonical is a pioneering tech firm that is at the forefront of the global move to open source. As the company that publishes Ubuntu, one of the most important open source projects and the platform for AI, IoT and the cloud, we are changing the world on a daily basis. We recruit on a global basis and set a very high standard for people joining the company. We expect excellence - in order to succeed, we need to be the best at what we do.Canonical has been a remote-first company since its inception in 2004. Work at Canonical is a step into the future, and will challenge you to think differently, work smarter, learn new skills, and raise your game. Canonical provides a unique window into the world of 21st-century digital business.Canonical is an equal opportunity employerWe are proud to foster a workplace free from discrimination. Diversity of experience, perspectives, and background create a better work environment and better products. Whatever your identity, we will give your application fair consideration."
Sr. Data Engineer || Remote,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3702365686/?eBP=JOB_SEARCH_ORGANIC&refId=LrRO5gXX%2Fb1L%2BdZGZl1WXw%3D%3D&trackingId=kGGvZ1nx4KLBLMG042npiw%3D%3D&trk=flagship3_search_srp_jobs,3702365686,"About the job
            
 
Must have 5+ years US based work experience Top Skills 5+ years of data engineering experience Experience providing Cloud solutions is a mustMust have Hadoop experienceEnd to end use case implementation, Ingestion to consumptionHands on with wide variety of use cases like ETL, Data Hubs, Data warehousing, Data lakesProgramming experience using PythonWorking knowledge in one of the ETL tools (Teradata/Informatica/Infoworks/Ab initio)Experience with Azure Cloud or AWS environments"
Data Integrations Engineer,Voyager Sopris Learning,"Dallas, TX (Remote)",https://www.linkedin.com/jobs/view/3771262015/?eBP=JOB_SEARCH_ORGANIC&refId=LrRO5gXX%2Fb1L%2BdZGZl1WXw%3D%3D&trackingId=l000k5LOqhX5QhTx5dbhIA%3D%3D&trk=flagship3_search_srp_jobs,3771262015,"About the job
            
 
Job Responsibilities Responds to requests via phone, case/email, and conference calls for data integration and technical support with Lexia products. Handles complex data integration cases and escalated customer requestsChampions Lexia customers by reproducing and reporting bugs and requesting features from our customers’ point of viewCollaborates with teammates and other departments to troubleshoot issues and advocate for customersKnows all Lexia products and data integration methods, in-depth, in order to explain them to customers, both external and internalContinually seeks to improve technical knowledge and service skills as well as Lexia’s understanding of and service to our customersHas a direct impact in making our customers successful through ensuring the voice and perspective of the customer is heard and improving our products based on that perspective.Creates content and updates documentation in the internal knowledgebase, templates, and help center articlesCollaborates with customers to gain a deep understanding of their intended outcomes, processes, system environments, and data infrastructure.
Job Requirements Minimum 3 years experience in technical customer supportEducation or EdTech experience preferredSuperb communication skills, both written and spokenExcellent collaborator with a proactive mindsetStrong knowledge of Software as a Service, web browser technologies, network technologies, SQL, Excel, and basic database conceptsSolid knowledge of data integration methods; previous knowledge of Clever or OneRoster integration is a plusFamiliarity with case tracking and customer relationship management software
To learn more about our organization and the exciting work we do, visit https://www.lexialearning.com/An Equal Opportunity EmployerWe are dedicated to fostering a culture that celebrates unique backgrounds, ideas, and experiences. All qualified applicants will receive consideration for employment without discrimination on the basis of race, color, age, religion, sex, gender, gender identity/expression, sexual orientation, national origin, protected veteran status, or disability."
Senior Data Engineer - Remote EU/UK,Syrup Tech,United States (Remote),https://www.linkedin.com/jobs/view/3729489299/?eBP=JOB_SEARCH_ORGANIC&refId=LrRO5gXX%2Fb1L%2BdZGZl1WXw%3D%3D&trackingId=snLoKZrbrPyxinhT10bhgg%3D%3D&trk=flagship3_search_srp_jobs,3729489299,"About the job
            
 
About Syrup🌍 In a world with deep supply chain disruptions, true omni-channel networks, and sustainability-minded consumers, there is a need for next-generation inventory systems.🍯 That’s why we are building Syrup, an AI-powered predictive software for inventory decisions in commerce. By leveraging more data in real-time, we empower merchandising and planning teams with allocation, ordering, and planning recommendations.🚀 Our mission is for commerce to no longer be a wasteful industry, as our intelligent platform empowers inventory excellence at every step in the value chain.👔 We're VC-backed by Google's AI fund (Gradient Ventures), as well as former executives from Adidas, Zalando, ASOS, Reebok, Bonobos, Salesforce, ThredUp, and Stripe. We're working with fast-growing brands such as Faherty, Reformation, and Desigual.About The RoleAs a Senior Data Engineer, you will be an integral part of our core team, driving the development of our groundbreaking data ingestion and processing pipeline. This role offers a unique opportunity to contribute to a predominantly greenfield project in a fast-growing, machine-learning startup. You will collaborate closely with a dynamic and diverse team of data scientists, software engineers, business experts, and visionaries.In this critical engineering function, you will play a key role in revolutionizing how inventory decisions are made for apparel brands and retailers. Join us on our mission to empower 100x better decision-making through the creation of intuitive and predictive software!You Will Lead the development and implementation of our state-of-the-art data ingestion and processing pipeline. Partner with product managers, data scientists, and other engineers to deliver an accurate and robust data infrastructure managing first and second-party data. Collaborate on a predominantly greenfield project, leveraging your expertise to drive innovation and deliver impactful solutions. Contribute to a fast-growing startup, where your ideas and contributions will have a direct and meaningful impact. Be part of a dynamic and diverse team that values collaboration, creativity, and pushing the boundaries of what's possible. Play a pivotal role in building intuitive and predictive software that will redefine decision-making processes in the industry. 
You Have Proven experience in a fast-paced startup environment, successfully building new systems from prototype to production. Collaborative mindset, able to work with customers, partners, Customer Success, Product and Analytics teams. Demonstrated pragmatic technical judgment in selecting and adapting data processing infrastructure, with a focus on scalability and building a robust data pipeline. Ability to start small and incrementally extend application functionality while improving scalability and performance. Strong commitment to operational excellence, ensuring the smooth operation of an always-on, always-shipping SAAS product. 4+ years of experience as a Data Engineer
Deep technical expertise in: Information Architecture, Data Engineering, and Data Warehousing, including proficiency in maintaining data quality, data versioning, and data documentation. Database architecture and data modeling, optimizing index performance, handling large time series datasets, working with stored procedures, star schema, snowflake schema, dimension tables, and fact tables. In-depth knowledge of PostgreSQL and its optimization techniques. Experience in optimizing data models and access patterns for modern warehouse and orchestration platforms such as Snowflake, Redshift, Databricks, Spark, DBT, Apache Airflow, and Prefect. Proficiency in Python, Pandas, and working with columnar formats like parquet and avro. Proven expertise in integrating large-scale external datasets, including tabular, text, and image data, through APIs, FTP, S3, or web scraping, building reliable and repeatable pipelines. Strong background in cloud deployment, infrastructure automation, and CI/CD, particularly with AWS S3, EC2, Lambda, and the ability to seamlessly integrate cloud-based infrastructure into the development lifecycle. Experience with data visualization/BI tools like Tableau, DOMO, PowerBI, or Looker is a plus
What We OfferFun, friendly, respectful, collaborative, diverse, driven, mission-oriented – that’s Syrup! We're a group of driven people who want to make a difference in the world by building game-changing software. We care deeply about people, planet and product. Concretely, we offer: Remote First: We’re a remote first team across Europe and East Coast North America. For our colleagues in New York, you can come into our office in SoHo as often as you like. Syrup summits: We bring the whole global team together every quarter for our Syrup summits. Previous locations: Barcelona, Madrid, Los Angeles, Warsaw, Boston, New York. Equity: We offer generous equity compensation for all our early employees. Generous health, vision, and dental coverage to our employees. Generous PTO to take all the time you need, whenever you need it. 
We offer competitive base pay with the potential salary range of 70,000 - 140,000 EUR, and a similarly competitive range in other locations, commensurate with experience, generous early-stage equity package, and benefits including medical/dental/vision."
Senior Data Engineer - Contractor,ComplexCare Solutions,"Bowie, MD (Remote)",https://www.linkedin.com/jobs/view/3766378090/?eBP=JOB_SEARCH_ORGANIC&refId=LrRO5gXX%2Fb1L%2BdZGZl1WXw%3D%3D&trackingId=sUkoZhspiqVTsVlDAUl4fA%3D%3D&trk=flagship3_search_srp_jobs,3766378090,"About the job
            
 
Overview: The Senior Data Engineer is responsible for contributing to data analytics, data operations, and software product development across the organization. They will support existing data operations such as data refresh ETL, data structure development, and troubleshooting of data issues. Additionally, they will contribute to development of new products through data analysis to inform requirements and creation of APIs to support software development. They will work across a variety of products and projects as needed and engage with a variety of data types. This position may require independent work, sharing information and assisting others with work request.Duties and Responsibilities: Research customer data related questions and provide mapping between requirements and data points. This person will be responsible for customer data releases which includes working with customers, data packaging, validation, and documentation; Proven analytical skills – Able to compare, contrast, and validate work with keen attention to detail; Develop predictive and other classification methods thorough understanding of healthcare specific data sets such as CMS LDS files, 837 and 835 EDI specifications; Develop APIs to allow for interaction with data sets from customer facing tools; Perform data updates on a routine basis to keep product data sets current; and Work and communicate in a cross-functional geographically dispersed team environment comprised of software engineers and product managers. Maintain compliance with Inovalon's policies, procedures and mission statement; Adhere to all confidentiality and HIPAA requirements as outlined within Inovalon's Operating Policies and Procedures in all ways and at all times with respect to any aspect of the data handled or services rendered in the undertaking of the position; and Fulfill those responsibilities and/or duties that may be reasonably provided by Inovalon for the purpose of achieving operational and financial success of the Company. 
Job Requirements: Minimum of 5 years healthcare industry experience with and strong knowledge of healthcare data formats such as CMS LDS files and/or X12/EDI claims processing (specifically 837I/837P); Strong working knowledge of Centers for Medicare & Medicaid Services (CMS) data; Comfortable writing SQL queries for data analysis; Comfortable working with Python to create APIs; Experience with AWS database tools; Ability to learn quickly and independently; Experience with HIPPA and PHI; Ability to effectively communicate with internal and external customers; Excellent verbal and written communication skills; Excellent computer proficiency (MS Office – Word, Excel and Outlook); Must be able to work under pressure and meet deadlines; and Ability to work independently and to carry out tasks to completion following standard accepted practices. 
Education: BS degree in Computer Science or Computer Engineering, Business, or equivalent experience. 
Physical Demands and Work Environment: Sedentary work (i.e., sitting for long periods of time); Exerting up to 10 pounds of force occasionally and/or negligible amount of force; Frequently or constantly to lift, carry push, pull or otherwise move objects and repetitive motions; Subject to inside environmental conditions; and Some travel (less than 10%) may be required for this position, primarily for training and collaboration purposes. 
Studies have shown that women and people of color are less likely to apply for jobs unless they believe they meet every one of the qualifications listed in a job description. If you don't meet every qualification listed but are excited about our mission and the work described, we encourage you to apply regardless. ComplexCare Solutions is most interested in finding the best candidate for the job and you may be just the right person for this or other roles.By embracing diversity, equity and inclusion we enhance our work environment and drive business success. ComplexCare Solutions strives to reflect the diversity of the communities where we operate and of our clients and everyone whom we serve. We endeavor to create a culture of inclusion in which our associates feel empowered to bring their full, authentic selves to work and pursue their professional goals in an equitable setting. We understand that by fostering this type of culture, and welcoming different perspectives, we generate innovation and growth.ComplexCare Solutions is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status. We also consider qualified applicants regardless of criminal histories, consistent with legal requirement.The Company maintains a drug free work environment for all of its associates, which includes employees, contractors and vendors. It is unlawful for associates to manufacture, sell, distribute, dispense, possess or use any controlled substance or marijuana in the workplace and doing so will result in disciplinary action, up to and including termination of employment or the contracted relationship."
IT ENGINEERING DATA ENGINEER,IVY TECH SOLUTIONS INC,"Vienna, VA (Remote)",https://www.linkedin.com/jobs/view/3667182026/?eBP=JOB_SEARCH_ORGANIC&refId=LrRO5gXX%2Fb1L%2BdZGZl1WXw%3D%3D&trackingId=gCeJSUWK6rLSxZ6t2OzErg%3D%3D&trk=flagship3_search_srp_jobs,3667182026,"About the job
            
 
HI,Kindly let me know if you have a suitable fit for the following positionThanksIT ENGINEERING ( DATA ENGINEER) Location: VADuration: 12+MonthsPlease send the resume to  or 847- 350-1008DATAWAREHOUSE, DB2, ETL, ORACLE, POWER BI, SQLWe will only accept the following: US Citizens, Green Card Holders, TN VisaDescription:Below are the primary skills required for the role: SQL, Oracle, DB2 serverHands on experience of building and maintaining artifacts in Datawarehouses, Data lakes etc for both on-prem (Teradata) and on-cloud (preferably on Azure)Hands on experience of using ETL tools such as InformaticaHands on experience on MS Azure; such as, Azure Data Factory , Azure Data Bricks(pyspark), Synapse, Azure DevOpsBusiness analytics and Intelligence tools such as Tableau, Power BI etcExperience with Agile frameworks
Charan Kumar | IVY Tech Sols Inc.3403 N Kennecott Avenue, Suite B&C Arlington Heights, IL 60004PH.( Direct: (847) 350-1008   |Gtalk : charan.ivytech|
Powered by JazzHR5dhv59Zytw"
Senior / Staff Data Engineer,Rad AI,United States (Remote),https://www.linkedin.com/jobs/view/3767184875/?eBP=JOB_SEARCH_ORGANIC&refId=LrRO5gXX%2Fb1L%2BdZGZl1WXw%3D%3D&trackingId=AgTA%2F0rZyRTCKjzlo0WHRA%3D%3D&trk=flagship3_search_srp_jobs,3767184875,"About the job
            
 
Rad AI is the fastest growing radiologist-led AI company. In addition to winning the 2021 award for ""Best New Radiology Vendor"" and 2023 award for ""Best New Radiology Software"" from AuntMinnie, Rad AI continued the momentum by earning a place on CB Insights' lists for ""Most Promising AI Companies,"" ""Most Innovative Digital Health Startups,"" and ""Most Promising Private Digital Health Companies"" in 2021, 2022, and 2023.Why Join Us: We are seeking a Staff or Senior Data Engineer to join our engineering team. Given our large client growth and projected movement in the year ahead, we're seeking to expand this team and add a strong resource on board who will play an instrumental role in the construction and maintenance of data collection systems, pipelines and management tools. This includes supervising both the functions of junior data engineers and the architectures themselves. This person would provide authority over the construction and continued operation of the Rad AI data systems. This position will report into our Director of Machine Learning and work alongside all critical parts of our business.Responsibilities: Design and implement the data architecture, ensuring scalability, flexibility, and efficiency using pipeline authoring tools like Metaflow and large-scale data processing technologies like SparkDefine and extend our internal standards for style, maintenance, and best practices for a high-scale data platformCollaborate with ML engineers and researchers to understand their data needs including model training and production monitoring systems and develop solutions that meet those requirementsProvide mentorship for all on your team to help them grow in their technical responsibilities. Support team members with the shipping of new features by setting direction and providing guidanceEnsure data quality, integrity, and security by implementing robust data validation, monitoring, and access controlsEvaluate and recommend data technologies and tools to improve the efficiency and effectiveness of the data engineering processContinuously monitor, maintain, and improve the performance and stability of the data infrastructure
Requirements: 6+ years relevant experience in data engineering Expertise in designing and developing distributed data pipelines using big data technologies on large scale data setsDeep and hands-on experience designing, planning, productionizing, maintaining and documenting reliable and scalable data infrastructure and data products in complex environmentsSolid experience with big data processing and analytics on AWS, using services such as Amazon EMR and AWS BatchExperience in large scale data processing technologies such as SparkExpertise in orchestrating machine learning workflows using tools like MetaflowExperience with various database technologies including SQL, NoSQL databases (e.g., AWS DynamoDB, ElasticSearch, Postgresql)Prior Software Engineering experience is a big plus
Nice to Have: Experience working at an early stage startup Experience in a HIPAA compliant environment Experience working on machine learning or healthcare related projects 
Rad AI offers a variety of benefits, including:Comprehensive Medical, Dental, Vision & Life insuranceHSA (with employer match), FSA, & DCFSA401(k)11 paid company holidaysLocation-flexibility (remote-first company!)Flexible PTO policyAnnual company-wide offsitePeriodic team offsitesAnnual equipment stipendCompany sponsored co-working space benefitFounded in 2018 by the youngest radiologist in US history, Rad AI has seen rapid adoption of its AI platform, and is already in use at 7 of the 10 largest private radiology practices in the US. Rad AI uses large language models and state-of-the-art machine learning / generative AI to streamline repetitive tasks for radiologists, which yields substantial time savings, alleviates burnout, and creates more time to focus on patient care. Its first product, Rad AI Omni, saves radiologists an average of 60 minutes per day, and helps achieve up to 20% time savings per report.Come join our world-class team as we build and deploy AI solutions that will make a difference in millions of people’s lives. Our team is mission-driven and focused on transparency, inclusion, close collaboration, and building an incredible team. Come and help us make a difference!Thank you for your interest. We look forward to hearing from you.At Rad AI, we value diversity and provide equal employment opportunities (EEO) to all employees and applicants without regard to race, color, religion, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. We will consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of the San Francisco Fair Chance Ordinance.The pay range at Rad AI is dependent upon a variety of factors such as years of experience, training, geographic location, and demands of the current market. That means that in some circumstances, the actual salary could fall outside of the above. The estimated range listed is subject to change and may be modified in the future."
Senior Data Engineer - Graph Specialist,Post,United States (Remote),https://www.linkedin.com/jobs/view/3696161512/?eBP=JOB_SEARCH_ORGANIC&refId=oxxoQjn826Ao1gqpbv6yGg%3D%3D&trackingId=gyx7B7FHxsDlojvVSmEfHg%3D%3D&trk=flagship3_search_srp_jobs,3696161512,"About the job
            
 
We’re looking for a Senior/Principal level Data Engineer with a Graph focus to help us build and polish the Post.News web application. This role is Remote within the U.S. We are not able to sponsor visas at this time. 
About UsMeet Post: A Social Platform for Real People, Real News, and Civil Conversations.Remember when social media was fun, introduced you to big ideas and cool people, and actually made you smarter? Remember when it didn't waste your time and make you angry or sad? When you could disagree with someone without being threatened or insulted? We want to bring that back with Post.What You’ll Be Doing:  Working alongside a small, scrappy, mission driven team. Collaborating with key stakeholders across other teams. Ownership of development end to end. Making an impact on an exciting new-comer to the online social space. Understanding and writing complex queries. Analyzing and Framing relationships between data sets. Working with Neptune/AWS/Neo4jReporting directly to the CTO 
What We’re Looking For: Significant experience working with and understanding complex queries. Amazon Neptune experience. AWS experience. Experience building out and analyzing relationships from pulled data. Preference to have worked with large data sets, at a large scale. 
Nice-To-Haves Start-up experience. Experience with social applications. Enterprise Social Networking experience. 
A Little About Us: Post is an extremely mission driven company. Enriching environment focused on diversity and inclusion. Post is still small and organizationally flat, meaning everyone, regardless of level/seniority is an individual contributor. 
Benefits: Competitive salary and equity. Unlimited PTO. Expect “Vacation?” in our one-on-one agenda until you start taking it 😅. Remote first. Unlimited co-working passes (e.g. WeWork) as needed. Top notch health insurance for you and your dependents, plus FSA, DCFSA, HSA, STD, LTD, basic life insurance coverage and free membership access to One Medical. 
If this sounds like you, please apply below and tell us about yourself. We look forward to hearing from you!Post’s success hinges on us hiring great people and creating an environment where we can be happy, feel challenged, and do our best work. We’re being deliberate about building that environment from the ground up. I hope that excites you enough to apply.Post provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability, genetics, sexual orientation, gender identity, or gender expression. We are committed to a diverse and inclusive workforce and welcome people from all backgrounds, experiences, perspectives, and abilities.Compensation Range: $150K - $200K"
Senior Data Engineer,BambooHR,"Charleston, SC (Remote)",https://www.linkedin.com/jobs/view/3760297281/?eBP=JOB_SEARCH_ORGANIC&refId=oxxoQjn826Ao1gqpbv6yGg%3D%3D&trackingId=lWVo3FPXFrQChlCqE%2FHi3A%3D%3D&trk=flagship3_search_srp_jobs,3760297281,"About the job
            
 
About UsOur mission is simple: we want to set people free to do meaningful work. People love our software—and it turns out that people love working here too. We've been recognized as a ""Best Company to Work For"" and we're proud of our team for creating software that makes an impact in the lives of HR pros and employees all over the world.What You'll DoAs a Senior Data Engineer on the data platform team, we'll rely on your expertise across multiple disciplines to develop, deploy and support data systems, data pipelines, data lakes, and lakehouses. Your ability to automate, performance tune, and scale the data platform will be key to your success.Your initial areas of focus will include: Collaborate with stakeholders to make effective use of core data assetsWith Spark and Pyspark libraries, load both streaming and batched dataEngineer lakehouse models to support defined data patterns and use casesLeverage a combination of tools, engines, libraries, and code to build scalable data pipelinesWork within an IT managed AWS account and VPC to stand up and maintain data platform development, staging, and production environmentsDocumentation of data pipelines, cloud infrastructure, and standard operating proceduresExpress data platform cloud infrastructure, services, and configuration as codeAutomate load, scaling, and performance testing of data platform pipelines and infrastructureMonitor, operate, and optimize data pipelines and distributed applicationsHelp ensure appropriate data privacy and securityAutomate continuous upgrades and testing of data platform infrastructure and servicesBuild data pipeline unit, integration, quality, and performance testsParticipate in peer code reviews, code approvals, and pull requestsIdentify, recommend, and implement opportunities for improvement in efficiency, resilience, scale, security, and performance
What You Need to Get the Job Done (if you don't have all, apply anyway!) Experience developing, scaling, and tuning data pipelines in Spark with PySparkUnderstanding of data lake, lakehouse, and data warehouse systems, and related technologiesKnowledge and understanding of data formats, data patterns, models, and methodologiesExperience storing data objects in hadoop or hadoop like environments such as S3Demonstrated ability to deploy, configure, secure, performance tune, and scale EMR and Spark Experience working with streaming technologies such as Kafka and KinesisExperience with the administration, configuration, performance tuning, and security of database engines like Snowflake, Databricks, Redshift, Vertica, or GreenplumAbility to work with cloud infrastructure including resource scaling, S3, RDS, IAM, security groups, AMIs, cloudwatch, cloudtrail, and secrets managerUnderstanding of security around cloud infrastructure and data systemsGit-based team coding workflows
Bonus Skills (Not Required, So Apply Anyway!) Experience deploying and implementing lakehouse technologies such as Hudi, Iceberg, and DeltaExperience with Flink, Presto, Dremio, Databricks, or KubernetesExperience with expressing infrastructure as code leveraging tools like TerraformExperience and understanding of a zero trust security frameworkExperience developing CI/CD pipelines for automated testing and code deploymentExperience with QA and test automationExposure to visualization tools like Tableau
Beyond the technical skills, we're looking for individuals who are: Clear communicators with team members and stakeholdersAnalytical and perceptive of patternsCreative in codingDetail-oriented and persistentProductive in a dynamic setting
If you love to learn, you'll be in good company. You'll likely have a Bachelor's degree in computer science, information systems, or equivalent working experience.An Equal Opportunity Employer--M/F/D/VBecause our team members are trusted to handle sensitive information, we require all candidates that receive and accept employment offers to complete a background check before being hired.For information on our Privacy Policy, click here."
Mortgage Data Engineer,IVY TECH SOLUTIONS INC,"Vienna, VA (Remote)",https://www.linkedin.com/jobs/view/3667180189/?eBP=JOB_SEARCH_ORGANIC&refId=oxxoQjn826Ao1gqpbv6yGg%3D%3D&trackingId=AZfRikxbL2OCzhHbHgW%2FvA%3D%3D&trk=flagship3_search_srp_jobs,3667180189,"About the job
            
 
HI,Kindly let me know if you have a suitable fit for the following positionThanksMortgage Data EngineerLocation: VADuration: 6 months temp-to-perm.Please send the resume to  or 847- 350-1008DATA MODELING, POWERSHELL, SQL, VBADescriptionWe will only accept the following: US Citizens, Green Card Holders, TN Visa, Green Card EAD's. ***Will work remotely. They will need to work in VA once we return to the office.OVERVIEW:There’s an immediate opening for a Mortgage Data Engineer in the Capital Markets division of a large financial institution that requires the use of programming, math, knowledge of automation tools, and analytical/business intelligence tools. Key responsibilities include the creation of end-to-end architectural solutions, portfolio & liquidity monitoring dashboards, and automation of existing processes. In addition, candidate is expected to independently collaborate with business units to develop actionable analytics. Position will have exposure to market risk, credit risk, liquidity risk, portfolio valuation and is expected to utilize a variety of technology tools.KEY RESPONSIBILITIES: Demonstrate a high degree of independence and out of box thinkingSupport Capital Markets in financial modeling, analytics, visualization, and data management.Develop/maintain data processes using Powershell, VBA, Tableau, R, SQL and other appropriate scripting/programming tools.Manage aggregation of data from multiple sources including different types of databases and flat filesCommunicate reporting and analysis to senior management.
OVERALL QUALIFICATIONS: Bachelor’s Degree or higher in Science/Engineering or related field.4+ years of experience with various aspects of data modeling/analytics/testing.2+ years of experience with a scripting/programming.Willingness to learn new languages and technology solutions.Must be a self-starter with a strong desire for continuous learning.Display strong communication skills (verbal, written, grammar, spelling) and be willing to ask questions and learn.Experience with mortgages/finance preferred, but not required.
Charan Kumar | IVY Tech Sols Inc.3403 N Kennecott Avenue, Suite B&C Arlington Heights, IL 60004PH.( Direct: (847) 350-1008   |Gtalk : charan.ivytech|
Powered by JazzHRI5wdqcz5sB"
Senior Data Engineer - Remote US,Syrup Tech,United States (Remote),https://www.linkedin.com/jobs/view/3729487533/?eBP=JOB_SEARCH_ORGANIC&refId=oxxoQjn826Ao1gqpbv6yGg%3D%3D&trackingId=XxE2bsJCzBbIqmqCrks6Yw%3D%3D&trk=flagship3_search_srp_jobs,3729487533,"About the job
            
 
About Syrup🌍 In a world with deep supply chain disruptions, true omni-channel networks, and sustainability-minded consumers, there is a need for next-generation inventory systems.🍯 That’s why we are building Syrup, an AI-powered predictive software for inventory decisions in commerce. By leveraging more data in real-time, we empower merchandising and planning teams with allocation, ordering, and planning recommendations.🚀 Our mission is for commerce to no longer be a wasteful industry, as our intelligent platform empowers inventory excellence at every step in the value chain.👔 We're VC-backed by Google's AI fund (Gradient Ventures), as well as former executives from Adidas, Zalando, ASOS, Reebok, Bonobos, Salesforce, ThredUp, and Stripe. We're working with fast-growing brands such as Faherty, Reformation, and Desigual.About The RoleAs a Senior Data Engineer, you will be an integral part of our core team, driving the development of our groundbreaking data ingestion and processing pipeline. This role offers a unique opportunity to contribute to a predominantly greenfield project in a fast-growing, machine-learning startup. You will collaborate closely with a dynamic and diverse team of data scientists, software engineers, business experts, and visionaries.In this critical engineering function, you will play a key role in revolutionizing how inventory decisions are made for apparel brands and retailers. Join us on our mission to empower 100x better decision-making through the creation of intuitive and predictive software!You Will Lead the development and implementation of our state-of-the-art data ingestion and processing pipeline. Partner with product managers, data scientists, and other engineers to deliver an accurate and robust data infrastructure managing first and second-party data. Collaborate on a predominantly greenfield project, leveraging your expertise to drive innovation and deliver impactful solutions. Contribute to a fast-growing startup, where your ideas and contributions will have a direct and meaningful impact. Be part of a dynamic and diverse team that values collaboration, creativity, and pushing the boundaries of what's possible. Play a pivotal role in building intuitive and predictive software that will redefine decision-making processes in the industry. 
You Have Proven experience in a fast-paced startup environment, successfully building new systems from prototype to production. Collaborative mindset, able to work with customers, partners, Customer Success, Product and Analytics teams. Demonstrated pragmatic technical judgment in selecting and adapting data processing infrastructure, with a focus on scalability and building a robust data pipeline. Ability to start small and incrementally extend application functionality while improving scalability and performance. Strong commitment to operational excellence, ensuring the smooth operation of an always-on, always-shipping SAAS product. 4+ years of experience as a Data Engineer
Deep technical expertise in: Information Architecture, Data Engineering, and Data Warehousing, including proficiency in maintaining data quality, data versioning, and data documentation. Database architecture and data modeling, optimizing index performance, handling large time series datasets, working with stored procedures, star schema, snowflake schema, dimension tables, and fact tables. In-depth knowledge of PostgreSQL and its optimization techniques. Experience in optimizing data models and access patterns for modern warehouse and orchestration platforms such as Snowflake, Redshift, Databricks, Spark, DBT, Apache Airflow, and Prefect. Proficiency in Python, Pandas, and working with columnar formats like parquet and avro. Proven expertise in integrating large-scale external datasets, including tabular, text, and image data, through APIs, FTP, S3, or web scraping, building reliable and repeatable pipelines. Strong background in cloud deployment, infrastructure automation, and CI/CD, particularly with AWS S3, EC2, Lambda, and the ability to seamlessly integrate cloud-based infrastructure into the development lifecycle. Experience with data visualization/BI tools like Tableau, DOMO, PowerBI, or Looker is a plus
What We OfferFun, friendly, respectful, collaborative, diverse, driven, mission-oriented – that’s Syrup! We're a group of driven people who want to make a difference in the world by building game-changing software. We care deeply about people, planet and product. Concretely, we offer: Remote First: We’re a remote first team across Europe and East Coast North America. For our colleagues in New York, you can come into our office in SoHo as often as you like. Syrup summits: We bring the whole global team together every quarter for our Syrup summits. Previous locations: Barcelona, Madrid, Los Angeles, Warsaw, Boston, New York. Equity: We offer generous equity compensation for all our early employees. Generous health, vision, and dental coverage to our employees. Generous PTO to take all the time you need, whenever you need it. 
We offer competitive base pay with the NYC salary range of $140,000 - $180,000, and a similarly competitive range in other locations, commensurate with experience, generous early-stage equity package, and benefits including medical/dental/vision."
Urgent Requirement________ETL/Data Engineer________(Remote),Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3661713208/?eBP=JOB_SEARCH_ORGANIC&refId=oxxoQjn826Ao1gqpbv6yGg%3D%3D&trackingId=3b0T1tUKsYVhKYdLj7YC8Q%3D%3D&trk=flagship3_search_srp_jobs,3661713208,"About the job
            
 
Hi,Please find attached Job Description. If you are interested please do share with me your updated resume or call me on ""302-440-2596"".Title:- ETL/Data EngineerLocation:- RemoteDuration:- 6+ Month (Remote)Visa:- Citizen, GC, GC-EAD, EAD-H4, TN VisaJob DescriptionRemote but prefer MST or PSTClient: Kaiser PermanenteEstimated/Targeted Start: ASAPOn-site/Remote: RemoteThe Minimum Skills They Will Need Are Listed Below 10+ years of migration from on-premise to cloud (large-scale PB in scope and size)10+ years of overall IT experience10+ years of ETL experience10+ years of SQL experience7+ years of experience administrating and developing using Informatica's Intelligent Cloud Services (IICS)5+ years of experience with Informatica Power Center5+ years of experience with the Azure platform5+ years of documenting processes2+ years of experience with Snowflake (SaaS)At least one Informatica IICS certification (Cloud Data and Application Integration, Product 360 SaaS R40 Developer, Cloud Data Quality)Excellent communication skillsBS degree in Comp/Sci, IT, or other technical disciplines
Nice To Have Snowflake certificationAzure CertificationMS degree in analytics or other technical disciplines
Additional RequirementsLooking for a highly technical Informatica resource.This is a six-month position to start immediately.The position is remote, with the resource working PST hours (08:00-17:00). The preference would be for PST or MST resource.It could be acceptable for the resource to live in CST or EST if they are willing to work the PST schedule .Ankit Kr AnandTalent Acquisition -North AmericaDirect:+1302-440-2596In my absence please reach out to Mr. Harish Sharma at harish@steneral.com & 302-721-6151"
Sr. Data Engineer (REMOTE),Syrinx Consulting,"New York, NY (Remote)",https://www.linkedin.com/jobs/view/3648834182/?eBP=JOB_SEARCH_ORGANIC&refId=oxxoQjn826Ao1gqpbv6yGg%3D%3D&trackingId=Mfwat43QfJF21SOte3kTEg%3D%3D&trk=flagship3_search_srp_jobs,3648834182,"About the job
            
 
Who We AreWorking with the leader in digital performance solutions, improving the impression quality and audience impact of digital advertising. Built on best practices, our solutions create value for media buyers and sellers by bringing transparency and accountability to the market, ensuring ad viewability, brand safety, fraud protection, accurate impression delivery and audience quality across campaigns to drive performance.This is a great opportunity to get in a remote role!! Please apply directly to ldavis@syrinx.comHeadquartered in New York City, DoubleVerify’s investors include JMI Equity, Institutional Venture Partners, Blumberg Capital, First Round Capital and Genacast Ventures. Learn more at doubleverify.com.OverviewDoubleVerify is seeking a data slayer, a one in a thousand DBA that loves to ask questions and be on the cutting edge of Big Data technology. The team is responsible for all technical data aspects of the company’s products. The company’s system is comprised of an extremely scalable, highly-available and rapidly-developing architecture, and includes big-data stores of all types – relational, massively-parallel-processing, and NoSQL. This individual will join a small yet highly capable and motivated team in order to maintain the system’s databases, and develop, integrate and deploy complex new modules – quickly and at scale.What You Will Do Develop and maintain Big Data system (Hadoop, Kafka, Hive, Spark)Maintain the system’s databases (Vertica, SQL Server, MongoDB)Develop, integrate and deploy complex new modules (TSQL, Python, Splunk)Design and support Database infrastructure, with consideration for performance, availability, and specific application requirements.Work closely with other departments on the implementation and deployment of new initiatives
Who You Are 5+ years of database development and/or administration.5+ Experience with Hadoop Big Data eco system and experience with Vertica is a mustSoftware development experience is preferredFast learner, creative thinker, problem solver. Must love BIG data!Accountable, dedicated and willing to be on-call as neededAdTech experience is a plus!"
Data Engineer with property and casualty experience || Remote,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3641964822/?eBP=JOB_SEARCH_ORGANIC&refId=oxxoQjn826Ao1gqpbv6yGg%3D%3D&trackingId=NKhqTpWWYQ%2B9sjC8zQxSdw%3D%3D&trk=flagship3_search_srp_jobs,3641964822,"About the job
            
 
Job Title: Data Engineer with property and casualty experienceLocation: 100% Remote {Someone open to work Eastern Standard Time (EST)}Duration: 12+ MonthsExperience Level: 12+ Years in Information Technology and as Data Engineer 7-8+ Years and recent Property and casualty experience in 2-3 projectsCandidate Details Required Full Name:Current Location with Zip Code:Phone:Email:LinkedIn Must:Education Details Bachelors or master:Any Photo ID and Visa Copy for H1B/EAD:
Experience Skill Matrix Data Engineer: YearsInsurance Industry: YearsMust - Property and casualty experience in recent 2-3 projects: YearsData warehousing, modelling, end-to-end BI solutions: YearsSpark, and PySpark: YearsBig data and cloud technologies (e.g., Azure): YearsSQL and query optimization: YearsPower BI data sources and reports: YearsAgile/SCRUM SDLC environment: Years
Job Description 12+ years in IT with at least 7-8+ years' experience in data warehousing, modelling, end-to-end BI solutionsStrong SQL, Spark, and PySpark programming skills for data analysis.Experience developing solutions for the Insurance industry.Strong understanding of Data Engineering Solutions, Data modelling, and Software Engineering principles and best practicesExperience in developing data platforms/ Big data and cloud technologies (e.g., Azure) Advanced knowledge of SQL and query optimization techniques and approachesExperience designing, developing, and supporting Power BI data sources and reports.Able to work as a team member and willing to work independently when required.Strong troubleshooting and problem-solving skillsExperience working in an Agile/SCRUM SDLC environment.Problem-solving aptitude, with a willingness to work in a fast-paced product development environment and hands-on mentality to do whatever it takes to deliver a successful product."
Data Engineer III,Definitive Logic a ManTech Company,"Arlington, VA (Remote)",https://www.linkedin.com/jobs/view/3739487875/?eBP=JOB_SEARCH_ORGANIC&refId=oxxoQjn826Ao1gqpbv6yGg%3D%3D&trackingId=80TQUUd28aappdcAA2cYJQ%3D%3D&trk=flagship3_search_srp_jobs,3739487875,"About the job
            
 
Definitive Logic is currently seeking an experienced Data Engineer. We are looking for an individual who appreciates the opportunity to be creative and challenged, has a curious mind and is passionate about solving problems quickly and bringing innovative ideas to the table.Roles and Responsibilities:  Creates database models and components that meet product specification and development schedules for customer/customers Participates in large system and subsystem planning Designs data pipelines, data models, profiling/cleansing the data, and performance tuning Develops and maintains data pipelines, data workflows, ETL/ELT scripts or packages Prepares comprehensive test plans Identifies data quality issues and potential remediations for consideration by PM and/or customer Implements quality assurance program for project deliverables Creates quality deliverables for customers Collaborates and provides influence to team on project deliverables Implements life-cycle of services/solution delivery for projects Acts as a technical resource for lower-level engineers May be assigned lead engineer role for small-to-medium sized projects Integration with source systems of record, establishing connections to transfer data from these systems to an interim environment and finally to the application database Map source data to target data and document these in a Source-to-Target Mapping document Establish automated data transformations Establish automated data quality checks Perform manual data quality checks, as needed Establish daily (or other periodic) data loads from source to target Data migration from legacy system to new system
Required Qualifications Active Interim Secret clearance or higher, or ability to obtain oneBachelor's DegreeMinimum of five (5+) years of relevant data solution experienceCompTIA Security+ Certification (current or obtained within 30 days of employment) Strong SQL experienceExperience working with Azure Data Factory and Azure Data Lake Experience with ETL (Extract, Transform, and Load) technologies and approachesExperience working with Microsoft Power Platform Dataverse and Power AutomateExperience developing data pipelines using modern Big Data ETL technologiesExperience with a modern programming language such as Python or .NETBackground developing solutions for high volume, low latency applicationsAbility to develop dashboards, analytics and reports using Microsoft Power BIAbility to quickly learn technical concepts and communicate with multiple functional groupsExperience working with software in a virtualized/cloud environmentA positive, can-do attitudeStrong verbal and written communication skillsIndependent, creative, and determinedAbility to operate in a fast paced, highly collaborative environment
Preferred Qualifications Experience working with the U.S. Army or other Military ServiceExperience working with big data
About Definitive LogicDefinitive Logic (DL) is a management and technology consulting firm known for delivering outcomes and ROI for agencies’ most complex business challenges.  DL delivers performance-based and outcome-driven technology consulting solutions that directly support the strategic intent of our Defense, Homeland Security, Emergency Management, Federal Civilian and Commercial clients. We’re the preferred technology integration partner for Federal agencies to apply the best of data science, app dev, DevSecOps, cyber and cloud solutions to improve decision support, empower front-line employees and enhance back-office operations. We serve as trusted advisors providing objective, fact-based, vendor & technology-neutral consulting services.Definitive Logic is ultimately a team of problem solvers — thought leaders, domain experts, coders, data enthusiasts, and technophiles.  Our exciting projects and learning and sharing culture have consistently resulted in validation as a Great Place to Work: 2023 Washington Post Top Workplaces (8-time winner) | 2023 Virginia Best Places to Work (10 years running, #1 midsize in 2019).Definitive Logic is committed to hiring and retaining a diverse workforce. We are proud to be an Equal Opportunity/Affirmative Action Employer, making decisions without regard to race, color, religion, creed, sex, sexual orientation, gender identity, marital status, national origin, age, veteran status, disability, or any other protected class.If you are a qualified individual with a disability or a disabled veteran, you have the right to request a reasonable accommodation if you are unable, or limited in your ability, to use or access our Careers page: https://www.definitivelogic.com/careers/open-opportunities/ as a result of your disability. You can request a reasonable accommodation by sending an e-mail to Recruiting@DefinitiveLogic.com or via phone: 703-955-4186. In order to quickly respond to your request, please use the words ""Accommodation Request"" as your e-mail subject line.DL BenefitsHealthDentalVisionLife/AD&D: Company paidSTD/LTD:Company paidSupplemental Plans: TriCare Supplement, Pet Insurance through Nationwide, Legal Resources and hospital/accidental indemnity plans and Wellness initiatives.Compensation Benefits:Competitive Base SalaryAnnual performance based bonus401(k) & Roth option: You are fully (100%) vested on day 1 and DL matches up to 5%Spot BonusesReferral BonusesAdditional Benefits:Flexible Time Off (FTO): Under our FTO plan, there is no cap in the amount of leave you choose to take, with proper coordination and prior approval.Volunteer Hours: DL allocates up to 8 hours for you to use every year to volunteer for a 501c3 organization of your choice and DL will donate to that charity based on how many hours you volunteer.Cell Phone Reimbursement: $80/monthLocation Specific Metro/ParkingTuition ReimbursementTraining & Certifications"
Data Engineer,Hyperhire,United States (Remote),https://www.linkedin.com/jobs/view/3759032713/?eBP=JOB_SEARCH_ORGANIC&refId=oxxoQjn826Ao1gqpbv6yGg%3D%3D&trackingId=w9rldJtbOMCcYUtt0mH53Q%3D%3D&trk=flagship3_search_srp_jobs,3759032713,"About the job
            
 
Hyperhire is a global recruitment firm specialised in contractual/temporary hiring.The RoleThis is a hands-on technology and leadership position for a Center Of Excellence in Analytics. You must possess these skills: Exceptional client communication skills, ability to understand client’s business challenge and map it to a technology architecture / approach, work with Sales team on customer presentations, white boarding solutioning and contribute to sales proposals, hire and manage technical analytics team. The ability to multi task across these items is vital.You Will Be Responsible For Developing scripts to process structured and unstructured data.Recommending, developing and implementing ways to improve data reliability, efficiency and quality.Supporting translation of data business needs into technical system requirements.Working with stakeholders to understand needs in order with respect to data structure, availability, scalability and accessibility.
Ideal Profile Having 10+ Years of experience in an application development team with hands-on architecting, designing, developing and deployment skillset. Without this skill set, you will not be able to guide, and lead the team of 3-4 consultants that will report to you.Bachelor's degree and 6 or more years of experience in Information Technology.Strong team ethics and team player.Cloud certification, Databricks, or Snowflake Certification is a plus.Experience in evaluating software estimating cost and delivery timelines and managing financials. Experience leading agile delivery & adhering to SDLC processes is required.Work closely with the business & IT stakeholders to manage delivery.Demonstrated ability to translate business requirements in a technical design and through to implementation. This is essential for us to convince clients of our value add, win opportunities and grow our business.Fully ‘plugged in’ (knowledgeable) about latest development in data systems, eg. Data fabric, data meshes, data products and how they could apply to a client challenge.Experienced Subject Matter Expert in designing & architecting BigData platforms services, and systems using Java/python, SQL, Databricks, Snowflake, and cloud-native tools on Azure and AWS. Ability to work in an on-shore/off-shore model working with development teams across continents.Use coding standards, secured application development, documentation, Release and configuration management and expertise in CI/CD.Well-versed in SDLC using Agile Scrum.Strong leadership skills, analytical problem-solving skills along with the ability to learn and adapt quickly.Self-motivated, quick learner and creative problem solver, organized, and responsible for managing a team of technical consultants.Ability to lead delivery, manage team members if required and provide feedback.Ability to make effective decisions and manage change.Communicates effectively in a professional manner both written and orally.Team player with a positive attitude, enthusiasm, initiative and self-motivation.
What's on Offer? Work alongside & learn from best in class talentExcellent career development opportunitiesLeadership Role"
Data Engineer,Wise Skulls,United States (Remote),https://www.linkedin.com/jobs/view/3726867255/?eBP=JOB_SEARCH_ORGANIC&refId=oxxoQjn826Ao1gqpbv6yGg%3D%3D&trackingId=CCs0Zp82oNc0gMwqeyBn8g%3D%3D&trk=flagship3_search_srp_jobs,3726867255,"About the job
            
 
Title: Data EngineerLocation: RemoteDuration: 6+ MonthsImplementation Partner: InfosysEnd Client: To be disclosedJd 5+ Years’ experience in DW/BI or Bigdata Project4 + years’ experience in ETL/Data ingestionProficient in creating in data pipeline using pythonStrong knowledge and working experience in Hadoop Tools and technologiesStrong knowledge and working experience in SPARKProficient in SQL, PL/SQLWorking experience with MPP Database Teradata/Greenplum/Netezza/Vertica)Working experience with cloud database (Redshift/Big Query/Snowflake)Good working experience with AirflowBig query experience is added advantage"
Lead Data Engineer - Remote,Enexus Global Inc.,United States (Remote),https://www.linkedin.com/jobs/view/3695496591/?eBP=JOB_SEARCH_ORGANIC&refId=oxxoQjn826Ao1gqpbv6yGg%3D%3D&trackingId=wRKOSbSMuoouU7icgMuoDQ%3D%3D&trk=flagship3_search_srp_jobs,3695496591,"About the job
            
 
Location - RemoteContract Type - W2/C2C/1099Minimum Experience - 12+ YearsResponsibilities Develop and enhance data-processing, orchestration, monitoring, and more by leveraging popular open-source software, AWS, and GitLab automation.Collaborate with product and technology teams to design and validate the capabilities of the data platformIdentify, design, and implement process improvements: automating manual processes, optimizing for usability, re-designing for greater scalabilityProvide technical support and usage guidance to the users of our platform’s services.Drive the creation and refinement of metrics, monitoring, and alerting mechanisms to give us the visibility we need into our production services.
Qualifications Experience building and optimizing data pipelines in a distributed environmentExperience supporting and working with cross-functional teamsProficiency working in Linux environment8+ years of advanced working knowledge of SQL, Python, and PySpark5+ years of experience with using a broad range of AWS technologiesExperience using tools such as: Git/Bitbucket, Jenkins/CodeBuild, CodePipelineExperience with platform monitoring and alerts tools"
Workplace Analytics Engineer,Canonical,"Las Vegas, NV (Remote)",https://www.linkedin.com/jobs/view/3738245724/?eBP=JOB_SEARCH_ORGANIC&refId=oxxoQjn826Ao1gqpbv6yGg%3D%3D&trackingId=9ct%2FgrwSdTqXPBbX2XRxJA%3D%3D&trk=flagship3_search_srp_jobs,3738245724,"About the job
            
 
Bring your people analytics, social science research and data mining/science skills to a unique team seeking to understand, and shape, the future of the digital workplace. We are interested in technology, of course, but we are also interested in the human mission of enabling the world's brightest and hardest working people to live where they want and work from anywhere. Most of our colleagues could move to a tech hub but they choose Canonical because of our mission and our approach to the workplace.We'd like to understand what really makes a distributed, remote-first workplace work. We think we're pretty good at this (being remote first for almost 20 years), but we know there is a lot still to understand, and the frontier of possibility continues to move outward. We'd like to invest in research, analytics and tooling which raises the bar even further for remote collaboration and organisation.If we are able to build tools that meaningfully improve our cooperation and our satisfaction, then we intend to share our insights with the world, both as a narrative and as SAAS or open source that helps other companies follow in our footsteps.The role of a Workplace Analytics Engineer at CanonicalSupport analytics and data mining in a cross-disciplinary team of organisational psychologists, web front end engineers, back end engineers and statistics / analytics experts to help us build a new definition for the 21st century digital workplace. Collaborate to figure out what really drives productivity, effectiveness and happiness in a remote-first globally distributed company.In addition to your existing people analytics work experience, this role will combine your skills in psychology, data analytics and visualisation, to help create a more effective workplace.Location: This role will be based remotely in the AMER region.All applicants applying must be legally authorized to work in the United States, as we cannot offer visa sponsorship for this job position.What your day will look like Utilize advanced data analytics to understand how we hire and how we work (productivity, happiness and effectiveness) across a global, remote first organisationFocus on quantitative and qualitative data analytics to find insights and meaningful business outcomesTell the story from the insights through dashboards, visualizations and presentationsDesign and conduct research into trends shaping talent science and remote workCollaborate with stakeholder teams (ex., engineering, information systems, etc) to improve the data and tool ecosystem supporting our digital workplace
What we are looking for in you Background in data science, mathematics, actuarial science, or engineeringFirst work experience in People Analytics Knowledge in advanced statistics, data sciences, coding/scripting languages (Python, R, etc), and databases (SQL, etc)Strength in data analytics and visualization (Looker Studio, Tableau, etc)Ability to translate business questions to key research objectivesAbility to identify the best methodology to execute research, synthesize and analyse findingsExcellent writing and communication skillsWillingness to examine the status quo and resilient in the face of challenges
What we offer youYour base pay will depend on various factors including your geographical location, level of experience, knowledge and skills. In addition to the benefits above, certain roles are also eligible for additional benefits and rewards including annual bonuses and sales incentives based on revenue or utilization. Our compensation philosophy is to ensure equity right across our global workforce.In addition to a competitive base pay, we provide all team members with additional benefits, which reflect our values and ideals. Please note that additional benefits may apply depending on the work location and, for more information on these, you can ask in the later stages of the recruitment process.🏠 Fully remote working environment - we've been working remotely since 2004!📚 Personal learning and development budget of 2,000USD per annum💰 Annual compensation review🏆 Recognition rewards🏝 Annual holiday leave👶 Parental Leave🧑‍💼 Employee Assistance Programme🧳 Opportunity to travel to new locations to meet colleagues at 'sprints'✈️ Priority Pass for travel and travel upgrades for long haul company eventsAbout CanonicalCanonical is a pioneering tech firm that is at the forefront of the global move to open source. As the company that publishes Ubuntu, one of the most important open source projects and the platform for AI, IoT and the cloud, we are changing the world on a daily basis. We recruit on a global basis and set a very high standard for people joining the company. We expect excellence - in order to succeed, we need to be the best at what we do.Canonical has been a remote-first company since its inception in 2004. Work at Canonical is a step into the future, and will challenge you to think differently, work smarter, learn new skills, and raise your game. Canonical provides a unique window into the world of 21st-century digital business.Canonical is an equal opportunity employerWe are proud to foster a workplace free from discrimination. Diversity of experience, perspectives, and background create a better work environment and better products. Whatever your identity, we will give your application fair consideration."
100% remote work - Need Data Engineer with EMR and/or EHR,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3640714850/?eBP=JOB_SEARCH_ORGANIC&refId=oxxoQjn826Ao1gqpbv6yGg%3D%3D&trackingId=4wkcTR7wcTv%2BOwL6GjZA5A%3D%3D&trk=flagship3_search_srp_jobs,3640714850,"About the job
            
 
Job Title: Data Engineer with EMR and/or EHRLocation: 100% Remote {Someone open to work Eastern Standard Time (EST)}Duration: 12+ MonthsExperience Level: 10+ YearsJob Description10+ years in IT with at least 3+ years' experience in data warehousing, modelling, end-to-end BI solutionsStrong SQL, Spark, and PySpark programming skills for data analysis.Experience with hospital/provider solutions like (EMH, EMR codes, Billing, etc.):Strong understanding of Data Engineering Solutions, Data modelling, and Software Engineering principles and best practicesExperience in developing data platforms/ Big data and cloud technologies (e.g., Azure)Advanced knowledge of SQL and query optimization techniques and approachesExperience designing, developing, and supporting Power BI data sources and reports.Able to work as a team member and willing to work independently when required.Strong troubleshooting and problem-solving skillsExperience working in an Agile/SCRUM SDLC environment.Problem-solving aptitude, with a willingness to work in a fast-paced product development environment and hands-on mentality to do whatever it takes to deliver a successful product."
Senior/First Data Engineer,Sleeper,"San Francisco, CA (Remote)",https://www.linkedin.com/jobs/view/3737470211/?eBP=JOB_SEARCH_ORGANIC&refId=oxxoQjn826Ao1gqpbv6yGg%3D%3D&trackingId=KaH%2BfZlfGJIjPsMktJYN2w%3D%3D&trk=flagship3_search_srp_jobs,3737470211,"About the job
            
 
Position SummaryThe Senior/First Data Engineer will play a crucial role in building, maintaining, and enhancing ETL processes that drive our analytics and machine learning platforms. This individual will be responsible for developing actionable insights from complex data sets, and work closely with various business units to inform strategy and decision-making.You will be the first hire in this function.Location SF Bay Area, NYC, or Remote
Key Responsibilities ETL & Backend Development:Design and optimize ETL pipelines. Develop robust backend systems for large-scale data processing using Elixir and database solutions like Cassandra/ScyllaDB. Data Architecture:Design scalable and efficient data models for Cassandra and ScyllaDB. Ensure data integrity, quality, and security. Data Science Support:Collaborate with data scientists, providing them with clean and reliable datasets. Assist in implementing and scaling data science models. Innovation & Research:Stay abreast of latest technologies. Recommend technical improvements for data processing and storage. 
QualificationsRequired Bachelor’s or Master’s degree in Computer Science, Engineering, or a related technical field. 5+ years of experience in backend development, with a strong focus on data engineering.  Technical skills: Expertise in Python, Java, Scala, and Elixer for backend and ETL processes. Mastery of ETL tools/frameworks (e.g. Apache Kafka, Apache Airflow). Deep knowledge of SQL/NoSQL databases, including Cassandra and ScyllaDB, and data warehousing solutions (e.g., Redshift, BigQueary, Snowflake). Proficiency in cloud platforms (AWS, GCP, Azure) and distributed systems. Familiarity with data science concepts, tools, and libraries (e.g. Pandas, Scikit-learn). Soft Skills: Exceptional problem-solving skills. Strong communication for technical and non-technical discussions. 

Nice-to-have Experience with cloud platforms like AWS, GCP, or Azure. Exceptional communication skills, both verbal and written. Expertise in machine learning algorithms and frameworks (e.g., TensorFlow, PyTorch, scikit-learn). 
Benefits Competitive salary ($150,000-$225,000/year) and stock optionsComprehensive health, dental, and vision plans401(k) Flexible working hours and remote work optionsRegular team building events and activities"
REMOTE PYTHON/JAVA DATA ENGINEER,Skiltrek,"Dearborn, MI (Remote)",https://www.linkedin.com/jobs/view/3768030991/?eBP=JOB_SEARCH_ORGANIC&refId=oxxoQjn826Ao1gqpbv6yGg%3D%3D&trackingId=BN1NUOCWkz3KW%2Bfo8t47eQ%3D%3D&trk=flagship3_search_srp_jobs,3768030991,"About the job
            
 
Job DescriptionVehicle Data Control Center Data Engineer In this role, you will design, build, and scale data pipelines that transform billions of records of data to measurable insights in a brand-new data ecosystem on GCP. In this role, you will collaborate with multiple organizations (Model e, Product Development, GDI&A) to convert business goals into data pipeline solutions. This role will work in a dynamic team and embrace lean and agile practices, software best practices, automated testing, and CI/CD to build and support the Vehicle Data Communications Center.Minimum Requirements5+ years of experience as a Data engineer working in creating batch and real-time data systems. * Hands-on experience with big data technologies like Hive, Spark, Hadoop. * Recent accomplishments working with the tech stack: Cloud Dataflow, Dataproc, Big Query, SQL, Hive, Kafka, Cloud storage, Data Fusion, Sqoop, NoSQL * Self-starter, strong technical skills, quality oriented and passionate * Familiar with performance tuning SQL and ETL pipelines * Proven skills with either Java or Python programming language * Excellent written and verbal communication and interpersonal skillsSkills Preferred:Any Cloud CertificationExperience Required:5+ years of experience as a Data engineer working in creating batch and real-time data systems. * 5+ years Hands-on experience with big data technologies like Hive, Spark, Hadoop."
Senior Data Engineer,SimplePractice,"Santa Monica, CA (Remote)",https://www.linkedin.com/jobs/view/3762873576/?eBP=JOB_SEARCH_ORGANIC&refId=oxxoQjn826Ao1gqpbv6yGg%3D%3D&trackingId=dhpUGNyCaC7un%2Fug6ka9Dg%3D%3D&trk=flagship3_search_srp_jobs,3762873576,"About the job
            
 
About UsAt SimplePractice, our team is dedicated to improving the health and wellness industry by building a suite of innovative solutions for practitioners and their clients. Our product supports practitioners on their clinical journey to becoming licensed, helps them manage their business and practice once they're up and running, and enables new clients to discover and interact with practitioners. Taking a practitioner-first approach in everything we do makes it possible for health and wellness practitioners to devote more time to their clients while they use SimplePractice to start, grow, and maintain a successful private practice.The Role We are seeking a Senior Data Engineer who will play a key role in driving data-driven insights across various departments within our organization. This position is crucial in shaping our strategy and optimizing operations in areas such as Product Analytics, Finance, Marketing, Risk Management, and more.The Senior Data Engineer will support data ingestion and transformations across our analytics platform. This includes building data pipelines covering a wide range of sources such as product databases, event streams, and marketing platforms. They will partner with the analysts to create dimensional data models to provide standardized metrics across the organization. This role offers an exciting opportunity for an analytical thinker who is looking to join a mission oriented company.Responsibilities Self-starter with excellent problem-solving & critical-thinking skills to meet complex data challenges Take ownership and champion data topics, with the flexibility and interpersonal skills to interact across the business, and manage a project / team where required. Develop data analytics competences in the team and drive out the analytics capability Manage the complete data infrastructure from data orchestration through data visualization Develop data ingestion pipelines for new data sources using tools or custom Python scripts Design, implement, and document data architecture and data modeling solution Assist in team planning and assist in the wider team's Change Management to enable change projects & business requests 
Desired Skills & Experience 4 years of experience in data engineeringDeep understanding of SQL. At SimplePractice we use SQL as a primary analytics tool, this skill is foundational Experience with processing large scale data in different formats (CSV, JSON, Parquet) Experience with orchestration tools such as Airflow or Prefect Experience with common cloud services (S3, DMS, CloudWatch, Lambda) Familiarity and comfortability facilitating projects and coordinating with multiple stakeholders Strong problem solving and structuring skills, with ability to communicate actionable insights Strong communication skills. Through every medium, and everyone you come in contact with, you know how to communicate effectively, elegantly, and with respect, always Ability to manage conflict and resolve difficult situations Outstanding work ethic, ability to perform under pressure, meet deadlines, prioritize and deliver multiple tasks on time
Bonus Points  Advanced data analytics, data science, knowledge of at least 1 scripting language  ETL experience is highly preferred, including DBT  Snowflake database engine  Experience with Cloud-based infrastructure, particularly AWS
Base Compensation Range$125,000 - $160,000 annuallyBase salary is one component of total compensation. Employees may also be eligible for an annual bonus or commission. Some roles may also be eligible for overtime pay.The above represents the expected base compensation range for this job requisition. Ultimately, in determining your pay, we'll consider many factors including, but not limited to, skills, experience, qualifications, geographic location, and other job-related factors.BenefitsWe offer a competitive benefits program including: Medical, dental, vision, life & disability insurance401(k) plan with company match Flexible Time Off (FTO), wellbeing days, paid holidays, and summer FridaysMental health resourcesPaid parental leave & Backup CareTuition reimbursementEmployee Resource Groups (ERGs)
California Job Applicant Privacy NoticeThank you for your interest in opportunities at SimplePractice LLC (""SimplePractice"" or ""us"" or ""we"" or ""our""). Please note that when you submit your resume or application materials to us for employment purposes, you are subject to the SimplePractice California Job Applicant Privacy Notice.For more information about our privacy practices, please contact us at privacy@simplepractice.com."
Senior/First Data Engineer,Sleeper,"Seattle, WA (Remote)",https://www.linkedin.com/jobs/view/3737464940/?eBP=JOB_SEARCH_ORGANIC&refId=oxxoQjn826Ao1gqpbv6yGg%3D%3D&trackingId=xihasVZUBu6xj1F324ufkA%3D%3D&trk=flagship3_search_srp_jobs,3737464940,"About the job
            
 
Position SummaryThe Senior/First Data Engineer will play a crucial role in building, maintaining, and enhancing ETL processes that drive our analytics and machine learning platforms. This individual will be responsible for developing actionable insights from complex data sets, and work closely with various business units to inform strategy and decision-making.You will be the first hire in this function.Location SF Bay Area, NYC, or Remote
Key Responsibilities ETL & Backend Development:Design and optimize ETL pipelines. Develop robust backend systems for large-scale data processing using Elixir and database solutions like Cassandra/ScyllaDB. Data Architecture:Design scalable and efficient data models for Cassandra and ScyllaDB. Ensure data integrity, quality, and security. Data Science Support:Collaborate with data scientists, providing them with clean and reliable datasets. Assist in implementing and scaling data science models. Innovation & Research:Stay abreast of latest technologies. Recommend technical improvements for data processing and storage. 
QualificationsRequired Bachelor’s or Master’s degree in Computer Science, Engineering, or a related technical field. 5+ years of experience in backend development, with a strong focus on data engineering.  Technical skills: Expertise in Python, Java, Scala, and Elixer for backend and ETL processes. Mastery of ETL tools/frameworks (e.g. Apache Kafka, Apache Airflow). Deep knowledge of SQL/NoSQL databases, including Cassandra and ScyllaDB, and data warehousing solutions (e.g., Redshift, BigQueary, Snowflake). Proficiency in cloud platforms (AWS, GCP, Azure) and distributed systems. Familiarity with data science concepts, tools, and libraries (e.g. Pandas, Scikit-learn). Soft Skills: Exceptional problem-solving skills. Strong communication for technical and non-technical discussions. 

Nice-to-have Experience with cloud platforms like AWS, GCP, or Azure. Exceptional communication skills, both verbal and written. Expertise in machine learning algorithms and frameworks (e.g., TensorFlow, PyTorch, scikit-learn). 
Benefits Competitive salary ($150,000-$225,000/year) and stock optionsComprehensive health, dental, and vision plans401(k) Flexible working hours and remote work optionsRegular team building events and activities"
Workplace Analytics Engineer,Canonical,"Oklahoma City, OK (Remote)",https://www.linkedin.com/jobs/view/3738248300/?eBP=JOB_SEARCH_ORGANIC&refId=oxxoQjn826Ao1gqpbv6yGg%3D%3D&trackingId=qf0UuPeUM83dqANKOZHPcg%3D%3D&trk=flagship3_search_srp_jobs,3738248300,"About the job
            
 
Bring your people analytics, social science research and data mining/science skills to a unique team seeking to understand, and shape, the future of the digital workplace. We are interested in technology, of course, but we are also interested in the human mission of enabling the world's brightest and hardest working people to live where they want and work from anywhere. Most of our colleagues could move to a tech hub but they choose Canonical because of our mission and our approach to the workplace.We'd like to understand what really makes a distributed, remote-first workplace work. We think we're pretty good at this (being remote first for almost 20 years), but we know there is a lot still to understand, and the frontier of possibility continues to move outward. We'd like to invest in research, analytics and tooling which raises the bar even further for remote collaboration and organisation.If we are able to build tools that meaningfully improve our cooperation and our satisfaction, then we intend to share our insights with the world, both as a narrative and as SAAS or open source that helps other companies follow in our footsteps.The role of a Workplace Analytics Engineer at CanonicalSupport analytics and data mining in a cross-disciplinary team of organisational psychologists, web front end engineers, back end engineers and statistics / analytics experts to help us build a new definition for the 21st century digital workplace. Collaborate to figure out what really drives productivity, effectiveness and happiness in a remote-first globally distributed company.In addition to your existing people analytics work experience, this role will combine your skills in psychology, data analytics and visualisation, to help create a more effective workplace.Location: This role will be based remotely in the AMER region.All applicants applying must be legally authorized to work in the United States, as we cannot offer visa sponsorship for this job position.What your day will look like Utilize advanced data analytics to understand how we hire and how we work (productivity, happiness and effectiveness) across a global, remote first organisationFocus on quantitative and qualitative data analytics to find insights and meaningful business outcomesTell the story from the insights through dashboards, visualizations and presentationsDesign and conduct research into trends shaping talent science and remote workCollaborate with stakeholder teams (ex., engineering, information systems, etc) to improve the data and tool ecosystem supporting our digital workplace
What we are looking for in you Background in data science, mathematics, actuarial science, or engineeringFirst work experience in People Analytics Knowledge in advanced statistics, data sciences, coding/scripting languages (Python, R, etc), and databases (SQL, etc)Strength in data analytics and visualization (Looker Studio, Tableau, etc)Ability to translate business questions to key research objectivesAbility to identify the best methodology to execute research, synthesize and analyse findingsExcellent writing and communication skillsWillingness to examine the status quo and resilient in the face of challenges
What we offer youYour base pay will depend on various factors including your geographical location, level of experience, knowledge and skills. In addition to the benefits above, certain roles are also eligible for additional benefits and rewards including annual bonuses and sales incentives based on revenue or utilization. Our compensation philosophy is to ensure equity right across our global workforce.In addition to a competitive base pay, we provide all team members with additional benefits, which reflect our values and ideals. Please note that additional benefits may apply depending on the work location and, for more information on these, you can ask in the later stages of the recruitment process.🏠 Fully remote working environment - we've been working remotely since 2004!📚 Personal learning and development budget of 2,000USD per annum💰 Annual compensation review🏆 Recognition rewards🏝 Annual holiday leave👶 Parental Leave🧑‍💼 Employee Assistance Programme🧳 Opportunity to travel to new locations to meet colleagues at 'sprints'✈️ Priority Pass for travel and travel upgrades for long haul company eventsAbout CanonicalCanonical is a pioneering tech firm that is at the forefront of the global move to open source. As the company that publishes Ubuntu, one of the most important open source projects and the platform for AI, IoT and the cloud, we are changing the world on a daily basis. We recruit on a global basis and set a very high standard for people joining the company. We expect excellence - in order to succeed, we need to be the best at what we do.Canonical has been a remote-first company since its inception in 2004. Work at Canonical is a step into the future, and will challenge you to think differently, work smarter, learn new skills, and raise your game. Canonical provides a unique window into the world of 21st-century digital business.Canonical is an equal opportunity employerWe are proud to foster a workplace free from discrimination. Diversity of experience, perspectives, and background create a better work environment and better products. Whatever your identity, we will give your application fair consideration."
Urgent Role - Senior Data Engineer || Remote,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3696912005/?eBP=JOB_SEARCH_ORGANIC&refId=oxxoQjn826Ao1gqpbv6yGg%3D%3D&trackingId=SX%2BYxFRI6EK53tY%2Bs%2FRJYA%3D%3D&trk=flagship3_search_srp_jobs,3696912005,"About the job
            
 
Job DescriptionTitle: Senior Data EngineerLocation: RemoteDuration: 6+ MonthsPosition DetailsSenior Data EngineerRole: ContractDuration: Contract position approved for 6 months, with likely extension and/or conversion.Location: MN Preferred-Hybrid, would consider 100% Remote from following approved states; AR, AZ, DE, FL, GA, IA, IL, KS, KY, MD, ME, MI, MN, MO, ND, NE, OK, SD, TN, TX, VA, WI.DescriptionThe Senior Data Engineer supports ETL Development. They will be responsible for developing and maintaining ETL process and support multiple lines of Business and IT initiatives. This individual may work independently or collaborate with a smaller group of developers in similar efforts. This position is responsible for technical analysis, design, development, testing, and deployment of IT solutions as needed by business or IT. The senior developer works closely with other subject matter experts to help design and build solutions for a multitude of applications, including batch processing, OLTP, OLAP and data warehouse architecture.The Senior Data Engineer will design, build, implement, and maintain data storage structures and processing pipelines for the extraction, transformation, and loading (ETL) of data from a variety of data sources. Develop robust and scalable solutions that transform data into a useful format for analysis, enhance data flow, and enable efficient, cost-effective consumption and analysis of data. Write complex SQL queries to support analytics needs. Evaluate and recommend tools and technologies for data infrastructure and processing.Minimum Knowledge and Skill  Complete working knowledge and skill with ability to be fully proficient in professional field.Bachelor's degree or equivalent experience in related field, plus 5 years of related work experience beyond degree.
Requirements For All 3 Positions Informatica ETL on premise and on cloud.Master Data Management experience. Informatica MDM experience a plus.Informatica IDQ/CDQ experience.Snowflake, Python, Spark experience"
Senior Data Engineer (Remote),"Roberts Recruiting, LLC","Boston, MA (Remote)",https://www.linkedin.com/jobs/view/3577130120/?eBP=JOB_SEARCH_ORGANIC&refId=oxxoQjn826Ao1gqpbv6yGg%3D%3D&trackingId=%2BH1HkpEvzWpUGaXuD7lIJg%3D%3D&trk=flagship3_search_srp_jobs,3577130120,"About the job
            
 
Sr. Data EngineerEngineering  Boston, Massachusetts * This is a remote or work-from-home role! *
About UsWe’re on a mission to improve health equity and drive improved health and financial outcomes across the country by providing direct SDOH solutions to partner with individuals navigating social health challenges. Our locally-hired, representative workforce is trained to engage and empower individuals using best practices while leveraging secure, accessible technology. Our goal is simple: improve poor outcomes and high costs due to unaligned care driven by addressable social challenges. We are confident of the potential of providing social services care navigation to positively impact the health of communities with an effective, efficient, and measurable model which is locally responsive and nationally scalable.About The TeamOur data team is building out the country’s richest data on social determinants of health and the impact of social needs on healthcare outcomes and spending. We will combine the results of the work of our care navigators across the country with potentially hundreds of data sources, from public government data sets to private health care claims records. We need your help in building a data platform that can scale along with fast-growing data to empower many different teams across our organization.Responsibilities Help us architect our new data pipeline for scale so that we can make sure to deliver the right social services to the right peoplePut in place the right tools for every department (from Data Science to Clinical Operations) to be able to get insights from our unique social determinants of health datasetPursue high-standard engineering principles to build a scalable, reliable, and maintainable data platformSupporting and working with cross-functional teams in a dynamic environment
Requirements  Experience with: architecting, implementing, and maintaining cloud-based modern data pipelinesAmazon Redshift, Databricks, Snowflake, or other similar technologiesdbt, Apache Airflow, Great Expectations, or other similar technologies
Must enjoy working in a collaborative working style. 
Nice To Haves  Experience with: managing insurance claims datamanaging Personally Identifiable Information (PII) or Protected Health Information (PHI)building and deploying Machine Learning models
Familiarity with Observational Medical Outcomes Partnership (OMOP) Common Data Model (CDM)"
Data Engineer,iTech Solutions,United States (Remote),https://www.linkedin.com/jobs/view/3714396845/?eBP=JOB_SEARCH_ORGANIC&refId=oxxoQjn826Ao1gqpbv6yGg%3D%3D&trackingId=t%2FCbkmZ1dbdb2WcDy1oKJA%3D%3D&trk=flagship3_search_srp_jobs,3714396845,"About the job
            
 
 Role: Data EngineerLocation: Remote (preferred west coast as the candidate shall be working in PST)Bill Rate: 65/hr. Description: Design, develop, test, deploy, support, enhance data integration solutions seamlessly to connect and integrate the enterprise systems in our Enterprise Data Platform.Innovate for data integration in Apache Spark-based Platform to ensure the technology solutions leverage cutting edge integration capabilities.Experience with ETL, data pipeline creation to load data from multiple data sources.
  Primary Skills: 4+ years working experience in data integration and pipeline development.BS degree in CS, CE or EE.2+ years of Experience with AWS Cloud on data integration with Apache Spark, EMR, Glue, Kafka, Kinesis, and Lambda in S3, Redshift, RDS, MongoDB/DynamoDB ecosystemsStrong real-life experience in python development especially in pySpark in AWS Cloud environment.Design, develop test, deploy, maintain and improve data integration pipeline.Experience in Python and common python libraries. Strong analytical experience with database in writing complex queries, query optimization, debugging, user defined functions, views, indexes etc.Strong experience with source control systems such as Git, Bitbucket, and Jenkins build and continuous integration tools.Databricks, Redshift Experience is a plus.
 Note: Please look for 4-5 years of work experience only as the budget here is limited."
W2 only - Remote work - Need Azure Cloud Data Engineer with Cosmos DB experience,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3705207089/?eBP=JOB_SEARCH_ORGANIC&refId=oxxoQjn826Ao1gqpbv6yGg%3D%3D&trackingId=l1WUlWtunzp56ayNf6xVrQ%3D%3D&trk=flagship3_search_srp_jobs,3705207089,"About the job
            
 
Azure Cloud Data Engineer w/ Cosmos DB - w2 candidates onlyLocation: St. Louis (client location) working hours Central Standard TimeREMOTE: Candidate may work FULL TIME REMOTEWork Status: USC / Green CardPerfect English/Communication skillsSkills Azure Cloud Data Engineer Cosmos DB, T-SQLMicrosoft Azure Cloud PaaSGit/Jenkins/BitbucketAzure Cloud workflowsAzure cloud managed DBs/Systems, Managed SQL InstanceBig Data solutions: Delta Lake by Databricks
As a Senior Engineer - DBA, Cloud Data Engineer you will be responsible for using your technical knowledge to solve business problems. We are looking for a talented individual that can serve as a subject matter expert in their area of focus and represent their department on complex assignments. You will be responsible for evaluating elements of technologies effectiveness through requirements gathering, testing, research and investigation and make recommendations for improvements that result in increased quality and effectiveness. You will be required to listen to and evaluate customer needs to determine and provide high quality solutions that align with customer expectations.In this role on the Cloud Data Engineering team, you will be a part of a team responsible for migrating existing data platform solutions to the cloud. You will also collaborate with others to develop new data pipelines. As a data engineer in the Cloud Data Engineering team, you must work in a data first mindset. You will use Microsoft Azure Cloud PaaS technologies to engineer our data solutions in a way that allows for us to optimize our information, make better decisions, and meet our customers needs. If you have a passion for engineering, working with massive amounts of data and empowering smart business decisions, this is the role for you.QualificationsRequired: Must be presently authorized to work in the U.S. without a requirement for work authorization sponsorship by our company for this position now or in the future. Must be committed to incorporating security into all decisions and daily job responsibilities3+ of related experience in Cosmos DB/Similar DB technologyExperience with configuration management and building automation capabilities such as Git/Jenkins/BitbucketExperience with Microsoft Azure Cloud workflowsExperience in T-SQL and scripting skills.Knowledge on Microsoft cloud managed DBs/Systems, e.g. Managed SQL Instance, Cosmos DB, Databricks Delta LakeExperience with Big Data solutions such as Delta Lake by Databricks and SQL DBMSsIndependently analyze, solve, and correct issues in real time, providing problem resolution end-to-endIdentify new opportunities and help refine automation of regular processes, track issues, and document changesSolve/Assist in complex query tuning and schema refinementExpert in troubleshooting performance issuesExperience rightsizing Database object workflow for cost managementAbility to multi-task and context-switch effectively between different activities and teamsJoin the on-call rotation with other EngineersMust be able to both collaborate in a team-oriented environment and work independently with directionMust be able to work in a fast-paced environment with the ability to handle multiple tasks
Preferred Bachelor's degree in Computer Science, Computer Information Systems, Management Information Systems, or related field preferredAzure SQL DB etc. will be a big plusExperience with Microsoft Azure platform technologies like Databricks applications, Event Hub, Data Factory, Azure SQL, Synapse Analytics, DeltaLake, Cosmos DB, or DevOpsPrior experience with large-scale projectsExperience with API developmentKnowledge and working experience with Agile methodologiesFamiliarity with JDBC connections to data sources"
Sr. Data Engineer,Insite AI,United States (Remote),https://www.linkedin.com/jobs/view/3775695636/?eBP=JOB_SEARCH_ORGANIC&refId=oxxoQjn826Ao1gqpbv6yGg%3D%3D&trackingId=Pee8foHau%2BQ69fKg3ZL6GQ%3D%3D&trk=flagship3_search_srp_jobs,3775695636,"About the job
            
 
As the Sr Data Engineer at Insite AI, you will be responsible for leading the architectural design, development, and deployment of scalable data management systems in a high-performance, customer-driven environment. Insite AI's AI-based platform enables autonomous decision-making that creates a connected assortment strategy, category and pricing execution and trade promotion environment. Insite AI's team comes from leading CPGs and brings their experiences to build a fully customizable AI-based platform that gives CPGs and Consumer Brands the most “explain-ability”, foresight, and granularity that results in an unprecedented quality of decision-making. If you are passionate about leveraging the power of AI to help clients reach their full potential, this is the opportunity for you!Responsibilities Build and maintain data integration, ETL pipelines, and data warehouse architectures on cloud platforms like AWS, Azure, and GCPWork with cross-functional teams comprising Data Scientists, Product Managers, and Software Engineers to design, build, and deploy robust and scalable data pipelines that power real-time decision-makingProvide technical mentorship and leadership to a team of data engineers, and drive innovation and best practices in the engineering organizationWork closely with the customer success team to troubleshoot and resolve issues related to data quality, ingestion, and integration
Requirements 8+ years of experience in designing, building, and deploying large-scale data management systems in the cloudStrong proficiency in programming languages such as Python & JavaExperience with cloud computing platforms such as AWS, Azure, and/or GCPExperience with data warehousing and ETL technologies such as Databricks, Snowflake, and AirflowStrong technical leadership skills and ability to mentor and guide team membersExcellent communication skills and ability to work collaboratively in a cross-functional team environment
Benefits Health Care Plan (Medical, Dental & Vision)Retirement Plan (401k, IRA)Life Insurance (Basic, Voluntary & AD&D)Paid Time Off (Vacation, Sick & Public Holidays)Remote - Work From HomeFamily Leave (Maternity, Paternity)Short Term & Long Term DisabilityTraining & Development"
AWS Data Engineer,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3734763302/?eBP=JOB_SEARCH_ORGANIC&refId=oxxoQjn826Ao1gqpbv6yGg%3D%3D&trackingId=yrRISSnk7jJKCYbibja%2BCw%3D%3D&trk=flagship3_search_srp_jobs,3734763302,"About the job
            
 
Share only one candidateLinkedIn is must100% Remote.Visa - GC/USC onlyAWS Expereince must be very strong.Position: - AWS Data EngineerLocation: Chanhassen, MN - 100% RemoteContract - 6 months contract to hire This is a contract with a potential contract to hire. Would like someone with green card or US Citizen. Prefer they are local or near MN but will consider non local as this is 100% remote. Must be strong communicator. This role is not fully approved yet but would like to see some resumes now as he is planning to hire for it but might take a few weeks on this so candidates should have some patience.This person should have 5-7+ years or more of data engineering on AWS platform leveraging some of the many AWS services that exist in the ecosystem.Should have AWS tooling exp- like Glue, Redshift, data analytics. Should have AWS native services exp.Any prior background around dev ops or ETL development before going into AWS is helpful but not at all required as that is not the focus of the job. We were just talking about what other experience could be useful.Certification in AWS Data Analytics would be a plus."
AzurCosmos DB - Cloud Data Engineer -Remote,"RIT Solutions, Inc.","St Louis, MO (Remote)",https://www.linkedin.com/jobs/view/3768010021/?eBP=JOB_SEARCH_ORGANIC&refId=oxxoQjn826Ao1gqpbv6yGg%3D%3D&trackingId=exbqGfzyheebyWtf9N2g9g%3D%3D&trk=flagship3_search_srp_jobs,3768010021,"About the job
            
 
This is W2 role..Must have a valid LinkedIn profileRequired Experience Below 4+ years Azure Cosmos DB (full managed NoSQL and Relational DB for App Dev)Experience with Microsoft Azure Cloud workflowsExperience with Managed SQL Instance, Cosmos DB, Databricks Delta LakeExperience with Big Data solutions (Delta Lake by Databricks and SQL DBMSs)Microsoft Azure Cloud workflowsConfiguration management with Git/Jenkins/BitbucketAzure SQL DB (Plus)Tasked with Migrating existing data platform solutions to the cloudDevelop new data pipelinesData first mindset. Use Microsoft Azure Cloud PaaS
As a Senior Engineer - DBA, Cloud Data Engineer you will be responsible for using your technical knowledge to solve business problems. We are looking for a talented individual that can serve as a subject matter expert in their area of focus and represent their department on complex assignments. You will be responsible for evaluating elements of technologies effectiveness through requirements gathering, testing, research and investigation and make recommendations for improvements that result in increased quality and effectiveness. You will be required to listen to and evaluate customer needs to determine and provide high quality solutions that align with customer expectations.In this role on the Cloud Data Engineering team, you will be a part of a team responsible for migrating existing data platform solutions to the cloud. You will also collaborate with others to develop new data pipelines. As a data engineer in the Cloud Data Engineering team, you must work in a data first mindset. You will use Microsoft Azure Cloud PaaS technologies to engineer our data solutions in a way that allows for us to optimize our information, make better decisions, and meet our customers needs. If you have a passion for engineering, working with massive amounts of data and empowering smart business decisions, this is the role for you.Equal Opportunity Employer/Disability/VeteransRequired Must be committed to incorporating security into all decisions and daily job responsibilities3+ of related experience in Cosmos DB/Similar DB technologyExperience with configuration management and building automation capabilities such as Git/Jenkins/BitbucketExperience with Microsoft Azure Cloud workflowsExperience in T-SQL and scripting skills.Knowledge on Microsoft cloud managed DBs/Systems, e.g. Managed SQL Instance, Cosmos DB, Databricks Delta LakeExperience with Big Data solutions such as Delta Lake by Databricks and SQL DBMSsIndependently analyze, solve, and correct issues in real time, providing problem resolution end-to-endIdentify new opportunities and help refine automation of regular processes, track issues, and document changesSolve/Assist in complex query tuning and schema refinementExpert in troubleshooting performance issuesExperience rightsizing Database object workflow for cost managementAbility to multi-task and context-switch effectively between different activities and teamsJoin the on-call rotation with other EngineersMust be able to both collaborate in a team-oriented environment and work independently with directionMust be able to work in a fast-paced environment with the ability to handle multiple tasks
Preferred Bachelor's degree in Computer Science, Computer Information Systems, Management Information Systems, or related field preferredAzure SQL DB etc. will be a big plusExperience with Microsoft Azure platform technologies like Databricks applications, Event Hub, Data Factory, Azure SQL, Synapse Analytics, Delta Lake, Cosmos DB, or DevOpsPrior experience with large-scale projectsExperience with API developmentKnowledge and working experience with Agile methodologiesFamiliarity with JDBC connections to data sources"
Senior Data Engineer & Analyst,Life House,"New York, NY (Remote)",https://www.linkedin.com/jobs/view/3673087295/?eBP=JOB_SEARCH_ORGANIC&refId=jprbWfaxGWningdTClzsyA%3D%3D&trackingId=LYrqM%2BOkGPMAeAXlaiBhbA%3D%3D&trk=flagship3_search_srp_jobs,3673087295,"About the job
            
 
Our CompanyLife House is a venture-backed, vertically integrated hotel brand, operator and technology company. We develop, design, and operate boutique lifestyle hotels with a mission to make travel more meaningful and reliable for travelers, and to make hotels more seamless and more profitable for owners. We recently raised our Series C and we are looking for talented software developers to join our in-house technology team. We are fast-paced, highly collaborative, and have a friendly, flexible work environment.RoleAt Life House, Sr. Data Engineers & Analysts will work in a product environment to build a game changer dynamic pricing AI- based solution. The Agile team is composed of Data Scientists, Product Managers, Software Engineers, QA's and Revenue Managers. Life House looks to build world-class solutions to the hospitality industry's greatest challenges. With new hotels opening every month, our team is acutely focused on implementing scalable solutions while consistently improving our value proposition to hotel owners - increasing hotel revenue and decreasing hotel operating cost.Responsibilities You will share your technical data engineering knowledge, skills and leadership to build a scalable and a top notch product. Design, build, and productionize complex data pipelinesDevelop data ingestion modules that will feed the AI models Develop baseline AI models with the guidance of Data Scientist(s)Design and Dev ETL pipeline (Reporting, Machine learning task, Data lake)Productionize AI models in the cloud and ensure its scalabilityRun or participate in running ML/OR models/experiments with the Data ScientistsDesign, code, test, and integrate new features and functionalityApply CI/CD practices to prevent integration problems as well as ensure that the code is releasable at any point in timeParticipate in scrum project meetings and update stories using project management toolsParticipate in the estimation of the Stories based on defined Acceptance Criteria and Definition of DoneRefactor and test codeDesign and implement software architecture that will allow scalability and maintainabilityPerform data profiling and analysis
Requirements Bachelor's degree in related field (Computer/Software Engineering, Computer Science, Artificial Intelligence, Mathematics)3+ years professional experience in software development or data engineering (post-degree)3+ years professional experience in designing and developing ETL pipelines
Benefits An environment that encourages initiative and leadershipWork with highly talented people who are extremely passionate about their craftWork in Agile, highly collaborative environmentCompetitive salary ($90,000 to $150,000 CAD), full benefits, and unlimited PTO$1200 Travel StipendDiscount on room reservations at Life House managed propertiesWellness BenefitsMuch more…"
Data Migration Engineer,Mark43,"Pittsburgh, PA (Remote)",https://www.linkedin.com/jobs/view/3645998542/?eBP=JOB_SEARCH_ORGANIC&refId=jprbWfaxGWningdTClzsyA%3D%3D&trackingId=GPFWLaknZGlB%2ByokUz7TLw%3D%3D&trk=flagship3_search_srp_jobs,3645998542,"About the job
            
 
Mark43 is approved to hire in 40 states, including AL, AZ, CA-excluding San Francisco, CO, CT, DC, DE, FL, GA, IA, ID, IL, IN, KS, LA, MA, MD, ME, MI, MN, MO, NE, NV, NH, NJ, NM, NY, NC, OH, OR, PA, SC, SD, TN, TX, UT, VA, VT, WA, and WI. Before applying to a remote role, please ensure that you are able to perform the position in one of the states listed above. State locations and specifics are subject to change as our hiring requirements shift.Mark43's mission is to empower communities and their governments with new technologies that improve the safety and quality of life for all. We build powerful, scalable, and elegant software that sets a new standard for the tools upon which our first responders rely. Our users are diverse, and we are therefore committed to embracing diversity of thought and experience within our team.We are looking for a Data Migration Engineer (ETL Developer) to join our team. You will be responsible for designing, developing, and executing data migrations that allow our customers to seamlessly transition critical operations to Mark43. As a Data Migration Engineer you will not only manage your own small portfolio of migration projects but will also play a critical role in helping our migration partners perform successful projects for our customers. You will partner with our migration contractors to provide them with project management, decision making and executable feedback. You will also partner with our customers to understand and find new value in their legacy data. You will collaborate with Mark43 migration, engineering, and deployments teams to continuously improve the data migration process to help us scale over time.This role requires a sharp attention to detail, strong organizational skills, an interest in working directly with public safety customers, and a passion for data. Members of our current team have backgrounds in data migration engineering, ETL development and SQL development at large SaaS companies.What you can expect to work on: Perform mid to large-scale data migrations from customers' legacy systems into a multi-cloud environmentManage our migration contractor partners through our end to end migration lifecycleAnalyze legacy databases to understand their unique data models and how that can be transformed to live in Mark43Write and execute complex SQL-based ETLs to Extract, Transform and Load legacy data into Mark43Become an expert on the Mark43 database and the implications of data decisions to our customersWork towards a worthwhile mission with a team of friendly and intelligent coworkers
What we expect from you: 2+ years of experience working in a data migration engineering, ETL development and/or SQL Development roleComprehensive understanding of the ETL processExperience writing complex SQL scriptsDeep knowledge & understanding of database design, setup, and maintenanceExperience managing contractors is a plus Interest in becoming an expert in public safety data as well as Mark43's productsStrong written and oral communication skillsOwnership over your own work and a commitment to every part of a task, from big picture to small detailsAn attitude that is humble, detail oriented, and committed to qualityExperience working with public safety data systems a plus
What you can expect from us: Constant collaboration with numerous Mark43 teams, including Global Services, Engineering, and ProductBuilding mission critical and socially responsible software to enable first responders to better serve their communitiesA team that respects and embraces your ideas and expertiseCoworkers that are motivated by pursuing excellence, rather than the prospect of personal gainA workplace dedicated to supporting and bettering public safety and government agencies
We feel passionately about equal pay for equal work, and transparency in compensation is one vehicle to achieve that. Total compensation for this role is market competitive, including a target base annual salary range of $80,000 - $100,000, plus bonus opportunity, company stock options, and a full benefits package, including health insurance, paid time off, and a 401k with a company match. Please note that the higher end of this range will be reserved for candidates with appropriate experience who reside in high cost of labor locations.Mark43 is committed to the full inclusion of all qualified individuals. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex disability, age, sexual orientation, gender identity, national origin, veteran status, or genetic information. As part of this commitment, we will ensure that persons with disabilities are provided reasonable accommodations. If reasonable accommodation is needed, please email recruiting@Mark43.com requesting the accommodation."
AWS Data Engineer,Sonitalent Corp,United States (Remote),https://www.linkedin.com/jobs/view/3728516848/?eBP=JOB_SEARCH_ORGANIC&refId=jprbWfaxGWningdTClzsyA%3D%3D&trackingId=p4LDJW2nLG1WTIauN98TvQ%3D%3D&trk=flagship3_search_srp_jobs,3728516848,"About the job
            
 
Role: AWS Data EngineerJob Decription AWS ServicesCompute: Glue, Lambda, AWS Batch,Storage: S3Database: Redshift, Aurora, RDSStrong SAS/R/Python/PySpark Scripting ExperienceJava experience nice to have.Familiarity Domino Data LabSagemaker"
"Staff Data Engineer, Analytics Platform",Afresh,"San Francisco, CA (Remote)",https://www.linkedin.com/jobs/view/3767563518/?eBP=JOB_SEARCH_ORGANIC&refId=jprbWfaxGWningdTClzsyA%3D%3D&trackingId=%2BBJ3CA97CzKYr1Au5tg1vw%3D%3D&trk=flagship3_search_srp_jobs,3767563518,"About the job
            
 
Afresh is on a mission to eliminate food waste and make fresh food accessible to all. Our first A.I.-powered solution optimizes ordering, forecasting, and store operations for fresh food departments in brick-and-mortar grocers. With our Fresh Operating System, regional and national grocery retailers have placed $1.6 billion in produce orders across the US and we've helped our partners prevent 34 million pounds of food from going to waste. Working at Afresh represents a one-of-a-kind opportunity to have massive social impact at scale by leveraging uncommonly impactful software – we hope you'll join us!About the Role:The goal of our analytics architecture is to allow teams across Afresh to read and write meaningful, consistent, and reliable metrics derived from our disparate data sources, and to use those metrics to track internal performance, power new reporting products, drive decision-making through experimentation, and alert on significant changes in performance.In the next 6 months, we are releasing new products that build on top of our analytics architecture, namely a customer facing product for reporting insights that will help our customers drive down food waste. As we venture into building more customer facing products that leverage analytics insight, we’re looking to strengthen our analytics platform foundation.The Data Science team sits under the larger Prediction, Optimization, and Planning (POP) team at Afresh. You will regularly interact with data engineers, applied scientists, data scientists, full stack engineers, and product managers in the course of your work.As a staff data engineer on the Data Science team, you will own the ongoing development of our analytics platform. In this role, you will evolve our data warehouse schema, solidify our transform architecture, and establish data governance patterns to serve our internal and external analytics needs. Some of your responsibilities will include: Improve and extend our data analytics architecture to enable consistent analytic results and easy access across multiple analytics use casesCollaborate with engineers, product managers, and data scientists to understand their data needs, and then build extensible dimensiional models and semantic layer metrics that allow for consistent and reliable insightsEvolve our existing data quality and data governance processesMentor and up-skill other engineers
This is a high-impact role with ownership of highly visible projects and a lot of room to grow in your scope.Skills and Experience: 6+ years of experience as an data engineer, analytics engineer, data warehouse engineer, or a similar role.Strong understanding of advanced concepts in SQL.Exceptional communication and leadership skills, with a proven ability to facilitating cross-team and cross-functional collaboration and information sharing.1+ years of experience working with SQL-driven transform libraries that support an ELT paradigm, like dbt or sqlmesh, at scale, including setting up CI/CD pipelines that ensure high quality transformations.Expert knowledge about the differences between OLTP and OLAP database design.Familiarity with the differences between data engineering concepts like Data Mesh, Data Lake, Data Warehouse, Data Fabric, and Data Lakehouse.Experience with setting up a semantic layer defined with code (LookML, Cube.dev, AtScale, dbt semantic layer).Technologies: SQL, Python, Airflow, dbt, Snowflake/Databricks/BigQuery, Spark.
Salary Band:About AfreshFounded in 2017, Afresh is working on the #1 solution to curb climate change: reducing food waste. By combining human insight and transformative technology, we're helping grocers provide fresher food to customers at more affordable prices.Afresh sits at an incredible intersection of positive social impact, rocket ship financial growth, and cutting-edge technology. Our best-in-class AI research has been published in top journals including ICML, and we've raised over $148 million in funding from investors including former co-CEO of Whole Foods Market Walter Robb and Eric Schmidt's Innovation Endeavors.Fresh is the past, present, and future of our food system – the waste we create today will impact our planet for years to come. Join us as we continue to build a vibrant, diverse, and inclusive team that embodies our company’s values of proactivity, kindness, candor, and humility.Afresh provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability, genetics, sexual orientation, gender identity/expression, marital status, pregnancy or related condition, or any other basis protected by law.Here at Afresh, many of our employees work remotely provided that they reside in one of the following states: AR, CA, CO, FL, GA, IL, KY, MA, MI, MT, MO, NV, NJ, NY, NC, OR, PA, TX, WA, WI. However, there may be key roles that will require a candidate/employee to be local to our San Francisco, CA office. In which case this requirement will be included in the job posting details under ""Skills and experience"" for reference."
Job Opening for Sr. Data Engineer (pl/sql developer) - Remote,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3715675408/?eBP=JOB_SEARCH_ORGANIC&refId=jprbWfaxGWningdTClzsyA%3D%3D&trackingId=ZKJuGkZg%2Ftl0Tl1jfo5WEg%3D%3D&trk=flagship3_search_srp_jobs,3715675408,"About the job
            
 
Hi,Please find attached Job Description. If you are interested please do share with me your updated resume or call me on ""3025492448"".Title:- Sr. Data Engineer (pl/sql developer)Location:- RemoteDuration:- 12+ YearsVisa:- Citizen, GCInterview Mode:- Video DescriptionData Engineer Consultant for Houston Client, and they can be under a VISA , Green Card or USC.""Typically working on big data and cloud technologies is not going to be helpful to us  such as; "" Hadoop ecosystem, Bigdata, PySpark, Spark, AWS, GCP, Big Query, Data Warehousing""Please note the following preferred skills as they truly need these to get the Manager's attention:Expert In Pl/sql Development Work Experience With Oracle 19Must have working experience with tools like Airflow, GitMust have python coding experience and REST API development experiencePlus Would Be Experience With Accounting And Financial DatasClient would also like to see 2 code samples of their best code to gauge their coding style, and they can just show it to Client during the interview, They don't need copies of the code. Interview will be Teams based.They have commented , they don't want to see resumes just with cloud technology experience (like with Azure, AWS, GCS etc), they are not using those at site.Candidates need to have the drive to learn new technologies, and can pick up new technologies rapidly, able to self-manage a growing list of tasks and priorities.Remote or Hybrid: We prefer someone local to Houston, who is willing to come to the office at least 3 days a week. But remote option is ok, if you are able to find great candidates.Gaurav VermaTalent Acquisition -North AmericaDirect:+1 3025492448gaurav.verma@steneral.comIn my absence please reach out to Mr. Harish Sharma at harish@steneral.com & 302-721-6151"
Onsite Opportunity for Data Engineer with D365 -  Only USC and GC candidates- Chicago IL,ISmile Technologies,United States (Remote),https://www.linkedin.com/jobs/view/3645971561/?eBP=JOB_SEARCH_ORGANIC&refId=jprbWfaxGWningdTClzsyA%3D%3D&trackingId=6GxOQR5HoSkkwb2VPXtdog%3D%3D&trk=flagship3_search_srp_jobs,3645971561,"About the job
            
 
Job DescriptionWe are seeking a talented and experienced Data Engineer with expertise in Dynamics 365 and .NET to join our dynamic team. As a Data Engineer, you will be responsible for designing, developing, and maintaining robust data solutions using the Microsoft Dynamics 365 platform and .NET technologies. You will collaborate with cross-functional teams to gather requirements, design data models, and implement scalable and efficient data pipelines. This is a critical role that contributes to the success of our data-driven initiatives.Responsibilities Design, develop, and maintain data solutions using the Microsoft Dynamics 365 platform and .NET technologies.Collaborate with stakeholders, including business analysts, data scientists, and software developers, to gather requirements and translate them into technical specifications.Design and implement efficient and scalable data models, databases, and data integration workflows.Develop customizations, plugins, and extensions for the Dynamics 365 platform to meet specific data processing requirements.Implement data extraction, transformation, and loading (ETL) processes to ensure data accuracy, integrity, and timeliness.Optimize data storage and retrieval processes for performance and scalability.Monitor data quality, troubleshoot data-related issues, and implement data governance and security measures.Collaborate with IT infrastructure teams to ensure data infrastructure meets the needs of the data engineering initiatives.Stay up-to-date with emerging technologies, industry trends, and best practices related to data engineering and Dynamics 365.Document technical specifications, data flows, and system architecture for reference and future maintenance.
Requirements Bachelor's degree in Computer Science, Software Engineering, or a related field. Relevant certifications are a plus.Proven experience as a Data Engineer, Database Developer, or a similar role, with a focus on Microsoft Dynamics 365 and .NET technologies.Strong knowledge of Microsoft Dynamics 365 platform, including customization, configuration, and integration capabilities.Proficiency in .NET programming languages (C#, ASP.NET, etc.) and frameworks.Experience with data modeling, database design, and SQL query optimization.Hands-on experience with ETL tools, data integration techniques, and data warehousing concepts.Familiarity with data governance, data security, and data quality best practices.Strong analytical and problem-solving skills, with the ability to translate business requirements into technical solutions.Excellent communication and collaboration skills, with the ability to work effectively in cross-functional teams.Ability to adapt to a fast-paced and changing environment, with a strong commitment to meeting project deadlines.
Preferred Skills Experience with other Microsoft technologies like Azure, Power BI, and SharePoint.Familiarity with cloud-based data solutions and technologies (Azure Data Factory, Azure SQL Database, etc.).Knowledge of data science concepts and machine learning frameworks.Experience with Agile/Scrum methodologies."
AWS Data Engineer,IVY TECH SOLUTIONS INC,"Washington, DC (Remote)",https://www.linkedin.com/jobs/view/3667175557/?eBP=JOB_SEARCH_ORGANIC&refId=jprbWfaxGWningdTClzsyA%3D%3D&trackingId=2baWoG3ZxVQgLOOOnbKcQQ%3D%3D&trk=flagship3_search_srp_jobs,3667175557,"About the job
            
 
CLIENT : AMTRAKLocation : DCAWS, AWS GLUE, ETL, Informatica, KINESIS, LAMBDAThis position is for a Mid level AWS/Data Engineer, responsible for designing and implementing data processing pipelines using AWS Glue and other AWS services. The qualified individual will work closely with clients/onsite counterparts as needed.Must-haves: Familiarity with AWS data services and modules.5+ years of hands-on experience with AWS services (Lambda, S3, RDS, Aurora, DynamoDB, Kinesis, AWS Glue, AWS Data Pipeline)3+ years of experience with data migration, data analysis, and SQLs3+ years of experience with informaticaExperience with Structured Query Language (SQL), should be able to analyze, compare and profiling data setsAbility to work in globally distributed teamsKnowledge of IT processes, including quality assurance, release management, and production supportExcellent analytical, troubleshooting, and problem-solving skillsExcellent communicator (written and verbal, formal, and informal).Flexible and proactive/self-motivated working style with strong personal ownership.Ability to multi-task and prioritize under pressure.Ability to work independently with minimal supervision as well as in a team environment.Undergraduate or graduate degree in Computer Science, Data Science, or equivalent education/professional experience is required
Please feel free to share resumes to sangeetha.sirivala@ivytechsol.us ,Adarsh@ivytechsol.comPowered by JazzHRvINzbmxXts"
Lead Data Engineer,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3633724130/?eBP=JOB_SEARCH_ORGANIC&refId=jprbWfaxGWningdTClzsyA%3D%3D&trackingId=4zMuNCPsdum%2BR2GHijdnSA%3D%3D&trk=flagship3_search_srp_jobs,3633724130,"About the job
            
 
Fully Remote but looking from DE, MD, NC, VA, DC, WV onlyFully Remote but looking from DE, MD, NC, VA, DC, WV onlyResponsibilities IncludeImmediate need for a Lead Cloudera Engineer with specific knowledge in Apache Solr. The Lead Cloudera Engineer is responsible for orchestrating, deploying, maintaining and scaling cloud OR on-premise infrastructure targeting big data and platform data management (e.g., data warehouses, data lakes) including data access APIs.  Hands-on/advanced (expert preferred) level experience in administrating and engineering relational databases (ex. MySQL, PostgreSQL), Big Data systems (ex. Cloudera Data Platform Private Cloud and Public Cloud), Apache Solr as SME, ETL (ex. Ab Initio), BI (ex. MicroStrategy), automation tools (ex. Ansible, Terraform, Bit Bucket) and experience working cloud solutions (specifically data products on AWS) are necessary. 40% of the week will be hands-on technical support.Leading several teams of Big Data Administrators/Engineers to ensure project cohesiveness/completion. 60% of the week will be lead related responsibilities.
Required Skills Education Level: Bachelor's Degree. In lieu of a Master's degree, an additional 4 years of relevant work experience is required in addition to the required work experience. At least 8 years of overall experience with all the tasks involved in administration of big data and Meta Data Hub such as Cloudera. Must have a minimum of 3 years of Lead experience (teams of 5+ is preferred) Must have expert level knowledge of Solr Excellent communication skills both written and verbal. Expert Experience with Advance knowledge of UNIX and SQL
Preferred  Experience with Ab Initio, EMR, S3, Dynamo DB, Mongo DB, ProgreSQL, RDS, DB2 DevOps (CI/CD Pipeline) is a Plus. Prior experience with migration from on-premise to AWS Cloud. Cloudera Data Platform Private Cloud big plus"
Sr. Data Engineer (with ETL testing experience),Netorbit Inc,United States (Remote),https://www.linkedin.com/jobs/view/3660827303/?eBP=JOB_SEARCH_ORGANIC&refId=jprbWfaxGWningdTClzsyA%3D%3D&trackingId=0ApBr3t9m%2FZCKwS%2F8wctUQ%3D%3D&trk=flagship3_search_srp_jobs,3660827303,"About the job
            
 
Title : Sr. Data Engineer (with ETL testing experience)Location: Austin (Remote OK)JdGood engineers who have knowledge of programming in Python/Pyspark functional programming experience is a definite plus.Experience in Cloud and building data flow/pipelines in mandatory AWS is preferred but any cloud is fine.Should know how to program distributed systemsExpected to do Quality engineering of Data Pipelines, Datawarehouse including Snowflake, Redshift etc.Knowledge of Test Automation preferred"
"Senior Data Engineer (Remote, US)",Collective[i],United States (Remote),https://www.linkedin.com/jobs/view/3772667351/?eBP=JOB_SEARCH_ORGANIC&refId=jprbWfaxGWningdTClzsyA%3D%3D&trackingId=79Ik28fqoIBvlTQ0sFKLLg%3D%3D&trk=flagship3_search_srp_jobs,3772667351,"About the job
            
 
Collective[i] is a remote company on a mission to fuel global prosperity, helping companies around the world forecast, optimize and grow revenue. Our applications and network support highly productive, enlightened teams with everything they need to work smarter and win more.We're in search of a Senior Data Engineer who finds satisfaction in constructing applications to manage operations on a global scale. In this role, you'll collaborate extensively with engineering, operations, and product teams to implement new applications onto our data framework, enhance existing outdated components, and conceive novel functionalities for our products. This opportunity calls for an individual enthusiastic about large-scale data technologies, adept at tackling intricate problems, skilled in teamwork, and possessing a mindset geared towards continual development.Qualifications Proficiency in Open Source and Big Data technologiesFamiliarity with Hadoop Cloudera or similar platformsExtensive Java programming backgroundPython programming experienceJava programming experienceProficient in messaging systems like KafkaAdept in utilizing SQL/NoSQL technologiesSkilled in developing oozie/airflow workflowsTrack record of crafting secure, high-scalable, dependable, and high-performing big data frameworks for processing, integrating, and analyzing intricate data via diverse open-source toolsKnowledge of cloud computing infrastructure (e.g., AWS, GCP, Snowflake) and factors related to scalable, distributed systemsPrevious exposure to analytical tools, programming languages, or librariesAdded advantage if experienced in devising, planning, and executing large-scale data intelligence solutions within the Snowflake Cloud Data Warehouse environmentExcellent communication skills with the ability to engage effectively with business, product, and development teamsPrioritizes teamwork and takes pride in collective achievementsCollaboration with other teams to unravel intricate challenges
Who you are working for - About Collective[i]:Collective[i] is passionate about using ML, RPA and other AI technologies along with a network to automate the myriad of tasks that distract sales professionals from selling and provide timely intelligence that helps to grow revenue. Our revenue optimization engine is one of the most transformative technologies to hit the enterprise since CRM. Founded and managed by the early teams behind LinkShare (purchased for $425m) and Overstock (NASDAQ:OSTK), Collective[i] is a private 100% remote company.Our core values help shape our culture: We are curious. We are direct. We deliver. We succeed together. We strive for the extraordinary. If you enjoy a challenge, thrive in an innovative environment and welcome the opportunity to work with amazing humans operating on the bleeding edge of technology, Collective[i] is the place for you.Recent press:Forbes: Stephen Messer: Amazon Missed The AI BoomCNBC: Harvard professor on A.I. job risks: We need to upskill ad update business modelsZDNet: Why open source is essential to allaying AI fearsInformation about the founders:Tad MartinStephen MesserHeidi MesserSalary ranges can vary significantly based on a multitude of factors, reflecting the diverse and complex nature of today's job market. These factors encompass a wide range of elements, including industry, experience, education, and geographic location."
Senior Data Engineer,Atlassian,"Mountain View, CA (Remote)",https://www.linkedin.com/jobs/view/3737405985/?eBP=JOB_SEARCH_ORGANIC&refId=jprbWfaxGWningdTClzsyA%3D%3D&trackingId=WoRuH7QqWlqinDjKgylE2A%3D%3D&trk=flagship3_search_srp_jobs,3737405985,"About the job
            
 
OverviewWorking at AtlassianAtlassians can choose where they work – whether in an office, from home, or a combination of the two. That way, Atlassians have more control over supporting their family, personal goals, and other priorities. We can hire people in any country where we have a legal entity. Interviews and onboarding are conducted virtually, a part of being a distributed-first company.Atlassian is looking for a Senior Data Engineer to join our Data Engineering Team and build world-class data solutions and applications that power crucial business decisions throughout the organization. We are looking for an open-minded, structured thinker who is passionate about building systems at scale. You will enable a world-class engineering practice, drive the approach with which we use data, develop backend systems and data models to serve the needs of insights, and play an active role in building Atlassian’s data-driven culture. You love thinking about the ways the business can consume data and then figuring out how to build it.ResponsibilitiesYour future teamOur Data Engineering Team is comprised of data experts. We build world-class data solutions and applications that power crucial business decisions throughout the organization. We manage multiple analytical data models and pipelines across Atlassian, covering finance, growth, product analysis, customer analysis, sales and marketing, and so on. We maintain Atlassian's data lake that provide a unified way of analyzing our customers, our products, our operations, and the interactions among them.We're hiring a Senior Data Engineer, reporting to the Data Engineering Manager who’s based in Sydney. Here, you'll enable a world-class engineering practice, drive the approach with which we use data, develop backend systems and data models to serve the needs of insights and help build Atlassian's data-driven culture. You love thinking about the ways the business can consume data and then figuring out how to build it.QualificationsWhat you'll do You'll partner with the product analytics and data scientist team to build the data solutions that allow them to obtain more insights from our data and use that to support important business decisions.You'll work with different stakeholders to understand their needs and architect/build the data models, data acquisition/ingestion processes and data applications to address those requirements.You'll add new sources, code business rules, and produce new metrics that support the product analysts and data scientists.You'll be the data domain expert who understand all the nitty-gritty of our products.You'll own a problem end-to-end. Requirements could be vague, and iterations will be rapidYou'll improve data quality by using & improving internal tools/frameworks to automatically detect DQ issues.
Your background A BS in Computer Science or equivalent experience with 5+ years of professional experience as a Sr. Data Engineer or in a similar role.Strong programming skills using PythonWorking knowledge of relational databases and query authoring (SQL).Experience designing data models for optimal storage and retrieval to meet product and business requirements.Experience building scalable data pipelines using Spark (SparkSQL) with Airflow scheduler/executor framework or similar scheduling tools.Experience working with AWS data services or similar Apache projects (Spark, Flink, Hive, and Kafka).Understanding of Data Engineering tools/frameworks and standards to improve the productivity and quality of output for Data Engineers across the team.Well versed in modern software development practices (Agile, TDD, CICD)
CompensationAt Atlassian, we strive to design equitable, explainable, and competitive compensation programs. To support this goal, the baseline of our range is higher than that of the typical market range, but in turn we expect to hire most candidates near this baseline. Base pay within the range is ultimately determined by a candidate's skills, expertise, or experience. In the United States, we have three geographic pay zones. For this role, our current base pay ranges for new hires in each zone are:Zone A: $163,300 - $217,700Zone B: $147,500 - $196,600Zone C: $135,600 - $180,700This role may also be eligible for benefits, bonuses, commissions, and equity.Please visit go.atlassian.com/payzones for more information on which locations are included in each of our geographic pay zones. However, please confirm the zone for your specific location with your recruiter.Our Perks & BenefitsAtlassian offers a variety of perks and benefits to support you, your family and to help you engage with your local community. Our offerings include health coverage, paid volunteer days, wellness resources, and so much more. Visit go.atlassian.com/perksandbenefits to learn more.About AtlassianAt Atlassian, we're motivated by a common goal: to unleash the potential of every team. Our software products help teams all over the planet and our solutions are designed for all types of work. Team collaboration through our tools makes what may be impossible alone, possible together.We believe that the unique contributions of all Atlassians create our success. To ensure that our products and culture continue to incorporate everyone's perspectives and experience, we never discriminate based on race, religion, national origin, gender identity or expression, sexual orientation, age, or marital, veteran, or disability status. All your information will be kept confidential according to EEO guidelines.To provide you the best experience, we can support with accommodations or adjustments at any stage of the recruitment process. Simply inform our Recruitment team during your conversation with them.To learn more about our culture and hiring process, visit go.atlassian.com/crh ."
Senior Staff Data Engineer,"Nav Technologies, Inc.","Atlanta, GA (Remote)",https://www.linkedin.com/jobs/view/3775332165/?eBP=JOB_SEARCH_ORGANIC&refId=jprbWfaxGWningdTClzsyA%3D%3D&trackingId=wk4Nh54vIBPhcvCqtzjdGQ%3D%3D&trk=flagship3_search_srp_jobs,3775332165,"About the job
            
 
Nav is democratizing small business financing. In other words, we give small business owners access and control. Yes, this challenges the norm, but it means working with curious, purpose driven, dedicated, and inquisitively smart people who push themselves, our company and the community to the next level (and every level after that). We are the people behind the tech. And when it's good, we look for better. We don't over think the value we bring nor spend time trying to revamp mantras. We also do not come up with some crafty way to tell you who we are and what we offer. We are Nav! Here, you'll gain a wealth of experience, learn the tricks of the trade, and work with winners. All companies say people are connected to their mission but in our case our mission and our people are one – it is a way of being not just a cause you are committed to. And since 2013, Nav has holistically and organically developed its own ideology because Nav can only be Nav.We are seeking an innovative and passionate Data Engineer to join our team. In this role you will be responsible for building the data and reporting infrastructure to power our systems, working with our team of engineers, product managers and designers; helping us create a better experience for the millions of Nav Small business users. You'll also work with our business intelligence and data science teams to improve the data platform, ensuring that the entire company is able to make better data-driven decisions.YOU WILL: Building cool stuff by maintaining and fulfilling a technical data engineering roadmap and vision. Creating data and reporting infrastructure by building and optimizing production-grade data pipelines through the use of continuous integration. Exploring data sets for insight and understanding that will help drive great product changes and priority. Communicating results and findings to company audiences of varying technical ability in a clear, engaging manner. Breaking down complex technical concepts into digestible tid-bits. Supporting self-service data pipeline management (ETL) and self-directed learning process. Addressing competing explanations in result validity, and use formal reasoning approaches to avoid bias and miscommunication. Collaboration with Engineering and Data Analysts to identify and design ways to solve critical business problems and ensure funnel optimization. Ability to design, develop, and maintain scalable, reusable code. 
WHO YOU ARE: You are a driven data engineer with experience working with big data technologies such as Docker, ECS, S3, Redshift, Kafka, and RDS. You have experience with ETL/ELT and data warehousing using tools such as dbt, Azure Data Factory, Matillion and/or Fivetran You love building data-driven products and have expertise in one or more programming languages (ideally Python). You have strong SQL experience, with expert level skill in Postgres, Snowflake and /or AWS Redshift. You're an Apache Airflow practitioner with experience managing cross-DAG dependencies in Airflow. You know how to implement, advocate, and teach agile practices, and adapt them to your people and circumstances, while working in accordance with data ethics & privacy standards. You can consistently evolve data models & data schema based on business and engineering needs. You've developed and extended design patterns, processes, standards, frameworks and reusable components for use in various data engineering functions and areas. Ability to implement systems for tracking data quality and consistency. You're comfortable working directly with all internal data consumers and are eager to share your work through calibrations and organizational product demos. You are committed to quality! And you're someone who's willing to tackle technical debt and constantly evolving our processes to help nav grow. We'd love it if you have a high level understanding of credit scores, ecommerce, and B2B financing. 
Inclusion at Nav:At Nav, we celebrate what makes our employees unique because the businesses we serve are progressively diverse and distinctly original. Navericks are diverse, side hustlers, immigrants, veterans, queer, and we push generational boundaries. We are college dropouts, PhDs, special needs parents, allies, pet owners and community leaders. Navericks are human. We are committed to upholding a safe, supportive environment where everyone matters. We are committed to making a better future for all of us. We have created a workplace where people of all backgrounds can express their identities authentically. To put it simply, we want you to be proud to be you.Our Compensation Philosophy is simple but powerful:At Nav, we are transparent about our total rewards, including pay, across all levels and roles. We believe great, enduring relationships are grounded in trust and transparency. Compensation shouldn't be a distraction, and employees should understand how pay and career advancement decisions are made. Providing equal pay for equal work is table stakes for being a great place to work. Gender and ethnic inequity should only be something that our children read about in history books. We believe providing Navricks with company ownership, competitive pay, and a range of meaningful benefits is the start of creating a culture where people want to give the best they've got — not because they're simply making money, but because they've fallen in love with our vision, mission, values, and team.During the interview process, your recruiter will be explaining how our rubrics work across all of our total rewards ( base, equity, bonus, perks, and benefit) offerings . The base salary for this role is targeted between $160K - $170K per year. Final offer amount is determined by your proficiencies within this level.Our impact on you:Competitive Pay. Company Ownership. Unlimited Vacation. Benefits Day One. 6 Weeks Paid Parental Leave. Work From Anywhere (yes we were distributed before it was cool). Flexible Work Arrangements. Free Telehealth and Telemental Health For All Employees. Employee Networking and Events. Community Network Groups (women's, PRIDE, culture). Meaningful Perks and Rewards. Learning and Development Opportunities. Pet Insurance.A Naverick's DNA:  We look at the future and say ""why not""; we see possibilities where others see problems or routines. We show the way ahead and are committed to achieving ambitious goals. We practice straight talk and listen generously to each other with empathy. We value different opinions and point of views. We ensure that we connect outside as well as inside to learn from others and inspire each other. We hold ourselves accountable for delivering results. We choose to not to be a victim of circumstance. We make decisions & take responsibility so that we can act & support each other, rather than adopting defensive, and ""finger pointing"" behaviors. As leaders we motivate & engage our teams to undertake beyond what they originally thought possible, by developing our teams & creating the conditions for people to grow and empower themselves through enabling & coaching. 
If you are based in California, we encourage you to read our privacy notice for California residents linked here."
Senior Data Engineer,Tail Wind Informatics Corporation,"Minnesota, United States (Remote)",https://www.linkedin.com/jobs/view/3769567047/?eBP=JOB_SEARCH_ORGANIC&refId=jprbWfaxGWningdTClzsyA%3D%3D&trackingId=k8FzFYkG1YfD42ZaMJclgg%3D%3D&trk=flagship3_search_srp_jobs,3769567047,"About the job
            
 
About Us: Tail Wind Informatics Corporation -Microsoft Solutions Partner- is a dynamic and innovative IT consulting services company that specializes in delivering Data Architecture and Business Intelligence solutions. We are currently seeking a talented and experienced Senior Data Engineer to join our team and play a crucial role in managing and optimizing data infrastructure. If you are passionate about data, have a strong background in Microsoft technologies, and enjoy solving complex data challenges, we want to hear from you!Position Overview: As a Senior Data Engineer at Tail Wind, you will be responsible for designing, developing, and maintaining data pipelines and systems on the Azure cloud platform. You will work closely with cross-functional teams to ensure that data is accurate, accessible, and supports data-driven decision-making. The ideal candidate has hands-on experience with Azure Data Factory, SQL, Stored Procedures, Python, Databricks, Snowflake, Synapse, and Power BI.Qualifications Bachelor's degree in Computer Science, Information Technology, or a related field (or equivalent work experience)5+ years of experience as a Data Engineer with a focus on Microsoft technologiesStrong proficiency in Azure Data Factory, or related product, for ETL processes and data orchestrationProficiency in SQL and T-SQL, including the ability to write complex queries and stored proceduresExperience with Python for data manipulation and automationKnowledge of Databricks for big data processing and analyticsFamiliarity with Snowflake or Azure Synapse Analytics for data warehousingProficiency in Power BI for data visualization and reportingKnowledgeable in methodologies of data warehouse designPersonal experience modeling and building data warehousesStrong problem-solving skills and attention to detailExcellent communication and teamwork abilities
Key Responsibilities Design, build, and maintain data pipelines and ETL processes using Azure Data FactoryDevelop and optimize SQL queries, stored procedures, and scripts for data transformation and extractionCollaborate with data scientists and analysts to understand data requirements and ensure data availabilityImplement data quality checks and data validation processes to ensure data accuracy and consistencyUtilize Databricks for advanced data processing, transformation, and analyticsLeverage Python for data manipulation, automation, and data integration tasksManage and optimize data storageBuild and maintain data warehouses and analytics solutionsCreate interactive reports and dashboards using Power BI for data visualization and insightsMonitor and troubleshoot data pipelines, addressing any issues in a timely mannerStay up-to-date with the latest Azure data technologies and best practices
Why join Tail Wind? Competitive salary and benefits packageOpportunity to work with cutting-edge technologies and solve challenging data problemsCollaborative and innovative work environmentProfessional development opportunities and support for various certifications
Application Process:To apply for this position, please click ""apply for this job"" at the top or bottom of the page, then upload and submit a current copy of your resume. One of the members of our Talent Acquisition team will reach out to schedule a phone interview if you meet the qualifications listed above.If you are a passionate Senior Data Engineer with a strong background in Azure Data Factory, SQL, Python, Databricks, Snowflake, Synapse, and Power BI, we encourage you to apply and be a part of our incredible team.Tail Wind Informatics Corp., is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees. Salary range can vary depending on experience and will be determined based on the results of a Technical Interview"
Senior/First Data Engineer,Sleeper,"New York, NY (Remote)",https://www.linkedin.com/jobs/view/3737464941/?eBP=JOB_SEARCH_ORGANIC&refId=jprbWfaxGWningdTClzsyA%3D%3D&trackingId=6th%2FbzYeGQkmUe15G%2BwH5g%3D%3D&trk=flagship3_search_srp_jobs,3737464941,"About the job
            
 
Position SummaryThe Senior/First Data Engineer will play a crucial role in building, maintaining, and enhancing ETL processes that drive our analytics and machine learning platforms. This individual will be responsible for developing actionable insights from complex data sets, and work closely with various business units to inform strategy and decision-making.You will be the first hire in this function.Location SF Bay Area, NYC, or Remote
Key Responsibilities ETL & Backend Development:Design and optimize ETL pipelines. Develop robust backend systems for large-scale data processing using Elixir and database solutions like Cassandra/ScyllaDB. Data Architecture:Design scalable and efficient data models for Cassandra and ScyllaDB. Ensure data integrity, quality, and security. Data Science Support:Collaborate with data scientists, providing them with clean and reliable datasets. Assist in implementing and scaling data science models. Innovation & Research:Stay abreast of latest technologies. Recommend technical improvements for data processing and storage. 
QualificationsRequired Bachelor’s or Master’s degree in Computer Science, Engineering, or a related technical field. 5+ years of experience in backend development, with a strong focus on data engineering.  Technical skills: Expertise in Python, Java, Scala, and Elixer for backend and ETL processes. Mastery of ETL tools/frameworks (e.g. Apache Kafka, Apache Airflow). Deep knowledge of SQL/NoSQL databases, including Cassandra and ScyllaDB, and data warehousing solutions (e.g., Redshift, BigQueary, Snowflake). Proficiency in cloud platforms (AWS, GCP, Azure) and distributed systems. Familiarity with data science concepts, tools, and libraries (e.g. Pandas, Scikit-learn). Soft Skills: Exceptional problem-solving skills. Strong communication for technical and non-technical discussions. 

Nice-to-have Experience with cloud platforms like AWS, GCP, or Azure. Exceptional communication skills, both verbal and written. Expertise in machine learning algorithms and frameworks (e.g., TensorFlow, PyTorch, scikit-learn). 
Benefits Competitive salary ($150,000-$225,000/year) and stock optionsComprehensive health, dental, and vision plans401(k) Flexible working hours and remote work optionsRegular team building events and activities"
MySQL Data Engineer with Java,Sonitalent Corp,United States (Remote),https://www.linkedin.com/jobs/view/3750133947/?eBP=JOB_SEARCH_ORGANIC&refId=jprbWfaxGWningdTClzsyA%3D%3D&trackingId=lC2k%2BpdCwr2ybVOfbxkk0A%3D%3D&trk=flagship3_search_srp_jobs,3750133947,"About the job
            
 
Role: MySQL Data Engineer with JavaLocation: RemoteKey skills: Data Engineer, MySql, SQL, JavaMUST Have8+ years’ experience as a Data Engineer/ DeveloperWell versed with MySQL database queries and creation of database views.Development experience with REST, SOAP, LDAP, MySQLDevelopment experience with Java applications"
Data Engineer (Azure Data bricks/ Pyspark) - Remote,Lorven Technologies Inc.,United States (Remote),https://www.linkedin.com/jobs/view/3726476199/?eBP=JOB_SEARCH_ORGANIC&refId=jprbWfaxGWningdTClzsyA%3D%3D&trackingId=WC9rzcJ41f2ldicLM4ihIw%3D%3D&trk=flagship3_search_srp_jobs,3726476199,"About the job
            
 
Our client is looking for The Data Engineer for a Long-term Remote below are the detailed requirements.Kindly share your Updated Resume with the Best Reachable Number.Role: Data Engineer (Azure Data bricks/ Pyspark)Location: RemoteDuration: Long-term ContractRequired Skills: Azure Data bricksNice to have skills: Azure Data Factory, Python.Job Description Bachelors in Computer Science and/or equivalent combination of education with 09+ Years of work experience.The Client is looking for AZURE DEVELOPER (RAMP UP).Candidates must have advanced knowledge on Azure Cloud components - Data lake store, Data Factory, SQL DW. Hands-on Experience in SQL Server project development writing SQL, TSQL Code, Views development, Stored Procedures and functions is a must.Knowledge on SQL Code Performance Tuning and optimizing SQL Code.Develop Cosmos DB Server side program, Collections, Partitions etc. Work on ADF routines to load data from Azure Data Lake Store to Cosmos DB.Work closely with business analysts/users / API / Reporting to understand which entities are expected for exposing to Users, so that the Cosmos Collection can be built efficiently.Ability to use strong industry knowledge to relate to customer needs and dissolve customer concerns and a high level of focus and attention to detail.Strong work ethic with good time management with the ability to work with diverse teams.
NoteTHESE EMAILS ARE GENERATED BY KEYWORD AND I APOLOGIZE IF THESE SKILLS SETS DO NOT MATCH YOUR EXPERTISE, OR IF THE LOCATION IS OUT OF RANGE.We do have other opportunities available. If you are interested, please send me your latest resume. If you are not currently seeking employment, or if you would prefer, I contact you at some later date, please indicate your date of availability so that I may honor your request."
Data Conversion Engineer,"InvoiceCloud, Inc.","Salt Lake City, UT (Remote)",https://www.linkedin.com/jobs/view/3741803063/?eBP=JOB_SEARCH_ORGANIC&refId=jprbWfaxGWningdTClzsyA%3D%3D&trackingId=iEygKmYGM1Sd%2B8X7qiWBRw%3D%3D&trk=flagship3_search_srp_jobs,3741803063,"About the job
            
 
About InvoiceCloud:InvoiceCloud, an EngageSmart solution, is a leading provider of online bill payment services. Founded in 2009, the company has grown to be one of the leading disruptors in the cloud-based electronic bill presentment and payment (EBPP) space, helping institutions put customer experience first. By switching to InvoiceCloud, clients can improve customer engagement, loyalty, and efficiency while reducing churn and missed payments in the process. With over 50 million payments processed annually, InvoiceCloud is one of the most secure, innovative, and inclusive fintech solutions in the market. To learn more, visit www.InvoiceCloud.com.As a Data Conversion Engineer on our integrations team, you will work to convert data for InvoiceCloud clients as they transition or upgrade their core processing system, ensuring their customers' online bill paying experience remains consistent and uninterrupted. You will be responsible for analyzing the source data, designing the target data format, extracting existing data, and performing the transform & load. You will be encouraged to develop automated scripts and tools to facilitate repeatable conversions where possible.As a Data Conversion Engineer You Will:  Understand how to write and troubleshoot complex SQL queriesUnderstand ETL (Extract, Transform, and Load) processes and toolsCreate wikis for data conversion processes and toolsAble to handle a fast-paced environmentAble to multitask efficiently
What We Seek:  Bachelor's Degree preferred or equivalent combination of education and experience required5 years of Experience using Microsoft Technologies including VB.NET, ASP.NET, C#, Visual StudioProficiency with SQL Server and Microsoft ETL toolsMachine learning and automation experience a plusSelf-led, capable of working with little directionSkilled communicator with a collaborative spirit
Base Compensation Range: ($70,000.00 to $90,000.00) annuallyBase salary is one component of total compensation. Employees may also be eligible for an annual bonus or commission and equity. Some roles may also be eligible for overtime pay.The above represents the expected base compensation range for this job requisition. Ultimately, in determining your pay, we'll consider many factors including, but not limited to, skills, experience, qualifications, geographic location, and other job-related factors.BenefitsWe offer a competitive benefits program including: Medical, dental, vision, life & disability insurance401(k) plan with company match & employee stock purchase plan (ESPP)Flexible Time Off (FTO), wellbeing days, paid holidays, and summer FridaysMental health resourcesPaid parental leave & Backup CareTuition reimbursementEmployee Resource Groups (ERGs)
Invoice Cloud is an Equal Opportunity Employer. Invoice Cloud provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training. If you have a disability under the Americans with Disabilities Act or similar law, or you require a religious accommodation, and you wish to discuss potential accommodations related to applying for employment at our company, please contact jobs@engagesmart.com.Click here to review EngageSmart's Job Applicant Privacy Policy.To all recruitment agencies: Invoice Cloud does not accept agency resumes. Please do not forward resumes to our job's alias, employees, or any other organization location. Invoice Cloud is not responsible for any fees related to unsolicited resumes."
Lead Data Engineer | Industry: Insurance,Vodastra Technologies,"New York, NY (Remote)",https://www.linkedin.com/jobs/view/3736514777/?eBP=JOB_SEARCH_ORGANIC&refId=jprbWfaxGWningdTClzsyA%3D%3D&trackingId=gIqbdyl6QFob05buCCSzFg%3D%3D&trk=flagship3_search_srp_jobs,3736514777,"About the job
            
 
Job Title: Lead Data EngineerJob Type: Full TimeLocation: Fully RemoteIndustry: Insurance JOB DESCRIPTION: With a startup spirit and 90,000+ curious and courageous minds, we have the expertise to go deep with the world’s biggest brands—and we have fun doing it. Now, we’re calling you all who see the world differently and are bold enough to reinvent it. Transformation happens here. Come, and be a part of our exciting journey!QUALIFICATIONS:  10+ years of experience in data engineering 5+ years of experience managing a team of data engineers. Strong understanding of SQL Server SSIS Excellent communication and interpersonal skills Ability to work independently and as part of a team. Ability to meet deadlines and work under pressure. Strong desire to be hands-on with the development and implementation of data pipelines and data warehouses. Experience working with stakeholders to develop product backlog grooming, sprint planning data engineering, and QA Testing Migration of SQL Server to Azure is desired. Experience with Azure Data Factory, ADLS, and CI/CD highly desired 
The Hands-On Data Engineering Lead is responsible for leading a team of data engineers in the design, development, and implementation of data pipelines and data warehouses. The ideal candidate will have a strong understanding of data engineering principles and practices, as well as experience with SQL Server SSIS. The Hands-On Data Engineering Lead will also be responsible for managing the team's workload, mentoring junior engineers, and communicating with stakeholders. Additionally, the Hands-On Data Engineering Lead will be expected to be hands-on with the development and implementation of data pipelines and data warehouses and to have experience working with stakeholders to develop product backlog grooming, sprint planning data engineering, and QA Testing.JOB RESPONSIBILITIES:  Lead the design, development, and implementation of data pipelines and data warehouses. Develop and implement data pipelines and data warehouses using SQL Server SSIS Troubleshoot data pipelines and data warehouses. Communicate with stakeholders to understand their needs and ensure that projects meet their requirements. Manage the team's workload and ensure that projects are completed on time and within budget. Mentor junior data engineers and help them develop their skills. Stay up to date on the latest data engineering technologies and best practices. Provide support to production teams and QA teams. Work with stakeholders to develop product backlog grooming, sprint planning data engineering, and QA Testing Working with other engineers to design and implement data pipelines and data warehouses using Azure Data Factory, ADLS, and CI/CD Experience in migrating SQL servers to Azure 30% team management and 70% hands-on 
CANDIDATE DETAILS  10+ to 15 years of experience Seniority Level - Mid-Senior Management Experience Required - No Minimum Education - bachelor’s degree Willingness to Travel - Occasionally
Powered by JazzHRg3kXP6OKMd"
"Staff Data Engineer, Applied Machine Learning",Red Cell Partners,United States (Remote),https://www.linkedin.com/jobs/view/3736533089/?eBP=JOB_SEARCH_ORGANIC&refId=jprbWfaxGWningdTClzsyA%3D%3D&trackingId=ZhhgJvE7w8hMDVCaExgyRw%3D%3D&trk=flagship3_search_srp_jobs,3736533089,"About the job
            
 
About UsRed Cell Partners is an incubation firm building rapidly-scalable technology-led companies that are bringing revolutionary advancements to market in healthcare and national security. United by a shared sense of duty and deep belief in the power of innovation, Red Cell is developing powerful tools and solutions to address our Nation’s most pressing problems.Job Description:Are you passionate about driving innovation through machine learning? Do you thrive in building and optimizing ML systems? If so, we have an exciting opportunity for you as a founding team member and lead data engineer at a stealth healthcare incubation.As a Staff Data Engineer specializing in Applied Machine Learning, you will be at the forefront of driving the development and optimization of our ML systems. You will play a crucial role in training ML models, building training pipelines, fine-tuning Language Models (LLMs), and designing feedback systems to continuously improve their performance. If you're a forward-thinking, technically adept individual with a passion for machine learning, this role is for you.Responsibilities: Lead the team in architecting, building, and optimizing ML systems to deliver high-quality, real-world results. Design and implement robust training pipelines for machine learning models, ensuring efficiency and scalability. Fine-tune Language Models (LLMs) to meet specific business needs and optimize their performance for various applications. Develop and implement feedback mechanisms to continuously improve the accuracy and effectiveness of LLMs. Collaborate with cross-functional teams to understand business requirements and translate them into actionable ML solutions. Stay up-to-date with the latest advancements in machine learning and implement best practices to enhance our ML infrastructure. Coach and mentor junior data engineers, fostering a culture of continuous learning and growth within the team. Communicate complex technical concepts and findings to non-technical stakeholders in a clear and concise manner. 
Requirements: Proven experience in a leadership role driving ML system development and optimization. Demonstrated expertise in training ML models and building robust training pipelines. Extensive knowledge of fine-tuning Language Models (LLMs) to achieve desired outcomes. Strong understanding of machine learning frameworks such as TensorFlow, PyTorch, or similar. Proficiency in programming languages like Python or Go, and the ability to write efficient, clean, and maintainable code. Excellent written and verbal communication skills, with the ability to convey technical concepts to both technical and non-technical audiences. A Bachelor's or Master's degree in Computer Science, Engineering, or a related field. A Ph.D. in Machine Learning or a relevant area is a plus. A track record of delivering impactful machine learning solutions that have been successfully deployed in real-world applications. 
Benefits: Competitive salary and performance-based bonuses. Comprehensive health and wellness benefits package. Flexible work hours. Opportunities for professional development and continued learning. Collaborative and inclusive work environment. 
We’re an Equal Opportunity Employer: You’ll receive consideration for employment without regard to race, sex, color, religion, sexual orientation, gender identity, national origin, protected veteran status, or on the basis of disability."
Data Engineer (GCP),Experfy,United States (Remote),https://www.linkedin.com/jobs/view/3761188188/?eBP=JOB_SEARCH_ORGANIC&refId=jprbWfaxGWningdTClzsyA%3D%3D&trackingId=cmJdvRB82S5EHdO71c0mHA%3D%3D&trk=flagship3_search_srp_jobs,3761188188,"About the job
            
 
Required Qualifications: Hands on-experience working with cloud technologies - GCP knowledge strongly preferredETL development experience with strong SQL backgroundHands-on experience of building and operationalizing data processing systemsExperience in NoSQL databases and close familiarity with technologies/languages such as Python/R, Scala, Java, Hive, Spark, KafkaExperience with any traditional RDBMS (e.g., Teradata, Oracle, DB2)Experience working with data platforms (Data warehouse, Data Lake, ODS)Experience working with tools to automate CI/CD pipelines (e.g., Jenkins, GIT, Control-M)
RequirementsPreferred Qualifications: GCP (google cloud platform) experiencePython Experience working on healthcare / clinical dataData analysis / Data mapping skillsJSON and XMLFamiliarity with HL7 / FHIR"
Lead Data Engineer,Lorven Technologies Inc.,United States (Remote),https://www.linkedin.com/jobs/view/3714698407/?eBP=JOB_SEARCH_ORGANIC&refId=jprbWfaxGWningdTClzsyA%3D%3D&trackingId=TFAG0nyOOoaQ2xUjrjO16A%3D%3D&trk=flagship3_search_srp_jobs,3714698407,"About the job
            
 
Job Title: Lead Data EngineerLocation: Remote Duration: 6 to 12+ Months Contract 10+ years of prior experience Lead Data Engineer.AWS/Snowflake/DBT experiencesMust be proficient in SQL, Python. Automate repetitive personal tasks through Python and SQLExperience with Tableau or any visualization tool, Data Warehousing, Data ModelingStrong experience with relational databases like SQL Server, MySQL and Vertica. NoSQL databases experience is a plusExperience with user-defined workflows (e.g., Airflow)Experience with writing Kafka consumers and producers.Experience Apache Spark Streaming and Hive is plus.Experience with Linux servers and Docker is required.Problem solver that is action-oriented with the ability to look at problems in new ways.Working knowledge of data management software like Airflow, or other ETL tools a plus.Strong analytical and problem-solving ability to design an effective solution.Ability to support multiple on-going projects in a fast-paced environment.Strong communicational skills, organizational skills, negotiation skills, and flexibility to address competing demands.Superior business judgement ability to flex between big picture thinking, understand and distill complex ideas, and analyze data to drive strategic objectives."
Senior/Staff Data Engineer,Shiftsmart,United States (Remote),https://www.linkedin.com/jobs/view/3729896864/?eBP=JOB_SEARCH_ORGANIC&refId=jprbWfaxGWningdTClzsyA%3D%3D&trackingId=2p%2BHNwoXkQ5%2FAN5Fc6kIWw%3D%3D&trk=flagship3_search_srp_jobs,3729896864,"About the job
            
 
Why ShiftsmartWe’re building the Amazon of labor - a platform for hourly work in the enterprise. At Shiftsmart we work with the largest companies and government agencies in the world - including Walmart, PepsiCo, Google, and Circle K - to fractionalize jobs into shifts, increasing labor supply and liquidity, and enabling our global network of over 3M workers to work across locations, employers, and categories of work. We’ve grown to >$100M ARR, doubling each year since we started, paid over $130M in wages to hourly workers, and raised $120M from top-tier investors like D1 Capital & Imaginary Ventures. And we’re only getting started.Our mission is to build the world’s leading platform for hourly workers and enterprises enabling flexibility, choice, and upward social mobility. Join a rocketship. This is a unique opportunity to join the ground floor of a rapidly scaling business that is changing the way hourly labor works and capturing a $2 trillion dollar opportunity in the US alone. Ownership + impact. Join a team that is challenging and supporting one another to build a great business and create flexibility, choice, and upward social mobility for workers everywhere. Captain your career path. Enjoy accelerated technical development and coaching working with a team of top engineers in their domains (e.g Lyft, Amazon, DoorDash, PayPal)Become a world-class builder. You'll have the opportunity to not only build both individual tooling and features but also influence the architecture of our entire platform. Every member of the engineering team will be involved in building from the ground up. 
Mission: What we do and why we need youWe are seeking a highly skilled and experienced Senior / Staff Data Engineer to join our Core Platform Engineering team. As a founding member of this team, you will be a key contributor to every critical decision, and responsible for driving the design, development, and governance of Shiftsmart's data. Your expertise in software engineering, data systems, and data governance will play a critical role in building scalable and efficient data platforms and tools while ensuring data integrity and compliance. This is a unique opportunity to make a significant impact on our organization, contribute to the success of our team, and champion data governance practices.Shiftsmart's headquarters is located in New York City, but this is a remote position open to candidates residing in the US and Canada.Outcomes: The problems you will solve Technical Leadership: Provide technical leadership and guidance in designing and implementing data solutions that meet business requirements and align with data governance principles. Solution Development: Collaborate with stakeholders to understand product and business needs, and translate them into scalable and reliable data systems and tools, while ensuring data quality, privacy, and compliance. Data Governance: Champion and enforce data governance practices, including data lineage, metadata management, data quality controls, and privacy regulations. Data Architecture: Design and develop large-scale data systems, including databases, data warehouses, and big data platforms, with a strong focus on data governance and compliance requirements. Software Engineering: Apply software engineering best practices to build robust and maintainable data solutions, ensuring code quality, performance, and scalability in line with data governance guidelines. Data Integration: Collect and integrate data from various sources to create a unified and accurate source of truth for financial and compliance domains, while adhering to data governance policies. Performance Optimization: Optimize data processing and query performance while ensuring adherence to data governance SLAs. Automation and Efficiency: Drive automation initiatives by developing scripts, utilities, and frameworks to streamline data processes, improve efficiency, and enforce data governance practices. Collaboration and Mentoring: Collaborate with cross-functional teams, mentor junior engineers, and foster a culture of data governance and compliance awareness within the team. Innovation and Continuous Improvement: Stay updated with the latest industry trends and technologies in data engineering and data governance, evaluate new tools and techniques, and propose innovative solutions to enhance data systems, processes, and governance practices. Documentation and Knowledge Sharing: Maintain comprehensive documentation of data solutions, processes, best practices, and data governance frameworks, and actively share knowledge with the team. 
Competencies: Who you are Experience: 5-7 years of experience as a software engineer, with a strong focus on data engineering and large-scale data systems. Technical Expertise: Proficiency in technologies such as MongoDB Atlas, Google Cloud Platform (GCP), BigQuery, Prefect, Typescript, and Python. Experience with additional data technologies and tools is a plus. Data Governance: Strong understanding of data governance principles, regulations, and industry best practices, with practical experience in implementing and enforcing data governance frameworks. Data Architecture: Solid understanding of data architecture principles and proven experience in designing and developing scalable data systems, while considering data governance requirements. Software Engineering Skills: Excellent programming skills, with expertise in building robust and scalable software solutions using modern software engineering practices in alignment with data governance guidelines. Problem-Solving Abilities: Analytical mindset with the ability to understand complex business or technical problems and propose effective data solutions, considering data governance and compliance aspects. Leadership Skills: Demonstrated ability to provide technical leadership, mentor junior engineers, and drive successful outcomes in a collaborative team environment, with a focus on data governance excellence. Communication Skills: Strong verbal and written communication skills to effectively convey complex technical conceptsEducation: Bachelor’s or master’s degree in computer science, engineering, or a related field
Compensation PhilosophyTo provide greater transparency we share base salary ranges, which are based on role and level benchmarked against similar stage, high growth companies. Offers are determined based on multiple factors including skills, work experience, and relevant credentials.In addition to competitive salaries and meaningful equity we offer the following benefits: Comprehensive healthcare coverage: We cover 100% of employee premiums for medical, dental, and vision care (60-75% for dependents)401(k) match program: We match 100% on the first 3% of your contributions and 50% on the next 2% for a maximum match of 4%Generous, fully paid parental and family leave policiesPre-tax commuter benefitsUnlimited and flexible PTOCollaborative office with fully stocked kitchen @ 1 World Trade in Manhattan
Equal opportunity employerShiftsmart is committed to creating a diverse environment and is proud to be an equal-opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status.Operating Principles @ ShiftsmartInspired by Leadership Principles @ AmazonExecution Is BinaryWe #GetShiftDone. We take immense pride in both the quality of our work and our relentless determination to deliver on our commitments. If we say we are going to do something, we do it.We own the outcome with an unstoppable mindset through the finish line and are impatient to move the ball forward. This means we work really hard, execute with urgency, and ruthlessly challenge timelines for anything important. As a result, we do not defer responsibility to other teams or individuals. Instead, we take the problem as far as we can and only when needed ask others for help.Each time a crisis or opportunity emerges we take the hill as one team, because we are allergic to the words “it can’t be done”.Missionaries, Not MercenariesWe before me. We believe in our mission to build a better world for workers. We understand why our work matters and take seriously how it impacts our customers and our partners. This belief permeates everything we do from the strategic to the mundane.We are energetic, ambitious, and want to win. We constantly raise the standards for ourselves and everyone around us. We show up for our customers, our partners, and most importantly our teammates, and make every effort to build lasting relationships with each of them.We do not measure success based on our titles or the size of our empires. This also means we put the needs of the business before the details of our job descriptions. Rather than fight for a bigger piece of the pie, we fight to grow the entire thing and recognize this is how to grow our careers too.Inputs > OutcomesWe work really hard. Fundamentally changing how labor works is not easy. It often requires long days, late nights, and weekends to deliver on our commitments. We lean into this challenge.We focus on the process. We think in terms of value chains and appreciate that a bad process with a good outcome is simply dumb luck.We lead with data. We use facts, not fiction, to build narratives and make decisions. To do this we prepare written memos in advance and resist the urge to engage in endless water cooler what ifs, because we value the time and attention of our teammates.We hire and develop the best. When we decide to hire a new team member, we do so because we believe they will increase the talent density on our team. We view ourselves as leverage maximizers rather than inconvenience reducers and strive to increase the output of everyone we interact with.Honesty Over HarmonyWe share the truth even when it is painful. We do not, however, share the truth callously to hurt people’s feelings or make them look bad. We also assume positive intent. If someone is not delivering in a way that we need, we ask them and tell them before assuming the worst.We embrace mutual feedback. As people leaders we care more about our team’s growth and success than how much others like us. As individuals we seek, accept, and apply feedback. We do not give or take feedback personally because we understand it enables us to learn and grow.We tell the truth to ourselves. We reject a pollyannaish view of our world. Instead if something isn’t going well that we are responsible for, we call it out. And when someone calls out their own truth that may be less optimal, we don’t punish them for it.We have the meeting in the meeting. If something is broken or we disagree, we call it out and say something in the moment even if it feels uncomfortable to do so. This means that if something is broken, we do not just accept it and complain later.Invent & IterateWe are inventors @ heart. We categorically reject the phrase “that is how it’s always been done”, and constantly discover new and better ways to do more with less. This means we are resourceful and often do things that don’t scale, only to create ways to scale them later. We’re builders.We think BIG. At every level of the company, we embrace big, hairy, audacious, and transformative goals. We fear lack of progress and incremental thinking more than failing to deliver or falling short of an audacious goal. We believe courage means to try without fear and learn without ego.We do not let perfect get in the way of better. When faced with the choice we prioritize delivering something, even if imperfect, over endless debate and alignment. We embrace good mistakes.Compensation Range: $150K - $250K"
Data QA Engineer,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3735330013/?eBP=JOB_SEARCH_ORGANIC&refId=jprbWfaxGWningdTClzsyA%3D%3D&trackingId=qR%2FX5W2fRshx4dNdfgKYBQ%3D%3D&trk=flagship3_search_srp_jobs,3735330013,"About the job
            
 
Share only 2 profilesSkillsNEW ROLE – NEED EXCELLENT COM SKILLS AND QUALTILY CANDIDATE NEED DOB ( MM/DD)NEED LINKEDIN PROFILENEED PHONE NUMBER (NO VIOP OF GOOGLE VOICE PLEASE ) NEED DL COPYNEED VISA COPY DATA ENGINEER CERTFICATE IS GOOD YO HAVE Title : Data QA Engineer Location : Remote Duration : 6+ monthsTitle : We are seeking a dedicated and technically skilled Data QA Engineer. The Engineer will work closely with development leads and data engineers to define analysis strategies and create test plans that ensure the highest quality of code delivery. The candidate will build, manage, and maintain a robust testing environment.What You'll Do Collaborate with development leads and data engineers to define analysis strategy and test plans to ensure quality delivery.Define, build, automate, and execute testing for code in an agile practice in accordance with sprint planning.Build, manage, and maintain unit, integration, and regression tests using DBT, Vertex, and Colab.Troubleshoot issues found during the development and testing phases, proposing and implementing efficient solutions.Design, monitor, and maintain QA reports, KPIs, and quality trends to evaluate the effectiveness of QA processes.Communicate accurately the status and risks for ongoing work and timelines to stakeholders.
What You Have Expert-level proficiency in SQL and Python.Hands-on experience with DBT, Vertex, and Colab.Strong experience with agile development methodologies.Knowledge of software QA methodologies, tools, and processes.Familiarity with CI/CD tools and methods.Bachelor's degree in Computer Science, Engineering, related field, or equivalent experience.
The ideal Data QA Engineer for your environment should possess:  **Expertise in SQL**: Profound knowledge of SQL to craft intricate queries, especially in BigQuery, to validate data structures, relationships, and values. Their SQL expertise is foundational for comprehensive data validation and ensures that complex datasets are correctly interrogated. **Python Mastery**: Proficient in Python scripting for developing automation tasks, custom validation scripts, and integrating with GCP services. Their Python expertise is pivotal for building custom QA tools and bridging various services in your tech stack. **BigQuery Proficiency**: Ability to optimize for performance and deeply understand BigQuery's unique functions, using it as a primary tool for data validation. **dbt Familiarity**: Experience with dbt for creating and validating data models, utilizing its testing functions, and automating these tests. **Looker Understanding**: Capability to validate LookML models, ensuring that visualizations correctly represent the data underneath. **GCP Automation Tools**: Skilled in GCP's tools, especially Cloud Composer, for orchestrating automated QA workflows. **Version Control and CI/CD**: Knowledge of Git for versioning and familiarity with CI/CD processes, particularly in a dbt context. **Data Quality Monitoring**: Proficiency with tools that monitor data quality, potentially leveraging Python for custom solutions. **Containerization**: Knowledge of Docker and Kubernetes for deploying automation tasks within GCP. **Integration and Performance Testing**: Ability to validate the entire data transformation pipeline from source to visualization and ensure optimal query and transformation performance.
This individual's expertise in SQL and Python, combined with their experience in the mentioned tools and practices, will equip them to uphold and assure data quality in your specific environment."
Remote Opportunity: AIML-Data Infrastructure-Data Virtualization Infrastructure Engineer,SPAR Information Systems LLC,United States (Remote),https://www.linkedin.com/jobs/view/3689038986/?eBP=JOB_SEARCH_ORGANIC&refId=jprbWfaxGWningdTClzsyA%3D%3D&trackingId=Q011ZN81UMfEwxZ4BBPmXQ%3D%3D&trk=flagship3_search_srp_jobs,3689038986,"About the job
            
 
Hello All, Hope you are doing great Please go through the job description and let me know your interest. Role: AIML-Data Infrastructure-Data Virtualization Infrastructure Engineer Location: Cupertino, CA (REMOTE) Duration: Long Term Contract Job Description: AIML Data Infrastructure provides the foundation and core technologies all higher level abstractions run on. We strive to provide the best experience to our stakeholders. Our objective is to provide our engineers and data scientists a highly stable, reliable and performant platform to perform both analytics and data engineering activities on. Our goal is simple, provide a world class infrastructure to enable our engineers and data scientists to do their best work. *Should be passionate about automation, optimizations and tuning. Be part of the Data Infrastructure journey. Key Qualifications: Passionate about Data Compute and Storage Technologies (Object Storage, Caching, Optimization) Experience configuring and troubleshooting Spark jobs when interfacing with our storage infrastructure Experience optimizing, tuning and automating a large distributed compute environment Experience in configuration management (Spinnaker, Helm, Terraform, Crossplanes, Puppet or similar) Experience with alerting, monitoring and remediation automation in a large scale environment Fluent in at least one scripting or systems programming language (Python, Java, Go, etc.) Interest or knowledge in using public or private Kubernetes frameworks for scaling data and services infrastructure Job Description: You will be responsible for the data abstraction layer of our storage platform used for analytics and machine learning teams. To run our environment efficiently, we drive for proper monitoring, alerting and automation. The team's goal is to ensure the reliability and performance at the highest level. As a member of the AIML Data Virtualization Infrastructure Team: You will manage one of the largest ML infrastructure supporting hundreds of millions of Client's customers. You will diagnose, fix, improve, and automate complex issues across the entire stack to ensure maximum uptime and performance. You will take part in designing and building out our next generation ML Data Platform and push our services to the next level. You will advise other teams on proper integration of our platform. You will manage the core infrastructure which powers all Machine Learning, ETL, Analytics, and Privacy Efforts within AIML Education. BS, MS, or PhD degree in Computer Science or equivalent and 5+ years experience in data technologies.Thanks & Regards,Satnam SinghDirect: 201 623 3660Email : Satnam.singh@sparinfosys.com"
Senior Data Engineer,Tail Wind Informatics Corporation,"Minnesota, United States (Remote)",https://www.linkedin.com/jobs/view/3769564200/?eBP=JOB_SEARCH_ORGANIC&refId=jprbWfaxGWningdTClzsyA%3D%3D&trackingId=FuD9Rnc68p1THDN2UwHb8Q%3D%3D&trk=flagship3_search_srp_jobs,3769564200,"About the job
            
 
About Us: Tail Wind Informatics Corporation -Microsoft Solutions Partner- is a dynamic and innovative IT consulting services company that specializes in delivering Data Architecture and Business Intelligence solutions. We are currently seeking a talented and experienced Senior Data Engineer to join our team and play a crucial role in managing and optimizing data infrastructure. If you are passionate about data, have a strong background in Microsoft technologies, and enjoy solving complex data challenges, we want to hear from you!Position Overview: As a Senior Data Engineer at Tail Wind, you will be responsible for designing, developing, and maintaining data pipelines and systems on the Azure cloud platform. You will work closely with cross-functional teams to ensure that data is accurate, accessible, and supports data-driven decision-making. The ideal candidate has hands-on experience with Azure Data Factory, SQL, Stored Procedures, Python, Databricks, Snowflake, Synapse, and Power BI.Qualifications Bachelor's degree in Computer Science, Information Technology, or a related field (or equivalent work experience)5+ years of experience as a Data Engineer with a focus on Microsoft technologiesStrong proficiency in Azure Data Factory, or related product, for ETL processes and data orchestrationProficiency in SQL and T-SQL, including the ability to write complex queries and stored proceduresExperience with Python for data manipulation and automationKnowledge of Databricks for big data processing and analyticsFamiliarity with Snowflake or Azure Synapse Analytics for data warehousingProficiency in Power BI for data visualization and reportingKnowledgeable in methodologies of data warehouse designPersonal experience modeling and building data warehousesStrong problem-solving skills and attention to detailExcellent communication and teamwork abilities
Key Responsibilities Design, build, and maintain data pipelines and ETL processes using Azure Data FactoryDevelop and optimize SQL queries, stored procedures, and scripts for data transformation and extractionCollaborate with data scientists and analysts to understand data requirements and ensure data availabilityImplement data quality checks and data validation processes to ensure data accuracy and consistencyUtilize Databricks for advanced data processing, transformation, and analyticsLeverage Python for data manipulation, automation, and data integration tasksManage and optimize data storageBuild and maintain data warehouses and analytics solutionsCreate interactive reports and dashboards using Power BI for data visualization and insightsMonitor and troubleshoot data pipelines, addressing any issues in a timely mannerStay up-to-date with the latest Azure data technologies and best practices
Why join Tail Wind? Competitive salary and benefits packageOpportunity to work with cutting-edge technologies and solve challenging data problemsCollaborative and innovative work environmentProfessional development opportunities and support for various certifications
Application Process:To apply for this position, please click ""apply for this job"" at the top or bottom of the page, then upload and submit a current copy of your resume. One of the members of our Talent Acquisition team will reach out to schedule a phone interview if you meet the qualifications listed above.If you are a passionate Senior Data Engineer with a strong background in Azure Data Factory, SQL, Python, Databricks, Snowflake, Synapse, and Power BI, we encourage you to apply and be a part of our incredible team.Tail Wind Informatics Corp., is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees. Salary range can vary depending on experience and will be determined based on the results of a Technical Interview"
Workplace Analytics Engineer,Canonical,"San Francisco, CA (Remote)",https://www.linkedin.com/jobs/view/3738246455/?eBP=JOB_SEARCH_ORGANIC&refId=FnulRbCiFZIyO9RCHEEslQ%3D%3D&trackingId=jeWxyYcrFckMtPJPnudZYw%3D%3D&trk=flagship3_search_srp_jobs,3738246455,"About the job
            
 
Bring your people analytics, social science research and data mining/science skills to a unique team seeking to understand, and shape, the future of the digital workplace. We are interested in technology, of course, but we are also interested in the human mission of enabling the world's brightest and hardest working people to live where they want and work from anywhere. Most of our colleagues could move to a tech hub but they choose Canonical because of our mission and our approach to the workplace.We'd like to understand what really makes a distributed, remote-first workplace work. We think we're pretty good at this (being remote first for almost 20 years), but we know there is a lot still to understand, and the frontier of possibility continues to move outward. We'd like to invest in research, analytics and tooling which raises the bar even further for remote collaboration and organisation.If we are able to build tools that meaningfully improve our cooperation and our satisfaction, then we intend to share our insights with the world, both as a narrative and as SAAS or open source that helps other companies follow in our footsteps.The role of a Workplace Analytics Engineer at CanonicalSupport analytics and data mining in a cross-disciplinary team of organisational psychologists, web front end engineers, back end engineers and statistics / analytics experts to help us build a new definition for the 21st century digital workplace. Collaborate to figure out what really drives productivity, effectiveness and happiness in a remote-first globally distributed company.In addition to your existing people analytics work experience, this role will combine your skills in psychology, data analytics and visualisation, to help create a more effective workplace.Location: This role will be based remotely in the AMER region.All applicants applying must be legally authorized to work in the United States, as we cannot offer visa sponsorship for this job position.What your day will look like Utilize advanced data analytics to understand how we hire and how we work (productivity, happiness and effectiveness) across a global, remote first organisationFocus on quantitative and qualitative data analytics to find insights and meaningful business outcomesTell the story from the insights through dashboards, visualizations and presentationsDesign and conduct research into trends shaping talent science and remote workCollaborate with stakeholder teams (ex., engineering, information systems, etc) to improve the data and tool ecosystem supporting our digital workplace
What we are looking for in you Background in data science, mathematics, actuarial science, or engineeringFirst work experience in People Analytics Knowledge in advanced statistics, data sciences, coding/scripting languages (Python, R, etc), and databases (SQL, etc)Strength in data analytics and visualization (Looker Studio, Tableau, etc)Ability to translate business questions to key research objectivesAbility to identify the best methodology to execute research, synthesize and analyse findingsExcellent writing and communication skillsWillingness to examine the status quo and resilient in the face of challenges
What we offer youYour base pay will depend on various factors including your geographical location, level of experience, knowledge and skills. In addition to the benefits above, certain roles are also eligible for additional benefits and rewards including annual bonuses and sales incentives based on revenue or utilization. Our compensation philosophy is to ensure equity right across our global workforce.In addition to a competitive base pay, we provide all team members with additional benefits, which reflect our values and ideals. Please note that additional benefits may apply depending on the work location and, for more information on these, you can ask in the later stages of the recruitment process.🏠 Fully remote working environment - we've been working remotely since 2004!📚 Personal learning and development budget of 2,000USD per annum💰 Annual compensation review🏆 Recognition rewards🏝 Annual holiday leave👶 Parental Leave🧑‍💼 Employee Assistance Programme🧳 Opportunity to travel to new locations to meet colleagues at 'sprints'✈️ Priority Pass for travel and travel upgrades for long haul company eventsAbout CanonicalCanonical is a pioneering tech firm that is at the forefront of the global move to open source. As the company that publishes Ubuntu, one of the most important open source projects and the platform for AI, IoT and the cloud, we are changing the world on a daily basis. We recruit on a global basis and set a very high standard for people joining the company. We expect excellence - in order to succeed, we need to be the best at what we do.Canonical has been a remote-first company since its inception in 2004. Work at Canonical is a step into the future, and will challenge you to think differently, work smarter, learn new skills, and raise your game. Canonical provides a unique window into the world of 21st-century digital business.Canonical is an equal opportunity employerWe are proud to foster a workplace free from discrimination. Diversity of experience, perspectives, and background create a better work environment and better products. Whatever your identity, we will give your application fair consideration."
Senior Data Engineer,Act Digital Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3643167692/?eBP=JOB_SEARCH_ORGANIC&refId=FnulRbCiFZIyO9RCHEEslQ%3D%3D&trackingId=TS9myRP0zdjmeVSkFc6YmA%3D%3D&trk=flagship3_search_srp_jobs,3643167692,"About the job
            
 
Our client is looking for a Senior Data Engineer who will be responsible for designing and building scalable and robust data systems. The ideal candidate will be an expert in data engineering technologies, have a strong understanding of data architecture, and be able to work with large and complex data sets. The Senior Data Engineer will also work closely with the data science and analytics teams to ensure data integrity and develop data pipelines.Duties & ResponsibilitiesDesign and build large scale data pipelines to process and analyze large volumes of data.Build and maintain efficient data infrastructure, ensuring data quality and consistency.Work with the data science and analytics teams to ensure data accuracy and completeness.Collaborate with other engineers to build scalable and maintainable data systems.Develop and implement data governance and security policies.Continuously evaluate and improve data processes to optimize system performance.Keep up to date with the latest data engineering technologies and trends.Technical SkillsExpertise in SQL and NoSQL databases, data warehousing, and data modeling.Fluency in Python and SQL. Additional data or system languages (e.g. Java, Scala, Go, R) a plus.Experience using data pipeline frameworks such as Apache Beam or Apache Spark at scale.Experience using data orchestration / automation frameworks such as Airflow, Databricks and MLFlow.Hands on experience with one or more of the major cloud providers (GCP, AWS, Azure). Experience with infrastructure-as-code (e.g. Cloud Formation, Terraform) a plus.Experience with data visualization tools such as Tableau or Power BI.Minimum QualificationsBachelor's degree in Computer Science, Engineering, or a related field.5+ years of experience in data engineering or related field.Excellent problem-solving skills and ability to work independently.Strong communication and collaboration skills.Preferred QualificationsMaster's degree in Computer Science, Engineering, or a related field.Experience in machine learning or data science.All positions with our client require an applicant who has accepted an offer to undergo a background check. The specific checks are based on the nature of the position. Background checks may include some or all of the following: SSN/SIN validation, education verification, employment verification, and criminal check, search against global sanctions and government watch lists, fingerprint verification, credit check, and/or drug test. By applying for a position with Press Ganey, you understand that you will be required to undergo a background check should you be made an offer. You also understand that the offer is contingent upon successful completion of the background check and results consistent with Client's employment policies. You will be notified during the hiring process which checks are required for the position.Our client is an Equal Employment Opportunity/Affirmative Action employer and well committed to a diverse workforce. We do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, veteran status, and basis of disability or any other federal, state or local protected class.Pay Transparency Non-Discrimination Notice Client will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor's legal duty to furnish information."
Data / Software Engineer - Remote,"Tygart Technology, Inc.","Clarksburg, WV (Remote)",https://www.linkedin.com/jobs/view/3645644854/?eBP=JOB_SEARCH_ORGANIC&refId=FnulRbCiFZIyO9RCHEEslQ%3D%3D&trackingId=K%2F5mzh%2BlSh42H%2FmYT4U3sA%3D%3D&trk=flagship3_search_srp_jobs,3645644854,"About the job
            
 
Tygart is currently seeking a Data Engineer to support the data integration via applications or adaptors using XML data exchange specifications. The candidate will be part of a team, collaborating closely with the government staff and external criminal justice agencies, to enable data sharing and management, and establish connectivity with a national data repository. The ideal candidate will have practical experience in the areas of information sharing, Extensible Markup Language (XML), relational data bases, N-DEx IEPDs, Global Justice XML Data Model (GJXDM) and the (National Information Exchange Model) NEIM. This position requires that the candidate have a current Secret security clearanceResponsibilities Include Provide engineering support to facilitate inter-agency information sharingProduce data integration applications or adaptors from various sources to the N-DEx XML data exchange specificationsMapping data to Information Exchange Package Documentation (IEPD) standards, developing transformation code, validating and verifying data processes, and troubleshooting and making recommendations both to internal and external stakeholdersProcess all new agency data and adapters and maintain existing agency data and adapters
QualificationsThe ideal candidate will have the following: Bachelor's degree in Engineering, Information Technology, Computer Science, or related field.Three (3) to five (5) years’ experience in software development, and management and support of information technology systems.Two of more years’ experience in XML, relational data bases, and web services is desired.Three (3) years’ Java development experienceProficiency with Oracle databases and procedural Language/Structured Query Language (PL/SQL), Microsoft SQL server databases, subversion in XML and XML Schema Definition (XSD).Proficiency in XML Stylesheet Language Transformations (XSLT) generation.Experience in managing and troubleshooting secure file transfer protocols (SFTP) and web services for data submissions and user access.Experience using Python is a plusMinimum three (3) years’ experience in the analysis and assessment of large data sets; managing and coordinating major parallel IT initiatives; and extracting data for analysis and consumption via appropriate form such as diagrams, reports, or tables.Experience with the Logical Entity Exchange Specification (LEXS) Publication and Discovery (PD) and LEXS Search and Retrieve (SR), N-DEx IEPDs, GJXDM and NEIM is highly desire.Must be able to well under pressure, and possess excellent oral and written communication skills, as well as excellent organizational skills. 
Tygart Technology, Inc. is a premier professional services and software development organization providing a broad range of Information Technology (IT) services to public and commercial sector customers. Founded in 1992, Tygart's customer first mind set and agile development methodology have led to our continued success. Tygart supports the Department of Defense, Federal Bureau of Investigation, Intelligence Community, Federal Election Commission, Pension Benefit Guaranty Corporation, and various other Federal and State organizations and maintains offices in West Virginia and Northern Virginia.Tygart Technology, Inc. is an equal opportunity/affirmative action employer and considers qualified applicants for employment without regard to race, gender, age, color, religion, disability, veterans status, sexual orientation, or any other protected factor.For information on Tygart and our career openings, please visit http://www.tygart.com/careers/."
Lead Data/ML Engineer - REMOTE,Sierra Solutions,United States (Remote),https://www.linkedin.com/jobs/view/3749313253/?eBP=JOB_SEARCH_ORGANIC&refId=FnulRbCiFZIyO9RCHEEslQ%3D%3D&trackingId=89GW53SNrEY%2BsNDxxiUJVg%3D%3D&trk=flagship3_search_srp_jobs,3749313253,"About the job
            
 
Job SummaryWe are looking for a Data/Machine Learning Engineer for our technology client.  The ideal candidate will be passionate about building large scale AL/Client systems, building data pipelines and working with modern technology to help manage the ever-growing data needs of clients. Primary Responsibilities Partner with teammates to create complex data and Client processing pipelines in order to solve challengesCollaborate with Data Scientists in order to design scalable implementations of their modelsHelp to design and develop end-to-end MLOps/CD4ML practices as part of a diverse teamCommunicate with MLOps and stakeholders on project insights and enable the team to make informed decisionsPair to write clean and iterative code using TDDLeverage various continuous delivery practices to deploy, support and operate data pipelinesAdvise and educate clients on how to use different distributed storage and computing technologies from the plethora of options availableDevelop and operate modern data architecture approaches to meet key business objectives and provide end-to-end data solutionsCreate data models and speak to the tradeoffs of different modeling approachesSeamlessly incorporate data quality into your day-to-day work as well as into the delivery processAssure effective collaboration between colleagues and the client's teams, encouraging open communication and advocating for shared outcomes
 Education and Experience You are equally happy coding and leading a team to implement a solutionPossess a track record in Data Engineering or Machine Learning EngineeringExperience with popular machine learning packages like scikit-learn, tensor flow and pytorchExperience with machine learning techniques and knowledge of statistical modelsYou have a deep understanding of data modeling and experience with data engineering tools and platforms such as Kafka, Spark, and HadoopExperienced in deploying and operating machine learning systems in production, including in cloud environmentsUnderstanding of data infrastructure and operations needs for machine learning, including automation and how to operate them on premise and in cloud environmentsAbility to assess and speak to the tradeoffs of using different approaches to Model Serving, Model Deployment, and Model Observability and the tools associated with themYou have built large-scale data pipelines and data-centric applications using any of the distributed storage platforms such as HDFS, S3, ADLS, NoSQL databases (Hbase, Cassandra, etc.) and any of the distributed processing platforms like Hadoop, Spark, Hive, Oozie, and Airflow in a production settingUnderstand and design suitable end-to-end CD4ML practices, depending on project settings.Hands on experience in Databricks, Cloudera, Hortonworks and/or cloud (AWS EMR, Azure HDInsights, Qubole etc.) based Hadoop distributionsYou are comfortable taking data-driven approaches and applying data security strategy to solve business problemsYou're genuinely excited about data infrastructure and operations with a familiarity working in cloud environmentsExperience in Big data architecture build and operate data pipelines, and maintain data storage, all within distributed systems"
Customer Data Platform Engineer - California – Remote - Contract Position,Lorven Technologies Inc.,United States (Remote),https://www.linkedin.com/jobs/view/3645145665/?eBP=JOB_SEARCH_ORGANIC&refId=FnulRbCiFZIyO9RCHEEslQ%3D%3D&trackingId=cIuT50Jl5hkUqRA58iRL%2Bg%3D%3D&trk=flagship3_search_srp_jobs,3645145665,"About the job
            
 
Role: Customer Data Platform EngineerLocation: California RemoteProject duration 3 months to long termExperience: 12+ Years MinimumMandatory Skill: AEP- Adobe Experience PlatformJob Description  We are looking for a CDP Engineer/Developer to join our team. Understand Adobe Experience PlatformPlan, design, and build XDM schemas in AEP to host imported data. Focus on Data collection Strategy, Real-Time CDP, Journey Orchestration, Personalization, Customer Journey Analytics and external reporting dashboard solutionsDesign and advocate solutions using modern cloud technologies, design principles, integration points, and automation methods.The AEP Engineer creates quality software and data structures that meet the functional and non-functional project requirements in the implementation, enhancement, and support of marketing projects.This includes producing application code on-time, on-budget, and in compliance with company implementation standards & practices as well as general industry & platformLead, design, develop, and deliver large-scale data systems, data processing, and data transformation projects.

NoteTHESE EMAILS ARE GENERATED BY KEYWORD AND I APOLOGIZE IF THESE SKILLS SETS DO NOT MATCH YOUR EXPERTISE, OR IF THE LOCATION IS OUT OF RANGE.We do have other opportunities available. If you are interested, please send me your latest resume. If you are not currently seeking employment, or if you would prefer, I contact you at some later date, please indicate your date of availability so that I may honor your request."
Data Engineer with Mortgage Exp_Remote (Only W2 Contract) - Remote | WFH,Get It Recruit - Real Estate,"McLean, VA (Remote)",https://www.linkedin.com/jobs/view/3755005401/?eBP=JOB_SEARCH_ORGANIC&refId=FnulRbCiFZIyO9RCHEEslQ%3D%3D&trackingId=qkCL3h2ol%2Fq7xSuSHOhASQ%3D%3D&trk=flagship3_search_srp_jobs,3755005401,"About the job
            
 
*Are you passionate about driving innovation in the Mortgage Technology industry? Join our dynamic team as we embark on a journey to transition our Mortgage Loan Servicing platform! We are seeking Data Engineers to play a pivotal role in this exciting transformation.*Job Responsibilities  Collaborate with our Mortgage Technology leadership team to facilitate the transition from our current in-house Mortgage Loan Servicing platform to a new host system. Leverage your expertise in data engineering to ensure a smooth migration to the future system. While experience with Loan Servicing software is a plus, it's not a requirement. We value your enthusiasm and willingness to learn.
Required Skills  Proficiency in Kafka Data Streaming (open source tool). Strong knowledge of T-SQL/SQL. Experience with DataStage or Informatica. Familiarity with ETL development, especially in Snowflake. AWS experience, including Redshift or other data-related modules.
This is an exciting opportunity to be a part of a forward-thinking team in the Mortgage Technology sector. We value diversity and are committed to creating an inclusive work environment.How To ApplyIf you're ready to be a part of our team, please submit your application and resume. We look forward to hearing from you!*Best Regards,**Your Name - Technical Recruitment Team**Company Address: [Company Address]*Contact Information  Direct: [Your Contact Information] Email: [Your Email Address] Website: [Company Website]
About UsWe are a global Business and IT solutions provider headquartered in [Location], dedicated to assisting clients worldwide in their application development, integration, conversion, consolidation, and support efforts. We are also committed to partnering with emerging growth enterprise software companies to bring the best technology solutions to market. Our team is known for its quality and dedication to our partners' goals, and we offer open, flexible, and easily extendible solutions to clients around the world.Employment Type: Full-Time"
Lead Data Engineer - Data Interoperability,b.well Connected Health,United States (Remote),https://www.linkedin.com/jobs/view/3778760006/?eBP=JOB_SEARCH_ORGANIC&refId=FnulRbCiFZIyO9RCHEEslQ%3D%3D&trackingId=s6HpssDzh3AKLP1Vevd5Kg%3D%3D&trk=flagship3_search_srp_jobs,3778760006,"About the job
            
 
Lead Data Engineer - Data Interoperability TeamAs a Lead Data Engineer, you will be a critical member of the Data Interoperability team responsible for building and maintaining data pipelines and data infrastructure that connects to thousands of data sources around the country to bring together a person’s health record in one place.b.well Connected Health has the largest set of connected health data for any person in the United States. By bringing a person’s health data into one place, we are able to help everyone get convenient and affordable health care.This position is available for fully remote work.What You'll Do Design, build, and maintain b.well’s data pipeline infrastructure using Python, Spark, Prefect, Kubernetes and other modern technologiesLead a team of data engineers to build data pipelines and infrastructure that connects to thousands of data sources around the country including health providers, insurance companies, pharmacies and labs.Launch new projects from ideation to completionHelp lead other developers to improve their career development and coding abilitiesYou will safeguard sensitive data by following policies and training concerning your security and privacy responsibilities
Job Requirements 7+ years of professional programming experience (must include Python)2+ years building microservices in PythonExceptional and demonstrable data engineering experienceExperience in loading, validating, cleaning, and manipulating data filesStrong experience with unit testing and test-driven developmentStrong experience with relational and/or NoSQL databasesStrong experience with cloud-based infrastructureComfort with Linux/Unix command line
Great To Have 7+ years of Advance Python experience5+ years of data pipeline engineering experience1+ years of experience with SparkExperience with Airflow or PrefectExperience with DockerExperience with streaming dataExperience scaling technology solutions to hundreds of thousands active usersExperience mentoring other developersDeep understanding of common API methodologiesStartup experience
Blow Us Away Experience working with third-party healthcare APIs, HL7, data streams, and/or flat filesExperience in cybersecurityExperience with HIPAA, HITECH, and HITRUSTAn active GitHub profile or other public code portfolioActive Stack Overflow profileDocumented work on open source projects
Data shows that women, people of color, and other underrepresented groups may be less likely to apply for jobs unless they believe they are a perfect match. But b.well holds diversity amongst its key values, and we have a strong commitment to building our workforce and products through that lens.You don't have to check every box in this job description to be a great fit for the role! If you're excited about this position and the prospect of working for b.well, please apply. If it turns out this role isn't for you, there may be other openings that could align with your experience and expertise!We are committed to an inclusive and diverse b.well. We are an equal opportunity employer. We do not discriminate based on race, ethnicity, color, ancestry, national origin, religion, sex, sexual orientation, gender identity, age, disability, veteran, genetic information, marital status or any other legally protected status."
Azure Data engineer with Azure data factory and pyspark-- EST/CST-no corps --remote,RemoteWorker US,"Philadelphia, PA (Remote)",https://www.linkedin.com/jobs/view/3780836787/?eBP=JOB_SEARCH_ORGANIC&refId=FnulRbCiFZIyO9RCHEEslQ%3D%3D&trackingId=D8dSg6aYd8KmqPuQkM%2Fmow%3D%3D&trk=flagship3_search_srp_jobs,3780836787,"About the job
            
 
NOTICE- Any pay ranges displayed are estimations. Actual pay is determined by an applicant's experience, technical expertise, and other qualifications as listed in the job description. All qualified applicants are welcome to apply. Senior Associate should have hand on PySpark, ADF, ADLS, Delta tables, SparkSQL, etc. Architects, in addition to the above must have Architecture experience designing solutions and frameworks ground up and not to just build data pipelines using the existing setup Top Skill Sets/ Experiences Required: These requirements must be strong and reflected on the resume: Azure Data Factory Azure Pyspark, Databricks APIs Your Skills & Experience: Demonstrable experience in enterprise level data platforms involving implementation of end to end data pipelinesGood communication and willingness to work as a teamHands-on experience with at least one of the leading public cloud data platforms (Amazon Web Services, Azure or Google Cloud)Experience with column-oriented database technologies (i.e. Big Query, Redshift, Vertica), NoSQL database technologies (i.e. DynamoDB, BigTable, Cosmos DB, etc.) and traditional database systems (i.e. SQL Server, Oracle, MySQL)Experience in architecting data pipelines and solutions for both streaming and batch integrations using tools/frameworks like Glue ETL, Lambda, Google Cloud DataFlow, Azure Data Factory, Spark, Spark Streaming, etc.Ability to handle multiple responsibilities simultaneously in leadership and contributing to tasks “hands-on”Understanding of data modeling, warehouse design and fact/dimension concepts Thanks Renu Goel 857-207-2676 renu.goel@yoh.com"
"Data Engineer WITH INSURANCE BACKGROUND, Remote",Stellent IT,United States (Remote),https://www.linkedin.com/jobs/view/3608073285/?eBP=JOB_SEARCH_ORGANIC&refId=FnulRbCiFZIyO9RCHEEslQ%3D%3D&trackingId=B7vclrNC0vmRkx62hp27Sw%3D%3D&trk=flagship3_search_srp_jobs,3608073285,"About the job
            
 
Data Engineer WITH INSURANCERemote (100%)Phone+skypeJob DescriptionUnderstanding data in its context/insurance world sales/insurance/broker data for exampleIdeal candidate 2-3 years of insurance p&c side/insurance5-10 years of data engineeringPython and RAzure Databricks needed (platform being used)DAX or AAS is not required, but helpfulETLWill take sharp young talent who knows insurance and data engineering."
Data Engineer,Stripe,United States (Remote),https://www.linkedin.com/jobs/view/3769105801/?eBP=JOB_SEARCH_ORGANIC&refId=FnulRbCiFZIyO9RCHEEslQ%3D%3D&trackingId=jSphwOVrWycFd4hmJxBxFA%3D%3D&trk=flagship3_search_srp_jobs,3769105801,"About the job
            
 
Who we areAbout StripeStripe is a financial infrastructure platform for businesses. Millions of companies—from the world’s largest enterprises to the most ambitious startups—use Stripe to accept payments, grow their revenue, and accelerate new business opportunities. Our mission is to increase the GDP of the internet, and we have a staggering amount of work ahead. That means you have an unprecedented opportunity to put the global economy within everyone’s reach while doing the most important work of your career.About The TeamThe Data Science team builds data and intelligence into our product, sales, and operations. This spans across building data foundations and applying statistical techniques and machine learning to measure and optimize our product, build data-driven products, and conduct in-depth analysis to inform strategic decisions.What you’ll doWe’re looking for people with a strong background in data engineering and analytics to help us scale while maintaining correct and complete data. You’ll be working with a variety of internal teams -- Engineering, Business -- to help them solve their data needs. Your work will provide teams with visibility into how Stripe’s products are being used and how we can better serve our customers.Responsibilities You’ll be working with a variety of internal teams -- Engineering, Business -- to help them solve their data needsYour work will provide teams with visibility into how Stripe’s products are being used and how we can better serve our customersIdentify data needs for business and product teams, understand their specific requirements for metrics and analysis, and build efficient and scalable data pipelines to enable data-driven decisions across StripeDesign, develop, and own data pipelines and models that power internal analytics for product and business teamsHelp the Data Science team apply and generalize statistical and econometric models on large datasetsDrive the collection of new data and the refinement of existing data sources, develop relationships with production engineering teams to manage our data structures as the Stripe product evolvesDevelop strong subject matter expertise and manage the SLAs for those data pipelines
Who you areIf you are data curious, excited about designing data pipelines, and motivated by having an impact on the business, we want to hear from you.Minimum Requirements Have a strong engineering background and are interested in data5+ years of experience with writing and debugging data pipelines using a distributed data framework (Hadoop/Spark/Pig etc…)Have an inquisitive nature in diving into data inconsistencies to pinpoint issuesStrong coding skills in Scala, Python, Java or another language for building performance data pipelines.Strong understanding and practical experience with systems such as Hadoop, Spark, Presto, Iceberg, and AirflowThe ability to communicate cross-functionally with solid stakeholder management to derive requirements and architect scalable solutions."
"Specialist Data Engineer (Tampa, FL or East Coast Remote",BioSpace,"Washington, DC (Remote)",https://www.linkedin.com/jobs/view/3769457083/?eBP=JOB_SEARCH_ORGANIC&refId=FnulRbCiFZIyO9RCHEEslQ%3D%3D&trackingId=kd%2FrsA5Z66MX5IVcJ7j1Sg%3D%3D&trk=flagship3_search_srp_jobs,3769457083,"About the job
            
 
HOW MIGHT YOU DEFY IMAGINATION?Youve worked hard to become the professional you are today and are now ready to take the next step in your career. How will you put your skills, experience and passion to work toward your goals? At Amgen, our shared missionto serve patientsdrives all that we do. It is key to our becoming one of the worlds leading biotechnology companies, reaching over 10 million patients worldwide. Come do your best work alongside other innovative, driven professionals in this meaningful role.Specialist Data Engineer (Tampa, FL or East Coast Remote)LiveWhat You Will DoLets do this. Lets change the world. In this vital role you will be a key contributor to our mission of leveraging data and technology to transform the pharmaceutical industry. You will work closely with cross-functional teams to design, develop, and maintain data solutions that enable AI-driven capabilities such as chatbots and automation for telephony systems, optimizing interactions with healthcare professionals (HCP) and patients.Responsibilities Utilize your extensive data engineering experience to design, develop, and maintain robust data pipelines, ensuring data availability and reliability for AI-driven solutions.Leverage your hands-on experience with technologies such as Google Dialogflow, BigQuery database, and related tools to develop and integrate AI capabilities into our systems.Own the development and improvement of chatbot solutions for HCP and patient interactions, optimizing communication channels and improving user experiences.Drive the automation of telephony systems to streamline and improve communication processes, ensuring efficient and effective interactions.Collaborate with cross-functional teams to improve call center capabilities, utilizing AI-driven solutions to improve response times and service quality.Work on integrating data from various sources, ensuring data consistency, accuracy, and security!Continuously monitor and optimize data pipelines and AI solutions. Maintain clear and comprehensive documentation of data pipelines, processes, and system configurations. Generate regular reports and provide insights to partners.Collaborate effectively with data scientists, software developers, business analysts to ensure successful project outcomes.
WinWhat We Expect Of YouWe are all different, yet we all use our unique contributions to serve patients. The professional we seek will have these qualifications.Basic QualificationsDoctorate degreeORMasters degree and 3 years of data engineering experienceOrBachelors degree and 5 years of data engineering experienceOrAssociates degree and 10 years of data engineering experienceOrHigh school diploma / GED and 12 years of data engineering experiencePreferred Qualifications Hands-on experience in data engineering with a strong focus on pharmaceutical industry experience.Proficiency in technologies such as Google Dialogflow and BigQuery database.Experience with the following AWS cloud, including Lambda function, DynamoDB, RDS, API Gateway, Docker container on AWS ECS/EKS, Google Cloud, including Cloud Function, Dialogflow, App Engine, Cloud Builda, Kubernete environmentExperience in developing and deploying chatbots and automation solutions.Solid understanding of AI and machine learning concepts.Excellent problem-solving skills and attention to detail.Strong communication and collaboration skills.Ability to work independently and as part of a team.Knowledge of healthcare data compliance regulations (HIPAA, GDPR) and SAFe methodology is a plus.
ThriveWhat You Can Expect Of UsAs we work to develop treatments that take care of others, we also work to care for our teammates professional and personal growth and well-being.Amgen Offers a Total Rewards Plan Comprising Health And Welfare Plans For Staff And Eligible Dependents, Financial Plans With Opportunities To Save Towards Retirement Or Other Goals, Work/life Balance, And Career Development Opportunities Including Comprehensive employee benefits package, including a Retirement and Savings Plan with generous company contributions, group medical, dental and vision coverage, life and disability insurance, and flexible spending accounts. A discretionary annual bonus program, or for field sales representatives, a sales-based incentive planStock-based long-term incentives Award-winning time-off plans and bi-annual company-wide shutdowns Flexible work models, including remote work arrangements, where possible
Apply nowfor a career that defies imaginationObjects in your future are closer than they appear. Join us.careers.amgen.comAmgen is an Equal Opportunity employer and will consider you without regard to your race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status.We will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation."
Senior Data Engineer,Jenni Kayne,"Los Angeles, CA (Remote)",https://www.linkedin.com/jobs/view/3747934973/?eBP=JOB_SEARCH_ORGANIC&refId=FnulRbCiFZIyO9RCHEEslQ%3D%3D&trackingId=cOBJdiEUxE4yAS8fGzE6fw%3D%3D&trk=flagship3_search_srp_jobs,3747934973,"About the job
            
 
Jenni Kayne is a California-based lifestyle brand that aims to empower an elevated approach to everyday living. Whether it's our edited style ethos or coveted interiors sensibility, we work hard to create a world that's inviting and intentional. From our stores across the country to our operations and corporate teams, we believe in the power of a workplace that's built on diversity and inclusion—where the varied voices and viewpoints of our community pave the way.About This Role:Jenni Kayne is looking for a Sr. Data with expertise in design, development, test and deployment of large Lake House (Data Lake and enterprise data warehouse) data solutions using cloud technologies and modern data stack. This is an exciting role for individuals looking for an entrepreneurial environment with clear ownership and opportunity to make direct business impact.Role and Responsibilities:As the Senior Data Engineer, your primary responsibilities include the following: Building data pipelines: Create, maintain, and optimize workloads from development to production for specific business use cases. Responsible for using innovative and modern tools, techniques and architectures to drive automation of most-common, repeatable data preparation and integration tasks with goal of minimizing reducing defects and improving productivity. Responsible of data engineering architecture and framework to seamlessly integrate different business application and data sources with different formats (API, XML, JSON , CSV etc.)Create strategy for master data for customer, product by unifying disparate source of customer and product across digital and offline channels (retail)Develop and follow data integration and data quality standards across all development initiatives according to the organization's policies as well as best practices. Assist in data management infrastructure, governance data observability, integration with metadata management tools and techniques (TBD in future). Continuously tracking data consumption in collaboration with Analytics and Data sciences teams to prioritize the highest impact projects. Triage data issues, analyzing end to end data pipelines and working with data analyst, business users in troubleshooting and resolving data quality or pipeline issues. Work in agile model alongside data architect, data analysts, data scientist, business partners and other developers in delivery of dataBuild and continuously manage the data lake and enterprise data-warehouse pipelines and shared transformation libraries for code reusability, speed to market and lineage. 
Qualifications: Requires a bachelor's degree or equivalent experience. Requires at least 6 years of prior relevant experience. Hands on experience with programming languages including SQL, Python on cloud data platforms like Snowflake, Redshift etc. Strong technical understanding of data modeling (dimensional model), master data management, data integration, data architecture, data warehousing and data quality techniquesWorking knowledge of Git repositories (bitbucket, GitHub), CI/CD (Jenkins etc.) and software development tools, including incident tracking, version control, release management, subversion change management (Atlassian toolset – Jira/Confluence), testing tools and systems and scheduling software (Airflow)Experience working with popular BI software tools like Looker, Tableau, Qlik, PowerBI etc. Nice to have: Experience with enterprise ELT platforms like Talend, Fivetran and flexibility to build an in-house transformation code base using SQL, Python, Airflow etc. Basic experience in working with data governance and data security and specifically information stewards and privacy and security officers in moving data pipelines into production with appropriate data quality, governance and security standards and certification. Adept in agile methodologies and capable of applying DevOps and increasingly Data Operations principles to data pipelines to improve the integration, reuse and automation of data flows to improve data trust and democratization. 
Physical Requirements: Prolonged periods sitting at a desk and working on a computerMust be able to move and lift heavy objects (15 pounds or more) from time to time as required
Additional Notes:This job description is not all inclusive. In addition, Kayne, LLC DBA Jenni Kayne reserves the right to amend this job description at any time. Kayne, LLC DBA Jenni Kayne is committed to a diverse and inclusive work environment.The annual base salary range for this position is $120,000 - $165,000. The base salary is determined by experience, education, skills, and location."
Azure Data engineer with Azure data factory and pyspark-- EST/CST-no corps --remote,RemoteWorker US,"Tampa, FL (Remote)",https://www.linkedin.com/jobs/view/3780838620/?eBP=JOB_SEARCH_ORGANIC&refId=FnulRbCiFZIyO9RCHEEslQ%3D%3D&trackingId=cWUUpJ5XQtC3LUwp26LXMQ%3D%3D&trk=flagship3_search_srp_jobs,3780838620,"About the job
            
 
NOTICE- Any pay ranges displayed are estimations. Actual pay is determined by an applicant's experience, technical expertise, and other qualifications as listed in the job description. All qualified applicants are welcome to apply. Senior Associate should have hand on PySpark, ADF, ADLS, Delta tables, SparkSQL, etc. Architects, in addition to the above must have Architecture experience designing solutions and frameworks ground up and not to just build data pipelines using the existing setup Top Skill Sets/ Experiences Required: These requirements must be strong and reflected on the resume: Azure Data Factory Azure Pyspark, Databricks APIs Your Skills & Experience: Demonstrable experience in enterprise level data platforms involving implementation of end to end data pipelinesGood communication and willingness to work as a teamHands-on experience with at least one of the leading public cloud data platforms (Amazon Web Services, Azure or Google Cloud)Experience with column-oriented database technologies (i.e. Big Query, Redshift, Vertica), NoSQL database technologies (i.e. DynamoDB, BigTable, Cosmos DB, etc.) and traditional database systems (i.e. SQL Server, Oracle, MySQL)Experience in architecting data pipelines and solutions for both streaming and batch integrations using tools/frameworks like Glue ETL, Lambda, Google Cloud DataFlow, Azure Data Factory, Spark, Spark Streaming, etc.Ability to handle multiple responsibilities simultaneously in leadership and contributing to tasks “hands-on”Understanding of data modeling, warehouse design and fact/dimension concepts Thanks Renu Goel 857-207-2676 renu.goel@yoh.com"
Data Engineer,Homecare Homebase,"Dallas, TX (Remote)",https://www.linkedin.com/jobs/view/3774815039/?eBP=JOB_SEARCH_ORGANIC&refId=FnulRbCiFZIyO9RCHEEslQ%3D%3D&trackingId=YBwA6%2FqGwXMiBKHo2%2BVsYg%3D%3D&trk=flagship3_search_srp_jobs,3774815039,"About the job
            
 
Job DescriptionThe Data Engineer will focus on building data pipelines to support our data ingesting, cleansing, enriching, and presentation efforts in support of our flagship SaaS applications.Responsibilities Build data pipelines to support our data ingesting, cleansing, enriching, and presentation efforts in support of our flagship SaaS applicationsCollaborate with our data warehouse, platform, and product teams to build scalable, performant data pipelines.Create and extend data models based on changing source data and business requirements.Maintain and optimize existing pipelines.Automate testing, build, and deployment of data pipelines.Participate in code reviews.Investigate and diagnose root cause for data warehouse operational issues.Function as a technical resource for team members and internal users.
Desired Experience and Skills:  2+ years of experience with Python, common packages, and tools.Ability to write solid unit tests.Familiarity with object-oriented concepts and TDD.3+ years relational database experience and advanced SQL writing and optimization skills.Experience with revision control (git), code reviews, iterative/incremental development processes.Experience with batch and streaming ETL/ELT.Experience with Snowflake (or similar cloud database) and cloud warehouse data modeling.Experience/familiarity with AWS services (serverless, S3, SNS, SQS).Strong troubleshooting and analytical skills.Comfortable working with bash/zsh and shell scripting.
Additional Experience And Skills Experience with CI/CD tools (Jenkins, Docker) and build/deploy automation.Experience with Airflow pipeline development and AWS MWAA.Experience with AWS Analytics services.Familiarity with DataDog, Sentry, Prometheus.
Behavioural Competencies: Planning and Organization: Able to establish and execute a course of action to accomplish specific short- and long-term goals, to plan appropriate allocation of resources and time, and to approach all responsibilities in an organized manner. Driven to Achieve. Able to manage self to drive performance by balancing efforts across the team to exceed targets. Initiative: Actively influence events to achieve goals. Acts beyond what is called for; generates and/or recognizes imaginative, creative solutions. Adaptability/Flexibility: Able to be effective in many situations and to develop and accept new methods of doing things. Able to react quickly and to successfully implement changes as needed. Curiosity: Inquisitive with a thirst for knowledge and desire to learn about the market and all aspects of the business"
Actuarial Data Engineer -- Remote | WFH,Get It Recruit - Finance,"Plymouth, MN (Remote)",https://www.linkedin.com/jobs/view/3774135291/?eBP=JOB_SEARCH_ORGANIC&refId=FnulRbCiFZIyO9RCHEEslQ%3D%3D&trackingId=1c%2BVPrKhsu1gWAh4SD2Lxw%3D%3D&trk=flagship3_search_srp_jobs,3774135291,"About the job
            
 
Are you ready to join a global organization dedicated to improving the health and well-being of millions of people around the world? At our company, we believe in the power of technology to connect individuals with the care, pharmacy benefits, data, and resources they need to live their best lives. We are committed to fostering a culture of diversity and inclusion, offering talented peers, comprehensive benefits, and numerous opportunities for career development. Join us in making a positive impact on the communities we serve as we strive to advance global health equity. It's an opportunity to care, connect, and grow together.About The RoleAs a Data Engineer in our healthcare analytics team, you will play a crucial role in harnessing detailed healthcare claims data and various data sources to develop and implement data solutions. Your work will support actuarial and predictive modeling as well as data science solutions within the Medicare, Medicaid, and Commercial lines of business. Your responsibilities will include designing data solutions, applying advanced statistical techniques, and creating actionable insights for our clients. You'll also have the chance to identify and propose solutions for complex issues, all while creating visually appealing, client-ready deliverables.Key Responsibilities  Extract, transform, analyze, aggregate, and interpret data to support predictive model development. Design data solutions and tools. Analyze different data sources to determine value and provide recommendations. Ensure data governance, data security, and data quality. Perform assessments on data assets and suggest resolutions for moderate complexity issues. Prepare client reports and deliver summaries of results. Collaborate effectively with the team, offering support for ad-hoc analytics. Serve as a valuable resource to others. Embrace continuous learning of engineering practices, including DevOps, Cloud, and Agile thinking. Maintain high-quality documentation of data definitions, transformations, and processes to ensure data governance and security.
QualificationsRequired:  3+ years of experience in data engineering. Advanced proficiency in Python, SQL, and/or SAS. Experience with data visualization tools like Power BI, Tableau, R Studio. Knowledge of Cloud environments, such as Azure, AWS, Databricks. Ability to communicate technical information to both technical and non-technical audiences. Experience debugging code developed by others.
Preferred  Intellectual curiosity and a drive to serve clients. Experience applying software engineering practices. Familiarity with health care claims and/or pharmacy claims. Experience with consultancy, actuarial, research, or healthcare data engineering.
Work EnvironmentEnjoy the flexibility to work remotely from anywhere within the U.S. while taking on challenging and rewarding projects in a fast-paced, exciting environment.BenefitsYou'll be recognized and rewarded for your performance, and we'll provide clear direction on what it takes to succeed in your role, along with opportunities for professional development.Note For Remote WorkersAll employees working remotely must adhere to our company's Telecommuter Policy.Location:This opportunity is open to residents of California, Colorado, Connecticut, Nevada, New Jersey, New York, Rhode Island, or Washington.Equal Opportunity EmployerOur company is an Equal Employment Opportunity/Affirmative Action employer. We consider all qualified applicants for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law.We are a drug-free workplace, and candidates are required to pass a drug test before beginning employment.Employment Type: Full-Time"
Power System Analytics Engineer,Sentient Energy,"Santa Clara, CA (Remote)",https://www.linkedin.com/jobs/view/3765290826/?eBP=JOB_SEARCH_ORGANIC&refId=FnulRbCiFZIyO9RCHEEslQ%3D%3D&trackingId=ibztoB%2B7Fd6Jui0APKLpZA%3D%3D&trk=flagship3_search_srp_jobs,3765290826,"About the job
            
 
Your JobSentient Energy, a leading player in the field of utility analytics, grid edge control, and intelligent sensing, is seeking talented Power System Analytics Engineer to join its Power System Data Science and Analytics team. This full-time position offers the opportunity to work on cutting-edge projects involving the analysis of smart grid data, making use of advanced Artificial Intelligence and power system principles. With the increasing adoption of utility IoT deployments and grid modernization programs, the need for deriving valuable insights from operational and non-operational data has never been greater. This position ideally will be located in Milpitas, CA or Frisco, TX, but open to remote work.Our TeamThis team is responsible for research, technology development, and solution delivery of analytics applications based on the synergistic use of digital grid data coupled with the state-of-the-art Artificial Intelligence and power system first principles.What You Will Do Developing and maintaining data-driven algorithms and AI models to characterize and predict power grid behavior under various conditions.Establishing and optimizing data processing pipelines based on industry best practices to support the testing of hypotheses at scale with extensive field device data.Collaborating with the Analytics team to provide innovative ideas and analytics expertise from related industries.Assisting product engineering teams in validating and verifying sensor system performance.Analyzing grid events and failure signatures to gain a comprehensive understanding of outages or disturbances.Contributing to utility analytics partnership initiatives and roadmap development.Participating in intellectual property development and representing the company at technical conferences and trade shows.Involvement in 3rd party funded R&D programs and initiatives.Publishing internal reports, external articles, and white papers to showcase company success and maintain industry thought leadership.
Who You Are (Basic Qualifications) Proficiency in grid modeling and simulation and advanced machine learning applied to Power systemsExperience with statistical modeling, time-series analysis, and programming tools for data science (e.g., R, Python, Matlab)Familiarity with database query languages (SQL)
What Will Put You Ahead An advanced degree in electrical engineering, computer science, applied math/statistics, or a closely related STEM field.Demonstrated utility analytics experience in a technology vendor, national laboratory, or utility environment.Expert knowledge of Python libraries for signal processing, time series modeling, database querying, statistics, machine learning, and visualization (e.g., numpy, statsmodel, scipy, scikit-learn, matplotlib).More than 2 years of relevant work experience, including experience applying advanced analytics to power and energy industry problems.A track record of prior work or official coursework in analytics demonstrated by publications, certifications, GitHub projects, etc.Familiarity with electrical distribution system operations and management practices and technologies.Ability to convey complex concepts and results to a diverse audience, including stakeholder executives.
At Koch companies, we are entrepreneurs. This means we openly challenge the status quo, find new ways to create value and get rewarded for our individual contributions. Any compensation range provided for a role is an estimate determined by available market data. The actual amount may be higher or lower than the range provided considering each candidate's knowledge, skills, abilities, and geographic location. If you have questions, please speak to your recruiter about the flexibility and detail of our compensation philosophy.Hiring PhilosophyAll Koch companies value diversity of thought, perspectives, aptitudes, experiences, and backgrounds. We are Military Ready and Second Chance employers. Learn more about our hiring philosophy here.Who We AreAs a Koch Engineered Solutions company, Sentient Energy provides innovative line sensing, data analytics, optimization and control technologies for the distribution grid. Our grid edge software solutions help electric utilities make data-driven decisions to enhance the delivery of safe, reliable and efficient power.At Koch, employees are empowered to do what they do best to make life better. Learn how our business philosophy helps employees unleash their potential while creating value for themselves and the company.Our BenefitsOur goal is for each employee, and their families, to live fulfilling and healthy lives. We provide essential resources and support to build and maintain physical, financial, and emotional strength - focusing on overall wellbeing so you can focus on what matters most. Our benefits plan includes - medical, dental, vision, flexible spending and health savings accounts, life insurance, ADD, disability, retirement, paid vacation/time off, educational assistance, and may also include infertility assistance, paid parental leave and adoption assistance. Specific eligibility criteria is set by the applicable Summary Plan Description, policy or guideline and benefits may vary by geographic region. If you have questions on what benefits apply to you, please speak to your recruiter.Equal OpportunitiesEqual Opportunity Employer, including disability and protected veteran status. Except where prohibited by state law, all offers of employment are conditioned upon successfully passing a drug test. This employer uses E-Verify. Please visit the following website for additional information: http://www.kochcareers.com/doc/Everify.pdf"
Azure Data bricks Data engineer - Remote,Lorven Technologies Inc.,United States (Remote),https://www.linkedin.com/jobs/view/3727940847/?eBP=JOB_SEARCH_ORGANIC&refId=FnulRbCiFZIyO9RCHEEslQ%3D%3D&trackingId=6OVRyhwE2umCUYHcdsS1CA%3D%3D&trk=flagship3_search_srp_jobs,3727940847,"About the job
            
 
Our client is looking for an Azure Databricks Data engineer for a Long-term Remote below are the detailed requirements.Kindly share your Updated Resume with the Best Reachable Number.Role: Azure Data bricks Data engineerLocation: RemoteDuration: Long-term ContractJob DescriptionKey Skills: Python, Azure Databricks.Roles & Responsibilities Develop Python code to read the XML source data cleansed and generate the necessary output files for Azure cloud ingestion. XML Shredding and Parsing need to be done, Prepare Requirement specification, Tracker & Traceability Matrix.Prepare Low-level design. Unit test cases preparation and carry out unit testing. Capture unit test results, Issue Clarification & Resolution. Update JIRA at the user story level every day and drive it to completion. Highlight any potential impediments, Ensures adherence to Chubb standards, policies, and quality compliance, enterprise metadata definition. Understand existing applications/systems, if applicable, UAT / QA support. Project Planning and Set-up- Understand the project scope, identify activities/ tasks, task level estimates, schedule, dependencies, and risks, and provide inputs to Module Lead for review Provide inputs to testing strategy, configuration, deployment, hardware/software requirement, etc. Ability to use strong industry knowledge to relate to customer needs and dissolve customer concerns and a high level of focus and attention to detail.Strong work ethic with good time management with the ability to work with diverse teams.
NoteTHESE EMAILS ARE GENERATED BY KEYWORD AND I APOLOGIZE IF THESE SKILLS SETS DO NOT MATCH YOUR EXPERTISE, OR IF THE LOCATION IS OUT OF RANGE.We do have other opportunities available. If you are interested, please send me your latest resume. If you are not currently seeking employment, or if you would prefer, I contact you at some later date, please indicate your date of availability so that I may honor your request."
Sr Data Engineer - VHA,Trilogy Federal,"Arlington, VA (Remote)",https://www.linkedin.com/jobs/view/3764350051/?eBP=JOB_SEARCH_ORGANIC&refId=FnulRbCiFZIyO9RCHEEslQ%3D%3D&trackingId=wVPE96r57YrrHMJwEAsanQ%3D%3D&trk=flagship3_search_srp_jobs,3764350051,"About the job
            
 
Trilogy Federal provides financial management, information technology (IT) consulting, program management services, and strategic consulting to federal agencies. Trilogy has an extensive history helping federal clients achieve their most ambitious business modernization and optimization goals with the ability to deliver targeted subject matter expertise and full life cycle support.Trilogy Federal is looking for a motivated and independent Sr Data Engineer to support a variety of projects across a large program. The ideal candidate will have outstanding technical, interpersonal, communication, presentation, writing, analytical, problem solving, and information gathering skills along with fundamental technique troubleshooting abilities. The Sr Data Engineer must be able to work off of minimal information and define project scope and goals. Only candidates who have worked with VHA data will be considered for this role.The selected candidate will join a high-functioning team that focuses on cleaning and analyzing data, answering questions, and providing metrics to solve business problems. The Sr Data Engineer will develop, test, and maintain data pipelines and architectures, which data scientist teammates will use for analysis. The Sr Data Engineer will do the legwork to help the data scientist team provide accurate metrics. Our Sr Data Engineer will have an important role on a high-performing team improving healthcare for Veterans.Primary Responsibilities: Develop, construct, expand, and optimize data and data pipeline architectureDefine data assets to populate data modelsDesign data integrations and data quality frameworkDesign and evaluate open source and vendor tools for data lineageWork closely with all business units and engineering teams to develop strategy for long term data platform architectureIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etcBuild appropriate analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metricsSupport software developers, database architects, and data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projectsWork with stakeholders to assist with data-related technical issues and support their data infrastructure needsCreate data tools for analytics and data scientist team members that assist them in building and optimizing products into an innovative environmentPlan and conduct data engineering projects and/or studies to evaluate, design, and tabulate statistical sampling plans and analytical procedures and processes. Responsibilities include detailed problem definition; identification and investigation of key variables or parameters; development and validation of assumptions; formulation of alternatives; development of performance measures of merit; establishment of data collection requirements; selection, adaptation, or development of appropriate analytical methodology; systems analysis or evaluation of alternatives; development of recommendations, and presentation of results or findings in multiple formats, forums, and to audiences of varying degree of technical knowledge and seniorityInterface with senior leadership on complex problems and resolves major issues, based on sound data engineering principles and affordable solutions. Serves as the principal investigator for data engineering requiring a high degree of technical competence to gauge the extent to which the perimeters of the state-of-the-art can be pushedEngage with organizations, Industry and Academia to conduct necessary stakeholder analysis to ensure data engineering projects support IVC and end-user requirementsDevelop and aids in the production of documents, roadmaps and strategies for data environment and architectureProvide support to the development of requirements documents and program baseline and program recommendations as appropriateCollaborate with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility, and fostering data-driven decision making across the organizationImplement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on itWrite unit/integration tests, contributes to engineering wiki, and documents workPerform data analysis required to troubleshoot data related issues and assist in the resolution of data issues
Minimum Requirements: Bachelor’s degree in mathematics, statistics, computer science, data science or a field directly related to the position5+ years of relevant work experience in healthcare data analyticsDemonstrated work experience with VHA dataExperience using CDWMust have experience with relational SQL (specifically t-SQL), Spark, Spark-Streaming, Python, R, PowerBI, Github, PySpark, DataBricks, Palantir, Data Lakes, and ScalaExperience with Azure DevOps, Azure Cloud Services, Machine LearningExperience writing custom objective functions for equity scaling in machine learningFamiliarity with ensemble modelingFamiliarity with ICD9 / 10 diagnosis and procedural codes and CPT procedural codesExperience with REGEXMicrosoft Word, Excel, PowerPoint, and OutlookStrong analytical, problem-solving, facilitation, and communication skills, including written, verbal, and interpersonal. Ability to think about projects holistically to identify next steps and courses of action that will support the client’s goals. Ability and drive to learn from challenges and take on progressive responsibility. Excellent organizational skills and strong attention to detail. The ability to obtain a Public Trust Clearance. 
Preferred Qualifications: A Master's degree in mathematics, statistics, computer science, data science or a field directly related to the positionHypertext Preprocessor (PHP)Statistical Analysis System(SAS)TableauCollibraAWS Cloud ServicesHadoopKafkaNoSQL databases, including Postgres and Cassandra Data pipeline and workflow management tools: Azkaban, Luigi, and AirflowCloud services: Elastic Compute Cloud (EC2), Elastic MapReduce (EMR), Remote Desktop Services (RDS), and RedshiftStormJavaC++
Benefits (including but not limited to): Health, dental, and vision plansOptional FSAPaid parental leave Safe Harbor 401(k) with employer contributions 100% vested from day 1Paid time off and 11 paid holidaysNo cost group term life/AD&D plan, and optional supplemental coveragePet insuranceMonthly phone and internet stipendTuition and training reimbursement
Regarding remote positions, Trilogy Federal is only able to offer virtual employment in the following states: Colorado, Connecticut, Delaware, D.C., Florida, Illinois, Indiana, Maryland, Massachusetts, New York, South Carolina, Texas, and Virginia.Trilogy Federal is an Equal Employment Opportunity employer. We do not discriminate based upon race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.This range is not a guarantee of compensation or salary, as Trilogy Federal conducts an individual equity review for every candidate based on experience, location, education, industry experience, and comparisons to internal pay bands. In addition to salary, Trilogy offers robust benefits including medical/dental/vision insurance coverage, 401(k) match, paid holidays, paid time off, tuition reimbursement, and a very supportive work/life balance."
Senior Data Engineer,Velir,"Somerville, MA (Remote)",https://www.linkedin.com/jobs/view/3738083935/?eBP=JOB_SEARCH_ORGANIC&refId=FnulRbCiFZIyO9RCHEEslQ%3D%3D&trackingId=c4IqBfSdlXUuF4Th2vvEhA%3D%3D&trk=flagship3_search_srp_jobs,3738083935,"About the job
            
 
Senior Data Engineers play a crucial role in helping our client organizations manage and leverage their data effectively. Their responsibilities include Data Architecture Design; Data Warehousing; Data Quality Assurance; Scalability and Performance Optimization; and Data Security. As senior-level individual contributors, SDEs are also responsible for recommending and implementing data engineering tools and technologies that best suit the client’s needs.Because our clients are mostly US-based organizations, we look for the ability to communicate with professional proficiency in English, verbally and in writing.Data Engineering Expertise You are responsible for building the infrastructure to support the storing and movement of data, so that it can be prepared by analytics engineers to eventually be interpreted by analysts. Your job is informing, developing, and implementing data accessibility solutions, enabling our clients to utilize data for performance evaluation and optimization. As a senior member of the Data Engineering function, you serve as a mentor to other engineers, both individually and in group settings. Consistently seeks out and delivers on engagement level vision, tasks and problemsActively assists in scoping and executing most impactful work for the teamRegularly delivers large features and product improvements that have a meaningful impact on clients’ data infrastructure and capabilitiesAutonomous in approach and may direct or coach other less experienced EngineersActively mentors other Engineers in the team on individual basis or in group settingsHelps others grow through technical guidance, code reviews, documentation, etc.
Cross-Team Collaboration You are responsible for collaborating with peers and other functional departments to develop and implement data engineering strategies and approaches that support engagement goals and understanding client needs. Promotes a positive culture within and across different teams, collaborating with analytics engineers and data analysts on end-to-end client requirementsCollaborate with clients and functional managers to plan for data engineering needs for a product or feature launchPair with a teammate or with someone at a client on strategies for solving a data engineering problemCreate a process or reporting template that helps cross-functional teams solve for common data engineering problemsRegularly engages with other teams to make the organization more effectiveTake initiative to identify and solve important problems. Coordinates with others on cross-cutting technical issuesDrives data solutions improvements that impacts the client experience or empowers internal stakeholders (teams like Operations, Growth / Partnerships, Finance, etc.) to do their job effectively
Project Delivery You are responsible for ensuring that large and/or more complex data engineering projects are delivered on time, within scope, and within budget. Architects and designs services/systems using design patterns that allow for iterative delivery and future scalingProactively identifies and tackles technical debt becoming too big through planning work and aligning the team. Does this with careful evaluation of additional cost on developmentOptimizes for the predictability and regular cadence of deliverablesKeeps reliability, maintainability and scalability of our clients’ systems top of mindEmbraces long-term ownership of projects while training others to reduce the bus factor or becoming a blockerPrioritizes and values undesirable/unowned work that enables the team to move faster
Tools & Technologies  Programming languages (e.g. SQL, Python)Data Processing (e.g. Apache Spark, dbt)Cloud-based data warehouses (e.g., Snowflake, Google BigQuery)Data orchestration (e.g., Apache Airflow, Azure Data Factory, Prefect)

Technical Skills (Hard Skills)See the latest Data Engineering framework. Data Movement You can reduce latency of end-to-end pipelines through data orchestration in addition to incrementalization or streaming. You have strong knowledge of common data integration patterns (CDC, ELT, etc.).Data Warehousing You have a high proficiency in warehousing, including working knowledge of common ingestion SaaS platforms (e.g., Fivetran) and / or frameworks (e.g., Meltano, Airbyte), an ability to configure warehouse ingestion tools (e.g., Snowpipe) and can provision, maintain and optimize at least one cloud data warehouse (e.g., Snowflake).Programming You are considered a highly proficient programmer, approaching your code holistically, achieving a high standard routinely. You can optimize performance for large workloads and are able to troubleshoot complex queries / functions. Proficiency in Python required.Domain Expertise You have a strong foundation of knowledge in domains in which you’re working. You are able to relate how the business works with the goals of the immediate team.Technical Management Is able to display a clear technical confidence and understanding. For the most part, can use organizational- and team-specific tools independently.
Bonus points for Data Modeling & Transformation You have high proficiency with data transformation tools such as dbt and expert proficiency in data modeling approaches and philosophies (Kimball, OBT). Data Orchestration. You’re familiar with at least one data orchestration platform (Azure Data Factory, Apache Airflow, Prefect, etc.). Data Infrastructure You understand more complex infrastructure approaches, including the implications and suitability of different deployment options and how to deploy self-hosted applications for clients with high security requirements.
Essential Skills (Intangible Skills) Curiosity & Versatility You help your immediate peers to make decisions based on what projects need, not what they feel most comfortable doing. You have taken the initiative to seek out new ways to apply existing skills and knowledge. Collaboration & Partnership You can facilitate collaborative group activities and/or workshops with colleagues or external stakeholders. You are considered a role model for collaboration and creating alignment across teams because of your consistency and predictability.Effective Communication You reliably foster a culture of clear, concise, effective, audience-oriented communication for your team, other departments, and external stakeholders, ensuring those around you are actively listening as well as are understood.Developing Others You understand your team's domain, share knowledge frequently with your teammates and contribute to the team's documentation. You proactively watch for opportunities to share knowledge and encourage others to do the same.Culture & Togetherness You've openly stated your expectations of how your team works and acts, then demonstrated those expectations yourself. You help to coordinate and activate efforts towards a fairer, more diverse and safer workplace, using your position of influence to get things done. 
Qualifications (Must Haves) Proven experience as a Data Engineer or related role, with a focus on designing and developing data pipelines.Strong programming skills in Python and SQL. Experience with Scala and Rust is a plus but not required.Deep knowledge of data warehousing and ETL/ELT processes.Intermediate / expert proficiency with common data integration / orchestration platforms (e.g., Fivetran, Azure Data Factory, Apache Airflow)Hands-on experience with data warehouses like Snowflake, BigQuery, Databricks, or similar.Experience with streaming solutions such as Spark Streaming, Kafka, or Flink is desirable but not required.Familiarity with cloud platforms such as AWS, Azure, or Google Cloud.Familiarity with machine learning operations (MLOps) techniques and platforms is a plus but not required.Experience mentoring and advising other engineersStrong analytical and problem-solving skills.Excellent communication and collaboration skills.
Physical Requirements Frequent sitting at a desk performing work on a computerReasonable accommodations may be made to enable individuals with disabilities to perform the essential functions
CompensationWe believe all team members should be rewarded competitively, using practices that are equitable and transparent. This philosophy ensures we’re able to find, grow, and retain exceptional talent from a variety of backgrounds.The pay range for this role is up to $150,000 if based in the USA and up to $76,000 if based in El Salvador.Please note that compensation packages are finalized after the interview process is concluded. We use a competency-based approach to base pay, which means it is based on the competencies and skills demonstrated for this role.Core Company ValuesVelir is an established mid-sized agency with a top-tier portfolio of clients, ranging from the world’s largest non-profits to Fortune 500 brands. We pride ourselves on our people-first culture and a low-ego workplace that embraces experimentation, collaboration and continuous improvement. We have a fun office environment located in Davis Square (Somerville, MA) and offer competitive pay and excellent benefits. Take the Long View - Ensure the company is built to lastBe Courageous - Make the right decisions even when they aren't the easiest decisionsBe Genuine - Bring honesty and authenticity to all that you doWork with Focus + Passion - Display purpose and pride in your work and never stop learning
As an equal opportunity employer, we are firmly committing to diversity, equity, and inclusion in our hiring efforts. We recognize that we need team members from all backgrounds and experiences to successfully shape a positive employee experience as well as deliver our product and service solutions. To that end, we actively seek candidates who can bring diverse experiences and backgrounds to our team. We know that complex factors and systemic bias can get in the way of us meeting strong candidates, so please don't hesitate to apply even if you're not 100% sure.At this time, Velir does not sponsor candidates and unfortunately cannot accept those on OPT or CPT."
Lead Data Engineer / Data Architect,Enexus Global Inc.,United States (Remote),https://www.linkedin.com/jobs/view/3766088969/?eBP=JOB_SEARCH_ORGANIC&refId=FnulRbCiFZIyO9RCHEEslQ%3D%3D&trackingId=zwbyf4A7pwy1PLJlP%2BxvsQ%3D%3D&trk=flagship3_search_srp_jobs,3766088969,"About the job
            
 
Job Title - Lead Data Engineer / Data Architect (12 Years)Location - RemoteContract Type - W2/C2C/1099Experience - 11+ Years minimumSkills - Pyspark, Azure, Matillion, PythonJob Description :- Overall 10+ years of experience in Data Engineering, Data Modelling.Excellent (Hands-On) in Azure, ADF, Databricks (PySpark), Python, SQL, Unix Shell scripting. Good experience in Snowflake Datawarehouse.Experience in ETL, tools like Matillion, QLIK, DataStage, and performance tuning / optimization.Experience in building dimensional data models.Experience in working with ML teams to coordinate and enable the promotion of ML models to a governed production environment to bring stability and robustness to be supported by AMS (Data Ops and ML Ops team) teams.Coordinating with onshore and offshore cross-functional teams to deliver concurrent projects.Strong ETL experience in handling large volumes of data in the complex heterogeneous data warehouses and processing high volume jobs.Experience in building data ingestion pipeline and data replication into cloud environments hosted on Azure platform.Ability to architect scalable data pipelines, following the best practices."
"Data Engineer, Digital Transformation - Remote | WFH",Get It Recruit - Information Technology,"Boston, MA (Remote)",https://www.linkedin.com/jobs/view/3767569982/?eBP=JOB_SEARCH_ORGANIC&refId=FnulRbCiFZIyO9RCHEEslQ%3D%3D&trackingId=x0Xp1jUglfZFYyPKzw1CJw%3D%3D&trk=flagship3_search_srp_jobs,3767569982,"About the job
            
 
Join a dynamic and innovative team at the forefront of digital transformation in education. We're the Harvard Business School Digital Transformation team, a ""startup with assets,"" offering a unique opportunity to contribute to cutting-edge digital and emerging technology solutions. If you're passionate about technology, world-class education, and making a lasting impact, this is the place for you.PositionAs a Data Engineer, you'll play a key role in implementing architecture blueprints, conducting data analyses, and finding solutions to complex technical challenges. Your expertise will shape and build systems to enhance agility, improve security, reduce costs, and meet utilization targets. Collaborating closely with the Data Science and Infrastructure Architecture & Platform teams, you'll contribute to a culture of engineering excellence.Duties And ResponsibilitiesProvide technical support and training in data frameworks across HBS.Design and build production-ready applications for batch and streaming data processing in a multi-tenancy cloud data platform.Collaborate with the Group Architecture team to enhance data platform capabilities using cloud platform best practices.Determine tools and technologies for the Data Platform, exploring new solutions for potential benefits and improvements.Partner with technology teams within HBS, Harvard, and vendors to implement solutions.QualificationsAdditional responsibilities as assigned.RequiredBachelor's degree in mathematics, physics, computer science, engineering, statistics, or an equivalent technical discipline.3-5 years of experience in cloud and on-premise environments, with expertise in data processing tools, testing, automation, and CI/CD using agile best practices.3 years of experience in building data security frameworks compliant with GDPR and CCPA guidelines.5 years of hands-on experience with distributed data processing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark) and relational/non-relational databases (SQL, MongoDB, Redis, Cassandra).PreferredExtensive experience in performance tuning applications on cloud systems.Experience in real-time data processing using scalable data streaming frameworks.Expert level experience with cloud ecosystems (AWS, GCP, Azure).Experience with descriptive statistics and data analysis.Strong software development experience in object-oriented programming languages (C/C++, Java, Python, Scala).Experience with Unix-based systems, including bash scripting.Additional InformationThis role offers the possibility of remote or hybrid work, with periodic visits to our Boston, MA based campus. Candidate interviews may be conducted virtually or in-person. A cover letter is required for consideration, and Harvard Business School will not offer visa sponsorship for this opportunity.Commitment To DiversityWe are an equal opportunity employer committed to fostering a diverse and inclusive workplace. Learn more about our work culture here.BenefitsExplore our outstanding benefits package, including paid time off, comprehensive health and welfare benefits, retirement plans, tuition assistance, professional development opportunities, and more. Visit Harvard's Total Rewards website for details.Work Format DetailsThis is a hybrid position based in Massachusetts. Further details will be discussed during the interview process. Remote work is limited to Harvard Registered Payroll States.Employment Type: Full-Time"
AWS Data engineer,IVY TECH SOLUTIONS INC,"Washington, DC (Remote)",https://www.linkedin.com/jobs/view/3682836051/?eBP=JOB_SEARCH_ORGANIC&refId=FnulRbCiFZIyO9RCHEEslQ%3D%3D&trackingId=ujBV4ULfDUBQnw8NIoHbzA%3D%3D&trk=flagship3_search_srp_jobs,3682836051,"About the job
            
 
HI,Kindly let me know if you have a suitable fit for the following positionThanksAWS Data engineer Location: Washington, DCDuration: 12+MonthsInitially RemotePlease send the resume to  or 847- 350-1008Must-haves: Familiarity with AWS data services and modules.5+ years of hands-on experience with AWS services (Lambda, S3, RDS, Aurora, DynamoDB, Kinesis, AWS Glue, AWS Data Pipeline)3+ years of experience with data migration, data analysis, and SQLs3+ years of experience with informaticaExperience with Structured Query Language (SQL), should be able to analyze, compare and profiling data setsAbility to work in globally distributed teamsKnowledge of IT processes, including quality assurance, release management, and production supportExcellent analytical, troubleshooting, and problem-solving skillsExcellent communicator (written and verbal, formal, and informal).Flexible and proactive/self-motivated working style with strong personal ownership.Ability to multi-task and prioritize under pressure.Ability to work independently with minimal supervision as well as in a team environment.Undergraduate or graduate degree in Computer Science, Data Science, or equivalent education/professional experience is required
Warm Regards,Charan Kumar | IVY Tech Sols Inc.3403 N Kennecott Avenue, Suite B&C Arlington Heights, IL 60004PH.( Direct: (847) 350-1008   |Gtalk : charan.ivytech|
Powered by JazzHRBimNQHf5mR"
Workplace Analytics Engineer,Canonical,"Chicago, IL (Remote)",https://www.linkedin.com/jobs/view/3738244822/?eBP=JOB_SEARCH_ORGANIC&refId=FnulRbCiFZIyO9RCHEEslQ%3D%3D&trackingId=X73ELc%2BbTzmUP%2BRv7v2qXg%3D%3D&trk=flagship3_search_srp_jobs,3738244822,"About the job
            
 
Bring your people analytics, social science research and data mining/science skills to a unique team seeking to understand, and shape, the future of the digital workplace. We are interested in technology, of course, but we are also interested in the human mission of enabling the world's brightest and hardest working people to live where they want and work from anywhere. Most of our colleagues could move to a tech hub but they choose Canonical because of our mission and our approach to the workplace.We'd like to understand what really makes a distributed, remote-first workplace work. We think we're pretty good at this (being remote first for almost 20 years), but we know there is a lot still to understand, and the frontier of possibility continues to move outward. We'd like to invest in research, analytics and tooling which raises the bar even further for remote collaboration and organisation.If we are able to build tools that meaningfully improve our cooperation and our satisfaction, then we intend to share our insights with the world, both as a narrative and as SAAS or open source that helps other companies follow in our footsteps.The role of a Workplace Analytics Engineer at CanonicalSupport analytics and data mining in a cross-disciplinary team of organisational psychologists, web front end engineers, back end engineers and statistics / analytics experts to help us build a new definition for the 21st century digital workplace. Collaborate to figure out what really drives productivity, effectiveness and happiness in a remote-first globally distributed company.In addition to your existing people analytics work experience, this role will combine your skills in psychology, data analytics and visualisation, to help create a more effective workplace.Location: This role will be based remotely in the AMER region.All applicants applying must be legally authorized to work in the United States, as we cannot offer visa sponsorship for this job position.What your day will look like Utilize advanced data analytics to understand how we hire and how we work (productivity, happiness and effectiveness) across a global, remote first organisationFocus on quantitative and qualitative data analytics to find insights and meaningful business outcomesTell the story from the insights through dashboards, visualizations and presentationsDesign and conduct research into trends shaping talent science and remote workCollaborate with stakeholder teams (ex., engineering, information systems, etc) to improve the data and tool ecosystem supporting our digital workplace
What we are looking for in you Background in data science, mathematics, actuarial science, or engineeringFirst work experience in People Analytics Knowledge in advanced statistics, data sciences, coding/scripting languages (Python, R, etc), and databases (SQL, etc)Strength in data analytics and visualization (Looker Studio, Tableau, etc)Ability to translate business questions to key research objectivesAbility to identify the best methodology to execute research, synthesize and analyse findingsExcellent writing and communication skillsWillingness to examine the status quo and resilient in the face of challenges
What we offer youYour base pay will depend on various factors including your geographical location, level of experience, knowledge and skills. In addition to the benefits above, certain roles are also eligible for additional benefits and rewards including annual bonuses and sales incentives based on revenue or utilization. Our compensation philosophy is to ensure equity right across our global workforce.In addition to a competitive base pay, we provide all team members with additional benefits, which reflect our values and ideals. Please note that additional benefits may apply depending on the work location and, for more information on these, you can ask in the later stages of the recruitment process.🏠 Fully remote working environment - we've been working remotely since 2004!📚 Personal learning and development budget of 2,000USD per annum💰 Annual compensation review🏆 Recognition rewards🏝 Annual holiday leave👶 Parental Leave🧑‍💼 Employee Assistance Programme🧳 Opportunity to travel to new locations to meet colleagues at 'sprints'✈️ Priority Pass for travel and travel upgrades for long haul company eventsAbout CanonicalCanonical is a pioneering tech firm that is at the forefront of the global move to open source. As the company that publishes Ubuntu, one of the most important open source projects and the platform for AI, IoT and the cloud, we are changing the world on a daily basis. We recruit on a global basis and set a very high standard for people joining the company. We expect excellence - in order to succeed, we need to be the best at what we do.Canonical has been a remote-first company since its inception in 2004. Work at Canonical is a step into the future, and will challenge you to think differently, work smarter, learn new skills, and raise your game. Canonical provides a unique window into the world of 21st-century digital business.Canonical is an equal opportunity employerWe are proud to foster a workplace free from discrimination. Diversity of experience, perspectives, and background create a better work environment and better products. Whatever your identity, we will give your application fair consideration."
Workplace Analytics Engineer,Canonical,"Minneapolis, MN (Remote)",https://www.linkedin.com/jobs/view/3738247330/?eBP=JOB_SEARCH_ORGANIC&refId=FnulRbCiFZIyO9RCHEEslQ%3D%3D&trackingId=3GOg9bC8EGM9qzXL1FHw%2FA%3D%3D&trk=flagship3_search_srp_jobs,3738247330,"About the job
            
 
Bring your people analytics, social science research and data mining/science skills to a unique team seeking to understand, and shape, the future of the digital workplace. We are interested in technology, of course, but we are also interested in the human mission of enabling the world's brightest and hardest working people to live where they want and work from anywhere. Most of our colleagues could move to a tech hub but they choose Canonical because of our mission and our approach to the workplace.We'd like to understand what really makes a distributed, remote-first workplace work. We think we're pretty good at this (being remote first for almost 20 years), but we know there is a lot still to understand, and the frontier of possibility continues to move outward. We'd like to invest in research, analytics and tooling which raises the bar even further for remote collaboration and organisation.If we are able to build tools that meaningfully improve our cooperation and our satisfaction, then we intend to share our insights with the world, both as a narrative and as SAAS or open source that helps other companies follow in our footsteps.The role of a Workplace Analytics Engineer at CanonicalSupport analytics and data mining in a cross-disciplinary team of organisational psychologists, web front end engineers, back end engineers and statistics / analytics experts to help us build a new definition for the 21st century digital workplace. Collaborate to figure out what really drives productivity, effectiveness and happiness in a remote-first globally distributed company.In addition to your existing people analytics work experience, this role will combine your skills in psychology, data analytics and visualisation, to help create a more effective workplace.Location: This role will be based remotely in the AMER region.All applicants applying must be legally authorized to work in the United States, as we cannot offer visa sponsorship for this job position.What your day will look like Utilize advanced data analytics to understand how we hire and how we work (productivity, happiness and effectiveness) across a global, remote first organisationFocus on quantitative and qualitative data analytics to find insights and meaningful business outcomesTell the story from the insights through dashboards, visualizations and presentationsDesign and conduct research into trends shaping talent science and remote workCollaborate with stakeholder teams (ex., engineering, information systems, etc) to improve the data and tool ecosystem supporting our digital workplace
What we are looking for in you Background in data science, mathematics, actuarial science, or engineeringFirst work experience in People Analytics Knowledge in advanced statistics, data sciences, coding/scripting languages (Python, R, etc), and databases (SQL, etc)Strength in data analytics and visualization (Looker Studio, Tableau, etc)Ability to translate business questions to key research objectivesAbility to identify the best methodology to execute research, synthesize and analyse findingsExcellent writing and communication skillsWillingness to examine the status quo and resilient in the face of challenges
What we offer youYour base pay will depend on various factors including your geographical location, level of experience, knowledge and skills. In addition to the benefits above, certain roles are also eligible for additional benefits and rewards including annual bonuses and sales incentives based on revenue or utilization. Our compensation philosophy is to ensure equity right across our global workforce.In addition to a competitive base pay, we provide all team members with additional benefits, which reflect our values and ideals. Please note that additional benefits may apply depending on the work location and, for more information on these, you can ask in the later stages of the recruitment process.🏠 Fully remote working environment - we've been working remotely since 2004!📚 Personal learning and development budget of 2,000USD per annum💰 Annual compensation review🏆 Recognition rewards🏝 Annual holiday leave👶 Parental Leave🧑‍💼 Employee Assistance Programme🧳 Opportunity to travel to new locations to meet colleagues at 'sprints'✈️ Priority Pass for travel and travel upgrades for long haul company eventsAbout CanonicalCanonical is a pioneering tech firm that is at the forefront of the global move to open source. As the company that publishes Ubuntu, one of the most important open source projects and the platform for AI, IoT and the cloud, we are changing the world on a daily basis. We recruit on a global basis and set a very high standard for people joining the company. We expect excellence - in order to succeed, we need to be the best at what we do.Canonical has been a remote-first company since its inception in 2004. Work at Canonical is a step into the future, and will challenge you to think differently, work smarter, learn new skills, and raise your game. Canonical provides a unique window into the world of 21st-century digital business.Canonical is an equal opportunity employerWe are proud to foster a workplace free from discrimination. Diversity of experience, perspectives, and background create a better work environment and better products. Whatever your identity, we will give your application fair consideration."
Senior Data Engineer,"Changeis, Inc.","Arlington, VA (Remote)",https://www.linkedin.com/jobs/view/3778344143/?eBP=JOB_SEARCH_ORGANIC&refId=FnulRbCiFZIyO9RCHEEslQ%3D%3D&trackingId=KL%2Bqm4%2FcCnzpyAMZomljew%3D%3D&trk=flagship3_search_srp_jobs,3778344143,"About the job
            
 
Changeis, Inc. is an award-winning 8(a) certified, woman-owned small business that provides management consulting and engineering services to the public sector. Changeis' work has resulted in the successful execution of numerous programmatic initiatives, development of acquisition-sensitive deliverables, and establishment of a variety of long-term innovative strategic priorities for its customers. Changeis focuses on delivering unparalleled expertise in the areas of strategy and transformation management, investment analysis and acquisition management, governance, and innovation management. Inc. magazine has ranked the management consulting firm, Changeis Inc., among the top 1000 firms on its 35th annual Inc. 5000, the most prestigious ranking of the nation's fastest-growing private companies. Changeis offers a full benefit package that includes medical, dental, and vision, short and long term disability, retirement plan with immediate vesting and company match, and a generous annual leave plan.The Senior Data Engineer will partner with a Federal Agency Office of Human Resources, focusing on essential areas such as business management, strategic planning, and decision-making. By developing and maintaining data architectures, engaging in acquisition/contract management, and applying expertise in information technology, data analytics, and knowledge management, the Senior Data Engineer will significantly contribute to the optimization and innovation of organizational processes. The Senior Data Engineer will collaborate with product design and engineering teams to understand their needs, and then research and devise innovative statistical models for data analysis. By communicating findings to all stakeholders and using analytics for meaningful insights, they will enable smarter business processes and stay abreast of current technical and industry developments.Roles And Responsibilities  Identifying new datasets and integrate them, focusing on enhancing product capabilities.  Conducting experiments with analytical techniques to solve complex problems across various domains.  Recognizing relevant data sources and gather structured and unstructured data to meet client needs.  Developing and running ETL (Extract, Transform, Load) processes to manage data flow and ensure data quality.  Supporting the development of data platforms (e.g., AWS data lake), collaborating with teams to ensure alignment with organizational goals.  Designing algorithms and models to mine big data, perform data and error analysis, and clean and validate data.  Analyzing data for trends and patterns, interpret insights, and apply them to meet clear objectives.  Collaborating with software developers and machine-learning engineers to implement models in production environments. 
Requirements  15+ years of experience in data analysis, data management, data science, or operations research.  7 years of experience in data and software engineering, data analytics, and machine learning, including design and implementation of end-to-end production level software and/or data engineering solutions.  5 years of experience working with cloud computing and database services such as Amazon Web Service (AWS).  5 years of professional software engineering experience in at least one of the following: Python, JavaScript (e.g., Node.js, React, Vue.js), or C#.  Experience with data ETL (Extract, Transform, and Load).  Experience with the ELK Stack (Elastic Search, Logstash, and Kibana).  Ability to formulate business needs and translate them into technical functional and non-functional requirements.  Experience with Container-based technologies such as Docker, Kubernetes, and similar technologies.  Experience in data modeling, including transactional and data warehouse design.  Prefer candidates holding active AWS certifications"
Senior Data Engineer - Remote,Get It Recruit - Information Technology,"San Mateo, CA (Remote)",https://www.linkedin.com/jobs/view/3774809543/?eBP=JOB_SEARCH_ORGANIC&refId=kONGqVOhoqrjpz5UVPcqkQ%3D%3D&trackingId=RjsiOXbwgjLTP5wozJxSxQ%3D%3D&trk=flagship3_search_srp_jobs,3774809543,"About the job
            
 
We are actively searching for a talented Senior Data Engineering Developer to play a pivotal role in shaping the design, development, scaling, and maintenance of our cutting-edge SaaS infrastructure. Join our dynamic team where you'll collaborate closely with cross-functional experts in Data Science, Engineering, and Product/Business Technology. Your mission: construct robust data infrastructure, streamline processes, and enhance our toolset.ResponsibilitiesVisionary Leadership:Spearhead the vision for Business Intelligence (BI) and Data Warehousing, steering the strategic plan to fruition.Assemble a high-caliber BI and Data Warehouse team, fostering their growth and skill development.CollaborationCultivate collaborative relationships with Product Managers, Analysts, and Software Engineers to decipher data requirements and deliver impactful solutions.Infrastructure ExcellenceArchitect, construct, oversee, and optimize foundational data infrastructure.Implement a monitoring infrastructure for real-time insights into the status of our data pipelines.Implement and supervise processes that enhance implemented solution performance.Optimize schemas, including partitions, compression, and distribution, to balance costs and performance.Craft bespoke data infrastructure solutions not readily available off-the-shelf.Create and sustain custom data ingestion pipelines and seamless integrations with third-party platforms.Data Quality And DashboardsChampion Data Quality and the creation of high-impact dashboards.ManagementDefine and manage Service Level Agreements (SLAs) for all production datasets and processes.Provide guidance and support to our data team, assisting with design decisions and performance optimization strategies.QualificationsEducation:Bachelor's degree in a technical and/or quantitative field of study—e.g., computer science, mathematics, physics, statistics, or equivalent and/or substantial related experience.ExperienceRemarkable track record of 8+ years in the realm of distributed data technologies.Demonstrable experience in ETL and ELT in cloud SaaS/PaaS infrastructures.Proficiency in serverless Microservices like GCP Cloud Function, AWS Lambda.Hands-on experience on streaming and batch data pipeline.Expertise in databases such as Bigquery, MS SQL on data/domain architecture.Expertise in SQL language to be able to transform raw source data into SQL columns.Experience with GCP solutions such as DataFlow and Pubsub are a huge plus.Understanding and experience in AI/ML platform and pipeline, such as Vertex AI.Employee BenefitsCompetitive compensation packages including bonus and options.Medical, dental, and vision benefits.Matching 401(K).Paid time off.Telecommuting and remote-work options.Support for continuing education.Team off-sites, social events, annual company events, and frequent extracurricular activities.Unlimited snacks and drinks.Remote work.Employment Type: Full-Time"
Senior Data Engineer,Kobie Marketing,"St Petersburg, FL (Remote)",https://www.linkedin.com/jobs/view/3765717278/?eBP=JOB_SEARCH_ORGANIC&refId=kONGqVOhoqrjpz5UVPcqkQ%3D%3D&trackingId=J9HLCMqsJM9iERBMd9zJIQ%3D%3D&trk=flagship3_search_srp_jobs,3765717278,"About the job
            
 
Why you will love working for a National Top WorkplaceWe are a global leader in loyalty marketing. We work with some of the most well-known brands in the world to deliver market-leading, end-to-end loyalty solutions to enable customer experiences. With a strategy-led, technology-enabled approach, we are consistently named an industry leader by Forrester. The programs we deliver reach more than 330M consumers through loyalty. The impact of these loyalty programs affords us deep brand partnerships, owning a niche in the loyalty space where outcomes matter most.We Are a Mission And Values Driven Company.Our mission is to grow enterprise value through loyalty for our clients. Every role within Kobie has a purpose and directly contributes to us achieving this mission.We are values driven at every point. Over our 30+ year journey, we've created a fun, high-trust, transparent workplace. We believe in leadership and ownership. Our hybrid work environment, personal holidays, casual dress code and focus on diversity and inclusion add to a culture that makes our teammates proud. That pride shines through in the work we do for our clients.About The Team And What We'll Build TogetherWe find actionable insights in our clients' loyalty data that helps drive enterprise value. We create real time dashboards to inform internal and external stakeholders.As a Senior Data Engineer at Kobie, you will play a vital role within our data engineering team, under the direct supervision of the Manager of Data Engineering. Your expertise will be instrumental in implementing ETL/ELT processes and data integrations, populating Kimball style star schemas from a diverse range of data sources across multiple data warehouse implementations in support of our product. Your involvement will extend from requirements gathering to designing business processes and dimensional models. Your profound comprehension of OLTP, Data Vault, and star schemas will be crucial as you delve into source data analysis to assess its potential in addressing business needs. Your objective will be to create scalable, efficient, auditable, and as much as possible, reusable processes.How You Will Make An Impact Ensure seamless production support of daily running ETL/ELT processes. Develop, design, optimize, and maintain ETL/ELT processes. Conduct data profiling, and source to target mappings, capturing ETL and business metadata for populating Kimball style dimensional models. Design automated tests and audit logging processes for every stage of our data pipelines. Design, manage and build event-driven architectures to support real-time data flows and event processing. Document ETL processes comprehensively including process flow diagrams. Conduct functional and performance testing to identify bottlenecks and data quality issues. Implement slowly changing dimensions as well as transaction, accumulating snapshot, and periodic snapshot fact tables. 
What You Need To Be Successful At least 6 years of Data Engineering experience, with a minimum of 2 operating in Snowflake. Deep understanding of Snowflake Data Platform (data sharing, data clean rooms, marketplace). Understanding of Database Replication and how data flows from OLTP database systems, ELT architecture and design. The ability to work independently across multiple projects, communicating effectively to internal data stakeholders across the organization. The ability to integrate with a wide range of data sources, including APIs messaging systems to capture and normalize streaming data. Possess the ability to design data pipelines from end to end and train other team members in best practices and processes. Have a good grasp of the Software Development Life Cycle (SDLC) and Agile Development processes. Deep understanding of Event Driven Architectures, how they impact Data Pipeline designs and integrations. Cloud Experience with Azure or OCI, preferred. Proficient in scripting languages such as Python, and JavaScript. Experience with orchestration tools like Apache Airflow, Matillion, Mage.ai is preferred. Data Replication tools experience like Kafka, Goldengate, HVR, Qlik Replicate, preferred. 
Our teammates are at the heart of everything we do Healthy people are happy people, which makes mental and physical health a top priority at Kobie. From robust health insurance and benefits options to free fitness programs like FitOn, to generous vacation time for yourself, we support your health needs fully. In today's job market, we know that employees are choosing only what works best for their life. For those that want career growth, Kobie is the perfect place. We have developed a comprehensive people strategy that helps every teammate know how to advance and progress on their career journey. Beyond title progression, Kobie's competitive pay, 401k matching, annual profit sharing and bonuses all make Kobie a perfect place to build your career.Kobie a place for allWe don't just accept differences – we embrace, share, and celebrate them!Employment at Kobie is based solely on a person's merit and qualifications, directly related to professional competence. We do not discriminate against any teammate or applicant because of race, creed, color, religion, gender, sexual orientation, gender identity/expression, national origin, disability, age, genetic information, veteran status, marital status, pregnancy or related condition (including breastfeeding), or any other basis under the law.We are fiercely committed to fostering a workplace where teammates draw upon their own diverse backgrounds, experiences, and perspectives so that they feel welcomed to bring their authentic self to work every day. While our leadership team fully and completely supports our policy of nondiscrimination and equal opportunity, all teammates share the responsibility to ensure we incorporate the principles of equity, diversity, and inclusion throughout Kobie."
Azure Data engineer with Azure data factory and pyspark-- EST/CST-no corps --remote,RemoteWorker US,"Raleigh, NC (Remote)",https://www.linkedin.com/jobs/view/3780834961/?eBP=JOB_SEARCH_ORGANIC&refId=kONGqVOhoqrjpz5UVPcqkQ%3D%3D&trackingId=QkHnD9nilUWmfW503gBBRw%3D%3D&trk=flagship3_search_srp_jobs,3780834961,"About the job
            
 
NOTICE- Any pay ranges displayed are estimations. Actual pay is determined by an applicant's experience, technical expertise, and other qualifications as listed in the job description. All qualified applicants are welcome to apply. Senior Associate should have hand on PySpark, ADF, ADLS, Delta tables, SparkSQL, etc. Architects, in addition to the above must have Architecture experience designing solutions and frameworks ground up and not to just build data pipelines using the existing setup Top Skill Sets/ Experiences Required: These requirements must be strong and reflected on the resume: Azure Data Factory Azure Pyspark, Databricks APIs Your Skills & Experience: Demonstrable experience in enterprise level data platforms involving implementation of end to end data pipelinesGood communication and willingness to work as a teamHands-on experience with at least one of the leading public cloud data platforms (Amazon Web Services, Azure or Google Cloud)Experience with column-oriented database technologies (i.e. Big Query, Redshift, Vertica), NoSQL database technologies (i.e. DynamoDB, BigTable, Cosmos DB, etc.) and traditional database systems (i.e. SQL Server, Oracle, MySQL)Experience in architecting data pipelines and solutions for both streaming and batch integrations using tools/frameworks like Glue ETL, Lambda, Google Cloud DataFlow, Azure Data Factory, Spark, Spark Streaming, etc.Ability to handle multiple responsibilities simultaneously in leadership and contributing to tasks “hands-on”Understanding of data modeling, warehouse design and fact/dimension concepts Thanks Renu Goel 857-207-2676 renu.goel@yoh.com"
Lead Data Engineer-Azure Databricks- US,Zortech Solutions,United States (Remote),https://www.linkedin.com/jobs/view/3745133198/?eBP=JOB_SEARCH_ORGANIC&refId=kONGqVOhoqrjpz5UVPcqkQ%3D%3D&trackingId=znsoQ9sjFscomJFYd8N1Yg%3D%3D&trk=flagship3_search_srp_jobs,3745133198,"About the job
            
 
Role: Lead Data Engineer-Azure DatabricksLocation: Remote/USDuration: 6+ MonthsJob Description Ability to design and develop Azure framework which should be able to pull data from any type of unstructured/ structured sources of data, run transformation based on business rules and load in various types of targetsCandidate need strong Java programming and troubleshooting skills.Expertise with Azure Databricks ServicesShould have BigData, K8 / ETL experience"
Data Engineer with property and casualty experience,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3644951672/?eBP=JOB_SEARCH_ORGANIC&refId=kONGqVOhoqrjpz5UVPcqkQ%3D%3D&trackingId=64Cbk86Dn2csV5Pq7UPeGQ%3D%3D&trk=flagship3_search_srp_jobs,3644951672,"About the job
            
 
Job Title: Data Engineer with property and casualty experienceLocation: 100% Remote {Someone open to work Eastern Standard Time (EST)}Duration: 12+ MonthsExperience Level: 12+ Years in Information Technology and as Data Engineer 7-8+ Years and recent Property and casualty experience in 2-3 projectsCandidate Details Required Full Name:Current Location with Zip Code:Phone:Email:LinkedIn Must:Education Details Bachelors or master:Any Photo ID and Visa Copy for H1B/EAD:
Experience Skill Matrix Data Engineer: YearsInsurance Industry: YearsMust - Property and casualty experience in recent 2-3 projects: YearsData warehousing, modelling, end-to-end BI solutions: YearsSpark, and PySpark: YearsBig data and cloud technologies (e.g., Azure): YearsSQL and query optimization: YearsPower BI data sources and reports: YearsAgile/SCRUM SDLC environment: Years
Job Description 12+ years in IT with at least 7-8+ years' experience in data warehousing, modelling, end-to-end BI solutionsStrong SQL, Spark, and PySpark programming skills for data analysis.Experience developing solutions for the Insurance industry.Strong understanding of Data Engineering Solutions, Data modelling, and Software Engineering principles and best practicesExperience in developing data platforms/ Big data and cloud technologies (e.g., Azure) Advanced knowledge of SQL and query optimization techniques and approachesExperience designing, developing, and supporting Power BI data sources and reports.Able to work as a team member and willing to work independently when required.Strong troubleshooting and problem-solving skillsExperience working in an Agile/SCRUM SDLC environment.Problem-solving aptitude, with a willingness to work in a fast-paced product development environment and hands-on mentality to do whatever it takes to deliver a successful product."
Big Data Engineer (Azure),Tiger Analytics,"Chicago, IL (Remote)",https://www.linkedin.com/jobs/view/3590302059/?eBP=JOB_SEARCH_ORGANIC&refId=kONGqVOhoqrjpz5UVPcqkQ%3D%3D&trackingId=mXDQ2TVtxdqTp8lnyDMejw%3D%3D&trk=flagship3_search_srp_jobs,3590302059,"About the job
            
 
Tiger Analytics is a fast-growing advanced analytics consulting firm. Our consultants bring deep expertise in Data Science, Machine Learning and AI. We are the trusted analytics partner for multiple Fortune 500 companies, enabling them to generate business value from data. Our business value and leadership has been recognized by various market research firms, including Forrester and Gartner. We are looking for top-notch talent as we continue to build the best global analytics consulting team in the world.The Big Data Azure Engineer will be responsible for architecting, designing, and implementing advanced analytics capabilities. These capabilities include batch and streaming analytics, machine learning models, natural language generation, and other emerging technologies in the field of advanced analytics.Requirements Bachelor’s degree in Computer Science or similar field 4+ years of experience in traditional and modern Big Data technologies (HDFS, Hadoop, Hive, Pig, Sqoop, Kafka, Apache Spark, hBase, Oozie, No SQL databases)Experience in Java/Python/Scala Experience extracting/querying/joining large data sets at scale Experience building data platforms using Azure stack Experience building data ingestion pipelines using Azure Data Factory to ingest structured and unstructured data Strong knowledge on Azure Storage schematics such as Gen1 and Gen2Experience in harmonizing raw data into a consumer-friendly format using Azure Databricks Knowledge of Azure networking, security, key vaults, etc. Experience in data wrangling, advanced analytic modeling, and AI/ML capabilities is preferred Experience utilizing Snowflake to build data marts with the data residing in Azure storage is a plusStrong communication and organizational skills
BenefitsThis position offers an excellent opportunity for significant career development in a fast-growing and challenging entrepreneurial environment with a high degree of individual responsibility."
Sr. Data Engineer,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3729201419/?eBP=JOB_SEARCH_ORGANIC&refId=kONGqVOhoqrjpz5UVPcqkQ%3D%3D&trackingId=eTDU7jRrCas8Oyl%2Fsk%2B8jA%3D%3D&trk=flagship3_search_srp_jobs,3729201419,"About the job
            
 
Title -Sr. Data EngineerLocation- RemoteLinkedin must.Please share 2 best candidate.JD-A minimum of 12 years experience in Data Engineering.A minimum of 5 years in Azure.Must have excellent communication skills.We are looking for a talented and experienced Azure Data Engineer with expertise in Azure Databricks, Azure Synapse Analytics, Azure Data Factory, RDBMS, and NoSQL databases. The ideal candidate will play a crucial role in designing, implementing, and optimizing data solutions on the Azure platform to support our data-driven initiatives.Key ResponsibilitiesData Pipeline Development: Design, develop, and maintain data pipelines using Azure Data Factory and Azure Databricks/Spark to efficiently extract, transform, and load (ETL) data from various sources into data lakes and data warehouses.Data Modeling: Create and manage data models and schemas within Azure Synapse Analytics, NoSQL (Cosmos, MongoDB) to ensure data accuracy, performance, and scalability.Data Integration: Collaborate with cross-functional teams to integrate data from diverse sources, including RDBMS (e.g., Oracle, SQL Server) and NoSQL databases (e.g., MongoDB, Cosmos DB).Data Migration : Lead data migration projects, including data extraction, transformation, and loading (ETL), from on-premises systems and other cloud platforms to Azure. Ensure data quality, accuracy, and consistency throughout the migration process.Data Quality and Profiling: Implement data quality checks, Data Profiling and Cleansing to maintain data integrity, security, and compliance with industry standards and regulations.QualificationsBachelor's degree in Computer Science, Information Technology, or a related field. Master's degree preferred.Proven experience as a data engineer with a strong focus on Azure data services.Proficiency in Azure Databricks/Spark, Azure Synapse Analytics, Azure Data Factory, and other Azure data-related tools.Strong SQL skills and experience with data modeling in both RDBMS and NoSQL databases.Familiarity with data warehousing concepts and best practices.Knowledge of data governance, security, and compliance standards.Programming skills in languages such as Python and Microservices (Python) is desired.Excellent problem-solving and communication skills.Strong Databricks experience in AWS/GCP experience can also be considered.Ability to work independently and collaboratively within a team environment."
Workplace Analytics Engineer,Canonical,"San Antonio, TX (Remote)",https://www.linkedin.com/jobs/view/3738244831/?eBP=JOB_SEARCH_ORGANIC&refId=kONGqVOhoqrjpz5UVPcqkQ%3D%3D&trackingId=B0t48V2MwhoqBeAEZ5GqNg%3D%3D&trk=flagship3_search_srp_jobs,3738244831,"About the job
            
 
Bring your people analytics, social science research and data mining/science skills to a unique team seeking to understand, and shape, the future of the digital workplace. We are interested in technology, of course, but we are also interested in the human mission of enabling the world's brightest and hardest working people to live where they want and work from anywhere. Most of our colleagues could move to a tech hub but they choose Canonical because of our mission and our approach to the workplace.We'd like to understand what really makes a distributed, remote-first workplace work. We think we're pretty good at this (being remote first for almost 20 years), but we know there is a lot still to understand, and the frontier of possibility continues to move outward. We'd like to invest in research, analytics and tooling which raises the bar even further for remote collaboration and organisation.If we are able to build tools that meaningfully improve our cooperation and our satisfaction, then we intend to share our insights with the world, both as a narrative and as SAAS or open source that helps other companies follow in our footsteps.The role of a Workplace Analytics Engineer at CanonicalSupport analytics and data mining in a cross-disciplinary team of organisational psychologists, web front end engineers, back end engineers and statistics / analytics experts to help us build a new definition for the 21st century digital workplace. Collaborate to figure out what really drives productivity, effectiveness and happiness in a remote-first globally distributed company.In addition to your existing people analytics work experience, this role will combine your skills in psychology, data analytics and visualisation, to help create a more effective workplace.Location: This role will be based remotely in the AMER region.All applicants applying must be legally authorized to work in the United States, as we cannot offer visa sponsorship for this job position.What your day will look like Utilize advanced data analytics to understand how we hire and how we work (productivity, happiness and effectiveness) across a global, remote first organisationFocus on quantitative and qualitative data analytics to find insights and meaningful business outcomesTell the story from the insights through dashboards, visualizations and presentationsDesign and conduct research into trends shaping talent science and remote workCollaborate with stakeholder teams (ex., engineering, information systems, etc) to improve the data and tool ecosystem supporting our digital workplace
What we are looking for in you Background in data science, mathematics, actuarial science, or engineeringFirst work experience in People Analytics Knowledge in advanced statistics, data sciences, coding/scripting languages (Python, R, etc), and databases (SQL, etc)Strength in data analytics and visualization (Looker Studio, Tableau, etc)Ability to translate business questions to key research objectivesAbility to identify the best methodology to execute research, synthesize and analyse findingsExcellent writing and communication skillsWillingness to examine the status quo and resilient in the face of challenges
What we offer youYour base pay will depend on various factors including your geographical location, level of experience, knowledge and skills. In addition to the benefits above, certain roles are also eligible for additional benefits and rewards including annual bonuses and sales incentives based on revenue or utilization. Our compensation philosophy is to ensure equity right across our global workforce.In addition to a competitive base pay, we provide all team members with additional benefits, which reflect our values and ideals. Please note that additional benefits may apply depending on the work location and, for more information on these, you can ask in the later stages of the recruitment process.🏠 Fully remote working environment - we've been working remotely since 2004!📚 Personal learning and development budget of 2,000USD per annum💰 Annual compensation review🏆 Recognition rewards🏝 Annual holiday leave👶 Parental Leave🧑‍💼 Employee Assistance Programme🧳 Opportunity to travel to new locations to meet colleagues at 'sprints'✈️ Priority Pass for travel and travel upgrades for long haul company eventsAbout CanonicalCanonical is a pioneering tech firm that is at the forefront of the global move to open source. As the company that publishes Ubuntu, one of the most important open source projects and the platform for AI, IoT and the cloud, we are changing the world on a daily basis. We recruit on a global basis and set a very high standard for people joining the company. We expect excellence - in order to succeed, we need to be the best at what we do.Canonical has been a remote-first company since its inception in 2004. Work at Canonical is a step into the future, and will challenge you to think differently, work smarter, learn new skills, and raise your game. Canonical provides a unique window into the world of 21st-century digital business.Canonical is an equal opportunity employerWe are proud to foster a workplace free from discrimination. Diversity of experience, perspectives, and background create a better work environment and better products. Whatever your identity, we will give your application fair consideration."
Remote - Need Sr. Data Software Engineer (x3) and Jr. Data Software Engineer (x2),Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3665295664/?eBP=JOB_SEARCH_ORGANIC&refId=kONGqVOhoqrjpz5UVPcqkQ%3D%3D&trackingId=9rbLlHYnZ5dBsHSe%2Fn%2FXKQ%3D%3D&trk=flagship3_search_srp_jobs,3665295664,"About the job
            
 
Job: Sr. Data Software Engineer (x3) and Jr. Data Software Engineer (x2)Location: RemoteDuration: 6 month CTH (USC/GC only)Top Skills SQLAzureData FactoryDBTETL workProven ability to complete projects in a timely manner while clearly measuring progressStrong software engineering fundamentals (data structures, algorithms, async programming patterns, object-oriented design, parallel programming) Strong understanding and demonstrated experience with at least one popular programming language (.NET or Java) and SQL constructs.Experience writing and maintaining frontend client applications, Angular preferredStrong experience with revision control (Git)Experience with cloud-based systems (Azure / AWS / GCP).High level understanding of big data design (data lake, data mesh, data warehouse) and data normalization patternsDemonstrated experience with Queuing technologies (Kafka / SNS / RabbitMQ etc)Demonstrated experience with Metrics, Logging, Monitoring and Alerting toolsStrong communication skillsStrong experience with use of RESTful APIsHigh level understanding of HL7 V2.x / FHIR based interface messages.High level understanding of system deployment tasks and technologies. (CI/CD Pipeline, K8s, Terraform).
Project/Day To Day Communicate with business leaders to help translate requirements into functional specificationDevelop broad understanding of business logic and functionality of current systemsAnalyze and manipulate data by writing and running SQL queriesAnalyze logs to identify and prevent potential issues from occurringDeliver clean and functional code in accordance with business requirementsConsume data from any source, such a flat files, streaming systems, or RESTful APIs Interface with Electronic Health RecordsEngineer scalable, reliable, and performant systems to manage dataCollaborate closely with other Engineers, QA, Scrum master, Product Manager in your team as well as across the organizationBuild quality systems while expanding offerings to dependent teamsComfortable in multiple roles, from Design and Development to Code Deployment to and monitoring and investigating in production systems."
Data Engineer (3) - REMOTE - Active TS/SCI Clearance with Security Clearance,ClearanceJobs,"Alexandria, VA (Remote)",https://www.linkedin.com/jobs/view/3780558585/?eBP=JOB_SEARCH_ORGANIC&refId=kONGqVOhoqrjpz5UVPcqkQ%3D%3D&trackingId=D%2BwG4fs%2FP%2FDFZXGuGwik8Q%3D%3D&trk=flagship3_search_srp_jobs,3780558585,"About the job
            
 
Candidates must hold an active TS/SCI clearance and live in MD, DC or VA We are currently seeking a Data Engineer with data pipeline expertise for a full-time, permanent position. This is a hybrid role that is primarily remote work. Candidates must live in VA, MD or DC in order to work onsite when needed. The Data Engineers will support the Office of Intelligence and Analysis (I&A) at DHS, this role will be part of a team focused on supporting the development of a new capability for an I&A mission customer. Duties: Develop and design data pipelines to support an end-to-end solutionDevelop and maintain artifacts i.e. schemas, data dictionaries, and transforms related to ETL processesIntegrate data pipelines with AWS cloud services to extract meaningful insightsManage production data within multiple datasets ensuring fault tolerance and redundancyDesign and develop robust and functional dataflows to support raw data and expected dataProvide Tier 3 technical support for deployed applications and dataflowsExperience with cloud message APIs and usage of push notificationsCollaborate with the rest of data engineering team to design and launch new features. Includes coordination and documentation of dataflows, capabilities, etc.Keen interest in learning and using the latest software tools, methods, and technologies to solve real world problem sets vital to national security Skills: Database administration and development experience will be a plus for consideration. System One, and its subsidiaries including Joulé, ALTA IT Services, CM Access, and MOUNTAIN, LTD., are leaders in delivering outsourced services and workforce solutions across North America. We help clients get work done more efficiently and economically, without compromising quality. System One not only serves as a valued partner for our clients, but we offer eligible employees health and welfare benefits coverage options including medical, dental, vision, spending accounts, life insurance, voluntary plans, as well as participation in a 401(k) plan. System One is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, age, national origin, disability, family care or medical leave status, genetic information, veteran status, marital status, or any other characteristic protected by applicable federal, state, or local law."
Workplace Analytics Engineer,Canonical,"Tulsa, OK (Remote)",https://www.linkedin.com/jobs/view/3738245748/?eBP=JOB_SEARCH_ORGANIC&refId=kONGqVOhoqrjpz5UVPcqkQ%3D%3D&trackingId=VUVP%2FVCNOTWwz6Sbr3GbbQ%3D%3D&trk=flagship3_search_srp_jobs,3738245748,"About the job
            
 
Bring your people analytics, social science research and data mining/science skills to a unique team seeking to understand, and shape, the future of the digital workplace. We are interested in technology, of course, but we are also interested in the human mission of enabling the world's brightest and hardest working people to live where they want and work from anywhere. Most of our colleagues could move to a tech hub but they choose Canonical because of our mission and our approach to the workplace.We'd like to understand what really makes a distributed, remote-first workplace work. We think we're pretty good at this (being remote first for almost 20 years), but we know there is a lot still to understand, and the frontier of possibility continues to move outward. We'd like to invest in research, analytics and tooling which raises the bar even further for remote collaboration and organisation.If we are able to build tools that meaningfully improve our cooperation and our satisfaction, then we intend to share our insights with the world, both as a narrative and as SAAS or open source that helps other companies follow in our footsteps.The role of a Workplace Analytics Engineer at CanonicalSupport analytics and data mining in a cross-disciplinary team of organisational psychologists, web front end engineers, back end engineers and statistics / analytics experts to help us build a new definition for the 21st century digital workplace. Collaborate to figure out what really drives productivity, effectiveness and happiness in a remote-first globally distributed company.In addition to your existing people analytics work experience, this role will combine your skills in psychology, data analytics and visualisation, to help create a more effective workplace.Location: This role will be based remotely in the AMER region.All applicants applying must be legally authorized to work in the United States, as we cannot offer visa sponsorship for this job position.What your day will look like Utilize advanced data analytics to understand how we hire and how we work (productivity, happiness and effectiveness) across a global, remote first organisationFocus on quantitative and qualitative data analytics to find insights and meaningful business outcomesTell the story from the insights through dashboards, visualizations and presentationsDesign and conduct research into trends shaping talent science and remote workCollaborate with stakeholder teams (ex., engineering, information systems, etc) to improve the data and tool ecosystem supporting our digital workplace
What we are looking for in you Background in data science, mathematics, actuarial science, or engineeringFirst work experience in People Analytics Knowledge in advanced statistics, data sciences, coding/scripting languages (Python, R, etc), and databases (SQL, etc)Strength in data analytics and visualization (Looker Studio, Tableau, etc)Ability to translate business questions to key research objectivesAbility to identify the best methodology to execute research, synthesize and analyse findingsExcellent writing and communication skillsWillingness to examine the status quo and resilient in the face of challenges
What we offer youYour base pay will depend on various factors including your geographical location, level of experience, knowledge and skills. In addition to the benefits above, certain roles are also eligible for additional benefits and rewards including annual bonuses and sales incentives based on revenue or utilization. Our compensation philosophy is to ensure equity right across our global workforce.In addition to a competitive base pay, we provide all team members with additional benefits, which reflect our values and ideals. Please note that additional benefits may apply depending on the work location and, for more information on these, you can ask in the later stages of the recruitment process.🏠 Fully remote working environment - we've been working remotely since 2004!📚 Personal learning and development budget of 2,000USD per annum💰 Annual compensation review🏆 Recognition rewards🏝 Annual holiday leave👶 Parental Leave🧑‍💼 Employee Assistance Programme🧳 Opportunity to travel to new locations to meet colleagues at 'sprints'✈️ Priority Pass for travel and travel upgrades for long haul company eventsAbout CanonicalCanonical is a pioneering tech firm that is at the forefront of the global move to open source. As the company that publishes Ubuntu, one of the most important open source projects and the platform for AI, IoT and the cloud, we are changing the world on a daily basis. We recruit on a global basis and set a very high standard for people joining the company. We expect excellence - in order to succeed, we need to be the best at what we do.Canonical has been a remote-first company since its inception in 2004. Work at Canonical is a step into the future, and will challenge you to think differently, work smarter, learn new skills, and raise your game. Canonical provides a unique window into the world of 21st-century digital business.Canonical is an equal opportunity employerWe are proud to foster a workplace free from discrimination. Diversity of experience, perspectives, and background create a better work environment and better products. Whatever your identity, we will give your application fair consideration."
Senior Data Engineer II - Product Data,OncoHealth,United States (Remote),https://www.linkedin.com/jobs/view/3778567485/?eBP=JOB_SEARCH_ORGANIC&refId=kONGqVOhoqrjpz5UVPcqkQ%3D%3D&trackingId=U7UNsNfQRZiTOBnP14EIXA%3D%3D&trk=flagship3_search_srp_jobs,3778567485,"About the job
            
 
About OncoHealthOncoHealth is a leading digital health company dedicated to helping health plans, employers, providers, and patients navigate the physical, mental, and financial complexities of cancer through technology enabled services. Supporting more than 8 million people in the US and Puerto Rico, OncoHealth offers digital solutions for treatment review and virtual care across all cancer types.About The RoleThe Senior Data Engineering II is primarily responsible for development and maintenance of data products and pipelines which support Oncohealth’s OneUM and Iris offerings. This role is a technical leader who participates in cross-functional projects, designs and implements scalable data solutions, and mentors junior engineers and analysts. The ideal candidate will display demonstratable experience with Databricks, SQL, Python, and data pipelines (Airflow/Prefect, ADF, Workflows), as well as a working knowledge of event driven architecture and APIs. Technical leader for data engineering efforts within the enterprise data and analytics team to provide data pipeline development and maintenance utilizing best practices, CI/CD, and test-driven developmentImplement modern data pipelines supporting healthcare technical services on public cloud (Python, Databricks, SQL Server, Azure)Deliver innovative data solutionsReview and guide design, development, and support of all data systemsConsult on and implement data security protocolsParticipate in production and incident management for OneUM and IrisCollaboratively work with functional leadership, business users, and engineers across the organization to develop data products, identify and track key milestones, and implement solutions to enable data-drive culture (20%)Partner with DevOps, security, compliance, and technical teams in multiple lines of business to create innovative technical solutions within Oncohealth’s product offeringsCollaborate with business users, clients, and vendors to translate complex business requirements into data solutionsCoordinate with data governance and data architects to create maintainable and scalable solutionsEstablish and maintain a culture that emphasizes two-way feedback, continuous quality improvement, and individual and team developmentAssist with training and onboarding new hires, facilitating their assimilation into the team and work environment
About You Bachelor’s degree or relevant experience required. MBA or related Master’s preferred.7+ years of data engineering experience with project and team leadership, or relevant educational attainment required.The ideal candidate projects high energy, possesses a passion for state-of-the-art technology, and speaks comfortably from a technical perspective with clients, business partners, and across their team.Strong technical security expertise and systems management, with experience in an agile operating model.Demonstratable knowledge of creating and managing technical data products in support of enterprise initiatives.Demonstrated ability to analyze complex business needs and recommend practical business solutions is required.Must be familiar with healthcare data including claims and eligibility data.Knowledge of medical ontologies. Ability to understand data de-identification and skilled with de-identified data linking.
About the LocationOncoHealth is committed to remote, hybrid or in office work options. The majority of the team will be remote or in hybrid work arrangements with offices in Atlanta, GA and Guaynabo, PR. We are open to employees nationwide, but work primarily in the Eastern and Central Time Zones.Our CultureTaking ownership of quick action, critically thinking through the needs, and working well with others are key competencies of team member success. Our leadership is dedicated to building a culture based on respect, clinical excellence, innovation – all with a focused mission of putting patients first!We offer a full benefit package on your first day, along with a company bonus. You may visit or work from our very modern and engaging offices, and experience a fun, collaborative environment where social activities and community events matter. We enjoy being together!OncoHealth is committed to providing an environment of mutual respect where equal employment opportunities are available to all applicants and team members without regard to race, color, religion, marital status, age, national origin, ancestry, physical or mental disability, medical condition, pregnancy, genetic information, gender, sexual orientation, gender identity or expression, veteran status, or any other status protected under federal, state, or local law. All employment decisions are based on qualifications, merit, and business need.The OpportunityThe cost of cancer related medical services and prescription drugs in the United States is expected to reach $246 billion by 2030. OncoHealth has enjoyed rapid growth over the past 3 years and seeks smart, collaborative people to join its team. We have just under 250 team members, so we can move swiftly but precisely to the market needs of our customers. Strongly backed financially by Arsenal Capital Partners & McKesson Corporation, we remain in an investment and growth mode. This means we are open-minded to how we get the work done – now is the perfect time to talk to us!Our Current SolutionsThrough the use of OncoHealth's utilization management system, OneUM, our customers can use a single e-Prior Authorization portal for all oncology drug request and treatments. Our system improves quality of care, reduces provider abrasion and gives health plans visibility into the total cost of oncology treatment.OncoHealth offers Oncology Insights Pro, an analytic software solution that enables health plans to use data and analytics to improve oncology programs. Using real world data, our engineers normalize data to create analytic dashboards with drill down compatibilities. The data is the paired with expert guidance providing the strategies an insight needed to keep up with the continuing evolving cancer treatment landscape.OncoHealth offers Pharmacy Consulting services to health plans and pharmaceutical companies. New cancer treatments are entering the market at an unrelenting pace. Since 2018, the FDA approved 121 new cancer applications including 49 novel cancer drug entities. Our Board-Certified Oncology Pharmacologists can help health plans update drug policies, offer utilization management and formulary advice, and development training for staff.OncoHealth's latest offering is Iris, a digital telehealth platform that delivers personalized, oncology-specific support to navigate the physical symptoms and emotional challenges caused by cancer and cancer treatment. Powered by technology, staffed 24X7, and delivered with empathy, Iris allows patients to connect with trained oncology experts and receive personalized, oncology-specific telehealth support."
Collibra Ranger Certified Data Engineer,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3760880084/?eBP=JOB_SEARCH_ORGANIC&refId=kONGqVOhoqrjpz5UVPcqkQ%3D%3D&trackingId=IBDh%2FtN7k0jMahfVEITnnQ%3D%3D&trk=flagship3_search_srp_jobs,3760880084,"About the job
            
 
100% remoteMust have a Collibra Ranger Certification (active or expired)We currently have a opportunity for a Data Engineer to lend specific subject matter knowledge around Collibra. Integrate ServiceNow with CollibraDevelop a custom connector bridging ServiceNow and CollibraProvide solutioning expertise in resolving technical challengesSupport the team as a Collibra administrator, overseeing admin access and creating customizationsDesign and implement APIs to enhance functionality and extend capabilities on the Collibra platform
Qualifications Demonstrated 4-6 years of data management experience in an enterprise environment.Capable of supporting the establishment of a Collibra operating model aligned with a data governance operating modelProficient in designing automated and bulk upload metadata import/export methodsSkilled in assessing, recommending, and implementing capabilities related to business and technical metadata, data lineage, data profiling, data quality improvement efforts, and issue/request management in Collibra.Collibra Ranger certified - required (would consider Active or Expired certification)"
"Data Engineer, Database Engineering",Experfy,"San Diego, CA (Remote)",https://www.linkedin.com/jobs/view/3646111644/?eBP=JOB_SEARCH_ORGANIC&refId=kONGqVOhoqrjpz5UVPcqkQ%3D%3D&trackingId=OYH7Vmg1YYlMqtc8xw8Rhg%3D%3D&trk=flagship3_search_srp_jobs,3646111644,"About the job
            
 
As a Data Engineer for our Data Platform Engineering team you will join skilled Scala/ Spark engineers and core database developers responsible for developing hosted cloud analytics infrastructure (Apache Spark-based), distributed SQL processingframeworks, proprietary data science platforms, and core database optimization. This team is responsible for building the automated, intelligent, and highly performant query planner and execution engines, RPC calls between datawarehouse clusters, shared secondary cold storage, etc. This includes building new SQL features and customer-facing functionality, developing novel query optimization techniques for industry-leading performance, and building a databasesystem that's highly parallel, efficient and fault-tolerant. This is a vital role reporting to exec leadership and senior engineering leadershipRequirementsResponsibilities: Writing Scala code with tools like Apache Spark + Apache Arrow + Apache Kafka to build a hosted, multi-cluster data warehouse for Web3Developing database optimizers, query planners, query and data routing mechanisms, cluster-to-cluster communication, and workload management techniquesScaling up from proof of concept to ""cluster scale"" (and eventually hundreds of clusters with hundreds of terabytes each), in terms of both infrastructure/architecture and problem structureCodifying best practices for future reuse in the form of accessible, reusable patterns, templates, and code bases to facilitate meta data capturing and managementManaging a team of software engineers writing new code to build a bigger, better, faster, more optimized HTAP database (using Apache Spark, Apache Arrow, Kafka, and a wealth of other open source data tools)Interacting with exec team and senior engineering leadership to define, prioritize, and ensure smooth deployments with other operational componentsHighly engaged with industry trends within analytics domain from a data acquisition processing, engineering, management perspectiveUnderstand data and analytics use cases across Web3 / blockchains
Skills & Qualifications Bachelor's degree in computer science or related technical field. Masters or PhD a plus6+ years experience engineering software and data platforms / enterprise-scale data warehouses, preferably with knowledge of open source Apache stack (especially Apache Spark, Apache Arrow, Kafka, and others)3+ years experience with Scala and Apache Spark (or Kafka)A track record of recruiting and leading technical teams in a demanding talent marketRock solid engineering fundamentals; query planning, optimizing and distributed data warehouse systems experience is preferred but not requiredNice to have: Knowledge of blockchain indexing, web3 compute paradigms, Proofs and consensus mechanisms... is a strong plus but not requiredExperience with rapid development cycles in a web-based environmentStrong scripting and test automation knowledgeNice to have: Passionate about Web3, blockchain, decentralization, and a base understanding of how data/analytics plays into this
Apply for this job"
Senior Data Engineer,Quantori,"Cambridge, MA (Remote)",https://www.linkedin.com/jobs/view/3557434523/?eBP=JOB_SEARCH_ORGANIC&refId=kONGqVOhoqrjpz5UVPcqkQ%3D%3D&trackingId=vHLcPFjII%2F73Dk2S9p3PeQ%3D%3D&trk=flagship3_search_srp_jobs,3557434523,"About the job
            
 
Location:Quantori is an international team: we have colleagues who work not only from office but also remotely from all over the world.ResponsibilitiesData Analysis and organizing raw data: structured, semi-structured, unstructured, image type of dataBuilding data systems and pipelinesPreparing data for prescriptive and predictive modellingPipelines from data ingestion to consumption within a hybrid big data architecture, using Cloud Native AWS, Java, Python, Scala, R, SQL, etc.AWS Cloud services in terms of building data pipeline and monitoringCollaborating with data scientists and architects across various projectsWhat We ExpectExperience with Data lake, building data warehouse ETL, and/or leveraging Cloud based servicesGood expertise in SparkCloud Database related experience (noSQL, Relational DBs)Knowledge of any of these cloud platforms: AWS, Azure, GCPAbility to write robust code with PythonUnderstanding of data structures, data modeling, and software architectureStrong intermediate English or higher (B1+)Analytical and problem solving skillsNice To HaveExperience in development using R and/or JavaWe OfferCompetitive compensationRemote or office workFlexible working hoursOne-on-one English lessonsHealthcare benefits: medical insurance and paid sick leaveContinuous education, mentoring, and professional development programsA team with an excellent tech expertiseCertifications paid by the company"
Data/ML Engineer,eSolutionsFirst,United States (Remote),https://www.linkedin.com/jobs/view/3743839661/?eBP=JOB_SEARCH_ORGANIC&refId=kONGqVOhoqrjpz5UVPcqkQ%3D%3D&trackingId=7yb1PRjKLLdAuYdf1M0cZQ%3D%3D&trk=flagship3_search_srp_jobs,3743839661,"About the job
            
 
Position : C2HUSC/GC onlyThis position is a remote opportunity. Responsibilities:Job InformationData/Client Engineer is responsible for solution engineering of enterprise scale data management best practices. This includes patterns such as - modern data integration frameworks, building of scalable distributed systems using emerging cloud-based data design patterns. This role will be responsible for developing data integration tasks in data and analytics space. This position will report to director of data management group under Data Operations organization. This is an individual performer role.Key Job Functions• Demonstrate expert ability in implementing data warehouse solutions using Snowflake.• Building data integration solutions between transaction systems and analytics platform.• Expand data integration solutions to ingest data from internal and external sources and to further transform as per the business consumption needs.• Create security policies in Snowflake to manage fine grained access control• Develop tasks for a multitude of data patterns, e.g., real-time data integration, advanced analytics, machine learning, BI and reporting.• Lead POC efforts to build foundational AI/Client services for Predictive Analytics.• Building of data products by data enrichment and Client.• Be a team player and share knowledge with the existing team members.Qualifications:Education• Bachelor’s degree in computer science or a related fieldMinimum Experience• Minimum of 5-7 years of experience in building data driven solutions.• Applicants must be authorized to work in the US without requiring employer sponsorship currently or in the future. does not offer H-1B sponsorship for this position.Specialized Knowledge & Skills• Expertise in real-time data solutions, good to have knowledge of streams processing, Message Oriented Platforms and ETL/ELT Tools.• Strong scripting experience using Python and SQL• Working knowledge of foundational AWS compute, storage, networking and IAM.• Solid scripting experience in AWS using lambda functions. Good to have knowledge of CloudFormation template. Overall experience with AWS services should be over three years.• Hands on experience with popular cloud-based data warehouse platforms, viz. Redshift, Snowflake.• Experience with one or more data integration tools viz. Attunity (Qlik), AWS Glue ETL, Talend, Kafka etc.• Strong understanding of data security – authorization, authentication, encryption, and network security.• Experience in building data pipelines with related understanding of data ingestion, transformation of structured, semi-structured and unstructured data across cloud services• Hands on experience in using and extending machine learning framework and libraries, e.g, scikit-learn, PyTorch, TensorFlow, XGBoost etc.• Experience with AWS SageMaker family of services or similar tools to develop machine learning models• Demonstrated ability to be self-directed with excellent organization, analytical and interpersonal skills, and consistently meet or exceed deadline deliverables.• Strong communication skills to facilitate meetings and workshops to collect data, functional and technology requirements, document processes, data flows, gap analysis, and associated data to support data management/governance related efforts.• Knowledge and understanding of data standards and principles to drive best practices around data management activities and solutions.• Strong understanding of the importance and benefits of good data quality, and the ability to champion results across functions.• Ability to lead collaborative meetings which result in clearly documented outcomes, a concrete understanding of meeting attendee performance/reliability, and ongoing management & follow-up for action items.• Acts with integrity and proactively seeks ways to ensure compliance with regulations, policies, and procedures."
Azure Data engineer with Azure data factory and pyspark-- EST/CST-no corps --remote,RemoteWorker US,"Fort Worth, TX (Remote)",https://www.linkedin.com/jobs/view/3780836799/?eBP=JOB_SEARCH_ORGANIC&refId=kONGqVOhoqrjpz5UVPcqkQ%3D%3D&trackingId=Esr4l2lRsaeaUJkn5UrdFA%3D%3D&trk=flagship3_search_srp_jobs,3780836799,"About the job
            
 
NOTICE- Any pay ranges displayed are estimations. Actual pay is determined by an applicant's experience, technical expertise, and other qualifications as listed in the job description. All qualified applicants are welcome to apply. Senior Associate should have hand on PySpark, ADF, ADLS, Delta tables, SparkSQL, etc. Architects, in addition to the above must have Architecture experience designing solutions and frameworks ground up and not to just build data pipelines using the existing setup Top Skill Sets/ Experiences Required: These requirements must be strong and reflected on the resume: Azure Data Factory Azure Pyspark, Databricks APIs Your Skills & Experience: Demonstrable experience in enterprise level data platforms involving implementation of end to end data pipelinesGood communication and willingness to work as a teamHands-on experience with at least one of the leading public cloud data platforms (Amazon Web Services, Azure or Google Cloud)Experience with column-oriented database technologies (i.e. Big Query, Redshift, Vertica), NoSQL database technologies (i.e. DynamoDB, BigTable, Cosmos DB, etc.) and traditional database systems (i.e. SQL Server, Oracle, MySQL)Experience in architecting data pipelines and solutions for both streaming and batch integrations using tools/frameworks like Glue ETL, Lambda, Google Cloud DataFlow, Azure Data Factory, Spark, Spark Streaming, etc.Ability to handle multiple responsibilities simultaneously in leadership and contributing to tasks “hands-on”Understanding of data modeling, warehouse design and fact/dimension concepts Thanks Renu Goel 857-207-2676 renu.goel@yoh.com"
SDET Data Engineer_______________remote,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3730970839/?eBP=JOB_SEARCH_ORGANIC&refId=kONGqVOhoqrjpz5UVPcqkQ%3D%3D&trackingId=SPNfu%2BE%2FrkOPkujPFBF5cg%3D%3D&trk=flagship3_search_srp_jobs,3730970839,"About the job
            
 
Location: Remote BUT needs to live in one of these states - Florida, Georgia, Texas, North Carolina, Colorado, Virginia, TennesseeLength: 12 + MonthsVisa: USC or GCLet me know how many years of each experience they haveExperience with Azure APIs _____Strong doing multi-tenant data pipeline testing _____Hands-on working knowledge of parquet files and data lake ____Strong scripting using JavaScript or Typescript _____Strong C# for backend API Automation _____Advanced Microsoft SQL _____Experience using Entity Framework ______Testing API experience using Postman _____Azure DevOps experience (CI/CD pipelines) – Integration of tests and execution _______PowerShell scripting ____Playwright/ Jest preferred UI automation framework experience, but Selenium is fine ___This person ideally would be a software developer that wants to go into data engineering or vice versa.___Key Experience  Knowledge of Azure APIs (Azure AD Provisioning API, Azure Data Lake API, Azure Data Factory API. Multi-tenant data pipeline testing. Hands-on working knowledge of parquet files and data lake. .NET, QA, Data Engineering
Bonus PointsSr. Software Engineer interested in QA Automation and Data or SDET with an emphasis on Data.Day In The Life  60% Backend work – (Automation part / APIs) 30% UI Component – creating Automation for UI Test 10% Data Test (REAL Data Scenarios)
Work Profile  UI scripting (Playwright with Typescript) API scripting (Playwright with Typescript) Data testing (Use Entity framework) Maintaining or enhancing existing frameworks CI/CD Pipeline work using PowerShell scripting
Must Haves  Knowledge of Azure APIs (Azure AD Provisioning API, Azure Data Lake API, Azure Data Factory API) Strong doing multi-tenant data pipeline testing Hands-on working knowledge of parquet files and data lake Strong scripting using JavaScript or Typescript Strong C# for backend API Automation Advanced Microsoft SQL Experience using Entity Framework Testing API experience using Postman Azure DevOps experience (CI/CD pipelines) – Integration of tests and execution"
Workplace Analytics Engineer,Canonical,"Washington, United States (Remote)",https://www.linkedin.com/jobs/view/3735320529/?eBP=JOB_SEARCH_ORGANIC&refId=kONGqVOhoqrjpz5UVPcqkQ%3D%3D&trackingId=aESxt0LetInxQYS6MkL3RQ%3D%3D&trk=flagship3_search_srp_jobs,3735320529,"About the job
            
 
Bring your people analytics, social science research and data mining/science skills to a unique team seeking to understand, and shape, the future of the digital workplace. We are interested in technology, of course, but we are also interested in the human mission of enabling the world's brightest and hardest working people to live where they want and work from anywhere. Most of our colleagues could move to a tech hub but they choose Canonical because of our mission and our approach to the workplace.We'd like to understand what really makes a distributed, remote-first workplace work. We think we're pretty good at this (being remote first for almost 20 years), but we know there is a lot still to understand, and the frontier of possibility continues to move outward. We'd like to invest in research, analytics and tooling which raises the bar even further for remote collaboration and organisation.If we are able to build tools that meaningfully improve our cooperation and our satisfaction, then we intend to share our insights with the world, both as a narrative and as SAAS or open source that helps other companies follow in our footsteps.The role of a Workplace Analytics Engineer at CanonicalSupport analytics and data mining in a cross-disciplinary team of organisational psychologists, web front end engineers, back end engineers and statistics / analytics experts to help us build a new definition for the 21st century digital workplace. Collaborate to figure out what really drives productivity, effectiveness and happiness in a remote-first globally distributed company.In addition to your existing people analytics work experience, this role will combine your skills in psychology, data analytics and visualisation, to help create a more effective workplace.Location: This role will be based remotely in the AMER region.All applicants applying must be legally authorized to work in the United States, as we cannot offer visa sponsorship for this job position.What your day will look like Utilize advanced data analytics to understand how we hire and how we work (productivity, happiness and effectiveness) across a global, remote first organisationFocus on quantitative and qualitative data analytics to find insights and meaningful business outcomesTell the story from the insights through dashboards, visualizations and presentationsDesign and conduct research into trends shaping talent science and remote workCollaborate with stakeholder teams (ex., engineering, information systems, etc) to improve the data and tool ecosystem supporting our digital workplace
What we are looking for in you Background in data science, mathematics, actuarial science, or engineeringFirst work experience in People Analytics Knowledge in advanced statistics, data sciences, coding/scripting languages (Python, R, etc), and databases (SQL, etc)Strength in data analytics and visualization (Looker Studio, Tableau, etc)Ability to translate business questions to key research objectivesAbility to identify the best methodology to execute research, synthesize and analyse findingsExcellent writing and communication skillsWillingness to examine the status quo and resilient in the face of challenges
What we offer youYour base pay will depend on various factors including your geographical location, level of experience, knowledge and skills. In addition to the benefits above, certain roles are also eligible for additional benefits and rewards including annual bonuses and sales incentives based on revenue or utilization. Our compensation philosophy is to ensure equity right across our global workforce.In addition to a competitive base pay, we provide all team members with additional benefits, which reflect our values and ideals. Please note that additional benefits may apply depending on the work location and, for more information on these, you can ask in the later stages of the recruitment process.🏠 Fully remote working environment - we've been working remotely since 2004!📚 Personal learning and development budget of 2,000USD per annum💰 Annual compensation review🏆 Recognition rewards🏝 Annual holiday leave👶 Parental Leave🧑‍💼 Employee Assistance Programme🧳 Opportunity to travel to new locations to meet colleagues at 'sprints'✈️ Priority Pass for travel and travel upgrades for long haul company eventsAbout CanonicalCanonical is a pioneering tech firm that is at the forefront of the global move to open source. As the company that publishes Ubuntu, one of the most important open source projects and the platform for AI, IoT and the cloud, we are changing the world on a daily basis. We recruit on a global basis and set a very high standard for people joining the company. We expect excellence - in order to succeed, we need to be the best at what we do.Canonical has been a remote-first company since its inception in 2004. Work at Canonical is a step into the future, and will challenge you to think differently, work smarter, learn new skills, and raise your game. Canonical provides a unique window into the world of 21st-century digital business.Canonical is an equal opportunity employerWe are proud to foster a workplace free from discrimination. Diversity of experience, perspectives, and background create a better work environment and better products. Whatever your identity, we will give your application fair consideration."
Senior Data Engineer,Covalent Resource Group,"Austin, TX (Remote)",https://www.linkedin.com/jobs/view/3324317303/?eBP=JOB_SEARCH_ORGANIC&refId=kONGqVOhoqrjpz5UVPcqkQ%3D%3D&trackingId=jU0lJyTUteqPOFYxNlWUxA%3D%3D&trk=flagship3_search_srp_jobs,3324317303,"About the job
            
 
We're looking for a Senior Data Engineer to work on our massive data processing pipeline and lead our data lake and data warehouse building, to help us deliver more insights and scale our data infrastructure. You will have a chance to contribute to the company's evolving culture, bring innovative approaches and learn from your talented colleagues.Responsibilities Designs and develops Databricks Notebooks to support data ingestion, curation and provisioning of complex enterprise data to achieve analytics & reporting on our current technology stackDesigns and builds data extracts, integrations and transformations Provides successful deployment and provisioning of data solutions to required environmentDesigns and builds data architecture and applications that successfully enable speed, quality and efficient pipelinesInteracts with cross-functional customers and development team to gather and define requirements.Develops understanding of the data and builds business acumen.Reviews discrepancies in requirements and resolves with stakeholders.Identifies and recommends appropriate continuous improvement opportunities and ensures integrations are automated and have proper exception handling.Key team member of project team designing and deploying a ground up cloud data pipeline"
Data Engineer (Contract),Aputure,"Glendale, CA (Remote)",https://www.linkedin.com/jobs/view/3575816598/?eBP=JOB_SEARCH_ORGANIC&refId=kONGqVOhoqrjpz5UVPcqkQ%3D%3D&trackingId=HmjTJIUZwJO0PLkZEJKAAA%3D%3D&trk=flagship3_search_srp_jobs,3575816598,"About the job
            
 
Aputure is one of the fastest-growing cinema technology companies. Our equipment has emerged a global newcomer to watch for with hundreds of thousands of Aputure lights now being used on film sets worldwide. Beyond just products, our marketing team also works in a way that is equally visionary. Electing to create communities and content for filmmakers rather than advertisements, the Aputure A-Team is composed of like-minded creatives that genuinely enjoy working with filmmakers every day.The Data Engineer will be instrumental in delivering and maintaining the data pipelines and infrastructure, as well as building dashboards needed for business metrics as per Aputure's needs. As one of the early hires of this team, you will get to shape the project’s direction and success.The Role: Contribute to the data collection/data cleaning processValidate the metrics set out by Aputure’s executive teamArchitect, build, monitor and maintain data ETL pipelines that support Aputure’s business domainsImplement the data dashboards needed for business metrics delivery and interpretation. Integrate seamlessly with the existing Aputure’s data workflow and legacy systems. Implement deployment workflows to take prototype data systems to production. 
Required Qualifications: At least Bachelor’s in Computer Science or relevant fieldsStrong knowledge of data structures, ETL pipelines and data processing/cleaningStrong experience with data visualization libraries such as Tableau and Plotly2+ years of experience building production grade data platforms using AWS or GCP, and be willing to ramp up quickly on AliyunAdvanced knowledge of SQL and NoSQL database technologies (SQL, Postgres, Cassandra, Mongo etc.)Proficiency in Python, Spark, Kafka, RabbitMQ and related data engineering toolsProficiency in working with/implementing REST and GraphQL APIsWorking knowledge of Git, Docker, and associated deployment toolsAware of CI/CD best practicesAware of modern data offerings such as Databricks and SnowflakeExperience in participating in on-call rotations and dealing with hot fixes
Specifications: 6 months contract with potential for renewal or conversion to full time40 hours/week working hoursRemote eligible (Los Angeles presence strongly preferred for some in-person sync ups)Work authorization: US citizen or permanent residentDirect hire only - no agencies
Our mission is to make filmmaking better, more creative and more accessible for all. Ensuring a diverse and inclusive workplace where we learn from each other is core to Aputure’s values. We welcome people of different backgrounds, experiences, abilities and perspectives. We are an equal opportunity employer and strive to be the most supportive place to work.Salary: $50-$70/hour depending on experience"
Data Protection Engineer(Direct Hire/Remote),Connectria,"St Louis, MO (Remote)",https://www.linkedin.com/jobs/view/3761292976/?eBP=JOB_SEARCH_ORGANIC&refId=kONGqVOhoqrjpz5UVPcqkQ%3D%3D&trackingId=PzQj915%2BQz3X%2BmtQblc1Jg%3D%3D&trk=flagship3_search_srp_jobs,3761292976,"About the job
            
 
DescriptionABOUT USAt Connectria, LLC, we provide award-winning Cloud Computing and Managed Hosting from our world-class data centers in North America to more than 1,000 customers in 30+ countries throughout the world, and we help customers run their systems securely in Amazon’s AWS cloud and Microsoft’s Azure cloud.For over twenty years, Connectria’s ‘No Jerks Allowed’ philosophy has been the foundation of our culture. We are a company that believes in promoting from within and take an employee-first approach. Due to these practices, Connectria has won St. Louis Post-Dispatch Best Places to Work 9 times, Computerworld Best Places to Work in IT 9 times, HIRE Vets Medallion Award 2 times and The Nation’s Best and Brightest in Wellness Award.If you are looking for your next career opportunity and enjoy working in a fast-paced and growing business environment and want to be on a team with a “No Jerks Allowed” company philosophy that treats everyone with respect in a fun and supportive work environment, Connectria has an exciting opportunity for you!POSITION PROFILEThe Data Protection Engineer position is responsible for ensuring the protection of data for Connectria and its customers. The candidate will be responsible for implementing and maintaining these critical services, while also exhibiting a professional demeanor and positive attitude within a dynamic and fast-paced work environment.Key Responsibilities Of RoleThe candidate will be tasked with the following duties and responsibilities: Day-to-day monitoring, administration, and operational support of Connectria’s suite of data protection and migration technologies, including (but not limited to): Commvault, TSM/Spectrum Protect, Veeam, N2WS, EVault, Iron Mountain Cloud, Wasabi Cloud, Zerto, and AWS DRS/FSx.Manage all aspects of day-to-day tape operations including (but not limited to): ejecting tapes, replacing tapes, tracking tapes, and working with offsite tape services.Ensure up to date and complete documentation is kept as required for all products.Continually seek opportunities to increase customer satisfaction.Communicate with customers via phone, ticketing system, and/or virtual meetings to assist with technical questions or issues regarding data protection products.Implementing and supporting Connectria data protection and migration services in accordance with established policies and procedures.Required to participate in an off-hours on-call rotation and work off hours (including weekends and holidays) in support of upgrades, changes, and problem determination tasks.
RequirementsRequired Skills and Experience Extensive administrative and operational experience with one or more technologies (but not limited to): Commvault, TSM/Spectrum Protect, Veeam, N2WS, EVault, Zerto, and AWS DRS/FSx.Three or more years of total experience working in the Information Technology field.Communicate professionally with internal and external customers.Ability to multi-task effectively and manage time/resources efficiently, while delivering a critical service to our customers and to our own organization.Self-motivated and able to work effectively and professionally without direct supervision.Must have enthusiasm for providing the highest level of service possible to end-users and clients.Understanding of basic network concepts and protocols (i.e., ICMP, TCP, etc.) to aid in the troubleshooting of data protection products.Excellent troubleshooting skills and the ability to logically think through problem situations.Excellent verbal and written communication skills.Must be detail oriented and have the ability to document processes and procedures for others to follow.Ability to correctly follow documented processes and procedures created by other teammates.
Preferred Skills Previous experience with Azure or other data protection and migration technologies is preferred.Knowledge of data center concepts and procedures.Familiarity with Microsoft and Linux operating systems to allow for the installation, configuration, and troubleshooting of data protection products on those platforms.
EducationAssociates Degree in the technology or equivalent in trade school preferred.Salary: 74,316 MinimumWork AuthorizationConnectria, LLC will only employ those who are legally authorized to work in the United States. This is not a position for which sponsorship will be provided. Individuals with temporary visas such as E, F-1, H-1, H-2, L, B, J, or TN or who need sponsorship for work authorization now or in the future, are not eligible for hire.In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.Equal Opportunity EmployerConnectria, LLC is an Equal Employment/Affirmative Action employer. All qualified applicants will receive consideration of employment without regard to sex, race, color, religion, national origin, age, marital status, political affiliation, sexual orientation, gender identity or expression, genetic information, disability status, protected veteran status, or any other characteristic protected by federal, state, or local laws. We are committed to providing a workplace free of any discrimination or harassment. If you are an individual with a disability and you need an accommodation during the interview process, please reach out to your recruiter.This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation, and training. (2023)"
Senior Data Engineer,IHI Terrasun Solutions,"Chicago, IL (Remote)",https://www.linkedin.com/jobs/view/3673090223/?eBP=JOB_SEARCH_ORGANIC&refId=kONGqVOhoqrjpz5UVPcqkQ%3D%3D&trackingId=kkR2wcmTtC6O32IkL5RMJg%3D%3D&trk=flagship3_search_srp_jobs,3673090223,"About the job
            
 
At IHI Terrasun, we are at the forefront of changing the world with green energy solutions. To build on our incredible success, we need a driven, curious, and collaborative people to join our growing team.We know our most important assets are our people, and your role will be critical to our future success.Position SummaryThis position is responsible for the development, implementation, and maintenance of data systems for both data in motion and data at rest to monitor and control battery storage systems. In this role, you'll work with setting up systems to capture hardware metrics and track their performance for outage tracking and compliance for one of the largest collections of industrial automation data worldwide.What You'll Be Doing Create services to receive, translate, and send data in motion to data stores via APIs and query languages. Developing and deploying data pipelines using docker/containerization, Jenkins and Kubernetes. Participate in the entire software development lifecycle of a product from ideation, wireframing/prototyping, development, testing, deployment, commissioning, and long-term support. Own automated testing and deployment for products that you create. 
The ideal candidate would have the following experience: Developing in Python including libraries like pandas, the SciPy stack, request, and data tools in the context of data processing workflows, scripts, and services. Working in every stage of the data lifecycle from converting business specifications/requirements to technical solutions through deployment, and maintenance. Integrating software and services, including developing adaptors between different data protocols (e.g., MQTT to Prometheus exposition format) Creating scalable and performant systems. Analyzing messaging protocols using tools like tcpdump, WireShark, and custom data producers and consumers. Reading, writing, and converting data formats like json, yaml, csv, pandas dataframes, and relational databases. Working with Prometheus, PromQL, and Grafana. Developing and deploying Docker workflows. Working in a Linux environment including basic shell scripting and system monitoring. Participating in Agile development including scrum ceremonies, ticket management, GIT version control, unit testing, code reviews, and documentation. Experience in compiled languages like Go, Rust, or C++ are a plus but not required. 
Qualifications 5+ years experience in data engineeringBS, MS or PhD in CS, Engineering, Math, Physical Sciences or equivalent real-world experience. Ability to work independently, with a development team, and within large multi-team projects. Excited to learn and grow with new technologies and adaptability to handle evolving requirements. Has a shared sense of responsibility and ownership both in your code and the larger system that it operates in. Ability to communicate issues openly and honestly even when difficult. Previous Energy Storage System experience is preferred, power industry or military systems experience is a plus 
Work Environment IHI has its main office in Chicago, IL, however this position can be remotely located.  Those in the Chicago-land area have the flexible option of working from the office or remotely from home. Limited travel may be required for company All-Hands or other meetings. Travel expectations for this position are up to 5% within the US. 
The above job description identifies the essential job functions and skills needed by the person or persons assigned to this position. These job functions and skills are not intended to be a complete and exhaustive list of all responsibilities, duties and skills required. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential job functions. The information contained herein is subject to change at the company's discretion.About IHI Terrasun Solutions: IHI Terrasun Solutions is a subsidiary of IHI Corporation, a 165-year-old, $15 Billion organization with deep energy industry experience. IHI Terrasun Solutions is a solar + storage systems integration and lifecycle services provider with highly integrated hardware and software capabilities.The robust software and top tier energy storage solutions are developed by the expert team at IHI Terrasun Solutions. Employees have extensive industry knowledge and experience, and enthusiastically seek to build on IHI's advanced product offerings.To design systems, IHI Terrasun Solutions uses proprietary software that operates on the same algorithm later used to deploy the system in real-time. This end-to-end algorithm structure coupled with the support offered by a well-established parent organization enables IHI Terrasun Solutions to provide an advanced warranty to customers, reducing project risk and increasing clarity on system scheduling and deployment.With solar + storage expertise, robust service offerings, and technology-agnostic solutions, IHI Terrasun Solutions develops efficient and streamlined systems to achieve your energy storage goals.IHI Terrasun has over 480MWh of projects currently installed, contracted, and in construction with over 1GWh of projects in advanced phase of contracting.Benefits:Not only do our employees get the chance to work in a rapidly growing energy business with global impact, they also have access to some of the best benefits in the industry, including: 100% employer paid health, dental, and vision insurance for our premium Anthem Blue Cross PPO plan401(k) plan contribution matchingEmployer sponsored Life, AD&D, Short-Term and Long-Term Disability InsuranceTuition and continuing education stipendFantastic employee culture"
Azure Data engineer with Azure data factory and pyspark-- EST/CST-no corps --remote,RemoteWorker US,"Orlando, FL (Remote)",https://www.linkedin.com/jobs/view/3780834962/?eBP=JOB_SEARCH_ORGANIC&refId=kONGqVOhoqrjpz5UVPcqkQ%3D%3D&trackingId=Fw2Um0b8f8O3YKgNrKHiOQ%3D%3D&trk=flagship3_search_srp_jobs,3780834962,"About the job
            
 
NOTICE- Any pay ranges displayed are estimations. Actual pay is determined by an applicant's experience, technical expertise, and other qualifications as listed in the job description. All qualified applicants are welcome to apply. Senior Associate should have hand on PySpark, ADF, ADLS, Delta tables, SparkSQL, etc. Architects, in addition to the above must have Architecture experience designing solutions and frameworks ground up and not to just build data pipelines using the existing setup Top Skill Sets/ Experiences Required: These requirements must be strong and reflected on the resume: Azure Data Factory Azure Pyspark, Databricks APIs Your Skills & Experience: Demonstrable experience in enterprise level data platforms involving implementation of end to end data pipelinesGood communication and willingness to work as a teamHands-on experience with at least one of the leading public cloud data platforms (Amazon Web Services, Azure or Google Cloud)Experience with column-oriented database technologies (i.e. Big Query, Redshift, Vertica), NoSQL database technologies (i.e. DynamoDB, BigTable, Cosmos DB, etc.) and traditional database systems (i.e. SQL Server, Oracle, MySQL)Experience in architecting data pipelines and solutions for both streaming and batch integrations using tools/frameworks like Glue ETL, Lambda, Google Cloud DataFlow, Azure Data Factory, Spark, Spark Streaming, etc.Ability to handle multiple responsibilities simultaneously in leadership and contributing to tasks “hands-on”Understanding of data modeling, warehouse design and fact/dimension concepts Thanks Renu Goel 857-207-2676 renu.goel@yoh.com"
Data Analyst & Testing Engineer,Sonitalent Corp,United States (Remote),https://www.linkedin.com/jobs/view/3657150422/?eBP=JOB_SEARCH_ORGANIC&refId=kONGqVOhoqrjpz5UVPcqkQ%3D%3D&trackingId=%2F4WOZL3CjK8Ubofv%2F6%2FYfQ%3D%3D&trk=flagship3_search_srp_jobs,3657150422,"About the job
            
 
Role: Data Analyst & Testing EngineerLocation: Washington, DC 100% Remote/ Must be in EST Time ZoneDuration: 6-12+ monthsVisa: USC, GC, GC EAD, H4 EAD USC/GC would be highly preferredInterview: MS Team VideoPositions: 2They must have Freddie Mac or Fannie Mae experience or (Ginnie Mae or Ginny Mae).Mortgage OR Financial, Oracle, SQL, Gherkin, Python OR Java, Data Analysis, Modeling, Linux, AWSTop Technical Skills Secondary Mortgage Or Financial experienceOracle/SQL advanced SQLTest Case, Plans, Strategy Acceptance CriteriaBDD...GherkinPython.... for scripting for Data analysisPython and Java..... Ability to read for code analysisData ModelingAWS/Linux/Unix
Top 3 Soft Skills Good Communications... to all levelsAgileStrong Documentation (for internal controls)
Company / Project Description / Business DriverSupporting AWS Cloud Migration, Enterprise Mandate to get to the CloudJob Description Analyze, document and articulate business requirements for complex mathematical, business, and financial modeling logic for software coding. Design and execute test cases for modeling and analytical software applications to ensure they meet business needs and model requirements.Develop detailed specifications of application and document in a form that it can be used for coding application. This includes architecture diagrams, process flows, and other information or processes needed to describe required system changes for development, QA, and other internal customers.Collaborate with managers or practitioners in the business unit to determine systems requirements and functionalities needed in new or revised application.Extract data requirements through various methods including individual and group discussions, independent data analysis and by extracting from a suite of mathematical functions and code.Confer with business units and technical staff to understand data usage, lineage and attributes. Perform or review coding done to render specifications into application functionalities, screens, or outputs.Develop test plan and/or test application in development status or debug application in production mode.Document or review documentation of steps in specification development, coding, testing and user acceptance for future reference and for internal control purposes.May provide support to applications in production by tracking production problems and troubleshooting them to sustain application in production.Participate in project meetings to plan rewrite of addition to application in production or being revised for release into production.May mentor or provide technical guidance to less experienced staff.Lead cross functions to ensure application enhancements quality meets expected business results. May lead test strategy and facilitate customer and end user testing through test & learn development iterations and closing the loop on customer feedback. Manage end to end business process impacts for cross functional/complex solutionsPromote productive relationships between stakeholders and technology partners, providing clarity & ensuring processes are streamlined."
Data Engineer 3,"Resource Informatics Group, Inc",United States (Remote),https://www.linkedin.com/jobs/view/3727209338/?eBP=JOB_SEARCH_ORGANIC&refId=FQN8TYAI4tjJr8%2B7nGayZQ%3D%3D&trackingId=1R0ziI7K9rh%2Fe5ilbUcyfQ%3D%3D&trk=flagship3_search_srp_jobs,3727209338,"About the job
            
 
Job DescriptionDescription: This candidate will focus on data integrations and mapping between Snowflake & ThoughtSpot.Candidate will be responsible for evaluating existing dashboards & data, mapping/remapping new data to fit those dashboards, and creating/optimizing views in Snowflake.In addition, this role may interface with Dealers or Business
 Typical Day: This position can be remote, but expected to work in US time zone.Candidate will work closely with the DataOps team lead to understand the current use-cases, establish a plan for migration to Snowflake data sources, and be autonomous in delivery of that work.15% requirements gathering, 15% meetings & collaborative work, 70% technical work in Snowflake & ThoughtSpot
Technical Skills Required: Snowflake (creating/modifying/optimizing views & procs)ThoughtSpot (Data Visualization)Data Mapping/Modeling
(Desired) Familiarity with PythonExperience in handling dealer/customer data
Soft Skills Required:(Required) Will communicate with business unit partners, Dealers, and internal technical teams so solid communication & presentation skills are important
Education Requirements: 4 year degree in Computer Science or equivalent experience plus 5-7 years exp. (additional expperience acceptable in lieu of education)
Travel: None"
Senior Staff Data Engineer,Act Digital Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3643169587/?eBP=JOB_SEARCH_ORGANIC&refId=FQN8TYAI4tjJr8%2B7nGayZQ%3D%3D&trackingId=%2BZnUrQhjrJytV0Fx1lP0ew%3D%3D&trk=flagship3_search_srp_jobs,3643169587,"About the job
            
 
Our Client is looking for a Senior Staff Data Engineer to lead the design, development, and maintenance of our data infrastructure. You will lead a team of engineers and data analysts to build data pipelines, implement data models, and optimize the performance of our data systems. You will be responsible for ensuring that our data is accurate, reliable, and accessible to the right people at the right time. This is a leadership role, and you will be responsible for mentoring and guiding junior team members.Duties & ResponsibilitiesLead the design, development, and maintenance of our data infrastructure, including data pipelines, databases, and data warehouses.Work with cross-functional teams to define data requirements and design data models that meet those requirementsOptimize the performance of our data systems, including query performance and data ingestion speedEnsure data accuracy, reliability, and accessibility, and implement processes to monitor and maintain data qualityMentor and guide junior team members, providing technical guidance and career development supportStay up-to-date with industry trends and emerging technologies in data engineering and apply them to our data infrastructureTechnical SkillsExpertise in SQL and NoSQL databases, data warehousing, and data modeling.Fluency in Python and SQL. Additional data or system languages (e.g. Java, Scala, Go, R) a plus.Experience designing cloud-based data systems such as AWS, Azure, or GCP. Multi-cloud experience, experience with infrastructure-as-code and/or DevOps tooling a plus.Experience with data modeling and data governance best practices.Experience with data visualization tools such as Tableau or Power BI.Minimum QualificationsBachelor's or Master's degree in computer science, software engineering, or a related field.7+ years of experience in data engineering, with a proven track record of designing and implementing data systems at scale.Strong analytical and problem-solving skills.Excellent communication and leadership skills, with the ability to mentor and guide junior team members.Preferred QualificationsMaster's degree in Computer Science, Engineering, or a related field.Experience in machine learning or data science.All positions with the client require an applicant who has accepted an offer to undergo a background check. The specific checks are based on the nature of the position. Background checks may include some or all of the following: SSN/SIN validation, education verification, employment verification, and criminal check, search against global sanctions and government watch lists, fingerprint verification, credit check, and/or drug test. By applying for a position with the client, you understand that you will be required to undergo a background check should you be made an offer. You also understand that the offer is contingent upon successful completion of the background check and results consistent with client's employment policies. You will be notified during the hiring process which checks are required for the position.Client is an Equal Employment Opportunity/Affirmative Action employer and well committed to a diverse workforce. We do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, veteran status, and basis of disability or any other federal, state or local protected class.Pay Transparency Non-Discrimination Notice Client will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor's legal duty to furnish information."
Workplace Analytics Engineer,Canonical,"Tacoma, WA (Remote)",https://www.linkedin.com/jobs/view/3738245743/?eBP=JOB_SEARCH_ORGANIC&refId=FQN8TYAI4tjJr8%2B7nGayZQ%3D%3D&trackingId=lcbOAIXZJ8KBcQuaAF32pQ%3D%3D&trk=flagship3_search_srp_jobs,3738245743,"About the job
            
 
Bring your people analytics, social science research and data mining/science skills to a unique team seeking to understand, and shape, the future of the digital workplace. We are interested in technology, of course, but we are also interested in the human mission of enabling the world's brightest and hardest working people to live where they want and work from anywhere. Most of our colleagues could move to a tech hub but they choose Canonical because of our mission and our approach to the workplace.We'd like to understand what really makes a distributed, remote-first workplace work. We think we're pretty good at this (being remote first for almost 20 years), but we know there is a lot still to understand, and the frontier of possibility continues to move outward. We'd like to invest in research, analytics and tooling which raises the bar even further for remote collaboration and organisation.If we are able to build tools that meaningfully improve our cooperation and our satisfaction, then we intend to share our insights with the world, both as a narrative and as SAAS or open source that helps other companies follow in our footsteps.The role of a Workplace Analytics Engineer at CanonicalSupport analytics and data mining in a cross-disciplinary team of organisational psychologists, web front end engineers, back end engineers and statistics / analytics experts to help us build a new definition for the 21st century digital workplace. Collaborate to figure out what really drives productivity, effectiveness and happiness in a remote-first globally distributed company.In addition to your existing people analytics work experience, this role will combine your skills in psychology, data analytics and visualisation, to help create a more effective workplace.Location: This role will be based remotely in the AMER region.All applicants applying must be legally authorized to work in the United States, as we cannot offer visa sponsorship for this job position.What your day will look like Utilize advanced data analytics to understand how we hire and how we work (productivity, happiness and effectiveness) across a global, remote first organisationFocus on quantitative and qualitative data analytics to find insights and meaningful business outcomesTell the story from the insights through dashboards, visualizations and presentationsDesign and conduct research into trends shaping talent science and remote workCollaborate with stakeholder teams (ex., engineering, information systems, etc) to improve the data and tool ecosystem supporting our digital workplace
What we are looking for in you Background in data science, mathematics, actuarial science, or engineeringFirst work experience in People Analytics Knowledge in advanced statistics, data sciences, coding/scripting languages (Python, R, etc), and databases (SQL, etc)Strength in data analytics and visualization (Looker Studio, Tableau, etc)Ability to translate business questions to key research objectivesAbility to identify the best methodology to execute research, synthesize and analyse findingsExcellent writing and communication skillsWillingness to examine the status quo and resilient in the face of challenges
What we offer youYour base pay will depend on various factors including your geographical location, level of experience, knowledge and skills. In addition to the benefits above, certain roles are also eligible for additional benefits and rewards including annual bonuses and sales incentives based on revenue or utilization. Our compensation philosophy is to ensure equity right across our global workforce.In addition to a competitive base pay, we provide all team members with additional benefits, which reflect our values and ideals. Please note that additional benefits may apply depending on the work location and, for more information on these, you can ask in the later stages of the recruitment process.🏠 Fully remote working environment - we've been working remotely since 2004!📚 Personal learning and development budget of 2,000USD per annum💰 Annual compensation review🏆 Recognition rewards🏝 Annual holiday leave👶 Parental Leave🧑‍💼 Employee Assistance Programme🧳 Opportunity to travel to new locations to meet colleagues at 'sprints'✈️ Priority Pass for travel and travel upgrades for long haul company eventsAbout CanonicalCanonical is a pioneering tech firm that is at the forefront of the global move to open source. As the company that publishes Ubuntu, one of the most important open source projects and the platform for AI, IoT and the cloud, we are changing the world on a daily basis. We recruit on a global basis and set a very high standard for people joining the company. We expect excellence - in order to succeed, we need to be the best at what we do.Canonical has been a remote-first company since its inception in 2004. Work at Canonical is a step into the future, and will challenge you to think differently, work smarter, learn new skills, and raise your game. Canonical provides a unique window into the world of 21st-century digital business.Canonical is an equal opportunity employerWe are proud to foster a workplace free from discrimination. Diversity of experience, perspectives, and background create a better work environment and better products. Whatever your identity, we will give your application fair consideration."
Senior Data Engineer,MedArrive,"New York, NY (Remote)",https://www.linkedin.com/jobs/view/3673083833/?eBP=JOB_SEARCH_ORGANIC&refId=FQN8TYAI4tjJr8%2B7nGayZQ%3D%3D&trackingId=jMMAJseGcjIEvZU8WdeVUA%3D%3D&trk=flagship3_search_srp_jobs,3673083833,"About the job
            
 
MedArrive is a fast-paced and fast-growing start up on a simple mission: Improving people's lives by bringing more humanity to healthcare. Our enemy is an often soulless, transactional healthcare system that's increasingly engineering the vital human touch away from the experience - and often hard to access.We are looking for a seasoned data engineer to be a part of building a data-driven product and service culture at MedArrive.As a Senior Data Engineer on the MedArrive Data Science & Machine Learning team, you will be an integral part of building out the Data and Intelligence substrate that powers our platform from the ground up. You will work with amazing colleagues, brainstorm new ideas, and develop data processing techniques, pipelines, models, algorithms and visualizations to solve challenging problems and communicate insights that have a substantial impact.What you'll do: Design and build scalable and efficient data pipelines and ensure end to end orchestration (from data acquisition to analytics) in MedArrive's AWS/Databricks based data platform. Support pipeline modalities including unified batch, streaming, and API. Expand and integrate the MedArrive Intelligence Layer with application system wide event architecture and point to point API for real-time data. Enhance the data platform infrastructure, supporting IaC CI/CD, and ML Ops with Terraform on AWS and Databricks. Contribute to design and implementation of conceptual, physical, and logical data models based on understanding of MedArrive business processes, systems, data flows, dependencies, and relationships. Continuously learn and master new technologies and methods and creatively apply to real world problems. 
What you'll need: 5+ years of hands-on experience in a Data Engineering or Analytical Engineering roleBachelor's Degree in Computer Science, Engineering, Mathematics (or related technical field) Demonstrated ability to apply software development skills for data acquisition, data standardization, enrichment and monitoringProficiency with Python and/or Scala programming languagesProficiency with data engineering frameworks & tools (e.g. Apache Spark/Pyspark, SQL, Hive, Snowflake, DBT)Working experience supporting data pipelines on cloud platforms (e.g. AWS, Azure, GCP). 
Preferred qualifications: Experience building data pipelines with Spark on DatabricksExperience supporting real time, streaming data sourcesUnderstanding of healthcare data domains and workflows (e.g. Claims, Clinical, HL7)Experience configuring orchestration frameworks (e.g. Databricks, Airflow, Dagster, etc.)Experience with IaS CI/CD using Terraform on AWS
More About UsMedArrive exists to expand access to care, drive efficiency, and reduce healthcare expenses. We enable healthcare payers and providers to seamlessly extend care services into the home, unlocking access to high-quality healthcare for more people at a fraction of the cost.Our logistics and care management platform allows providers and payers to bridge the virtual care gap, integrating physician-led telemedicine with in-person care from EMS professionals, Nurses, Community Health Workers, Phlebotomists, and more. As a result, patients can access trusted medical expertise from their homes' comfort and safety without interruption to the continuity of care, ultimately resulting in better patient outcomes, a better-utilized healthcare workforce, and significant cost savings for patients and providers alike.MedArrive has more than 50k highly-skilled ""Field Providers'' including EMS professionals in its national network, and services span dozens of clinical use cases, including complex condition monitoring, transitional care and readmission prevention, vaccinations and immunizations, medication administration and much more.MedArrive is a proud Equal Opportunity Employer – we recruit, train, compensate and promote our team members based on qualifications. We know how important it is not only to include, but to actively seek out a diversity of opinions and voices.We want to hear from you regardless of your race, religion, national origin, sex, gender identity, sexual orientation, disability, age, veteran status, or any other applicable legally protected characteristics.For candidates living in Colorado and/or New York, the expected total compensation (includes base salary, bonus and equity) is $140,000 - $220,000 range , depending on a variety of non-discriminatory factors, including qualifications, experience, and geographic location."
Senior Data and Integration Engineer (Remote),A-Line Staffing Solutions,United States (Remote),https://www.linkedin.com/jobs/view/3779241979/?eBP=JOB_SEARCH_ORGANIC&refId=FQN8TYAI4tjJr8%2B7nGayZQ%3D%3D&trackingId=hRkrcyY1VPG2TuhYaOHscg%3D%3D&trk=flagship3_search_srp_jobs,3779241979,"About the job
            
 
Principal Data and Integration Engineer 60-75.00/hr on w-2 (No C2C)M-F 9am to 5pm CSTJob Description Develops, tests and maintains code using software development methodology and appropriate technologies for the system being used.Works closely with Business Analysts or System Analyst to develop detail systems design and written test plans for on-line and report application programs.Performs analysis on projects and provides a project plan that shows the tasks needing to be completed and a time estimate for each task.Provides status reports that give a detailed description of the current projects progress and indicates time devoted to each task.
Required 10+ years related experience including a minimum of 8+ years designing, building Datawarehouse and Data Bases in Microsoft SQL server.Critical thinker.Demonstrated problem solving techniques.Strong verbal and written communication skills.Some ETL/data movement certifications.Bachelor’s degree in Computer and Information ScienceHands-on experience in designing, coding, enhancing, testing and production support of custom SQL Server Datawarehouse to meet business process requirementsProficient with Excel and creating Apps in Excel using VBCodeConfident in Microsoft SQL Server (using SSMS – Advanced TSQL, Stored Procedures, best practices in RDBMS) best practices for writing clean effective code and balancing Declarative customizations with Programmatic customizations
Nice To Have Skills Knowledge/experience in Oracle (PLSQL using TOAD)Salesforce knowledge and experienceSnapLogic knowledge and experienceTableau knowledge and experience
If interested in this role, please apply to this posting or reach out to rpartlan@alinestaffing.com (No C2C)#ALINE11"
Arity - Lead Data Analytics Engineer - Remote,Arity,"Chicago, IL (Remote)",https://www.linkedin.com/jobs/view/3729709846/?eBP=JOB_SEARCH_ORGANIC&refId=FQN8TYAI4tjJr8%2B7nGayZQ%3D%3D&trackingId=D8RjZJfMHO%2FmxHD%2FC9hUrg%3D%3D&trk=flagship3_search_srp_jobs,3729709846,"About the job
            
 
745607-en_USFounded by The Allstate Corporation in 2016, Arity is a data and analytics company focused on improving transportation. We collect and analyze enormous amounts of data, using predictive analytics to build solutions with a single goal in mind: to make transportation smarter, safer and more useful for everyone.At the heart of that mission are the people that work here—the dreamers, doers and difference-makers that call this place home. As part of that team, your work will showcase both your intelligence and your creativity as you tackle real problems and put your talents towards transforming transportation.That’s because at Arity, we believe work and life shouldn’t be at odds with one another. After all, we know that your unique qualities give you a unique perspective. We don’t just want you to see yourself here. We want you to be yourself here.The TeamOur Data Science team is fueled by a passion to impact the future of mobility. We love to find the meaning within the 100’s of billions of miles of driving data we collect each year. We push the boundaries of telematics and transportation by building data and machine learning solutions to power our groundbreaking products.We are a team of data scientists, data engineers, and analysts that take full ownership of our data pipelines and feature generation and use AWS Managed Services as the basis of our technology stack. As a part of the team, in collaboration with product owners and other experts, your work will showcase your intelligence and creativity as you solve real problems that will improve transportation and even save lives. Personally, we think that’s game-changing stuff.The RoleWe are seeking a highly motivated Senior Data Analytics Engineer to join our Analytics Data Engineering team inside of Data Science. You’ll help us continue to build out an analytics data ecosystem that organizes our data for research, merges disparate sources together, and ensures data privacy best practices. You will work with a team of data engineers dedicated to creating the best telematics data and insights platform in the market.You will help us build the next generation of deep mobility insights by extracting relevant behavioral and geospatial patterns from users’ trip data. Your team will help us find new sources of telematics data and figure out how to ingest, normalize, and process it at a scale of over 500 trips per second with full system observability.Responsibilities Build pipelines that source data from operational systems and then process and organize it to optimize for R&D efforts across Arity Apply appropriate methodologies for the problem using a variety of tool sets and writing code in Scala or Python.Create highly reusable and reliable code and leverage CI/CD principles to create robust data applications Explore data across the company and work cross-functionally to find opportunities for new data sets that can support our products Ensure projects have appropriate measures of success that support data-driven development Create reusable validations of data pipelines and guide the implementation of monitoring Support a culture of reproducibility via peer review, code review, and documentation Drive continuous improvement of data science and data engineering practices to create world-class capabilities Influence analytics strategy and roadmap with your combination of data engineering expertise and domain experience Build the Arity technical brand by engaging in conferences, meetups, blogs, and other external public engagements Drive recruiting by building relationships in the industry and supporting our interviewing efforts 
Qualifications Bachelor’s degree in Geospatial Science, Mathematics, Statistics, Physics, Computer Science, Engineering, or related quantitative field. A Master's degree is preferred. At least five years of industry experience in data engineering or related roles such as Software EngineeringRemote employment experienceExperience with several of EMR, Spark, Kinesis, Athena, and AirflowRespected by peers for technical prowess in Scala. Python is a plusAbility to translate business problems into well-defined data and analytics problems with quantitative success measures Experience driving end-to-end data engineering projects to generate measurable business value, including ideation, development, deployment, and maintenance & monitoring Ability to envision and articulate radical change through highly innovative thought leadership Inspires, mentors, and enables team to be bold and deliver high quality work Comfortable in a fast-paced environment with high ambiguity; inspires others to embrace these conditions 
Nice to Have Geospatial, sensor, or telematics experience is quite valuable, but not required Docker, Kubernetes, CI/CD, TerraformData Science and machine learning (Pandas, Scikit learn) You write code to transform data between data models and formats, preferably in Scala / Spark, Python or PySpark.Experience moving trained machine learning models
Compensation offered for this role is $130,400.00-$179,675.00 per year and is based on experience and qualifications.The candidate(s) offered this position will be required to submit to a background investigation, which includes a drug screen.That’s the day-to-day, now let’s talk about the rest of it. As we mentioned, Arity was founded by The Allstate Corporation. But you’ll be working for—and at—Arity. It’s the best of both worlds. You’ll get access to the full suite of Allstate benefits and work in a fast-paced startup culture. That’s more than just free breakfasts and brain breaks. It’s a culture that encourages you to be you.Sound like a fit? Apply now! We can’t wait to meet you.Arity.com Instagram Twitter LinkedInAllstate generally does not sponsor individuals for employment-based visas for this position.Effective July 1, 2014, under Indiana House Enrolled Act (HEA) 1242, it is against public policy of the State of Indiana and a discriminatory practice for an employer to discriminate against a prospective employee on the basis of status as a veteran by refusing to employ an applicant on the basis that they are a veteran of the armed forces of the United States, a member of the Indiana National Guard or a member of a reserve component.For jobs in San Francisco, please click ""here"" for information regarding the San Francisco Fair Chance Ordinance.For jobs in Los Angeles, please click ""here"" for information regarding the Los Angeles Fair Chance Initiative for Hiring Ordinance.To view the “EEO is the Law” poster click “here”. This poster provides information concerning the laws and procedures for filing complaints of violations of the laws with the Office of Federal Contract Compliance ProgramsTo view the FMLA poster, click “here”. This poster summarizing the major provisions of the Family and Medical Leave Act (FMLA) and telling employees how to file a complaint.It is the Company’s policy to employ the best qualified individuals available for all jobs. Therefore, any discriminatory action taken on account of an employee’s ancestry, age, color, disability, genetic information, gender, gender identity, gender expression, sexual and reproductive health decision, marital status, medical condition, military or veteran status, national origin, race (include traits historically associated with race, including, but not limited to, hair texture and protective hairstyles), religion (including religious dress), sex, or sexual orientation that adversely affects an employee's terms or conditions of employment is prohibited. This policy applies to all aspects of the employment relationship, including, but not limited to, hiring, training, salary administration, promotion, job assignment, benefits, discipline, and separation of employment"
Workplace Analytics Engineer,Canonical,"Daytona Beach, FL (Remote)",https://www.linkedin.com/jobs/view/3738247313/?eBP=JOB_SEARCH_ORGANIC&refId=FQN8TYAI4tjJr8%2B7nGayZQ%3D%3D&trackingId=DT3uFeyTeDbCZEd8FIlIYg%3D%3D&trk=flagship3_search_srp_jobs,3738247313,"About the job
            
 
Bring your people analytics, social science research and data mining/science skills to a unique team seeking to understand, and shape, the future of the digital workplace. We are interested in technology, of course, but we are also interested in the human mission of enabling the world's brightest and hardest working people to live where they want and work from anywhere. Most of our colleagues could move to a tech hub but they choose Canonical because of our mission and our approach to the workplace.We'd like to understand what really makes a distributed, remote-first workplace work. We think we're pretty good at this (being remote first for almost 20 years), but we know there is a lot still to understand, and the frontier of possibility continues to move outward. We'd like to invest in research, analytics and tooling which raises the bar even further for remote collaboration and organisation.If we are able to build tools that meaningfully improve our cooperation and our satisfaction, then we intend to share our insights with the world, both as a narrative and as SAAS or open source that helps other companies follow in our footsteps.The role of a Workplace Analytics Engineer at CanonicalSupport analytics and data mining in a cross-disciplinary team of organisational psychologists, web front end engineers, back end engineers and statistics / analytics experts to help us build a new definition for the 21st century digital workplace. Collaborate to figure out what really drives productivity, effectiveness and happiness in a remote-first globally distributed company.In addition to your existing people analytics work experience, this role will combine your skills in psychology, data analytics and visualisation, to help create a more effective workplace.Location: This role will be based remotely in the AMER region.All applicants applying must be legally authorized to work in the United States, as we cannot offer visa sponsorship for this job position.What your day will look like Utilize advanced data analytics to understand how we hire and how we work (productivity, happiness and effectiveness) across a global, remote first organisationFocus on quantitative and qualitative data analytics to find insights and meaningful business outcomesTell the story from the insights through dashboards, visualizations and presentationsDesign and conduct research into trends shaping talent science and remote workCollaborate with stakeholder teams (ex., engineering, information systems, etc) to improve the data and tool ecosystem supporting our digital workplace
What we are looking for in you Background in data science, mathematics, actuarial science, or engineeringFirst work experience in People Analytics Knowledge in advanced statistics, data sciences, coding/scripting languages (Python, R, etc), and databases (SQL, etc)Strength in data analytics and visualization (Looker Studio, Tableau, etc)Ability to translate business questions to key research objectivesAbility to identify the best methodology to execute research, synthesize and analyse findingsExcellent writing and communication skillsWillingness to examine the status quo and resilient in the face of challenges
What we offer youYour base pay will depend on various factors including your geographical location, level of experience, knowledge and skills. In addition to the benefits above, certain roles are also eligible for additional benefits and rewards including annual bonuses and sales incentives based on revenue or utilization. Our compensation philosophy is to ensure equity right across our global workforce.In addition to a competitive base pay, we provide all team members with additional benefits, which reflect our values and ideals. Please note that additional benefits may apply depending on the work location and, for more information on these, you can ask in the later stages of the recruitment process.🏠 Fully remote working environment - we've been working remotely since 2004!📚 Personal learning and development budget of 2,000USD per annum💰 Annual compensation review🏆 Recognition rewards🏝 Annual holiday leave👶 Parental Leave🧑‍💼 Employee Assistance Programme🧳 Opportunity to travel to new locations to meet colleagues at 'sprints'✈️ Priority Pass for travel and travel upgrades for long haul company eventsAbout CanonicalCanonical is a pioneering tech firm that is at the forefront of the global move to open source. As the company that publishes Ubuntu, one of the most important open source projects and the platform for AI, IoT and the cloud, we are changing the world on a daily basis. We recruit on a global basis and set a very high standard for people joining the company. We expect excellence - in order to succeed, we need to be the best at what we do.Canonical has been a remote-first company since its inception in 2004. Work at Canonical is a step into the future, and will challenge you to think differently, work smarter, learn new skills, and raise your game. Canonical provides a unique window into the world of 21st-century digital business.Canonical is an equal opportunity employerWe are proud to foster a workplace free from discrimination. Diversity of experience, perspectives, and background create a better work environment and better products. Whatever your identity, we will give your application fair consideration."
Workplace Analytics Engineer,Canonical,"Salt Lake City, UT (Remote)",https://www.linkedin.com/jobs/view/3738249126/?eBP=JOB_SEARCH_ORGANIC&refId=FQN8TYAI4tjJr8%2B7nGayZQ%3D%3D&trackingId=YRmsiE4sOt46lo9Ih075QA%3D%3D&trk=flagship3_search_srp_jobs,3738249126,"About the job
            
 
Bring your people analytics, social science research and data mining/science skills to a unique team seeking to understand, and shape, the future of the digital workplace. We are interested in technology, of course, but we are also interested in the human mission of enabling the world's brightest and hardest working people to live where they want and work from anywhere. Most of our colleagues could move to a tech hub but they choose Canonical because of our mission and our approach to the workplace.We'd like to understand what really makes a distributed, remote-first workplace work. We think we're pretty good at this (being remote first for almost 20 years), but we know there is a lot still to understand, and the frontier of possibility continues to move outward. We'd like to invest in research, analytics and tooling which raises the bar even further for remote collaboration and organisation.If we are able to build tools that meaningfully improve our cooperation and our satisfaction, then we intend to share our insights with the world, both as a narrative and as SAAS or open source that helps other companies follow in our footsteps.The role of a Workplace Analytics Engineer at CanonicalSupport analytics and data mining in a cross-disciplinary team of organisational psychologists, web front end engineers, back end engineers and statistics / analytics experts to help us build a new definition for the 21st century digital workplace. Collaborate to figure out what really drives productivity, effectiveness and happiness in a remote-first globally distributed company.In addition to your existing people analytics work experience, this role will combine your skills in psychology, data analytics and visualisation, to help create a more effective workplace.Location: This role will be based remotely in the AMER region.All applicants applying must be legally authorized to work in the United States, as we cannot offer visa sponsorship for this job position.What your day will look like Utilize advanced data analytics to understand how we hire and how we work (productivity, happiness and effectiveness) across a global, remote first organisationFocus on quantitative and qualitative data analytics to find insights and meaningful business outcomesTell the story from the insights through dashboards, visualizations and presentationsDesign and conduct research into trends shaping talent science and remote workCollaborate with stakeholder teams (ex., engineering, information systems, etc) to improve the data and tool ecosystem supporting our digital workplace
What we are looking for in you Background in data science, mathematics, actuarial science, or engineeringFirst work experience in People Analytics Knowledge in advanced statistics, data sciences, coding/scripting languages (Python, R, etc), and databases (SQL, etc)Strength in data analytics and visualization (Looker Studio, Tableau, etc)Ability to translate business questions to key research objectivesAbility to identify the best methodology to execute research, synthesize and analyse findingsExcellent writing and communication skillsWillingness to examine the status quo and resilient in the face of challenges
What we offer youYour base pay will depend on various factors including your geographical location, level of experience, knowledge and skills. In addition to the benefits above, certain roles are also eligible for additional benefits and rewards including annual bonuses and sales incentives based on revenue or utilization. Our compensation philosophy is to ensure equity right across our global workforce.In addition to a competitive base pay, we provide all team members with additional benefits, which reflect our values and ideals. Please note that additional benefits may apply depending on the work location and, for more information on these, you can ask in the later stages of the recruitment process.🏠 Fully remote working environment - we've been working remotely since 2004!📚 Personal learning and development budget of 2,000USD per annum💰 Annual compensation review🏆 Recognition rewards🏝 Annual holiday leave👶 Parental Leave🧑‍💼 Employee Assistance Programme🧳 Opportunity to travel to new locations to meet colleagues at 'sprints'✈️ Priority Pass for travel and travel upgrades for long haul company eventsAbout CanonicalCanonical is a pioneering tech firm that is at the forefront of the global move to open source. As the company that publishes Ubuntu, one of the most important open source projects and the platform for AI, IoT and the cloud, we are changing the world on a daily basis. We recruit on a global basis and set a very high standard for people joining the company. We expect excellence - in order to succeed, we need to be the best at what we do.Canonical has been a remote-first company since its inception in 2004. Work at Canonical is a step into the future, and will challenge you to think differently, work smarter, learn new skills, and raise your game. Canonical provides a unique window into the world of 21st-century digital business.Canonical is an equal opportunity employerWe are proud to foster a workplace free from discrimination. Diversity of experience, perspectives, and background create a better work environment and better products. Whatever your identity, we will give your application fair consideration."
Data Integration Engineer (Remote Option Available),Partners Health Management,"Elkin, NC (Remote)",https://www.linkedin.com/jobs/view/3704973884/?eBP=JOB_SEARCH_ORGANIC&refId=FQN8TYAI4tjJr8%2B7nGayZQ%3D%3D&trackingId=HXBmotPKqpb%2BjriWMkz1Vw%3D%3D&trk=flagship3_search_srp_jobs,3704973884,"About the job
            
 
Competitive Compensation & Benefits Package! Position eligible for –  Annual incentive bonus plan  Medical, dental, and vision insurance with low deductible/low cost health plan  Generous vacation and sick time accrual  12 paid holidays  State Retirement (pension plan)  401(k) Plan with employer match  Company paid life and disability insurance  Wellness Programs 
See attachment for additional details. Location:  Remote option available for any of our locations Projected Hiring Range  : Depending on Experience Closing Date  :  Open Until Filled Primary Purpose Of PositionThe Data Integration Engineer will install back-end, automated data integrations to state systems and providers’ electronic medical records, billing, data warehouses or other applicable state and providers’ systems in order to ingest customer healthcare data into applicable environments. Will also help create, manage, and optimize ETL from multiple internal data systems to provide a foundation for internal analytic dashboards. Will lead data discovery and validation projects to ensure accurate data is delivered in a timely manner for business users and state reporting requirements.Role And Responsibilities Coordinate integration systems installation and monitor equipment functioning to ensure specifications are met.Determine and recommend integration performance standards.Analyze user needs and software requirements to determine feasibility of design within time and cost constraints.Develop and direct software system testing and validation procedures, programming, and documentation related to integrations and interfaces.Train users to use new or modified processes and solutions.Store, retrieve, and manipulate data for analysis of system capabilities and requirements.Confer with systems analysts, engineers, developers, and others to design integrations and to obtain information on project limitations and capabilities, performance requirements and interfaces.Modify existing integrations to correct errors, allow it to adapt to new modifications, or to improve its performance.Analyze information to determine, recommend, and plan modifications and adjustments as needed for optimum performance and transfer of information / data with internal and external partners.Design, build, document, and troubleshoot interfaces that facilitate data exchange between Partners and third-party systems including the State, providers, etc.Ownership of data integrity checks across interfacesTests and validates implemented interfaces based on requirements.Perform internal training as well as customer training as needed.Analyze rejected and queued transactions and perform root cause analysis.Routinely review interface transaction statistics by vendor, client and transaction type. Provide ad hoc written or verbal status of critical issues to management.Coordinate update to technical documentation. Document processes and procedures. 
Plan, coordinate and facilitate with clients, vendors, development, and implementation teams. 
Perform other related duties as assigned.Knowledge, Skills And Abilities Knowledge and familiarity working in healthcare software integration; using HL7, FHIR, Json, XML, or other related integration protocols or interoperability toolsets.Experience with Mirth Connect/Nextgen Connect, Rhapsody or other healthcare interface engines.Experience working in software development using Java, JavaScript, or other similar languages.Experience working with relational databases writing SQL, Stored Procedure, and reports.Experience analyzing specifications, software design, documentation, and testing.Team-oriented, ability to handle multiple tasks at once, and meet deadlines.Ability to effectively communicate with providers, external partners, management and end users both within and external to the organization.Excellent time management skills.Excellent verbal and written communication skills.Attention to detail and accuracy of facts and documentation.Knowledge of Microsoft Windows 10, Microsoft 365, Server 2019, Knowledge of Microsoft Remote Desktop, Computer Management, and Event Viewer.Proficient in MS OfficeExperience with Azure integrations and Azure DevOps processes.
Education/Experience Required Bachelor’s degree in Computer Science or related field required.3+ years of experience in extract, transform, and load (ETL) programming.3+ years of experience working with relational databases.Experience with Health Level Seven (HL7).Ability to work independently solving application software problems.Experience with establishing external access and extract, transform, and load (ETL) strategies with multiple types of data sources, particularly a variety of electronic medical record systems.Ability to assemble and query complex data from databases, perform in depth analysis and decipher computational data.Working experience with relational databases including Microsoft SQL, and/or MySQLProficient in Microsoft Office 365 Suite, specifically Word, Excel, Outlook, and general working knowledge of Internet for business use.Conditions of Employment: Individuals must successfully complete pre-employment process, which includes criminal background check, drug screening, and reference verification.
Education/Experience Preferred Candidates must have a strong background, advanced knowledge and working experience in IT technologies, security architecture and system life-cycle phases are recommended.3+ years of experience of ETL programming in an analytic/dashboard building environment, preferably with healthcare related data.Programming and scripting skills with Java Script.Familiarity and past work experience deploying ETL solutions using Mirth Connect, CloverETL, and/or other ETL tools.Drive for Results (Service, Quality, and Continuous Improvement) – Ensure procedures and processes are in place that leads to delivery of quality results and continually reassess their effectiveness to achieve continuous improvement.Communication – Proficient verbal and written communication skills. Willingness to share and receive information and ideas from all levels of the organization to achieve the desired results.Teamwork – Commitment to the successful achievement of team and organizational goals through a desire to participate with and help other members of the team.Customer Service Focus – Demonstrate a focus on listening to and understanding client/customer needs and then delighting the client/customer by exceeding service and quality expectations."
Senior Data Engineer,Cherre,"New York, NY (Remote)",https://www.linkedin.com/jobs/view/3520074039/?eBP=JOB_SEARCH_ORGANIC&refId=FQN8TYAI4tjJr8%2B7nGayZQ%3D%3D&trackingId=woT8v7r1zv7zmMuV%2Bmfkqw%3D%3D&trk=flagship3_search_srp_jobs,3520074039,"About the job
            
 
Cherre is the leader in real estate data and insight. We connect decision makers to accurate property and market information, and help them make faster, smarter decisions. By providing a unique ""single source of truth,"" Cherre empowers customers to evaluate opportunities and trends faster and more accurately, while saving millions of dollars in manual data collection and analytics costs.Cherre is looking for an enthusiastic Senior Data Engineer who is interested in working with a fast-growing team in building industry-leading real estate data services. You will be part of designing and implementing server side services to ingest, organize, analyze, and display real estate data and insight. You will be working in a small team and be a real partner in the design and implementation of all aspects of our product.We are remote-first company.You will Develop and implement ETL processesDesign data warehouse solutions to support ETL processes and data analytics applicationsWrite SQL/NoSQL database queries, stored procedures, triggers, user defined functions, analytic functions, etcOwn features that you develop end to end, develop and test your code, implement new processes in production, and maintain and support them over timeDrive our data platform and help evolve our technology stack and development best practicesDevelop and unit test assigned features to meet product requirements
You have 7-10 years experience in engineeringBS in CS or related field or equivalent years of experienceStrong experience in database technologies and data warehousingStrong experience in PythonExperience with service oriented architecture and good understanding of distributed systems, data stores, data modeling, and indexing (experience with Event Sourcing and/or CQRS preferred)Hands on experience developing APIs and SDKsHands on experience with BigQuery, PostgreSQL, and large-scale distributed storage and database systemsAbility to deal with ambiguity and communicate well with both technical and non-technical teams
Nice to have AirflowDockerKubernetes
Benefits EquityRange of Healthcare PlansPaid Parental LeaveUnlimited VacationFlexible Work ScheduleCompensation Range: $120,000 - $250,000+ 
If this opportunity sounds interesting, apply or reach out to our internal talent team. We are happy to tell you more about Cherre: the technology we work with, the problems we solve, the team we are assembling, and the culture we all contribute to. We are excited you are considering working with us and look forward to hearing from you!“At the top of the mountain we are all snow leopards.” - Hunter S. ThompsonCherre is an equal opportunity employer. We pride ourselves on hiring the best people for the job no matter their race, sex, orientation, nationality, religion, disability, or age."
Data Engineer III (Remote),Physician Health Partners,"Denver, CO (Remote)",https://www.linkedin.com/jobs/view/3727755942/?eBP=JOB_SEARCH_ORGANIC&refId=FQN8TYAI4tjJr8%2B7nGayZQ%3D%3D&trackingId=sHF7i5iFF9zlGrDtxC3Emg%3D%3D&trk=flagship3_search_srp_jobs,3727755942,"About the job
            
 
Are you looking to work for a company that has been recognized for over a decade as a Top Place to Work? Apply today to become a part of a company that continues to commit to putting our employees first.Salary Range:$108,000-$135,000Job Description Summary:Job Description:The Data Engineer III is a subject matter expert within the Software Development team and will support the business intelligence team by implementing new and maintaining existing data flows and dimensional models with a focus on consistent and accurate enterprise data solutions on premise and in the cloud.Essential Functions: Takes part in regular Scrum activities (daily standup meetings, weekly planning poker, post-sprint retrospectives, etc.)Develops foundational patterns to be utilized by other members of the business intelligence teamAddresses issues in data flow logic and data model design, as well as the interoperability of new datasets with existing data modelsParticipates in and conducts code-reviews for all changes to the codebase and conveys coding standards clearly and conciselyTests work on each assignment before working with Product Owners to ensure business requirements are fulfilledPerformance tunes ETL/ELT processes, queries, notebooks, and other data flowsCoordinates identification of requirements and recommending new data features in conjunction with development manager, product owners, and department managersActs as a subject matter expert by sharing information and providing support and training to others, as well as spearheading team projects and establishing goals and milestones for projectsEnsures goals and commitments to the team are metAdheres to the company’s Compliance Program and to federal and state laws and regulationsOther duties as assigned
Knowledge, Skills and Abilities: Extensive experience with the Databricks platform under Microsoft Azure (ADF, ADLS, Databricks Delta Lake, Databricks Unity Catalog, SQL Serverless, Auto Loader, etc.)Advanced familiarity with on premise ETL frameworks, data modelling tools, relational database design, dimensional modelling, and SQLIntermediate familiarity with the C# languageIn-depth knowledge of data integration strategies and supporting technologiesIn-depth knowledge of standards used for electronic data exchange (FHIR, HL7, CSV, EDI, etc.)Ability to mentor othersExcellent verbal and written communication skillsGreat customer service skillsGreat teamwork and leadership skillsIndependent problem-solving skillsSelf-motivated and self-managedProficient in Microsoft Office Suite
Qualifications: Bachelor’s degree in computer science, information systems, or equivalent work experience7+ years working with the following concepts and technologies: Relational and Dimensional Data Models, T-SQL, Microsoft SQL Server, etc. 
3+ years working with the Databricks Platform under Azure, designing, and implementing a lake house architectureExperience migrating on premise data warehousing solutions to Azure solutions (preferred)Experience with Visual Studio and .NET technologies such as C# (preferred)Experience working on an agile/scrum-driven software development team (preferred)Experience working with common health care datasets (preferred)Home office that is HIPAA compliant for all remote or telecommuting positions as outlined by the company policies and procedures
About Physician Health Partners:PHP is an integrated team of physicians and health care professionals committed to supporting effective patient care throughout the health care continuum. PHP partners with Anthem, Centura Health, and Primary Physician Partners to create Colorado Community Health Alliance (CCHA) and meet the needs of Health First Colorado (Colorado’s Medicaid Program) members.We believe that our employees are our biggest assets, so we offer a robust benefits package that is not limited to: Company pays majority of medical, dental, and vision premiums for employees/dependentsHSAs, including employer contribution FSAs, including medical, limited purpose, and dependent carePaid Time Off401(k) matchingShort-Term and Long-Term Disability coverageTuition ReimbursementRewards and Recognition program
If you are interested in working for an innovative and collaborative workplace in the healthcare market, apply now!"
DevOps & Data Security Engineer,Frubana,"Arkansas, United States (Remote)",https://www.linkedin.com/jobs/view/3718232306/?eBP=JOB_SEARCH_ORGANIC&refId=FQN8TYAI4tjJr8%2B7nGayZQ%3D%3D&trackingId=oBcccjfe4Y8xedT5BQi90g%3D%3D&trk=flagship3_search_srp_jobs,3718232306,"About the job
            
 
¿Te has imaginado unirte a una empresa que tiene gran impacto social y ambiental y que tiene como misión llevar alimentos más baratos a América Latina? 🤩🥑Somos una empresa de tecnología con un crecimiento exponencial que está revolucionando el mundo de la alimentación y el sector de restaurantes en América Latina. Nos enfocamos en tener una cadena de suministro eficiente, una aplicación innovadora, productos altamente seleccionados y con un enfoque en la experiencia de nuestro cliente.Operamos en Brasil, Colombia y México, conectando el campo y diferentes proveedores con la ciudad.Atendemos a +90.000 clientes en Latinoamérica; crecemos a un ritmo de +15% mensual, somos parte de Y Combinator, la misma aceleradora de AirBnB, Dropbox y Rappi. Contamos con inversionistas sólidos como: Monashees, GGV, Softbank, Tiger Global, Lightspeed, entre otros. 😯Increíble, ¿verdad? ¡Ven con nosotros a hacer historia! 🧡🍍Lo que harás... Trabajar de manera autónoma junto a diversos equipos, definiendo requisitos y proceso en materia de seguridadImplementación de herramientas SAST, DASTDefinir e implementar políticas transversales de seguridad para toda la compañía. Adoptar a nivel organizacional postura de seguridad de la informaciónImplementar mejores prácticas propuestas por OWASP, MITREProponer y generar iniciativas que mejoren la seguridad de nuestra infraestructura
Lo que necesitas...  +5 años de experiencia en un puesto similar.  Experiencia realizando pentest en entornos cloud.  Experiencia en configuración de herramientas SAST, DAST, SCA y IAST.  Conocimientos avanzados en controles de seguridad por medio de RBAC, IAM y diferentes protocolos de autenticación (SAML,OAUTH). Experiencia en generación de informes que permita realizar el seguimiento del estado actual de seguridad. Conocimiento en la implementación de aplicaciones y mejores prácticas propuestas por OWASP, MITREConocimiento en la configuración de herramientas de seguridad en capa 7. Manejo de herramientas de integración continua orientado el flujo de DevSecOps (Jenkins, ArgoCD, CodePipeline). Contar con conocimientos avanzados de networking en la nube y on premise
¡Descubre más sobre Frubana!Nuestro Site: https://www.frubana.com/Nuestro Career Site: https://jobs.frubana.com/FrubanaAlgunas noticias sobre nosotros: 🤩https://www.fastcompany.com/90600367/latin-america-most-innovative-companies-2021 https://forbes.co/2022/11/25/emprendedores/frubana-ha-desembolsado-mas-de-us2-millones-en-creditos-a-restauranteshttps://www.portafolio.co/revista-portafolio/frubana-la-meta-es-crear-una-compania-que-genere-impacto-en-el-pais-572974"
"Data Engineer, Database Engineering",Experfy,"New York, NY (Remote)",https://www.linkedin.com/jobs/view/3590300667/?eBP=JOB_SEARCH_ORGANIC&refId=FQN8TYAI4tjJr8%2B7nGayZQ%3D%3D&trackingId=J2g%2B8lu%2BDbChC5oYBS9eBg%3D%3D&trk=flagship3_search_srp_jobs,3590300667,"About the job
            
 
As a Data Engineer for our Data Platform Engineering team you will join skilled Scala/ Spark engineers and core database developers responsible for developing hosted cloud analytics infrastructure (Apache Spark-based), distributed SQL processingframeworks, proprietary data science platforms, and core database optimization. This team is responsible for building the automated, intelligent, and highly performant query planner and execution engines, RPC calls between datawarehouse clusters, shared secondary cold storage, etc. This includes building new SQL features and customer-facing functionality, developing novel query optimization techniques for industry-leading performance, and building a databasesystem that's highly parallel, efficient and fault-tolerant. This is a vital role reporting to exec leadership and senior engineering leadershipRequirementsResponsibilities: Writing Scala code with tools like Apache Spark + Apache Arrow + Apache Kafka to build a hosted, multi-cluster data warehouse for Web3Developing database optimizers, query planners, query and data routing mechanisms, cluster-to-cluster communication, and workload management techniquesScaling up from proof of concept to ""cluster scale"" (and eventually hundreds of clusters with hundreds of terabytes each), in terms of both infrastructure/architecture and problem structureCodifying best practices for future reuse in the form of accessible, reusable patterns, templates, and code bases to facilitate meta data capturing and managementManaging a team of software engineers writing new code to build a bigger, better, faster, more optimized HTAP database (using Apache Spark, Apache Arrow, Kafka, and a wealth of other open source data tools)Interacting with exec team and senior engineering leadership to define, prioritize, and ensure smooth deployments with other operational componentsHighly engaged with industry trends within analytics domain from a data acquisition processing, engineering, management perspectiveUnderstand data and analytics use cases across Web3 / blockchains
Skills & Qualifications Bachelor's degree in computer science or related technical field. Masters or PhD a plus6+ years experience engineering software and data platforms / enterprise-scale data warehouses, preferably with knowledge of open source Apache stack (especially Apache Spark, Apache Arrow, Kafka, and others)3+ years experience with Scala and Apache Spark (or Kafka)A track record of recruiting and leading technical teams in a demanding talent marketRock solid engineering fundamentals; query planning, optimizing and distributed data warehouse systems experience is preferred but not requiredNice to have: Knowledge of blockchain indexing, web3 compute paradigms, Proofs and consensus mechanisms... is a strong plus but not requiredExperience with rapid development cycles in a web-based environmentStrong scripting and test automation knowledgeNice to have: Passionate about Web3, blockchain, decentralization, and a base understanding of how data/analytics plays into this
Apply for this job"
Snowflake Data Pipeline Engineer,TekIntegral,United States (Remote),https://www.linkedin.com/jobs/view/3724268891/?eBP=JOB_SEARCH_ORGANIC&refId=FQN8TYAI4tjJr8%2B7nGayZQ%3D%3D&trackingId=kX2hWbeZ2p%2Fv%2BilcrNpWAw%3D%3D&trk=flagship3_search_srp_jobs,3724268891,"About the job
            
 
Job DescriptionJob Title: Snowflake Data Pipeline EngineerPosition Type: 6+ ContractWork Location: 100% remote - West Coast (PST hours) In-depth Healthcare / Health Insurance / Health Plan / Health Provider Industry experience is required for this role
About Our Client / About This RoleOur client is currently seeking to hire a Snowflake Data Pipeline Engineer for a contract position with one of their top healthcare analytics clients.Our client is a data-driven organization, focused on combining clinical expertise and data science to help solve the issue of medication mismanagement across the healthcare industry. This Snowflake Data Pipeline Engineer will be responsible for designing, developing, and maintaining data pipelines that will feed into a Snowflake Data Warehouse for end-client analytics and reporting.ResponsibilitiesWorking in bi-weekly sprints to design, develop and maintain data pipelines for clinical, claims and Rx related healthcare dataIngesting, transforming, and loading data from various sources into AWS, and ultimately into the Snowflake Data Warehouse for analysisUsing Python to orchestrate data pipelinesIntegrating clinical and claims data from a variety of sources, including databases, APIs, third-party services, and streaming data, into the data platform.Handling data synchronization and replication tasks to keep data up-to-date.Working with Python and PySpark for data pre-processing and curation pre client requirementsEnsuring data pipelines are reliable, scalable, and optimized for performance.Monitoring data pipelines and infrastructure for errors, bottlenecks, and performance issues.Implementing solutions to optimize and troubleshoot issues promptly.Create and maintain documentation for data pipelines, infrastructure, and data models.Ensure that documentation is accessible to other team members.Required Qualifications & Technical SkillsRequired 7-10 + years of professional experienceRequired 3+ years of in-depth Healthcare background - working with Clinical and Claims dataRequired 3 + Years of experience in SnowflakeExpertise in PythonExpertise in SQL (Snowflake SQL preferred)Experience With PySpark PreferredExperience in a Cloud Data Engineering environment - using AWS ServicesAdditional InformationWork location is 100% remote working PST hours. West Coast is preferred"
Senior Data Engineer,Adame Services,United States (Remote),https://www.linkedin.com/jobs/view/3637898630/?eBP=JOB_SEARCH_ORGANIC&refId=FQN8TYAI4tjJr8%2B7nGayZQ%3D%3D&trackingId=oxOCeVKohpanTnNhMXLIMA%3D%3D&trk=flagship3_search_srp_jobs,3637898630,"About the job
            
 
Senior Data Engineer role100% remoteData Engineer Job Responsibilities Develops and maintains scalable data pipelines and builds out new API integrations to support continuing increases in data volume and complexity.Collaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.Implements processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it.Writes unit/integration tests, contributes to engineering wiki, and documents work.Performs data analysis required to troubleshoot data related issues and assist in the resolution of data issues.Works closely with a team of frontend and backend engineers, product managers, and analysts.Defines company data assets (data models), spark, sparkSQL, and hiveSQL jobs to populate data models.Designs data integrations and data quality framework.Designs and evaluates open source and vendor tools for data lineage.Works closely with all business units and engineering teams to develop strategy for long term data platform architecture.
Data Engineer Qualifications / Skills Knowledge of best practices and IT operations in an always-up, always-available serviceExperience with or knowledge of Agile Software Development methodologiesExcellent problem solving and troubleshooting skillsProcess oriented with great documentation skillsExcellent oral and written communication skills with a keen sense of customer service
Education, Experience, And Licensing Requirements BS or MS degree in Computer Science or a related technical field4+ years of Python or Java development experience4+ years of SQL experience (No-SQL experience is a plus)4+ years of experience with schema design and dimensional data modelingAbility in managing and communicating data warehouse plans to internal clientsExperience designing, building, and maintaining data processing systemsExperience working with either a Map Reduce or an MPP system on any size/scale"
Senior Data Engineer,Begin,"Ontario, CA (Remote)",https://www.linkedin.com/jobs/view/3767891033/?eBP=JOB_SEARCH_ORGANIC&refId=FQN8TYAI4tjJr8%2B7nGayZQ%3D%3D&trackingId=yvji%2FidGeFQvZ1Yk056Sng%3D%3D&trk=flagship3_search_srp_jobs,3767891033,"About the job
            
 
BEGiN has an exciting opportunity for a Senior Data Engineer to join our growing team! This role will be remote in Ontario, Canada.BEGiN is an award-winning educational technology company with world-wide impact. With products that are as effective as they are fun, BEGiN’s family of brands builds critical skills for school and life.We’re a diverse team of talented people passionate about creating educational content kids love. At BEGiN, we have the rare opportunity to make a dent in the universe by bringing high-quality at-home learning to kids globally!Reporting into our Director, Data Engineering, the Senior Data Engineer will implement reliable data pipelines upholding the best practices that are pivotal to analytics, data science, and reporting across the organization as well as develop the data platform capabilities to ensure data platform performance, scalability and maintainability.You will: Own efficacy and quality of data pipelines and ETL processes that bring data into the enterprise data warehouse.Develop, maintain, and improve tools to enable team members to rapidly consume and understand data.Design and architect scalable infrastructure to build, train, and deploy machine learning models, ETL, and CI/CD with an eye on efficiency.Be hands-on with multiple cloud technologies, tools and programming languages (Python, PySpark, SQL, AWS, GCP, Databricks, etc.)
Responsibilities: Work closely with the other data engineers,, data scientists/analysts, and our product engineering team to translate requirements into deliverable data pipelines.Execute the strategy for the data platform to support the business while optimizing performance and minimizing cost.Partner with stakeholders and engineering teams to deliver solutions in an iterative and incremental manner, leveraging lean and agile principles, fostering an environment of learning and collaboration.Ensure that our applications and operational data remain in sync and all integrations are flowing with no data errors.Lead root cause analysis, prioritize and manage data quality and remediation, and ensure data integrity to all downstream data systems.You will be an expert on understanding how data is collected, maintained, and interpreted and be knowledgeable on the official sources of data in scope to address use case requirements and business needs.
Must Haves: Bachelor degree in Computer Science or related field.Deep understanding of Spark (Databricks) and expertise on Data Warehousing approaches in the Databricks Lakehouse. Expert in Python/PySpark/Spark SQL and follow/evolve established SDLC, coding best practices, version control etc. Data Platform Architecture experience in AWS and/or GCP.Previous hands-on experience with data modeling.5+ years of experience as a data engineer.Excellent communication skills tailored for target audience.Experience in BI tools (i.e. Looker)
Nice-to-Haves: Graduate degree in Computer Science or related field.Understanding of Analytics use cases (i.e. customer360, marketing channel optimization etc).Prior experience with AWS Infrastructure (Networking, VPCs etc).Prior experience with tools such as Fivetran, Airflow, Metarouter, Terraform etc 
We like people who: Are open to suggestions, collaborative, and thrive in team environments.Love and are willing to learn new technologies and styles.Are scrappy, entrepreneurial with the ability to turnaround high-quality projects quickly without depending on a large team.
What you’ll get: BEGiN offers competitive compensation including equity and full benefits.Smart, passionate, and engaged co-workers.Excellent top-tier Medical/Dental/Vision benefitsThe chance to have a big impact, quicklyThe rare opportunity to make a dent in the universe. We’re bringing a love of reading and learning to children globally!
BEGiN is a proud equal opportunity employer. All qualified applicants will be considered without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status.At BEGiN, we are committed to building a diverse team of talented people who are passionate about creating educational content kids love. We believe in fostering a culture where productivity can flourish, one that is empathetic, respectful, and inclusive. At BEGiN, we know that diversity, equity, and inclusion aren’t just an idea, a one-time initiative, or phrases to throw into a job post: they’re a daily practice and an ongoing conversation. We survey our team about inclusivity, run training on DEI topics, and have a committee to ensure we are all continuing to learn and grow."
Senior Data Engineer,iTech Solutions,United States (Remote),https://www.linkedin.com/jobs/view/3715201209/?eBP=JOB_SEARCH_ORGANIC&refId=FQN8TYAI4tjJr8%2B7nGayZQ%3D%3D&trackingId=tjhN2aGVTKlY%2Bk9KNhAFkw%3D%3D&trk=flagship3_search_srp_jobs,3715201209,"About the job
            
 
 Senior Data Engineer with expertise in AWS, Spark and Python"
Senior Data Engineer (Druid Engineer),QuantumBricks,United States (Remote),https://www.linkedin.com/jobs/view/3634715047/?eBP=JOB_SEARCH_ORGANIC&refId=FQN8TYAI4tjJr8%2B7nGayZQ%3D%3D&trackingId=d0PDy%2B%2BZYwVsVBa%2FYuT3zA%3D%3D&trk=flagship3_search_srp_jobs,3634715047,"About the job
            
 
Job Title: Senior Data Engineer (druid Engineer)Loc: RemoteExp: 6+ YrsJob DescriptionMinimum required qualifications:At least 5 years of experience in Data Engineering.Working knowledge of modelling, loading, optimizing large amounts of data into druid to be queried by a low latency web applications via API.Prior working experience in building data pipelines using Spark, Hive, Python, Airflow or similar technologies.Prior working experience in Optimizing performance of data pipelines jobs.Working knowledge of data modelling, data processing infrastructure, including databases, data lakes, and data warehouses.Quick learner, self-starter, being able to operate with little to no guidance.Good communication skills and being productive in a fast-paced development environment.Nice to Have, but not mandatoryPrior experience and working knowledge of microservicesPrior Software engineering experiencePrior knowledge of implementing real-time data processing pipelines.Prior experience and working knowledge of data bricks.Prior knowledge and working experience of AWS technologies like SQS, EC2"
AWS Redshift Data Engineer - 100% REMOTE,Information Resource Group,United States (Remote),https://www.linkedin.com/jobs/view/3726807454/?eBP=JOB_SEARCH_ORGANIC&refId=FQN8TYAI4tjJr8%2B7nGayZQ%3D%3D&trackingId=VN%2BpruBc6JM7rD2GS9rocg%3D%3D&trk=flagship3_search_srp_jobs,3726807454,"About the job
            
 
Title: AWS Redshift Data EngineerLocation: 100% Remote Duration: 12+ Months Job OverviewWe are seeking a talented and experienced AWS Redshift Data Engineer / Consultant to join our team in designing, developing, and optimizing data pipelines and ETL processes for our AWS Redshift-based data lake house. In this role, you will collaborate closely with cross-functional teams, leveraging your expertise in SQL, Redshift stored procedures, AWS DMS, Airflow, Python scripting and other pertinent AWS services to ensure the seamless ingestion, integration, transformation and orchestration of data. Your experience with complex ETL pipelines, Changed Data Capture (CDC), Slowly Changing Dimension (SCD) strategies will be instrumental in creating a scalable, high-performance data environment. By adhering to best practices and industry standards, you will collaborate with our engineering and data teams to design forward thinking solutions.Key Responsibilities Collaborate with data engineering and development teams to design, develop, test, and maintain robust and scalable ELT/ETL pipelines using SQL scripts, Redshift stored procedures, and other AWS tools and services.Collaborate with our engineering and data teams to understand business requirements and data integration needs, translate them into effective data solutions, that yield top-quality outcomes.Architect, implement, and manage end-to-end data pipelines, ensuring data accuracy, reliability, data quality, performance, and timeliness.Employ AWS DMS and other services for efficient data ingestion from on-premises databases into Redshift. Design and implement ETL processes, encompassing Changed Data Capture (CDC) and Slow Changing Dimension (SCD) logics, to seamlessly integrating data from diverse source systems.Provide expertise in Redshift database optimization, performance tuning, and query optimization.Design and implement efficient orchestration workflows using Airflow, ensuring seamless coordination of complex ETL processes.Integrate Redshift with other AWS services, such as AWS DMS, AWS Glue, AWS Lambda, Amazon S3, Airflow, and more, to build end-to-end data pipelines. Perform data profiling and analysis to troubleshoot data-related challenges / issues and build solutions to address those concerns. Proactively identify opportunities to automate tasks and develop reusable frameworks.Work closely with version control team to maintain a well-organized and documented repository of codes, scripts, and configurations using Git.Provide technical guidance and mentorship to fellow developers, sharing insights into best practices, tips, and techniques for optimizing Redshift-based data solutions.
Qualifications And Skills Bachelor's or Master's degree in Computer Science, Information Technology, or a related field.Extensive hands-on experience designing, developing, and maintaining data pipelines and ETL processes on AWS Redshift, including data lakes and data warehouses.Proficiency in SQL programming and Redshift stored procedures for efficient data manipulation and transformation.Hands-on experience with AWS services such as AWS DMS, Amazon S3, AWS Glue, Redshift, Airflow, and other pertinent data technologies.Strong understanding of ETL best practices, data integration, data modeling, and data transformation.Experience with complex ETL scenarios, such as CDC and SCD logics, and integrating data from multiple source systems.Demonstrated expertise in AWS DMS for seamless ingestion from on-prem databases to AWS cloud.Proficiency in Python programming with a focus on developing efficient Airflow DAGs and operators.Experience in converting Oracle scripts and Stored Procedures to Redshift equivalents.Familiarity with version control systems, particularly Git, for maintaining a structured code repository.Proficiency in identifying and resolving performance bottleneck and fine-tuning Redshift queries, Strong coding and problem-solving skills, and attention to detail in data quality and accuracy.Ability to work collaboratively in a fast-paced, agile environment and effectively communicate technical concepts to non-technical stakeholders.Proven track record of delivering high-quality data solutions within designated timelines.Experience working with large-scale, high-volume data environments.The ideal candidate possesses several years of hands-on experience working with Redshift and other AWS services and a proven track record of delivering high-performing, scalable data platforms and solutions within the AWS cloud.AWS certifications related to data engineering or databases are a plus."
Data Engineer,Pomelo Care,United States (Remote),https://www.linkedin.com/jobs/view/3752565222/?eBP=JOB_SEARCH_ORGANIC&refId=FQN8TYAI4tjJr8%2B7nGayZQ%3D%3D&trackingId=9zqdcqS%2BVN4ePLD%2BieA3IA%3D%3D&trk=flagship3_search_srp_jobs,3752565222,"About the job
            
 
About UsPomelo Care is a multi-disciplinary team of clinicians, engineers and problem solvers who are passionate about improving care for moms and babies. We are transforming outcomes for pregnant people and babies with evidence-based pregnancy and newborn care at scale. Our technology-driven care platform enables us to engage patients early, conduct individualized risk assessments for poor pregnancy outcomes, and deliver coordinated, personalized virtual care throughout pregnancy, NICU stays, and the first postpartum year. We measure ourselves by reductions in preterm births, NICU admissions, c-sections and maternal mortality; we improve outcomes and reduce healthcare spend.What You'll DoTrustworthy, actionable data plays a critical role in Pomelo’s mission to improve pregnancy outcomes. Real-time insights empower our clinicians to focus their time efficiently and deliver outstanding care to our patients. As a data engineer, you will Build and orchestrate pipelines to ingest and harmonize data from many sourcesCollaborate on the design and improvement of our data infrastructurePartner with product managers and data scientists to turn raw data into actionable insightsPhotoshop custom slack emojis to express new feelings that aren't yet part of our library
Who You AreYou're an enthusiastic and collaborative problem-solver who is comfortable with ambiguity and enjoys finding patterns to bring order to chaos. You have a nose for value and proactively identify problems that have a big impact on the business. You have a passion for using technology to effect real, tangible good in the world. In particular, you have: A bachelors, masters or PhD in computer science or a related fieldAt least two years of experience in a data engineering role building systems in a fast-paced environmentProficiency in Python and SQL or similar languagesHave experience developing and maintaining data pipelines in a production settingKnowledge of visualization tools such as Metabase or Jupyter NotebooksEnjoy tackling complex problems -- but strive to avoid unnecessary complexity
We'll Be Super Excited If You Have experience with dbt and orchestration platforms like Dagster or AirflowHave developed distributed data processing systems (ex. Spark or Beam) against heterogeneous data setsHave experience at an early stage startupAre motivated by our mission to improve pregnancy outcomes
Why you should join our teamBy joining Pomelo, you will get in on the ground floor of a fast-moving, well-funded, and mission-driven startup that always puts the patient first. You will learn, grow and be challenged -- and have fun with your team while doing it.We strive to create an environment where employees from all backgrounds are respected. We also offer: Competitive healthcare benefitsGenerous equity compensationUnlimited vacationMembership in the First Round Network (a curated and confidential community with events, guides, thousands of Q&A questions, and opportunities for 1-1 mentorship)
At Pomelo, we are committed to hiring the best team to improve outcomes for all mothers and babies, regardless of their background. We need diverse perspectives to reflect the diversity of problems we face and the population we serve. We look to hire people from a variety of backgrounds, including but not limited to race, age, sexual orientation, gender identity and expression, national origin, religion, disability, and veteran status.Our salary ranges are based on paying competitively for our company’s size and industry, and are one part of the total compensation package that also includes equity, benefits, and other opportunities at Pomelo Care. In accordance with New York City, Colorado, California, and other applicable laws, Pomelo Care is required to provide a reasonable estimate of the compensation range for this role. Individual pay decisions are ultimately based on a number of factors, including qualifications for the role, experience level, skillset, geography, and balancing internal equity. Given that this role is open to candidates of different skill levels, determining a salary range is challenging. A reasonable estimate of the current salary range is $120,000 to $150,000. We expect most candidates to fall in the middle of the range. We also believe that your personal needs and preferences should be taken into consideration, so we allow some choice between equity and cash."
Workplace Analytics Engineer,Canonical,"Omaha, NE (Remote)",https://www.linkedin.com/jobs/view/3738247328/?eBP=JOB_SEARCH_ORGANIC&refId=FQN8TYAI4tjJr8%2B7nGayZQ%3D%3D&trackingId=QTezDstrfeUxBbLgKH9sfA%3D%3D&trk=flagship3_search_srp_jobs,3738247328,"About the job
            
 
Bring your people analytics, social science research and data mining/science skills to a unique team seeking to understand, and shape, the future of the digital workplace. We are interested in technology, of course, but we are also interested in the human mission of enabling the world's brightest and hardest working people to live where they want and work from anywhere. Most of our colleagues could move to a tech hub but they choose Canonical because of our mission and our approach to the workplace.We'd like to understand what really makes a distributed, remote-first workplace work. We think we're pretty good at this (being remote first for almost 20 years), but we know there is a lot still to understand, and the frontier of possibility continues to move outward. We'd like to invest in research, analytics and tooling which raises the bar even further for remote collaboration and organisation.If we are able to build tools that meaningfully improve our cooperation and our satisfaction, then we intend to share our insights with the world, both as a narrative and as SAAS or open source that helps other companies follow in our footsteps.The role of a Workplace Analytics Engineer at CanonicalSupport analytics and data mining in a cross-disciplinary team of organisational psychologists, web front end engineers, back end engineers and statistics / analytics experts to help us build a new definition for the 21st century digital workplace. Collaborate to figure out what really drives productivity, effectiveness and happiness in a remote-first globally distributed company.In addition to your existing people analytics work experience, this role will combine your skills in psychology, data analytics and visualisation, to help create a more effective workplace.Location: This role will be based remotely in the AMER region.All applicants applying must be legally authorized to work in the United States, as we cannot offer visa sponsorship for this job position.What your day will look like Utilize advanced data analytics to understand how we hire and how we work (productivity, happiness and effectiveness) across a global, remote first organisationFocus on quantitative and qualitative data analytics to find insights and meaningful business outcomesTell the story from the insights through dashboards, visualizations and presentationsDesign and conduct research into trends shaping talent science and remote workCollaborate with stakeholder teams (ex., engineering, information systems, etc) to improve the data and tool ecosystem supporting our digital workplace
What we are looking for in you Background in data science, mathematics, actuarial science, or engineeringFirst work experience in People Analytics Knowledge in advanced statistics, data sciences, coding/scripting languages (Python, R, etc), and databases (SQL, etc)Strength in data analytics and visualization (Looker Studio, Tableau, etc)Ability to translate business questions to key research objectivesAbility to identify the best methodology to execute research, synthesize and analyse findingsExcellent writing and communication skillsWillingness to examine the status quo and resilient in the face of challenges
What we offer youYour base pay will depend on various factors including your geographical location, level of experience, knowledge and skills. In addition to the benefits above, certain roles are also eligible for additional benefits and rewards including annual bonuses and sales incentives based on revenue or utilization. Our compensation philosophy is to ensure equity right across our global workforce.In addition to a competitive base pay, we provide all team members with additional benefits, which reflect our values and ideals. Please note that additional benefits may apply depending on the work location and, for more information on these, you can ask in the later stages of the recruitment process.🏠 Fully remote working environment - we've been working remotely since 2004!📚 Personal learning and development budget of 2,000USD per annum💰 Annual compensation review🏆 Recognition rewards🏝 Annual holiday leave👶 Parental Leave🧑‍💼 Employee Assistance Programme🧳 Opportunity to travel to new locations to meet colleagues at 'sprints'✈️ Priority Pass for travel and travel upgrades for long haul company eventsAbout CanonicalCanonical is a pioneering tech firm that is at the forefront of the global move to open source. As the company that publishes Ubuntu, one of the most important open source projects and the platform for AI, IoT and the cloud, we are changing the world on a daily basis. We recruit on a global basis and set a very high standard for people joining the company. We expect excellence - in order to succeed, we need to be the best at what we do.Canonical has been a remote-first company since its inception in 2004. Work at Canonical is a step into the future, and will challenge you to think differently, work smarter, learn new skills, and raise your game. Canonical provides a unique window into the world of 21st-century digital business.Canonical is an equal opportunity employerWe are proud to foster a workplace free from discrimination. Diversity of experience, perspectives, and background create a better work environment and better products. Whatever your identity, we will give your application fair consideration."
Azure Data Engineer | REMOTE,eStaffing Inc.,United States (Remote),https://www.linkedin.com/jobs/view/3657954457/?eBP=JOB_SEARCH_ORGANIC&refId=FQN8TYAI4tjJr8%2B7nGayZQ%3D%3D&trackingId=y1Oe0EGxSoWYPhbi2DWh0Q%3D%3D&trk=flagship3_search_srp_jobs,3657954457,"About the job
            
 
TITLE : SOFTWARE ENGINEERLOCATION : REMOTEDURATION : W2Experience 5 years' experience in application development.At least five years of experience in Azure big data services/technologies (Spark, Scala, Azure Data Factory, Azure Data Bricks, Synapse Analytics).
Skills/Knowledge/Abilities (ska) Required Deep knowledge of SQL/TSQLStrong understanding of Azure Big Data Services such as Azure Data Lake Storage, Azure Data Factory (MUST), Logic Apps, Azure Functions Azure DataBricks experience is a PLUSExperience with RDBMS (Oracle 10g/11g, MS SQL), including database design, developing stored procedures and functions, and performance tuning. Working experiences with No-SQL databases.Agile software processStrong coding skills (C#)experience working with python is a plusWorking experiences of test-driven development framework."
Data Warehouse Engineer - Remote | WFH,Get It Recruit - Information Technology,"Franklin, TN (Remote)",https://www.linkedin.com/jobs/view/3774138104/?eBP=JOB_SEARCH_ORGANIC&refId=FQN8TYAI4tjJr8%2B7nGayZQ%3D%3D&trackingId=EP7HH79bmO2u3DDQgSI5eA%3D%3D&trk=flagship3_search_srp_jobs,3774138104,"About the job
            
 
Join a Thriving Team as a Data Warehousing EngineerAbout The OpportunityAre you a seasoned Data Warehousing Engineer seeking a challenging and rewarding role within a dynamic team? If so, we invite you to explore this exciting opportunity with one of our esteemed clients. This contract-to-hire position offers a hybrid schedule, allowing you to work two days in the office and three days remotely.ResponsibilitiesDesign, develop, and maintain T-SQL procedures, functions, and scripts to meet the organization's unique data analysis and integration requirements.Safeguard data quality, consistency, and reliability across all databases and data-driven applications.Analyze complex datasets, extracting insights and presenting findings in a clear and understandable manner to both technical and non-technical stakeholders.Implement data warehousing concepts to ensure the design and performance of the data infrastructure align with the organization's needs.Oversee data migration, transformation, and cleansing projects, ensuring the highest level of data integrity.QualificationsBachelor's degree in Computer Science, Information Technology, Data Analytics, or a closely related field.A minimum of 5 years of professional experience in data analysis within the P&C insurance sector.Proficiency in T-SQL, with the ability to craft efficient and sophisticated queries.Thorough understanding of data warehousing principles.Familiarity with data visualization tools and BI platforms.Strong grasp of data quality principles, with experience in ensuring data accuracy and consistency.Excellent communication skills, with the ability to articulate complex data concepts and findings to diverse audiences.BenefitsCompetitive salary and benefits packageOpportunity to work with a seasoned leadership team in a dynamic and collaborative environmentFlexible work arrangement with a hybrid schedule (2 days in office, 3 days remote)Chance to contribute to the organization's growth and success by leveraging your expertise in data warehousingAbout Our ClientOur client is a leading organization committed to innovation and excellence. They are seeking a highly skilled and experienced Data Warehousing Engineer to join their team and play a pivotal role in their data-driven initiatives.Join UsIf you are a passionate Data Warehousing Engineer seeking a challenging and rewarding opportunity, we encourage you to apply. We are committed to fostering a diverse and inclusive workplace where everyone feels valued and respected.Employment Type: Full-Time"
Data Engineer – AI Applications,HP,"Washington, United States (Remote)",https://www.linkedin.com/jobs/view/3772981057/?eBP=JOB_SEARCH_ORGANIC&refId=FQN8TYAI4tjJr8%2B7nGayZQ%3D%3D&trackingId=KkQHVwDBA9kfJec7VwI%2FDg%3D%3D&trk=flagship3_search_srp_jobs,3772981057,"About the job
            
 
Our compensation reflects the cost of labor across several U.S. geographic markets, and we pay differently based on those defined markets. The typical base pay range for this role across the U.S. is $127,500.00 -- $196,350.00 annually with additional opportunities for pay in the form of bonus and/or equity. Pay within this range varies by work location and may also depend on job-related knowledge, skills, and experience. Your recruiter can share more about the specific salary range for the job location during the hiring process.Applies advanced subject matter knowledge to solve complex business issues and is regarded as a subject matter expert. Frequently contributes to the development of new ideas and methods. Works on complex problems where analysis of situations or data requires an in-depth evaluation of multiple factors. Leads and/or provides expertise to functional project teams and may participate in cross-functional initiatives. Acts as an expert providing direction and guidance to process improvements and establishing policies. Frequently represents the organization to external customers/clients. Exercises significant independent judgment within broadly defined policies and practices to determine best method for accomplishing work and achieving objectives. May provide mentoring and guidance to lower level employees.Responsibilities Leads one or more project teams of other data engineers for all stages of design and development for complex, secure and performant data solutions and models, including design, analysis, coding, testing, and integration of structured/unstructured data.Builds and manages relationships throughout the organizationReviews and evaluates designs and project activities for compliance with architecture, security and quality guidelines and standards; provides tangible feedback to improve product quality and mitigate failure risk.Provides domain-specific expertise and overall data systems leadership and perspective to cross-organization projects, programs, and activities.Drives innovation and integration of new technologies into projects and activities in the big data space.Collaborates and communicates with project team regarding project progress and issue resolution.Represents the data engineering team for all phases of larger and more-complex development projects.Provides guidance and mentoring to less experienced staff members.
Knowledge & Skills Extensive experience with data engineering tools, languages, frameworks to mine, cleanse and explore data.Excellent analytical and problem-solving skills.Fluent in NoSQL & relational based systems.Strong experience in overall architecture of big data systems, cloud services/systems.Designing data systems/solutions to manage complex data in complex, distributed and massively parallel systems.Evaluating forms and processes for database architecture testing and methodology, including writing and execution of test plans, debugging, and testing scripts and tools.Excellent written and verbal communication skills; mastery in English and local language.Ability to effectively communicate product architectures, design proposals and negotiate options at senior management levels.
Scope & Impact Collaborates with peers, junior engineers, data scientists and project team.Typically interacts with high- level Individual Contributors, Managers, Directors and Program Core Teams.Leads multiple projects requiring data engineering solutions development.Drives design innovation.
Education & Experience Bachelor's or Master's degree in Computer Science, Information Systems, Engineering, or equivalent.Typically 6-10 years’ experience.
About HPYou’re out to reimagine and reinvent what’s possible—in your career as well as the world around you.So are we. We love taking on tough challenges, disrupting the status quo, and creating what’s next. We’re in search of talented people who are inspired by big challenges, driven to learn and grow, and dedicated to making a meaningful difference.HP is a technology company that operates in more than 170 countries around the world united in creating technology that makes life better for everyone, everywhere.Our history: HP’s commitment to diversity, equity and inclusion – it's just who we are.From the boardroom to factory floor, we create a culture where everyone is respected and where people can be themselves, while being a part of something bigger than themselves. We celebrate the notion that you can belong at HP and bring your authentic self to work each and every day. When you do that, you’re more innovative and that helps grow our bottom line. Come to HP and thrive!"
"Data Engineer, Database Engineering",Experfy,"Boston, MA (Remote)",https://www.linkedin.com/jobs/view/3669899571/?eBP=JOB_SEARCH_ORGANIC&refId=FQN8TYAI4tjJr8%2B7nGayZQ%3D%3D&trackingId=69pyRHMyv7ciF2hXBlv8Ww%3D%3D&trk=flagship3_search_srp_jobs,3669899571,"About the job
            
 
As a Data Engineer for our Data Platform Engineering team you will join skilled Scala/ Spark engineers and core database developers responsible for developing hosted cloud analytics infrastructure (Apache Spark-based), distributed SQL processingframeworks, proprietary data science platforms, and core database optimization. This team is responsible for building the automated, intelligent, and highly performant query planner and execution engines, RPC calls between datawarehouse clusters, shared secondary cold storage, etc. This includes building new SQL features and customer-facing functionality, developing novel query optimization techniques for industry-leading performance, and building a databasesystem that's highly parallel, efficient and fault-tolerant. This is a vital role reporting to exec leadership and senior engineering leadershiRequirementsResponsibilities: Writing Scala code with tools like Apache Spark + Apache Arrow + Apache Kafka to build a hosted, multi-cluster data warehouse for Web3Developing database optimizers, query planners, query and data routing mechanisms, cluster-to-cluster communication, and workload management techniquesScaling up from proof of concept to ""cluster scale"" (and eventually hundreds of clusters with hundreds of terabytes each), in terms of both infrastructure/architecture and problem structureCodifying best practices for future reuse in the form of accessible, reusable patterns, templates, and code bases to facilitate meta data capturing and managementManaging a team of software engineers writing new code to build a bigger, better, faster, more optimized HTAP database (using Apache Spark, Apache Arrow, Kafka, and a wealth of other open source data tools)Interacting with exec team and senior engineering leadership to define, prioritize, and ensure smooth deployments with other operational componentsHighly engaged with industry trends within analytics domain from a data acquisition processing, engineering, management perspectiveUnderstand data and analytics use cases across Web3 / blockchains
Skills & Qualifications Bachelor's degree in computer science or related technical field. Masters or PhD a plus6+ years experience engineering software and data platforms / enterprise-scale data warehouses, preferably with knowledge of open source Apache stack (especially Apache Spark, Apache Arrow, Kafka, and others)3+ years experience with Scala and Apache Spark (or Kafka)A track record of recruiting and leading technical teams in a demanding talent marketRock solid engineering fundamentals; query planning, optimizing and distributed data warehouse systems experience is preferred but not requiredNice to have: Knowledge of blockchain indexing, web3 compute paradigms, Proofs and consensus mechanisms... is a strong plus but not requiredExperience with rapid development cycles in a web-based environmentStrong scripting and test automation knowledgeNice to have: Passionate about Web3, blockchain, decentralization, and a base understanding of how data/analytics plays into this
Apply for this job"
"Data Engineer, Database Engineering",Experfy,"Riverside, CA (Remote)",https://www.linkedin.com/jobs/view/3590303110/?eBP=JOB_SEARCH_ORGANIC&refId=XUUXD7jduifZpMXMz48Iow%3D%3D&trackingId=gKslgikpJrqb%2B%2BSMqIry5A%3D%3D&trk=flagship3_search_srp_jobs,3590303110,"About the job
            
 
As a Data Engineer for our Data Platform Engineering team you will join skilled Scala/ Spark engineers and core database developers responsible for developing hosted cloud analytics infrastructure (Apache Spark-based), distributed SQL processingframeworks, proprietary data science platforms, and core database optimization. This team is responsible for building the automated, intelligent, and highly performant query planner and execution engines, RPC calls between datawarehouse clusters, shared secondary cold storage, etc. This includes building new SQL features and customer-facing functionality, developing novel query optimization techniques for industry-leading performance, and building a databasesystem that's highly parallel, efficient and fault-tolerant. This is a vital role reporting to exec leadership and senior engineering leadershipRequirementsResponsibilities: Writing Scala code with tools like Apache Spark + Apache Arrow + Apache Kafka to build a hosted, multi-cluster data warehouse for Web3Developing database optimizers, query planners, query and data routing mechanisms, cluster-to-cluster communication, and workload management techniquesScaling up from proof of concept to ""cluster scale"" (and eventually hundreds of clusters with hundreds of terabytes each), in terms of both infrastructure/architecture and problem structureCodifying best practices for future reuse in the form of accessible, reusable patterns, templates, and code bases to facilitate meta data capturing and managementManaging a team of software engineers writing new code to build a bigger, better, faster, more optimized HTAP database (using Apache Spark, Apache Arrow, Kafka, and a wealth of other open source data tools)Interacting with exec team and senior engineering leadership to define, prioritize, and ensure smooth deployments with other operational componentsHighly engaged with industry trends within analytics domain from a data acquisition processing, engineering, management perspectiveUnderstand data and analytics use cases across Web3 / blockchains
Skills & Qualifications Bachelor's degree in computer science or related technical field. Masters or PhD a plus6+ years experience engineering software and data platforms / enterprise-scale data warehouses, preferably with knowledge of open source Apache stack (especially Apache Spark, Apache Arrow, Kafka, and others)3+ years experience with Scala and Apache Spark (or Kafka)A track record of recruiting and leading technical teams in a demanding talent marketRock solid engineering fundamentals; query planning, optimizing and distributed data warehouse systems experience is preferred but not requiredNice to have: Knowledge of blockchain indexing, web3 compute paradigms, Proofs and consensus mechanisms... is a strong plus but not requiredExperience with rapid development cycles in a web-based environmentStrong scripting and test automation knowledgeNice to have: Passionate about Web3, blockchain, decentralization, and a base understanding of how data/analytics plays into this"
AWS Data Engineer,Extend Information Systems Inc.,United States (Remote),https://www.linkedin.com/jobs/view/3652274467/?eBP=JOB_SEARCH_ORGANIC&refId=XUUXD7jduifZpMXMz48Iow%3D%3D&trackingId=%2B%2B2pXG9dsCKislI%2B2tzctg%3D%3D&trk=flagship3_search_srp_jobs,3652274467,"About the job
            
 
Hi Partner,I hope you are doing well!We have an opportunity Information Security/ Cyber Security Application Project Manager with one of our clients for Mc Lean, VA.Please see the job details below and let me know if you would be interested in this role.If interested, please send me a copy of your resume, contact details, availability, and a good time to connect with you.Job Title: Information Security/ Cyber Security Application Project ManagerLocation: Mc Lean, VADuration: C2CExperience level : 10hands on experience on financials / budget analysis, tracking actuals, working with leads to address the gapsJob DescriptionIdentify business unit requirements, create project and process specifications, facilitate testing, coordinate with project teams and ensure that projects are delivered on-time and within budget Identify process improvements and implementTake ownership of end-to-end project deliveryResponsible for acting as the primary interface between a specific business/functional area and their IT partners.Represent the organization as the principal customer contact for projects, light enhancements, and other change initiativesInteract with senior customer personnel on significant technical matters frequently requiring coordination across organizational linesIdentify and resolve operational issues with business areas.Develops detailed work plans, schedules, project estimates, resource plans, and status reports.Ability to work independently and effectively with staff at all levels and from multiple divisions.Ability to manage multiple priorities and deadlines.Proven ability to learn new business concepts supporting product and/or business process development.Strong organization and presentation skills.
Required Skills And Knowledge At least 9+ years of experience in all aspects of the development and implementation of assigned projects and provides a single point of contact for those projects.At least 9+ years of experience in taking projects from original concept through final implementation. Interfaces with all areas affected by the project including end users, computer services, and client services.At least 6+ years of experience in defining project scope and objectives.PMP is a plus
Thanks & RegardsAnoop TiwariExtend Information SystemsCell: - 571 - 386 - 2431 Email: Anoop@extendinfosys.comAddress: 44258 Mercure Circle, UNIT 102 A, Sterling VA, USA 20166Web: www.extendinfosys.com"
Senior Data Engineer,Pierce,United States (Remote),https://www.linkedin.com/jobs/view/3748512527/?eBP=JOB_SEARCH_ORGANIC&refId=XUUXD7jduifZpMXMz48Iow%3D%3D&trackingId=133f5hJiIPBVU4qg8Gsyow%3D%3D&trk=flagship3_search_srp_jobs,3748512527,"About the job
            
 
Strategy Creation: Collaborate with cross-functional teams to define the data engineering strategy aligned to business objectives, including data modeling that unifies data assets across a range of source systems used to manage the operations of our partnering hospitals. Pipeline Development: Define and execute processes needed to develop, test, deploy, and maintain high quality data pipelines. Oversee the end-to-end development of data pipelines from source data extraction through to production-grade analytical dataset delivery, ensuring data quality and security throughout the pipeline. Performance Optimization: Continuously monitor and optimize data processing performance and efficiency. Identify and address bottlenecks, optimize query performance, and improve overall system stability. Data Governance: Establish and enforce data quality management policies, data access controls, and data privacy standards. Technical Leadership: Stay abreast of the latest developments in engineering tools and best practices. Provide guidance to the team about technical challenges. Documentation: Maintain clear and comprehensive documentation of data pipelines, architecture, and processes to ensure knowledge sharing and team continuity. Third-party Management: Evaluate and manage relationships with third-party vendors and tools, making informed decisions about when to leverage external solutions. 
Requirements 6+ years in data engineering roles with progressively increasing responsibilities. Deep understanding of data modeling, data architecture, and data integration best practices. Strong hands-on experience with Apache Spark and cloud computing technologies. Advanced proficiency in Python and SQL. Familiarity with data governance, security, and privacy principles. Comfort using collaboration tools such as GitHub or equivalent to manage development life cycle. Excellent data modeling and engineering skills, and a talent for translating business objectives into technical solutions. High energy, humble team player with “get it done” attitude, seeking collaboration with colleagues. Ability to manage multiple projects simultaneouslyExperience engineering in Databricks strongly preferred. 3+ years of software engineering with python in a production environment. Experience with the Azure cloud ecosystem. Experience developing production-ready, real-time machine learning model serving pipelines. Comfort developing in the Apache Spark Structured Streaming paradigm. Experience working in the veterinary services industry, or a private equity-backed services company. Working knowledge of Microsoft Excel and Office 365."
Senior Data Engineer - Remote | WFH,Get It Recruit - Information Technology,"Pittsburgh, PA (Remote)",https://www.linkedin.com/jobs/view/3764317455/?eBP=JOB_SEARCH_ORGANIC&refId=XUUXD7jduifZpMXMz48Iow%3D%3D&trackingId=oL3UCVXkpKDuL0Nsh8MktA%3D%3D&trk=flagship3_search_srp_jobs,3764317455,"About the job
            
 
Are you ready to embark on a transformative journey in the realm of data engineering? We are looking for a seasoned professional to join our dynamic team as a Senior Data Engineer. In this role, you will play a pivotal part in designing, optimizing, and maintaining data pipelines that propel our clients to new heights. If you're passionate about leveraging cutting-edge technologies and have a knack for turning complex data challenges into streamlined solutions, this might be the perfect opportunity for you.About UsWe provide Data & Analytics expertise to drive measurable business outcomes, often solving complex business problems for our clients. Our data analytics advisory services enable our customers to transform data into insights by driving a culture of empowerment and ownership of results. Our team consists of highly motivated individuals who are passionate about learning, understanding, collaborating, and who are intellectually curious.Position: Senior Data Engineer - Full-timeKey ResponsibilitiesSystem Architecture: Develop and maintain scalable and efficient data pipelines for ETL processes, utilizing AWS, Databricks, Python, and SQL technologies.Cross-functional Collaboration: Collaborate with various teams to grasp data requirements and implement solutions aligning with business needs.Performance Optimization: Enhance and troubleshoot existing data pipelines to improve performance and reliability.Data Quality Assurance: Implement processes for data quality and validation to ensure accuracy and consistency.Continuous Improvement: Stay informed on industry trends and best practices in data engineering for ongoing enhancement of our data infrastructure.Stakeholder Support: Work closely with data scientists, analysts, and other stakeholders, providing infrastructure and support for data-related initiatives.Data Assembly: Compile large, complex data sets that meet business requirements effectively.Process Enhancement: Identify and implement internal process improvements, optimizing infrastructure for scalability and automating manual processes.Migration Expertise: Proven experience in executing a data migration.RequirementsEducation: Bachelor's degree or experience in Data Science and Management.Experience: Proven experience as a Data Engineer or in a similar role, showcasing proficiency in data manipulation.Technical Skills: Strong SQL proficiency, experience with relational databases, and competency in at least one programming language (e.g., Python, Java, Scala) for data processing and scripting.Data Modeling: Knowledge of data modeling and database design principles.Cloud Familiarity: Understanding of cloud platforms such as AWS, Azure, or Google Cloud.Problem-solving and Communication: Excellent problem-solving skills and effective communication abilities.Preferred QualificationsEducation: Bachelor’s degree in Computer Science, Engineering, Mathematics, or a related field.Stream Processing: Experience with stream processing technologies.Data Governance: Understanding of data governance and security best practices.Certifications: Experience and certifications in AWS and Databricks technologies.Work EnvironmentRemote work from home.Hours: Monday through Friday, specific business hours will depend on client needs.Physical DemandsMust be able to remain in a stationary position 50% of the time.The person in this position must occasionally move about inside the office to access file cabinets, library stacks, office machinery, etc.Constantly operates a computer and other office productivity machinery, such as a calculator, copy machine, and printer.The person in this position frequently communicates with clients and coworkers. Must be able to exchange accurate information in these situations.BenefitsUnlimited Discretionary Time Off PolicyInsurance (medical, dental, vision) for employees100% company paid - short and long-term disability insurance for employees100% company paid - life insurance and AD&D insurance for employees100% company paid – employee assistance programRetirement plans with company matchTraining and Certification Reimbursement annuallyPerformance-based incentive programCommission incentive programProfit Sharing PlanReferral BonusesWe are an Equal Opportunity Employer. We believe in the power of diversity and inclusion.Employment Type: Full-Time"
Senior Data Engineer,FinTech LLC,United States (Remote),https://www.linkedin.com/jobs/view/3770640001/?eBP=JOB_SEARCH_ORGANIC&refId=XUUXD7jduifZpMXMz48Iow%3D%3D&trackingId=61KxsXsMXy4p%2Fo9%2Bm8A0QA%3D%3D&trk=flagship3_search_srp_jobs,3770640001,"About the job
            
 
About Client:The client is a global provider of digital business transformation, digital engineering, and information technology (IT) outsourcing services that accelerate our clients’ journey to their digital future. The company readily understands its clients' business challenges and uses its domain expertise to deliver innovative applications of technology to address their critical business challenges.It helps clients execute successful end-to-end digital business transformation initiatives.Rate Range: $50-$53/Hr.Job Description: Overall 10+ in IT Experience. Minimum of 7+ years of experience in Data Engineering.
Required Technical Skills: Python, PySpark, SQL, AWS
Required Domain: Healthcare/Insurance Candidate is expected to have experience in Shell scripting Extensive data analysis skills and Good communication skills
Skill Matrix: SkillYears of ExperienceRating Out of 10Data Engineer   Python   Pyspark   AWS   SQL   Shell Scripting   Healthcare   Insurance About ApTask:Join ApTask, a global leader in workforce solutions and talent acquisition services, as we shape the future of work. We offer a comprehensive suite of offerings, including staffing and recruitment services, managed services, IT consulting, and project management, providing unparalleled opportunities for professional growth and development. As a member of our dynamic team, you'll have the chance to connect businesses with top-tier professionals, optimize workforce performance, and drive success for our clients across diverse industries. If you are passionate about excellence, collaboration, and innovation, and aspire to make a meaningful impact in the world of work, come join us at ApTask and be a part of our mission to empower organizations to thrive.Applicants may be required to attend interviews in person or by video conference. In addition, candidates may be required to present their current state or government issued ID during each interview. Candidate Data Collection Disclaimer:At ApTask, we prioritize safeguarding your privacy. As part of our recruitment process, certain Personally Identifiable Information (PII) may be requested by our clients for verification and application purposes. Rest assured, we strictly adhere to confidentiality standards and comply with all relevant data protection laws. Please note that we only collect the necessary information as specified by each client and do not request sensitive details during the initial stages of recruitment. If you have any concerns or queries about your personal information, please feel free to contact our compliance team at businessexcellence@aptask.com ."
Senior Data Engineer - (Remote - Latin America),Tesorio,"Only, TN (Remote)",https://www.linkedin.com/jobs/view/3723353144/?eBP=JOB_SEARCH_ORGANIC&refId=XUUXD7jduifZpMXMz48Iow%3D%3D&trackingId=SG2B%2FA4vGgjCkbw6zEjFfw%3D%3D&trk=flagship3_search_srp_jobs,3723353144,"About the job
            
 
Tesorio empowers the Fortune 5,000,000 to run as if they had a Fortune 500 finance team. We plug into a company's cashflow data sources and leverage data science to surface the most impactful actions they can take to optimize their cash flow. Then, we let them implement those actions with the click of a button.We listen to our customers, and empower them to share their best practices and wishlist with us to make our product better every day. We are developing machine learning algorithms to understand business cash needs, predictive algorithms to forecast future cash flow, and a sleek UI/UX to make our products enjoyable to work with.We're looking for talented data engineers to take ownership over the data pipelines and workflows required to drive the data science, machine learning, and analytics that powers our platform. You'll be joining an early-stage company with a small, tight-knit team, backed by top-tier VCs (including First Round, Floodgate, Fuel and Y Combinator). You'll work closely with the entire engineering team, our Head of Product, and the co-founders. Learn more at tesorio.com.About You You must be comfortable owning end-to-end data engineering pipeline as part of a small teamYou can independently execute on data engineering projects from concept to completion You're looking to have a large impact on the success of the businessYou have strong opinions, but you hold them looselyYou're always learningYou love being a crucial part of a team that is building and shipping magical products that will help thousands of companiesYou enjoy the dynamic and fast-paced nature of a startup
What you’ll do day-to-day Take ownership over the current and future state of the data architectureCollaborate with engineers, data scientists, and product managers to understand their data requirements and translate them into designBuild scalable data frameworks and data pipelines required to support data science, analytics, machine learning, and product use casesOwn the data quality, efficiency, automation, and observability of data pipelines and data transformationsPartner with different teams within the company to drive data driven projects and deliver data artifacts that integrate seamlessly into our product experienceTackle a wide variety of technical problems and contribute daily to improve system health and code base
The ideal candidate Bachelor's degree in Computer Science, Engineering, or related disciplineHas 5+ years of work experience (including 3+ years in data engineering)Proficiency in SQL and one high-level programming language (preferably python)Experience with one or more data orchestration, big data processing and transformation frameworks (for example: Airflow, DBT)Experience with one or more SQL stores and data warehouse (for example: Postgres, Snowflake) is requiredExperience with docker and kubernetesExperience with other related technologies (for example: NoSQL data stores, Pandas) a plusIs resourceful and agile, and remains positive in the face of problemsEmpathetic towards colleagues and usersExcited about the challenge of working in a fast-paced environment with a small and talented team
Note: we currently cannot sponsor visas."
Senior Data Engineer,Filevine,United States (Remote),https://www.linkedin.com/jobs/view/3765138060/?eBP=JOB_SEARCH_ORGANIC&refId=XUUXD7jduifZpMXMz48Iow%3D%3D&trackingId=9Msa53Dlhdh8mjdtI0Ztkg%3D%3D&trk=flagship3_search_srp_jobs,3765138060,"About the job
            
 
About Filevine:Filevine is changing the way legal work gets done for law practitioners and their clients. As the leading legal operating system, Filevine is dedicated to empowering organizations with tools to simplify and elevate complex, high-stakes legal work. Powering everything from document and case management to timekeeping, billing and business analytics, over 3,400 law firms and legal teams use Filevine daily to deliver excellence.2023 was a groundbreaking year for Filevine, as we launched a suite of AI-powered features that are transforming the legal industry.  LeadsAI helps law firms evaluate cases faster, analyze client sentiment, identify potential problems, and predict case success DemandsAI is an AI-driven demand letter generation solution that helps law firms prepare demand letters more quickly and accurately ImmigrationAI streamlines the immigration process by automating tasks, reducing errors, and ensuring accuracy AI Fields is a powerful tool that can enhance legal work by minimizing manual tasks, facilitating fact-checking, and quickly answering complex queries
With these groundbreaking AI features, Filevine is empowering law firms and legal teams to deliver excellence to their clients with unprecedented speed and efficiency.We are seeking a highly skilled and experienced Senior Data Engineer with expertise in structured and unstructured data architectures to join our dynamic team. As a Senior Data Engineer, you will play a crucial role in developing and implementing data workflows and access to extract insights from complex textual data and solve challenging business problems. You will have the opportunity to work on cutting-edge projects and contribute to the development of innovative solutions.Responsibilities: Develop and maintain robust data architecture leveraging Snowflake and QdrantCreate Data Warehouse infrastructure for use in Data Science, Analytics and Reporting functionalitiesDevelop and maintain ETL (Extract, Transform, Load) pipelines to ensure data consistency and availabilityHelp define workflow and datastores to get data out of unstructured stores and into usable Data Science formatsCollaborate with cross-functional teams, including product owners, software developers, and domain experts, to understand business requirements and develop end-to-end data solutionsCommunicate findings, insights, and technical concepts effectively to both technical and non-technical stakeholders through reports, presentations, and visualizationsSupport implementation of analytics tools and methodologies within our engineering tech stack. 
Requirements: Relevant experience and/or Master's or Ph.D. degree in Computer Science, Economics, Statistics, or a related data fieldStrong background and expertise in data warehousing techniques, including structured and unstructured data formatsProven experience in designing and implementing data warehouses for application useProficiency in programming languages such as Python, Spark or Java, and SQLExperience with data manipulation, analysis, and visualization using tools such as SQL, BI Tooling, PandasSolid knowledge of software development practices, version control systems, and agile methodologiesExcellent problem-solving skills, analytical thinking, and attention to detailEffective communication skills and ability to collaborate in a team-oriented environmentProven track record of delivering high-quality results on time and effectively managing high profile projects and priorities. 
Preferred: Experience with true big data (exabytes and higher) procession practices. Knowledge of cloud computing platforms such as AWS, Azure, or GCPAbility to mentor and educate on Data Engineering deployment and best practices to technical groups 
Cool Company Benefits:  A dynamic, rapidly growing company, focused on helping organizations thrive Medical, Dental, & Vision Insurance (for full-time employees) Competitive & Fair Pay Maternity & paternity leave (for full-time employees) Short & long-term disability Ergonomic and height-adjustable workstations for onsite employees Opportunity to learn from a dedicated leadership team Weekly Taco Lunches in the summer/fall/spring for onsite employees Centrally located open office building in Sugar House Flexible hybrid work schedules depending on the department with some departments offering fully remote positions in the United States (R&D) Top-of-the-line company swag"
Remote Data Engineer (Oracle Graph),RemoteWorker US,"Raleigh, NC (Remote)",https://www.linkedin.com/jobs/view/3780836744/?eBP=JOB_SEARCH_ORGANIC&refId=XUUXD7jduifZpMXMz48Iow%3D%3D&trackingId=XiywwFjT7qG2rinvZh5YSQ%3D%3D&trk=flagship3_search_srp_jobs,3780836744,"About the job
            
 
Title: Data Engineer Terms: W2 Contract/ 2 yrs + Location: Remote in USA/Must be able to work PST Hours Qualifications 5+ yrs of data engineering experience 1+ yr of experience working with Oracle Graph Database"
Enterprise Data Engineer,Open Systems Inc.,United States (Remote),https://www.linkedin.com/jobs/view/3769269259/?eBP=JOB_SEARCH_ORGANIC&refId=XUUXD7jduifZpMXMz48Iow%3D%3D&trackingId=zshwzlACHFi0HUfZK9OFfQ%3D%3D&trk=flagship3_search_srp_jobs,3769269259,"About the job
            
 
Enterprise Data Engineer – (Contract Position)Atlanta, GA RemoteJob DescriptionOur client is seeking an experienced hands-on enterprise Data Engineer lead. The successful candidate must have Big Data engineering experience and must demonstrate an affinity for working with others to create successful solutions. They must be a great communicator, both written and verbal, and have some experience working with business areas to translate their business data needs and data questions into project requirements. The candidate will participate in all phases of the Data Engineering life cycle and will work independently and collaboratively write project requirements, architect solutions, and perform data ingestion development and support duties.This Data Engineer will be responsible for creating new data flows into AWS including coordinating with business and functional areas to establish and communicate our data fabric – best practices, framework, and tools. Together we will establish a new data fabric for our client that will help create a common view of data and provide a centralized mechanism for its aggregation, cleansing, transformation, augmentation, validation, and syndication.EducationB.A./B.S. degree or equivalent work experience in computer science, information technology, business administration, engineering, or another relevant fieldResponsibilities Sharing project solutions and outcomes with colleagues to improve delivery on future projectsAnalyzing and translating business needs into long-term solution data pipelines.Evaluating existing data systems.Working with the development team to create conceptual data flows.Developing best practices for data coding to ensure consistency within the system.Reviewing modifications of existing systems for cross-compatibility.Implementing data strategies and developing data integration points.Evaluating implemented data systems for variances, discrepancies, and efficiency.Troubleshooting and optimizing data systems.Interpreting and delivering impactful strategic plans improving data integration, data quality, and data delivery in support of business initiatives and roadmapsFormulating and articulating architectural trade-offs across solution options before recommending an optimal solution ensuring technical requirements are metMotivating and developing staff through teaching, empowering, and influencing technical and consulting “soft” skillsDriving innovative technology solutions through thought leadership on emerging trends
SkillsRequired:  6+ years of overall IT experience3+ years of experience with high-velocity high-volume stream processing: Apache SparkExperience with real-time data processing and streaming techniques using Spark structured streaming and KafkaDeep knowledge of troubleshooting and tuning Spark applications3+ years of experience with data ingestion from Message Queues (Tibco, IBM, etc.) and different file formats across different platforms like JSON, XML, CSV3+ years of experience with Big Data tools/technologies like Hadoop, Spark, Spark SQL, Kafka, Sqoop, Hive, S3, or HDFS3+ years of experience building, testing, and optimizing ‘Big Data’ data ingestion pipelines, architectures, and data sets2+ years of experience with Python (and/or Scala) and PySpark/Scala-Spark3+ years of experience with AWS cloud platform3+ years of experience with database solutions like Databricks or Snowflake2+ years of experience with NoSQL databases, including HBASE and/or CassandraKnowledge of Unix/Linux platform and shell scripting is a mustStrong analytical and problem-solving skills
Preferred (Not Required) Experience with Cloudera/Hortonworks CDP, HDP and HDF platformsStrong SQL skills with ability to write intermediate complexity queriesStrong understanding of Relational & Dimensional modelingExperience with GIT code versioning softwareExperience with REST API and Web ServicesGood business analyst and requirements gathering/writing skills
Who We AreOpen Systems Inc. (OSI) was founded in 1994 to provide information technology solutions and staffing services to large and mid-size companies across the U.S. Our corporate office is located at 6495 Shiloh Road, Ste 310 Alpharetta, GA 30005. We provide a full range of staffing services including contract, contract-to-hire, and direct hire solutions. Our technical recruiting experts are experienced in technical screening, candidate sourcing, and behavioral interviewing techniques. They focus on providing candidates who match your technical requirements and fit seamlessly into your company culture.Contact Open Systems, Inc. anytime by website, phone or email. We look forward to hearing from you!!"
Azure Data engineer with Azure data factory and pyspark-- EST/CST-no corps --remote,RemoteWorker US,"San Antonio, TX (Remote)",https://www.linkedin.com/jobs/view/3780838628/?eBP=JOB_SEARCH_ORGANIC&refId=XUUXD7jduifZpMXMz48Iow%3D%3D&trackingId=UO05Nwmw%2BUJx1NZyoHhsRg%3D%3D&trk=flagship3_search_srp_jobs,3780838628,"About the job
            
 
NOTICE- Any pay ranges displayed are estimations. Actual pay is determined by an applicant's experience, technical expertise, and other qualifications as listed in the job description. All qualified applicants are welcome to apply. Senior Associate should have hand on PySpark, ADF, ADLS, Delta tables, SparkSQL, etc. Architects, in addition to the above must have Architecture experience designing solutions and frameworks ground up and not to just build data pipelines using the existing setup Top Skill Sets/ Experiences Required: These requirements must be strong and reflected on the resume: Azure Data Factory Azure Pyspark, Databricks APIs Your Skills & Experience: Demonstrable experience in enterprise level data platforms involving implementation of end to end data pipelinesGood communication and willingness to work as a teamHands-on experience with at least one of the leading public cloud data platforms (Amazon Web Services, Azure or Google Cloud)Experience with column-oriented database technologies (i.e. Big Query, Redshift, Vertica), NoSQL database technologies (i.e. DynamoDB, BigTable, Cosmos DB, etc.) and traditional database systems (i.e. SQL Server, Oracle, MySQL)Experience in architecting data pipelines and solutions for both streaming and batch integrations using tools/frameworks like Glue ETL, Lambda, Google Cloud DataFlow, Azure Data Factory, Spark, Spark Streaming, etc.Ability to handle multiple responsibilities simultaneously in leadership and contributing to tasks “hands-on”Understanding of data modeling, warehouse design and fact/dimension concepts Thanks Renu Goel 857-207-2676 renu.goel@yoh.com"
Data engineer with Azure data Factory :: REMOTE,"LanceSoft, Inc.","Raleigh, NC (Remote)",https://www.linkedin.com/jobs/view/3764201334/?eBP=JOB_SEARCH_ORGANIC&refId=XUUXD7jduifZpMXMz48Iow%3D%3D&trackingId=fqCiFFIdU8qu0jgKj8YVNw%3D%3D&trk=flagship3_search_srp_jobs,3764201334,"About the job
            
 
Looking for Data enginner include Oracle, DB2, MySQL, and SQL Server, as well as desktop-based applications such as MS Access and MS Excel. establish an enterprise data model with a primary goal of standardizing shared data entities and attributes and a secondary goal of standardizing database management systems and tools.Related transactional databases and data warehouses will be hosted in Azure, generally using Microsoft Dataverse and SQL Server. The Senior Data Analyst/Architect will provide analysis and leadership on data analysis projects across the client enterprise. You will also provide subject matter expertise related to Microsoft Dataverse, SQL Server, Azure Data Factory, and SSIS.Skills Experience with Azure Data Factory as an ETL tool.Experience with MS Dataverse is Required.Experience with MS SSIS.Proven track record of solving real world problems using data. Excellent critical thinking skills is a must have for this role..Understanding of different database platforms: Oracle, SQL Server (On-Prem / Azure), DB2, and MS Access.Understanding of Data Warehousing and data mining.Understanding of modeling strategies (dimensional, snowflake, relational, unstructured).Strong interest in playing a technical data steward role across our business and technology partners to understand.Proven experience with ERD/Data Modelling tools (Toad or others).Experience in executing projects in an Agile / Hybrid environment.Experience with data reporting tools (Power BI, others)."
Senior Data Engineer,Freestar,"Los Angeles, CA (Remote)",https://www.linkedin.com/jobs/view/3100528488/?eBP=JOB_SEARCH_ORGANIC&refId=XUUXD7jduifZpMXMz48Iow%3D%3D&trackingId=iWSIRFEkR%2FpQrF1hxldN%2FA%3D%3D&trk=flagship3_search_srp_jobs,3100528488,"About the job
            
 
About FreestarFreestar engineer teams develop cutting-edge monetization solutions for websites and mobile apps. By combining industry-leading technology, data, and massive scale, we enable busy site owners and mobile app publishers to seamlessly maximize revenue while freeing themselves of the hassles of ad operations. The result is publishers having more time to do what they do best: create content and focus on their apps.Senior Engineer (Remote)We’re looking for a Senior Engineer for our Data Platform Team - the product that drives our core business. As a Senior Engineer at Freestar, you'll be one of the main contributors to a talented team of developers to innovate features for our product that separate us from our competitors.What You'll Be Doing Run points on whole projects by recommending, prototype, build and debug data infrastructures on Google Cloud Platform (GCP) with other principal and senior engineers within the team, and to mentor less experienced Data Engineers. Demonstrate repeated delivery of project architectures and critical components that other engineers demur to you for lack of expertise.Participate in early-stage conversation with our product development team about product / features. Improve decision quality across the company by ensuring metrics are trustworthy, discoverable, and easily consumable.
Skills And Experience You'll Need To Have Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive, Kafka, PubSub).4+ years of experience designing and implementing large-scale, complex, data-driven applications on the cloud, preferably on Google Cloud / AWS.4+ years of hands-on experience using SQL to perform complex data manipulation3+ years of experience modeling data warehouses3+ years of experience building data pipelines, CI/CD pipelines, and fit for purpose data storesData migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.Experience writing software in one or more languages such as Python, Java, Scala, etc.Experience with systems monitoring/alerting, capacity planning and performance tuningEnjoy analyzing and organizing rapidly-changing business data to support product and business solutionsNice to have Qualifications: *Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, BigQuery, Dataprep, Composer, etc)Experience with IoT architectures and building real-time data streaming pipelinesExperience operationalizing machine learning models on large datasetsDemonstrated leadership and self-direction -- a willingness to teach others and learn new techniquesDemonstrated skills in selecting the right statistical tools given a data analysis problemComfortable interacting across multiple teams and management levels within the organizationPrevious background in the ad tech or media landscape (linear, digital, or social) is a plus
Why You Want To Work With Us Full-Time, Salaried PositionWorking remotelyGenerous Medical, Dental, and Vision benefits401K with company match, vested immediatelyThe opportunity to be part of something high value, high impact, and high growth...Who doesn't love that!
Freestar is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran or disabled status, or genetic information."
Senior Data Engineer (Remote),ICF,"Reston, VA (Remote)",https://www.linkedin.com/jobs/view/3775904975/?eBP=JOB_SEARCH_ORGANIC&refId=XUUXD7jduifZpMXMz48Iow%3D%3D&trackingId=yJ99mARJjfNtun1gt8ACUw%3D%3D&trk=flagship3_search_srp_jobs,3775904975,"About the job
            
 
ICF is a mission-driven company filled with people who care deeply about improving the lives of others and making the world a better place. Our core values include Embracing Difference; we seek candidates who are passionate about building a culture that encourages, embraces, and hires dimensions of difference. Our SemanticBits team works side by side with customers to articulate a vision for success, and then make it happen. We know success doesn't happen by accident. It takes the right team of people, working together on the right solutions for the customer.We are seeking a talented Data Engineer who is responsible for creating OLTP queries used by front-end engineers to display reports to the end users. You will be working with a team of likeminded data engineers. If you are someone who wants your work to contribute to systems that collect healthcare data used by hundreds of thousands of daily users, we want to (virtually) meet you!You will work on projects that support the Centers for Medicare and Medicaid Services (CMS) as we develop a next-generation analytics and reporting system that directly impacts healthcare quality. This program allows for the continued quality of clinicians’ work according to CMS standards. We are a collaborative company, so we want you to use your knowledge of Spark to teach others, inform design decisions, and debug runtime problems.Our mission is to help the government improve healthcare for patients and reduce costs. We value individuals that are experts in their disciplines, highly communicative, and self-motivated to own their work. Technology and domain experts work side-by-side in highly dynamic teams with all the roles necessary to deliver high-quality digital services. In addition, critical to our success is forming teams of highly diverse individuals passionate about making a difference.Tools & Technology Postgres, Python, SQL, Stored Procedures Jenkins AWS Redshift Git and GitHub AWS AirflowConfluence PySparkDBTAWS services such as EMR, EKS, Lambda
Key Responsibilities Write new database stored procedures and functions to support QMIR data needs. Write new and modify. Existing scripts to maintain QMIR data.Maintain existing stored procedures and functions in Redshift and Postgres database. Support Production issues with investigation and performance tuning of SQL code. Maintain and perform alembic migrations scripts in Git repo for script changes and incident tickets. 
Basic Qualifications Master’s degree with 3 years’ experience, Bachelor’s Degree with 5 years’ experience, or Associates degree with 7 years’ experience. 5+ years of experience with Python, Postgres, SQL, and Redshift Must have lived in the United States for 3 full years of the last 5 yearsCandidate must be able to obtain and maintain a Public TrustCandidate must reside in the US, be authorized to work in the US, and work must be performed in the US
Preferred Qualifications Experience working in the healthcare industry with PHI/PII Federal Government contracting work experience. Expertise working as part of a dynamic, interactive Agile team. Strong written and verbal communication skills Demonstrated time management skills. Strong organizational skills with attention to detail Curiosity about how things work, ability to look out for potential risks. 
Job Location: This position requires that the job be performed in the United States. If you accept this position, you should note that ICF does monitor employee work locations and blocks access from foreign locations/foreign IP addresses, and also prohibits personal VPN connections.Working at ICFICF is a global advisory and technology services provider, but we’re not your typical consultants. We combine unmatched expertise with cutting-edge technology to help clients solve their most complex challenges, navigate change, and shape the future.We can only solve the world's toughest challenges by building an inclusive workplace that allows everyone to thrive. We are an equal opportunity employer, committed to hiring regardless of any protected characteristic, such as race, ethnicity, national origin, color, sex, gender identity/expression, sexual orientation, religion, age, disability status, or military/veteran status. Together, our employees are empowered to share their expertise and collaborate with others to achieve personal and professional goals. For more information, please read our EEO & AA policy.Reasonable Accommodations are available, including, but not limited to, for disabled veterans, individuals with disabilities, and individuals with sincerely held religious beliefs, in all phases of the application and employment process. To request an accommodation please email icfcareercenter@icf.com and we will be happy to assist. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. Read more about non-discrimination: Know Your Rights and Pay Transparency Statement.Pay Range - There are multiple factors that are considered in determining final pay for a position, including, but not limited to, relevant work experience, skills, certifications and competencies that align to the specified role, geographic location, education and certifications as well as contract provisions regarding labor categories that are specific to the position. The pay range for this position is:$82,673.00 - $140,544.00Nationwide Remote Office (US99)"
W2 only - Remote work - Data Analytics Engineer - 10+ years experienced only,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3698354357/?eBP=JOB_SEARCH_ORGANIC&refId=XUUXD7jduifZpMXMz48Iow%3D%3D&trackingId=qR1i0AbTdR2KlB5PvgK31w%3D%3D&trk=flagship3_search_srp_jobs,3698354357,"About the job
            
 
remote rolew2 candidates only10+ Years exp neededManager NotesExperience with cloud computing platforms, preferably AWS with prior experience developing and optimizing DynamoDB, Lambdas, Step Functions, Glue, S3, RedshiftStrong proficiency in developing, debugging ETL pipelines using Data Integration tool Pentaho or similar ETL tools such as Talend, InformaticaHands-on programming experience in Python or Java with focus towards building ETL workflows and data driven solutionsExperience with big data batch computing tool Spark and developing distributed data processing solutionsIn-depth knowledge of warehousing concepts such as star schema and experience using variety of data stores (RDBMS, analytic database, scalable document stores)Strong expertise in developing and debugging SQL scripts, functions, stored procedures, triggersSolid understanding of software engineering tools, workflows (CI/CD, git)Good understanding and business acumen in financial industries, preferably insuranceProficiency in Data visualization tools preferably Tableau, and Pentaho or similar tools such as Power BI, lookerOther Preferred SkillsWorking knowledge on shell scripting, TCP/IP networking concepts, JenkinsETL (DataStage & Autosys)PythonSQLData Modeling with Oracle and SQL DatabasesData & Analytics Development Engineer Primary Role Definition: Data & Analytics developers are people who works with ETL (extract data, transform it, and load it) tools like DataStage, Autosys scheduling tools.Skills Required Very skilled in Python and DataStage coding and developmentVery skilled in performing Python/Data Stage Code reviewsVery skilled in SQLVery skilled in Data Modeling with both Oracle and SQL DatabasesDatabase query tuningCloud experience (AWS/GCP) is a plus.
Hit ground running, self-starter, great communicator (verbally and writtenWill have to support nightly production batch cycles on a rotation basisRajesh or Tamil will screen for interviews; schedule hands on coding interview and technical screening after short screen with candidates they like. Hands on coding should include SQL and Python.A lot of moving parts, scrum calls, tasks to do, meetings to discuss successes and failuresImpact to AmFam: expand sales channel capabilities to utilize IA channelInterviews will be conducted via Teams video calls.Remote work is permitted with 8 hours workday (excl breaks), No Overtime hoursVery strong in the following areas DW methodologies, Data Marts, Cloud experience (preferably GCP), Data Ingestion via multiple sources like SQL server, mainframe files, APIs, JSON feeds etc using Python, DataStage, LinuxNice to have Guidewire, Autosys, developing Architecture and Dataflow diagramsSelf-motivated, thorough with quality work they do, work with minimal supervisionStrong verbal and communication skills, effective communication is essential as team dynamics change quicklyNo customer service skills are needed but will interact with internal business partnersInsurance is strongly preferred but not mandatory10 years of experience"
"GCP Data Engineer - Detroit, MI (Initial Remote) - Lorven Technologies Inc.",Lorven Technologies Inc.,"Detroit, MI (Remote)",https://www.linkedin.com/jobs/view/3617894308/?eBP=JOB_SEARCH_ORGANIC&refId=XUUXD7jduifZpMXMz48Iow%3D%3D&trackingId=wltD0H6H0O3s0X59Raj64A%3D%3D&trk=flagship3_search_srp_jobs,3617894308,"About the job
            
 
Hi,Our client is looking GCP DATA ENGINEER for a Long term project in Detroit, MI (Initial Remote) below are the detailed requirements.Job Title: GCP DATA ENGINEERLocation: Detroit, MI (Initial Remote)Duration: Long termResponsibilities Bachelor's degree in Computer science or equivalent, with a minimum of 10+ years of experience.You will be responsible for developing scalable big data pipeline solutions. Advanced knowledge of the GCP ecosystem with a focus on Big Query.Designing and coding Big Query to analyze data collections.Analyze user needs to determine how software should be built or if existing software should be modified.Participate in design, delivery estimates and code reviews Develop and/or perform software automated testing procedures.Translate business requirements and specifications into usable and scalable software.Process and understand capabilities and limitations of data outputs from the software. Understand and assist with the technical infrastructure of an application or system. Determine and execute the software deployment process and troubleshoot performance issues.Develop data quality and validation routines.Build distributed reliable and scalable data pipelines to ingest and process data in real-time and other unstructured data. Skills Required: Data design, data architecture and data modeling (both transactional and analytic) Demonstrate excellent communication skills, including effectively communicating with internal and external customers.Ability to use strong industry knowledge to relate to customer needs and dissolve customer concerns and high level of focus and attention to detail.Strong work ethic with good time management with ability to work with diverse teams and lead meetings.
NoteTHESE EMAILS ARE GENERATED BY KEYWORD AND I APOLOGIZE IF THESE SKILLS SETS DO NOT MATCH YOUR EXPERTISE, OR IF THE LOCATION IS OUT OF RANGE.We do have other opportunities available. If you are interested, please send me your latest resume. If you are not currently seeking employment, or if you would prefer, I contact you at some later date, please indicate your date of availability so that I may honor your request."
Remote Opportunity: Sr. Data Engineer,SPAR Information Systems LLC,United States (Remote),https://www.linkedin.com/jobs/view/3777290939/?eBP=JOB_SEARCH_ORGANIC&refId=XUUXD7jduifZpMXMz48Iow%3D%3D&trackingId=GweTXyLT3Ag6eLPCsfiSow%3D%3D&trk=flagship3_search_srp_jobs,3777290939,"About the job
            
 
Hello All, Hope you are doing great. Please go through the job description and let me know your interest. Role: Sr. Data Engineer Locations: Remote Duration: 3 Months Contract to hire Requirement: Client treats Data as Product and our Senior Engineer will be a key member of the engineering staff working across Business Services Engineering, Data Engineering, Platform Engineering, and Infrastructure Engineering to ensure that we provide a fiction-less experience to our customers, maintain the highest standards of protection and availability. Our team thrives and succeeds in supporting Data Driven company and delivering high quality technology products and services in a hyper-growth environment where priorities shift quickly. The ideal candidate has broad and deep technical knowledge in data, typically ranging from front-end UIs through back-end systems and all points in between. Required: 3+ years of experience in data software development, programming languages and developing with big data technologies 2+ years of experience designing and building on existing and new data applications 2+ years of experience in Cloud DevOps concepts, Cloud Services and Architecture, and Azure/AWS/GCP DevOps Operational Framework 1+ years of experience in open-source data tools and frameworks, or one of the following: .net Core, asp.Net, Angular, or Express. Additional Skills: Experience in data software development, using data technologies such as Relational & NoSQL databases, open data formats, and programming languages such as Python, Scala, and/or other frameworks, building data pipelines (ETL and ELT) with batch or streaming ingestion, error handling, loading, and transforming data, and developing with big data technologies such as Spark, Hadoop, and MapReduce. Experience with analytics solutions. Experience in development using Python or PySpark, Spark, Scala. Advanced understanding of designing and building for data quality assurance, reliability, availability, and scalability, on existing and new data applications. Advanced understanding of DevOps Concepts, Cloud Architecture, and Azure DevOps Operational Framework, Pipelines, Kubernetes. Advanced understanding of designing and building solutions for data quality and observability, metadata management, data lineage, and data discovery. Advanced understanding of building products of micro-services oriented architecture and extensible REST APIs. Advanced understanding of open-source frameworks. Experience with continuous delivery and infrastructure as code. Experience in existing Monitoring Portals: Splunk or Application Insights. Advanced understanding of Security Protocols & Products: Understanding of Active Directory, Windows Authentication, SAML, OAuth. Advanced understanding of Azure Network (Subscription, Security zoning, etc) & tools like Genesis. Advanced understanding of existing Operational Portals such as Azure Portal. Knowledge of CS data structures and algorithms. Knowledge of developer tooling across the software development life cycle (task management, source code, building, deployment, operations, real-time communication). Practical knowledge of working in an Agile environment (Scrum/Kanban/SAFe). Strong problem-solving ability. Ability to excel in a fast-paced, startup-like environment. Bachelor's degree in Computer Science, Information Systems, or equivalent education or work experienceThanks & Regards,Satnam SinghDirect: 201 623 3660Email : Satnam.singh@sparinfosys.com"
Azure Data Engineer [REMOTE],eStaffing Inc.,United States (Remote),https://www.linkedin.com/jobs/view/3645149467/?eBP=JOB_SEARCH_ORGANIC&refId=XUUXD7jduifZpMXMz48Iow%3D%3D&trackingId=fyT%2BIKn01yCAuM%2FKZtetrw%3D%3D&trk=flagship3_search_srp_jobs,3645149467,"About the job
            
 
Position: Azure Data EngineerClient: Retail ServicesLocation: REMOTEDuration: C2C & C2HJob RequirementsBasic Qualification  Bachelor's degree in Computer Science, Information Systems, Business Administration, or other related field.Minimum of 7+ years IT experience encompassing the following:Working experience with SDLC and deliverables associated with each phase.Experience in Data Modeling and Advanced SQL techniquesExperience engineering data pipelines using latest technologies and techniquesExperience in Data Visualization Best PracticesExperience working with industry-leading Business Intelligence toolsExperience working with industry-leading database technologiesExperience in Azure Cloud technologiesExperience in Big Data / Hadoop
Technical Competencies Develop Business Intelligence applications/dashboards/Reports in MicroStrategy and Power BI.Develop complex SQL/USQL/Py Spark code for Data Engineering pipelines in Azure Data Lake analytics and azure data factory.Develop complex data science algorithms using R programming language and industrialize them.Develop and Implement big data applications using Hadoop components (HDInsight - Microsoft Azure).Create requirement documents, review the functional design documents created by vendor partners as a part of software development life cycle.Work closely with business teams in every stage from gathering requirements to closure of project.Work with the key stakeholders to create requirements for each business solution, ensure the requirements are developed in concert with and agreed upon by the business partner, and that the solution delivers the agreed upon business needs."
Senior Data Engineer - Remote - Not Less than 12+ Years,Enexus Global Inc.,"California, United States (Remote)",https://www.linkedin.com/jobs/view/3778378739/?eBP=JOB_SEARCH_ORGANIC&refId=XUUXD7jduifZpMXMz48Iow%3D%3D&trackingId=G0y%2FNrFhda20R7YIsuN1BQ%3D%3D&trk=flagship3_search_srp_jobs,3778378739,"About the job
            
 
Job Title - Senior Data Engineer (Not less than 12+ Years)Location - RemoteContract Type - W2/C2C/1099Experience -- 12+ Years onlyResponsibilities Develop and enhance data-processing, orchestration, monitoring, and more by leveraging popular open-source software, AWS, and GitLab automation.Collaborate with product and technology teams to design and validate the capabilities of the data platformIdentify, design, and implement process improvements: automating manual processes, optimizing for usability, re-designing for greater scalabilityProvide technical support and usage guidance to the users of our platform's services.Drive the creation and refinement of metrics, monitoring, and alerting mechanisms to give us the visibility we need into our production services.
Qualifications Experience building and optimizing data pipelines in a distributed environmentExperience supporting and working with cross-functional teamsProficiency working in Linux environment8+ years of advanced working knowledge of SQL, Python, and PySpark5+ years of experience with using a broad range of AWS technologiesExperience using tools such as: Git/Bitbucket, Jenkins/CodeBuild, CodePipelineExperience with platform monitoring and alerts tools"
Data Engineer,Fisker,"Manhattan Beach, CA (Remote)",https://www.linkedin.com/jobs/view/3619682083/?eBP=JOB_SEARCH_ORGANIC&refId=XUUXD7jduifZpMXMz48Iow%3D%3D&trackingId=V909YWWUOaNs4Ao4Rb%2BqaQ%3D%3D&trk=flagship3_search_srp_jobs,3619682083,"About the job
            
 
About Fisker Inc.California-based Fisker Inc. is revolutionizing the automotive industry by developing the most emotionally desirable and eco-friendly electric vehicles on Earth. Passionately driven by a vision of a clean future for all, the company is on a mission to become the No. 1 e-mobility service provider with the world’s most sustainable vehicles. To learn more, visit www.FiskerInc.com – and enjoy exclusive content across Fisker’s social media channels: Facebook, Instagram, Twitter, YouTube and LinkedIn. Download the revolutionary new Fisker mobile app from the App Store or Google Play store.Job Responsibilities Work with large, complex datasets to solve complex analysis problems, applying advanced analytical methods (e.g., statistical and machine learning models) as needed. Conduct analysis that includes problem formulation, data gathering and requirements specification, processing, analysis, ongoing deliverables, and presentations.A deep understanding of Enterprise SAAS business models and metrics like Pipeline, MQLs and Market Mix Modelling.Build and prototype analysis pipelines iteratively to provide insights at scale. Develop comprehensive knowledge of Google data structures and metrics, advocating for changes where needed.Interact cross-functionally, making business recommendations (e.g., cost-benefit, forecasting, experiment analysis) with effective presentations of findings at multiple levels of stakeholders through visual displays of quantitative information.Develop and automate reports, iteratively build and prototype dashboards to provide insights at scale, solving for business priorities.
Skills And Experience 5+ years of professional consulting experience in Data Science or technology consulting or similar roles.5+ years of experience in quantitative marketing data science, product data science, risk modeling, or similar field.3+ years of experience SQL/Teradata as well as modern analytical systems in Azure ecosystem, Spark SQL, PySparkStrong proficiency in running statistical analyses in Python or RKnowledge with data visualization tools such as Power BI preferredExperience with Web Services and REST APIs is preferredExperience leveraging a variety of services to act as data sources such as Azure Data Lake, Azure Synapse Analytics, Azure SQL, Azure EventHub/IoT Hub, etc.Hands-on experience with analytics and big data technologies within Microsoft Azure, with experiences in tools such as Azure Data Factory, Azure Machine Learning, Azure Cognitive Services, Azure Databricks and Azure Synapse Analytics.Knowledge and experience with leveraging distributed techniques for training and scoring machine learning models, ideally using Azure DatabricksKnowledge and experience with one or more cloud available Machine Learning frameworks and tools such as Tensor Flow, PyTorch, ONNX, NumPy, etc.Knowledge and experience with Model Management, ideally using Azure ML service and/or MLFlow as well as deployment of models using Azure Kubernetes Service
Qualifications Advanced degree in Computer Science, Mechanical engineering, Statistics or related STEM field.4+ years of Data or Machine learning Science experience working on highly complex problems in a dynamic setting
Fisker Inc. is an Equal Opportunity Employer; employment at Fisker Inc. is governed based on merit, competence and qualifications and will not be influenced in any manner by race, color, religion, gender, national origin/ethnicity, veteran status, disability status, age, sexual orientation, gender identity, marital status, mental or physical disability or any other legally protected status."
Data Migration Engineer,"MetroSys, Inc.",NAMER (Remote),https://www.linkedin.com/jobs/view/3780457727/?eBP=JOB_SEARCH_ORGANIC&refId=XUUXD7jduifZpMXMz48Iow%3D%3D&trackingId=4djyPqImeHffGW%2Fuzh8Ayw%3D%3D&trk=flagship3_search_srp_jobs,3780457727,"About the job
            
 
Responsibilities: Collaborate with stakeholders to understand data migration requirements, including scope, timelines, and specific data sets.Design and develop a comprehensive data migration plan, considering factors such as data volume, complexity, and business continuity.Utilize Komprise data management software to perform assessments, identify target data, and orchestrate the migration process.Configure and optimize Komprise settings to align with migration goals, ensuring efficient and secure data transfer.Conduct pre-migration validation tests to ensure data integrity and accuracy before the actual migration process.Monitor the data migration process in real-time, addressing any issues or discrepancies as they arise.Implement data validation and reconciliation procedures to verify the successful completion of the migration.Collaborate with cross-functional teams, including storage administrators and system engineers, to ensure seamless integration with the NetApp environment.Document the entire migration process, including configurations, settings, and any custom scripts or workflows used.Provide knowledge transfer and training to internal teams for ongoing management and maintenance of the migrated data.
Requirements: Bachelor's degree in Information Technology, Computer Science, or a related field (preferred) or equivalent work experience.Proven work experience as a Data Migration Engineer with specific expertise in migrating data from Isilon to NetApp using Komprise.Strong proficiency in Komprise data management software and related tools.In-depth knowledge of Isilon and NetApp storage platforms, including file systems, protocols, and administration.Experience with scripting languages (e.g., Python, PowerShell) for automation and customization of migration processes.Excellent problem-solving and analytical skills, with the ability to diagnose and resolve complex data migration issues.Strong communication and interpersonal skills, with the ability to collaborate effectively with technical and non-technical stakeholders.
Powered by JazzHR8OdpMjDiZk"
Senior Data Engineer - Remote,Enexus Global Inc.,United States (Remote),https://www.linkedin.com/jobs/view/3716039716/?eBP=JOB_SEARCH_ORGANIC&refId=XUUXD7jduifZpMXMz48Iow%3D%3D&trackingId=OeoOCxAi2SlOEQpF1eJg7A%3D%3D&trk=flagship3_search_srp_jobs,3716039716,"About the job
            
 
Senior Data EngineerLocation - RemoteContract Type - W2/C2C/1099Minimum Experience - 12+ YearsResponsibilities Develop and enhance data-processing, orchestration, monitoring, and more by leveraging popular open-source software, AWS, and GitLab automation.Collaborate with product and technology teams to design and validate the capabilities of the data platformIdentify, design, and implement process improvements: automating manual processes, optimizing for usability, re-designing for greater scalabilityProvide technical support and usage guidance to the users of our platform’s services.Drive the creation and refinement of metrics, monitoring, and alerting mechanisms to give us the visibility we need into our production services.
Qualifications Experience building and optimizing data pipelines in a distributed environmentExperience supporting and working with cross-functional teamsProficiency working in Linux environment8+ years of advanced working knowledge of SQL, Python, and PySpark5+ years of experience with using a broad range of AWS technologiesExperience using tools such as: Git/Bitbucket, Jenkins/CodeBuild, CodePipelineExperience with platform monitoring and alerts tools
Thanks & RegardsSahil510-925-0283 EXT 131"
Job Opening for Senior Data Solutions Engineer - backfill - Remote,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3657140007/?eBP=JOB_SEARCH_ORGANIC&refId=XUUXD7jduifZpMXMz48Iow%3D%3D&trackingId=HV9b%2Bn1KDxZpfE%2BU%2BiCljA%3D%3D&trk=flagship3_search_srp_jobs,3657140007,"About the job
            
 
Hi,Please find attached Job Description. If you are interested please do share with me your updated resume or call me on ""3025492448"".Title:- Senior Data Solutions Engineer - backfillLocation:- Miami, FLDuration:- 6+ MonthsVisa:- Citizen, GCInterview Mode:- Video Descriptionremote roleNeed last 6 of SSN, Full DOB and LinkedInJob DescriptionThe Senior Data Solutions Engineer is responsible for building, managing, and optimizing complex reusable enterprise data pipelines effectively and in a timely manner through the development lifecycle to be used by internal consumers, such as business/data analysts and data scientists. Additionally, this role will operate in a fast pace environment working closely with the Data Science and Business teams on Revenue Management initiatives which concentrates in building data solutions, curated data to enable intelligent pricing automation, which relies on machine-learning and data-derived business rules. The Senior Engineer would use both technical and analytical skills to understand and solve business problems using available resources and current technology stack, while ensuring data governance and data security compliance.  Create and maintain technical design documentation Gather and document business requirements, data mapping and designing Create, build, and maintain complex data pipelines from disparate sources that meet functional / non-functional business requirements Create, maintain, and reuse existing ETL/ELT processes, employing a variety of data integration and data preparation tools Mentor engineers in finding optimal and efficient solutions for designing, preparing, and storing data for analytical and operational use cases. Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing pipelines for greater scalability, etc. Work with stakeholders including Product, Data and Business teams to assist with data-related technical issues and support their data needs Create datasets for: (1) operational reports, key performance indicators/metrics, or other insights into current organizational activities, (2) analytics and data science to provide the ability to uncover the answers to major questions that help organizations make objective decisions and/or gain a competitive edge Write, debug and implement complex queries involving multiple tables or databases across platform(s) Collaborate with the Enterprise Architecture team to ensure alignment on data standards and processes Work with data and analytics experts to strive for greater functionality in data systems Manages and develops data processing and assigns customers to marketing segments in the CDP; using demographic data, behavioral data, and intent signals and works with data scientists to incorporate machine learning outcomes into the overall customer segmentation model
Required Skills  Significant experience in using best practices in designing, building and managing data pipelines that require data transformations as well as metadata and workload management Significant experience in working with large, heterogeneous datasets in building and optimizing data pipelines, pipeline architectures and integrated datasets using traditional and new data integration technologies (such as ETL, ELT, data replication, change data captures, message-oriented data movement, API design, stream data integration and data virtualization) Significant experience with streaming technologies (Kafka, Pubsub, Kinesis) and log-based architectures and experience writing batch and stream processing jobs (i.e. Apache Beam, Google Cloud DataFlow, Apache Spark, Apache Storm) Significant experience in performing root cause analysis on internal and external data and processes to identify issues and opportunities for improvement Expert level knowledge with programming languages including SQL, PL/SQL, T-SQL Expert level knowledge with relational SQL databases such as Oracle and SQL Server Significant experience with a scripting language & streaming technologies: Python, Java, Scala, Kafka, etc. Experience with NoSQL databases are a plus Experience supporting and working with cross-functional teams in a dynamic environment
Required EducationBachelor of Science in Computer Science, Information Technology or equivalentRequired Years Of Experience5+ years of experience in a data/cloud engineering role5+ years of experience working and creating datasets for a data warehouse5+ years of experience with ETL development tools, Informatica or Azure Data Factory (ADF) preferred3+ years of cloud experience, Azure preferredDesired Skills  Clear understanding of data modeling patterns (relational and dimensional) Proven ability to collaborate with technical peers Capable of working independently as well as part of a team Strive to provide orientation and direction to junior engineers requiring their expertise Experienced with continuous integration and continuous deployment practices Ability to approach complex problems with creativity and display analytical and problem-solving skills Display curiosity in understanding the data for the specific area of responsibility Knowledge and experience working with agile methodologies and tools (such as Jira) a plus
Additional InformationNumber Of Allowed Submittals Per Vendor3Project assignments, allocation % each project, capital % for capital projects:100% C23-001 - RM Platform and Automation1 resource will work on CEL RM Automation1 resource will work on RCI RM AutomationGaurav VermaTalent Acquisition -North AmericaDirect:+1 3025492448gaurav.verma@steneral.comIn my absence please reach out to Mr. Harish Sharma at harish@steneral.com & 302-721-6151"
Senior Data Engineer - Remote,Enexus Global Inc.,United States (Remote),https://www.linkedin.com/jobs/view/3675227051/?eBP=JOB_SEARCH_ORGANIC&refId=XUUXD7jduifZpMXMz48Iow%3D%3D&trackingId=Cw2w%2FJwLhol5%2FO%2BUcdPNvw%3D%3D&trk=flagship3_search_srp_jobs,3675227051,"About the job
            
 
Location - RemoteContract Type - W2/C2C/1099Minimum Experience - 11+ YearsResponsibilities Develop and enhance data-processing, orchestration, monitoring, and more by leveraging popular open-source software, AWS, and GitLab automation.Collaborate with product and technology teams to design and validate the capabilities of the data platformIdentify, design, and implement process improvements: automating manual processes, optimizing for usability, re-designing for greater scalabilityProvide technical support and usage guidance to the users of our platform's services.Drive the creation and refinement of metrics, monitoring, and alerting mechanisms to give us the visibility we need into our production services.
Qualifications Experience building and optimizing data pipelines in a distributed environmentExperience supporting and working with cross-functional teamsProficiency working in Linux environment8+ years of advanced working knowledge of SQL, Python, and PySpark5+ years of experience with using a broad range of AWS technologiesExperience using tools such as: Git/Bitbucket, Jenkins/CodeBuild, CodePipelineExperience with platform monitoring and alerts tools"
"Data Engineer (AWS, Azure, GCP)",CapTech,"Charlotte, NC (Remote)",https://www.linkedin.com/jobs/view/3774199173/?eBP=JOB_SEARCH_ORGANIC&refId=XUUXD7jduifZpMXMz48Iow%3D%3D&trackingId=fpG88DAuM8C1X%2FIsCWpU2A%3D%3D&trk=flagship3_search_srp_jobs,3774199173,"About the job
            
 
Company DescriptionCapTech is an award-winning consulting firm that collaborates with clients to achieve what’s possible through the power of technology. At CapTech, we’re passionate about the work we do and the results we achieve for our clients. From the outset, our founders shared a collective passion to create a consultancy centered on strong relationships that would stand the test of time. Today we work alongside clients that include Fortune 100 companies, mid-sized enterprises, and government agencies, a list that spans across the country.Job DescriptionCapTech Data Engineering consultants enable clients to build and maintain advanced data systems that bring together data from disparate sources in order to enable decision-makers. We build pipelines and prepare data for use by data scientists, data analysts, and other data systems. We love solving problems and providing creative solutions for our clients. Cloud Data Engineers leverage the client’s cloud infrastructure to deliver this value today and to scale for the future. We enjoy a collaborative environment and have many opportunities to learn from and share knowledge with other developers, architects, and our clients.Specific responsibilities for the Data Engineer – Cloud position include:  Developing data pipelines and other data products using Amazon Web Services (AWS), Microsoft Azure, or Google Cloud Platform (GCP) Advising clients on specific technologies and methodologies for utilizing cloud resources to efficiently ingest and process data quickly Utilizing your skills in engineering best practices to solve complex data problems Collaborating with end users, development staff, and business analysts to ensure that prospective data architecture plans maximize the value of client data across the organization. Articulating architectural differences between solution methods and the advantages/disadvantages of each 
QualificationsTypical experience for successful candidates includes:  Experience delivering solutions on a major cloud platform Ability to think strategically and relate architectural decisions/recommendations to business needs and client culture Experience in the design and implementation of data architecture solutions A wide range of production database experience, usually including substantial SQL expertise, database administration, and scripting data pipelines Ability to assess and utilize traditional and modern architectural components required based on business needs. A demonstrable ability to deliver production data pipelines and other data products. This could be hands on experience, degree, certification, bootcamp, or other learning. 
SkillsSuccessful candidates usually have demonstrable experience with technologies in some of these categories: Languages: SQL, Python, Java, R, C# / C++ / C Database: SQL Server, PostgreSQL, Snowflake, Redshift, Aurora, Presto, BigQuery, Oracle DevOps: git, docker, subversion, Kubernetes, Jenkins Additional Technologies: Spark, Databricks, Kafka, Kinesis, Hadoop, Lambda, EMR Popular Certifications: AWS Cloud Practitioner, Microsoft Azure Data Fundamentals, Google Associate Cloud Engineer
Additional InformationWe want everyone at CapTech to be able to envision a lasting and rewarding career here, which is why we offer a variety of career paths based on your skills and passions. You decide where and how you want to develop, and we help get you there with customizable career progression and a comprehensive benefits package to support you along the way. Alongside our suite of traditional benefits encompassing generous PTO, health coverage, disability insurance, paid family leave and more, we’ve launched extended benefits to help meet our employees’ needs. CapFlex – Employee-first mentality that supports a remote and hybrid workforce and empowers daily flexibility while servicing our clientsLearning & Development – Programs offering certification and tuition support, digital on-demand learning courses, mentorship, and skill development pathsModern Health –A mental health and well-being platform that provides 1:1 care, group support sessions, and self-serve resources to support employees and their families through life’s ups and downsCarrot Fertility –Inclusive fertility and family-forming coverage for all paths to parenthood – including adoption, surrogacy, fertility treatments, pregnancy, and more – and opportunities for employer-sponsored funds to help pay for careFringe –A company paid stipend program for personalized lifestyle benefits, allowing employees to choose benefits that matter most to them – ranging from vendors like Netflix, Spotify, and GrubHub to services like student loan repayment, travel, fitness, and moreEmployee Resource Groups – Employee-led committees that embrace and incorporate diversity and inclusion into our day-to-day operationsPhilanthropic Partnerships – Opportunities to engage in partnerships and pro-bono projects that support our communities. 401(k) Matching – Generous matching and no vesting period to help you continue to build financial wellness
CapTech is an equal opportunity employer committed to fostering a culture of equality, inclusion and fairness — each foundational to our core values. We strive to create a diverse environment where each employee is encouraged to bring their unique ideas, backgrounds and experiences to the workplace. For more information about our Diversity, Inclusion and Belonging efforts, click HERE. As part of this commitment, CapTech will ensure that persons with disabilities are provided reasonable accommodations. If reasonable accommodation is needed to participate in the job application or interview process, to perform essential job functions, and/or to receive other benefits and privileges of employment, please contact Laura Massa directly via email lmassa@captechconsulting.com.At this time, CapTech cannot transfer nor sponsor a work visa for this position. Applicants must be authorized to work directly for any employer in the United States without visa sponsorship."
Tableau Data Engineer-US,Zortech Solutions,United States (Remote),https://www.linkedin.com/jobs/view/3726693828/?eBP=JOB_SEARCH_ORGANIC&refId=XUUXD7jduifZpMXMz48Iow%3D%3D&trackingId=uwu4BR2saVkRcKklenja7Q%3D%3D&trk=flagship3_search_srp_jobs,3726693828,"About the job
            
 
Role: Tableau Data EngineerLocation: USA (Remote) or NJ-HybridDuration: FulltimeJob DescriptionSUMMARY OF ESSENTIAL JOB FUNCTIONS  Plan, facilitate, and document business requirements and formulate visualization design recommendations. Communicate solutions and details on various views of the data environment to audiences. Lead and interpret business requirements into actionable technical requirements. Collaborate with other work stream leads to ensure the overall developments are in sync. Identify risks and opportunities of potential logic and data issues within the analytics environment. Collaborate effectively with the global team and ensure day to day deliverables are met.
Minimum Requirements  Bachelor's degree and 5-12+ years of experience in related data and analytics area 5+ year experience in developing Tableau in a large enterprise. Team lead experience preferred Strong familiarity with Jira, Confluence, and other project management tools Strong experience in Data and analytics projects. 3+ years of manufacturing/Hi-tech experience preferred. Demonstrated knowledge of data solutions Strong source to target mapping experience and ETL principles/knowledge. Excellent verbal and written communication skills. Must be a native level speaker. Strong documentation skills including Gathering, Documenting, and Designing analytics Requirements. Strong quantitative and analytical skills with accuracy and attention to detail Ability to work well independently with minimal supervision and can manage multiple priorities. Must be an expert in Tableau. Knowledge/experience using Superset is highly preferred.
Please share your updated resume to pavan@zortechsolutions.ca"
"Data Engineer (AWS, Azure, GCP)",CapTech,"Atlanta, GA (Remote)",https://www.linkedin.com/jobs/view/3774802047/?eBP=JOB_SEARCH_ORGANIC&refId=var%2FYEIrWSv9gstoHniY8A%3D%3D&trackingId=bqgvluc8nVXW4zgNNq9FDg%3D%3D&trk=flagship3_search_srp_jobs,3774802047,"About the job
            
 
Company DescriptionCapTech is an award-winning consulting firm that collaborates with clients to achieve what’s possible through the power of technology. At CapTech, we’re passionate about the work we do and the results we achieve for our clients. From the outset, our founders shared a collective passion to create a consultancy centered on strong relationships that would stand the test of time. Today we work alongside clients that include Fortune 100 companies, mid-sized enterprises, and government agencies, a list that spans across the country.Job DescriptionCapTech Data Engineering consultants enable clients to build and maintain advanced data systems that bring together data from disparate sources in order to enable decision-makers. We build pipelines and prepare data for use by data scientists, data analysts, and other data systems. We love solving problems and providing creative solutions for our clients. Cloud Data Engineers leverage the client’s cloud infrastructure to deliver this value today and to scale for the future. We enjoy a collaborative environment and have many opportunities to learn from and share knowledge with other developers, architects, and our clients.Specific responsibilities for the Data Engineer – Cloud position include:  Developing data pipelines and other data products using Amazon Web Services (AWS), Microsoft Azure, or Google Cloud Platform (GCP) Advising clients on specific technologies and methodologies for utilizing cloud resources to efficiently ingest and process data quickly Utilizing your skills in engineering best practices to solve complex data problems Collaborating with end users, development staff, and business analysts to ensure that prospective data architecture plans maximize the value of client data across the organization. Articulating architectural differences between solution methods and the advantages/disadvantages of each 
QualificationsTypical experience for successful candidates includes:  Experience delivering solutions on a major cloud platform Ability to think strategically and relate architectural decisions/recommendations to business needs and client culture Experience in the design and implementation of data architecture solutions A wide range of production database experience, usually including substantial SQL expertise, database administration, and scripting data pipelines Ability to assess and utilize traditional and modern architectural components required based on business needs. A demonstrable ability to deliver production data pipelines and other data products. This could be hands on experience, degree, certification, bootcamp, or other learning. 
SkillsSuccessful candidates usually have demonstrable experience with technologies in some of these categories: Languages: SQL, Python, Java, R, C# / C++ / C Database: SQL Server, PostgreSQL, Snowflake, Redshift, Aurora, Presto, BigQuery, Oracle DevOps: git, docker, subversion, Kubernetes, Jenkins Additional Technologies: Spark, Databricks, Kafka, Kinesis, Hadoop, Lambda, EMR Popular Certifications: AWS Cloud Practitioner, Microsoft Azure Data Fundamentals, Google Associate Cloud Engineer
Additional InformationWe want everyone at CapTech to be able to envision a lasting and rewarding career here, which is why we offer a variety of career paths based on your skills and passions. You decide where and how you want to develop, and we help get you there with customizable career progression and a comprehensive benefits package to support you along the way. Alongside our suite of traditional benefits encompassing generous PTO, health coverage, disability insurance, paid family leave and more, we’ve launched extended benefits to help meet our employees’ needs. CapFlex – Employee-first mentality that supports a remote and hybrid workforce and empowers daily flexibility while servicing our clientsLearning & Development – Programs offering certification and tuition support, digital on-demand learning courses, mentorship, and skill development pathsModern Health –A mental health and well-being platform that provides 1:1 care, group support sessions, and self-serve resources to support employees and their families through life’s ups and downsCarrot Fertility –Inclusive fertility and family-forming coverage for all paths to parenthood – including adoption, surrogacy, fertility treatments, pregnancy, and more – and opportunities for employer-sponsored funds to help pay for careFringe –A company paid stipend program for personalized lifestyle benefits, allowing employees to choose benefits that matter most to them – ranging from vendors like Netflix, Spotify, and GrubHub to services like student loan repayment, travel, fitness, and moreEmployee Resource Groups – Employee-led committees that embrace and incorporate diversity and inclusion into our day-to-day operationsPhilanthropic Partnerships – Opportunities to engage in partnerships and pro-bono projects that support our communities. 401(k) Matching – Generous matching and no vesting period to help you continue to build financial wellness
CapTech is an equal opportunity employer committed to fostering a culture of equality, inclusion and fairness — each foundational to our core values. We strive to create a diverse environment where each employee is encouraged to bring their unique ideas, backgrounds and experiences to the workplace. For more information about our Diversity, Inclusion and Belonging efforts, click HERE. As part of this commitment, CapTech will ensure that persons with disabilities are provided reasonable accommodations. If reasonable accommodation is needed to participate in the job application or interview process, to perform essential job functions, and/or to receive other benefits and privileges of employment, please contact Laura Massa directly via email lmassa@captechconsulting.com.At this time, CapTech cannot transfer nor sponsor a work visa for this position. Applicants must be authorized to work directly for any employer in the United States without visa sponsorship."
Big Data Engineer,Experfy,United States (Remote),https://www.linkedin.com/jobs/view/3761185972/?eBP=JOB_SEARCH_ORGANIC&refId=var%2FYEIrWSv9gstoHniY8A%3D%3D&trackingId=JEgC%2BDobSi0HqdRFSNnzbQ%3D%3D&trk=flagship3_search_srp_jobs,3761185972,"About the job
            
 
Develop fast data infrastructure leveraging data streaming, batch processing, and machine learning to personalize experiences for our customersLead work and deliver elegant and scalable solutionsWork and collaborate with a nimble, autonomous, cross-functional team of makers, breakers, doers, and disruptors who love to solve real problems and meet real customer needs
#INDEXPRequirementsTechnical Qualifications: Bachelor-level Degree in engineering, Information Technology or Computer Science4 years of hands-on experience as a Data Engineer in a Big Data environment (Spark, Hive, HDFS, Sqoop)Strong SQL knowledge and data analysis skills for data anomaly detection and data quality assuranceProgramming experience in Scala, Python, shell scripting and automationExperience with modern workflow/orchestration tools (e.g. Apache Airflow, Oozie, Azkaban, etc.)Experience working with PostgreSQL, Teradata, Vertica and/or other DBMS platforms
Preferred Qualifications: Hadoop Certification or Spark CertificationExperience with BI tools such as Tableau or Qlik to create visualizations and dashboards for various data quality metrics

Desired Skills and Experience
AWSAirflowAzureBig DataCloud AnalyticsGCPHadoopHiveKafkaPythonScalaSparkSqoop"
Senior Data Engineer,Red Frog Solutions,"Cincinnati, OH (Remote)",https://www.linkedin.com/jobs/view/3776293106/?eBP=JOB_SEARCH_ORGANIC&refId=var%2FYEIrWSv9gstoHniY8A%3D%3D&trackingId=kgCso3bhvEecGMe6AEWEjg%3D%3D&trk=flagship3_search_srp_jobs,3776293106,"About the job
            
 
Data Engineer Cincinnati, OH - REMOTEFull Time Perm $120K - $150K Only applicants able to obtain a US Government Security Clearance (Requires US Citizenship) need apply*** 
Established consulting company with deep expertise in cutting-edge technologies for big data, advanced analytics, modern web application frameworks, and cloud computing, are looking for a Data Engineer to join their growing team of IT professionals.As a Data Engineer, you will work hands-on with challenging data engineering, data management, and analytics projects. You will collaborate with data scientists, analysts, business users, and IT teams to design, implement, and deploy data services and analytics.Requirements:10+ years of experience as a Data EngineerMust have or be willing to obtain Secret Clearance (this requires US Citizenship)Strong SQL skills in multiple database platformsExperience with Snowflake, Databricks, Spark SQL, PySpark, and PythonCloud experience: Azure, AWS, or GCPDevelop and maintain ETL pipelines.Database design and principlesData modeling, schema development, and data-centric documentationExperience integrating data from a variety of data source types.Recommend and advise on optimal data models for data ingestion, integration, and visualization.Experience improving code performance and query optimization.Use Continuous Integration/Continuous Delivery (CI/CD) concepts to engineer a standardized data environment."
Workplace Analytics Engineer,Canonical,"Pittsburgh, PA (Remote)",https://www.linkedin.com/jobs/view/3738249113/?eBP=JOB_SEARCH_ORGANIC&refId=var%2FYEIrWSv9gstoHniY8A%3D%3D&trackingId=9bqF5Q3LME4niwPtdZxV%2BA%3D%3D&trk=flagship3_search_srp_jobs,3738249113,"About the job
            
 
Bring your people analytics, social science research and data mining/science skills to a unique team seeking to understand, and shape, the future of the digital workplace. We are interested in technology, of course, but we are also interested in the human mission of enabling the world's brightest and hardest working people to live where they want and work from anywhere. Most of our colleagues could move to a tech hub but they choose Canonical because of our mission and our approach to the workplace.We'd like to understand what really makes a distributed, remote-first workplace work. We think we're pretty good at this (being remote first for almost 20 years), but we know there is a lot still to understand, and the frontier of possibility continues to move outward. We'd like to invest in research, analytics and tooling which raises the bar even further for remote collaboration and organisation.If we are able to build tools that meaningfully improve our cooperation and our satisfaction, then we intend to share our insights with the world, both as a narrative and as SAAS or open source that helps other companies follow in our footsteps.The role of a Workplace Analytics Engineer at CanonicalSupport analytics and data mining in a cross-disciplinary team of organisational psychologists, web front end engineers, back end engineers and statistics / analytics experts to help us build a new definition for the 21st century digital workplace. Collaborate to figure out what really drives productivity, effectiveness and happiness in a remote-first globally distributed company.In addition to your existing people analytics work experience, this role will combine your skills in psychology, data analytics and visualisation, to help create a more effective workplace.Location: This role will be based remotely in the AMER region.All applicants applying must be legally authorized to work in the United States, as we cannot offer visa sponsorship for this job position.What your day will look like Utilize advanced data analytics to understand how we hire and how we work (productivity, happiness and effectiveness) across a global, remote first organisationFocus on quantitative and qualitative data analytics to find insights and meaningful business outcomesTell the story from the insights through dashboards, visualizations and presentationsDesign and conduct research into trends shaping talent science and remote workCollaborate with stakeholder teams (ex., engineering, information systems, etc) to improve the data and tool ecosystem supporting our digital workplace
What we are looking for in you Background in data science, mathematics, actuarial science, or engineeringFirst work experience in People Analytics Knowledge in advanced statistics, data sciences, coding/scripting languages (Python, R, etc), and databases (SQL, etc)Strength in data analytics and visualization (Looker Studio, Tableau, etc)Ability to translate business questions to key research objectivesAbility to identify the best methodology to execute research, synthesize and analyse findingsExcellent writing and communication skillsWillingness to examine the status quo and resilient in the face of challenges
What we offer youYour base pay will depend on various factors including your geographical location, level of experience, knowledge and skills. In addition to the benefits above, certain roles are also eligible for additional benefits and rewards including annual bonuses and sales incentives based on revenue or utilization. Our compensation philosophy is to ensure equity right across our global workforce.In addition to a competitive base pay, we provide all team members with additional benefits, which reflect our values and ideals. Please note that additional benefits may apply depending on the work location and, for more information on these, you can ask in the later stages of the recruitment process.🏠 Fully remote working environment - we've been working remotely since 2004!📚 Personal learning and development budget of 2,000USD per annum💰 Annual compensation review🏆 Recognition rewards🏝 Annual holiday leave👶 Parental Leave🧑‍💼 Employee Assistance Programme🧳 Opportunity to travel to new locations to meet colleagues at 'sprints'✈️ Priority Pass for travel and travel upgrades for long haul company eventsAbout CanonicalCanonical is a pioneering tech firm that is at the forefront of the global move to open source. As the company that publishes Ubuntu, one of the most important open source projects and the platform for AI, IoT and the cloud, we are changing the world on a daily basis. We recruit on a global basis and set a very high standard for people joining the company. We expect excellence - in order to succeed, we need to be the best at what we do.Canonical has been a remote-first company since its inception in 2004. Work at Canonical is a step into the future, and will challenge you to think differently, work smarter, learn new skills, and raise your game. Canonical provides a unique window into the world of 21st-century digital business.Canonical is an equal opportunity employerWe are proud to foster a workplace free from discrimination. Diversity of experience, perspectives, and background create a better work environment and better products. Whatever your identity, we will give your application fair consideration."
Workplace Analytics Engineer,Canonical,"Raleigh, NC (Remote)",https://www.linkedin.com/jobs/view/3738243864/?eBP=JOB_SEARCH_ORGANIC&refId=var%2FYEIrWSv9gstoHniY8A%3D%3D&trackingId=kTwIfFDzfvwb7vhGeDt6bQ%3D%3D&trk=flagship3_search_srp_jobs,3738243864,"About the job
            
 
Bring your people analytics, social science research and data mining/science skills to a unique team seeking to understand, and shape, the future of the digital workplace. We are interested in technology, of course, but we are also interested in the human mission of enabling the world's brightest and hardest working people to live where they want and work from anywhere. Most of our colleagues could move to a tech hub but they choose Canonical because of our mission and our approach to the workplace.We'd like to understand what really makes a distributed, remote-first workplace work. We think we're pretty good at this (being remote first for almost 20 years), but we know there is a lot still to understand, and the frontier of possibility continues to move outward. We'd like to invest in research, analytics and tooling which raises the bar even further for remote collaboration and organisation.If we are able to build tools that meaningfully improve our cooperation and our satisfaction, then we intend to share our insights with the world, both as a narrative and as SAAS or open source that helps other companies follow in our footsteps.The role of a Workplace Analytics Engineer at CanonicalSupport analytics and data mining in a cross-disciplinary team of organisational psychologists, web front end engineers, back end engineers and statistics / analytics experts to help us build a new definition for the 21st century digital workplace. Collaborate to figure out what really drives productivity, effectiveness and happiness in a remote-first globally distributed company.In addition to your existing people analytics work experience, this role will combine your skills in psychology, data analytics and visualisation, to help create a more effective workplace.Location: This role will be based remotely in the AMER region.All applicants applying must be legally authorized to work in the United States, as we cannot offer visa sponsorship for this job position.What your day will look like Utilize advanced data analytics to understand how we hire and how we work (productivity, happiness and effectiveness) across a global, remote first organisationFocus on quantitative and qualitative data analytics to find insights and meaningful business outcomesTell the story from the insights through dashboards, visualizations and presentationsDesign and conduct research into trends shaping talent science and remote workCollaborate with stakeholder teams (ex., engineering, information systems, etc) to improve the data and tool ecosystem supporting our digital workplace
What we are looking for in you Background in data science, mathematics, actuarial science, or engineeringFirst work experience in People Analytics Knowledge in advanced statistics, data sciences, coding/scripting languages (Python, R, etc), and databases (SQL, etc)Strength in data analytics and visualization (Looker Studio, Tableau, etc)Ability to translate business questions to key research objectivesAbility to identify the best methodology to execute research, synthesize and analyse findingsExcellent writing and communication skillsWillingness to examine the status quo and resilient in the face of challenges
What we offer youYour base pay will depend on various factors including your geographical location, level of experience, knowledge and skills. In addition to the benefits above, certain roles are also eligible for additional benefits and rewards including annual bonuses and sales incentives based on revenue or utilization. Our compensation philosophy is to ensure equity right across our global workforce.In addition to a competitive base pay, we provide all team members with additional benefits, which reflect our values and ideals. Please note that additional benefits may apply depending on the work location and, for more information on these, you can ask in the later stages of the recruitment process.🏠 Fully remote working environment - we've been working remotely since 2004!📚 Personal learning and development budget of 2,000USD per annum💰 Annual compensation review🏆 Recognition rewards🏝 Annual holiday leave👶 Parental Leave🧑‍💼 Employee Assistance Programme🧳 Opportunity to travel to new locations to meet colleagues at 'sprints'✈️ Priority Pass for travel and travel upgrades for long haul company eventsAbout CanonicalCanonical is a pioneering tech firm that is at the forefront of the global move to open source. As the company that publishes Ubuntu, one of the most important open source projects and the platform for AI, IoT and the cloud, we are changing the world on a daily basis. We recruit on a global basis and set a very high standard for people joining the company. We expect excellence - in order to succeed, we need to be the best at what we do.Canonical has been a remote-first company since its inception in 2004. Work at Canonical is a step into the future, and will challenge you to think differently, work smarter, learn new skills, and raise your game. Canonical provides a unique window into the world of 21st-century digital business.Canonical is an equal opportunity employerWe are proud to foster a workplace free from discrimination. Diversity of experience, perspectives, and background create a better work environment and better products. Whatever your identity, we will give your application fair consideration."
"Data Engineer, Database Engineering",Experfy,"Hollywood, FL (Remote)",https://www.linkedin.com/jobs/view/3590303102/?eBP=JOB_SEARCH_ORGANIC&refId=var%2FYEIrWSv9gstoHniY8A%3D%3D&trackingId=WCHO%2FK1piokQH5g%2BdGJxzw%3D%3D&trk=flagship3_search_srp_jobs,3590303102,"About the job
            
 
As a Data Engineer for our Data Platform Engineering team you will join skilled Scala/ Spark engineers and core database developers responsible for developing hosted cloud analytics infrastructure (Apache Spark-based), distributed SQL processingframeworks, proprietary data science platforms, and core database optimization. This team is responsible for building the automated, intelligent, and highly performant query planner and execution engines, RPC calls between datawarehouse clusters, shared secondary cold storage, etc. This includes building new SQL features and customer-facing functionality, developing novel query optimization techniques for industry-leading performance, and building a databasesystem that's highly parallel, efficient and fault-tolerant. This is a vital role reporting to exec leadership and senior engineering leadershipRequirementsResponsibilities: Writing Scala code with tools like Apache Spark + Apache Arrow + Apache Kafka to build a hosted, multi-cluster data warehouse for Web3Developing database optimizers, query planners, query and data routing mechanisms, cluster-to-cluster communication, and workload management techniquesScaling up from proof of concept to ""cluster scale"" (and eventually hundreds of clusters with hundreds of terabytes each), in terms of both infrastructure/architecture and problem structureCodifying best practices for future reuse in the form of accessible, reusable patterns, templates, and code bases to facilitate meta data capturing and managementManaging a team of software engineers writing new code to build a bigger, better, faster, more optimized HTAP database (using Apache Spark, Apache Arrow, Kafka, and a wealth of other open source data tools)Interacting with exec team and senior engineering leadership to define, prioritize, and ensure smooth deployments with other operational componentsHighly engaged with industry trends within analytics domain from a data acquisition processing, engineering, management perspectiveUnderstand data and analytics use cases across Web3 / blockchains
Skills & Qualifications Bachelor's degree in computer science or related technical field. Masters or PhD a plus6+ years experience engineering software and data platforms / enterprise-scale data warehouses, preferably with knowledge of open source Apache stack (especially Apache Spark, Apache Arrow, Kafka, and others)3+ years experience with Scala and Apache Spark (or Kafka)A track record of recruiting and leading technical teams in a demanding talent marketRock solid engineering fundamentals; query planning, optimizing and distributed data warehouse systems experience is preferred but not requiredNice to have: Knowledge of blockchain indexing, web3 compute paradigms, Proofs and consensus mechanisms... is a strong plus but not requiredExperience with rapid development cycles in a web-based environmentStrong scripting and test automation knowledgeNice to have: Passionate about Web3, blockchain, decentralization, and a base understanding of how data/analytics plays into this"
Azure Cloud Data Engineer w/ Cosmos DB,Adame Services,United States (Remote),https://www.linkedin.com/jobs/view/3706394774/?eBP=JOB_SEARCH_ORGANIC&refId=var%2FYEIrWSv9gstoHniY8A%3D%3D&trackingId=oOtqctDsDbobkWLl0G17WQ%3D%3D&trk=flagship3_search_srp_jobs,3706394774,"About the job
            
 
Term: (1 year+)Location: St. Louis (client location) working hours Central Standard TimeREMOTE: Candidate may work FULL TIME REMOTEWork Status: USC / Green Card(no C2C to your firm, no C2C to the candidate)Skills Azure Cloud Data Engineer Cosmos DB, T-SQLMicrosoft Azure Cloud PaaSGit/Jenkins/BitbucketAzure Cloud workflowsAzure cloud managed DBs/Systems, Managed SQL InstanceBig Data solutions: Delta Lake by Databricks
Required Must be presently authorized to work in the U.S. without a requirement for work authorization sponsorship by our company for this position now or in the future. Must be committed to incorporating security into all decisions and daily job responsibilities3+ of related experience in Cosmos DB/Similar DB technologyExperience with configuration management and building automation capabilities such as Git/Jenkins/BitbucketExperience with Microsoft Azure Cloud workflowsExperience in T-SQL and scripting skills.Knowledge on Microsoft cloud managed DBs/Systems, e.g. Managed SQL Instance, Cosmos DB, Databricks Delta LakeExperience with Big Data solutions such as Delta Lake by Databricks and SQL DBMSsIndependently analyze, solve, and correct issues in real time, providing problem resolution end-to-endIdentify new opportunities and help refine automation of regular processes, track issues, and document changesSolve/Assist in complex query tuning and schema refinementExpert in troubleshooting performance issuesExperience rightsizing Database object workflow for cost managementAbility to multi-task and context-switch effectively between different activities and teamsJoin the on-call rotation with other EngineersMust be able to both collaborate in a team-oriented environment and work independently with directionMust be able to work in a fast-paced environment with the ability to handle multiple tasks
Preferred Bachelor's degree in Computer Science, Computer Information Systems, Management Information Systems, or related field preferredAzure SQL DB etc. will be a big plusExperience with Microsoft Azure platform technologies like Databricks applications, Event Hub, Data Factory, Azure SQL, Synapse Analytics, DeltaLake, Cosmos DB, or DevOpsPrior experience with large-scale projectsExperience with API developmentKnowledge and working experience with Agile methodologiesFamiliarity with JDBC connections to data sources"
Sr Data Engineer,Wise Skulls,United States (Remote),https://www.linkedin.com/jobs/view/3642856402/?eBP=JOB_SEARCH_ORGANIC&refId=var%2FYEIrWSv9gstoHniY8A%3D%3D&trackingId=mY0BysW%2Bq2%2B7kQDPfs%2BdBw%3D%3D&trk=flagship3_search_srp_jobs,3642856402,"About the job
            
 
Title: Sr Data EngineerLocation: Remote Job (Need to work in EST zone)Duration: 6+ Months (Possibility of Extension)Implementation Partner: InfosysEnd Client: To be DisclosedJd10+ years in Data Engineering.Must-Have Extensive 5+ years in Palantir Foundry should be able to suggest optimization strategies for the client platform.Pharmacy Benefit Management domain experience.Should be able to liaise with the business for requirement gathering and take ownership till deployment"
"Data Engineer, Database Engineering",Experfy,"California, United States (Remote)",https://www.linkedin.com/jobs/view/3719412209/?eBP=JOB_SEARCH_ORGANIC&refId=var%2FYEIrWSv9gstoHniY8A%3D%3D&trackingId=aOUKq3xKxi6ZydhEV1Zq4A%3D%3D&trk=flagship3_search_srp_jobs,3719412209,"About the job
            
 
As a Data Engineer for our Data Platform Engineering team you will join skilled Scala/ Spark engineers and core database developers responsible for developing hosted cloud analytics infrastructure (Apache Spark-based), distributed SQL processingframeworks, proprietary data science platforms, and core database optimization. This team is responsible for building the automated, intelligent, and highly performant query planner and execution engines, RPC calls between datawarehouse clusters, shared secondary cold storage, etc. This includes building new SQL features and customer-facing functionality, developing novel query optimization techniques for industry-leading performance, and building a databasesystem that's highly parallel, efficient and fault-tolerant. This is a vital role reporting to exec leadership and senior engineering leadershipRequirementsResponsibilities: Writing Scala code with tools like Apache Spark + Apache Arrow + Apache Kafka to build a hosted, multi-cluster data warehouse for Web3Developing database optimizers, query planners, query and data routing mechanisms, cluster-to-cluster communication, and workload management techniquesScaling up from proof of concept to ""cluster scale"" (and eventually hundreds of clusters with hundreds of terabytes each), in terms of both infrastructure/architecture and problem structureCodifying best practices for future reuse in the form of accessible, reusable patterns, templates, and code bases to facilitate meta data capturing and managementManaging a team of software engineers writing new code to build a bigger, better, faster, more optimized HTAP database (using Apache Spark, Apache Arrow, Kafka, and a wealth of other open source data tools)Interacting with exec team and senior engineering leadership to define, prioritize, and ensure smooth deployments with other operational componentsHighly engaged with industry trends within analytics domain from a data acquisition processing, engineering, management perspectiveUnderstand data and analytics use cases across Web3 / blockchains
Skills & Qualifications Bachelor's degree in computer science or related technical field. Masters or PhD a plus6+ years experience engineering software and data platforms / enterprise-scale data warehouses, preferably with knowledge of open source Apache stack (especially Apache Spark, Apache Arrow, Kafka, and others)3+ years experience with Scala and Apache Spark (or Kafka)A track record of recruiting and leading technical teams in a demanding talent marketRock solid engineering fundamentals; query planning, optimizing and distributed data warehouse systems experience is preferred but not requiredNice to have: Knowledge of blockchain indexing, web3 compute paradigms, Proofs and consensus mechanisms... is a strong plus but not requiredExperience with rapid development cycles in a web-based environmentStrong scripting and test automation knowledgeNice to have: Passionate about Web3, blockchain, decentralization, and a base understanding of how data/analytics plays into this
Apply for this job"
"Data Engineer, Database Engineering",Experfy,"San Francisco, CA (Remote)",https://www.linkedin.com/jobs/view/3590297967/?eBP=JOB_SEARCH_ORGANIC&refId=var%2FYEIrWSv9gstoHniY8A%3D%3D&trackingId=oTyrmeHrbeeemGJsTHHkxg%3D%3D&trk=flagship3_search_srp_jobs,3590297967,"About the job
            
 
As a Data Engineer for our Data Platform Engineering team you will join skilled Scala/ Spark engineers and core database developers responsible for developing hosted cloud analytics infrastructure (Apache Spark-based), distributed SQL processingframeworks, proprietary data science platforms, and core database optimization. This team is responsible for building the automated, intelligent, and highly performant query planner and execution engines, RPC calls between datawarehouse clusters, shared secondary cold storage, etc. This includes building new SQL features and customer-facing functionality, developing novel query optimization techniques for industry-leading performance, and building a databasesystem that's highly parallel, efficient and fault-tolerant. This is a vital role reporting to exec leadership and senior engineering leadershipRequirementsResponsibilities: Writing Scala code with tools like Apache Spark + Apache Arrow + Apache Kafka to build a hosted, multi-cluster data warehouse for Web3Developing database optimizers, query planners, query and data routing mechanisms, cluster-to-cluster communication, and workload management techniquesScaling up from proof of concept to ""cluster scale"" (and eventually hundreds of clusters with hundreds of terabytes each), in terms of both infrastructure/architecture and problem structureCodifying best practices for future reuse in the form of accessible, reusable patterns, templates, and code bases to facilitate meta data capturing and managementManaging a team of software engineers writing new code to build a bigger, better, faster, more optimized HTAP database (using Apache Spark, Apache Arrow, Kafka, and a wealth of other open source data tools)Interacting with exec team and senior engineering leadership to define, prioritize, and ensure smooth deployments with other operational componentsHighly engaged with industry trends within analytics domain from a data acquisition processing, engineering, management perspectiveUnderstand data and analytics use cases across Web3 / blockchains
Skills & Qualifications Bachelor's degree in computer science or related technical field. Masters or PhD a plus6+ years experience engineering software and data platforms / enterprise-scale data warehouses, preferably with knowledge of open source Apache stack (especially Apache Spark, Apache Arrow, Kafka, and others)3+ years experience with Scala and Apache Spark (or Kafka)A track record of recruiting and leading technical teams in a demanding talent marketRock solid engineering fundamentals; query planning, optimizing and distributed data warehouse systems experience is preferred but not requiredNice to have: Knowledge of blockchain indexing, web3 compute paradigms, Proofs and consensus mechanisms... is a strong plus but not requiredExperience with rapid development cycles in a web-based environmentStrong scripting and test automation knowledgeNice to have: Passionate about Web3, blockchain, decentralization, and a base understanding of how data/analytics plays into this"
Job Opportunity :: Senior Health Data Migration Engineer Remote :: 6 Months Contract :: Fully Remote,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3642847050/?eBP=JOB_SEARCH_ORGANIC&refId=var%2FYEIrWSv9gstoHniY8A%3D%3D&trackingId=M8p%2BNsfRZPyTvYjMQo53qg%3D%3D&trk=flagship3_search_srp_jobs,3642847050,"About the job
            
 
Hi, Hope you are doing well, Please find the requirement below , If you find yourself comfortable with the requirement please reply with your Updated Resume and I will get back to you or I would really appreciate if you can give me a call back at my contact number 302-721-5174Position: Senior Health Data Migration Engineer RemoteLocation: 100% remoteDuration: 6 months Contract Work Authorization: USC, GC onlyInterview: Skype Job Description 15+ years of professional work experience, to include experience with InterSystems IRIS in a healthcare environmentBachelor's degree in Computer Science, Engineering, Math, or equivalent, or an additional 8 years of relevant experience may be substituted for degree requirements
Activities Support the Department of Veterans Affairs (VA) Electronic Health Record Modernization Integration Office (EHRM-IO) as a Senior Health Data Migration Engineer.Assess the current EHRM data migration requirements; maintain and update the strategy to meet the requirements. Review error and trace logs. Track messages by domain and reconcile table counts with Cerner. Review secure data message transmission logs. Track the number of records sent per message by domain. Monitor Queue Depth by service/process/operation. Track retry attempts and suspended messages. Validate and update data integration reports in support of VX130 data domains.Review, update, and maintain Cerner to CDW, VistA, Millennium or Cloud Database data mappings for potential data migrations. Evaluate and integrate data from multiple sources, which requires data mapping from one data source to another minimizing any data loss. Document VistA Extraction and monitoring process and update existing documentation quarterly. Interpret Cerner's Data model to be used for the construction of API's, queries, and reports that will be consumed by internal or external applications.Review Domain adds to Ensemble Production. Validate edits made to Domain record type, schema version, status, and payload size via the GUI Interface and Rule Builder. Validate Ensemble data flows built using VX130 ClassBuilder. Validate the load of Cache/IRIS Objects, SQL Tables or other storage structures(Historical Pulls) in all regions/districts for classes in all environments with VistA or Data Syndication data.
RequirementsMinimum qualifications: 15+ years of professional work experience, to include experience with InterSystems IRIS in a healthcare environmentBe able to create strategies and plans for integration of multiple IT systems/subsystems into an operational unit, ensuring full functional and performance capabilities are retained.Able to coordinate with development and user teams to assess risks, goals and needs and ensure that all are adequately addressed.Experienced in introducing new hardware or software into a new or existing environment while minimizing disruption and mitigating risks.Able to be cost conscience as well addressing goals.Bachelor's degree in Computer Science, Engineering, Math, or equivalent, or an additional 8 years of relevant experience may be substituted for degree requirements
Preferred Qualifications Experience in the VAExperience implementing EHRExperience with VA and DoD legacy health data, and private sector health dataExperience with Extraction, Transformation, and Loading of data between systemsKnowledge and experience handling VistA data
Thank you and RegardsGaurav RajSr. Talent Acquisition Specialist -North AmericaDirect: +1 302-721-5174gaurav@steneral.com1007, N Orange St, 4th FL 329, Wilmington, DE 19801"
Senior Data Engineer,Freestar,"Phoenix, AZ (Remote)",https://www.linkedin.com/jobs/view/3619298438/?eBP=JOB_SEARCH_ORGANIC&refId=var%2FYEIrWSv9gstoHniY8A%3D%3D&trackingId=eCBXd3fRMy60Wruorytixw%3D%3D&trk=flagship3_search_srp_jobs,3619298438,"About the job
            
 
About FreestarFreestar engineer teams develop cutting-edge monetization solutions for websites and mobile apps. By combining industry-leading technology, data, and massive scale, we enable busy site owners and mobile app publishers to seamlessly maximize revenue while freeing themselves of the hassles of ad operations. The result is publishers having more time to do what they do best: create content and focus on their apps.Senior Engineer (Remote)We’re looking for a Senior Engineer for our Data Platform Team - the product that drives our core business. As a Senior Engineer at Freestar, you'll be one of the main contributors to a talented team of developers to innovate features for our product that separate us from our competitors.What You'll Be Doing Run points on whole projects by recommending, prototype, build and debug data infrastructures on Google Cloud Platform (GCP) with other principal and senior engineers within the team, and to mentor less experienced Data Engineers. Demonstrate repeated delivery of project architectures and critical components that other engineers demur to you for lack of expertise.Participate in early-stage conversation with our product development team about product / features. Improve decision quality across the company by ensuring metrics are trustworthy, discoverable, and easily consumable.
Skills And Experience You'll Need To Have Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive, Kafka, PubSub).4+ years of experience designing and implementing large-scale, complex, data-driven applications on the cloud, preferably on Google Cloud / AWS.4+ years of hands-on experience using SQL to perform complex data manipulation3+ years of experience modeling data warehouses3+ years of experience building data pipelines, CI/CD pipelines, and fit for purpose data storesData migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.Experience writing software in one or more languages such as Python, Java, Scala, etc.Experience with systems monitoring/alerting, capacity planning and performance tuningEnjoy analyzing and organizing rapidly-changing business data to support product and business solutionsNice to have Qualifications: *Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, BigQuery, Dataprep, Composer, etc)Experience with IoT architectures and building real-time data streaming pipelinesExperience operationalizing machine learning models on large datasetsDemonstrated leadership and self-direction -- a willingness to teach others and learn new techniquesDemonstrated skills in selecting the right statistical tools given a data analysis problemComfortable interacting across multiple teams and management levels within the organizationPrevious background in the ad tech or media landscape (linear, digital, or social) is a plus
Why You Want To Work With Us Full-Time, Salaried PositionWorking remotelyGenerous Medical, Dental, and Vision benefits401K with company match, vested immediatelyThe opportunity to be part of something high value, high impact, and high growth...Who doesn't love that!
Freestar is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran or disabled status, or genetic information."
"Data Engineer, Database Engineering",Experfy,"Los Angeles, CA (Remote)",https://www.linkedin.com/jobs/view/3590299677/?eBP=JOB_SEARCH_ORGANIC&refId=var%2FYEIrWSv9gstoHniY8A%3D%3D&trackingId=QGkTAQhn8qUfer2ZCS0ZnQ%3D%3D&trk=flagship3_search_srp_jobs,3590299677,"About the job
            
 
As a Data Engineer for our Data Platform Engineering team you will join skilled Scala/ Spark engineers and core database developers responsible for developing hosted cloud analytics infrastructure (Apache Spark-based), distributed SQL processingframeworks, proprietary data science platforms, and core database optimization. This team is responsible for building the automated, intelligent, and highly performant query planner and execution engines, RPC calls between datawarehouse clusters, shared secondary cold storage, etc. This includes building new SQL features and customer-facing functionality, developing novel query optimization techniques for industry-leading performance, and building a databasesystem that's highly parallel, efficient and fault-tolerant. This is a vital role reporting to exec leadership and senior engineering leadershipRequirementsResponsibilities: Writing Scala code with tools like Apache Spark + Apache Arrow + Apache Kafka to build a hosted, multi-cluster data warehouse for Web3Developing database optimizers, query planners, query and data routing mechanisms, cluster-to-cluster communication, and workload management techniquesScaling up from proof of concept to ""cluster scale"" (and eventually hundreds of clusters with hundreds of terabytes each), in terms of both infrastructure/architecture and problem structureCodifying best practices for future reuse in the form of accessible, reusable patterns, templates, and code bases to facilitate meta data capturing and managementManaging a team of software engineers writing new code to build a bigger, better, faster, more optimized HTAP database (using Apache Spark, Apache Arrow, Kafka, and a wealth of other open source data tools)Interacting with exec team and senior engineering leadership to define, prioritize, and ensure smooth deployments with other operational componentsHighly engaged with industry trends within analytics domain from a data acquisition processing, engineering, management perspectiveUnderstand data and analytics use cases across Web3 / blockchains
Skills & Qualifications Bachelor's degree in computer science or related technical field. Masters or PhD a plus6+ years experience engineering software and data platforms / enterprise-scale data warehouses, preferably with knowledge of open source Apache stack (especially Apache Spark, Apache Arrow, Kafka, and others)3+ years experience with Scala and Apache Spark (or Kafka)A track record of recruiting and leading technical teams in a demanding talent marketRock solid engineering fundamentals; query planning, optimizing and distributed data warehouse systems experience is preferred but not requiredNice to have: Knowledge of blockchain indexing, web3 compute paradigms, Proofs and consensus mechanisms... is a strong plus but not requiredExperience with rapid development cycles in a web-based environmentStrong scripting and test automation knowledgeNice to have: Passionate about Web3, blockchain, decentralization, and a base understanding of how data/analytics plays into this"
Senior Data Engineer,Blackbaud,"South Carolina, United States (Remote)",https://www.linkedin.com/jobs/view/3780463298/?eBP=JOB_SEARCH_ORGANIC&refId=var%2FYEIrWSv9gstoHniY8A%3D%3D&trackingId=wvCRVh6aTw0qJ4nm85pE7g%3D%3D&trk=flagship3_search_srp_jobs,3780463298,"About the job
            
 
Company Name: Blackbaud, Inc.Position Title: Senior Data EngineerLocation: 65 Fairchild Street, Charleston, SC 29492Summary of Duties: Develop production level code. Own process features from design to implementation. Assemble large, complex data sets for use in R&D, data science, machine learning, or projection to operational stores. Build the infrastructure that provides efficient and scalable ETL/ELT and integration of data from multiple proprietary and 3rd party sources. Code, test, and deploy pipelines in support of various initiatives. Ensure quality of code, processes, and data assets. Work on complex tasks and broad programs that are large, diverse in scope, and critical in nature. Exercise good judgement in determining technical approach. Design, develop, and operate high performance, large volume data structures for data-powered products and data science. Maintain Delta Lack ecosystem, which empowers reporting tools. Train and coach new team members. Work with cross-functional team for dependencies and task coverage. Undertake responsibilities of Azure Data Lake, Data Factory, Databricks, and Blob Storage for data orchestration and fabrication to bring consistency and reliability to overall reporting offering. Empowered reporting in Tableau by providing diversified data points to customers. Support end users in the use of the BI platform, reporting, and data visualization tools. Implement design patterns that support data ingestion, data movement, transformation, aggregation, and more. Design and develop breakthrough products, services, or technological advancements in the Data Intelligence space that expand business. Implement efficient, distributed, and scalable pipelines and integrate data from multiple sources to common data models. Involved in the movement from the legacy SSRS reports to robust data-warehouse built on top of AWS Redshift. Support ongoing requests for data issues, and work in agile environment. Contribute to ongoing process and technical improvements. Duties may be performed remotely.Qualifications: Master’s in Information Technology, Information Systems, Data Analytics, or related field, and 2 years of experience as a Data Engineer, Systems Engineer, or related role. Employer will alternatively accept a Bachelor's degree in Information Technology, Information Systems, Data Analytics, or related field and 4 years of experience as a Data Engineer, Systems Engineer, or related role. Must also have two (2) years of experience with: SQL programming; Tableau; Excel; and ETL. One (1) year of experience with: Scala; Spark SQL; Azure Databricks; Azure Data Factory; Azure SQL Datawarehouse; AWS; S3; AWS Redshift; SSRS; Elastic Search; Replication; Event Hub; Cosmos Db; Azure Devops Deployment; and Source Control.Stay up to date on everything Blackbaud, follow us on Linkedin, Twitter, Instagram, Facebook and YouTubeBlackbaud is a remote-first company which embraces a flexible remote work culture. Blackbaud supports hiring and career development for all roles from the location you are in today!Blackbaud is proud to be an equal opportunity employer and is committed to maintaining a diverse and inclusive work environment. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, physical or mental disability, age, or veteran status or any other basis protected by federal, state, or local law.To all recruitment agencies: We do not accept unsolicited agency resumes and are not responsible for any fees related to unsolicited resumes.A notice to candidates: Recruitment Fraudulent Alert: Your personal information and online safety as a candidate mean a lot to us! At Blackbaud and our portfolio of companies, recruiters only direct candidates to apply through our official careers page at https://careers.blackbaud.com/us/en or our official LinkedIn page. Recruiters will never request payments, ask for financial account information or sensitive information like social security numbers, or conduct interviews via Skype. Anyone suggesting otherwise is not a representative of Blackbaud. If you are unsure if a message is from Blackbaud, please email blackbaudrecruiting@blackbaud.com. R0011223"
Sr. Data engineer,"Resource Informatics Group, Inc",United States (Remote),https://www.linkedin.com/jobs/view/3630113332/?eBP=JOB_SEARCH_ORGANIC&refId=var%2FYEIrWSv9gstoHniY8A%3D%3D&trackingId=vbSeOY5gE4Y27HjBL69pBA%3D%3D&trk=flagship3_search_srp_jobs,3630113332,"About the job
            
 
Job Description :Sr Data Engineer (Must have GREAT communication skills) Must have experience:(""Asset management"" or ""Wealth management"")GCPAWSSnowflakeInformaticaETL"
Azure Data Engineer,Motion Recruitment,"Waltham, MA (Remote)",https://www.linkedin.com/jobs/view/3748282081/?eBP=JOB_SEARCH_ORGANIC&refId=var%2FYEIrWSv9gstoHniY8A%3D%3D&trackingId=8PAPjnnOsfIM0y6KLn%2Fa3w%3D%3D&trk=flagship3_search_srp_jobs,3748282081,"About the job
            
 
Are you ready to embark on a data engineering journey that combines innovation with state-of-the-art technology? Our client, a forward-thinking organization located in , is seeking a Data Engineer to join their team. This full-time position offers the chance to work with cutting-edge technologies, including PySpark, Databricks, and Azure. If you are looking for an opportunity to make a meaningful impact and advance your career, this is the perfect role for you. Whether you're seeking a full-time position or a contract-to-hire arrangement, this is your chance to join a company that values innovation and is committed to leveraging technology to its fullest.Here's the scoop: our client is all about empowering its team members to lead and excel. The #1 feature of this opportunity is the chance to work with industry-leading technologies and be part of a team that thrives on creativity and collaboration. They're in search of an exceptional Data Engineer who's passionate about PySpark, Databricks, and Azure, someone who thrives on solving complex data engineering challenges. In return, you can expect a learning experience like no other. This is an environment that encourages continuous growth, offering you the chance to expand your skill set and stay ahead of the curve. Not only will you have the opportunity to lead innovative projects, but you'll also enjoy a great work-life balance. This role is your gateway to personal and professional growth, where you can harness your PySpark, Databricks, and Azure expertise to make a significant impact and be a part of something truly special.Contract Duration: Full-time or Contract-to-HireRequired Skills & Experience Proficiency in PySpark, Databricks, and Azure for developing and optimizing data pipelines. Strong experience in ETL processes and data transformation, ensuring data quality and reliability. Collaborative and detail-oriented, with excellent problem-solving skills to address complex data engineering challenges. Strong communication skills and the ability to work effectively in a team environment. Continuous learning and adaptability to stay updated with evolving data technologies. 
Tech BreakdownWhat You Will Be Doing 100 PySpark 100% Databricks 100% Azure 
Daily Responsibilities 100% Hands on 
Posted By: Austin Getzloff"
"Data Engineer (AWS, Azure, GCP)",CapTech,"Richmond, VA (Remote)",https://www.linkedin.com/jobs/view/3774802046/?eBP=JOB_SEARCH_ORGANIC&refId=var%2FYEIrWSv9gstoHniY8A%3D%3D&trackingId=1m%2F3BszGv8xCDoqi5MPYZA%3D%3D&trk=flagship3_search_srp_jobs,3774802046,"About the job
            
 
Company DescriptionCapTech is an award-winning consulting firm that collaborates with clients to achieve what’s possible through the power of technology. At CapTech, we’re passionate about the work we do and the results we achieve for our clients. From the outset, our founders shared a collective passion to create a consultancy centered on strong relationships that would stand the test of time. Today we work alongside clients that include Fortune 100 companies, mid-sized enterprises, and government agencies, a list that spans across the country.Job DescriptionCapTech Data Engineering consultants enable clients to build and maintain advanced data systems that bring together data from disparate sources in order to enable decision-makers. We build pipelines and prepare data for use by data scientists, data analysts, and other data systems. We love solving problems and providing creative solutions for our clients. Cloud Data Engineers leverage the client’s cloud infrastructure to deliver this value today and to scale for the future. We enjoy a collaborative environment and have many opportunities to learn from and share knowledge with other developers, architects, and our clients.Specific responsibilities for the Data Engineer – Cloud position include:  Developing data pipelines and other data products using Amazon Web Services (AWS), Microsoft Azure, or Google Cloud Platform (GCP) Advising clients on specific technologies and methodologies for utilizing cloud resources to efficiently ingest and process data quickly Utilizing your skills in engineering best practices to solve complex data problems Collaborating with end users, development staff, and business analysts to ensure that prospective data architecture plans maximize the value of client data across the organization. Articulating architectural differences between solution methods and the advantages/disadvantages of each 
QualificationsTypical experience for successful candidates includes:  Experience delivering solutions on a major cloud platform Ability to think strategically and relate architectural decisions/recommendations to business needs and client culture Experience in the design and implementation of data architecture solutions A wide range of production database experience, usually including substantial SQL expertise, database administration, and scripting data pipelines Ability to assess and utilize traditional and modern architectural components required based on business needs. A demonstrable ability to deliver production data pipelines and other data products. This could be hands on experience, degree, certification, bootcamp, or other learning. 
SkillsSuccessful candidates usually have demonstrable experience with technologies in some of these categories: Languages: SQL, Python, Java, R, C# / C++ / C Database: SQL Server, PostgreSQL, Snowflake, Redshift, Aurora, Presto, BigQuery, Oracle DevOps: git, docker, subversion, Kubernetes, Jenkins Additional Technologies: Spark, Databricks, Kafka, Kinesis, Hadoop, Lambda, EMR Popular Certifications: AWS Cloud Practitioner, Microsoft Azure Data Fundamentals, Google Associate Cloud Engineer
Additional InformationWe want everyone at CapTech to be able to envision a lasting and rewarding career here, which is why we offer a variety of career paths based on your skills and passions. You decide where and how you want to develop, and we help get you there with customizable career progression and a comprehensive benefits package to support you along the way. Alongside our suite of traditional benefits encompassing generous PTO, health coverage, disability insurance, paid family leave and more, we’ve launched extended benefits to help meet our employees’ needs. CapFlex – Employee-first mentality that supports a remote and hybrid workforce and empowers daily flexibility while servicing our clientsLearning & Development – Programs offering certification and tuition support, digital on-demand learning courses, mentorship, and skill development pathsModern Health –A mental health and well-being platform that provides 1:1 care, group support sessions, and self-serve resources to support employees and their families through life’s ups and downsCarrot Fertility –Inclusive fertility and family-forming coverage for all paths to parenthood – including adoption, surrogacy, fertility treatments, pregnancy, and more – and opportunities for employer-sponsored funds to help pay for careFringe –A company paid stipend program for personalized lifestyle benefits, allowing employees to choose benefits that matter most to them – ranging from vendors like Netflix, Spotify, and GrubHub to services like student loan repayment, travel, fitness, and moreEmployee Resource Groups – Employee-led committees that embrace and incorporate diversity and inclusion into our day-to-day operationsPhilanthropic Partnerships – Opportunities to engage in partnerships and pro-bono projects that support our communities. 401(k) Matching – Generous matching and no vesting period to help you continue to build financial wellness
CapTech is an equal opportunity employer committed to fostering a culture of equality, inclusion and fairness — each foundational to our core values. We strive to create a diverse environment where each employee is encouraged to bring their unique ideas, backgrounds and experiences to the workplace. For more information about our Diversity, Inclusion and Belonging efforts, click HERE. As part of this commitment, CapTech will ensure that persons with disabilities are provided reasonable accommodations. If reasonable accommodation is needed to participate in the job application or interview process, to perform essential job functions, and/or to receive other benefits and privileges of employment, please contact Laura Massa directly via email lmassa@captechconsulting.com.At this time, CapTech cannot transfer nor sponsor a work visa for this position. Applicants must be authorized to work directly for any employer in the United States without visa sponsorship."
Senior Data Engineer,Freestar,"San Francisco, CA (Remote)",https://www.linkedin.com/jobs/view/3100530052/?eBP=JOB_SEARCH_ORGANIC&refId=var%2FYEIrWSv9gstoHniY8A%3D%3D&trackingId=FKcrPqzvn3%2Fp2rrd6kKnnA%3D%3D&trk=flagship3_search_srp_jobs,3100530052,"About the job
            
 
About FreestarFreestar engineer teams develop cutting-edge monetization solutions for websites and mobile apps. By combining industry-leading technology, data, and massive scale, we enable busy site owners and mobile app publishers to seamlessly maximize revenue while freeing themselves of the hassles of ad operations. The result is publishers having more time to do what they do best: create content and focus on their apps.Senior Engineer (Remote)We’re looking for a Senior Engineer for our Data Platform Team - the product that drives our core business. As a Senior Engineer at Freestar, you'll be one of the main contributors to a talented team of developers to innovate features for our product that separate us from our competitors.What You'll Be Doing Run points on whole projects by recommending, prototype, build and debug data infrastructures on Google Cloud Platform (GCP) with other principal and senior engineers within the team, and to mentor less experienced Data Engineers. Demonstrate repeated delivery of project architectures and critical components that other engineers demur to you for lack of expertise.Participate in early-stage conversation with our product development team about product / features. Improve decision quality across the company by ensuring metrics are trustworthy, discoverable, and easily consumable.
Skills And Experience You'll Need To Have Data warehouse modernization: building complete data warehouse solutions, including technical architectures, star/snowflake schema designs, infrastructure components, ETL/ELT pipelines, and reporting/analytic tools. Must have hands-on experience working with batch or streaming data processing software (such as Beam, Airflow, Hadoop, Spark, Hive, Kafka, PubSub).4+ years of experience designing and implementing large-scale, complex, data-driven applications on the cloud, preferably on Google Cloud / AWS.4+ years of hands-on experience using SQL to perform complex data manipulation3+ years of experience modeling data warehouses3+ years of experience building data pipelines, CI/CD pipelines, and fit for purpose data storesData migration: migrating data stores to reliable and scalable cloud-based stores, including strategies for near zero-downtime.Experience writing software in one or more languages such as Python, Java, Scala, etc.Experience with systems monitoring/alerting, capacity planning and performance tuningEnjoy analyzing and organizing rapidly-changing business data to support product and business solutionsNice to have Qualifications: *Experience working with Google Cloud data products (CloudSQL, Spanner, Cloud Storage, Pub/Sub, Dataflow, Dataproc, BigQuery, Dataprep, Composer, etc)Experience with IoT architectures and building real-time data streaming pipelinesExperience operationalizing machine learning models on large datasetsDemonstrated leadership and self-direction -- a willingness to teach others and learn new techniquesDemonstrated skills in selecting the right statistical tools given a data analysis problemComfortable interacting across multiple teams and management levels within the organizationPrevious background in the ad tech or media landscape (linear, digital, or social) is a plus
Why You Want To Work With Us Full-Time, Salaried PositionWorking remotelyGenerous Medical, Dental, and Vision benefits401K with company match, vested immediatelyThe opportunity to be part of something high value, high impact, and high growth...Who doesn't love that!
Freestar is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran or disabled status, or genetic information."
Staff Healthcare Data Engineer - remote,Vytalize Health,"Hoboken, NJ (Remote)",https://www.linkedin.com/jobs/view/3764804037/?eBP=JOB_SEARCH_ORGANIC&refId=var%2FYEIrWSv9gstoHniY8A%3D%3D&trackingId=8CcP1PlJ3Wdhjmaqf72Pkg%3D%3D&trk=flagship3_search_srp_jobs,3764804037,"About the job
            
 
About Our CompanyVytalize Health is a leading value-based care platform. It helps independent physicians and practices stay ahead in a rapidly changing healthcare system by strengthening relationships with their patients through data-driven, holistic, and personalized care. Vytalize provides an all-in-one solution, including value-based incentives, smart technology, and a virtual clinic that enables independent practices to succeed in value-based care arrangements. Vytalize's care delivery model transforms the healthcare experience for more than 250,000+ Medicare beneficiaries across 36 states by helping them manage their chronic conditions in collaboration with their doctors.About Our GrowthVytalize Health has grown its patient base over 100% year-over-year and is now partnered with over 1,000 providers across 36-states. Our all-in-one, vertically integrated solution for value-based care delivery is responsible for $2 billion in medical spending. We are expanding into new markets while increasing the concentration of practices in existing ones.Visit www.vytalizehealth.com for more information.Why you will love working hereWe are an employee first, mission driven company that cares deeply about solving challenges in the healthcare space. We are open, collaborative and want to enhance how physicians interact with, and treat their patients. Our rapid growth means that we value working together as a team. You will be recognized and appreciated for your curiosity, tenacity and ability to challenge the status quo; approaching problems with an optimistic attitude. We are a diverse team of physicians, technologists, MBAs, nurses, and operators. You will be making a massive impact on people's lives and ultimately feel like you are doing your best work here at Vytalize.We are building out a world-class engineering team to empower our business. As a Staff Data Engineer, you will be responsible for overseeing the data architecture implementation and integration processes, with a primary focus on seamlessly integrating structure/semi-structure/unstructured data from various data sources and driving integration initiatives with our internal/external systems. You will collaborate with cross-functional teams, leveraging your expertise to ensure the efficient flow of data, optimize data pipelines, and contribute to the overall success of our healthcare data ecosystem. This is a highly impactful role that delivers huge value to the company resulting in potential annual saving of tens or hundreds of millions of dollars.Responsibilities:  Data Architecture and Integration Lead efforts to enhance, extend, and strengthen our existing data architectures to support the current and future new business use cases, including ingesting new data sources (Payer claims, ADT, Clinical, Labs data, etc.) to our central data management system. Develop and optimize data pipelines for the extraction, loading, and transformation (ELT) of healthcare data and ensure delivery to internal/external systems with high quality. 

  Payer Claims / Clinical Data Integration Collaborate with stakeholders to understand business requirements related to Payer claims data while supporting existing needs for Centers for Medicare & Medicaid Services (CMS) data Implement solutions for integrating and harmonizing other forms of data including clinical data (Labs, ADT ect.), ensuring accuracy, completeness, and compliance with regulatory standards. 

  EMR Systems Integration Support key initiatives to integrate our internal tech platform with various Electronic Medical Record (EMR) systems. Work closely with EMR vendors and internal teams to establish seamless data exchange, inflow/outflow and interoperability. 

  Data Quality/Governance/Security Implement robust data quality assurance processes to validate and ensure the accuracy of integrated healthcare data. Collaborate with data governance and IT teams to establish and enforce data quality and security policies and standards for full business compliance 

  Execution and Delivery Act as a ""player-coach"" for other data engineers and be willing to dive into the implementation and get things done as needed. Be a strong partner to engineering leadership to achieve repeatable successes in project delivery while uphold high standards on engineering excellence and team building 

  Collaboration and Communication Collaborate with cross-functional teams, including our Lead Clinical Data Architect, product managers, analytics, and IT professionals, to understand data needs and deliver effective solutions. Communicate technical concepts and solutions to non-technical stakeholders, fostering a culture of data-driven decision-making. 

Qualifications Master's or PhD degree in Computer Science, Information Systems, or a related field. 15+ years of engineering experience. Proven expert experience in data engineering, having built complex data systems end to end while being accountable for the outcome. Strong domain knowledge working in the healthcare industry, expertise dealing with healthcare data is a must have. Prior experience in value-based Care space is a big bonus. Strong proficiency in SQL, Python, and other relevant programming languages. Experience with big data technologies (e.g., Hadoop, Spark) and cloud platforms (e.g., AWS, Azure, GCP). Familiarity with healthcare data standards, regulations, and interoperability standards such as (FHIR, HIE) Excellent communication and problem-solving skills and the ability to work in a dynamic and collaborative environment. Cultural value principles: Proactivity, Tenacity, Reliability, Excellence, Clarity, Optimism, Transparency, Kindness. If these resonate well with you, you should ping us. 
Perks/Benefits Competitive base compensationAnnual bonus potential Health benefits effective on start date; 100% coverage for base plan, up to 90% coverage on all other plans for individuals and familiesHealth & Wellness Program; up to $300 per quarter for your overall wellbeing401K plan effective on the first of the month after your start date; 100% of up to 4% of your annual salaryCompany paid STD/LTDUnlimited (or generous) paid ""Vytal Time"", and 5 paid sick days after your first 90 days. Technology setupAbility to help build a market leader in value-based healthcare at a rapidly growing organization. 
We are interested in every qualified candidate who is eligible to work in the United States. However, we are not able to sponsor visas.Please note at no time during our screening, interview, or selection process do we ask for additional personal information (beyond your resume) or account/financial information. We will also never ask for you to purchase anything; nor will we ever interview you via text message. Any communication received from a Vytalize Health recruiter during your screening, interviewing, or selection process will come from an email ending in @vytalizehealth.com"
AWS Data Engineer with Node JS,"Donato Technologies, Inc.",United States (Remote),https://www.linkedin.com/jobs/view/3725483547/?eBP=JOB_SEARCH_ORGANIC&refId=var%2FYEIrWSv9gstoHniY8A%3D%3D&trackingId=uthKwQJqMCfi1X7zA%2F%2FdWg%3D%3D&trk=flagship3_search_srp_jobs,3725483547,"About the job
            
 
Job Title: AWS Data Engineer with Node JSLocation: 100% remote (MST Time Zone)Duration: 6 Months ContractMust Haves AWS - general (IAM, role and policies, Secret Manager, KMS)Lambda functions with PythonSQS, SNS, Kinesis, EC2, EMR, S3Node.js for data pulls using API callsLanguages: Relational SQL, NoSql, Python
Description 4+ years of experience developing complex enterprise applicationsGreat leadership skillsImpeccable communication and team skills with shared ownership of code and other deliverables.Willingness to work with and learn new technologies.
Must have Good knowledge and experience of AWS Cloud. Kinesis, S3, SQSExtensive experience working with relational and NoSql databasesExperience with team development tooling (especially with Jira and Github).Familiarity with Docker architectures, and Terraform deployments.Expert in Node.jsExperience with MuleSoft and GraphQLExperience with distributed systems and federated authentication systems.Experience with Maven/Gradle build systems.Understanding of BFF (Backend-for-Frontend) patterns.Experience with development of self-healing, reliable and reactive systems.
Preferable Experience with Docker and Serverless architectures like AWS Lambda, DynamoDB, ECS, S3, Amazon Kinesis, EventBridge, SQS, CloudFormation, Terraform, and/or other similar cloud services.Experience writing microservices/lambdas in Javascript/Typescript for data processing.Experience working in a complex enterprise environment and developing complex programsExperience with REST API architecture and development, especially using Swagger or Apigee.Knowledge of Git including version control, branching, merging/rebasing, and pull requests.Experience writing SQL and a procedural language (Python, R, etc.) for data handling. (Preferred: Snowflake DBT and SingleStore DB).Strong focus on automation including Continuous Integration / Deployment with writing unit and integration tests.Experience in Agile/SCRUM Software Development ProcessExperience implementing data analytics, visualization tools and programs using Tableau, Grafana, and Google Sheets.Good security practices and experience writing code that manages customer data.Build architectural models with synchronous and asynchronous patterns to decouple, integrate and scale services.Experience with real-time data processing."
Sr. Data Engineer - Remote | WFH,Get It Recruit - Information Technology,"Fort Worth, TX (Remote)",https://www.linkedin.com/jobs/view/3763878703/?eBP=JOB_SEARCH_ORGANIC&refId=var%2FYEIrWSv9gstoHniY8A%3D%3D&trackingId=UIqtVV2fwUPUkDaMT5GSxA%3D%3D&trk=flagship3_search_srp_jobs,3763878703,"About the job
            
 
We are currently seeking an experienced and highly skilled Senior Data Engineer for an exciting opportunity. In this hybrid role, you'll be part of a dynamic team, contributing your expertise one day a week at our Fort Worth location. As a Senior Data Engineer, you will play a key role in analyzing, architecting, and engineering data pipelines and integration activities to support various data products.Key ResponsibilitiesDesign and develop data pipelines, ensuring efficiency and reliability throughout the data product lifecycle.Collaborate with Business Intelligence developers and analysts to create robust, scalable data architectures for reports, dashboards, integrations, and advanced analytics products.Interface with business customers to bridge the gap between business needs and technology solutions, ensuring technical solutions align with business requirements.Utilize strong expertise in SQL, data modeling, and ETL/ELT development, along with familiarity with modern cloud-based data platforms, integration technologies, and business intelligence tools.Demonstrate coding proficiency in at least one modern programming language (Python, Scala, Java, etc.).Align data architectures with business requirements and build data pipelines that interface well with various data sources.QualificationsHigh school diploma or GED required.Bachelor’s degree in Computer Science, Engineering, Management Information Systems (MIS), Business, or related field AND a minimum of 5 years of relevant experience.OR - Minimum of 7 years of experience in Data Engineering, Database Development, BI Development, Software Engineering, Programming, Technical or Systems Analysis, and/or Database Administration.Minimum of 5 years of experience in Information Technology, with at least 2 years in a Health Care environment preferred.Knowledge And ExperienceDemonstrated strength in SQL coding, data integration, data modeling, and ETL/ELT design and development.Experience with data lake and data warehouse methodologies, design/modeling, architecture, implementation, security, and support/maintenance.Coding proficiency in at least one modern programming language (Python, Scala, Java, Spark, etc.) preferred.Strong experience with data ingestion and transformation technologies such as Fivetran, Stitch, dbt, as well as traditional data integration/management tools such as SSIS, Informatica, etc.Familiarity with cloud data lake/data warehouse technologies such as Snowflake, Azure Synapse, AWS Redshift, etc. preferred.Experience with Agile and DataOps delivery methodologies.Comfortable with ambiguity, experimentation, and rapid iteration.Proven experience with business and technical requirements elicitation, analysis, and verification.Experience Working With Structured, Semi-structured, And Unstructured Data Preferred.Experience with healthcare-specific applications and data systems, business systems, and digital platforms preferred.Licensure/CertificationEpic certification(s) a plus, and may be required upon hire or within six months of hire date based on the primary responsibilities/focus of the role.Preferred Epic certification(s) include Cogito Systems Administration, Caboodle-Clarity Development, Caboodle Development, Clarity Data Model, Clinical Data Model, Revenue Cycle Data Model, Access Data model, and/or Caboodle Data Model.Snowflake and/or Microsoft certification(s) a plus.Job Type: Full-timeLocation: Hybrid role with one day per week onsite in Fort Worth.Work Arrangement: Remote work available.We look forward to welcoming a talented Senior Data Engineer to our team! If you're passionate about data and eager to contribute to innovative projects, we encourage you to apply and join our collaborative environment.Employment Type: Full-Time"
Senior Data Engineer,Tubi,"San Francisco, CA (Remote)",https://www.linkedin.com/jobs/view/3743844492/?eBP=JOB_SEARCH_ORGANIC&refId=var%2FYEIrWSv9gstoHniY8A%3D%3D&trackingId=T7a%2Fvwv84LKIA0uocXFhQw%3D%3D&trk=flagship3_search_srp_jobs,3743844492,"About the job
            
 
Join Tubi (www.tubi.tv), Fox Corporation's premium ad-supported video-on-demand (AVOD) streaming service leading the charge in making entertainment accessible to all. With over 200,000 movies and television shows, including a growing library of Tubi Originals, 200+ local and live news and sports channels, and 455 entertainment partners featuring content from every major Hollywood studio, Tubi gives entertainment fans an easy way to discover new content that is available completely free. Tubi's library has something for every member of our diverse audience, and we're committed to building a workforce that reflects that diversity. We're looking for great people who are creative thinkers, self-motivators, and impact-makers looking to help shape the future of streaming.About the Role:In this Data Engineering role, you will help us build out a new team of product-focused data engineers. As the most senior member on this new team, you will work closely with one of our data product verticals, setting the culture, best practices, and expectations for how your new team's data engineers will work with our different product verticals. Tubi has many data-hungry product verticals you may work with, including Performance Marketing, Growth Marketing, User Identity, Ad Tech, Content Analytics, Content Acquisition, and more. As the senior engineer on the team, you will also mentor and help newer engineers along the way. Each product team you will work with has unique data challenges, all revolving around massive datasets, so you should be adaptable, curious, and have a solid foundation in big data and engineering fundamentals. You will do everything from building efficient pipelines, helping data scientists and analysts get the data they need, and even build out specific data products to help non-engineering teams get better insight into their unique datasets.Your Responsibilities: Be the primary owner of the data needs of a specific product vertical. That means everything from raw data ingestion to end-user analysisBuild intuitive, easy-to-use, and high-quality datasets using Spark and/or DBTTrack down data quality issues when they arise, and then set appropriate data quality monitors and alerts to help prevent future incidentsParticipate in the occasional daytime on-call rotationReally understand your product vertical's datasets, with an ability to document and educate your team on how certain tables / fields are meant to be usedBe the liaison between your product vertical and the core data infrastructure team to make sure business needs are met in a computationally efficient manner
Your Background: 7+ years of data engineering experience building scalable, flexible, and always-on data pipelinesStrong experience using SQL, Python and/or Spark for data manipulationExperience with efficiently working with datasets at TB scaleFamiliarity with Databricks, DBT, Redshift, and AWS in particular is helpfulA passion for shipping production quality code with good test coverage
Benefits of Joining Our Team: Working with a talented, tight-knit team of passionate people with the mandate to make premium content accessible to everyoneUse the latest data engineering technologies and techniques to solve engineering problems at petabyte-scaleAutonomy and end-to-end ownership of what you createYour choice of hardware to work withOpportunity for internal growthWork with other fellow AVOD enthusiastsWe offer competitive pay, equity, full medical, dental & vision benefits, catered meals, gym subsidies, open vacation policy, and more
Pursuant to state and local pay disclosure requirements, the pay range for this role, with final offer amount dependent on education, skills, experience, and location is listed annually below. This role is also eligible for an annual discretionary bonus, long-term incentive plan and various benefits, including medical/dental/vision, insurance, a 401(k) plan, paid time off, and other benefits in accordance with applicable plan documents.California, New York City, Westchester County, NY and Seattle, WA$199,000—$234,000 USDColorado and Washington (excluding Seattle, WA)$178,000—$210,000 USDTubi is a division of Fox Corporation, and the FOX Employee Benefits summarized here, covers the majority of all US employee benefits. The following distinctions below outline the differences between the Tubi and FOX benefits: For US-based non-exempt Tubi employees, the FOX Employee Benefits summary accurately captures the Vacation and Sick Time.For all US-based employees, Tubi offers 12 paid ""Tubi Holidays"" in addition to the 10 FOX Corporate Company paid holidays.For all salaried/exempt employees, in lieu of the FOX Vacation policy, Tubi offers a Flexible Time off Policy to manage all personal matters.For all full-time, regular employees, in lieu of FOX Paid Parental Leave, Tubi offers a generous Parental Leave Program, which allows parents twelve (12) weeks of paid bonding leave within the first year of the birth, adoption, surrogacy, or foster placement of a child. This time is 100% paid through a combination of any applicable state, city, and federal leaves and wage-replacement programs in addition to contributions made by Tubi.For all full-time, regular employees, Tubi offers a monthly wellness reimbursement.
Tubi is proud to be an equal opportunity employer and considers qualified applicants without regard to race, color, religion, sex, national origin, ancestry, age, genetic information, sexual orientation, gender identity, marital or family status, veteran status, medical condition, or disability. Pursuant to the San Francisco Fair Chance Ordinance, we will consider employment for qualified applicants with arrest and conviction records. We are an E-Verify company."
Sr Data Engineer,Oxenham Group,United States (Remote),https://www.linkedin.com/jobs/view/3767526354/?eBP=JOB_SEARCH_ORGANIC&refId=var%2FYEIrWSv9gstoHniY8A%3D%3D&trackingId=7zKyYnfxJZ%2BDdmtP1ja%2BZg%3D%3D&trk=flagship3_search_srp_jobs,3767526354,"About the job
            
 
Sr Data EngineerOur client is a technology company building custom data solutions that empower companies to to more with their data. They are adding a Sr Data Engineer to their team:Requirements:  Proven experience as a Data Engineer or in a similar role, showcasing proficiency in data manipulation. Strong SQL proficiency, experience with relational databases, and competency in at least one programming language (e.g., Python, Java, Scala) for data processing and scripting. Knowledge of data modeling and database design principles. Understanding of cloud platforms such as AWS, Azure, or Google Cloud. Excellent problem-solving skills and effective communication abilities. 
Preferred Qualifications:  Education: Bachelor's degree in Computer Science, Experience with data streaming technologies Understanding of data governance and security best practices. Experience and certifications in AWS and Databricks technologies."
Workplace Analytics Engineer,Canonical,"Atlanta, GA (Remote)",https://www.linkedin.com/jobs/view/3738244806/?eBP=JOB_SEARCH_ORGANIC&refId=var%2FYEIrWSv9gstoHniY8A%3D%3D&trackingId=i0GNZjucmsy47ZdQKz2dng%3D%3D&trk=flagship3_search_srp_jobs,3738244806,"About the job
            
 
Bring your people analytics, social science research and data mining/science skills to a unique team seeking to understand, and shape, the future of the digital workplace. We are interested in technology, of course, but we are also interested in the human mission of enabling the world's brightest and hardest working people to live where they want and work from anywhere. Most of our colleagues could move to a tech hub but they choose Canonical because of our mission and our approach to the workplace.We'd like to understand what really makes a distributed, remote-first workplace work. We think we're pretty good at this (being remote first for almost 20 years), but we know there is a lot still to understand, and the frontier of possibility continues to move outward. We'd like to invest in research, analytics and tooling which raises the bar even further for remote collaboration and organisation.If we are able to build tools that meaningfully improve our cooperation and our satisfaction, then we intend to share our insights with the world, both as a narrative and as SAAS or open source that helps other companies follow in our footsteps.The role of a Workplace Analytics Engineer at CanonicalSupport analytics and data mining in a cross-disciplinary team of organisational psychologists, web front end engineers, back end engineers and statistics / analytics experts to help us build a new definition for the 21st century digital workplace. Collaborate to figure out what really drives productivity, effectiveness and happiness in a remote-first globally distributed company.In addition to your existing people analytics work experience, this role will combine your skills in psychology, data analytics and visualisation, to help create a more effective workplace.Location: This role will be based remotely in the AMER region.All applicants applying must be legally authorized to work in the United States, as we cannot offer visa sponsorship for this job position.What your day will look like Utilize advanced data analytics to understand how we hire and how we work (productivity, happiness and effectiveness) across a global, remote first organisationFocus on quantitative and qualitative data analytics to find insights and meaningful business outcomesTell the story from the insights through dashboards, visualizations and presentationsDesign and conduct research into trends shaping talent science and remote workCollaborate with stakeholder teams (ex., engineering, information systems, etc) to improve the data and tool ecosystem supporting our digital workplace
What we are looking for in you Background in data science, mathematics, actuarial science, or engineeringFirst work experience in People Analytics Knowledge in advanced statistics, data sciences, coding/scripting languages (Python, R, etc), and databases (SQL, etc)Strength in data analytics and visualization (Looker Studio, Tableau, etc)Ability to translate business questions to key research objectivesAbility to identify the best methodology to execute research, synthesize and analyse findingsExcellent writing and communication skillsWillingness to examine the status quo and resilient in the face of challenges
What we offer youYour base pay will depend on various factors including your geographical location, level of experience, knowledge and skills. In addition to the benefits above, certain roles are also eligible for additional benefits and rewards including annual bonuses and sales incentives based on revenue or utilization. Our compensation philosophy is to ensure equity right across our global workforce.In addition to a competitive base pay, we provide all team members with additional benefits, which reflect our values and ideals. Please note that additional benefits may apply depending on the work location and, for more information on these, you can ask in the later stages of the recruitment process.🏠 Fully remote working environment - we've been working remotely since 2004!📚 Personal learning and development budget of 2,000USD per annum💰 Annual compensation review🏆 Recognition rewards🏝 Annual holiday leave👶 Parental Leave🧑‍💼 Employee Assistance Programme🧳 Opportunity to travel to new locations to meet colleagues at 'sprints'✈️ Priority Pass for travel and travel upgrades for long haul company eventsAbout CanonicalCanonical is a pioneering tech firm that is at the forefront of the global move to open source. As the company that publishes Ubuntu, one of the most important open source projects and the platform for AI, IoT and the cloud, we are changing the world on a daily basis. We recruit on a global basis and set a very high standard for people joining the company. We expect excellence - in order to succeed, we need to be the best at what we do.Canonical has been a remote-first company since its inception in 2004. Work at Canonical is a step into the future, and will challenge you to think differently, work smarter, learn new skills, and raise your game. Canonical provides a unique window into the world of 21st-century digital business.Canonical is an equal opportunity employerWe are proud to foster a workplace free from discrimination. Diversity of experience, perspectives, and background create a better work environment and better products. Whatever your identity, we will give your application fair consideration."
Senior Data Infrastructure Engineer,Trendpop,"San Francisco, CA (Remote)",https://www.linkedin.com/jobs/view/3619298435/?eBP=JOB_SEARCH_ORGANIC&refId=var%2FYEIrWSv9gstoHniY8A%3D%3D&trackingId=civafQP7Zyj%2F8c2lY16sjw%3D%3D&trk=flagship3_search_srp_jobs,3619298435,"About the job
            
 
Trendpop is looking for a Senior Data Infrastructure Engineer to help us reverse engineer the algorithms that power virality on social media. Trendpop Core, our large-scale data infrastructure platform, ingests millions of social media posts and processes multiple terabytes per day to surface valuable insights for our customers. This person will work closely with our CTO to build the next phase of Trendpop Core infrastructure to support additional data sources, scale to the next order of magnitude, and glean smarter insights using cutting edge machine learning techniques. This person should be excited to work in a tiny but effective engineering team at a fast-paced startup. They will be given broad autonomy to tackle the toughest infrastructure problems we face every day!"
AWS Data Engineer(Remote),SmartIPlace,United States (Remote),https://www.linkedin.com/jobs/view/3768108050/?eBP=JOB_SEARCH_ORGANIC&refId=yXH%2FaU%2FioH8YhIBIXRcvKg%3D%3D&trackingId=hF1SYYsBs7VrtiXflg5JTw%3D%3D&trk=flagship3_search_srp_jobs,3768108050,"About the job
            
 
Title: AWS Data Engineer [100% remote]Experience: 10+ yearsVisa: GC USC TNSkills Required  AWS AWS Glue Lambda AWS S3 Athena"
Data Platform Engineer,Graphite Health,United States (Remote),https://www.linkedin.com/jobs/view/3747925466/?eBP=JOB_SEARCH_ORGANIC&refId=yXH%2FaU%2FioH8YhIBIXRcvKg%3D%3D&trackingId=vHl03x7bce%2BwT7rJbs%2BIbQ%3D%3D&trk=flagship3_search_srp_jobs,3747925466,"About the job
            
 
About GraphiteA Common Language for Digital HealthGraphite is a software company creating the first comprehensive digital ecosystem for the frictionless exchange of health data. This has been tried before, but never in this way. We are a non-profit, partnering with leading healthcare organizations to achieve the critical mass necessary to align around a de facto industry standard. By doing this together, we are ensuring this is done for the benefit of all. For more information please visit www.graphitehealth.io .Job SummaryWe are looking for passionate and skilled Data Platform Engineers with a focus on streaming technologies to join our dynamic team. The ideal candidate will have a strong background in data engineering, especially streaming technologies such as Kafka, Spark Streaming, and Flink, and big data storage technologies such as Iceberg, Hudi, and FoundationDB. You will also have experience with cloud computing platforms, such as AWS, Azure, and Google Cloud Platform. You will be responsible for designing, building, and maintaining our real-time data streaming platform and ensuring the optimal performance, quality, and responsiveness of our data services.We’re hiring Data Platform Engineers at multiple levels, including Senior, Staff, and Principal Engineer.This role can be 100% remote in the USA; it is a full-time role and offers extensive benefits including a healthcare plan.Responsibilities Design and build scalable and reliable real-time data streaming solutions using technologies such as Kafka, Flink, Spark Streaming, or similar Collaborate with other engineers and teams to implement large-scale data processing systems Maintain and optimize data pipelines, architectures, and data sets Troubleshoot and resolve issues with our streaming data platform Ensure high data quality and availability for business and operational uses Build, develop and maintain data models, reporting systems, data automation systems, dashboards, and performance metrics support Work with stakeholders including the Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs Keep our data separated and secure across customer boundaries through multiple data centers and cloud regions. 
Required Qualifications Bachelor's degree in Computer Science, Engineering, Mathematics, or a related field 5+ years of experience working as a software engineerStrong experience with distributed systems, and big data technologies and tools: Kubernetes, Kafka, Spark, Presto, etc. Experience with stream-processing systems: Flink, Spark, etc. Experience with big data storage systems: Iceberg, Hudi, FoundationDB, etc. Experience with cloud services across public cloud providers Strong programming experience with Java and Scala Strong analytical skills related to working with structured and unstructured datasets Strong communication, project management and organizational skills Experience supporting and working with cross-functional teams in a dynamic environment 
Work Environment Fast-paced work environmentWork in a quiet remote work environmentBe available for collaborative work (via computer, videoconference, teleconference, etc.) during Graphite working hours, Monday through FridaySit for prolonged periods working on a computerLift basic office equipment and suppliesOccasional travel (including air travel) may be required
Perks And BenefitsGraphite offers comprehensive benefit options designed to provide choice and flexibility, including: Top-tier medical, dental and vision plans Flexible Spending Account Short- and long-term disability plans Life and AD&D Voluntary benefits, including options such as accident insurance and pet insurance 
You Will Be Eligible For Annual performance bonuses 401(k) plan participation with employer match A home-office stipend
Generous Time-off Policy, With Unlimited PTO Paid Holidays 
Additional Benefits Include Remote-first work environment Flexible work schedules Professional development reimbursement 
Are you interested in tackling some of healthcare’s toughest challenges, working in a company culture rooted in Doing Good, and improving the lives of others? Come join us!"
"Senior Data Engineer, Platform Engineering",DApp360 Workforce,"Florida, United States (Remote)",https://www.linkedin.com/jobs/view/3766878904/?eBP=JOB_SEARCH_ORGANIC&refId=yXH%2FaU%2FioH8YhIBIXRcvKg%3D%3D&trackingId=qfpJ%2FbNJFxbX7EJuBDJoSw%3D%3D&trk=flagship3_search_srp_jobs,3766878904,"About the job
            
 



      This job is sourced from a job board.
      Learn More



DApp360 Workforce is recruiting for an experienced Senior Data Engineer, Platform Engineering. The Data Platform team builds and maintains the data processing and analytic pipelines that democratize access to data across our organization.Data Engineers Have Several Key ResponsibilitiesCreate and maintain optimal data pipeline architecture to extract, transform, and load data from a wide variety of data sources using SQL and Azure big data technologiesAssemble large, complex data sets that meet functional / non-functional business requirementsIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.Keep our data separated and secure across national boundaries through multiple data centers and Azure regionsCreate Data Tools For Analytics And Data Scientist Team Members That Assist Them In Building And Optimizing Our Product Into An Innovative Industry Leader. We Expect To See5+ years experience building and optimizing big data pipelines, architectures, and data sets5+ years working SQL knowledge and experience working with relational databases, query authoring (SQL), as well as working familiarity with a variety of database platforms5+ years working with PythonStrong analytic skills related to working with free-form text5+ years experience building processes supporting data transformation, data structures, metadata, dependencies, and workload managementA track record of leading and mentoring less experienced developers. You are eager to teach others and invested in the growth of your team.Self-motivating, self-directing, and a great communicator (written and oral). You thrive in an environment that grants you a lot of autonomy to explore creative solutions.Excellent problem solving skills. You excel at analyzing and solving problems using technology.Living and working within GMT-7:00 (US) to GMT+2:00 (Europe) time zones. We like to see (but not required):Experience with Microsoft technologies and Azure cloud services for building and operating data pipelinesExperience working remotely and/or working with teams that are distributed geographically.Experience with Agile methodologies such as Scrum, XP, or Kanban. Certification is a plus, but not a requirement.An active Stack Overflow profile, open source code, example projects that you're proud of (whether open source or worked on at a previous job), or any other evidence of your passion for building great software.Knowledge of how Stack Overflow works from our blog, podcasts, and other public artifacts. Ideas about how to evolve the platform and increase our impact on the developer community are even better.Experience with leveraging cloud-native technologies and techniques to build product ecosystems Base salary will range from: $150k - $175k USD What Youll Get in Return:Competitive Base SalaryGenerous paid vacationGenerous parental leave (16 weeks at 100% pay), family care leave, and unlimited sick daysEquity (RSUs) for all employees at all levelsIndustry-leading health benefits that are applicable per country of residence for all our full-time employeesCompany-paid Life InsuranceHealth & wellness stipendHome Internet stipendProfessional allocation for your growth and developmentHome office allowance of $2,000 (for remote employees) with an additional $450 allowance on each anniversary dateCompany-paid access to Calm, Bravely, LinkedIn Learning, MyAcademy and Overdrive One of the most popular websites in the world - a community-based space focused on increasing productivity, decreasing cycle times, accelerating time to market, and protecting institutional knowledge. Innovation is at the heart of everything done, as well as embracing collaboration, transparency, and believing in leading with empathy; creating an environment where everyone knows they belong. The company embraces the unique contributions and points of view of all contributers to their success.AsSenior Data Engineer, Platform Engineering your cross-functional team will optimize flow and collection of all product and business data, enabling data at scale. Youll ensure that the data delivery architecture is optimized and consistent, enabling software engineers, database architects, data analysts, and data scientists to do their best work."
"Data Engineer, Database Engineering",Experfy,"Seattle, WA (Remote)",https://www.linkedin.com/jobs/view/3590301397/?eBP=JOB_SEARCH_ORGANIC&refId=yXH%2FaU%2FioH8YhIBIXRcvKg%3D%3D&trackingId=3iqesBBVULfGRL4tt0ERvQ%3D%3D&trk=flagship3_search_srp_jobs,3590301397,"About the job
            
 
As a Data Engineer for our Data Platform Engineering team you will join skilled Scala/ Spark engineers and core database developers responsible for developing hosted cloud analytics infrastructure (Apache Spark-based), distributed SQL processingframeworks, proprietary data science platforms, and core database optimization. This team is responsible for building the automated, intelligent, and highly performant query planner and execution engines, RPC calls between datawarehouse clusters, shared secondary cold storage, etc. This includes building new SQL features and customer-facing functionality, developing novel query optimization techniques for industry-leading performance, and building a databasesystem that's highly parallel, efficient and fault-tolerant. This is a vital role reporting to exec leadership and senior engineering leadershipRequirementsResponsibilities: Writing Scala code with tools like Apache Spark + Apache Arrow + Apache Kafka to build a hosted, multi-cluster data warehouse for Web3Developing database optimizers, query planners, query and data routing mechanisms, cluster-to-cluster communication, and workload management techniquesScaling up from proof of concept to ""cluster scale"" (and eventually hundreds of clusters with hundreds of terabytes each), in terms of both infrastructure/architecture and problem structureCodifying best practices for future reuse in the form of accessible, reusable patterns, templates, and code bases to facilitate meta data capturing and managementManaging a team of software engineers writing new code to build a bigger, better, faster, more optimized HTAP database (using Apache Spark, Apache Arrow, Kafka, and a wealth of other open source data tools)Interacting with exec team and senior engineering leadership to define, prioritize, and ensure smooth deployments with other operational componentsHighly engaged with industry trends within analytics domain from a data acquisition processing, engineering, management perspectiveUnderstand data and analytics use cases across Web3 / blockchains
Skills & Qualifications Bachelor's degree in computer science or related technical field. Masters or PhD a plus6+ years experience engineering software and data platforms / enterprise-scale data warehouses, preferably with knowledge of open source Apache stack (especially Apache Spark, Apache Arrow, Kafka, and others)3+ years experience with Scala and Apache Spark (or Kafka)A track record of recruiting and leading technical teams in a demanding talent marketRock solid engineering fundamentals; query planning, optimizing and distributed data warehouse systems experience is preferred but not requiredNice to have: Knowledge of blockchain indexing, web3 compute paradigms, Proofs and consensus mechanisms... is a strong plus but not requiredExperience with rapid development cycles in a web-based environmentStrong scripting and test automation knowledgeNice to have: Passionate about Web3, blockchain, decentralization, and a base understanding of how data/analytics plays into this"
Lead Azure Data Engineer,"Donato Technologies, Inc.",United States (Remote),https://www.linkedin.com/jobs/view/3707657523/?eBP=JOB_SEARCH_ORGANIC&refId=yXH%2FaU%2FioH8YhIBIXRcvKg%3D%3D&trackingId=v0e%2Fr02bwCVU7QgYFxYlTw%3D%3D&trk=flagship3_search_srp_jobs,3707657523,"About the job
            
 
Job Title: Lead Azure Data EngineerLocation: 100% RemoteDuration: 6 Months Contract (Possibility for long term extension)Key Skills: ADF, ADLS, Synapse (Azure Sql Datawarehouse), T-Sql This is a lead level role, someone who has lead a team. Min 10-12+ years' experience.Should be purely focused on Azure Cloud, Microsoft Stack, Data Warehouse experience.Looking for someone who been a BI/Sql/ETL Developer etc.. and moved in to Azure cloud Data engineering.Should have good experience in Azure Data lake, Azure Data Factory, Azure Databricks, Azure Synapse and data warehousing concepts.
Description  Enterprise Data modelling / Design Azure SQL Data Warehouse (Synapse) T-SQL ( Hands-on experience, writing queries, building stored procs, performance optimization, etc..) ADF / Any Enterprise ETL Tool ADLS / any cloud storage Should be able to understand the technical specifications and able to work independently with minimal or no supervision
Must Have Skills Extensive experience providing practical direction within azure native services , implementing data migration and data processing using Azure services: ADLS, Azure Data Factory, Synapse/DW /Azure SQL DB, Fabric.Proven experience with SQL, namely schema design and dimensional data modellingSolid knowledge of data warehouse best practices, development standards and methodologiesStrong experience with Azure Cloud on data integration with DatabricksBe an independent self-learner with the ""let's get this done"" approach and ability to work in Fast paced and Dynamic environment
Nice-to-Have Skills Basic understanding on ML Studio, AI/ML, MLOps etc.Good to have Event Hub, IOT Hub, Azure Stream Analytics, Azure Analysis Service, Cosmo Db knowledge.Good to have SAP Hana knowledgeIntermediate knowledge on Power BIGood to have knowledge in DevOps and CI/CD deployments, Cloud migration methodologies and processes."
Data Visualization Engineer II,Compassion International,United States (Remote),https://www.linkedin.com/jobs/view/3772919425/?eBP=JOB_SEARCH_ORGANIC&refId=yXH%2FaU%2FioH8YhIBIXRcvKg%3D%3D&trackingId=TXI7rkc9wDwGhXSCUn5kJg%3D%3D&trk=flagship3_search_srp_jobs,3772919425,"About the job
            
 
The expected salary range for this position is $75,200.00 - $94,000.00. Employees in specific high cost of labor locations in the United States (such as San Francisco, CA and Seattle, WA) may qualify for a geographic differential. Compassion International is not responsible for third parties who omit this information when copying and re-posting job openings.Come join our Data Actionability team where you can have a positive influence on the lives of children around the world! As part of the Data Insights team, we are passionate about empowering the local church and leveraging data to address children's needs. Our team’s objective is to deliver highly actionable, data-driven insights to our neighbours.You will create action-focused, decision-making dashboards and tools that assist neighbours in a wide variety of contexts. To build high-value products, you will interpret a broad set of data and analyses and then communicate them in a compelling, contextualized story. The ideal candidate for this position will excel in data visualization, analysis, and manipulation. They will be skilled in efficient data storytelling, communication, and cross-team collaboration.PLEASE NOTE: This is a remote, US-based position. What will you do?  Maintain a personal relationship with Jesus Christ. Is a consistent witness for Jesus Christ, maintains a courteous, Christ-like attitude in dealing with people within and outside of Compassion, and faithfully upholds Compassion’s ministry in prayer.Act as an advocate to raise the awareness of the needs of children. Understand Christ’s mandate to protect children. Commit to and prioritize child protection considerations in all decision-making, tasks and activities across the ministry. Abide by all behavioral expectations in Compassion’s Statement of Commitment to Child Protection and Code of Conduct. Report any concerns of abuse, neglect or exploitation of children through Compassion’s internal reporting process and appropriately support responses to incidents if they occur.Accountable for supporting, upholding, and engaging in Compassion's core ""Cultural Behaviors"" in all internal and external communication and relationships.Develop actionable stories that assist in data-informed decision-making in a wide variety of contexts.Depict data in a format understandable to non-technical individuals.Leverage various visualization tools to create dashboards and other data visualizations using data sets from various sources.Transform data into the most understandable format for the target audience.Works closely with subject matter experts to understand and align conclusions from various sources.Incorporates ministry strategy impacts to ensure products are relevant and actionable.Collaborate with partnership facilitators, national, regional, or global program support team members and leaders, to provide insight and expertise regarding intervention milestones analysis, financial report status, child development milestones, partner maturity progress and such other programmatic aspects. This leads to the realization of outcomes or to realization of global outcomes in line with Compassion's theory of change.May be called on to provide technical support for existing reports, dashboards, or other tools or systems, including design of related databases, spreadsheets, or outputs, as well as maintain a library of model documents, templates, or other reusable knowledge assets.
What do you bring? Bachelor's degree in engineering, data science, statistics, mathematics, economics, computer science, quantitative analysis, monitoring & evaluation, impact evaluation, or equivalent technical field.Three years of experience in engineering, business analytics, business intelligence, quantitative analysis, impact research and/or data science.Proficiency with data visualization tools to create insightful, impactful, and interactive dashboards. Experience in Tableau required.Strong oral and written communication with the ability to explain technical information in a simple way to non-technical audiences.Experience with SQL relational databases.Experience with Python or similar programming language to analyze, clean, manipulate and present data preferred.Multicultural experience desired.Fluency in reading, writing, and speaking English is required.
Equivalent education, training and/or certification may be substituted for experience and education shown above.Travel Requirements: May be required to travel domestically and internationally up to 20% of normal schedule. Travelers must follow the vaccination rule of each country they travel to. Some countries require the health record to be uploaded to their system to receive a QR code to present when arriving in immigration.Why work here? The mission: Join a team that is motivated to release children from poverty in Jesus’ name.Our benefits: Receive generous paid time off, 10% contribution to a 403(b) retirement fund, excellent healthcare coverage, and more. Spiritual growth: Participate in regular chapel services, prayer groups, and department devotionals."
Data Engineer,Motion Recruitment,"Washington, DC (Remote)",https://www.linkedin.com/jobs/view/3739119308/?eBP=JOB_SEARCH_ORGANIC&refId=yXH%2FaU%2FioH8YhIBIXRcvKg%3D%3D&trackingId=HZSt5fUblGbMqxnFS4LSIQ%3D%3D&trk=flagship3_search_srp_jobs,3739119308,"About the job
            
 
A provider of technical services for the federal government is seeking a Data Engineer to support their on-going projects. You would be supporting their relational database, automation, and geospatial work. You would be working within a team of data scientists, software developers, and agile to support federally funded projects.Responsibilities SQL Relational Databases ETL Python 
Nice-to-have ML/AI Geospatial Automation 
Applicants must be currently authorized to work in the US on a full-time basis now and in the future.Posted By: Derek Progin"
"Senior Data Engineer, Applied Machine Learning",WellSaid Labs,United States (Remote),https://www.linkedin.com/jobs/view/3744704574/?eBP=JOB_SEARCH_ORGANIC&refId=yXH%2FaU%2FioH8YhIBIXRcvKg%3D%3D&trackingId=156DGkfZdygYwlyxRjy0XA%3D%3D&trk=flagship3_search_srp_jobs,3744704574,"About the job
            
 
Who We Are: WellSaid Labs We’re creating Voice for everyone.At WellSaid Labs, we enable creatives around the globe by putting high-tech, human parity technology into their hands, giving them the ability to add voice-over to any project and iterate with ease. Creative teams use WellSaid Lab’s Voice Studio to create compelling employee training, design unique digital experiences, and narrate audiobooks. We believe deeply in AI for Good, and that technology should be empowering, engaging, and fair to all people.Who You Are: an Experienced Data Engineer working in Applied Machine LearningThe WellSaid Labs Applied Machine Learning Team works with stakeholders to identify, refine, and solve problems at the intersection of machine learning and customer needs. This team understands customer needs through quantitative and qualitative research and they work across WSL teams to understand how machine learning is utilized and how it can be improved.Improving and maintaining our ML solutions includes creating test datasets and metrics to define and gauge success, working with the ML Platform Team to prioritize model updates, training new models for deployment, coordinating releases, and educating the customer on any new capabilities. The Applied ML Team is consistently testing and reviewing any deployed models.As a Senior Data Engineer on the Applied ML Team at WellSaid Labs, you will be working to regularly improve our ML services, chiefly our text-to-speech service. You will own the strategy and development of projects pertaining to our testing frameworks, customer research, and model improvements, among others. As an individual contributor you will also add new datasets; train, deploy, and evaluate new models; and design experiments and algorithms for solving new and creative TTS challenges.You will build automated systems for evaluating ML performance: accuracy, consistency, and customer acceptance. You will summarize your findings into compelling reports and then collaborate with the Platform and Applied ML teams to build solutions. You should be familiar with Text-to-Speech technology, database querying, data labeling and preparing, crafting workflows for crowd-sourcing evaluations, and metrics reporting. It would be great if you have experience being a team leader and can advise the rest of the Applied ML team in ML-, data- and research-related questions and proposals.How You’ll ContributeIn your day-to-day, you will: Own, strategize, and manage efforts to evaluate and improve the model: gather customer insights, audit training data, and bridge gaps between model performance and customer expectationsWork directly with text and audio data: gathering, compiling, and organizing datasets, preparing data for machine training, evaluating results, debugging problematic dataTrain and deploy ML models: incorporating new data, monitoring training metrics, debugging failing code, deploying a model for customer useEvaluate ML models: consider causation or correlation between training data and ML predictions, design ML experiments and establish success criteria, gather and evaluate metrics including mean opinion scores; design evaluation tools for measuring pronunciation accuracy, naturalness, text normalization coverage, among others to minimize customer-model frictionAdditional research projects: interesting data or use cases, alternative services and solutions, internal process improvements, new quality evaluation exercises, etc.
In this role, you will need to take big ideas and build out specifications for achieving smaller milestones toward the ideal state. You will need to organize efforts into scrappy research, MVP executions, and long-term solutions. You should balance immediate results and technical debt with vision for ideal, automated systems. Additionally, this role requires you to write and review code that enables you, and others, to perform each of these tasks. It also requires you to think critically about Text-to-Speech tooling, customer experience, language, dialect, pronunciation, phonemics, and audio dynamics in order to build the highest quality voices and Studio/API experiences for our customers.What We’re Looking For To thrive in this role, you ideally have experience with and a solid understanding of ML concepts and best practices, a history of successfully managing datasets and metrics in a ML capacity, coding experience developing tools that can evaluate data and enable you and your team to establish recommendations based on data analysis results, and experience with software releases to production with strong considerations for both customer impact and ethical implementations of AI. You have worked in ML pipelines and have experience developing test suites, automating testing frameworks, and feeding data analysis back into model developmentYou have experience working with audio data or within the TTS domainYou have worked in a technical team at a high level, managing project expectations and communicating your plans, project statuses, and results frequently and comprehensivelyYou have worked with a wide array of data types, building analysis tools and establishing success criteria for evaluating the success of data-driven projectsYou have built and deployed ML models for use by a non-technical audience, clearly communicating usage guidelines and best practicesYou have experience building and documenting new processes, especially in a ML pipeline or similar capacityYou have a strong understanding of the importance of data preparation for ML training, data visualization and metrics for ML assessment, and analysis of ML resultsYou have familiarity with software and feature releases and can work closely with a Product team for exposing ML changes to customers[Bonus] You are fluent in Spanish or French[Bonus] You have studied Deep Learning and have applied models to solve technical challenges [Bonus] You have an interest in eventually managing an Applied ML Team, strategizing workload, and mentoring contributing engineers under your supervision
To Join Our Team You Must Also Be a U.S. Citizen or Permanent Residentpass a pre-employment background check 
What We Offer WSL is proud to support an inclusive work environment that emphasizes each team member’s personal and professional growth. Our team is fully distributed throughout the U.S., and we support flexible schedules - work where and when you work best. You’ll have teammates just a Slack message or video call away if you ever need help solving an exciting challenge, or even if you just have a funny story to tell.Other Perks And Benefits Competitive salary and stock optionsFull medical, dental, and vision insuranceMatching 401(k) planGenerous vacation policy/paid time offParental leaveLearning & development stipendHome office stipend
As a startup, we strive to be externally competitive with companies at a similar size and stage, and internally fair in our pay practices. The hiring salary range for this role is $130,000 - $160,000, and represents the target offer range given the scope and experience expectations for this role.What to Expect From Us We strongly encourage you to apply! If we feel your skills, experience, and values match, we’ll reach out about meeting with the team.During The Interview Stage, You Can Expect An introductory interview with the hiring manager (50 minutes); if there’s a match we’ll schedule an interview loop with the team.A technical screen, via a take-home assessment An Interview loop with 3-4 interviews (1 hour each) with the team members you will be potentially working with 
All interviews will be remote via Google Meets; we are happy to make accommodations you might need to feel comfortable and set up for success in our process.WellSaid Labs is honored to be an equal opportunity workplace. We realize that by bringing together teams rich in diverse thoughts and experiences, our people, company, and customers are free to flourish. We are committed to providing equal employment opportunities regardless of race, color, national origin, religion, creed, genetic information, sex (including pregnancy, sexual orientation or gender identity), age, marital status, disability, military or veteran status; or any other protected classifications or characteristics under applicable local laws."
Lead Data Engineer/ Sr. Data Engineer (13+ Years Minimum) -- Remote,Enexus Global Inc.,"California, United States (Remote)",https://www.linkedin.com/jobs/view/3750641625/?eBP=JOB_SEARCH_ORGANIC&refId=yXH%2FaU%2FioH8YhIBIXRcvKg%3D%3D&trackingId=tqTdLmgzzdl17sflmOJWag%3D%3D&trk=flagship3_search_srp_jobs,3750641625,"About the job
            
 
Role Lead Data Engineer / Senior Data (13+ Years minimum)Location RemoteContract Type W2/C2C/1099Experience 13+ Years minimumResponsibilities Develop and enhance data-processing, orchestration, monitoring, and more by leveraging popular open-source software, AWS, and GitLab automation.Collaborate with product and technology teams to design and validate the capabilities of the data platformIdentify, design, and implement process improvements: automating manual processes, optimizing for usability, re-designing for greater scalabilityProvide technical support and usage guidance to the users of our platform's services.Drive the creation and refinement of metrics, monitoring, and alerting mechanisms to give us the visibility we need into our production services.
Qualifications Experience building and optimizing data pipelines in a distributed environmentExperience supporting and working with cross-functional teamsProficiency working in Linux environment7+ years of advanced working knowledge of SQL, Python, and PySpark5+ years of experience with using a broad range of AWS technologiesExperience using tools such as: Git/Bitbucket, Jenkins/CodeBuild, CodePipelineExperience with platform monitoring and alerts tools"
Senior Data Engineer,brightwheel,United States (Remote),https://www.linkedin.com/jobs/view/3725571214/?eBP=JOB_SEARCH_ORGANIC&refId=yXH%2FaU%2FioH8YhIBIXRcvKg%3D%3D&trackingId=fcNYQnti63luMylvXTx1BQ%3D%3D&trk=flagship3_search_srp_jobs,3725571214,"About the job
            
 
Our Mission and OpportunityEarly education is one of the greatest determinants of childhood outcomes, is a must for working families, and has a lasting social and economic impact. Brightwheel’s vision is to enable high quality early education for every child — by giving teachers meaningfully more time with students each day, engaging parents in the development of their kids, and supporting the small businesses that make up the backbone of the $175 billion early education market. Brightwheel is the most loved technology brand in early education globally, trusted by thousands of educators and millions of families.Our TeamWe are a fully remote team with employees across every time zone in the US. Our team is passionate, talented, and customer-focused. Our exceptional investor group includes Addition, Bessemer Venture Partners, Chan Zuckerberg Initiative, GGV Capital, Lowercase Capital, Emerson Collective, and Mark Cuban.We believe that everyone—from our employees to the students, teachers, and administrators we serve— should be given the opportunity to learn and thrive, whatever their background may be. We celebrate diversity in all forms because it allows our team and the communities we serve to reach their full potential and do their best work.Who You AreBrightwheel is seeking a Senior Data Engineer to join the Data Engineering team.As a Senior Data Engineer at Brightwheel, you will play a key role in the implementation and evolution of our data platform. You will partner with technical leadership to craft and implement our data strategy. You will build and scale data pipelines that transform billions of records, across numerous systems, into measurable data that enable insights for our Analytics team.You are passionate about data engineering and possess deep technical skills. You have contributed to building data platforms from the ground up. You have experience juggling multiple projects with shifting priorities while continuing to deliver value to the business. You are a curious, detail oriented, self-starter who wants to take full ownership of high impact projects with visibility throughout the organization.What You’ll Do Use modern tooling to build robust, extensible, and performant data models in a cloud-based data warehouse that will drive business intelligence for the company Build extensible data acquisition and integration solutions to meet business requirements and reporting needsTroubleshoot, improve and scale existing data pipelines, models and solutionsBuild upon data engineering's CI/CD deployments, and infrastructure-as-code for provisioning AWS services
Required Qualifications, Skills, & Abilities 3+ years of work experience as a data engineer, coding in Python1+ years experience deploying data processing infrastructure into AWS / cloud environmentsAdvanced understanding of how at least one big data processing technology works under the hood (e.g. Spark / Hadoop / HDFS / Redshift / BigQuery / Snowflake / Parquet / Avro / Kinesis / Kafka)Experience with building ETL pipelines within Airflow / PythonExcellent analytical, problem solving, and troubleshooting skills to manage complex process and technology issues without much guidance
Preferred Experience 2+ years experience building data models with dbt in a cloud based data warehouse platformExperience with deploying Infrastructure as Code within a cloud environmentBuilding data ingestion from large scale transactional data storesHubspot / Salesforce / Mixpanel (Clickstream data analytics)Serverless / event driven architecture (Glue / Lambda)CubeJSParquet / Avro file storage
Brightwheel is committed to creating a diverse and inclusive work environment and is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity, gender expression, sexual orientation, national origin, genetics, disability, age, or veteran status.Brightwheel is committed to internal pay equity and offers a competitive compensation package, including base salary, equity, and benefits. Our benefits package includes premium medical, dental, and vision benefits, generous paid parental leave, a flexible paid time off policy, a monthly wellness and productivity stipend, and a Learning & Development stipend.For cash compensation, brightwheel sets standard ranges for all roles based on function, level, and geographic location, benchmarked against similar-stage growth companies. Multiple factors determine final offer amounts, including geographic location, candidate experience, and expertise. If you have questions about the compensation band for your region, please ask your recruiter."
Remote work - Need Data Engineer with Mulesoft API and c# experience,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3679879880/?eBP=JOB_SEARCH_ORGANIC&refId=yXH%2FaU%2FioH8YhIBIXRcvKg%3D%3D&trackingId=6axm1GtdYWO4jv6BJLMVYw%3D%3D&trk=flagship3_search_srp_jobs,3679879880,"About the job
            
 
Data governance specialist, that can build a MuleSoft API pipelines within azure DevOps to send applications/data to the cloud. A MuleSoft person who can build pipelines with Azure cloud.  Also Someone who can modify and configure with C# if needed.
C2H | | Data Engineer | Can be remote, best | 115-130k at conversion. Job Description: Preferred Office Location: Des MoinesWe are looking for a Data Engineer with experience in API Development and Data Governance tools like EDC. This team member will work with our business stakeholders and other data team members to create APIs to provide data to and from Data Governance tools like EDC and Profisee.Desired Skill Years of Experience 5+ years   Proficient in developing APIs - using Mulesoft, to retrieve data from and send to cloud based applicationsAble to understand and modify C# code mainly for administration and configuration of toolsExpertise in using and administering Data Governance tools like Axon, EDC, IDQ, MDMGood communication skills Written and VerbalPreferably in Des Moines, open for remote if the candidate is extremely good

Additional Preferred Experience Technical background/aptitude to learn now tools and technologies Experience working in Azure DevOpsFacilitating clear and effective communications across technical and non-technical individuals and teams at all levels of the organization."
Remote Opportunity: Sr. Data Engineer,SPAR Information Systems LLC,"Chevy Chase, MD (Remote)",https://www.linkedin.com/jobs/view/3742042565/?eBP=JOB_SEARCH_ORGANIC&refId=yXH%2FaU%2FioH8YhIBIXRcvKg%3D%3D&trackingId=sCXRPAWIpSCBEusFdsjCLQ%3D%3D&trk=flagship3_search_srp_jobs,3742042565,"About the job
            
 
Hello All, Hope you are doing great. Please go through the job description and let me know your interest. Role: Sr. Data Engineer Locations: Remote Duration: 3 Months Contract to hire Requirement: Client treats Data as Product and our Senior Engineer will be a key member of the engineering staff working across Business Services Engineering, Data Engineering, Platform Engineering, and Infrastructure Engineering to ensure that we provide a fiction-less experience to our customers, maintain the highest standards of protection and availability. Our team thrives and succeeds in supporting Data Driven company and delivering high quality technology products and services in a hyper-growth environment where priorities shift quickly. The ideal candidate has broad and deep technical knowledge in data, typically ranging from front-end UIs through back-end systems and all points in between. Required: 3+ years of experience in data software development, programming languages and developing with big data technologies 2+ years of experience designing and building on existing and new data applications 2+ years of experience in Cloud DevOps concepts, Cloud Services and Architecture, and Azure/AWS/GCP DevOps Operational Framework 1+ years of experience in open-source data tools and frameworks, or one of the following: .net Core, asp.Net, Angular, or Express. Additional Skills: Experience in data software development, using data technologies such as Relational & NoSQL databases, open data formats, and programming languages such as Python, Scala, and/or other frameworks, building data pipelines (ETL and ELT) with batch or streaming ingestion, error handling, loading, and transforming data, and developing with big data technologies such as Spark, Hadoop, and MapReduce. Experience with analytics solutions. Experience in development using Python or PySpark, Spark, Scala. Advanced understanding of designing and building for data quality assurance, reliability, availability, and scalability, on existing and new data applications. Advanced understanding of DevOps Concepts, Cloud Architecture, and Azure DevOps Operational Framework, Pipelines, Kubernetes. Advanced understanding of designing and building solutions for data quality and observability, metadata management, data lineage, and data discovery. Advanced understanding of building products of micro-services oriented architecture and extensible REST APIs. Advanced understanding of open-source frameworks. Experience with continuous delivery and infrastructure as code. Experience in existing Monitoring Portals: Splunk or Application Insights. Advanced understanding of Security Protocols & Products: Understanding of Active Directory, Windows Authentication, SAML, OAuth. Advanced understanding of Azure Network (Subscription, Security zoning, etc) & tools like Genesis. Advanced understanding of existing Operational Portals such as Azure Portal. Knowledge of CS data structures and algorithms. Knowledge of developer tooling across the software development life cycle (task management, source code, building, deployment, operations, real-time communication). Practical knowledge of working in an Agile environment (Scrum/Kanban/SAFe). Strong problem-solving ability. Ability to excel in a fast-paced, startup-like environment. Bachelor's degree in Computer Science, Information Systems, or equivalent education or work experience Thanks & Regards, Satnam Singh Direct: 201 623 3660 Email : Satnam.singh@sparinfosys.com"
Data Engineer,"Vision Government Solutions, Inc.",United States (Remote),https://www.linkedin.com/jobs/view/3770007243/?eBP=JOB_SEARCH_ORGANIC&refId=yXH%2FaU%2FioH8YhIBIXRcvKg%3D%3D&trackingId=GkqYcDw04xlbA0tbxLrGZg%3D%3D&trk=flagship3_search_srp_jobs,3770007243,"About the job
            
 
About VisionVision Government Solutions is a leading government technology firm providing cutting-edge software to the public sector. We are at an incredible inflection point of growth and are looking for exceptional individuals to join our Software Implementation team to help us successfully welcome new communities to our Vision software.Our software implementation philosophy emphasizes the importance of customer delight, speed, and long-term partnership. To that end, we are searching for ambitious, motivated, detail-oriented individuals looking to further a career in Implementation Engineering. The right candidate will be driven by customer happiness, be obsessed with continuous improvement, and have strong data engineering and ETL toolkit to master the vast landscape of unique implementation projects we encounter.Summary Of Role & ResponsibilitiesThe Data Engineer (Software Implementation) will primarily be responsible for writing and deploying custom data conversions, working in concert with technical project managers and architects. Sample responsibilities include:Data Engineering Deliver custom database conversions during new customer implementation projectsMigrate data from competitor solutions to our in-house softwareUpgrade existing customers from our previous Oracle/VB6 software version to our current SQL Server/C#/.Net productAssist with the development of ad hoc T-SQL scripts for occasional data engineering needsCollaborate with the Continuous Improvement team to streamline future processes, automate repeatable tasks, improve quality assurance, and minimize reworkDevelop custom routines to convert graphical building drawings from various formats, including (but not limited to) Traverse, SVG, AutoCAD DXF, and binary, to our proprietary Sketch XML formatCreate and alter client analytics dashboards using PowerBIEmbrace agile methodologies to achieve industry-leading project delivery schedules while maintaining ongoing customer engagement and excitementPartner with project teams to provide a seamless customer transition experience
Role EvolutionThis role will begin with a primary focus on municipal assessment software implementation projects (~90% of the time), with the remaining 10% focused on continuous improvement initiatives and team growth. In addition, role evolution may include calibration and modeling efforts, branching into our SaaS product world, advanced data engineering opportunities, and exciting continuous improvement projects.Who We Are Looking ForThe ideal person for this role will have demonstrated the following abilities and traits:Skills And Ambitions Ability to thrive in a fast-paced, ever-changing landscape5 + years of experience working in a SQL-focused conversion roleSolid knowledge of T-SQL and SQL Server Management StudioFamiliarity with Oracle SQL*Plus is desired but not requiredExperience with ETL and Data MigrationUnderstanding of relational database conceptsImpressive oral and written communication skillsExceptional time management capabilities; deadlines are not flexibleAbility to communicate and translate ideas between technical and non-technical partiesSQL Server 2012+/SSRS/SSIS/Jira/GitHub/ConfluenceSkills using Apache NiFi, R, or Python are a plus but not requiredBS/BA in Computer Science or related field preferred
Total Compensation Package: To be discussedBenefits Package: Vision offers health, dental, and vision plans, as well as a 401(k)-matching program.Work Location: RemoteEqual Employment OpportunityVision Government Solutions is an Equal Opportunity Employer and committed to a diverse and inclusive workplace. All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.We're proud to be an equal opportunity employer and celebrate our employees' differences, including race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability and Veteran status.Vision Government Solutions maintains a drug-free workplace."
Senior Data Engineer,Veda,United States (Remote),https://www.linkedin.com/jobs/view/3766989452/?eBP=JOB_SEARCH_ORGANIC&refId=yXH%2FaU%2FioH8YhIBIXRcvKg%3D%3D&trackingId=m0GlqPymUI5WJVke4ohyvg%3D%3D&trk=flagship3_search_srp_jobs,3766989452,"About the job
            
 
Veda helps patients get the care they need by untangling complex data management problems using advanced scientific approaches and in-depth collaboration. Our technology reflects what our people provide: quality without ego, honesty backed by science, and warmth in an industry not known for having much heat.Veda is made up of talented professionals that are driven to do meaningful work to change healthcare from the inside out. We are also friends, parents, partners and caregivers. Veda’s benefits reflect our values—we offer fully paid, low or no-deductible medical, dental and vision insurance for our employees and their families. We ensure that employees can take time off to recharge and have flexibility to care for themselves and their families.Veda is looking for sharp-minded do-gooders who share our values:Collaboration Working together to identify solutions to current problemsOpenness Actively listening, sharing and holding space for new ideas, perspectives and peopleIntegrity Doing the right thing, honestly and transparently.Grit Displaying passion and perseverance to achieve our goals.Ready to build the future with us?This Data Engineer will report to the Technical Engineering Manager for the Quantym team. You will be a trusted contributor to the design of our data pipeline and application architecture, keeping security, cost, & performance in mind. You will work within a cross-disciplinary team of data engineers, front end designers, data scientists, and cloud architects throughout all stages of development - from brainstorming to coding to ongoing support.As a Data Engineer on the team, you will help move our software development practices forward by guiding others on design and best practices. You'll mentor other engineers, conduct code reviews, and help write technical requirements across product-driven teams. Most importantly, you're excited to be an integral part of a team that values quick iteration, embraces new tech, and relies on data-driven decision making to deliver value for customers and the firm at large to move our products forward.About You: You are Self-motivated, driven to continue learningYou are proficient in shipping production Python and SQL codeYou have contributed to hiring, interviewing and training peer and junior engineersYou recognize what it takes to add value as a member of high-performing engineering teams. You can identify key success factors, inspire a culture of learning and knowledge sharing, and proactively intervene when goals are not metYou have a background in product development, and a love for wrangling data and delivering solutions to business problems on a fast-paced teamYou have the most fun when you get to share ideas and solve complex problems by applying excellent organizational skills and collaborative valuesYou’re comfortable working throughout the Software Development Lifecycle: refining requirements, designing solutions, testing changes, and delivering in collaboration with Engineering Management, Product, and Customer-facing teams
Required Qualifications: 4+ years of experience shipping production python code4+ years of professional experience working in a developer role (devops or similar) that heavily involves high-frequency, customer-related development. 
Including but not limited to: Database design and data modeling experience including performance optimizationExperience working in report development tools like tableau, powerBIExperience working in a variety of data stores (sql and no-sql) and file formats. Writing automated testsusing infrastructure as code, CICD pipelines, containerization technologiesSupporting, designing, building, and maintaining high volume/high frequency data pipelines for large scale, complex data setsWorking with event-driven and/or microservice ecosystemsMaintaining, expanding and evolving legacy code and frameworks
Preferred Qualifications/ Desired Experience: Designing applications that make use of AWS services including Lambda, S3, Glue, RDS, DynamoDB, ECS, EMR, Data Lake FormationETL and orchestration engines including Spark, airflow, InformaticaBuilding production applications with Javascript or TypescriptPython data science libraries like Pandas, NumPy, Matplotlib; familiarity with these is appreciated but the focus of this work is backend, not data science/model developmentWorking in highly regulated industries like finance, health care or defense
Our COVID Commitment: Veda is committed to prioritizing the health, safety and emotional well being of our employees and their families. Veda has always embraced the benefit of each employee working remotely, collaboratively.All employees are required to be located within the USA.We look forward to learning more about you -- apply to join the Veda team today!"
Data Visualization/D3/JavaScript Engineer,IVY TECH SOLUTIONS INC,"Indianapolis, IN (Remote)",https://www.linkedin.com/jobs/view/3667176447/?eBP=JOB_SEARCH_ORGANIC&refId=yXH%2FaU%2FioH8YhIBIXRcvKg%3D%3D&trackingId=cm0sTQwJHz8rkLOJVGchWw%3D%3D&trk=flagship3_search_srp_jobs,3667176447,"About the job
            
 
Data Visualization/D3/JavaScript Engineer | Quick SummaryYou are passionate about telling stories with data and can drive the development process from the initial prototype through the implementation of the final product. You will design and develop data-driven, dynamic user-facing interactive dashboard and visualization tools with high-performing responsiveness, usability, and visual appeal. You will implement solutions using JavaScript-based data visualization libraries and tools, primarily D3js (Experience with additional JS libraries such as React.js, Vue.js, Angular.js, Node.js, Bone.js, CartoDb.js would be helpful!) Having JMP, Tableau or Spotfire is a HUGE BONUS!JavaScript Engineer | Desired Skills & Experience 5+ years of JavaScript developmentIn-depth knowledge of JavaScript with extensive experience with D3.js required!Experience with Vue.js and/or React.js are preferredExperience implementing web services (SOAP and RESTful)Experience connecting front-end interfaces with SQL or NoSQLFluent in HTML5, CSS, Bootstrap, jQueryFluency in other complementary JavaScript-based tools and frameworks or scripting language Familiarity with UX design and image editing toolsProficient understanding of code versioning toolsStrong knowledge of computer science fundamentals and coding best practicesAn understanding of Agile/Scrum development and a collaborative, proactive attitudeEnterprise experience, healthcare, pharma, biotechnology experience is a bonusTableau or other Data Visualization tool experience is a bonus!
Powered by JazzHR4kDYknqLos"
Cloud Data Engineer,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3742030826/?eBP=JOB_SEARCH_ORGANIC&refId=yXH%2FaU%2FioH8YhIBIXRcvKg%3D%3D&trackingId=R8iP8GOy76a09dBYTSb2lg%3D%3D&trk=flagship3_search_srp_jobs,3742030826,"About the job
            
 
Remote roleW2 candidates onlyNeed valid LinkedInDescriptionThe Senior Cloud Data Engineer will work directly with our product lead on multiple algorithmic data science products. design, code, test, and analyze software programs and applications. This includes researching, designing, documenting, and modifying software specifications throughout the production lifecycle. This role will also create business critical reports, analyze, and amend software errors in a timely and accurate fashion and provide status reports where required. The position responsibilities outlined below are not all encompassing. Other duties, responsibilities, and qualifications may be required and/or assigned as necessary.Responsibilities  Work with Product team to determine requirements and propose approaches to address users' needs Analyze requirements to determine approach/proposed solution Design and build solutions using relevant programming languages Thoroughly test solutions using relevant approaches and tools Conduct research into software-related issues and products Bring out-of-box thinking and solutions to address challenging issues Effectively prioritize and execute tasks in a fast-paced environment Work both independently and in a team-oriented, collaborative environment Flexible and adaptable to learning and understanding new technologies Highly self-motivated and directed Demonstrate a commitment to Hyatt core values
Experience And Skills  Experience in designing, developing, and maintaining data pipelines on AWS cloud platform Experience in developing solutions using Snowflake database Hands-on software troubleshooting experience Proven analytical and problem-solving abilities Experience with every phase of the software development life cycle including requirements gathering, requirements analysis, design, development, and testing Work closely with the product leads to identify and prioritize data needs for the algorithmic data science products Collaborate with cross-functional teams to ensure data quality and accuracy Continuously monitor and optimize data pipeline performance Stay up-to-date with the latest technologies and industry trends in data engineering
Must Have Skills  Hands on experience with AWS cloud architecture and development data pipelines using S3, Redshift, Athena, DynamoDB, Lambda, Glue, EMR, Kinesis, API Gateway, and other AWS technologies Hands on experience is building and monitoring CloudWatch alarms Hands on experience with AWS CICD suite (Code commit, Code pipeline, Cloud formation) Hands on experience is using Python (intermediate/expert level) Hands on experience using Spark (strongly preferred) Hands on experience building solutions using Snowflake database Hands on experience in trouble shooting complex SQL problems Experience working with cross-functional teams in a fast-paced, dynamic environment Strong problem-solving skills and attention to detail Good communication and collaboration skills
Good To Have Skills  Hands on experience building visualizations/reports Good understanding of data modeling"
Big Data Engineer,AAA GLOBAL TECHNOLOGIES LLC,"Atlanta, GA (Remote)",https://www.linkedin.com/jobs/view/3776604338/?eBP=JOB_SEARCH_ORGANIC&refId=yXH%2FaU%2FioH8YhIBIXRcvKg%3D%3D&trackingId=BnkcF80ynIFeNOLSyfXNOg%3D%3D&trk=flagship3_search_srp_jobs,3776604338,"About the job
            
 
Responsibilities1. Must have hands-on experience with Databricks 2. Must have hands-on experience with high-velocity high-volume stream processing: Apache Kafka and Spark Streaming a. Experience with real-time data processing and streaming techniques using Spark structured streaming and Kafka b. Deep knowledge of troubleshooting and tuning Spark applications 3. Must have hands-on experience with Python and/or Scala i.e. PySpark/Scala-Spark 4. Must have experience with Databricks 5. Must have hands-on experience building, testing, and optimizing ‘Big Data’ data ingestion pipelines, architectures and data sets 6. Experience in successfully building and deploying a new data platform on Azure/ AWS 7. Experience in Azure / AWS Serverless technologies, like, S3, Kinesis/MSK, lambda, and Glue 8. Strong knowledge of Messaging Platforms like Kafka, Amazon MSK & TIBCO EMS or IBM MQ Series 9. Experience with Databricks UI, Managing Databricks Notebooks, Delta Lake with Python, Delta Lake with Spark SQL, Delta Live Tables, Unity Catalog 10. Experience with data ingestion of different file formats across like JSON, XML, CSV 11. Experience with NoSQL databases, including HBASE and/or Cassandra 12. Knowledge of Unix/Linux platform and shell scripting is a must 13. Experience with Cloud platforms e.g. AWS, GCP, etc. Experience with database solutions like Kudu/Impala, or Delta Lake or Snowflake or Big QueryQualificationsBachelor’s Degree required. Preferably in Information Systems, Computer Science, Computer Information Systems or related field"
Data Migration Engineer,"MetroSys, Inc.",NAMER (Remote),https://www.linkedin.com/jobs/view/3744161006/?eBP=JOB_SEARCH_ORGANIC&refId=yXH%2FaU%2FioH8YhIBIXRcvKg%3D%3D&trackingId=WElfKxwYE1L%2Bld4ly%2F5J3w%3D%3D&trk=flagship3_search_srp_jobs,3744161006,"About the job
            
 
Responsibilities: Collaborate with stakeholders to understand data migration requirements, including scope, timelines, and specific data sets.Design and develop a comprehensive data migration plan, considering factors such as data volume, complexity, and business continuity.Utilize Komprise data management software to perform assessments, identify target data, and orchestrate the migration process.Configure and optimize Komprise settings to align with migration goals, ensuring efficient and secure data transfer.Conduct pre-migration validation tests to ensure data integrity and accuracy before the actual migration process.Monitor the data migration process in real-time, addressing any issues or discrepancies as they arise.Implement data validation and reconciliation procedures to verify the successful completion of the migration.Collaborate with cross-functional teams, including storage administrators and system engineers, to ensure seamless integration with the NetApp environment.Document the entire migration process, including configurations, settings, and any custom scripts or workflows used.Provide knowledge transfer and training to internal teams for ongoing management and maintenance of the migrated data.
Requirements: Bachelor's degree in Information Technology, Computer Science, or a related field (preferred) or equivalent work experience.Proven work experience as a Data Migration Engineer with specific expertise in migrating data from Isilon to NetApp using Komprise.Strong proficiency in Komprise data management software and related tools.In-depth knowledge of Isilon and NetApp storage platforms, including file systems, protocols, and administration.Experience with scripting languages (e.g., Python, PowerShell) for automation and customization of migration processes.Excellent problem-solving and analytical skills, with the ability to diagnose and resolve complex data migration issues.Strong communication and interpersonal skills, with the ability to collaborate effectively with technical and non-technical stakeholders.
Powered by JazzHRT7ZuUtppMf"
Lead Big Data Engineer (Remote),Syrinx Consulting,"Massachusetts, United States (Remote)",https://www.linkedin.com/jobs/view/3648836284/?eBP=JOB_SEARCH_ORGANIC&refId=yXH%2FaU%2FioH8YhIBIXRcvKg%3D%3D&trackingId=5%2F7VdIzJCICRpk29sEp%2B9g%3D%3D&trk=flagship3_search_srp_jobs,3648836284,"About the job
            
 
As a Lead Software Engineer, you will be directly responsible for many of the innovative features we’ll be working on, whether collaborating with our outstanding design and product team or helping our data science group work on the bleeding-edge technology to personalize technology for our ever expanding population of members.In This Role You Will Align and drive team members towards an inspiring vision, yet an internship to deliver value incrementallyBuild highly scalable software to ensure data quality and reliability of our microservices architecture, leveraging Scala and technologies like HBase, Spark, etc.As a hands-on lead, you will work closely with our team of software engineers and product managers to deliver rapid valueMentor and coach technical team members
A PERFECT CANDIDATE HAS: Experience with designing and building large scale data pipelines and data warehousesThrived in complex microservices ecosystem and written robust and well-performing servicesThe ability to bring technology to the tableExcelled in cross-functional teams, working fluidly with Product Managers, Data Scientists, Mobile Engineers, Backend Engineers, and other highly skilled specialistsExperience leading an Agile Development TeamExperience mentoring and leading people at different stages in their career"
"Data Engineer, Azure-based Health Data System (Remote)",WorkatHome-JobBoard,"Reston, VA (Remote)",https://www.linkedin.com/jobs/view/3778123388/?eBP=JOB_SEARCH_ORGANIC&refId=yXH%2FaU%2FioH8YhIBIXRcvKg%3D%3D&trackingId=rCGKRVwlzSeoIRGXNAVRxQ%3D%3D&trk=flagship3_search_srp_jobs,3778123388,"About the job
            
 
We are open to supporting 100% remote work anywhere within the US.Our IT Modernization division is an information technology and management consulting department that offers integrated, strategic solutions to its public and private-sector clients. ICF has the expertise, agility, and commitment to design, build, and operate high-performance IT engines to support all aspects of our client's business.Job DescriptionThe Data Engineer will use a variety of full-stack software languages and tools to build a data processing system for health care data in a major Health & Sciences agency. The system will be built on and Azure infrastructure and will integrate data from both internal and external data sources. The Data Engineer will create new pipelines and build reusable components at scale to support reporting & analytics data products.Based on your experiences and interests, we may ask you as a technology professional to support growth-related activities, including (but not limited to) RFI, RFP, prototypes, and oral presentations. Team members are also expected to uphold and maintain appropriate certifications necessary for their practice expertise.You WillWrite complex queries to transform raw data sources into accessible models by coding.Clean, prepare, transform, and optimize data at scale for integration and consumption.Implement data management projects and restructure current web architecture.Solve complex data issues and perform root cause analysis to proactively resolve product issues.Own the data pipeline and support systems failures.RequirementsMust have previous experience working with any CDC Center, Institute, or Office or specific CDC application(s) that are in scope for modernization3+ years of experience programming with Java and/or Kotlin3+ years of experience working with Azure cloud native technologies2+ years of experience building a pipeline for Public health/health care data (HL7, FHIR, vocabulary, and HHS data standards)2+ years experience with developing ETL pipelines in Azure using tools such as Azure Data factory, Event hubs, Event grid, Azure functions,2+ years developing data engineering pipeline with Databricks or Spark2+ years of experience working in Agile teamsFamiliar with devOps process, CI/CD, security, coding standard and SDLC cyclePreferred1+ year working with technologies such as CosmoDB, AzureSQL, Redis, Synapse1+ year working with orchestration tool such as Airflow or OozieExperience working with streaming and batching solutionsExperience working with csv, json, xml and complex structuresWorking at ICFICF is a global advisory and technology services provider, but we're not your typical consultants. We combine unmatched expertise with cutting-edge technology to help clients solve their most complex challenges, navigate change, and shape the future.We can only solve the world's toughest challenges by building an inclusive workplace that allows everyone to thrive. We are an equal opportunity employer, committed to hiring regardless of any protected characteristic, such as race, ethnicity, national origin, color, sex, gender identity/expression, sexual orientation, religion, age, disability status, or military/veteran status. Together, our employees are empowered to share their expertise and collaborate with others to achieve personal and professional goals. For more information, please read our EEO & AA policy.Reasonable Accommodations are available, including, but not limited to, for disabled veterans, individuals with disabilities, and individuals with sincerely held religious beliefs, in all phases of the application and employment process. To request an accommodation please email and we will be happy to assist. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. Read more about non-discrimination: Know Your Rights and Pay Transparency Statement.Pay Range - There are multiple factors that are considered in determining final pay for a position, including, but not limited to, relevant work experience, skills, certifications and competencies that align to the specified role, geographic location, education and certifications as well as contract provisions regarding labor categories that are specific to the position. The pay range for this position is:$82,673.00 - $140,544.00Nationwide Remote Office (US99)"
GCP Data Engineer,TekIntegral,United States (Remote),https://www.linkedin.com/jobs/view/3663404911/?eBP=JOB_SEARCH_ORGANIC&refId=yXH%2FaU%2FioH8YhIBIXRcvKg%3D%3D&trackingId=0t4vQHLWGTKH6dY75LVOew%3D%3D&trk=flagship3_search_srp_jobs,3663404911,"About the job
            
 
Title: GCP Data EngineerLocation: Initial 6 months Remote then Dallas, Texas or Bentonville, AR (Local candidates of Texas or Arkansas)Duration: Long term contractWork Auth: No H1/ CPTLinkedIn: YesIV Process: 3 rounds (They do move quickly)Client: RetailMust Haves: Hive, Spark, Scala, PySpark, Kafka, Apache AirflowJob DescriptionOne of our largest national retail clients is looking for a Data Engineer to join their growing Data Ventures Organization. The Data Ventures team is a business within this enterprise level company focusing on maintaining and developing features on a new data platform sold to its vendors. More specifically, you will be joining their channel performance team responsible for writing data pipelines in Spark and Scala. 80% of your day to day will be heads down in the data. The other 20% will be spent gathering requirements and stakeholder management.Minimum Requirements 6-7 years of overall experience3+ years' experience in Data EngineeringProgramming with Scala/SparkWriting advance SQL queriesBig QueryCloud experience with GCP
Desired Skills Hadoop HiveGCPBig queryUsing Airflow as a scheduling toolPythonDruidAzure"
Data Systems Engineer,CyberCoders,"Baton Rouge, LA (Remote)",https://www.linkedin.com/jobs/view/3779630508/?eBP=JOB_SEARCH_ORGANIC&refId=yXH%2FaU%2FioH8YhIBIXRcvKg%3D%3D&trackingId=1EljTH6%2FQ1wQerieq9ck4A%3D%3D&trk=flagship3_search_srp_jobs,3779630508,"About the job
            
 
Position: Data Systems EngineerLocation:  FULLY REMOTE - Offices are in Houston, TX or Baton Rouge, LASalary:  $75-95k/year DOE + BenefitsRequirements:  Data Engineering, SQL queries, ETL, .NET, T-SQLWe are an industry leader in the oil & energy space manufacturing quality IoT products & accompanying SaaS products. Since 1959, we've been providing customers with integrated solutions & services that make a huge impact on the efficiency and reliability of their industrial systems. Our reputation for excellence has allowed the company to expand to over 12 locations with over 500 employees!We are seeking an experienced Data Systems Engineer to assist the Applications System Manager in developing solutions to support our business analytics and data processing needs.What You Need for this PositionMust Have Skills  Experience developing SQL queries & ETL processes T-SQL .NET, SSRS, SSIS, Excel Visual Studio, SQL Management Studio
Nice To Have Skills  Oracle PL/SQL
What You Will Be Doing  Assist in design, development, and maintenance of data lake\warehouse Work with product owners to understand and implement reports and data extracts as well as data sources for visualization tools, like Tableau. Ensure data quality amongst various data systems Deliver project/program objectives on-time, with quality, within scope
What's In It for You  Competitive base salary Comprehensive benefits package Great culture!
So, if you are a Data Systems Engineer, please apply today!Colorado employees will receive paid sick leave. For additional information about available benefits, please contact Samuel LeosEmail Your Resume In Word ToLooking forward to receiving your resume through our website and going over the position with you. Clicking apply is the best way to apply, but you may also:Samuel.Leos@CyberCoders.com Please do NOT change the email subject line in any way. You must keep the JobID: linkedin : SL5-1777409 -- in the email subject line for your application to be considered.***
Samuel Leos - Recruiting Manager - CyberCodersApplicants must be authorized to work in the U.S.CyberCoders is proud to be an Equal Opportunity EmployerAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any other characteristic protected by law.Your Right to Work – In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire."
BI Data Engineer - REMOTE,Motion Recruitment,"Rolling Meadows, IL (Remote)",https://www.linkedin.com/jobs/view/3762299212/?eBP=JOB_SEARCH_ORGANIC&refId=yXH%2FaU%2FioH8YhIBIXRcvKg%3D%3D&trackingId=XKYs4z9VncYOwile8VlYDQ%3D%3D&trk=flagship3_search_srp_jobs,3762299212,"About the job
            
 
Motion has partnered with a premier client in filling a full-time, fully REMOTE employee position for a BI Data Engineer. This is a great opportunity to expand your career and work with a well-known company in the greater Chicago area. Do you get excited working on Azure cloud platforms specifically ingesting data using Azure Data Factory (ADF)? Are you experienced with Snowflake, Databricks, SQL, Python, within the enterprise data warehouse environments? This position may be for you.Required Skills & Experience A relevant technical BS Degree in Information Technology and 5 years of relevant professional experience implementing well-architected data pipelines that are dynamically scalable, highly available, fault-tolerant, and reliable for analytics and platform solutions 3+ years of data engineering experience leveraging technologies such as Snowflake, Azure Data Factory, ADLS Gen 2, Logic Apps, Azure Functions, Databricks, Apache Spark, Scala, Synapse, SQL Server Understanding the pros and cons, and best practices of implementing Data Lake, using Microsoft Azure Data Lake Storage Experience structuring Data Lake for the reliability, security and performance 5 years writing SQL, TSQL queries against any RDBMS with query optimization and performance tuning Experience implementing ETL for Data Warehouse and Business intelligence solutions Working experience with Python, and Power Shell Scripting Skills to read and write effective, modular, dynamic, parameterized and robust code, establish and follow already established code standards, and ETL framework Strong analytical, problem solving, and troubleshooting abilities, experience performing root cause analysis Good understanding of unit testing, software change management, and software release management Experience working within an agile team, In-depth knowledge of agile process and principles 
What You Will Be Doing Build the infrastructure required for optimal ETL/ELT pipelines to ingest data from a wide variety of data sources using Microsoft Azure technologies such as Azure Data Factory and Databricks. Construct and maintain of enterprise level integrations using the Snowflake platform, Azure Synapse, Azure SQL and SQL Server. Design ETL pipelines and reusable components to implement specified business requirements Troubleshoot and optimize ETL code; interpret ETL logs, perform data validation, understand the benefits and drawbacks of parallelism, proper use of expressions, scoping of variables, commonly used transforms, event handlers and logging providers, understand and optimize the surrogate key generation and inconsistent data type handling Create data tools for data analytics and data science team members to deliver actionable insights into customer acquisition, operational efficiency and other key business performance metrics. Conduct code reviews, performance analysis and participate in technical design Orchestrate large, complex data sets that meet functional/non-functional business requirements. Seek out, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability. Partner with data and analytics talent to strive for greater functionality in our data systems. 
Applicants must be currently authorized to work in the US on a full-time basis now and in the future.Posted By: Aaron Rontal"
Senior Software / Data Engineer,DataTribe,"Columbia, MD (Remote)",https://www.linkedin.com/jobs/view/3667158755/?eBP=JOB_SEARCH_ORGANIC&refId=yXH%2FaU%2FioH8YhIBIXRcvKg%3D%3D&trackingId=Cdo8qm%2B4%2FprbtwT2emQVyA%3D%3D&trk=flagship3_search_srp_jobs,3667158755,"About the job
            
 
Do you want to help build the next generation network security planning solution?Company Overview: Sixmap is working on leading edge network intrusion detection technology that enables enterprises and network operators to gain insights into their complete network attack surface and identify network vulnerabilities at unheard of speed and comprehensiveness. Sixmap’s platform can complete IPv4 scans with deep and configurable service interrogation that is orders of magnitude faster than anything currently available. The team is building the world’s first platform to perform comprehensive IPv6 scans, previously thought to be impossible.Position Summary: We are looking for a data-oriented senior software engineer to help build the core network mapping and interrogation engine. Candidates should have deep hands-on experience working on data pipelines, ETL, data analysis processes, and database technologies in addition to a solid understanding of TCP/IP networking. The ideal candidate should be a well-rounded developer but be particularly strong in backend business-logic-oriented software development. Come join us, if you are ready to change the world of network security while having some fun along the way.Position Requirements:To be considered for this position, you must: Be a development athlete with at least 5 years’ experience and have a passion in understanding users’ needs and system requirements and turning them into working softwareHave a BS degree or higher in computer science, electrical/computer engineering, or related technical fieldBe fully fluent in Python and common data analysis Python libraries, C++, SQL, Airflow or other ETL / data pipeline tools, and preferably be a polyglot comfortable in many additional programming languages.Be an expert in using relational databases and NoSQL data stores - PostgreSQL experience is a must.Be experienced with Linux environments.Have experience working on container-based cloud infrastructure frameworks such as Docker or Kubernetes within common cloud service providers such as AWS, GCP, or Azure.Be experienced using Agile methodologies, operating cloud dev-ops, and coordinating with product development teamsHave the ability to thrive when presented a complex challenge in a fast-paced, performance-oriented culture with intelligent peopleHave exceptional level of integrity, raw intelligence, creativity, energy and passionOperate efficiently with individual responsibility in a highly collaborative environment 
Powered by JazzHR4lnILZIcTW"
"Data Engineer (AWS, Azure, GCP)",CapTech,"Columbus, OH (Remote)",https://www.linkedin.com/jobs/view/3774196805/?eBP=JOB_SEARCH_ORGANIC&refId=yXH%2FaU%2FioH8YhIBIXRcvKg%3D%3D&trackingId=J5OrbtA%2Be0XEjZkMEIaL%2FA%3D%3D&trk=flagship3_search_srp_jobs,3774196805,"About the job
            
 
Company DescriptionCapTech is an award-winning consulting firm that collaborates with clients to achieve what’s possible through the power of technology. At CapTech, we’re passionate about the work we do and the results we achieve for our clients. From the outset, our founders shared a collective passion to create a consultancy centered on strong relationships that would stand the test of time. Today we work alongside clients that include Fortune 100 companies, mid-sized enterprises, and government agencies, a list that spans across the country.Job DescriptionCapTech Data Engineering consultants enable clients to build and maintain advanced data systems that bring together data from disparate sources in order to enable decision-makers. We build pipelines and prepare data for use by data scientists, data analysts, and other data systems. We love solving problems and providing creative solutions for our clients. Cloud Data Engineers leverage the client’s cloud infrastructure to deliver this value today and to scale for the future. We enjoy a collaborative environment and have many opportunities to learn from and share knowledge with other developers, architects, and our clients.Specific responsibilities for the Data Engineer – Cloud position include:  Developing data pipelines and other data products using Amazon Web Services (AWS), Microsoft Azure, or Google Cloud Platform (GCP) Advising clients on specific technologies and methodologies for utilizing cloud resources to efficiently ingest and process data quickly Utilizing your skills in engineering best practices to solve complex data problems Collaborating with end users, development staff, and business analysts to ensure that prospective data architecture plans maximize the value of client data across the organization. Articulating architectural differences between solution methods and the advantages/disadvantages of each 
QualificationsTypical experience for successful candidates includes:  Experience delivering solutions on a major cloud platform Ability to think strategically and relate architectural decisions/recommendations to business needs and client culture Experience in the design and implementation of data architecture solutions A wide range of production database experience, usually including substantial SQL expertise, database administration, and scripting data pipelines Ability to assess and utilize traditional and modern architectural components required based on business needs. A demonstrable ability to deliver production data pipelines and other data products. This could be hands on experience, degree, certification, bootcamp, or other learning. 
SkillsSuccessful candidates usually have demonstrable experience with technologies in some of these categories: Languages: SQL, Python, Java, R, C# / C++ / C Database: SQL Server, PostgreSQL, Snowflake, Redshift, Aurora, Presto, BigQuery, Oracle DevOps: git, docker, subversion, Kubernetes, Jenkins Additional Technologies: Spark, Databricks, Kafka, Kinesis, Hadoop, Lambda, EMR Popular Certifications: AWS Cloud Practitioner, Microsoft Azure Data Fundamentals, Google Associate Cloud Engineer
Additional InformationWe want everyone at CapTech to be able to envision a lasting and rewarding career here, which is why we offer a variety of career paths based on your skills and passions. You decide where and how you want to develop, and we help get you there with customizable career progression and a comprehensive benefits package to support you along the way. Alongside our suite of traditional benefits encompassing generous PTO, health coverage, disability insurance, paid family leave and more, we’ve launched extended benefits to help meet our employees’ needs. CapFlex – Employee-first mentality that supports a remote and hybrid workforce and empowers daily flexibility while servicing our clientsLearning & Development – Programs offering certification and tuition support, digital on-demand learning courses, mentorship, and skill development pathsModern Health –A mental health and well-being platform that provides 1:1 care, group support sessions, and self-serve resources to support employees and their families through life’s ups and downsCarrot Fertility –Inclusive fertility and family-forming coverage for all paths to parenthood – including adoption, surrogacy, fertility treatments, pregnancy, and more – and opportunities for employer-sponsored funds to help pay for careFringe –A company paid stipend program for personalized lifestyle benefits, allowing employees to choose benefits that matter most to them – ranging from vendors like Netflix, Spotify, and GrubHub to services like student loan repayment, travel, fitness, and moreEmployee Resource Groups – Employee-led committees that embrace and incorporate diversity and inclusion into our day-to-day operationsPhilanthropic Partnerships – Opportunities to engage in partnerships and pro-bono projects that support our communities. 401(k) Matching – Generous matching and no vesting period to help you continue to build financial wellness
CapTech is an equal opportunity employer committed to fostering a culture of equality, inclusion and fairness — each foundational to our core values. We strive to create a diverse environment where each employee is encouraged to bring their unique ideas, backgrounds and experiences to the workplace. For more information about our Diversity, Inclusion and Belonging efforts, click HERE. As part of this commitment, CapTech will ensure that persons with disabilities are provided reasonable accommodations. If reasonable accommodation is needed to participate in the job application or interview process, to perform essential job functions, and/or to receive other benefits and privileges of employment, please contact Laura Massa directly via email lmassa@captechconsulting.com.At this time, CapTech cannot transfer nor sponsor a work visa for this position. Applicants must be authorized to work directly for any employer in the United States without visa sponsorship."
Big Data Engineer,ClifyX,United States (Remote),https://www.linkedin.com/jobs/view/3746813130/?eBP=JOB_SEARCH_ORGANIC&refId=urH%2B5plSvPeNh7%2FHe8SOzg%3D%3D&trackingId=KBr5m2lYn8HT6mzF6xrgrw%3D%3D&trk=flagship3_search_srp_jobs,3746813130,"About the job
            
 
For all the below Job Title : AI & GenAI (Generative Artificial Intelligence) Exp Mandatory --- Contract or FTE Any Location Open to Travel or Relocate Plz Check all the options... Client ---Client  12.Big Data Engineer: Specialized expertise in big data technologies and tools. Proficiency in working with distributed systems. Experience with data ingestion and processing at scale. GenAI (Generative Artificial Intelligence) Roles:"
"Data Engineer, Senior",Blackbaud,"Arkansas, United States (Remote)",https://www.linkedin.com/jobs/view/3753081377/?eBP=JOB_SEARCH_ORGANIC&refId=urH%2B5plSvPeNh7%2FHe8SOzg%3D%3D&trackingId=yL83v1%2BWiq%2FH69PWXZbeqg%3D%3D&trk=flagship3_search_srp_jobs,3753081377,"About the job
            
 
Data EngineerThe Data Engineer role spans a broad range of experience. Data Engineers are responsible for ingestion, transformation and integration of data to provide a platform that supports research, data analysis, data enrichment and data science as well as making data operationally available at scale to consuming products and services. This includes a particular focus on the reliability, efficiency and quality of data assets and processing/integration pipelines.Working with and utilizing standardized tools, technologies and processes Data Engineers design, build and manage the data architecture that enables scalable production of data insight and they work very closely with Data Science to build and operationalize the algorithms that identify insights, predict behavior and prescribe action. This intelligence fuels the development of Decision-Driven Experiences for our customers.Blackbaud Data Engineers understand how to leverage data for business value and social good, and our Data Engineering team develops, cultivates and curates our industry-leading platform of integrated data assets.What You'll Be Doing Develop production level code. Own process features from design to implementation.Assemble large, complex data sets for use in R&D, data science, machine learning, or projection to operational stores.Build the infrastructure that provides efficient and scalable ETL/ELT and integration of data from multiple proprietary and 3rd party sources.Code, test and deploy data pipelines in support of various initiatives.Ensure quality of code, processes, and data assets.Work on complex tasks and/or broad programs that are large and diverse in scope and/or critical in nature. Exercises good judgment in determining technical approach.
What We'll Want You To Have 3+ years of ETL/ELT development, implementation, and supportUnderstanding or knowledge of data integration concepts3+ years of demonstrated success developing with Core Programming Languages – SQL, PythonUnderstanding or knowledge of Azure cloud services – Azure Data Catalog, Azure Data Lake Store, Azure BLOB StoreUnderstanding or knowledge of AWS cloud services – S3, EC2, Data Pipelines3+ years utilizing and developing SQL for relational databases (Oracle, MS SQL Server, MySQL)Understanding or knowledge of Linux/Unix commands, scripting (bash) and file management
Stay up to date on everything Blackbaud, follow us on Linkedin, Twitter, Instagram, Facebook and YouTubeBlackbaud is a remote-first company which embraces a flexible remote work culture. Blackbaud supports hiring and career development for all roles from the location you are in today!Blackbaud is proud to be an equal opportunity employer and is committed to maintaining a diverse and inclusive work environment. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, physical or mental disability, age, or veteran status or any other basis protected by federal, state, or local law.To all recruitment agencies: We do not accept unsolicited agency resumes and are not responsible for any fees related to unsolicited resumes.A notice to candidates: Recruitment Fraudulent Alert: Your personal information and online safety as a candidate mean a lot to us! At Blackbaud and our portfolio of companies, recruiters only direct candidates to apply through our official careers page at https://careers.blackbaud.com/us/en or our official LinkedIn page. Recruiters will never request payments, ask for financial account information or sensitive information like social security numbers, or conduct interviews via Skype. Anyone suggesting otherwise is not a representative of Blackbaud. If you are unsure if a message is from Blackbaud, please email blackbaudrecruiting@blackbaud.com. The starting base pay is $117,200.00 to $157,500.00. Blackbaud may pay more or less based on employee qualifications, market value, Company finances, and other operational considerations.Benefits Include Medical, dental, and vision insuranceRemote-first workforce401(k) program with employer matchFlexible paid time offGenerous Parental LeaveVolunteer for vacationOpportunities to connect to build community and belongingPet insurance, legal and identity protectionTuition reimbursement program
R0011171"
Senior Data Engineer,Lorven Technologies Inc.,United States (Remote),https://www.linkedin.com/jobs/view/3775756000/?eBP=JOB_SEARCH_ORGANIC&refId=urH%2B5plSvPeNh7%2FHe8SOzg%3D%3D&trackingId=HpPtYNZapEFaGZJ5vmzAlw%3D%3D&trk=flagship3_search_srp_jobs,3775756000,"About the job
            
 
Job Title: Senior Data EngineerLocation: RemoteDuration: 6+ Months ContactJob Description Bachelor’s degree in computer science, Engineering, or related field; or equivalent work experience.Extensive experience 10+ years working with MS SQL Server, SSIS, SSRS, and proficient in writing and optimizing SQL queries.Proven track record in database design, optimization, performance tuning, and troubleshooting in enterprise environments.Strong expertise in developing and maintaining ETL processes using SSIS for data integration.Proficiency in creating meaningful reports and visualizations using SSRS.Experience in designing and implementing database solutions that align with business objectives.Advanced knowledge of stored procedures, functions, and views for data manipulation and extraction.Excellent problem-solving skills and ability to work effectively in a collaborative team environment.Strong communication skills to convey technical concepts to non-technical stakeholders.
Your Primary Responsibilities Will Include MS SQL server, SSIS, SSRS, strong SQL queries, DB design, performance tuning, stored procedures, functions, and views
NoteTHESE EMAILS ARE GENERATED BY KEYWORD AND I APOLOGIZE IF THESE SKILLS SETS DO NOT MATCH YOUR EXPERTISE, OR IF THE LOCATION IS OUT OF RANGE.We do have other opportunities available. If you are interested, please send me your latest resume. If you are not currently seeking employment, or if you would prefer, I contact you at some later date, please indicate your date of availability so that I may honor your request."
GCP Data Engineer-US,Zortech Solutions,United States (Remote),https://www.linkedin.com/jobs/view/3726823245/?eBP=JOB_SEARCH_ORGANIC&refId=urH%2B5plSvPeNh7%2FHe8SOzg%3D%3D&trackingId=sNq8muk5Yidj%2FIahsRT5ig%3D%3D&trk=flagship3_search_srp_jobs,3726823245,"About the job
            
 
Please share your Updated resume to pavan@zortechsolutions.caRole: GCP Data EngineerLocation: USA (Remote) or NJ-HybridDuration: FulltimeJob DescriptionMust be willing to have some overlap hours with the India based data engineering team.Must be willing and able to work in a global project environment across multiple time zones.Project is very fast paced with tight deadlines.  10+ years of experience in designing and implementing large scale data processing/data storage/data distribution systems Extensive experience working with large data sets with hands-on technology skills to design and build robust Big Data solutions using Spark framework, GCP Big data services and industry standard frameworks Ability to work with a multi-technology/cross-functional teams and key stakeholders to guide/manage a full life-cycle solution Extensive experience in Relational and MPP database platforms like (GCP Bigquery/Hive/Cloud SQL etc) Open-source Hadoop stack/Spark framework Strong understanding of Big Data Analytics platforms and ETL in the context of Big Data Excellent problem solving, hands-on engineering skills and communication skills Broad understanding and experience of real-time analytics Participate in full Software Development Life Cycle (SDLC) of the Big Data Solution
Technical Skills Required  Any combination of below technical skills Hadoop: HDFS, MapReduce, Hive, Airflow DW: Bigquery, Hive Languages: Python, PySpark, Shell Scripting, SQL Scripting Cloud: GCP Big data native services Any RDBMS/DWBI technologies Spark (Mandatory): Spark on GCP Dataproc
Roles & ResponsibilitiesPosition Activities and Tasks  Extensive design and development complex and high-performance Data architecture development experiences Participating and leading key engagements, in developing plans and strategies of data management processes and IT programs for the business, providing hands on assistance in data modeling, technical implementation of big data solutions. Facilitating, guiding, and influencing the clients and teams towards right information technology architecture and becoming interface between Business leadership, Tech leadership and the delivery teams Leading and mentoring other developers within the team Identify the performance bottlenecks and resolve the same. Ability to produce high quality work products under pressure and within deadlines. To coordinate with developers / other architects / other stakeholders and cross functional teams from organization"
"Healthcare - Sr Data Engineer, Montebello, CA",Mantek Solutions Inc.,United States (Remote),https://www.linkedin.com/jobs/view/3759341873/?eBP=JOB_SEARCH_ORGANIC&refId=urH%2B5plSvPeNh7%2FHe8SOzg%3D%3D&trackingId=abpFbhI4PFCti0laGQHtzQ%3D%3D&trk=flagship3_search_srp_jobs,3759341873,"About the job
            
 
Sr Data Engineer needed for a full time position in Montebello, CAOnsite/Local candidates encouraged to applyNotes: ETL experience with SSIS, SQL, SSRS, Tableau experienceHealthcare experienceJob SummaryUnder the direction of Director – Data Engineer, the Senior Data Engineer works closely with business leaders, managers, staff and vendor to accurately gather and interpret requirements and specifications. Develop technical specifications and recommend, design, develop, test, implement, and support innovative and optimal data solutions. Serves as a coach and mentor to Data engineering teamSkills And Abilities Be able to consult on complex data engineering efforts and lead project teams through the solution design process Be able to teach and mentor to less-experience technical team members. Ability to compliance with standards and procedures such as standard of communication, work management, change management, version control, implementation and/or consistency of coding. Recognizes code, process and/or standard inefficiencies and suggests new standards and opportunities for improvement. Ability to build and integrate a data-driven intelligent solution into our business processes. Manage the innovation development processes and be responsible for driving the data architecture for the company's products and IT processes. Ability to research, evaluate and formally recommend third party software and technology package Keep big picture concepts in mind when designing solutions; fully understand business needs Strong experience on database technologies, data warehouse, data validation, data quality, metadata management and data governance Providing proactive technical oversight and advice to application architecture and development teams fostering re-use, design for scale, stability, and operational efficiency of data/analytical solutions Knowledge of and experienced in rolling out best practices in all facets of DW architecture, data flow strategy, data modeling, metadata and master data management. Ability to interact and develop relationships with all levels of personnel and management. 
Physical Requirements Ability to sit, stand, lift (up to 20 lbs.), bend, and walk, as required, for carrying out the duties of position May require travel to sites/program and special functions 
Environmental Conditions Critical To Performance Work is based in an office environment, climate controlled through central air conditioning May be required to attend off site meetings and other related functions 
If qualified and interested in this opportunity, please apply with an updated resume and annual salary requirements.JPC-6912"
BIG DATA ENGINEER/DEVELOPER | REMOTE 100%,eStaffing Inc.,United States (Remote),https://www.linkedin.com/jobs/view/3617897361/?eBP=JOB_SEARCH_ORGANIC&refId=urH%2B5plSvPeNh7%2FHe8SOzg%3D%3D&trackingId=Sf7g8zvHse84fMm%2F8WOQLQ%3D%3D&trk=flagship3_search_srp_jobs,3617897361,"About the job
            
 
Client: Health and Care ServicesTitle: Big Data Developer (Big Data Software Engineer)Type: C2H & C2CLocation: 100% REMOTEJob RequirementsTechnical Requirements Bachelor's degree in Computer Engineering or Computer Science and 7+ years of post-bachelor's degree experience in related field.5+ years of Big Data Software Engineering experience Spark, Hadoop and similar frameworks4+ years of experience in Apache Spark/Hadoop Framework3+ years' experience programming in Scala and Python required2+ years' experience in Java, PL/SQL and/or SQL (overall should have general programming experience with one of more of these languages)2+ year of experience working with Azure Cloud including having at least 1+ year of both HDInsight and Databricks experience.Experience with Apache Kakfa and HBase would be very helpful for this role but not required.Strong technical skills and ability to communicate complex technology solutions to clients, technical, business and management teams.Strong Analytical and critical thinking skills with ability to manage and solve multiple complex problems.Experience in Agile Development methodologiesFamiliarity with code versioning tools like Git , CICD tools like Jenkins and XL Deploy"
Senior Data Engineer,Gentis Solutions,"Cincinnati, OH (Remote)",https://www.linkedin.com/jobs/view/3759573070/?eBP=JOB_SEARCH_ORGANIC&refId=urH%2B5plSvPeNh7%2FHe8SOzg%3D%3D&trackingId=iOmNzsPgeXToxGdAQHCxuA%3D%3D&trk=flagship3_search_srp_jobs,3759573070,"About the job
            
 
Gentis Solutions is seeking a Senior Data Analyst to join our team. This contract-to-hire position is with one of our Fortune 50 clients interested in full-time flex/remote consultants. The ideal candidates will have the required skills listed below and will be eligible and open to being hired by our client at the end of the project's duration. This position works alongside an existing team and leverages enterprise-level technologies and processes. If you would like to work at a company that has been recognized for its diversity and inclusion, its work to drive positive social change, and as an environmental leader, make sure you apply below.Requirements 2+ years of direct experience in managing and curating data catalogs, with a proven track record of technical expertise2+ years of experience working closely with data stewardship principles and practices, demonstrating a strong understanding of data governance and stewardship conceptsExperience with Cloud Platforms, including Azure Data Lake Storage (ADLS), Unity Catalog, Databricks, and Azure Synapse, with a strong understanding of metadata management, security principles, and fundamental cloud conceptsExpertise with Python and SQL for querying and managing databases for data storage and retrievalStrong experience in network administration, with a solid foundation in managing data access and security in a networked environmentStrong previous experience working with Alation or any other leading data catalog platform, showcasing proficiency in configuring and customizing such toolsProficiency integrating data catalog processes, customizing configurations, implementing connectors, and creating data processing scriptsFamiliarity with other data catalog solutions in addition to Alation, highlighting versatility and adaptability in managing various cataloging technologiesSupport, maintain, and document software functionality, best practices, standards, and processesDemonstrable critical thinkingExperience with an E-commerce or multi-channel retail environmentExcellent verbal and written communication skillsBe passionate about Metadata and Databases!
Desirable Skills Relevant certifications in data management, data governance, or related fields are a plusKnowledge with Databricks or Jupyter Notebooks for data engineering, data preparation, collaborative data analytics, data analysis and documentation a plusFamiliarity with Tableau, Power BI or other data visualization and reporting tools a plus
Typical Duties Collaborate with cross-functional teams to contribute to and validate the technical aspects of the Data Catalog Roadmap, ensuring alignment with organizational goals and objectivesConfigure and customize the data catalog platform, such as Alation, to accurately represent our organization's data landscape, making it a valuable resource for data usersImplement and maintain permissions in Alation, ensuring that the Data Catalog is used in accordance with data access policies and security protocolsTake ownership of data source-specific configurations in Alation, and delegate specific actions to Data Source Administrators, overseeing their work to maintain consistency and accuracySet up and manage authentication mechanisms, user accounts, groups, and other relevant security components to safeguard data access and maintain complianceWrite, maintain, and execute Security Plans, IT Playbooks, Support Plans, and Data Access Policies to ensure the reliable and secure operation of the Data CatalogAssign specific tasks and responsibilities to other members of the technical team, ensuring efficient collaboration and workload distributionWork closely with the product management team to prioritize data source requirements and project plans, ensuring alignment with organizational objectivesCollaborate with product management to define operational standards and procedures for the data catalog, promoting consistency and best practicesPlan and coordinate the ingestion of new or updated Business Intelligence (BI) and data sources, ensuring seamless integration into the catalogWork with product management to define the settings and repeatable patterns for configuring new data sources and managing access permissionsCoordinate with all data management and IT organizations as required, including compliance and governance organizations, to ensure cross-functional alignmentCollaborate with the Data Discoverability team to discuss the status of data discoverability initiatives and identify both technical and non-technical issues that may ariseReport on operational service level agreements (SLAs) and support performance, providing transparency and accountabilityInvestigate and resolve escalated technical issues, ensuring timely resolution and minimal disruption to data usersDelegate the investigation and resolution of issues to the technical team and Data Source Administrators as appropriate, fostering a collaborative problem-solving approachParticipate in IT-related communication planning and execution, ensuring that relevant stakeholders are informed of important developmentsPlan and participate in training sessions to enhance the skills and knowledge of team members and data source administratorsRecruit, train, and mentor Data Stewards and/or other team members, fostering their professional development and growth within the organizationActively participate in program status and planning meetings, contributing valuable insights and expertise"
Data Engineer II (Talend),Centene Corporation,"Texas, United States (Remote)",https://www.linkedin.com/jobs/view/3760306411/?eBP=JOB_SEARCH_ORGANIC&refId=urH%2B5plSvPeNh7%2FHe8SOzg%3D%3D&trackingId=pMduuU8wTpfQY2H8UD%2FZPw%3D%3D&trk=flagship3_search_srp_jobs,3760306411,"About the job
            
 
You could be the one who changes everything for our 28 million members by using technology to improve health outcomes around the world. As a diversified, national organization, Centene's technology professionals have access to competitive benefits including a fresh perspective on workplace flexibility.Position Purpose Develops and operationalizes data pipelines to make data available for consumption (reports and advanced analytics), including data ingestion, data transformation, data validation / quality, data pipeline optimization, and orchestration. Engages with the DevSecOps Engineer during continuous integration and continuous deployment. Talend implementation of data flows. Designs and implements standardized data management procedures around data staging, data ingestion, data preparation, data provisioning, and data destruction (scripts, programs, automation, assisted by automation, etc.)Designs, develops, implements, tests, documents, and operates large-scale, high-volume, high-performance data structures for business intelligence analyticsDesigns, develops, and maintains real-time processing applications and real-time data pipelinesEnsure quality of technical solutions as data moves across Centene’s environmentsProvides insight into the changing data environment, data processing, data storage, and utilization requirements for the company and offers suggestions for solutionsDevelops, constructs, tests, and maintains architectures using programming language and toolsIdentifies ways to improve data reliability, efficiency, and quality; use data to discover tasks that can be automatedPerforms other duties as assignedComplies with all policies and standards
Education/Experience A Bachelor's degree in a quantitative or business field (e.g., statistics, mathematics, engineering, computer science).Requires 2 – 4 years of related experience.Or equivalent experience acquired through accomplishments of applicable knowledge, duties, scope and skill reflective of the level of this position.Technical Skills One or more of the following skills are desired.Experience developing Talend (Programming Language) Experience with SQL (Programming Language)Experience with AWSExperience with Healthcare (specifically HEDIS)Experience with Big Data; Data ProcessingExperience with diagnosing system issues, engaging in data validation, and providing quality assurance testingExperience with Data Manipulation; Data MiningExperience working in a production cloud infrastructureKnowledge of Microsoft SQL Servers
Soft Skills Strong Communication and Organizational skills
Our Comprehensive Benefits Package Flexible work solutions including remote options, hybrid work schedules and dress flexibility, Competitive pay, Paid time off including holidays, Health insurance coverage for you and your dependents, 401(k) and stock purchase plans, Tuition reimbursement and best-in-class training and development.Centene is an equal opportunity employer that is committed to diversity, and values the ways in which we are different. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, veteran status, or other characteristic protected by applicable law."
Remote work - Need Senior Health Data Migration Engineer Remote,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3707378312/?eBP=JOB_SEARCH_ORGANIC&refId=urH%2B5plSvPeNh7%2FHe8SOzg%3D%3D&trackingId=doFSKTwL8pRl36xsB93DoQ%3D%3D&trk=flagship3_search_srp_jobs,3707378312,"About the job
            
 
Experience With IRIS And SQL Experience MustMust need hands on Cache IRIS developer experienceMust have hands on InterSystems experience or IRIS.Title: Senior Health Data Migration EngineerPosition Type: ContractLocation: Remote, United States 15+ years of professional work experience, to include experience with InterSystems IRIS in a healthcare environment Bachelor's degree in Computer Science, Engineering, Math, or equivalent, or an additional 8 years of relevant experience may be substituted for degree requirementsActivitiesSupport the Department of Veterans Affairs (VA) Electronic Health Record Modernization Integration Office (EHRM-IO) as a Senior Health Data Migration Engineer.Assess the current EHRM data migration requirements; maintain and update the strategy to meet the requirements. Review error and trace logs. Track messages by domain and reconcile table counts with Cerner. Review secure data message transmission logs. Track the number of records sent per message by domain. Monitor Queue Depth by service/process/operation. Track retry attempts and suspended messages. Validate and update data integration reports in support of VX130 data domains.Review, update, and maintain Cerner to CDW, VistA, Millennium or Cloud Database data mappings for potential data migrations. Evaluate and integrate data from multiple sources, which requires data mapping from one data source to another minimizing any data loss. Document VistA Extraction and monitoring process and update existing documentation quarterly. Interpret Cerner's Data model to be used for the construction of API's, queries, and reports that will be consumed by internal or external applications.Review Domain adds to Ensemble Production. Validate edits made to Domain record type, schema version, status, and payload size via the GUI Interface and Rule Builder. Validate Ensemble data flows built using VX130 ClassBuilder. Validate the load of Cache/IRIS Objects, SQL Tables or other storage structures(Historical Pulls) in all regions/districts for classes in all environments with VistA or Data Syndication data.RequirementsMinimum qualifications:15+ years of professional work experience, to include experience with InterSystems IRIS in a healthcare environmentBe able to create strategies and plans for integration of multiple IT systems/subsystems into an operational unit, ensuring full functional and performance capabilities are retained.Able to coordinate with development and user teams to assess risks, goals and needs and ensure that all are adequately addressed.Experienced in introducing new hardware or software into a new or existing environment while minimizing disruption and mitigating risks.Able to be cost conscience as well addressing goals.Bachelor's degree in Computer Science, Engineering, Math, or equivalent, or an additional 8 years of relevant experience may be substituted for degree requirementsPreferred QualificationsExperience in the VAExperience implementing EHRExperience with VA and DoD legacy health data, and private sector health dataExperience with Extraction, Transformation, and Loading of data between systemsKnowledge and experience handling VistA data"
Need Sr Data Engineer (Direct Client) - Remote,SPAR Information Systems LLC,United States (Remote),https://www.linkedin.com/jobs/view/3731654778/?eBP=JOB_SEARCH_ORGANIC&refId=urH%2B5plSvPeNh7%2FHe8SOzg%3D%3D&trackingId=%2FHOl6EQi%2BWIIIqHIUTbRBw%3D%3D&trk=flagship3_search_srp_jobs,3731654778,"About the job
            
 
Hi Associate,Hope you are doing good.I have urgent requirement kindly let me know if you have any resource available. Role: Sr Data EngineerLocation: RemoteDuration: 3 Months Contract to hireJob DescriptionWe are seeking a highly motivated Senior Data Engineer to join our Vendor data product team. As a Senior Data Engineer you will be responsible for creating products using data from both internal and external vendor data sources to help realize goals of attracting customers through more accurate pricing, greater customer retention, and an increase in underwriting profitability. The right candidate will develop well-designed, testable, and efficient data ingestion, data enrichment, and data transformation solutions using best software development practices. You should be an analytical thinker, a self-learner, and comfortable supporting the needs of multiple projects. This role is a part of the Data Movement team of Data, Security & Infrastructure (DSI) in our Technology Solutions organization.In This Role, You Will Identify, design, and implement internal process improvements, automating manual processes, optimizing data delivery for greater scalabilityBuild the processes required for optimal extraction, transformation, and loading of data using a variety of languages and technologies such as Scala, Python, Kafka, Azure Data Factory, Fivetran/HVR, dbt, and DatabricksCollaborate with stakeholders including the Product, Data Engineering, and Agile Delivery teams in an agile environment to assist and resolve data-related issues and support data delivery needsWork with other data platform or data domain teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologiesPerform unit tests and conduct reviews with other team members to make sure code is rigorously designed, elegantly coded, and effectively tuned for performanceShare your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community
Experience & Skills At least 2 years of experience with designing, developing, implementing, and maintaining solutions for Big Data or data warehouse systemAt least 2 Years of experience working in a cloud environment such as Azure, AWS or other private or public cloudExperience performing root cause analysis on internal and external data and processes to answer business questions and identify opportunities for improvementStrong analytical skills related to working with unstructured datasetsGood experience with bringing data into a centralized data repository or manipulating the available data to build additional data sets for Analytics and Reporting purposes.Experienced with maintaining data quality throughout the lifecycle of the data.Experience with Data Modeling, source to target mapping, automated testing frameworks, CI/CD pipelines and task automation using scriptingExperienced with working in Agile environment and end to end automationDeveloping new and enhancing existing data processing (Data Ingest, Data Transformation, Data Store, Data Management, Data Quality) componentsStrong working knowledge of SQL and the ability to write, debug and optimize SQL queries and ETL jobs to reduce the execution window or reduce resource utilizationData Engineering experience focused on batch and real-time data pipelines development, Data processing/data transformation using ETL/ELT tools, SnowPipe, dbt, or DatabricksExperience with Cloud Data Warehouse solutions experience (Snowflake, Azure DW, Redshift or similar technology in other private or public clouds).Complete software development lifecycle experience including design, documentation, implementation, testing, and deployment
Basic Qualifications Bachelor's Degree in a computer-related field or equivalent professional experience requiredAt least 2 years of experience in data engineering using open-source technology stack along with cloud computing (AWS, Microsoft Azure, Google Cloud)At least 2 years of experience with designing, developing, implementing, and maintaining solutions for data ingestion and transformation projects with dbt, SnowPipe, or DataBricksAt least 2 years of advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with cloud databases
Preferred Qualifications 3+ years of experience with dbt, SnowPipe, or DataBricks3+ years of experience working on real-time data and streaming applications (Spark Streaming or Kafka)3+ years of experience working with Cloud Data Warehouse solutions (i.e., Snowflake, Synapse, Redshift)3+ years of experience with Agile engineering practices
Rahul.BTeam LeadSPAR Information SystemsEmail: rahul@sparinfosys.com"
Data Engineer (3) - REMOTE - Active TS/SCI Clearance,System One,"Alexandria, VA (Remote)",https://www.linkedin.com/jobs/view/3780592689/?eBP=JOB_SEARCH_ORGANIC&refId=urH%2B5plSvPeNh7%2FHe8SOzg%3D%3D&trackingId=ABlTkNYyfTLSIyZhUrTrOQ%3D%3D&trk=flagship3_search_srp_jobs,3780592689,"About the job
            
 
Candidates must hold an active TS/SCI clearance and live in MD, DC or VAWe are currently seeking a Data Engineer with data pipeline expertise for a full-time, permanent position. This is a hybrid role that is primarily remote work. Candidates must live in VA, MD or DC in order to work onsite when needed.The Data Engineers will support the Office of Intelligence and Analysis (I&A) at DHS, this role will be part of a team focused on supporting the development of a new capability for an I&A mission customer.Duties  Develop and design data pipelines to support an end-to-end solution Develop and maintain artifacts i.e. schemas, data dictionaries, and transforms related to ETL processes Integrate data pipelines with AWS cloud services to extract meaningful insights Manage production data within multiple datasets ensuring fault tolerance and redundancy Design and develop robust and functional dataflows to support raw data and expected data Provide Tier 3 technical support for deployed applications and dataflows Experience with cloud message APIs and usage of push notifications Collaborate with the rest of data engineering team to design and launch new features. Includes coordination and documentation of dataflows, capabilities, etc. Keen interest in learning and using the latest software tools, methods, and technologies to solve real world problem sets vital to national security
SkillsDatabase administration and development experience will be a plus for consideration.System One, and its subsidiaries including Joulé, ALTA IT Services, CM Access, and MOUNTAIN, LTD., are leaders in delivering outsourced services and workforce solutions across North America. We help clients get work done more efficiently and economically, without compromising quality. System One not only serves as a valued partner for our clients, but we offer eligible employees health and welfare benefits coverage options including medical, dental, vision, spending accounts, life insurance, voluntary plans, as well as participation in a 401(k) plan.System One is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, age, national origin, disability, family care or medical leave status, genetic information, veteran status, marital status, or any other characteristic protected by applicable federal, state, or local law."
Remote Work - Need Sr Data Software Engineer,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3735082431/?eBP=JOB_SEARCH_ORGANIC&refId=urH%2B5plSvPeNh7%2FHe8SOzg%3D%3D&trackingId=rHoHJsfWRNe%2FPA6UbJYfbA%3D%3D&trk=flagship3_search_srp_jobs,3735082431,"About the job
            
 
Job: Sr. Data Software EngineerLocation: 100% RemoteDuration: 6 month Contract to Hire (USC/ GC Holder ONLY)Top Skills SQLAzure Data FactoryDBTETL workProven ability to complete projects in a timely manner while clearly measuring progressStrong software engineering fundamentals (data structures, algorithms, async programming patterns, object-oriented design, parallel programming) Strong understanding and demonstrated experience with at least one popular programming language (.NET or Java) and SQL constructs.Experience writing and maintaining frontend client applications, Angular preferredStrong experience with revision control (Git)Experience with cloud-based systems (Azure / AWS / GCP).High level understanding of big data design (data lake, data mesh, data warehouse) and data normalization patternsDemonstrated experience with Queuing technologies (Kafka / SNS / RabbitMQ etc)Demonstrated experience with Metrics, Logging, Monitoring and Alerting toolsStrong communication skillsStrong experience with use of RESTful APIsHigh level understanding of HL7 V2.x / FHIR based interface messages.High level understanding of system deployment tasks and technologies. (CI/CD Pipeline, K8s, Terraform).
Project/Day To Day Communicate with business leaders to help translate requirements into functional specificationDevelop broad understanding of business logic and functionality of current systemsAnalyze and manipulate data by writing and running SQL queriesAnalyze logs to identify and prevent potential issues from occurringDeliver clean and functional code in accordance with business requirementsConsume data from any source, such a flat files, streaming systems, or RESTful APIs Interface with Electronic Health RecordsEngineer scalable, reliable, and performant systems to manage dataCollaborate closely with other Engineers, QA, Scrum master, Product Manager in your team as well as across the organizationBuild quality systems while expanding offerings to dependent teamsComfortable in multiple roles, from Design and Development to Code Deployment to and monitoring and investigating in production systems."
Big Data Engineer (Various Levels),Open Systems Inc.,United States (Remote),https://www.linkedin.com/jobs/view/3775776995/?eBP=JOB_SEARCH_ORGANIC&refId=urH%2B5plSvPeNh7%2FHe8SOzg%3D%3D&trackingId=z5hq5W3cVz%2BaHTEfnpKeMg%3D%3D&trk=flagship3_search_srp_jobs,3775776995,"About the job
            
 
Big Data EngineerRemoteCompanyOur client, a Fortune-300 transportation company specializing in freight railroading. They operate approximately 21,000 route miles in 22 states and the District of Columbia, serve every major container port in the eastern United States, and provide efficient connections to other rail carriers. Our client has the most extensive intermodal network in the East and is a major transporter of coal and industrial products.Job DescriptionOur client is currently seeking an experienced Data Engineer – Big Data individual for their Midtown office in Atlanta, GA. The successful candidate must have Big Data engineering experience and must demonstrate an affinity for working with others to create successful solutions. Join a smart, highly skilled team with a passion for technology, where you will work on our state of the art Big Data Platforms. They must be a very good communicator, both written and verbal, and have some experience working with business areas to translate their business data needs and data questions into project requirements. The candidate will participate in all phases of the Data Engineering life cycle and will independently and collaboratively write project requirements, architect solutions and perform data ingestion development and support duties.RequiredSkills and Experience:  6+ years of overall IT experience3+ years of experience with high-velocity high-volume stream processing: Apache Kafka and Spark Streaming Experience with real-time data processing and streaming techniques using Spark structured streaming and KafkaDeep knowledge of troubleshooting and tuning Spark applications
3+ years of experience with data ingestion from Message Queues (Tibco, IBM, etc.) and different file formats across different platforms like JSON, XML, CSV3+ years of experience with Big Data tools/technologies like Hadoop, Spark, Spark SQL, Kafka, Sqoop, Hive, S3, HDFS, or 3+ years of experience building, testing, and optimizing ‘Big Data’ data ingestion pipelines, architectures, and data sets2+ years of experience with Python (and/or Scala) and PySpark/Scala-Spark3+ years of experience with Cloud platforms e.g. AWS, GCP, etc. 3+ years of experience with database solutions like Kudu/Impala, or Delta Lake or Snowflake or BigQuery2+ years of experience with NoSQL databases, including HBASE and/or CassandraExperience in successfully building and deploying a new data platform on Azure/ AWSExperience in Azure / AWS Serverless technologies, like, S3, Kinesis/MSK, lambda, and GlueStrong knowledge of Messaging Platforms like Kafka, Amazon MSK & TIBCO EMS or IBM MQ SeriesExperience with Databricks UI, Managing Databricks Notebooks, Delta Lake with Python, Delta Lake with Spark SQL, Delta Live Tables, Unity Catalog Knowledge of Unix/Linux platform and shell scripting is a mustStrong analytical and problem-solving skills
Preferred (Not Required) Strong SQL skills with ability to write intermediate complexity queriesStrong understanding of Relational & Dimensional modeling Experience with GIT code versioning softwareExperience with REST API and Web ServicesGood business analyst and requirements gathering/writing skills
EducationBachelor’s Degree required. Preferably in Information Systems, Computer Science, Computer Information Systems or related fieldWho We AreOpen Systems Inc. (OSI) was founded in 1994 to provide information technology solutions and staffing services to large and mid-size companies across the U.S. Our corporate office is located at 6495 Shiloh Road, Ste 310 Alpharetta, GA 30005. We provide a full range of staffing services including contract, contract-to-hire, and direct hire solutions. Our technical recruiting experts are experienced in technical screening, candidate sourcing, and behavioral interviewing techniques. They focus on providing candidates who match your technical requirements and fit seamlessly into your company culture.Contact Open Systems, Inc. anytime by website, phone or email. We look forward to hearing from you!!"
Senior Data Engineer,"Blue Horizon Tek Solutions, Inc.",United States (Remote),https://www.linkedin.com/jobs/view/3740460001/?eBP=JOB_SEARCH_ORGANIC&refId=urH%2B5plSvPeNh7%2FHe8SOzg%3D%3D&trackingId=zKJ%2FSsUz3co59lZA47YjVg%3D%3D&trk=flagship3_search_srp_jobs,3740460001,"About the job
            
 
Position: Data EngineerIndustry: Media and EntertainmentLocation: New York City OR REMOTESummary:We're looking for an experienced Data Engineer to join our client's growing team of top notched IT professionals. You will work closely across multiple groups including Editorial and Audio, Marketing, Advertising, etc. to achieve their goals and objectives. The Data Engineering team builds tools throughout the stack while sharing knowledge across these departments.As the company grows, they're looking for Data Engineer who will help solidify and expand our pipelines and maintain their data warehouse. Their business model is based on performing complex and very detailed analyses of how our products perform with their subscribers. You'll be responsible for working alongside analysts, product, marketing, and other teams to define new data collection components and measurement schemes that support each new product and feature.How can add value to their mission: Create and maintain data pipelines to provide insights and drive business decisionsEstablish data warehousing strategy (ex. Kimball, Data Vault, etc.)Maintain data infrastructure on our AWS accountsCollaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.Write unit/integration tests, contributes to engineering wiki, and documents workImplement processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it
What You'll Need to Succeed: 7+ years of industry experience measuring product performance and user behaviorExperience working with a variety of data management technologies, including RedShift, Kinesis/Kafka, Glue, Spark, Postgres, Airflow, dbt, and others.Experience implementing BI reporting tools such as LookerExperience interfacing with engineers, product managers and analysts to understand data needsKnowledge of a variety of measurement beacons, SDKs, APIs including Google Analytics, Amplitude, Braze, Stripe, email service providersA commitment to building secure, resilient, fault-tolerant architectures, with clear documentation and procedures in place for supportA focus on accuracy and detail as you build, ensuring data pipelines produce clear, predictable resultsUnderstanding of the typical metrics a subscription and advertising-supported business needs to measure successExperience with Client techniques as applied to behavioral segmentation or anomaly identification is a definite plusFamiliarity using developer tools that increase productivity and facilitate the development of resilient code (eg. Docker, CircleCI, Serverless) is a plus

Desired Skills and Experience
                DATA ENGINEER"
Remote Opportunity: Sr. Data Engineer,SPAR Information Systems LLC,United States (Remote),https://www.linkedin.com/jobs/view/3777290940/?eBP=JOB_SEARCH_ORGANIC&refId=urH%2B5plSvPeNh7%2FHe8SOzg%3D%3D&trackingId=jtBSmoNPGoYjCzfhrNP0sw%3D%3D&trk=flagship3_search_srp_jobs,3777290940,"About the job
            
 
Hello All, Hope you are doing great. Please go through the job description and let me know your interest. Role: Sr. Data Engineer with Big Data and Cloud Exp. Locations: Remote Duration: 3 Months Contract to hire Requirement: Client treats Data as Product and our Senior Engineer will be a key member of the engineering staff working across Business Services Engineering, Data Engineering, Platform Engineering, and Infrastructure Engineering to ensure that we provide a fiction-less experience to our customers, maintain the highest standards of protection and availability. Our team thrives and succeeds in supporting Data Driven company and delivering high quality technology products and services in a hyper-growth environment where priorities shift quickly. The ideal candidate has broad and deep technical knowledge in data, typically ranging from front-end UIs through back-end systems and all points in between. Required: 3+ years of experience in data software development, programming languages and developing with big data technologies 2+ years of experience designing and building on existing and new data applications 2+ years of experience in Cloud DevOps concepts, Cloud Services and Architecture, and Azure/AWS/GCP DevOps Operational Framework 1+ years of experience in open-source data tools and frameworks, or one of the following: .net Core, asp.Net, Angular, or Express. Additional Skills: Experience in data software development, using data technologies such as Relational & NoSQL databases, open data formats, and programming languages such as Python, Scala, and/or other frameworks, building data pipelines (ETL and ELT) with batch or streaming ingestion, error handling, loading, and transforming data, and developing with big data technologies such as Spark, Hadoop, and MapReduce. Experience with analytics solutions. Experience in development using Python or PySpark, Spark, Scala. Advanced understanding of designing and building for data quality assurance, reliability, availability, and scalability, on existing and new data applications. Advanced understanding of DevOps Concepts, Cloud Architecture, and Azure DevOps Operational Framework, Pipelines, Kubernetes. Advanced understanding of designing and building solutions for data quality and observability, metadata management, data lineage, and data discovery. Advanced understanding of building products of micro-services oriented architecture and extensible REST APIs. Advanced understanding of open-source frameworks. Experience with continuous delivery and infrastructure as code. Experience in existing Monitoring Portals: Splunk or Application Insights. Advanced understanding of Security Protocols & Products: Understanding of Active Directory, Windows Authentication, SAML, OAuth. Advanced understanding of Azure Network (Subscription, Security zoning, etc) & tools like Genesis. Advanced understanding of existing Operational Portals such as Azure Portal. Knowledge of CS data structures and algorithms. Knowledge of developer tooling across the software development life cycle (task management, source code, building, deployment, operations, real-time communication). Practical knowledge of working in an Agile environment (Scrum/Kanban/SAFe). Strong problem-solving ability. Ability to excel in a fast-paced, startup-like environment. Bachelor's degree in Computer Science, Information Systems, or equivalent education or work experienceThanks & Regards,Satnam SinghDirect: 201 623 3660Email : Satnam.singh@sparinfosys.com"
Remote Opportunity: Sr. Data Engineer,SPAR Information Systems LLC,United States (Remote),https://www.linkedin.com/jobs/view/3777293649/?eBP=JOB_SEARCH_ORGANIC&refId=urH%2B5plSvPeNh7%2FHe8SOzg%3D%3D&trackingId=w78AArN6lCJFnenicim2BQ%3D%3D&trk=flagship3_search_srp_jobs,3777293649,"About the job
            
 
Hello All, Hope you are doing great. Please go through the job description and let me know your interest. Role: Sr. Data Engineer (Big Data + Azure Exp) Locations: Remote Duration: 3 Months Contract to hire Requirement: Client treats Data as Product and our Senior Engineer will be a key member of the engineering staff working across Business Services Engineering, Data Engineering, Platform Engineering, and Infrastructure Engineering to ensure that we provide a fiction-less experience to our customers, maintain the highest standards of protection and availability. Our team thrives and succeeds in supporting Data Driven company and delivering high quality technology products and services in a hyper-growth environment where priorities shift quickly. The ideal candidate has broad and deep technical knowledge in data, typically ranging from front-end UIs through back-end systems and all points in between. Required: 3+ years of experience in data software development, programming languages and developing with big data technologies 2+ years of experience designing and building on existing and new data applications 2+ years of experience in Cloud DevOps concepts, Cloud Services and Architecture, and Azure/AWS/GCP DevOps Operational Framework 1+ years of experience in open-source data tools and frameworks, or one of the following: .net Core, asp.Net, Angular, or Express. Additional Skills: Experience in data software development, using data technologies such as Relational & NoSQL databases, open data formats, and programming languages such as Python, Scala, and/or other frameworks, building data pipelines (ETL and ELT) with batch or streaming ingestion, error handling, loading, and transforming data, and developing with big data technologies such as Spark, Hadoop, and MapReduce. Experience with analytics solutions. Experience in development using Python or PySpark, Spark, Scala. Advanced understanding of designing and building for data quality assurance, reliability, availability, and scalability, on existing and new data applications. Advanced understanding of DevOps Concepts, Cloud Architecture, and Azure DevOps Operational Framework, Pipelines, Kubernetes. Advanced understanding of designing and building solutions for data quality and observability, metadata management, data lineage, and data discovery. Advanced understanding of building products of micro-services oriented architecture and extensible REST APIs. Advanced understanding of open-source frameworks. Experience with continuous delivery and infrastructure as code. Experience in existing Monitoring Portals: Splunk or Application Insights. Advanced understanding of Security Protocols & Products: Understanding of Active Directory, Windows Authentication, SAML, OAuth. Advanced understanding of Azure Network (Subscription, Security zoning, etc) & tools like Genesis. Advanced understanding of existing Operational Portals such as Azure Portal. Knowledge of CS data structures and algorithms. Knowledge of developer tooling across the software development life cycle (task management, source code, building, deployment, operations, real-time communication). Practical knowledge of working in an Agile environment (Scrum/Kanban/SAFe). Strong problem-solving ability. Ability to excel in a fast-paced, startup-like environment. Bachelor's degree in Computer Science, Information Systems, or equivalent education or work experienceThanks & Regards,Satnam SinghDirect: 201 623 3660Email : Satnam.singh@sparinfosys.com"
Senior Data Engineer,Hopscotch Health,United States (Remote),https://www.linkedin.com/jobs/view/3770481737/?eBP=JOB_SEARCH_ORGANIC&refId=urH%2B5plSvPeNh7%2FHe8SOzg%3D%3D&trackingId=SLIYPrEPix8gE4CDIcPhjw%3D%3D&trk=flagship3_search_srp_jobs,3770481737,"About the job
            
 
About Hopscotch HealthAt Hopscotch Health, we believe great healthcare should be accessible to all people across all communities. Today, almost 20% of Americans live in a rural community, yet only 11% of physicians practice in those same communities. We are on a mission to transform healthcare in rural America. We provide high-quality primary care tailored to meet the needs of our patients through our robust care model and comprehensive care team, delivering care in our clinics, and across settings, and wrapping resources around the patients who need them most.Our patients and the care teams who serve them sit at the center of everything we do at Hopscotch. Hopscotch Health takes a team approach to serve patient needs and provide the best care possible. Our goal is to provide the care each of us would want for ourselves or for our family members, in the right setting, and at the right time.Today, we are serving thousands of patients in our value-based care model and the number is growing every day. If you want to bring your experience, skill and passion to make a lasting impact in healthcare, we’d like to meet you.About The RoleWe are looking for a mission-driven Senior Data Engineer to join our early team at a critical and exciting time for the company. In this position, you will play a pivotal role in building out Hopscotch Health’s data and analytics platform and data team.  You will have an opportunity to own end-to-end execution of data products and initiatives that directly impact the care delivery process. You will be part of a skilled, passionate, and collaborative team committed to working together to realize Hopscotch Health’s ambitious and important vision.Responsibilities Near-term focus areas for this role will include, but are not limited to: Build out and maintenance of the data infrastructure that supports end-user workflows, 3rd party integrations, and analytics needs Support integration of new data (e.g. HIE, ADT feeds) as needed to enhance quality and breadth of information available to clinical care teams Build out and maintenance of data pipelines that drive actionable intelligence Requirements scoping and documentation with key stakeholders to continually improve the design of data products across the company Monitor and maintain data pipeline health from ingest to final data products Contribute to our organizational ontology (i.e. data modeling) Collaborate with product teams to design and build out the backing data and infrastructure for organizational workflows Generate analytic insights, build backend KPIs and other metrics to help provide visibility into company performance towards critical goals 
ABOUT YOU:You would be a great fit for this position if you have 4+ years of experience in data engineering, preferably in healthcare services, and you are: Knowledgeable about healthcare, preferably in the services/provider sector Results-driven and focus time/resources against the most important priorities Creative in finding solutions to arrive at mutually beneficial outcomes Strategic and can bring a structured, proactive approach to get things done Can quickly develop insights from data, and translate findings into action Demonstrate first-principles approach to understanding and solving new problems Can translate complex concepts, verbally and in writing, and use synthesized communications to collaborate across an organization 
From a cultural perspective, you are:  Thoughtful and can work effectively in a fast-paced, ever-changing environment Constantly seeking ways to simplify and improve how things are done Accountable, holding yourself and others to a high standard Willing to roll up your sleeves to support the work required and collaborate effectively with people of all backgrounds Concise and articulate, drive towards clarity Collaborative, assuming positive intentions Mission-driven, with a passion for serving patients and communities Intellectually curious, with a desire to learn new skills and explore new spaces 
ADDITIONAL QUALIFICATIONS: Experience shipping production level code and pipelines for data purposes Familiarity with healthcare data sources, interfaces and software including EHR, population health tools, patient portals, and clinical decision support technology Comfortable writing clear, testable python code, with pyspark experience preferred Experience with tools to build re-usable, testable, and maintainable data models (e.g. ontologies) Experience with Airflow, Prefect, or other task execution platforms Experience with data lake solutions such Palantir Foundry, Databricks Comfortable in Docker and on the Unix command line Experience with cloud infrastructure (e.g. AWS) and working with serverless computing services (e.g. Lambda) 
At Hopscotch Health, we embrace diversity, invest in a culture of inclusion and positivity and encourage all to apply to join our team. You will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status."
"Data Engineer, Database Engineering",Experfy,United States (Remote),https://www.linkedin.com/jobs/view/3719412208/?eBP=JOB_SEARCH_ORGANIC&refId=urH%2B5plSvPeNh7%2FHe8SOzg%3D%3D&trackingId=Uh0EPvAuoSnLmnpgLKmxSQ%3D%3D&trk=flagship3_search_srp_jobs,3719412208,"About the job
            
 
As a Data Engineer for our Data Platform Engineering team you will join skilled Scala/ Spark engineers and core database developers responsible for developing hosted cloud analytics infrastructure (Apache Spark-based), distributed SQL processingframeworks, proprietary data science platforms, and core database optimization. This team is responsible for building the automated, intelligent, and highly performant query planner and execution engines, RPC calls between datawarehouse clusters, shared secondary cold storage, etc. This includes building new SQL features and customer-facing functionality, developing novel query optimization techniques for industry-leading performance, and building a databasesystem that's highly parallel, efficient and fault-tolerant. This is a vital role reporting to exec leadership and senior engineering leadershipRequirementsResponsibilities: Writing Scala code with tools like Apache Spark + Apache Arrow + Apache Kafka to build a hosted, multi-cluster data warehouse for Web3Developing database optimizers, query planners, query and data routing mechanisms, cluster-to-cluster communication, and workload management techniquesScaling up from proof of concept to ""cluster scale"" (and eventually hundreds of clusters with hundreds of terabytes each), in terms of both infrastructure/architecture and problem structureCodifying best practices for future reuse in the form of accessible, reusable patterns, templates, and code bases to facilitate meta data capturing and managementManaging a team of software engineers writing new code to build a bigger, better, faster, more optimized HTAP database (using Apache Spark, Apache Arrow, Kafka, and a wealth of other open source data tools)Interacting with exec team and senior engineering leadership to define, prioritize, and ensure smooth deployments with other operational componentsHighly engaged with industry trends within analytics domain from a data acquisition processing, engineering, management perspectiveUnderstand data and analytics use cases across Web3 / blockchains
Skills & Qualifications Bachelor's degree in computer science or related technical field. Masters or PhD a plus6+ years experience engineering software and data platforms / enterprise-scale data warehouses, preferably with knowledge of open source Apache stack (especially Apache Spark, Apache Arrow, Kafka, and others)3+ years experience with Scala and Apache Spark (or Kafka)A track record of recruiting and leading technical teams in a demanding talent marketRock solid engineering fundamentals; query planning, optimizing and distributed data warehouse systems experience is preferred but not requiredNice to have: Knowledge of blockchain indexing, web3 compute paradigms, Proofs and consensus mechanisms... is a strong plus but not requiredExperience with rapid development cycles in a web-based environmentStrong scripting and test automation knowledgeNice to have: Passionate about Web3, blockchain, decentralization, and a base understanding of how data/analytics plays into this"
AWS Data Visualization Engineer,Oxenham Group,United States (Remote),https://www.linkedin.com/jobs/view/3749343486/?eBP=JOB_SEARCH_ORGANIC&refId=urH%2B5plSvPeNh7%2FHe8SOzg%3D%3D&trackingId=SE68wbttq%2BpEq99ESLKS4g%3D%3D&trk=flagship3_search_srp_jobs,3749343486,"About the job
            
 
AWS Data EngineerOur client is a technology services firm implementing solutions for clients across a variety of exciting industries. They are looking to add an AWS Data Engineer to their team to build data solutions around warehousing and visualization:Qualifications Bachelor's or master's degree in computer science, Engineering or a related fieldAWS Certified Big Data - SpecialtyExperience with Amazon QuickSight, Amazon API Gateway, and RedshiftProven experience working as a Data Engineer, preferably in a professional services or consulting environmentStrong proficiency in programming languages such as Python, Java, or Scala, with expertise in data processing frameworks and libraries (e.g., Spark, Hadoop, SQL, etc.)In-depth knowledge of database systems (relational and NoSQL), data modeling, and data warehousing conceptsProficiency in designing and implementing ETL processes and data integration workflows using tools like Apache Airflow, Informatica, or TalendFamiliarity with data governance practices, data quality frameworks, and data security principles"
Senior Azure Data Engineer - Remote | WFH,Get It Recruit - Information Technology,"Elk Grove Village, IL (Remote)",https://www.linkedin.com/jobs/view/3774100198/?eBP=JOB_SEARCH_ORGANIC&refId=urH%2B5plSvPeNh7%2FHe8SOzg%3D%3D&trackingId=rTa72Rk%2FYKtBKY7Nvh9lDg%3D%3D&trk=flagship3_search_srp_jobs,3774100198,"About the job
            
 
As a skilled Azure Data Engineer, you will play a crucial role in designing, implementing, and managing data architecture on the Azure cloud platform. You will collaborate closely with diverse teams to ensure that the data infrastructure aligns with business needs and adheres to industry best practices. If you are a strategic thinker with strong technical expertise, this role offers an exciting opportunity to shape the data landscape and make a significant impact.ResponsibilitiesDesign, develop, and maintain data pipelines using Azure Data Factory and other relevant Azure technologiesOptimize ETL processes for performance and scalability, leveraging Azure services such as Azure Data Factory Mapping Data Flows, Azure Data Lake Analytics, or Azure Synapse PipelinesExtract data from various sources, transform it into a usable format, and load it into Azure data storage solutions such as Azure SQL Database, or Azure Synapse AnalyticsCollaborate with cross-functional teams to understand data requirements and design scalable and efficient ETL processes using Azure servicesIdentify, design, and implement ETL solutions for extraction and integration of data to and from data warehouses and data marts for the purposes of reporting, decision support, and analysisLead the design, development, and deployment of data solutions on the Azure cloud platformDemonstrate expert-level understanding of Azure Data Factory, Azure Synapse, Azure SQL, Azure Data Lake, and Azure App ServiceDesign and build data pipelines using API ingestion and Streaming ingestion methodsPossess knowledge of Dev-Ops processes (including CI/CD) and Infrastructure as codeDemonstrate knowledge of Azure Databricks, Azure IoT, Azure HDInsight + Spark, Azure Stream Analytics, and Power BI (preferable)QualificationsBachelor's degree in Computer Science, Information Technology, or a related fieldA minimum of 2 years' experience in Azure Data FactoryA minimum of 7 years' experience with the Microsoft Azure platform and servicesAzure certifications (e.g., Azure Data Engineer, Azure Solutions Architect) are preferredStrong expertise in data modeling, ETL processes, and database managementProficiency in SQL, Azure SQL, and Azure Data Lake StorageFamiliarity with data warehousing concepts and toolsExcellent problem-solving and communication skillsStrong project management and leadership abilitiesAbility to work collaboratively in cross-functional teamsLocationThis position can be located in Chicago, IL, Pittsburgh, PA, or remotely.Employment Type: Full-Time"
Senior Data Engineer,Wise Skulls,United States (Remote),https://www.linkedin.com/jobs/view/3753079691/?eBP=JOB_SEARCH_ORGANIC&refId=urH%2B5plSvPeNh7%2FHe8SOzg%3D%3D&trackingId=MF7cYiuqLCIrHlorfj5HWQ%3D%3D&trk=flagship3_search_srp_jobs,3753079691,"About the job
            
 
Title: Senior Data EngineerLocation: RemoteDuration: 6 MonthsImplementation Partner: InfosysEnd Client: To be disclosedJd Experience establishing data flow patterns that cover ADLS 2 -> Bronze (preferably via Autoloader w/ event integration from ADLS + automatic schema mapping) -> Silver -> Gold.Well versed in differentiating between datasets destined for simple “data lake” consumption vs datasets that should be incorporated into a dimensional or aggregated model under the gold layer of the medallion architecture.Experience laying the foundation for generic data flows to support our goals around re-use and developer experience.Experience integrating and controlling Databricks, Azure Infrastructure, and other BI Pipeline artifacts with source control under a CI/CD paradigm.Experience with best practices for ad-hoc querying under Databricks (SQL Serverless) and data consumption under Power BI (certified datasets, snapshots, direct query)Experience with the Databricks “Unity Catalog” (we understand the hive metastore that comes out of the box is not compatible with the metastore used for unity catalog, but are interested in using features under Unity Catalog to drive Data Discovery and Governance).Experience migrating existing, on-premise data warehouse to cloud solutions using ADF to support hybrid migration strategies."
Lead Data Engineer - AWS (Work from Home),Aegon,United States (Remote),https://www.linkedin.com/jobs/view/3778274855/?eBP=JOB_SEARCH_ORGANIC&refId=urH%2B5plSvPeNh7%2FHe8SOzg%3D%3D&trackingId=oFIRnFsiwmwGJB77E7fSZw%3D%3D&trk=flagship3_search_srp_jobs,3778274855,"About the job
            
 
The Lead Data Engineer is responsible for the design, architecture and support of systems, services and applications required for the collection, storage, processing, and analysis of all forms of data to enable data-driven decisions and outcomes across the enterprise.Responsibilities Work collaboratively with other engineers, data scientists, analytics teams, and business product owners in an agile environment:Architect, build, and support the operation of AWS Cloud data infrastructure and tools.Design robust, reusable and scalable data driven solutions and data pipeline frameworks to automate the ingestion, processing and delivery of both structured and unstructured batch and real-time streaming data.Lead the development of data APIs and data delivery services to support critical operational processes, analytical models and machine learning applications.Assist the selection and integration of data related tools, frameworks and applications required to expand platform capabilities.Understand and implement best practices in management of enterprise data, including master data, reference data, metadata, data quality and lineage.
Qualifications Bachelor’s degree in computer science, math, engineering, or relevant technical fieldFive years of collective experience in the application of data engineering, data analytics, data warehousing, business intelligence, database administration and data integration concepts and methodologiesFour years of experience architecting, building, and administering big data and real-time streaming analytics architectures in on-premises and cloud environmentsThree years of experience architecting, building and administering large-scale distributed applicationsThree years of experience with Linux operations and development, including basic commands and shell scriptingThree years of experience executing DevOps methodologies and continuous integration/continuous deliveryDemonstrated skills in delivery of high-quality technical project solutions.Expertise in SQL for data profiling, analysis, and extractionFamiliarity with data science techniques and frameworksStrong technical communication skills
Preferred Qualifications Master’s degree in a technical field (e.g. computer science, math, engineering)Understanding of big data and real time streaming analytics processing architecture and ecosystemsExperience with data warehousing architecture and implementation, including hands on experience with source to target mappings and developing ETL codeExperience with advanced analytics and machine learning concepts and technology implementationsRelevant technology or platform certification (AWS, Microsoft, etc.)Software development experience in relevant programming languages (e.g. Java, Python, Scala, Node.js)
Compensation**Please note that the compensation information that follows is a good faith estimate for this position only and is provided pursuant to applicable pay transparency and compensation posting laws. It is estimated based on what a successful candidate might be paid in certain Company locations.**The Salary for this position generally ranges between $98,000 - $130,000 annually. This range is an estimate, based on potential qualifications and operational needs. Salary may vary above and below the stated amounts, as permitted by applicable law.Additionally, this position is typically eligible for an Annual Bonus based on the Company Bonus Plan/Individual Performance and is at the Company’s discretion.Working Conditions Office environmentWork outside of normal business hours may be requiredModerate travel
Apply"
GCP Data Engineer::Fully Remote,TekIntegral,United States (Remote),https://www.linkedin.com/jobs/view/3667473655/?eBP=JOB_SEARCH_ORGANIC&refId=urH%2B5plSvPeNh7%2FHe8SOzg%3D%3D&trackingId=ncOSVlAtCdRJ0WYdMxr4lw%3D%3D&trk=flagship3_search_srp_jobs,3667473655,"About the job
            
 
No H1B or TN visaGCP Data EngineerLocation :Remote (need to be in EST / CST only)Duration : 8 monthsImpt : Need to have solid communication skills We are a team of highly motivated designers and engineers that help our clients at every phase of their cloud journey. If it touches the cloud, involves data, or lives as an application, we do it.Who You AreSelf-motivated and driven with a desire to find solutions and make an impact. You look to dig deeper beyond the surface level of a job profile researching market trends, domain technology, and seek out information to have a solid understanding of the business. You ask for feedback and want to grow and develop in your career.What You Do Collaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.Design, implement and deploy new data models and data processesExperience in data processing using traditional and distributed systems such as Hadoop, Spark, Dataproc, Airflow, etc...Experience designing data models and data warehouses and using SQL and NoSQL database management systems.Develops and maintains scalable data pipelines and builds out new API integrations to support continuing increases in data volume and complexity.Implements processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it.Writes unit/integration tests, contributes to engineering wiki, and documents work.Performs data analysis required to troubleshoot data related issues and assist in the resolution of data issues.Defines company data assets and data models, spark, sparkSQL, and hiveSQL jobs to populate data models.Designs data integrations and data quality framework.
What You Bring BS in Computer Science or a related technical field4+ years experience with Python4+ years of experience with Terraform4+ years experience with Spark (Databricks)4+ years experience with SQL and NoSQL 4+ years of experience with schema design and dimensional data modeling2+ years experience in GCP2+ years experience with Looker or TableauAbility in managing and communicating data warehouse plans to team membersExperience designing, building, and maintaining data processing systems
Required Skills;Active Google Cloud Data Engineer Professional Certification or willingness to get one during first 30 days of the project.Please share your resume at career@tekintegral.com and nkumar@tekintegral.com"
Senior Data Engineer,Tendo,United States (Remote),https://www.linkedin.com/jobs/view/3728848965/?eBP=JOB_SEARCH_ORGANIC&refId=urH%2B5plSvPeNh7%2FHe8SOzg%3D%3D&trackingId=Hx2sVF6rgD9kwh2gIb98cQ%3D%3D&trk=flagship3_search_srp_jobs,3728848965,"About the job
            
 
As a Senior Data Engineer, you will work within the Informatics team and contribute to Tendo’s strategic data engineering solutions by ingesting, transforming, and warehousing healthcare-related data from various sources.You will collaborate with Tendo’s Data Scientists, Product Managers, and Machine Learning Engineers to produce quality data flows and transformations. You will develop tools and solutions to facilitate data integration, data warehousing, and modeling. Your work will enable Data Scientists to experiment and train machine learning models to produce useful insights for Tendo’s customers.The ideal candidate should have a strong background in software engineering, data modeling, data warehousing, ETL pipelines, and database design. The candidate should also have a desire to learn and enrich their knowledge in data science, machine learning, and other related fields.About TendoMake an impact—join our team!We’re a fast-growing, mission-driven company building a culture that enables teams and individuals to thrive. Led by an experienced and proven team, we live by our values and are always on the hunt for motivated people with diverse experiences and backgrounds to help us improve the care journey for patients, clinicians, and caregivers by creating software that provides seamless, intuitive, and user-friendly experiences.If you like working with innovative technologies and want to be part of a growing team that will help transform the healthcare experience, we encourage you to apply today!Responsibilities Build, maintain, and monitor complex ETL/ELT (SSIS) jobs and data extracts.Collaborate with Data Scientists and Business Intelligence Analysts to ensure efficient and effective data processing and analysis.Optimize data infrastructure and processes to ensure optimal performance and scalability.Develop and maintain data documentation and data lineage.Stay current with emerging technologies and industry trends related to data engineering.
Requirements 5+ years of experience in data engineering.Experience with Epic’s data and analytics stack (Cogito).Experience with Microsoft SQL and Azure products.Extensive experience in the design, build, and maintenance of data ETL pipelines.Extensive knowledge of coding in SQL with a focus on data processing.Experience with data and entity relationship modeling to support data warehouses and analytics solutions.Deep understanding of relational and non-relational databases (SQL/NOSQL).Comfortable working with unstructured and semi-structured data (Web scraping).Experience working in a professional software environment using source control (git), an issue tracker (JIRA, Confluence, etc.), continuous integration, code reviews, and agile development process (Scrum/Lean).Working knowledge of basic data privacy and security principles.
Nice to Have Experience with AWS technology stack (S3, Glue, Athena, EMR, etc.).Knowledge of, or experience with, healthcare data standards such as HL7, FHIR, ICD, SNOMED, LOINC.Experience with Delta Lake and/or Databricks.Experience using Apache Spark (PySpark or Scala).Experience with machine learning workflows and data requirements for use with ML frameworks.Experience validating data quality, preferably with test automation.Experience with containerization using Docker.
Base Salary Range$127,500-$172,500This salary range is offered with the understanding that final compensation is based on a number of factors including geography and experience. Tendo also offers an equity package, annual bonuses, and benefits.BenefitsFor full time employees, Tendo also offers full health benefits (medical, dental, and vision), flexible spending and health savings accounts, company paid life insurance, company paid short-term and long-term disability, company equity, voluntary benefits, 401(k), company paid holidays, flexible time off, and an employee wellness program (“Breathe”).Tendo is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, sex, sexual orientation, gender identity or expression, religion, national origin or ancestry, age, disability, marital status, pregnancy, protected veteran status, protected genetic information, political affiliation, or any other characteristics protected by local laws, regulations, or ordinances."
Senior Data Engineer,"Changeis, Inc.","Arlington, VA (Remote)",https://www.linkedin.com/jobs/view/3759558676/?eBP=JOB_SEARCH_ORGANIC&refId=urH%2B5plSvPeNh7%2FHe8SOzg%3D%3D&trackingId=DwiopjyqANrq3ZjmuU4KVQ%3D%3D&trk=flagship3_search_srp_jobs,3759558676,"About the job
            
 
Changeis, Inc. is an award-winning 8(a) certified, woman-owned small business that provides management consulting and engineering services to the public sector. Changeis' work has resulted in the successful execution of numerous programmatic initiatives, development of acquisition-sensitive deliverables, and establishment of a variety of long-term innovative strategic priorities for its customers. Changeis focuses on delivering unparalleled expertise in the areas of strategy and transformation management, investment analysis and acquisition management, governance, and innovation management. Inc. magazine has ranked the management consulting firm, Changeis Inc., among the top 1000 firms on its 35th annual Inc. 5000, the most prestigious ranking of the nation's fastest-growing private companies. Changeis offers a full benefit package that includes medical, dental, and vision, short and long term disability, retirement plan with immediate vesting and company match, and a generous annual leave plan.The Senior Data Engineer will partner with a Federal Agency Office of Human Resources, focusing on essential areas such as business management, strategic planning, and decision-making. By developing and maintaining data architectures, engaging in acquisition/contract management, and applying expertise in information technology, data analytics, and knowledge management, the Senior Data Engineer will significantly contribute to the optimization and innovation of organizational processes. The Senior Data Engineer will collaborate with product design and engineering teams to understand their needs, and then research and devise innovative statistical models for data analysis. By communicating findings to all stakeholders and using analytics for meaningful insights, they will enable smarter business processes and stay abreast of current technical and industry developments.Roles And Responsibilities  Identifying new datasets and integrate them, focusing on enhancing product capabilities.  Conducting experiments with analytical techniques to solve complex problems across various domains.  Recognizing relevant data sources and gather structured and unstructured data to meet client needs.  Developing and running ETL (Extract, Transform, Load) processes to manage data flow and ensure data quality.  Supporting the development of data platforms (e.g., AWS data lake), collaborating with teams to ensure alignment with organizational goals.  Designing algorithms and models to mine big data, perform data and error analysis, and clean and validate data.  Analyzing data for trends and patterns, interpret insights, and apply them to meet clear objectives.  Collaborating with software developers and machine-learning engineers to implement models in production environments. 
Requirements  15+ years of experience in data analysis, data management, data science, or operations research.  7 years of experience in data and software engineering, data analytics, and machine learning, including design and implementation of end-to-end production level software and/or data engineering solutions.  5 years of experience working with cloud computing and database services such as Amazon Web Service (AWS).  5 years of professional software engineering experience in at least one of the following: Python, JavaScript (e.g., Node.js, React, Vue.js), or C#.  Experience with data ETL (Extract, Transform, and Load).  Experience with the ELK Stack (Elastic Search, Logstash, and Kibana).  Ability to formulate business needs and translate them into technical functional and non-functional requirements.  Experience with Container-based technologies such as Docker, Kubernetes, and similar technologies.  Experience in data modeling, including transactional and data warehouse design.  Prefer candidates holding active AWS certifications"
"Data Engineer (AWS, Azure, GCP)",CapTech,"Philadelphia, PA (Remote)",https://www.linkedin.com/jobs/view/3774801032/?eBP=JOB_SEARCH_ORGANIC&refId=ifnjjNsg7HeCdebP5C%2B%2FEw%3D%3D&trackingId=ZnUm1yhFg9yBWWhj0REi%2BQ%3D%3D&trk=flagship3_search_srp_jobs,3774801032,"About the job
            
 
Company DescriptionCapTech is an award-winning consulting firm that collaborates with clients to achieve what’s possible through the power of technology. At CapTech, we’re passionate about the work we do and the results we achieve for our clients. From the outset, our founders shared a collective passion to create a consultancy centered on strong relationships that would stand the test of time. Today we work alongside clients that include Fortune 100 companies, mid-sized enterprises, and government agencies, a list that spans across the country.Job DescriptionCapTech Data Engineering consultants enable clients to build and maintain advanced data systems that bring together data from disparate sources in order to enable decision-makers. We build pipelines and prepare data for use by data scientists, data analysts, and other data systems. We love solving problems and providing creative solutions for our clients. Cloud Data Engineers leverage the client’s cloud infrastructure to deliver this value today and to scale for the future. We enjoy a collaborative environment and have many opportunities to learn from and share knowledge with other developers, architects, and our clients.Specific responsibilities for the Data Engineer – Cloud position include:  Developing data pipelines and other data products using Amazon Web Services (AWS), Microsoft Azure, or Google Cloud Platform (GCP) Advising clients on specific technologies and methodologies for utilizing cloud resources to efficiently ingest and process data quickly Utilizing your skills in engineering best practices to solve complex data problems Collaborating with end users, development staff, and business analysts to ensure that prospective data architecture plans maximize the value of client data across the organization. Articulating architectural differences between solution methods and the advantages/disadvantages of each 
QualificationsTypical experience for successful candidates includes:  Experience delivering solutions on a major cloud platform Ability to think strategically and relate architectural decisions/recommendations to business needs and client culture Experience in the design and implementation of data architecture solutions A wide range of production database experience, usually including substantial SQL expertise, database administration, and scripting data pipelines Ability to assess and utilize traditional and modern architectural components required based on business needs. A demonstrable ability to deliver production data pipelines and other data products. This could be hands on experience, degree, certification, bootcamp, or other learning. 
SkillsSuccessful candidates usually have demonstrable experience with technologies in some of these categories: Languages: SQL, Python, Java, R, C# / C++ / C Database: SQL Server, PostgreSQL, Snowflake, Redshift, Aurora, Presto, BigQuery, Oracle DevOps: git, docker, subversion, Kubernetes, Jenkins Additional Technologies: Spark, Databricks, Kafka, Kinesis, Hadoop, Lambda, EMR Popular Certifications: AWS Cloud Practitioner, Microsoft Azure Data Fundamentals, Google Associate Cloud Engineer
Additional InformationWe want everyone at CapTech to be able to envision a lasting and rewarding career here, which is why we offer a variety of career paths based on your skills and passions. You decide where and how you want to develop, and we help get you there with customizable career progression and a comprehensive benefits package to support you along the way. Alongside our suite of traditional benefits encompassing generous PTO, health coverage, disability insurance, paid family leave and more, we’ve launched extended benefits to help meet our employees’ needs. CapFlex – Employee-first mentality that supports a remote and hybrid workforce and empowers daily flexibility while servicing our clientsLearning & Development – Programs offering certification and tuition support, digital on-demand learning courses, mentorship, and skill development pathsModern Health –A mental health and well-being platform that provides 1:1 care, group support sessions, and self-serve resources to support employees and their families through life’s ups and downsCarrot Fertility –Inclusive fertility and family-forming coverage for all paths to parenthood – including adoption, surrogacy, fertility treatments, pregnancy, and more – and opportunities for employer-sponsored funds to help pay for careFringe –A company paid stipend program for personalized lifestyle benefits, allowing employees to choose benefits that matter most to them – ranging from vendors like Netflix, Spotify, and GrubHub to services like student loan repayment, travel, fitness, and moreEmployee Resource Groups – Employee-led committees that embrace and incorporate diversity and inclusion into our day-to-day operationsPhilanthropic Partnerships – Opportunities to engage in partnerships and pro-bono projects that support our communities. 401(k) Matching – Generous matching and no vesting period to help you continue to build financial wellness
CapTech is an equal opportunity employer committed to fostering a culture of equality, inclusion and fairness — each foundational to our core values. We strive to create a diverse environment where each employee is encouraged to bring their unique ideas, backgrounds and experiences to the workplace. For more information about our Diversity, Inclusion and Belonging efforts, click HERE. As part of this commitment, CapTech will ensure that persons with disabilities are provided reasonable accommodations. If reasonable accommodation is needed to participate in the job application or interview process, to perform essential job functions, and/or to receive other benefits and privileges of employment, please contact Laura Massa directly via email lmassa@captechconsulting.com.At this time, CapTech cannot transfer nor sponsor a work visa for this position. Applicants must be authorized to work directly for any employer in the United States without visa sponsorship."
Senior Data Engineer (Azure),iManage,"Illinois, United States (Remote)",https://www.linkedin.com/jobs/view/3767003017/?eBP=JOB_SEARCH_ORGANIC&refId=ifnjjNsg7HeCdebP5C%2B%2FEw%3D%3D&trackingId=SR1e4c0QgY3SPfOVpXqEJg%3D%3D&trk=flagship3_search_srp_jobs,3767003017,"About the job
            
 
We offer a flexible working policy that supports the health and well-being of our iManage employees. As an organization, we value collaborating and learning from our peers in person, while providing the necessary flexibility for our employees to have a meaningful work-life balance. Please reach out to learn more.Being a Senior Data Engineer at iManage Means…You are excited about data and believe in the democratization of data to support data driven decision-making. You will partner with our Information Technology team to implement, support, and extend our Enterprise Data Lake hosted on Azure and built using Azure Synapse. You will gather requirements from iManage business units and craft solutions which provide access to critical business data. You will develop data models and data pipelines for our Enterprise Data Lake, and provide integration with BI platforms and tools such as Totango and Power BI. You are passionate about lakehouse architecture and have experience using Delta Lake and bronze, silver, and gold data lake design.Here is what one of our leaders, Senior Director of Enterprise Data and Integration Engineering ( “As a Senior Data Engineer on our team, you will get the opportunity to showcase your expertise and make a real difference across the organization. You will be part of a truly collaborative team that is passionate about delivering quality solutions. You will be the in-house expert in the data models of multiple, disparate enterprise SaaS systems and utilize your wealth of knowledge to provide recommendations and solutions for consolidation, transformation, and integration of the disparate data sources.”iM Responsible For… Modeling, managing, and reporting of data stored in Azure Data Lake.Gathering data requirements from various business units and translating these requirements into data models. Using Python, PySpark, and system specific APIs to extract, transform, store and analyze data from a variety of systems.Data modeling, defining data pipelines, and integrations necessary to present data in BI platforms such as Totango, or BI tools like Power BI.Identifying and modeling all current disparate data sources and the data flows between these data sources. Analyzing current repositories and proposing changes to data repositories and data flows to better support company objectives for the measurement of user experience and customer success.Understanding the business needs of data integration and governance from disparate systems to drive the enhancement of the enterprise data lake. Applying best practices to ensure the security and privacy of the data repositories.Ensuring data repositories meet company standards for storage of PII.
iM Qualified Because I Have... A Bachelor’s degree or higher in Computer Science or related field.2-3 years of experience working with data in a business setting.Proficiency in data extraction and transformation utilizing Spark, Python and REST APIs.Proficiency in data reporting and data integration utilizing Transact-SQL, SQL Views, REST APIs, or other BI Tools.Experience designing data pipelines with a cloud-native mindset using Azure or AWS.Experience ingesting data from SaaS solutions and other services via API or other related technologies.Experience with Azure Synapse or adjacent technologies like Databricks.A passion to work collaboratively within a team using an agile framework. Commitment to understanding data requirements and delivering high quality, scalable solutions that meet those requirements.High attention to detail.A creative mindset with a desire to explore new technologies and create innovative solutions.
Bonus Points If I Have… Familiarity with delta lake or lakehouse technologies.Familiarity with Microsoft Azure Event Hub or a similar event streaming solution.A background with relational databases and data warehouse design using star schemas.Experience with cloud-based data models for business solutions like Salesforce, Zendesk, and NetSuite.
Don't meet every qualification listed above? Studies show that women and people of color are less likely to apply to jobs unless they meet all qualifications. At iManage, we are committed to building a diverse and inclusive environment and encourage everyone to show up as their full authentic selves. We welcome those that come with a growth mindset and a hunger for learning; so, if you are excited about this role but your past experience doesn't align perfectly with every qualification, we encourage you to apply anyways!iM Getting To… Join a supportive, experienced team with an inclusive, encouraging, and vibrant culture.Have flexible work hours that allow me to balance my ‘me time’ with my work commitments.Collaborate in a modern open plan workspace, with a gaming area, free snacks, drinks and regular social events.Focus on impactful work, solving complex, real challenges utilizing the latest technologies and protocols.Own my career path with our internal development framework. Ask us more about this!Learn new skills and earn certifications with access to unlimited courses in LinkedIn Learning.Join an innovative, industry leading SaaS company that is continuing to grow & scale!
iManage Is Supporting Me By... Creating an inclusive environment where I can help shape the culture not just by fitting in, but by adding to it.Providing a market competitive salary that is applied through a consistent process, equitable for all our employees, and regularly reviewed based on industry data.Rewarding me with an annual performance-based bonus.Offering comprehensive Health/Vision/Dental/Life Insurance, and a 401k Retirement Savings Plan with a company match up to 4%. Giving access to HealthJoy, a healthcare concierge service, to help me maximize my health benefits.Granting enhanced leave for expecting parents; 20 weeks 100% paid for primary leave, and 10 weeks 100% paid for secondary leave. Providing me with a flexible time off policy to take the time off that I need. Be it for vacation, volunteering, celebrating holidays, spending time with family, or simply taking time to recharge and reset.Caring for my mental health and well-being with multiple company wellness days and free access to the Healthy Minds app for mindfulness, meditation and more.
About IManage…iManage is dedicated to Making Knowledge WorkTM. Over one million professionals across 65+ countries rely on our intelligent, cloud-enabled, secure knowledge work platform to uncover and activate the knowledge that exists inside their business content and communications.We are continuously innovating to solve the most complex professional challenges and enable better business outcomes; Our work is not always easy but it is ambitious and rewarding.So we’re looking for people who love a challenge. People who are happiest when they’re solving problems and collaborating with the industry’s best and brightest. That’s the iManage way. It’s how we do things that might appear impossible. How we develop our employees’ strengths and unlock their potential. How we find meaning in everything we do.Whoever you are, whatever you do, however you work. Make it mean something at iManage.iManage provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.Learn more at:Please see our privacy statement for more information on how we handle your personal data:Powered by JazzHRUuYhGbq2Nf"
Data Engineer with SAP HANA || Fully Remote,Noralogic Inc,United States (Remote),https://www.linkedin.com/jobs/view/3760339539/?eBP=JOB_SEARCH_ORGANIC&refId=ifnjjNsg7HeCdebP5C%2B%2FEw%3D%3D&trackingId=OGzhP40eSIhIvQTslxFDgw%3D%3D&trk=flagship3_search_srp_jobs,3760339539,"About the job
            
 
Data Engineer with SAP HANA USA Fully Remote (Must support in PST hours)Long Term ContractRoles And ResponsibilitiesThe Candidate should have hands on experience 6 to 10 years in SAP HANA design, development, coding, fixes with experience in developing complex transformation logic. Also with have hands on experience in Tableau and SAP BO will added advantage.Technical And Professional Requirements Should adhere to Data Architecture, Modelling and Coding guidelinesShould understand functional requirementsPreparation of Design documents and/or Technical DocumentsShould have Experience into HANA Modelling - Calculation views, Stored procedures, Scalar & Table functions, Performance tuning techniques XS Development - XSO Data, XSJS Services, Debugging DS - Job development end to end including Transformation, DS Scripting, Consume External services Mandatory Skills SAP Native HANA, Implementation, Configuration, Safe Agile Methodology
Preferred Skills SAP HANA XS / Native HANA->SAP HANA XS / Native HANASAP HANA XS / Native HANAMust have experience with SAP Native HANA SQL scriptCandidate should have hand of experience Data Analysis and Analyst knowledge with writing functional and technical specifications.Reviewing developer designs and sometimes designing solutions for an enterprise data warehouse built using SAP HANA ( Native)Converting ad hoc and proof of concept solutions/models into a production ready solution in SAP HANA ( Native)SAP HANA 2.0 configuration, architecture, reporting/visualization, predictive analytics, and data/solution modelling.Must have Project experience in advanced modeling concepts including Analytic Views, Attribute Views, Hierarchies, Creating Restricted & Calculated Columns, Filter Operations, Variables, Creating Calculation Views, SAP HANA SQL, SQL Script and Procedures, Currency Conversion, Turning Business Rules into Decision Tables
Nice To Have Below Teradata, Snowflake, Oracle, Tableau and SAP Business Objects (BO) will add advantageCandidate should have good analytical skills and problem-solving skills. 
Additional Responsibilities Ability to develop value-creating strategies and models that enable clients to innovate, drive growth and increase their business profitabilityGood knowledge on software configuration management systemsAwareness of latest technologies and Industry trendsLogical thinking and problem-solving skills along with an ability to collaborateUnderstanding of the financial processes for various types of projects and the various pricing models availableAbility to assess the current processes, identify improvement areas and suggest the technology solutionsOne or two industry domain knowledgeClient Interfacing skills
Regards,Priti KumariTrainee Recruiter Noralogic Inc.109 E 17th St, Cheyenne WY 82001+1.307-274-3112(*************) |www.noralogic.comUSA: WY, MD, NJwww.noralogic.comMexico: Guadalajara, MonterreyIndia: Noida UP**WBE and MBE company**** ISO 9001:2015****WY Top 50 Minority owned growing company**"
AWS Developer (data Engineer) and Date Governance Manager,IVY TECH SOLUTIONS INC,"Maryland City, MD (Remote)",https://www.linkedin.com/jobs/view/3667176584/?eBP=JOB_SEARCH_ORGANIC&refId=ifnjjNsg7HeCdebP5C%2B%2FEw%3D%3D&trackingId=HnoQ8f5RPH8nl8yuKVEZdw%3D%3D&trk=flagship3_search_srp_jobs,3667176584,"About the job
            
 
HI,Kindly let me know if you have a suitable fit for the following positionThanksAWS Developer (data Engineer) and Date Governance ManagerLocation: MDDuration: 12+MonthsInitially RemotePlease send the resume to  or 847- 350-1008Skills:(data Engineer) and Date Govervance ManagerCharan Kumar | IVY Tech Sols Inc.3403 N Kennecott Avenue, Suite B&C Arlington Heights, IL 60004PH.( Direct: (847) 350-1008   |Gtalk : charan.ivytech|
Powered by JazzHRmHLW2CY4MA"
Remote Data Engineer/ Only W2 Candidates,Vaco,"The Woodlands, TX (Remote)",https://www.linkedin.com/jobs/view/3775998677/?eBP=JOB_SEARCH_ORGANIC&refId=ifnjjNsg7HeCdebP5C%2B%2FEw%3D%3D&trackingId=Navf%2BNpZqTSHH6Uh3OEClA%3D%3D&trk=flagship3_search_srp_jobs,3775998677,"About the job
            
 
Remote Data EngineerDuration: 12 Months Rate: $65/hr on w2.Responsibilities Prototype, build, deploy and manage data engineering pipelines.Contribute to the design and creation of high-quality solutions.Work with other data engineers, business intelligence and machine learning experts to solve real-life, challenging business problems.
Required Qualifications 3+ years of relevant professional experienceDeep experience with data engineering, big data and analytical technologies using Azure cloud-based data platforms.Experience with batch and real-time data processing tools and technologies: Azure Data Factory, Databricks/Spark, Azure Synapse/DW, Azure Analysis ServicesExtensive experience in SQLKnowledge of distributed data solutions, storage systems and columnar databasesFamiliarity with Continuous Integration/Continuous Deployment, & GitFluent in spoken and written EnglishKnowledge about Agile development methods like Scrum and Kanban
Nice to Have Python (3.x), PySpark, R, KafkaKnowledge of key machine learning concepts & ML frameworks (like scikit-learn, H2O.ai, Keras, etc.)"
Senior Data Engineer,When I Work,"Minneapolis, MN (Remote)",https://www.linkedin.com/jobs/view/3758250587/?eBP=JOB_SEARCH_ORGANIC&refId=ifnjjNsg7HeCdebP5C%2B%2FEw%3D%3D&trackingId=tJD3Dubl73%2FidjMLaRvhDA%3D%3D&trk=flagship3_search_srp_jobs,3758250587,"About the job
            
 
When I Work is a remote first company. We are open to hiring candidates in the continental US and Ontario, Canada. If an onsite location is important to you in your search, you are welcome to work from our Minneapolis HQ office. 
Who We AreWe help hourly teams get shift done. At When I Work, everything we do starts with a mission to make shift work awesome. We deliver on that mission by making every piece of hourly workforce management - scheduling, time tracking, shift trading, team messaging, and more - easy and straightforward for managers and employees alike.The Data and RevOps team at When I Work is a group of inquisitive and driven individuals who love solving problems using data. We have built a best-in-class data environment and fuel insights throughout the organization on our product and customers. We work collaboratively together, invest in our processes and tooling, and move slow to move fast. We focus on projects that will have a big impact to the company and work to enable anyone to be a savvy data user.What You'll DoOver the last few years we have been building out a best-in-class data environment that we've used to transform When I Work into a data-driven company. You will be a key contributor in continuing to grow and mature this environment as well as develop and sustain data projects that will have significant impact on our company and our users. Proactively identify opportunities to improve and update data platform infrastructure and research new technologies and strategies Design, build and implement tools aimed at allowing business users to collect and analyze data in an efficient and effective wayDesign, build and maintain integrations between our internal data platform and 3rd party tools utilized throughout the company Develop and manage ETLs and data pipelinesCreate data products for consumption by internal When I Work team membersBe part of a team that owns all aspects of its service delivery -- from cloud infrastructure, to application code, to operations
Our Technology Stack We use a lot of different technologies to get the job done, and each member of our team brings their own mix of technology experience. If you have familiarity with even a few of these (or equivalents), you could make a valuable contribution: Python, Go, SQL, Terraform, Jupyter, Git, GitLab, Spark, Flink, Presto, Kafka, MySQL, NoSQL, Kubernetes, DBT, Prefect, Airflow, lots of AWS(EC2, EKS, Lambda, S3, RDS, DynamoDB, Aurora, Redshift, Athena, EMR, CloudSearch, Kinesis, API Gateway).Who You AreYou are a programmer who is excited by data and its endless possible use cases. Someone that enjoys creating tools and infrastructure to empower your peers. Collaboration and teamwork are a must, but you also have the ability to work independently when needed to get things done. Above all, you are driven to learn and a motivated problem solver who wants to help tackle the new and interesting challenges that we encounter as a fast-growing startup.Experience And Skills Needed 3+ years of experience in data engineeringYou have strong programming fundamentalsYou are comfortable with agile software processesYou have experience with multiple programming languages (Python, SQL, etc.)Comfortable working with APIs/WebhooksYou have significant experience working with structured and unstructured dataYou have significant experience with cloud computing environments and infrastructureYou are a proponent of DevOps and enthusiastic about DataOpsYou practice empathy and kindness, and you look to help others
What Would Be Awesome To Have Advanced Python and data package (Numpy, Pandas, etc.) skillsYou are comfortable with different data modeling techniques and have experience with a data warehouse platform (Redshift, Snowflake, etc.)You have experience working with message queues and event buses to collect and process data in near real timeUnderstanding & perspective on data catalogs and schema registries
What's In It For You Professional development allowancePaid parental leaveMedical benefits - employee premiums paid 100% by When I WorkDental benefits- employee premiums paid 100% by When I WorkPaid vacation and holidaysFlexible work environment401K MatchRemote first culture including home office set-up stipend and ongoing telecommuter stipendCasual dress codeDynamic and dedicated team
We believe actions speak louder than words. Every encounter with our people and products should be memorable and helpful. Challenges are exciting, failure is how we learn, and we all have an entrepreneurial spirit. Building an inclusive and equitable workplace isn't lip service. We invest our time and our money in organizations that are not only working to diversify the current jobscape, but also investing in the future of talent. We're motivated by a strong, innovative, and passionate work culture and we're constantly searching for ways to improve and get shift done.Whether you're a perfect match or not, if it sounds like a good fit, we encourage you to apply.The tech industry is notorious for its lack of diverse representation, and we're aware of the research showing that historically underrepresented groups are less likely to apply to a job if they don't believe that they meet all of the criteria. Are you hesitant to submit an application because you're not sure if you check every box? Apply anyway! We would love to hear from you and figure out what you can add to the culture here at When I Work. We'd love to talk to you! Please submit the following to apply: Resume (including months/years of employment for each position). Cover letter including: an overview of your existing experience a convincing reason why you'd like to work at When I Work. Must already be authorized to work in the United States or Canada on a full-time basis for any employer."
Data Analytics Engineer,Motion Recruitment,"Philadelphia, PA (Remote)",https://www.linkedin.com/jobs/view/3762986218/?eBP=JOB_SEARCH_ORGANIC&refId=ifnjjNsg7HeCdebP5C%2B%2FEw%3D%3D&trackingId=1KgL8muCb5YVJ6tchfihag%3D%3D&trk=flagship3_search_srp_jobs,3762986218,"About the job
            
 
We are working with a company who run a people search engine that utilizes deep web crawlers to aggregate data. Their main goal is to reconnect friends, reunite families, prevent fraud, and more. They are looking to hire Data Engineer, Analytics.This candidate can be primarily remote but must be located close enough to the city of Philadelphia to travel to the office 2-3 times a month.Required Skills & Experience 2 + yrs of experience in Data Engineering Software Development background SQL (Can do simple code commands) NoSQL Databases Python AWS - EMR, RDS, Redshift, Kinesis, etc 
What You Will Be DoingTech Breakdown 40% SQL/NoSQL 40% AWS 20% Python 
Daily Responsibilities 100% Data Engineering/Analytics 
The Offer Bonus eligible 
You Will Receive The Following Benefits Medical Insurance Dental Benefits Vision Benefits Paid Time Off (PTO) 
Applicants must be currently authorized to work in the US on a full-time basis now and in the future.Posted By: Caroline Stranieri"
Principal Data Engineer,MD Anderson Cancer Center,"Houston, TX (Remote)",https://www.linkedin.com/jobs/view/3766419805/?eBP=JOB_SEARCH_ORGANIC&refId=ifnjjNsg7HeCdebP5C%2B%2FEw%3D%3D&trackingId=WN%2Fq0LgjOA2%2Bu8PX5ZkXWQ%3D%3D&trk=flagship3_search_srp_jobs,3766419805,"About the job
            
 
The Principal Data Engineer in the area of Data Analytics & Delivery is a pivotal role in the Enterprise Data Engineering & Analytics Department in operationalizing critical data and analytics for MD Anderson's digital business initiatives. The Principal Data Engineer manages business requirements gathering, end-to-end solution planning and optimizes data analytics delivery within the Context Engine. The Principal Data Engineer partners with other Enterprise Data Engineering & Analytics teams to manage & build analytics deliverables for production use by our key data and analytics consumers.The Principal Data Engineer also manages and coordinates data analytics delivery activities in compliance with data governance processes and data security requirements. This results in enabling faster data delivery, integrated data reuse and vastly improved time-to-solution for MD Anderson data and analytics initiatives.The Principal Data Engineer role requires working creatively and collaboratively with IS and Institutional leaders across the enterprise. It involves evangelizing effective data accessibility practices and promoting better understanding of data and analytics. The Principal Data Engineer partners closely with teams across MD Anderson, including Enterprise Development & Integration and Enterprise Data Science departments in the build out and delivery of end-to-end analytic solutions through the Context Engine Framework.Data Engineering - End-to-End Solution Delivery  Lead/Communicate/Participate End-to-end solution delivery that increases information capabilities and realizes data value across the institution. End-to-End solutions include build out of data sources and tools across the Context Engine framework by integrating data governance processes through data ingestion, ingress, egress, curation, pipeline build, data transformation and modeling steps. Incorporating highly integrated data governance processes that consistently tracking data provenance, security, data quality and ontology as well as through to data visualization and insights. Lead/Communicate/Participate in existing end-to-end data pipelines consisting of a series of stages through which data flows (for example, from data sources or endpoints of acquisition to integration to consumption for specific use cases). Lead/Communicate/Participate and incorporate data governance and metadata management processes into the data ingestion, curation and pipeline building efforts. Lead/Promote Data Analytics & Delivery efforts and manage relationships with stakeholders across the organization. This includes proactively communicating with stakeholders and prioritizing work for the team. Drive and lead data requirements for various end-to-end analytics deliverables to ensure we are delivering what is needed, not only what is requested. Lead/Communicate/Participate and implement complex data analytics deliverables, including data analysis, report requests, metrics, extracts, visualizations, projects or dashboards in a timely manner by leveraging tools and methodologies in line with the Context Engine Strategy. Lead/Communicate/Perform complex problem solving and formulation and testing and analysis of data. Designs queries using structure query language and NoSQL. Collaborate with other data engineers on integration efforts. Promote and ensure institutional data management strategies.
Standards, Testing and Maintenance  Manage, coordinate and adhere to standard operating procedures set by IS division as well as all MDA policies and maintain build standards (data steward / governance oversight sign off) for support of MDA Institutional data strategy including Context Engine. Manage Documentation preparation as needed for the implementation of enhancements or new technology Manage & follow documented change control processes and may perform change control audits Manage & perform quality control and testing and review the build of other analysts to ensure that solutions are technically sound Oversee analytics system updates/new releases for assigned modules Manage and execute the adherence to regulatory requirements, quality standards and best practices for systems and processes, and collaborate with internal and external stakeholders Lead and/or participate in after-hours application support and downtime procedures
Educate and Train  Lead, promote & train counterparts, such as data scientists, data analysts, LOB users or any data consumers, in data pipelining and preparation techniques, which make it easier for them to integrate and consume the data they need for their own use cases. Lead, plan & establish training plans for various systems in the Context Engine Tools suite and develop curricula in partnership with the MDA Training team and EDEA system experts. Provide institutional, department and one-on-one training on EDEA deliverables. Coach and provide advice, guidance, encouragement, constructive feedback and transfer knowledge to less experienced team members across OneIS and the institution. Manage liaison relationships with customers and OneIS to provide effective technical solutions and customer service.
OneIS  To provide innovative, quality, and sustainable IT solutions and services. Our success is driven by our people through Integrity and Trust, Partnership, and Quality. Promotes trust, respect, support, and honestly with customers and each other. Commits to being a good partner focused on building productive, collaborative, and trusting relationships with our customers and each other. Models a commitment to excellence and strives to continually improve. Achieves desired outcomes, usability, and value that exceed expectations of others and our own.
Other duties as assignedEducation Required: Bachelor's degree.Preferred Education: Master's Level DegreeCertification Required: Must obtain at least one Epic Data Model certification (Clinical, Access, or Revenue) issued by Epic within 180 days of date of entry into job.Preferred Certification: the Access Data Model or the Clinical Data Model.Experience Required: Seven years of relevant information technology experience. May substitute required education with years of related experience on a one to one basis. With preferred degree, five years of experience required.Preferred Experience: Epic Cogito Analytics ExperiencePrior data warehouse and business intelligence solutions experience.Healthcare industry experience.Web intelligence experiencePrior experience in building Foundry data pipelinesIt is the policy of The University of Texas MD Anderson Cancer Center to provide equal employment opportunity without regard to race, color, religion, age, national origin, sex, gender, sexual orientation, gender identity/expression, disability, protected veteran status, genetic information, or any other basis protected by institutional policy or by federal, state or local laws unless such distinction is required by law. http://www.mdanderson.org/about-us/legal-and-policy/legal-statements/eeo-affirmative-action.htmlAdditional Information  Requisition ID: 163593  Employment Status: Full-Time  Employee Status: Regular  Work Week: Days  Minimum Salary: US Dollar (USD) 119,500  Midpoint Salary: US Dollar (USD) 149,500  Maximum Salary : US Dollar (USD) 179,500  FLSA: exempt and not eligible for overtime pay  Fund Type: Hard  Work Location: Remote  Pivotal Position: No  Referral Bonus Available?: Yes  Relocation Assistance Available?: Yes  Science Jobs: No"
"Data Engineer - Remote (But prefer candidates from MN/TX/ San Fransico, CA)",Dice,United States (Remote),https://www.linkedin.com/jobs/view/3781601433/?eBP=JOB_SEARCH_ORGANIC&refId=ifnjjNsg7HeCdebP5C%2B%2FEw%3D%3D&trackingId=KpVnh4OA9at%2B3IaTm9q%2B5g%3D%3D&trk=flagship3_search_srp_jobs,3781601433,"About the job
            
 
Dice is the leading career destination for tech experts at every stage of their careers. Our client, Scadea Solutions Inc, is seeking the following. Apply via Dice today!Role: Data EngineerLocation: Remote (with preference for candidates in MN, TX or San Francisco)Duration: 6+ months Contract W2 RoleJob Description: Support for on-prem to cloud migration.Strong need for candidates with Spark/Scala or Spark/SQL skills. Azure exp requiredKubernetesScalaSparkPythonSynapse
They will be building Medallion architectureData Engineer - Remote (But prefer candidates from MN/TX/ San Fransico, CA)"
REMOTE: AWS Data Engineer,Stellar Professionals,United States (Remote),https://www.linkedin.com/jobs/view/3661713356/?eBP=JOB_SEARCH_ORGANIC&refId=ifnjjNsg7HeCdebP5C%2B%2FEw%3D%3D&trackingId=g6sMZXb6tzfGkyhdMQkD0g%3D%3D&trk=flagship3_search_srp_jobs,3661713356,"About the job
            
 
Applicant must have 5 years of relevant experience with the following:  Bachelor's or master's degree in computer science, Data Engineering, or a related field.Professional experience as a Data Engineer, with a focus on AWS data services and technologies.Strong expertise in designing and implementing ETL processes using AWS Glue, AWS Lambda, Apache Spark, or similar technologies.Proficient in programming languages such as Python, Scala, or Java, with experience in writing efficient and maintainable code for data processing aHands-on experience with AWS data storage services like Amazon S3, Amazon Redshift, or Amazon DynamoDB.In-depth understanding of data modeling, data warehousing, and data integration concepts and best practices.Familiarity with big data technologies such as Hadoop, Hive, or Presto is a plus.Solid understanding of SQL and experience with database technologies like PostgreSQL, MySQL, or Oracle.Excellent problem-solving skills, with the ability to analyze complex data requirements and design appropriate solutions.Strong communication and collaboration skills, with the ability to work effectively in a team-oriented environment"
"Senior Data Engineer, Operations",Sciata,United States (Remote),https://www.linkedin.com/jobs/view/3685815597/?eBP=JOB_SEARCH_ORGANIC&refId=ifnjjNsg7HeCdebP5C%2B%2FEw%3D%3D&trackingId=xzFz9Wdmk6eR9A63dKBJtQ%3D%3D&trk=flagship3_search_srp_jobs,3685815597,"About the job
            
 
As a Data Engineer at Spokeo, you will be responsible for developing, optimizing, and maintaining the ETL data pipeline. This involves working with infrastructure built in AWS, including Spark EMR, S3, and DynamoDB. This role will help build statistical tools, develop unit and stress tests, and create automation surrounding the orchestration of the ETL data pipeline. Responsibilities, including estimated time of how much of an average week is spent doing each item. This is subject to change: 25% - Build infrastructure and automation for the extraction, preparation, and loading of data from various sources25% - Create unit and stress test components to monitor technical performance and ensure identified issues are resolved10% - Build and maintain data-backed tools to give data insight and capture key metrics20% - Automate and integrate new components into the data pipeline.10% - Use best practices for data governance, quality, cleansing, and other ETL-related activities.10% - Maintain technical documentation
Requirements: 5+ years of development experience in data engineering and hands-on scripting experience with Python3+ years of professional experience working in big data ecosystems, preference for Spark3+ years of professional experience working with dataflow management tools, such as AirflowPreference for development experience in highly scalable, distributed systems and cluster architectures. AWS is highly preferred,Familiarity with complex NoSQL databases (e.g., DynamoDB, Cassandra, Elasticsearch, etc.)Prior experience working with large data sets (>100M+ records)B.S. preferred in Computer Science, Information Systems, or related fields (foreign education equivalent accepted)

Desired Skills and Experience
                DATA ENGINEERING"
Adobe CDP Senior Data Engineer,First Soft Solutions LLC,United States (Remote),https://www.linkedin.com/jobs/view/3645676529/?eBP=JOB_SEARCH_ORGANIC&refId=ifnjjNsg7HeCdebP5C%2B%2FEw%3D%3D&trackingId=o5afMDZ0mh4ki5iNJFrLGw%3D%3D&trk=flagship3_search_srp_jobs,3645676529,"About the job
            
 
Hi,please share Adobe CDP Data Engineer profilesRemote/C2CJDeering and people management skills. A successful candidate will be a highly motivated, collaborative individual; motivated to achieve results in a fast-paced environment.In this role, you work with the product owner and architects to define and implement features and functionality based on the product roadmap. In this role you will.Lead the implementation of the Adobe Experience PlatformProvide leadership for Digital initiatives/digital transformation for ecommerce leveraging Microservices, open-source frameworks, Cloud technologiesCollaborate with various areas of the organization including business partners, product owners, architecture, security mavens, digital and IT engineering teamsPromote proper implementation of SAFe process techniquesProvide support in all phases of SDLC and ensure to deliver high-quality productsProvide recommendations for continuous improvement. Provide technical leadership direction, determining and developing approaches to solutions by coordinating multiple resources to solve complex problemsEncourage a collaborative, open and dynamic team culture where individuals and teams can realize their full potentialRequired Qualifications: 4+ years development experience on enterprise class applications4+ years of experience building data pipelines and extracting, transforming, and loading marketing and customer data4+ years of experience with programming languages/frameworks: Java, J2EE, python4+ years of experience with CI/CD tools like Jenkins, Ansible, Git, GitHub, Bitbucket, Jira, Confluence2+ years of experience in a lead engineer role2+ years of experience in partnering with architecture, product and program management teams to influence product development assisting or improving productsCOVID Vaccine Required: N/A COVID Requirements :Preferred Qualifications: Experience with Adobe Experience Platform or similar platformsExperience With AWS, GCP And Other Cloud-based ServicesExperience working with data warehouse/cloud solutions solutions like Amazon Redshift, Google BigQuery, Snowflake, or similarExpertise in database testing using SQL and UNIX, including: designing and manipulating test data, validating stored procedures, jobs, triggers and replication Experience providing self-service to engineers through Infrastructure as Code (IaC) and cloud automation principles.Good understanding and working experience in IT Security best practices and other compliance standards such as ADA, HIPAA, PCI DSSExperience in SAFe Agile work environments; certification of SAFe for leaders would be a plusExcellent interpersonal and communication skills; thrives working in a highly iterative, agile, and open team environmentEducation: Bachelor's degree or equivalent years of experience.Regards,GeethaFirst Soft SolutionsDirect: 8556884377x141geetha@firstsoftsolutions.net , www.firstsoftsolutions.net"
Data Engineer for an Innovative Private Market Solutions Company,BEON.tech,"New York, NY (Remote)",https://www.linkedin.com/jobs/view/3596568903/?eBP=JOB_SEARCH_ORGANIC&refId=ifnjjNsg7HeCdebP5C%2B%2FEw%3D%3D&trackingId=aWNpq1Cm67Ypq9R0AWEuIQ%3D%3D&trk=flagship3_search_srp_jobs,3596568903,"About the job
            
 
Be a part of a leading provider of private market solutions, offering innovative technology and expertise to help companies achieve their liquidity goals. With a deep understanding of the complex and evolving private market landscape, the company provides an end-to-end platform that simplifies secondary market transactions for private companies, their employees, and investors.Their cutting-edge technology and robust compliance tools provide unparalleled efficiency and security for all stakeholders involved in private market transactions. The platform also offers a range of liquidity programs that can be tailored to meet each client's unique needs, including tender offers, block trades, and direct listings.The company has a proven track record of success, working with diverse clients, from startups to established private equity firms and venture capital funds. Their team of experts is dedicated to providing unparalleled service and support to ensure their clients achieve their desired outcomes.Key Details to Catch Your Eye You will have the opportunity to work with cutting-edge technology and robust compliance toolsCompany culture of innovation and collaboration, where each team member has the opportunity to make a meaningful impact
What Team Members Love About This Project""Great company. They have an innovative environment and a ton of market experience. There are no cons; it's a great place.""—Anonymus from Glassdoor.RequirementsYour Skills 4+ years as a Data or Software Engineer (Semi Sr/Sr)Work experience with Apache KafkaStrong experience with event-driven architecture and systems. Knowledge of PythonFamiliarity with Software Engineering best practices and API DesignBachelor's degree in Software EngineeringEnglish Proficiency
How You'll Stand Out Experience working in the financial industryFamiliarity with Ruby On Rails, JavaScript, AWS or DevOps. 
The Screening Process Verification by the BEON team1-hour Technical Interview with the Engineering Team30-minute Cultural Interview. 
Total expected timeframe: 4-6 days.About BEON.techBEON.tech connects the brightest Latin American talent with the most innovative and disruptive U.S. companies. You'll get access to a custom-vetted pool of full-time, long-term, remote software jobs with compensation comparable to U.S.-based positions.To join BEON.tech is to be in a devs-first company, which means you are the priority when it comes to decision-making, client selection, and growth planning.Develop your career at the pace you deserve.Benefits USD compensation comparable to U.S.-based positionsA US$ 1,500 welcome package to get you started with the right gearHealth insuranceInternet serviceTrip to Headquarters in Buenos AiresFlexible payments in crypto, wire transfer, Wise, PayPal, or PayoneerEnglish conversation club & workshopsRewards Program: Win prizes every month by participating in weekly challenges. The annual winner will earn a trip for two to NYC!Psychotherapy sessionsUnlimited reskilling in Udemy, Educación IT & O'Reilly"
Sr. Data Engineer,MHK TECH INC,United States (Remote),https://www.linkedin.com/jobs/view/3762679110/?eBP=JOB_SEARCH_ORGANIC&refId=ifnjjNsg7HeCdebP5C%2B%2FEw%3D%3D&trackingId=ucZElTQnM8zxCPNz2oNm2w%3D%3D&trk=flagship3_search_srp_jobs,3762679110,"About the job
            
 
Responsibilities:   Designing and implementing large-scale, distributed data processing systems using technologies such as Apache Hadoop, Apache Spark, or Apache Flink. Developing and optimizing data pipelines and workflows for ingesting, storing, processing, and analyzing large volumes of structured and unstructured data. Collaborating with data scientists, data analysts, and other stakeholders to understand data requirements and translate them into technical solutions. Building and maintaining data infrastructure, including data lakes, data warehouses, and real-time streaming platforms. Designing and implementing data models and schemas for efficient data storage and retrieval. Ensuring the scalability, availability, and fault-tolerance of big data systems through proper configuration, monitoring, and performance tuning. Identifying and evaluating new technologies, tools, and frameworks to improve the efficiency and effectiveness of big data processing. Implementing data security and privacy measures to protect sensitive information throughout the data lifecycle. Collaborating with cross-functional teams to integrate data from various sources, including structured databases, unstructured files, APIs, and streaming data. Developing and maintaining documentation, including data flow diagrams, system architecture, and technical specifications.    Requirements:  Bachelor's or higher degree in Computer Science, Engineering, or a related field. Proven experience as a big data engineer or a similar role, with a deep understanding of big data technologies, frameworks, and best practices. Strong programming skills in languages such as Java, Scala, or Python for developing big data solutions. Experience with big data processing frameworks like Apache Hadoop, Apache Spark, Apache Flink, or similar. Proficiency in SQL and NoSQL databases, as well as data modeling and database design principles. Familiarity with cloud platforms and services, such as Amazon Web Services (AWS), Microsoft Azure, or Google Cloud Platform (GCP). Knowledge of distributed computing principles and technologies, such as HDFS, YARN, and containerization (e.g., Docker, Kubernetes). Understanding of real-time streaming technologies and frameworks, such as Apache Kafka or Apache Pulsar. Strong problem-solving skills and ability to optimize and tune big data processing systems for performance and scalability. Excellent communication and teamwork skills to collaborate with cross-functional teams and stakeholders.

Desired Skills and Experience
                DATA ENGINEER"
Big Data Engineer (Various Levels),Open Systems Inc.,United States (Remote),https://www.linkedin.com/jobs/view/3775778801/?eBP=JOB_SEARCH_ORGANIC&refId=ifnjjNsg7HeCdebP5C%2B%2FEw%3D%3D&trackingId=wIL8%2FEipbXbw2v3IMb6aRQ%3D%3D&trk=flagship3_search_srp_jobs,3775778801,"About the job
            
 
Big Data EngineerRemoteCompanyOur client, Fortune-300 transportation company specializing in freight railroading. They operate approximately 21,000 route miles in 22 states and the District of Columbia, serve every major container port in the eastern United States, and provide efficient connections to other rail carriers. Our client has the most extensive intermodal network in the East and is a major transporter of coal and industrial products.Job DescriptionOur client is currently seeking an experienced Data Engineer – Big Data individual for their Midtown office in Atlanta, GA. The successful candidate must have Big Data engineering experience and must demonstrate an affinity for working with others to create successful solutions. Join a smart, highly skilled team with a passion for technology, where you will work on our state of the art Big Data Platforms. They must be a very good communicator, both written and verbal, and have some experience working with business areas to translate their business data needs and data questions into project requirements. The candidate will participate in all phases of the Data Engineering life cycle and will independently and collaboratively write project requirements, architect solutions and perform data ingestion development and support duties.RequiredSkills and Experience:  6+ years of overall IT experience3+ years of experience with high-velocity high-volume stream processing: Apache Kafka and Spark Streaming Experience with real-time data processing and streaming techniques using Spark structured streaming and KafkaDeep knowledge of troubleshooting and tuning Spark applications
3+ years of experience with data ingestion from Message Queues (Tibco, IBM, etc.) and different file formats across different platforms like JSON, XML, CSV3+ years of experience with Big Data tools/technologies like Hadoop, Spark, Spark SQL, Kafka, Sqoop, Hive, S3, HDFS, or 3+ years of experience building, testing, and optimizing ‘Big Data’ data ingestion pipelines, architectures, and data sets2+ years of experience with Python (and/or Scala) and PySpark/Scala-Spark3+ years of experience with Cloud platforms e.g. AWS, GCP, etc. 3+ years of experience with database solutions like Kudu/Impala, or Delta Lake or Snowflake or BigQuery2+ years of experience with NoSQL databases, including HBASE and/or CassandraExperience in successfully building and deploying a new data platform on Azure/ AWSExperience in Azure / AWS Serverless technologies, like, S3, Kinesis/MSK, lambda, and GlueStrong knowledge of Messaging Platforms like Kafka, Amazon MSK & TIBCO EMS or IBM MQ SeriesExperience with Databricks UI, Managing Databricks Notebooks, Delta Lake with Python, Delta Lake with Spark SQL, Delta Live Tables, Unity Catalog Knowledge of Unix/Linux platform and shell scripting is a mustStrong analytical and problem-solving skills
Preferred (Not Required) Strong SQL skills with ability to write intermediate complexity queriesStrong understanding of Relational & Dimensional modeling Experience with GIT code versioning softwareExperience with REST API and Web ServicesGood business analyst and requirements gathering/writing skills
EducationBachelor’s Degree required. Preferably in Information Systems, Computer Science, Computer Information Systems or related fieldWho We AreOpen Systems Inc. (OSI) was founded in 1994 to provide information technology solutions and staffing services to large and mid-size companies across the U.S. Our corporate office is located at 6495 Shiloh Road, Ste 310 Alpharetta, GA 30005. We provide a full range of staffing services including contract, contract-to-hire, and direct hire solutions. Our technical recruiting experts are experienced in technical screening, candidate sourcing, and behavioral interviewing techniques. They focus on providing candidates who match your technical requirements and fit seamlessly into your company culture.Contact Open Systems, Inc. anytime by website, phone or email. We look forward to hearing from you!!"
Senior Data Engineer - healthcare data,Vytalize Health,"Hoboken, NJ (Remote)",https://www.linkedin.com/jobs/view/3727309079/?eBP=JOB_SEARCH_ORGANIC&refId=ifnjjNsg7HeCdebP5C%2B%2FEw%3D%3D&trackingId=TxiPzVOUl59Bm60%2B%2FxJahw%3D%3D&trk=flagship3_search_srp_jobs,3727309079,"About the job
            
 
About Our CompanyVytalize Health is a leading value-based care platform. It helps independent physicians and practices stay ahead in a rapidly changing healthcare system by strengthening relationships with their patients through data-driven, holistic, and personalized care. Vytalize provides an all-in-one solution, including value-based incentives, smart technology, and a virtual clinic that enables independent practices to succeed in value-based care arrangements. Vytalize's care delivery model transforms the healthcare experience for more than 250,000+ Medicare beneficiaries across 36 states by helping them manage their chronic conditions in collaboration with their doctors.About Our GrowthVytalize Health has grown its patient base over 100% year-over-year and is now partnered with over 1,000 providers across 36-states. Our all-in-one, vertically integrated solution for value-based care delivery is responsible for $2 billion in medical spending. We are expanding into new markets while increasing the concentration of practices in existing ones.Visit www.vytalizehealth.com for more information.Why you will love working hereWe are an employee first, mission driven company that cares deeply about solving challenges in the healthcare space. We are open, collaborative and want to enhance how physicians interact with, and treat their patients. Our rapid growth means that we value working together as a team. You will be recognized and appreciated for your curiosity, tenacity and ability to challenge the status quo; approaching problems with an optimistic attitude. We are a diverse team of physicians, technologists, MBAs, nurses, and operators. You will be making a massive impact on people's lives and ultimately feel like you are doing your best work here at Vytalize.Your opportunity Data sits at the core of services we deliver to our patients and physicians. The Senior Data Engineer will work as part of the team responsible for ensuring that the data infrastructure is robust, scalable, and secure, enabling the organization to effectively use data to drive insights and make informed decisions. You will be responsible for designing, building, and maintaining scalable data systems that support the organization's business objectives. You will work with cross-functional teams to ensure data is collected, stored, processed, and analyzed in a timely, efficient, and accurate manner. This is a great role for a Senior Data Engineer who loves working with complex data and doing meaningful work that will have a positive impact on other people's lives.What You Will Do  Design, build, and maintain data pipelines to support business needs Work with cross-functional teams to understand data requirements and ensure data quality Implement data storage and processing systems that scale to handle large data sets Building and maintaining data pipelines to ingest data from various sources and ensure data is properly formatted for analysis. Develop and maintain data models, dictionaries, and metadata Collaborate with data scientists and analysts to understand their data needs and ensure that the data systems support their requirements. Develop and maintain documentation for data systems, processes, and procedures Implementing data quality control processes to validate and clean data before it is used for analysis. Designing and implementing security measures to protect sensitive data and ensure data privacy. Optimizing data systems for performance and scalability to ensure that the data infrastructure can support the growing needs of the organization. Staying up to date with the latest tools and technologies in the field of data engineering to continue to support the organization's data-related needs. 
Qualifications: Bachelor's degree in Computer Science, Information Systems, or a related field 5+ years of experience in data engineering10+ years engineering or engineering/analytics experience (note: we will consider exceptional candidates with fewer years of experience for Data Engineer roles)Strong understanding of data lake, data warehousing, data modeling, and SQL Strong experience with PythonExperience with healthcare dataExperience with big data technologies such as Hadoop, Spark, and NoSQL databases Knowledge of data processing and ETL tools such as Airflow or Talend, and understand the tradeoffs of them Experience with cloud computing platforms such as AWS (or Google Cloud, or Azure) Strong problem-solving and analytical skills Excellent written and verbal communication skills Senior experience in order to have the ability to be self-directed and work autonomously, as well as being part of a collaborative team
Perks/Benefits Competitive base compensationAnnual bonus potential Health benefits effective on start date; 100% coverage for base plan, up to 90% coverage on all other plans for individuals and familiesHealth & Wellness Program; up to $300 per quarter for your overall wellbeing401K plan effective on the first of the month after your start date; 100% of up to 4% of your annual salaryCompany paid STD/LTDUnlimited (or generous) paid ""Vytal Time"", and 5 paid sick days after your first 90 daysTechnology setupAbility to help build a market leader in value-based healthcare at a rapidly growing organization
We are interested in every qualified candidate who is eligible to work in the United States. However, we are not able to sponsor visas.Please note at no time during our screening, interview, or selection process do we ask for additional personal information (beyond your resume) or account/financial information. We will also never ask for you to purchase anything; nor will we ever interview you via text message. Any communication received from a Vytalize Health recruiter during your screening, interviewing, or selection process will come from an email ending in @vytalizehealth.com"
Big data engineer | remote 100%,eStaffing Inc.,United States (Remote),https://www.linkedin.com/jobs/view/3760557814/?eBP=JOB_SEARCH_ORGANIC&refId=ifnjjNsg7HeCdebP5C%2B%2FEw%3D%3D&trackingId=27tAJPBpzvxTpRxo818ykw%3D%3D&trk=flagship3_search_srp_jobs,3760557814,"About the job
            
 
TITLE - LEAD BIG DATA ENGINEERDURATION - W2CLIENT - HEALTH CARE SERVICESLOCATION - REMOTE 100%Must Have Skills Data Analysis and ArchitectureSpark & PySparkPythonSqoop, Pig, HiveNo SQL Data StoresObject StoreDesign and Development of APIs, KafkaLead experience
Role SummaryThis role will be responsible for transforming extensive and complex data into consumable business capabilities. Create system architecture, design, and specification using in-depth engineering skills and knowledge to solve complex development problems and achieve engineering goals. Determine and source appropriate data for a given analysis. Work with data modelers/analysts to understand the business problems they are trying to solve, then create or augment data assets to feed their analysis. Acts as a resource and mentor for colleagues with less experience.Core Responsibilities Lead a team of data engineers to develop data products and tools, explore new technologies, and continually improve tools and processes Hands-on experience working with Data Analysis and Architecture, Spark, PySpark, Python, Sqoop, Pig, Hive, No SQL Data Stores, Object Store, Design and Development of APIs, KafkaCreate business value by leading the design, get your hands dirty, write code, and ultimately deploy big data and machine learning capabilities.Possess expert knowledge in performance, large-scale data distributed system scalability, system architecture, and data engineering best practices.Give the highest priority to operational excellence, evaluate system performance, security, design system metrics, and driving quality improvements.Provide leadership, work collaboratively, and be a mentor in a fantastic team.Ability to produce both detailed technical work and high-level architectural designs.
Requirements 8+ years' of recent hands-on in an object-oriented language (Java, Scala, Python).5+ years' of experience designing and building data pipelines and data-intensive applications.1-2 years' of experience as a Lead Big Data Engineering or Technical LeadExperience using Big Data frameworks (e.g., Hadoop, Spark), databases for complex data assembly and transformation.Experience working with Healthcare data is a plus."
Senior Data Engineer,Hour Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3726911630/?eBP=JOB_SEARCH_ORGANIC&refId=ifnjjNsg7HeCdebP5C%2B%2FEw%3D%3D&trackingId=CLxkIGO28NTdTS%2FzxiZ2Vg%3D%3D&trk=flagship3_search_srp_jobs,3726911630,"About the job
            
 
Our client is in the Higher Education Technology space (Ed-Tech) and they are committed to supporting students succeed.  They are a SaaS company with supporting students actively using our platform globally.   Understanding students and institutions, as well as their behaviors through data, lies at the foundation of our work to improve student success. Every data point in the systems is important to helping us achieve that goal, so they are looking for people with a strong background in data engineering and analytics to help us design, build, scale, as well as maintain our data pipelines and models. As a Senior Data Science Engineer, you will be working with a variety of internal teams across engineering, product, and business to help solve their data needs. Your work will directly and tangibly impact the success of millions of students across the world.   In terms of the role and responsibilities, you will    Identify the data needs of our engineering, product, and business teams, understand their specific requirements for metrics and analysis, then build efficient, scalable, accurate, and complete data pipelines to enable data-informed decisions across the company  Architect data pipelines and models that power internal analytics for our teams, as well as customer-facing data visualization product features  Drive the collection of new data and the evolution of existing data sources, collaborate with the engineering teams to manage our product instrumentation strategies and data structures  Help the product and engineering teams understand and generalize statistical models from our research efforts, and help build data systems that would allow these models to be used directly in our product to drive student success  Work with Product Management to ensure productive, fast-moving sprints that deliver the maximum value to our customers  Help to continuously improve the team processes of our engineering team 
    You should    Have at least 5-7 years of experience in a Data Engineering or Data Science role, with a focus on instrumenting data collection, building data pipelines and conducting data-intensive analysis  Have a strong engineering background and are interested in data  Care deeply about the integrity of data, have a good nose for inconsistencies in data, and be able to pinpoint the issue to ensure that the team is not making decisions based on inaccurate or incomplete data  Have extensive experience of a scientific computing language (e.g. Python) and SQL  Have experience building an Amazon Web Services-based system that processes data across multiple data stores and technologies, including MySQL, Redis, Elasticsearch  Know the best practices of how different types of data should be visualized in different contexts  Be comfortable using multiple communication and collaboration tools to work effectively with colleagues across North America and Europe 
    What will make you stand out    Experience working with Python or Java web applications  Experience working in the higher-ed technology space, particularly in a Data Engineering related role  Experience leading a team  Experience working with a remote or distributed team 
    What you can expect    A chance to work towards an amazing mission of helping students succeed as a team member of a global tech startup  Remote-friendly work environment: Ability to work from anywhere in the EST in the US  Generous paid vacation time  Continuous learning and growth culture with many opportunities to develop professionally  Health Benefits including health, dental, life , disability insurance and travel coverage  Participation in matching 401K plan  Home Office Set up support with a company laptop, equipment and support to set up your home office  A chance to work with a global collaborative, friendly and diverse team

Desired Skills and Experience
                DATA ENGINEER"
SENIOR BIG DATA ENGINEER – FULLY REMOTE,Vodastra Technologies,"New York, NY (Remote)",https://www.linkedin.com/jobs/view/3756990715/?eBP=JOB_SEARCH_ORGANIC&refId=ifnjjNsg7HeCdebP5C%2B%2FEw%3D%3D&trackingId=bEDubZz1MucAUzcIg5ORfA%3D%3D&trk=flagship3_search_srp_jobs,3756990715,"About the job
            
 
Industry: Healthcare / Health ServicesJob Category: Medical / Health - Healthcare ITJOB SUMMARY:This job is responsible for designing and engineering data solutions for the enterprise and, working closely with business, analytic and IT stakeholders, assists with the development and maintenance of these solutions. This includes coding data ingestion pipelines, transformations and delivery programs/logic for people and systems to access data for operational and/or analytic needs. Duties include but are not limited to the coding, testing, and implementation of ingestion and extraction pipelines, transformation and cleansing processes, and processes that load and curate data in conformed, fit-for-purpose data structures. The incumbent is expected to partner with others throughout the organization (including other engineers, architects, analysts, data scientists, and non-technical audiences) in their daily work. The incumbent will work with cross-functional teams to deliver and maintain data products and capabilities that support and enable strategies at business unit and enterprise levels. The incumbent is expected to utilize technologies such as, but not limited to: Google Cloud Platform.ESSENTIAL RESPONSIBILITIES:In partnership with other business, platform, technology, and analytic teams across the enterprise, design, build and maintain well-engineered data solutions in a variety of environments, including traditional data warehouses, Big Data solutions, and cloud-oriented platforms. Create high performance cloud and big data systems to be used with operational and analytic applications. Work with internal and external platforms and systems to connect and align on data sourcing, flow, structure, and subject matter expertise. Work with business stakeholders and strategic partners to implement and support operational and analytic platforms. This may include products purchased by the organization that must be ingested or modeled/derived data maintained by enterprise platforms and data consumers. Working across multiple, disparate systems and platforms, design, code, test, implement, and maintain scalable and extensible frameworks that support data engineering services. Align with security, data governance and data quality programs by driving assigned components of metadata management, data quality management, and the application of business rules. Develop and maintain associated data engineering processes and participate in required operating procedures as part of the enterprise’s overall information management activities. Includes data cleansing, standardization, technical metadata documentation, and the de-identification and/or tokenization of data. Develop, optimize and/or maintain machine learning and AI engineering processes (MLOps) that are deployed to cloud or big data environments. These may be based on prototypes built by data scientists or capability frameworks implemented to allow data scientists to build efficiently in production environments. Develop tasks across multiple projects with limited need for guidance. This includes providing guidance and education to Intermediate and Junior contributors within team. Manage relationships with customers of the function. Attend meetings with customers on a stand-alone basis or with team as needed. Establish standards and patters for high performance data ingestion, transformation, and delivery of data analytic needs. Keep current with Big Data technologies in order to recommend best tools in order to perform current and future work Other duties as assigned or requested.EDUCATION: RequiredBachelor's Degree in Software Engineering, Information Systems, Computer Science, Data Science or related fieldEXPERIENCE:Required 5 years in Data platform development, data engineering, software development, or data science3 years in Big data or cloud data platform Preferred3 years in Healthcare Industry3 years of Data Warehousing3 years of Database AdministrationSKILLS:SQL Data WarehousingProblem-SolvingCommunication SkillsAnalytical SkillsSpark or Python or related toolCloud TechnologiesPowered by JazzHRg4JEOLNM0S"
Sr. Data Engineer - Remote | WFH,Get It Recruit - Information Technology,"Phoenix, AZ (Remote)",https://www.linkedin.com/jobs/view/3769184471/?eBP=JOB_SEARCH_ORGANIC&refId=ifnjjNsg7HeCdebP5C%2B%2FEw%3D%3D&trackingId=972DgHcaxMU12L2NqhObJA%3D%3D&trk=flagship3_search_srp_jobs,3769184471,"About the job
            
 
Join a recognized leader in promoting health and well-being! We are currently seeking a skilled and experienced professional to fill a remote position based in the beautiful state of Arizona. The remote work opportunity requires residency within the State of Arizona.Role OverviewAs a Data Engineer, you will play a crucial role in designing and implementing business intelligence and extract, transform, and load (ETL) solutions. The ideal candidate will bring a wealth of experience in advanced SQL techniques, relational database performance, Big Data technologies, and scripting languages like Python.Key ResponsibilitiesMentor team members in ETL processes and standards, participating in code reviews.Develop and maintain ETL processes, ensuring data accuracy and quality.Design and implement automated tests, including unit tests and integration tests.Collaborate with other technology teams to define and develop solutions.Research and experiment with emerging Data Integration technologies and tools.Support database clustering, mirroring, and replication among other SQL Server technologies.Participate in the Software Development Life Cycle and adhere to best practices.Analyze, map data, and contribute to the team's knowledge base.QualificationsRequired:4+ years of experience in computer programming, query design, and databases.High-School Diploma or GED in a general field of study.PreferredBachelor's Degree in Information Technology or related field.4+ years of experience building and managing complex Data Integration solutions.MS SQL Certification or other certification in current programming languages.Essential CompetenciesProficiency in spreadsheet, database, and word processing software.Strong knowledge of business intelligence, programming, and data analysis software.Excellent troubleshooting skills and working knowledge of PowerShell.Effective interpersonal skills and ability to prioritize tasks.Our CommitmentWe are an equal opportunity employer, fostering a diverse and inclusive workplace. We do not discriminate in hiring or employment based on race, ethnicity, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, protected veteran status, or any other protected group.Thank you for considering this exciting opportunity!Employment Type: Full-Time"
Data Engineer II (Talend),Centene Corporation,"Missouri, United States (Remote)",https://www.linkedin.com/jobs/view/3760308101/?eBP=JOB_SEARCH_ORGANIC&refId=ifnjjNsg7HeCdebP5C%2B%2FEw%3D%3D&trackingId=epcrfO2I8LHgqchGItqs%2BQ%3D%3D&trk=flagship3_search_srp_jobs,3760308101,"About the job
            
 
You could be the one who changes everything for our 28 million members by using technology to improve health outcomes around the world. As a diversified, national organization, Centene's technology professionals have access to competitive benefits including a fresh perspective on workplace flexibility.Position Purpose Develops and operationalizes data pipelines to make data available for consumption (reports and advanced analytics), including data ingestion, data transformation, data validation / quality, data pipeline optimization, and orchestration. Engages with the DevSecOps Engineer during continuous integration and continuous deployment. Talend implementation of data flows. Designs and implements standardized data management procedures around data staging, data ingestion, data preparation, data provisioning, and data destruction (scripts, programs, automation, assisted by automation, etc.)Designs, develops, implements, tests, documents, and operates large-scale, high-volume, high-performance data structures for business intelligence analyticsDesigns, develops, and maintains real-time processing applications and real-time data pipelinesEnsure quality of technical solutions as data moves across Centene’s environmentsProvides insight into the changing data environment, data processing, data storage, and utilization requirements for the company and offers suggestions for solutionsDevelops, constructs, tests, and maintains architectures using programming language and toolsIdentifies ways to improve data reliability, efficiency, and quality; use data to discover tasks that can be automatedPerforms other duties as assignedComplies with all policies and standards
Education/Experience A Bachelor's degree in a quantitative or business field (e.g., statistics, mathematics, engineering, computer science).Requires 2 – 4 years of related experience.Or equivalent experience acquired through accomplishments of applicable knowledge, duties, scope and skill reflective of the level of this position.Technical Skills One or more of the following skills are desired.Experience developing Talend (Programming Language) Experience with SQL (Programming Language)Experience with AWSExperience with Healthcare (specifically HEDIS)Experience with Big Data; Data ProcessingExperience with diagnosing system issues, engaging in data validation, and providing quality assurance testingExperience with Data Manipulation; Data MiningExperience working in a production cloud infrastructureKnowledge of Microsoft SQL Servers
Soft Skills Strong Communication and Organizational skills
Our Comprehensive Benefits Package Flexible work solutions including remote options, hybrid work schedules and dress flexibility, Competitive pay, Paid time off including holidays, Health insurance coverage for you and your dependents, 401(k) and stock purchase plans, Tuition reimbursement and best-in-class training and development.Centene is an equal opportunity employer that is committed to diversity, and values the ways in which we are different. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, veteran status, or other characteristic protected by applicable law."
Data Engineer II (Talend),Centene Corporation,"Illinois, United States (Remote)",https://www.linkedin.com/jobs/view/3775421410/?eBP=JOB_SEARCH_ORGANIC&refId=ifnjjNsg7HeCdebP5C%2B%2FEw%3D%3D&trackingId=phurqmQC1ZC1X9CMFxSWjw%3D%3D&trk=flagship3_search_srp_jobs,3775421410,"About the job
            
 
You could be the one who changes everything for our 28 million members by using technology to improve health outcomes around the world. As a diversified, national organization, Centene's technology professionals have access to competitive benefits including a fresh perspective on workplace flexibility.Position Purpose Develops and operationalizes data pipelines to make data available for consumption (reports and advanced analytics), including data ingestion, data transformation, data validation / quality, data pipeline optimization, and orchestration. Engages with the DevSecOps Engineer during continuous integration and continuous deployment. Talend implementation of data flows. Designs and implements standardized data management procedures around data staging, data ingestion, data preparation, data provisioning, and data destruction (scripts, programs, automation, assisted by automation, etc.)Designs, develops, implements, tests, documents, and operates large-scale, high-volume, high-performance data structures for business intelligence analyticsDesigns, develops, and maintains real-time processing applications and real-time data pipelinesEnsure quality of technical solutions as data moves across Centene’s environmentsProvides insight into the changing data environment, data processing, data storage, and utilization requirements for the company and offers suggestions for solutionsDevelops, constructs, tests, and maintains architectures using programming language and toolsIdentifies ways to improve data reliability, efficiency, and quality; use data to discover tasks that can be automatedPerforms other duties as assignedComplies with all policies and standards
Education/Experience A Bachelor's degree in a quantitative or business field (e.g., statistics, mathematics, engineering, computer science).Requires 2 – 4 years of related experience.Or equivalent experience acquired through accomplishments of applicable knowledge, duties, scope and skill reflective of the level of this position.Technical Skills One or more of the following skills are desired.Experience developing Talend (Programming Language) Experience with SQL (Programming Language)Experience with AWSExperience with Healthcare (specifically HEDIS)Experience with Big Data; Data ProcessingExperience with diagnosing system issues, engaging in data validation, and providing quality assurance testingExperience with Data Manipulation; Data MiningExperience working in a production cloud infrastructureKnowledge of Microsoft SQL Servers
Soft Skills Strong Communication and Organizational skills
Our Comprehensive Benefits Package Flexible work solutions including remote options, hybrid work schedules and dress flexibility, Competitive pay, Paid time off including holidays, Health insurance coverage for you and your dependents, 401(k) and stock purchase plans, Tuition reimbursement and best-in-class training and development.Centene is an equal opportunity employer that is committed to diversity, and values the ways in which we are different. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, veteran status, or other characteristic protected by applicable law."
Sr. Data Engineer,MHK TECH INC,United States (Remote),https://www.linkedin.com/jobs/view/3762678132/?eBP=JOB_SEARCH_ORGANIC&refId=ifnjjNsg7HeCdebP5C%2B%2FEw%3D%3D&trackingId=Z29PzfaeHNMtry%2FZJPsrog%3D%3D&trk=flagship3_search_srp_jobs,3762678132,"About the job
            
 
Responsibilities:Designing and implementing large-scale, distributed data processing systems using technologies such as Apache Hadoop, Apache Spark, or Apache Flink.Developing and optimizing data pipelines and workflows for ingesting, storing, processing, and analyzing large volumes of structured and unstructured data.Collaborating with data scientists, data analysts, and other stakeholders to understand data requirements and translate them into technical solutions.Building and maintaining data infrastructure, including data lakes, data warehouses, and real-time streaming platforms.Designing and implementing data models and schemas for efficient data storage and retrieval.Ensuring the scalability, availability, and fault-tolerance of big data systems through proper configuration, monitoring, and performance tuning.Identifying and evaluating new technologies, tools, and frameworks to improve the efficiency and effectiveness of big data processing.Implementing data security and privacy measures to protect sensitive information throughout the data lifecycle.Collaborating with cross-functional teams to integrate data from various sources, including structured databases, unstructured files, APIs, and streaming data.Developing and maintaining documentation, including data flow diagrams, system architecture, and technical specifications.Requirements:Bachelor's or higher degree in Computer Science, Engineering, or a related field.Proven experience as a big data engineer or a similar role, with a deep understanding of big data technologies, frameworks, and best practices.Strong programming skills in languages such as Java, Scala, or Python for developing big data solutions.Experience with big data processing frameworks like Apache Hadoop, Apache Spark, Apache Flink, or similar.Proficiency in SQL and NoSQL databases, as well as data modeling and database design principles.Familiarity with cloud platforms and services, such as Amazon Web Services (AWS), Microsoft Azure, or Google Cloud Platform (GCP).Knowledge of distributed computing principles and technologies, such as HDFS, YARN, and containerization (e.g., Docker, Kubernetes).Understanding of real-time streaming technologies and frameworks, such as Apache Kafka or Apache Pulsar.Strong problem-solving skills and ability to optimize and tune big data processing systems for performance and scalability.Excellent communication and teamwork skills to collaborate with cross-functional teams and stakeholders.

Desired Skills and Experience
                DATA ENGINEER"
Data Integration Engineer,Firework,United States (Remote),https://www.linkedin.com/jobs/view/3770360358/?eBP=JOB_SEARCH_ORGANIC&refId=ifnjjNsg7HeCdebP5C%2B%2FEw%3D%3D&trackingId=RIuMs4ScI27Y%2BcdM9JiiiQ%3D%3D&trk=flagship3_search_srp_jobs,3770360358,"About the job
            
 
Firework is the world’s leading unified video commerce platform that empowers its global partners to personalize the customer experience and engagement at scale. Firework bridges the offline and online for a robust omnichannel immersive brand experience cultivating a deeper emotional human connection between our partners and their end consumers. We are customer-centric and inspired to win together offering total solutions with endless possibilities to help our customers increase purchases and conversions using the power of video. At the heart, we are a global and diverse team of “SuperSpark” creators, entrepreneurs, life-long learners, and data geeks driven by the future of authenticity to transform commerce. Firework has raised over $235M to date, with its latest Series B round led by SoftBank Vision Fund 2. Come reimagine the online customer experience with us.SummaryWe’re looking for a Data Integration Engineer to provide high quality integrations with our customers. This is a highly visible, customer-facing role where you'll have critical impact for our customers and therefore for our business. It's also an engineering role where you'll be hands on with all facets of the technology stack.What you’ll be doing  As a data integration engineer, your role is hands-on coding, configuring, and maintaining data integration for our customers Assess customers requirements to propose solutions with our platform/product in regards to the data funnel, communicate the data integration process in an easy-to-understand manner directly with our new partners Be a technical focal of the customer on all data integration aspects Participate in strategic planning sessions to ensure alignment of the global data Solutions function with the overall product and business strategies and goals This will include tasks like: Building custom data solutions to our partners Script, maintain and deploy scripts to support Firework’s full functionality at the clients site. - Javascript Implement Firework’s API and deploy in client production sites Configuring Firework’s settings in the client's site Work closely with engineering and data teams for best practices deployment and development Reviewing source data for accuracy and completeness before integrating it into the database Creating reports that provide details on data quality issues and suggested solutions Creating data models and mapping systems to support the movement of data between different databases and applicationsPerforming complex data analysis to identify patterns in data sets or to predict future trendsCreating scripts to automate data integration processes to improve efficiency and reduce human errorDesigning a data warehouse architecture that supports business operationsCreating data marts, which are smaller databases designed to support specific business processes or functions within an organizationDesigning and implementing data security policies to protect sensitive information from being compromised Troubleshooting problems with data integration or data warehousing systems
We’ll be excited if you have  Bachelor's degree, preferably in Computer Science or other technical field 5+ years of data integration engineer or 3+ years of experience as software engineer Excellent communication and presentation skills with various technical and non-technical audiences; must have strong empathy and emotional intelligence with diverse customers Advanced technical discovery, documentation, and troubleshooting skills Experience with globally distributed teams across multiple time zones Start-up, team-oriented mindset
Technical Qualifications  Integration Experience with Google Analytics and/or Adobe Analytics * Experience developing rich applications with JavaScript (ES6), HTML5, and CSS3 * Knowledge of Wordpress; Shopify is a plus * Understanding concepts like data flows, APIs, web services, and the mechanics of software integration
A BIG PLUS if you have  Create, maintain and deploy complete landing pages and applications - HTML, Javascript, CSS
The role may be hybrid in one of our offices or remote. For remote, we are looking for candidates in the United States.Don’t hold backWe understand some candidates may see the above and not apply because they don’t meet all the qualifications. We encourage you to apply anyway; we often find talented candidates that fit many other opportunities we have and look for potential too, not just what you did in the past. As an equal employment opportunity employer, we are a diverse team that strives for an inclusive environment for all. We prohibit discrimination and harassment of any kind based on race, color, sex, religion, sexual orientation, national origin, age, disability, genetic information, pregnancy, or any other protected characteristic as outlined by federal, state, or local laws.Notice to Third Party Recruitment AgenciesPlease note that while Firework and its subsidiaries appreciate agency outreach, we currently do not accept unsolicited resumes from recruiters or employment agencies. In the absence of an executed Recruitment Services Agreement, there will be no obligation to any referral compensation or recruiter fee.In the event a recruiter or agency submits a resume or candidate without an agreement, Firework and its subsidiaries reserve the right to pursue and hire those candidate(s) without any financial obligation to the recruiter or agency.By submitting your application, you acknowledge that you have read and understood Firework's Applicant Privacy Policy located at: https://firework.com/legal/applicantpolicy/."
Senior Data Engineer - Remote,Restaurant Technologies,"Mendota Heights, MN (Remote)",https://www.linkedin.com/jobs/view/3769524731/?eBP=JOB_SEARCH_ORGANIC&refId=ifnjjNsg7HeCdebP5C%2B%2FEw%3D%3D&trackingId=o2KMYid6mQvaJTADowd0Sg%3D%3D&trk=flagship3_search_srp_jobs,3769524731,"About the job
            
 
Senior Data EngineerAre you ready to be a catalyst for change? We are seeking a dynamic Senior Data Engineer who thrives on driving transformation and spearheading pivotal projects in the data realm. In this role, you won't just be managing our data infrastructure and ETL pipelines; you'll be an agent of change and advocate for revolutionizing how we handle data.Your mission, should you choose to accept it, is to collaborate with our business leaders, enterprise data architect, analysts, outside consultants, and a diverse range of 3rd party stakeholders. You'll be the linchpin ensuring the availability, reliability, and excellence of our data systems. This is not just a role but a journey for a change-maker who is passionate about harnessing the power of data to fuel informed decision-making across our organization. If you're a go-getter, a leader by nature, and someone who makes things happen in the data engineering landscape, we want you on our team. Let's redefine what's possible together.This is a fully remote position, and must live in the continental US. No visa sponsorship is available at this time.Key Responsibilities: Lead Cross-Functional Team Engagement: Proactively engage with cross-functional teams to actively gather and analyze data requirements, and lead the design and implementation of data pipelines using Databricks. Drive ETL Process Innovation: Take charge of developing, optimizing, and maintaining robust ETL processes for data extraction, transformation, and loading, ensuring they meet evolving business needs. Architect Scalable Data Storage Solutions: Innovate in building and sustaining scalable data storage solutions on Databricks, including Delta Lake and other relevant technologies, to support growing data demands. Ensure Data Pipeline Integrity: Vigilantly monitor and troubleshoot data pipelines, taking swift action to guarantee data accuracy, completeness, and timeliness. Create Comprehensive Process Documentation: Develop and maintain detailed documentation for all data engineering processes and data models, setting a standard for excellence and clarity. Champion Data Security and Compliance: Actively implement and enforce data security and compliance best practices, aligning with industry standards and company policies to safeguard data integrity. Stay Ahead of Industry Trends: Constantly stay abreast of the latest trends in Databricks and data engineering, and proactively recommend improvements in processes and technologies to keep the organization at the forefront of the field. Mentor and Develop Team Talent: Commit to providing mentorship and guidance to team members, fostering a culture of continuous learning and professional growth. Lead After-Hours Support: Participate and lead in the after-hours on-call rotation, providing expert-level support and ensuring seamless data operations around the clock
Minumum Requirements: Demonstrated experience in Data Engineering: A solid track record as a Data Engineer, characterized by at least 8 years of substantial experience. While a Bachelor’s degree in Computer Science, Data Engineering, Information Technology, or related fields is appreciated, we value practical, hands-on experience and proven outcomes over formal education. Expertise in SQL and Data Modeling: A deep understanding of SQL and data modeling, showcasing the ability to architect and maintain sophisticated data warehouses, with or without formal qualifications. Advanced Problem-Solving Skills: Exceptional problem-solving and debugging capabilities, with a history of tackling and resolving complex data challenges effectively. Effective Communication and Teamwork: Outstanding communication abilities, with a proven record of working collaboratively in dynamic team environments, fostering cooperation and mutual support. Analytical and Critical Thinking: Strong analytical skills and the ability to think critically, demonstrated through a history of data-driven decision-making and innovative problem-solving. Commitment to Continuous Learning: A continuous pursuit of learning and staying updated with the latest trends and technologies in data engineering. 
Preferred Requirements: Mastery in Databricks Ecosystem: Exceptional proficiency in the entire Databricks suite, including Databricks Delta Lake, Runtime, and Notebooks. Your expertise here means you’re not just using these tools; you're pushing their boundaries and unlocking their full potential. Cloud-Platform Wizardry: A robust understanding of cloud-based data platforms like AWS, Azure, or Google Cloud. You're not just familiar with these environments; you're adept at leveraging their power to transform data landscapes. ETL Development Virtuoso: Deep expertise in ETL development with tools such as Apache Spark, Python, or Scala. Your skill set goes beyond mere proficiency – you're crafting data artistry that drives business insights. Version Control Guru: Knowledge of version control systems isn’t just a line on your resume; it's an integral part of your data engineering toolkit, ensuring precision and collaboration in your projects. Big Data Technologies Connoisseur: Experience with big data technologies and data pipeline orchestration tools. You're not just experienced; you're a trailblazer in navigating and harnessing the complexities of big data. 
Inclusive EmployerAt Restaurant Technologies, we celebrate diversity, believe in equity, and are committed to creating an inclusive environment for all employees; we’re proud to be an Equal Employment Opportunity and Affirmative Action employer. All aspects of employment including the decision to hire, promote, discipline, or discharge, will be based on merit, competence, performance, and business needs. We do not discriminate on the basis of race, color, religion, marital status, age, national origin, ancestry, physical or mental disability, medical condition, pregnancy, genetic information, gender, sexual orientation, gender identity or expression, veteran status, or any other status protected under federal, state, or local law.”If you need assistance or an accommodation due to a disability, please contact us by email at RTCareers@rti-inc.com or call 1-888-796-4997.Culture & BenefitsOur employees are the foundation of our success and we take care of them! Our comprehensive benefits include professional development, competitive health care coverage, incentives and a healthy work and life balance. We’ve cultivated an award-winning workplace for driven team members who enjoy a fast pace and rapid growth balanced by a flexible and supportive environment. We’ve earned recognition as a “Best Place to Work,” by the Minneapolis/St. Paul Business Journal along with The Minnesota Work Life Champions Award while consistently earning a spot on Minnesota’s Fastest Growing Companies list. In addition, we’ve earned numerous awards from our customer base who consider Restaurant Technologies a valued strategic partner.Who We AreRestaurant Technologies is the leading provider of highly innovative and value-enhancing bulk cooking oil management services to the food service industry. Our solution is environmentally sound, highly efficient and creates a safer, more productive work environment for restaurant personnel. With over 35,000 existing customers we are an established, profitable mid-sized company poised for double-digit growth in the coming year. We are headquartered in suburban Minneapolis, Minnesota with a growing number of sites strategically located across the U.S. serving over 41 metropolitan markets."
"Senior Data Engineer - $180k-$220k (Snowflake, Coding)",CyberCoders,"Chicago, IL (Remote)",https://www.linkedin.com/jobs/view/3766365237/?eBP=CwEAAAGMRQFE_4NiRVzum1eV8N7Kpit_tUiQjpeke9074opJRvdiFT0hHCmbmiEPo3d0dXTbM1ON1u4d_4HxVzpv7LtwKrDZIz94BMsuFAKRCqlDVSGXlJNZTjsJXDK4YyGBToq2m26RcfP_f0jd2Zhll4_dJ4sJ6bYOkugEkf8BWY5HxMpuHN3ubATvYLOTi60aY3TqfpNKuAf8ZkbfHmHvhs1tobZXpCVNWxE6PWUWSidvKHxzEjdU5fuu7-tkx80v51lH8YmQw4PToMTghjnfHWV51Nv4xNI8vC2a1Z_xkQjcVx8-lZBsdL4wAR9-HsZptY2ELWR-EWfFQlWJoDUmPxe-ePcNePC3h0VhXhiC5NJ1gAE8488JCnc3kMY8cH_wBm9tAA&refId=LoHTn4kkVPcv20fL1OcqCg%3D%3D&trackingId=%2FUZBkXtlogOgWffFe3zfGw%3D%3D&trk=flagship3_search_srp_jobs,3766365237,"About the job
            
 
Permanently Remote in USJob Title:Senior Data Engineer - $180k-$220k + Bonus (Snowflake, Coding)Salary: $180k-$220k Base + Bonus, No Stock, 401k, BenefitsRequirements: Expert w/ Snowflake & Coding AbilityBased in beautiful New York City, we are a cutting edge ad-tech org for television-based ads.We are founded and owned by T.V.s largest publishers.Our mission is to be bring simplicity & scale to audience based campaigns in television.We're working with over 100 advertisers and anticipating another year of significant growth!As a rapidly growing company (founded in 2017 & up 140% year over year) we've recently elevated our C-Suite Team in preparation for our next stage of growth and are building our Technology, Product, and Operations Executive Teams.We have been in business for 7 years and have around 40 employees.Due to growth, we are actively hiring a Senior Data Engineer with  Snowflake experience (ideally certified) Ability to write production level code (ideally JavaScript or Python) Experience building data pipelines from scratch and/or working with APIs
Experience with the following is a big plus!  Fivetran and/or DBT
You will be working with 12 other engineers on the data side + several other technical folks.This role will consist of tasks such as building out data pipelines, data architecture via snowflake, and data modeling.If you have this experience, please apply immediately. We are actively interviewing this week and next for this high profile position.Top Reasons to Work with Us  Compensation: $180k-$220k Base + Bonus, No Stock, 401k, Benefits Raid Growth: Founded in 2017 & up 140% year over year Culture: Fast paced, mission driven culture Technology: Cutting edge technology
What You Will Be Doing  Building data pipelines from scratch Data architecture via Snowflake Data modeling Technical review of everything this group builds. Mange development velocity, team capacity, and backlogs Partner closely with the product team Take on key assignments and delegate as needed Act as the main technical point of contact for engineering Translate technical requirements to the rest of the engineering team
What You Need for this PositionMust Have Experience  Snowflake experience Ability to write production level code (ideally JavaScript or Python) Experience building data pipelines from scratch and/or working with APIs
Some Experience With:-  Fivetran and/or DBT
What's In It for You $180k-$220k Base + Bonus, No Stock, 401k, Benefits 401k Vacation/PTO Medical Dental Vision Bonus 401k
Benefits  Vacation/PTO Medical Dental Vision Bonus
So, if you are a Senior Data Engineer - $180k-$220k + Bonus (Snowflake, Coding) with experience, please apply today!Colorado employees will receive paid sick leave. For additional information about available benefits, please contact Nitu Gulati-PaulyEmail Your Resume In Word ToLooking forward to receiving your resume through our website and going over the position with you. Clicking apply is the best way to apply, but you may also:Nitu.Gulati-Pauly@cybercoders.com Please do NOT change the email subject line in any way. You must keep the JobID: linkedin : MA5-1745335L569 -- in the email subject line for your application to be considered.***
Nitu Gulati-Pauly - VP of Recruiting - CyberCodersApplicants must be authorized to work in the U.S.CyberCoders is proud to be an Equal Opportunity EmployerAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any other characteristic protected by law.Your Right to Work – In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.CyberCoders will consider for Employment in the City of Los Angeles qualified Applicants with Criminal Histories in a manner consistent with the requirements of the Los Angeles Fair Chance Initiative for Hiring (Ban the Box) Ordinance."
Python Programmer (Remote),SynergisticIT,"Wilmington, NC (Remote)",https://www.linkedin.com/jobs/view/3775970519/?eBP=JOB_SEARCH_ORGANIC&refId=LoHTn4kkVPcv20fL1OcqCg%3D%3D&trackingId=42q1iYjgM4ZYaZ3XKenw0A%3D%3D&trk=flagship3_search_srp_jobs,3775970519,"About the job
            
 
The Job Market is Challenging due to more than 150,000 Tech Layoffs in 2022 and in 2023 more than 240,000 layoffs so almost 3,90,00 tech employees have been laid off since 2022 and its still going on . The effect of this has led hundreds of thousands of laid off Tech employees competing with existing Jobseekers. Entry level Job seekers struggle to get responses to their applications, are getting ghosted after interviews. In such a scenario the Job seekers need  to differentiate themselves by ensuring to obtain exceptional skills and technologies to be hired by clients as its an employer's market presently and they have a lot of hiring choices.For more than 12+ years Synergisticit has helped Jobseekers differentiate themselves by providing candidates the requisite skills and experience to outperform at interviews and clients. Here at SynergisticIT We just don't focus on getting you a Job we make careers.All Positions are open for all visas and US citizensWe are matchmakers we provide clients with candidates who can perform from day 1 of starting work. In this challenging economy every client wants to save $$$'s and they want the best value for their money. Jobseekers need to self-evaluate if they have the requisite skills to meet client requirements and needs as Clients now post covid can also hire remote workers which increases even more competition for jobseekers.We at Synergisticit understand the problem of the mismatch between employer's requirements and Employee skills and that's why since 2010 we have helped 1000's of candidates get jobs at technology clients like  apple, google, Paypal, western union, Client, visa, walmart labs etc to name a few.We have an excellent reputation with the clients. Currently, We are looking for  entry-level software programmers, Java Full stack developers, Python/Java developers, Data analysts/ Data Scientists, Machine Learning engineers for full time positions with clients.Who Should Apply Recent Computer science/Engineering /Mathematics/Statistics or Science Graduates or People looking to switch careers or who have had gaps in employment and looking to make their careers in IT Industry We assist in filing for STEM extension and also for H1b and Green card filing to CandidatesWe also offer optionally Skill and technology enhancement programs for candidates who are either missing skills or are lacking Industry/Client experience with Projects and skills. Candidates having difficulty in finding jobs or cracking interviews or who wants to improve their skill portfolio. If they are qualified with enough skills and have hands on project work at clients then they should be good to be submitted to clients. Shortlisting and selection is totally based on clients discretion not ours. please check the below links to see success outcomes of our candidates  and our participation at different Tech industry events and how we are different from other organizations in helping Jobseekers secure Tech careers https://www.synergisticit.com/candidate-outcomes/  We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2023/2022 and at Gartner Data Analytics Summit (Florida)-2023 Oracle CloudWorld Event (OCW) Las Vegas 2023/ 2022 | SynergisticIT - YouTube https://youtu.be/Rfn8Y0gnfL8?si=p2V4KFv5HukJXTrn  https://youtu.be/-HkNN1ag6Zk?si=1NRfgsvL_HJMVb6Q  https://www.youtube.com/watch?v=NVBU9RYZ6UI  https://www.youtube.com/watch?v=EmO7NrWHkLM  https://www.youtube.com/watch?v=NVBU9RYZ6UI  https://www.youtube.com/watch?v=OAFOhcGy9Z8  https://www.youtube.com/watch?v=Yy74yvjatVg For preparing for interviews please visit  https://www.synergisticit.com/interview-questions/  We are looking for the right matching candidates for our clients  Please apply via the job posting  REQUIRED SKILLS For Java /Full Stack/Software Programmer   Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT  Highly motivated, self-learner, and technically inquisitive  Experience in programming language Java and understanding of the software development life cycle  Project work on the skills  Knowledge of Core Java , javascript , C++ or software programming  Spring boot, Microservices, Docker, Jenkins and REST API's experience  Excellent written and verbal communication skills 
 For data Science/Machine learning Positions Required Skills  Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT  Project work on the technologies needed  Highly motivated, self-learner, and technically inquisitive  Experience in programming language Java and understanding of the software development life cycle  Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools  Excellent written and verbal communication skills 
 Preferred skills: NLP, Text mining, Tableau, PowerBI, SAS, Tensorflow  If you get emails from our skill enhancement team please email them or ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team who only connect with candidates who are matching client requirements.  No phone calls please.  Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidatesAt SynergisticIT, we're all about making connections. Whatever IT goals you have, our software programmers can help achieve those. Our Software development teams can take up turnkey projects and execute them in an effective and efficient manner. If you are looking to source talent our recruiters will find the ideal IT talent for your company. What's the secret to our success? Well, it all starts with taking quality time to listen to each client's specific needs. After we have a thorough grasp of your IT goals, we can better customize our Developments as per your specific needs. We can also tailor make recruiting programs to exceed your expectations. Since our founding in 2010, SynergisticIT's strategies have earned the company an enviable position in the software development, IT staffing and IT skill enhancement fields. SynergisticIT continues to work with hundreds of satisfied American clients with our software programmers working on our projects and after gaining hands on experience on cutting edge technologies moving to contribute their skills to great clients like Apple, Google, Client, Ebay, Paypal, Kroger, the Walt Disney Company and hundreds more. If you are tired of working with inefficient programmers who take a lot of time to ramp up we want you to try us. Our software programmers can hit the ground running and get you the maximum return on your investment. You have already tried the rest its time you tried the best. SynergisticIT - Home of the Best Data Scientists and Software Programmers in the Bay Area. Why Us ? SynergisticIT has a proven track record of successfully skill enhancement and staffing IT employees for some of the world's most iconic brands. Our team takes the time to fully understand every client's needs so we could best meet your IT staffing requirements. The knowledgeable staff at SynergisticIT is always more than happy to work with clients to ensure they reach their software development goals. Besides staffing, SynergisticIT is also committed to helping young IT professionals advance their career with a robust upskill program . Everyone who goes through SynergisticIT's program learns all the skills necessary to succeed in many IT fields ranging from Java to Machine Learning. Additionally, everyone trained at SynergisticIT has been through extensive mock and technical interview screenings to bolster their career prospects. Last, but certainly not least, SynergisticIT takes great care to respect the privacy considerations for every client. All companies who work with SynergisticIT can rest assured their confidential data is protected using the most up-to-date encryption technologies. SynergisticIT also complies with all the latest NDA agreements. REQUIRED SKILLS For Java /Software Programmers   Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT  Highly motivated, self-learner, and technically inquisitive  Experience in programming language Java and understanding of the software development life cycle  Project work on the skills  Knowledge of Core Java , javascript , C++ or software programming  Spring boot, Microservices, Docker, Jenkins and REST API's experience  Excellent written and verbal communication skills 
 For data Science/Machine learning Required Skills  Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT  Project work on the technologies needed  Highly motivated, self-learner, and technically inquisitive  Experience in programming language Java and understanding of the software development life cycle  Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools  Excellent written and verbal communication skills 
 Preferred skills: NLP, Text mining, Tableau, PowerBI, Time series analysis  We regularly interact with the Top Tech companies to give our candidates a competitive advantage-Please see us exhibiting at Oracle Cloud world /Oracle Java one (Las vegas) -2022 and at Gartner Data Analytics Summit (Florida)-2023 Oracle CloudWorld Event (OCW) Las Vegas 2022 | SynergisticIT - YouTube https://www.youtube.com/watch?v=OAFOhcGy9Z8  https://www.youtube.com/watch?v=EmO7NrWHkLM  https://www.youtube.com/watch?v=NVBU9RYZ6UI  https://www.youtube.com/watch?v=Yy74yvjatVg SynergisticIT at Gartner Data and Analytics Summit 2023 - YouTubeFor preparing for interviews please visit  https://www.synergisticit.com/interview-questions/  We are looking for the right matching candidates for our clients  Please apply via the job posting  REQUIRED SKILLS For Java /Full Stack/Software Programmer   Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT  Highly motivated, self-learner, and technically inquisitive  Experience in programming language Java and understanding of the software development life cycle  Project work on the skills  Knowledge of Core Java , javascript , C++ or software programming  Spring boot, Microservices, Docker, Jenkins and REST API's experience  Excellent written and verbal communication skills 
 For data Science/Machine learning Positions Required Skills  Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT  Project work on the technologies needed  Highly motivated, self-learner, and technically inquisitive  Experience in programming language Java and understanding of the software development life cycle  Knowledge of Statistics, SAS, Python, Computer Vision, data visualization tools  Excellent written and verbal communication skills 
 Preferred skills: NLP, Text mining, Tableau, PowerBI, Tensorflow  If you get emails from our skill enhancement team please ask them to take you off their distribution list and make you unavailable as they share the same database with the client servicing team.  No phone calls please.  Shortlisted candidates would be reached out. No third party or agency candidates or c2c candidates."
Principal Data Engineer - Remote,Get It Recruit - Information Technology,"Martin's Addition, MD (Remote)",https://www.linkedin.com/jobs/view/3767571673/?eBP=JOB_SEARCH_ORGANIC&refId=LoHTn4kkVPcv20fL1OcqCg%3D%3D&trackingId=ozyR9FXrhqtb%2FiCMGqI9%2Bg%3D%3D&trk=flagship3_search_srp_jobs,3767571673,"About the job
            
 
Join Our Team and Make an Impact in the World of Customer Data!We believe that customer data is the key to our growth and transformation. We are embarking on a transformative journey, and your expertise can be a driving force behind one of our most critical initiatives. The Single View of the Customer (SVOC) Team is in search of talented senior to principal-level data engineers to join our dynamic team.About UsWe are committed to harnessing the power of customer data to drive innovation and enhance the customer experience. As a part of our SVOC team, you will collaborate with business analysts, product owners, ADLs, and managers to deliver agile projects, as well as design, develop, deploy, and maintain our codebase.Who We Are Looking ForWe are seeking an intellectually curious and solution-oriented data engineer who thrives on learning new tools and techniques. As a member of our team, you will have the opportunity to lead important efforts, including re-platforming our data services, providing real-time streaming capabilities to our business applications, and enhancing GEICO ID - the source of record for customer data throughout the GEICO journey.Position ResponsibilitiesAs a Principal Data Engineer, you will:Focus on a few key areas and provide leadership to the engineering teams.Take ownership of the complete solution across its entire life cycle.Design, build, and optimize scalable data pipelines and ETL processes to support data ingestion, transformation, and storage.Ensure data quality and integrity through robust data validation and cleansing processes.Identify and evaluate new technologies and tools in the data engineering space to improve efficiency and drive innovation.Stay updated with industry trends and advancements in data engineering and recommend relevant strategies and technologies for adoption.Collaborate with product managers, team members, customers, and other engineering teams to solve complex problems for building enterprise-class business applications.Lead in design sessions and code reviews to elevate the quality of engineering across the organization.Mentor junior team members professionally to help them realize their full potential.Consistently share best practices and improve processes within and across teams.QualificationsExperience in data software development, including data technologies such as Relational and NoSQL databases, open data formats, and programming languages like Python, Scala, and other frameworks.Experience in building data pipelines (ETL and ELT) with batch or streaming ingestion, loading and transforming data, and developing with big data technologies such as Spark, Hadoop, and MapReduce.Experience with Azure data stack such as ADF, Eventhub, Databricks in an Azure Kubernetes environment (AKS) is a plus.Experience with NoSQL databases like Cassandra.Strong background in SQL.Experience with Graph data is a plus (Neo4j, Datastax, etc).Experience working with data warehousing stacks such as Snowflake, Delta Lake, or ADLS.Experience implementing Customer Data Platform or MDM initiatives.Demonstrated history of defining standards and best practices as well as mentoring developers.Ability to work independently as well as function as an integral part of a team, take initiative, and contribute in a fast-paced environment.Knowledge of developer tooling across the software development life cycle (task management, source code, building, deployment, operations, real-time communication).Ability to balance multiple projects concurrently and manage changes in scope along the way.Strong communication and interpersonal skills to collaborate with vendors, business users, and executives, with the ability to communicate technical solutions in business terms.Azure certification, at the associate level (Solutions Architect or Developer), or specialty (Big Data) is a strong advantage.Experience6+ years of professional software development experience.3+ years of experience with architecture and design.3+ years of experience with AWS, GCP, Azure, or another cloud service.4+ years of experience in open source frameworks.1+ years of people management experience.EducationBachelor's degree in Computer Science, Information Systems, or equivalent education or work experience.Location:This position offers remote work options, allowing you to collaborate with a diverse team from the comfort of your home.Employment Type: Full-Time"
Senior Principal Data Engineer - Remote | WFH,Get It Recruit - Information Technology,"New Brunswick, NJ (Remote)",https://www.linkedin.com/jobs/view/3763878720/?eBP=JOB_SEARCH_ORGANIC&refId=LoHTn4kkVPcv20fL1OcqCg%3D%3D&trackingId=e%2BbZMR38D8wji8eJ74ch0g%3D%3D&trk=flagship3_search_srp_jobs,3763878720,"About the job
            
 
Discover a workplace that goes beyond the ordinary. At our organization, every day presents uniquely interesting challenges across various departments, from optimizing production lines to groundbreaking advancements in cell therapy. We take pride in work that not only transforms the lives of patients but also propels the careers of those dedicated to making a difference. Here, you'll find opportunities for growth and success that are unparalleled in scale and scope, surrounded by high-achieving teams rich in diversity.Our Commitment to YouWe understand the importance of balance and flexibility in the workplace. That's why we offer a comprehensive range of competitive benefits, services, and programs designed to empower our employees to pursue their goals, both professionally and personally.The Role: Senior Principal Data Engineer (Fully Remote)Joining our diverse and high-achieving team means becoming part of the Informatics and Predictive Sciences (IPS) mission. At IPS, we pioneer, partner, and predict to drive transformative insights for patient benefit. Our applied computational research spans genomic, structural, and molecular informatics, computational and systems biology, patient selection and translational biomarker research, and broader fields like knowledge science, epidemiology, and machine learning.ResponsibilitiesInnovate and advise on the latest technologies and standard methodologies in Data Engineering.Collaborate across departments to make a wide variety of data accessible and relevant for researchers.Design and develop infrastructure and processes for loading public and proprietary data from multiple source systems.Lead multi-functional teams to implement cloud-based, integrated data platforms.Collaborate with analysts to apply solutions for the integration and delivery of complex biologic data.Collaborate with project management, solution architects, infrastructure teams, and external vendors as needed.Basic QualificationsBachelor's Degree with 10+ years of academic/industry experience in engineering or biology, or equivalent.Master's Degree with 8+ years of academic/industry experience in engineering or biology, or equivalent.PhD with 6+ years of academic/industry experience in engineering or biology, or equivalent.Preferred QualificationsDemonstrated proficiency in current software engineering methodologies.Proficiency in processing, analyzing, and constructing complex data capture, management, and dissemination solutions.Experience with predictive modeling approaches and/or working with large language models preferred.Excellent skills in an object-oriented programming language (Python, Java), proficiency in SQL and R.High degree of proficiency in cloud computing, with a preference for AWS experience.Solid understanding of container strategies (Docker, Fargate, ECS, ECR).Excellent skills and deep knowledge of databases (Postgres, Elasticsearch, Redshift, Aurora).Experience developing web applications in frameworks like Shiny, Vue, React is a plus.Employment Type: Full-Time"
GCP Data Architect/Engineer,Experfy,"Boston, MA (Remote)",https://www.linkedin.com/jobs/view/3646108777/?eBP=JOB_SEARCH_ORGANIC&refId=LoHTn4kkVPcv20fL1OcqCg%3D%3D&trackingId=xbVndk3MJBFI%2B2lQQ%2BDwPQ%3D%3D&trk=flagship3_search_srp_jobs,3646108777,"About the job
            
 
ResponsibilitiesWe are looking for a GCP Data Architect/Engineer who has experience in building enterprise level solution on GCP cloud environment with Kafka, Pubsub, Snowflake, BigQuery, Cloud function, AI Platform, Dataflow, Dataproc, Cloud Compute, AppEngine, etc.Ability to continuously learn, work independently, and make decisions with minimal supervision. You like nothing more than to watch colleagues flourish under your guidance. Be a technological cloud advocate to a wider audience inside and outside of the business. Curiosity - Bring fresh and bold ideas to the team and wider firm be it new software or containerisation of existing infrastructure and projects Job SummaryThis role is for the Infrastructure Engineering team. The teams mission is to be the best Infrastructure team ever, by using the right tools for the right job, failing fast and automating like our lives depend on it. Right from the leadership level, our team are fully committed to the DevOps culture and are consistently trying to change to ensure that culture is nurtured and grows.We operate at genuine scale taking advantage of our alliance partnerships with the industry leading public cloud providers to deliver cutting edge solutions to clients on a global stage. If the public cloud is your future, youre Google Cloud Platform practitioner, youre amazing at solving difficult, interesting and complex challenges and actual writing the code You will be working to continuously advance and standardise our clients infrastructure and pipelined deployments, whilst collaborating with colleagues to write infrastructure as code that scales and takes advantage of the technologies available in the marketRequirementsProven experience managing a team of engineers and being responsible for delivering projects Ability to context switching between talking technical to other DevOps engineers, to explaining the business benefits of containers to non-technical business partnersProven experience with Linux/Unix/Microsoft configurationHands on experience with GCPProven experience in software development or scripting languages such as Python, Ruby, Go, Shell or equivalentSolid CI/CD pipelining skills using any recent DevOps tooling or equivalent Product focussed, build once and reuse Build it run it, take ownership of your code and infrastructure Automates everything including testing"
Senior Azure Data Engineer,Techstra Solutions,"Chicago, IL (Remote)",https://www.linkedin.com/jobs/view/3749310522/?eBP=JOB_SEARCH_ORGANIC&refId=LoHTn4kkVPcv20fL1OcqCg%3D%3D&trackingId=i0hTBP1id3ct2gjj7fvF8A%3D%3D&trk=flagship3_search_srp_jobs,3749310522,"About the job
            
 
As an Azure Data Engineer at Techstra Solutions, you will play a pivotal role in designing, implementing, and managing data architecture on the Azure cloud platform. You will collaborate with cross-functional teams to ensure that the data infrastructure meets business needs and adheres to industry best practices. If you are a strategic thinker with strong technical expertise, this role offers an exciting opportunity to shape the data landscape.Responsibilities: Develop and maintain data pipelines using Azure Data Factory and other relevant Azure technologiesOptimize ETL processes for performance and scalability, leveraging Azure services such as Azure Data Factory Mapping Data Flows, Azure Data Lake Analytics, or Azure Synapse PipelinesExtract data from various sources, transform it into a usable format, and load it into Azure data storage solutions such as Azure SQL Database, or Azure Synapse AnalyticsCollaborate with cross-functional teams to understand data requirements and design scalable and efficient ETL processes using Azure servicesIdentify, design and implement ETL solutions for extraction and integration of data to and from data warehouses and data marts for the purposes of reporting, decision support and analysisLead the design, development, and deployment of data solutions on the Azure cloud platformExpert level understanding on Azure Data Factory, Azure Synapse, Azure SQL, Azure Data Lake, and Azure App Service is required.
Designing and building data pipelines using API ingestion and Streaming ingestion methods.Knowledge of Dev-Ops processes (including CI/CD) and Infrastructure as code is essential.Knowledge in Azure Databricks, Azure IoT, Azure HDInsight + Spark, Azure Stream Analytics, Power BI is desirableManage Azure SQL Databases, Azure Data Factory, Azure Data Warehouses, and other Azure-based data storesKnowledge of C# and Hands-on with PowerShell scripting is desirable.
 Working knowledge of Python is desirable
Qualifications: Bachelor's degree in Computer Science, Information Technology, or related fieldMust have at least2 years’ experience in Azure Data FactoryMust have at least 7 years experience with Microsoft Azure platform and services
Azure certifications (e.g., Azure Data Engineer, Azure Solutions Architect) preferredStrong expertise in data modeling, ETL processes, and database managementProficiency in SQL, Azure SQL, and Azure Data Lake StorageFamiliarity with data warehousing concepts and toolsExcellent problem-solving and communication skills
Strong project management and leadership abilitiesAbility to work collaboratively in cross-functional teams
Location:This position is located in Chicago, IL, Pittsburgh, PA, or remote.At Techstra Solutions, we help top companies and brands achieve the business value of Digital and Talent Transformation. We believe there are three components in successful business transformation: Business Strategy, Technology and Talent. It is the coming together of these three disciplines that enable companies to take full advantage of opportunities. It differentiates us. Our approach is holistic and all encompassing. We consider the full picture as we guide our clients on this journey.We are experts in transformation, business strategy, technology, innovation, and human capital management. We deliver our expertise through client consulting, innovative staffing solutions and software development. From strategy through implementation, we are dedicated to bringing our clients world-class business and talent solutions that fit strategic requirements and most importantly, deliver results."
Senior GCP Data Engineer,Damco Solutions,United States (Remote),https://www.linkedin.com/jobs/view/3705759917/?eBP=JOB_SEARCH_ORGANIC&refId=LoHTn4kkVPcv20fL1OcqCg%3D%3D&trackingId=B9F5jvP99Lg5UGSShiCGSg%3D%3D&trk=flagship3_search_srp_jobs,3705759917,"About the job
            
 
Immediate Position with Known Ford Manager Need submittal. Senior GCP Data EngineerFulltime permanentImmediate start100% RemotePlease note that this position is a hands-on coding experience.Job Responsibilities. Overall experience in Data 10+ Years 5 to 6 years Hands on experience in implementing GCP services in enterprise setup 2-3 years of python development experience Lead complex's big data and data warehousing projects Expert in airflow and Apache beam 3-4 years' experience in implementing IAC and CICD pipelines Provide technical and thought leadership to the Teams and drive the implementationThe interview round will be a fully technical with the tech leads as panel members (who are hands-on with the code and data)Below are the questions asked by the panel during the client round:Q1. How to Set up Data Pipelines (for loading primarily batch input and streaming input)Q2. How to use AirflowQ3. How to use BigQuery (for loading data, stored procedures, consuming from BigQuery for analytics)Q4. How to handle security and exceptionsQ5. Best practices with GCP elines

Desired Skills and Experience
                DATA ENGINEER"
Senior Health Data Migration Engineer Remote,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3642379153/?eBP=JOB_SEARCH_ORGANIC&refId=LoHTn4kkVPcv20fL1OcqCg%3D%3D&trackingId=n0m5ysW%2F61EQHch%2Bxif7ng%3D%3D&trk=flagship3_search_srp_jobs,3642379153,"About the job
            
 
Title: Senior Health Data Migration EngineerPosition Type: ContractLocation: Remote, United States15+ years of professional work experience, to include experience with InterSystems IRIS in a healthcare environmentBachelor's degree in Computer Science, Engineering, Math, or equivalent, or an additional 8 years of relevant experience may be substituted for degree requirementsLocation: RemoteActivitiesSupport the Department of Veterans Affairs (VA) Electronic Health Record Modernization Integration Office (EHRM-IO) as a Senior Health Data Migration Engineer.Assess the current EHRM data migration requirements; maintain and update the strategy to meet the requirements. Review error and trace logs. Track messages by domain and reconcile table counts with Cerner. Review secure data message transmission logs. Track the number of records sent per message by domain. Monitor Queue Depth by service/process/operation. Track retry attempts and suspended messages. Validate and update data integration reports in support of VX130 data domains.Review, update, and maintain Cerner to CDW, VistA, Millennium or Cloud Database data mappings for potential data migrations. Evaluate and integrate data from multiple sources, which requires data mapping from one data source to another minimizing any data loss. Document VistA Extraction and monitoring process and update existing documentation quarterly. Interpret Cerner's Data model to be used for the construction of API's, queries, and reports that will be consumed by internal or external applications.Review Domain adds to Ensemble Production. Validate edits made to Domain record type, schema version, status, and payload size via the GUI Interface and Rule Builder. Validate Ensemble data flows built using VX130 ClassBuilder. Validate the load of Cache/IRIS Objects, SQL Tables or other storage structures(Historical Pulls) in all regions/districts for classes in all environments with VistA or Data Syndication data.RequirementsMinimum qualifications:15+ years of professional work experience, to include experience with InterSystems IRIS in a healthcare environmentBe able to create strategies and plans for integration of multiple IT systems/subsystems into an operational unit, ensuring full functional and performance capabilities are retained.Able to coordinate with development and user teams to assess risks, goals and needs and ensure that all are adequately addressed.Experienced in introducing new hardware or software into a new or existing environment while minimizing disruption and mitigating risks.Able to be cost conscience as well addressing goals.Bachelor's degree in Computer Science, Engineering, Math, or equivalent, or an additional 8 years of relevant experience may be substituted for degree requirementsPreferred QualificationsExperience in the VAExperience implementing EHRExperience with VA and DoD legacy health data, and private sector health dataExperience with Extraction, Transformation, and Loading of data between systemsKnowledge and experience handling VistA data"
AWS Data Warehouse-Data Lake Architect/Engineer || Remote,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3636430068/?eBP=JOB_SEARCH_ORGANIC&refId=LoHTn4kkVPcv20fL1OcqCg%3D%3D&trackingId=VmU619a3UZ2oPyQ0D%2BqGHQ%3D%3D&trk=flagship3_search_srp_jobs,3636430068,"About the job
            
 
Role title - AWS Data Warehouse-Data Lake Architect/Engineer100% remote working PST6 month contractCommunication MUST be excellent
Here Is Skill Set 5+ years of experience as a data warehouse architect Experience designing and implementing an AWS data lake and Redshift data warehouse Excellent written and verbal communication skills Advanced SQL programming skills Advanced AWS skills in S3, Lake Formation, Redshift, Glue, Athena, Data Pipeline and EC2 Experience developing data pipelines in AWS 5+ years of experience as a data engineer and/or data integration developer Experience migrating from an on-prem data warehouse to AWS cloud data lake and Redshift environment Advanced Python development Desired Skills Experience migrating Informatica PowerCenter mappings to Informatica IDMC Infrastructure as code using Terraform Kafka data streams Experience with AWS RDS administration, specifically SQL Server Experience establishing governance processes, development standards and access controls within the AWS data lake environment.Experience administering Informatica PowerCenter and IDMC"
GCP Cloud Data Engineer,Wise Skulls,United States (Remote),https://www.linkedin.com/jobs/view/3726862051/?eBP=JOB_SEARCH_ORGANIC&refId=LoHTn4kkVPcv20fL1OcqCg%3D%3D&trackingId=YBKYV%2Fg5CrGjQq2drTQjag%3D%3D&trk=flagship3_search_srp_jobs,3726862051,"About the job
            
 
Title: GCP Cloud Data EngineerLocation: RemoteDuration: 6+ MonthsImplementation Partner: InfosysEnd Client: To be disclosed:Jd 6+ years of experienceCloud Migration experience - preferably on GCPPython, Pyspak, HadoopBig Query, GCP servicesGood handson on SQL"
Senior Big Data Engineer (Remote),Alivia Analytics,"Boston, MA (Remote)",https://www.linkedin.com/jobs/view/3771508465/?eBP=JOB_SEARCH_ORGANIC&refId=LoHTn4kkVPcv20fL1OcqCg%3D%3D&trackingId=FOt3GjGbqDQCh7%2FY9%2BwhjQ%3D%3D&trk=flagship3_search_srp_jobs,3771508465,"About the job
            
 



      This job is sourced from a job board.
      Learn More



Job DescriptionParticular Details Job TitleBig Data EngineerPosition LevelSenior-Level/Experience ProfessionalIndustryInformation TechnologyTotal Position01Job TypeFull TimeCompanyAlivia AnalyticsAbout The CompanyAlivia Analytics™ is following a mission to bend the healthcare cost curve by using analytic applications and data science to identify cases of fraud, waste, and abuse (FWA). By turning mountains of data into actionable answers, Alivia Analytics™ does the heavy lifting – delivering the accuracy, confidence, and speed you need to solve the healthcare payment integrity challenges. By putting powerful, easy-to-use, advanced technology into the hands of payment integrity business leaders and experts, Alivia Analytics™ empowers users to go beyond recovery to prevention and simulation within days. Alivia takes pride in stating that Alivia is observing the lowest false positive rate in the industry.Currently, approximately 8 trillion dollars are spent globally in healthcare annually from which FWA accounts for up to 10% of every dollar spent in the healthcare systems. That equates to almost 800 billion dollars lost annually. FWA is a growing problem - as providers find ways to evade traditional forms of rule-based fraud detection, our clients need to enhance their firepower with advanced analytic systems. Our development team and data science team build applications using JavaScript that leverage algorithms built using Python, R, and SQL to identify these bad actors.About The PositionWe have an opportunity for a highly motivated senior-level: Data Engineer to join our rapidly growing team. You will have broad opportunities to succeed and grow, both technically and non-technically. Our development team is the keystone of our corporate structure, directly translating business problems into technical solutions for our global clients. This makes communication skills as important as developing skills. As a start-up, we want you to be able to grow with us. You’ll be able to learn from a management team with a combined 60+ years of technical and medical expertise and a history of successful exits. Our founder has developed 70+ systems in his career, 2 of which are still number 1 in the world today.About The RoleWe are seeking a highly skilled Cloud Data Engineer with at least 5 years of experience in designing, developing, documenting, and integrating applications using Big Data platforms like Snowflake, Databricks, Hadoop, and Hive. The successful candidate will have expertise in deploying these pipelines to cloud infrastructure hosted in AWS or Azure.Job Description/Key Areas Of Responsibilities Gather requirements from business/user groups to analyze, design, develop, and implement data pipelines according to customer requirements. Process data from Azure/AWS data storage using Databricks and SnowflakeOptimize table design and indexing for end-user ease of use as well as workload performance. Work with various input file formats including delimited text files, log files, Parquet files, JSON files, XML files, Excel files, and others. Develop automated ETL procedures to load data from various sources into our application’s data warehouse. Ensure pipeline structure is standardized across different customers, each may have their own unique input data format. Configure monitoring systems to detect failure and performance degradation of ETL pipelines. Work with the DevOps team to design CI/CD pipelines to conduct ETL upgrades. Deploy and leverage cloud infrastructure and services to assist in ETL pipeline definition and automation. Understand data modeling (Dimensional and relational) concepts like Star-Schema Modeling, Schema Modeling, Fact, and Dimension tables. Have strong knowledge of both SQL and No SQL databases. Collaborate with business partners, operations, senior management, etc. on day-to-day operational support. Work with high volumes of data with stringent performance requirementsUse programming languages like Python to clean raw data before processing (e.g., removing newline characters/delimiters within fields)Define data quality and validation checks to preemptively detect potential issues. Ensure ETL pipelines are HIPAA-compliant, run with minimal permissions, and securely manage any passwords and secrets used for authentication. Document ETL pipeline logic, structure, and field lineage for review by both technical and non-technical audiencesExpertise in processing data from Azure/AWS data storage using Databricks and Snowflake. 
Key Technical Skills Set:Data Modeling: Understanding of data modeling concepts, including Star-Schema Modeling, Schema Modeling, Fact and Dimension tables.Database Knowledge: Strong knowledge of both SQL and NoSQL databases.ETL Development: Developing automated ETL procedures to load data from various sources into a data warehouse.Ensuring ETL pipelines are HIPAA-compliant and securely manage authentication credentials.File Formats: Working with various input file formats, including delimited text files, log files, Parquet files, JSON files, XML files, Excel files, etc.Performance Optimization: Optimizing table design and indexing for end-user ease of use and workload performance.Monitoring and CI/CD: Configuring monitoring systems to detect pipeline failures and performance degradation.Collaborating with the DevOps team to design CI/CD pipelines for ETL upgrades.Data Quality and Validation: Defining data quality and validation checks to preemptively detect potential issues in data.Programming Languages: Proficiency in programming languages like Python (for data cleaning and preprocessing), R, Java, and Scala.Version Control: Experience with version control systems like Git for managing code and configurations.Problem-Solving: Excellent problem-solving skills, including troubleshooting and resolving issues in data pipelines.Documentation: Documenting ETL pipeline logic, structure, and field lineage for technical and non-technical audiences.Communication and Collaboration: Strong communication and collaboration skills to work with stakeholders from different backgrounds and levels of expertise.Data Engineer Requirements Bachelor’s degree in computer science or a related field5+ years of experience in designing, developing, documenting, and integrating applications using Big Data platforms like Snowflake and DatabricksExtensive experience working on both Azure and AWS, ideally using native ETL tooling (e.g., Azure Data Factory)Strong experience in cleaning, pipelining, and analyzing large data sets. Adept in programming languages like R, Python, Java, and ScalaExperience with git for version controlExcellent problem-solving skills and ability to work independently and as part of a team. Strong communication and collaboration skills, with the ability to work with stakeholders from different backgrounds and levels of expertise. 
Company DescriptionAlivia Analytics is helping customers Achieve Healthcare Payment Integrity, Finally. By turning mountains of data into actionable answers, Alivia Analytics does the heavy lifting – delivering the accuracy, confidence, and speed our customers need to solve their healthcare payment integrity challenges. Through the Alivia Analytics Healthcare Payment Integrity Suite TM we help private and public healthcare payers achieve payment integrity globally. In the US alone, up to 10% of every dollar spent is attributed to Fraud, Waste, or Abuse which amounts to up to 370 Billion dollars lost annually. If your ambition is to grow your responsibilities and career while building world-class analytic SaaS systems and fixing a huge problem for social good, please come and join us.Alivia Analytics is helping customers Achieve Healthcare Payment Integrity, Finally. By turning mountains of data into actionable answers, Alivia Analytics does the heavy lifting – delivering the accuracy, confidence, and speed our customers need to solve their healthcare payment integrity challenges. Through the Alivia Analytics Healthcare Payment Integrity Suite TM we help private and public healthcare payers achieve payment integrity globally. In the US alone, up to 10% of every dollar spent is attributed to Fraud, Waste, or Abuse which amounts to up to 370 Billion dollars lost annually. If your ambition is to grow your responsibilities and career while building world-class analytic SaaS systems and fixing a huge problem for social good, please come and join us."
"Data Engineer, Senior",Blackbaud,"Kansas, United States (Remote)",https://www.linkedin.com/jobs/view/3753080442/?eBP=JOB_SEARCH_ORGANIC&refId=LoHTn4kkVPcv20fL1OcqCg%3D%3D&trackingId=EsR%2BqrVciLmLwhbQlOWX2g%3D%3D&trk=flagship3_search_srp_jobs,3753080442,"About the job
            
 
Data EngineerThe Data Engineer role spans a broad range of experience. Data Engineers are responsible for ingestion, transformation and integration of data to provide a platform that supports research, data analysis, data enrichment and data science as well as making data operationally available at scale to consuming products and services. This includes a particular focus on the reliability, efficiency and quality of data assets and processing/integration pipelines.Working with and utilizing standardized tools, technologies and processes Data Engineers design, build and manage the data architecture that enables scalable production of data insight and they work very closely with Data Science to build and operationalize the algorithms that identify insights, predict behavior and prescribe action. This intelligence fuels the development of Decision-Driven Experiences for our customers.Blackbaud Data Engineers understand how to leverage data for business value and social good, and our Data Engineering team develops, cultivates and curates our industry-leading platform of integrated data assets.What You'll Be Doing Develop production level code. Own process features from design to implementation.Assemble large, complex data sets for use in R&D, data science, machine learning, or projection to operational stores.Build the infrastructure that provides efficient and scalable ETL/ELT and integration of data from multiple proprietary and 3rd party sources.Code, test and deploy data pipelines in support of various initiatives.Ensure quality of code, processes, and data assets.Work on complex tasks and/or broad programs that are large and diverse in scope and/or critical in nature. Exercises good judgment in determining technical approach.
What We'll Want You To Have 3+ years of ETL/ELT development, implementation, and supportUnderstanding or knowledge of data integration concepts3+ years of demonstrated success developing with Core Programming Languages – SQL, PythonUnderstanding or knowledge of Azure cloud services – Azure Data Catalog, Azure Data Lake Store, Azure BLOB StoreUnderstanding or knowledge of AWS cloud services – S3, EC2, Data Pipelines3+ years utilizing and developing SQL for relational databases (Oracle, MS SQL Server, MySQL)Understanding or knowledge of Linux/Unix commands, scripting (bash) and file management
Stay up to date on everything Blackbaud, follow us on Linkedin, Twitter, Instagram, Facebook and YouTubeBlackbaud is a remote-first company which embraces a flexible remote work culture. Blackbaud supports hiring and career development for all roles from the location you are in today!Blackbaud is proud to be an equal opportunity employer and is committed to maintaining a diverse and inclusive work environment. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, physical or mental disability, age, or veteran status or any other basis protected by federal, state, or local law.To all recruitment agencies: We do not accept unsolicited agency resumes and are not responsible for any fees related to unsolicited resumes.A notice to candidates: Recruitment Fraudulent Alert: Your personal information and online safety as a candidate mean a lot to us! At Blackbaud and our portfolio of companies, recruiters only direct candidates to apply through our official careers page at https://careers.blackbaud.com/us/en or our official LinkedIn page. Recruiters will never request payments, ask for financial account information or sensitive information like social security numbers, or conduct interviews via Skype. Anyone suggesting otherwise is not a representative of Blackbaud. If you are unsure if a message is from Blackbaud, please email blackbaudrecruiting@blackbaud.com. The starting base pay is $117,200.00 to $157,500.00. Blackbaud may pay more or less based on employee qualifications, market value, Company finances, and other operational considerations.Benefits Include Medical, dental, and vision insuranceRemote-first workforce401(k) program with employer matchFlexible paid time offGenerous Parental LeaveVolunteer for vacationOpportunities to connect to build community and belongingPet insurance, legal and identity protectionTuition reimbursement program
R0011171"
Senior Data Engineer (Remote Available),Vanderbilt University Medical Center,Nashville Metropolitan Area (Remote),https://www.linkedin.com/jobs/view/3713091902/?eBP=JOB_SEARCH_ORGANIC&refId=LoHTn4kkVPcv20fL1OcqCg%3D%3D&trackingId=GNAF3gwbNRUOvc%2F%2B%2FeoU1Q%3D%3D&trk=flagship3_search_srp_jobs,3713091902,"About the job
            
 
Discover Vanderbilt University Medical Center: Located in Nashville, Tennessee, and operating at a global crossroads of teaching, discovery, and patient care, VUMC is a community of diverse individuals who come to work each day with the simple aim of changing the world. It is a place where your expertise will be valued, your knowledge expanded, and your abilities challenged. Vanderbilt Health recognizes that diversity is essential for excellence and innovation. We are committed to an inclusive environment where everyone has the chance to thrive and where your diversity of culture, thinking, learning, and leading is sought and celebrated. It is a place where employees know they are part of something that is bigger than themselves, take exceptional pride in their work and never settle for what was good enough yesterday. Vanderbilt’s mission is to advance health and wellness through preeminent programs in patient care, education, and research.Organization:HealthIT Data Platform SvcsJob Summary:Job SummaryThe Data Platform Services team is seeking an experienced Data Engineer with an inquisitive and analytical mindset to join our team. In your role, you will be responsible for developing, maintaining, and optimizing our cloud-based data infrastructure, as well as designing and implementing efficient data extraction and ingestion processes. You will also be responsible for working with stakeholders to identify data needs, investigating opportunities to improve platform scalability, building generic solutions for patterned problems, and ensuring data accuracy and integrity.Key ResponsibilitiesIndependently and full proficient to: Design and develop performant data pipelines that support a variety of source system types (flat files, APIs, databases, etc.)Troubleshoot complex issues with production pipelines and work with internal stakeholders to deploy fixesDevelop tools and services used by other teams to create, test, and deliver data-related assetsDesign and develop cloud infrastructure-as-code to support the data platform Participate in code review sessions for merge requests and assist in mentoring developers in best development practicesMay assist with onboarding and training new employees as needed
Technical CapabilitiesMinimum Qualifications Bachelor's Degree4+ years C# and/or Python in an OOP paradigm experience2+ years with SQL experience2+ years implementing cloud solutions with Azure, AWS, or GCP2+ years with containerization with Docker or Podman
Preferred Qualifications 2+ years with CI/CD automation in GitLab, GitHub, or Azure DevOpsExperience with REST data services, APIs, and microservicesExperience with Infrastructure as Code solutions using Terraform, Bicep, or ARMExperience with Git
Bonus Qualifications Experience with Databricks or Apache SparkExperience with PowerShell and/or BashGitHub profile link to some personal code examples
Our professional administrative functions include critical supporting roles in information technology and informatics, finance, administration, legal and community affairs, human resources, communications and marketing, development, facilities, and many more.At our growing health system, we support each other and encourage excellence among all who are part of our workforce. High-achieving employees stay at Vanderbilt Health for professional growth, appreciation of benefits, and a sense of community and purpose.Core Accountabilities:Organizational Impact: Independently delivers on objectives with understanding of how they impact the results of own area/team and other related teams. Problem Solving/ Complexity of work: Utilizes multiple sources of data to analyze and resolve complex problems; may take a new perspective on existing solution. Breadth of Knowledge: Has advanced knowledge within a professional area and basic knowledge across related areas. Team Interaction: Acts as a ""go-to"" resource for colleagues with less experience; may lead small project teams.Core Capabilities : Supporting Colleagues: - Develops Self and Others: Invests time, energy, and enthusiasm in developing self/others to help improve performance e and gain knowledge in new areas. - Builds and Maintains Relationships: Maintains regular contact with key colleagues and stakeholders using formal and informal opportunities to expand and strengthen relationships. - Communicates Effectively: Recognizes group interactions and modifies one's own communication style to suit different situations and audiences. Delivering Excellent Services: - Serves Others with Compassion: Seeks to understand current and future needs of relevant stakeholders and customizes services to better address them. - Solves Complex Problems: Approaches problems from different angles; Identifies new possibilities to interpret opportunities and develop concrete solutions. - Offers Meaningful Advice and Support: Provides ongoing support and coaching in a constructive manner to increase employees' effectiveness. Ensuring High Quality: - Performs Excellent Work: Engages regularly in formal and informal dialogue about quality; directly addresses quality issues promptly. - Ensures Continuous Improvement: Applies various learning experiences by looking beyond symptoms to uncover underlying causes of problems and identifies ways to resolve them. - Fulfills Safety and Regulatory Requirements: Understands all aspects of providing a safe environment and performs routine safety checks to prevent safety hazards from occurring. Managing Resources Effectively: - Demonstrates Accountability: Demonstrates a sense of ownership, focusing on and driving critical issues to closure. - Stewards Organizational Resources: Applies understanding of the departmental work to effectively manage resources for a department/area. - Makes Data Driven Decisions: Demonstrates strong understanding of the information or data to identify and elevate opportunities. Fostering Innovation: - Generates New Ideas: Proactively identifies new ideas/opportunities from multiple sources or methods to improve processes beyond conventional approaches. - Applies Technology: Demonstrates an enthusiasm for learning new technologies, tools, and procedures to address short-term challenges. - Adapts to Change: Views difficult situations and/or problems as opportunities for improvement; actively embraces change instead of emphasizing negative elements.Position Qualifications:Responsibilities:Certifications:Work Experience:Relevant Work ExperienceExperience Level:5 yearsEducation:Bachelor'sVanderbilt Health recognizes that diversity is essential for excellence and innovation. We are committed to an inclusive environment where everyone has the chance to thrive and to the principles of equal opportunity and affirmative action. EOE/AA/Women/Minority/Vets/Disabled"
Senior Azure Data Engineer,OmniData,"Portland, OR (Remote)",https://www.linkedin.com/jobs/view/3766867639/?eBP=JOB_SEARCH_ORGANIC&refId=LoHTn4kkVPcv20fL1OcqCg%3D%3D&trackingId=%2Bjn%2FW78ZBFIgOx0ZNHzFwg%3D%3D&trk=flagship3_search_srp_jobs,3766867639,"About the job
            
 
What We Are Looking For –A passionate, hungry, and motivated individual that is eager for a chance to join a young startup, experiencing rapid growth. At OmniData, we are searching for a remote Senior Azure Data Engineer that has hands on production experience in developing PySpark solutions in Synapse Analytics on data warehousing and analytics projects. We need a team player who has a an exceptional reputation for mentoring less experienced teammates. We seek someone with a strong technical aptitude with expertise in translating complex technical information that appropriately meets the needs of the client while skillfully deploying the best strategies for client analytics goals. In return, we offer deep mentorship, a great work/life balance, and the opportunity to be part of creating a consulting firm that makes a difference for our clients!What You Will Do –You will work on various Big Data, Data Warehouse and Analytics projects for our world class customers. In addressing complex client needs, you will be integrated into appropriately sized and skilled teams. This will give you the opportunity to analyze requirements, develop data and analytical solutions, and execute as part of the project team, all while working with the latest tools, such as Azure Synapse Analytics and related Microsoft technologies.Your Duties And Responsibilities – Contribute collaboratively to team meetings using your experience base to further the cause of innovating for OmniData clients. Instill confidence in the client as well as your teammatesWork independently toward client success, at the same time knowing your own limitations and when to call on others for help. 
What you must have to be considered – 7+ years of experience in Analytics and Data Warehousing on the Microsoft platform1-2 years advanced experience in PySpark solutions in Synapse analyticsExperience working with the Microsoft Azure stack (e.g. Synapse, Databricks, DataFactory etc.)
What Would Be Nice For You To Have – Experience with PythonExperience gathering requirements and working within various project delivery methodologiesExperienced working as a customer facing consultantExposure to DAXStrong communication skills tying together technologies and architectures to business results
Benefits And Perks Health Coverage: 100% Employee Coverage (Up to a 1500 PPO Plan), 60% Coverage for dependentsDental/VisionLife Insurance, Aflac, Short/Long term disability, HSA etc. 9 Company holidays (Ability to use them as 'floating' Holidays)Paid time off (PTO) 15 daysFlexible Sick TimeMaternity/Paternity Leave (Birth Parent: 3 months, Non-birth parent: 1 month401k - up to a 5% match. Eligibility to contribute the month after start date. Vests as soon as you are eligible for contribution. Opportunities for career growth and advancement, as well as helping to shape a young consulting firmAbility to learn from highly skilled consultants with years of industry experienceExposure to the latest and greatest data warehousing, analytics, and cloud technologiesFlexible schedules in a hybrid work environmentBasic Life Insurance (50k)LT and ST Disability (paid by employee)Flexible Spending Account (FSA) and Healthcare Savings Account (HSA)Employee Assistance Program (EAP)Mental health and substance abuse conditions are serious and sometimes require 24/7 access to resources. Ethics and Compliance Reporting & Pulse SurveysEmployee Engagement & Cultural initiatives: Health & Wellness, Pulse Surveys, and Kolbe Instinctive StrengthsCommuter BenefitsWeWork office provision
About OmniDataLearn more about our culture and read the testimonials from some members by visiting our Careers Page.OmniData is a Portland, Oregon based Data and Analytics focused consulting firm leveraging the Microsoft technology stack to help organizations build their Modern Data Estates, designed to serve their digital innovation needs for many years to come. To do this, we apply deep experience in Solution Architecture, Data, Analytics, and technology to simplify the complex.OmniData is offering you the opportunity to work with the entire lifecycle of large Data Projects, focused on next-generation data warehousing, with surface points to Analytics, Machine Learning and AI. We offer a collaborative work culture, that enables you to produce client results with a safety net from your team. You will get to work closely with very experienced consultants who will be able to provide mentorship and career guidance. At the same time, you will be rewarded for learning fast and executing within our teams to provide solutions for OmniData clients.OmniData ValuesWe build partnerships that last. We are ambitious and set aggressive goals. We embody professional humility. We are prepared. Visit www.omnidata.com/AboutUs to learn more.OmniData is a Portland - based Data and Analytics focused consulting firm leveraging the Microsoft technology stack to help organizations build their Modern Data Estates, designed to serve their digital innovation needs for many years to come. To do this, we apply deep experience in Solution Architecture, Data, Analytics, and technology to simplify the complex.OmniData is offering you the opportunity to work with the entire lifecycle of large Data Projects, focused on next generation data warehousing, with surface points to Analytics, Machine Learning and AI. We offer a collaborative work culture, that enables you to produce client results with a safety net from your team. You will get to work closely with very experienced consultants who will be able to provide mentorship and career guidance. At the same time, you will be rewarded for learning fast and executing within our teams to provide solutions for OmniData clients."
"Data Engineer, Senior",Blackbaud,"Alabama, United States (Remote)",https://www.linkedin.com/jobs/view/3753083149/?eBP=JOB_SEARCH_ORGANIC&refId=LoHTn4kkVPcv20fL1OcqCg%3D%3D&trackingId=QZDyezfmGLOup8HYF7NI%2Fg%3D%3D&trk=flagship3_search_srp_jobs,3753083149,"About the job
            
 
Data EngineerThe Data Engineer role spans a broad range of experience. Data Engineers are responsible for ingestion, transformation and integration of data to provide a platform that supports research, data analysis, data enrichment and data science as well as making data operationally available at scale to consuming products and services. This includes a particular focus on the reliability, efficiency and quality of data assets and processing/integration pipelines.Working with and utilizing standardized tools, technologies and processes Data Engineers design, build and manage the data architecture that enables scalable production of data insight and they work very closely with Data Science to build and operationalize the algorithms that identify insights, predict behavior and prescribe action. This intelligence fuels the development of Decision-Driven Experiences for our customers.Blackbaud Data Engineers understand how to leverage data for business value and social good, and our Data Engineering team develops, cultivates and curates our industry-leading platform of integrated data assets.What You'll Be Doing Develop production level code. Own process features from design to implementation.Assemble large, complex data sets for use in R&D, data science, machine learning, or projection to operational stores.Build the infrastructure that provides efficient and scalable ETL/ELT and integration of data from multiple proprietary and 3rd party sources.Code, test and deploy data pipelines in support of various initiatives.Ensure quality of code, processes, and data assets.Work on complex tasks and/or broad programs that are large and diverse in scope and/or critical in nature. Exercises good judgment in determining technical approach.
What We'll Want You To Have 3+ years of ETL/ELT development, implementation, and supportUnderstanding or knowledge of data integration concepts3+ years of demonstrated success developing with Core Programming Languages – SQL, PythonUnderstanding or knowledge of Azure cloud services – Azure Data Catalog, Azure Data Lake Store, Azure BLOB StoreUnderstanding or knowledge of AWS cloud services – S3, EC2, Data Pipelines3+ years utilizing and developing SQL for relational databases (Oracle, MS SQL Server, MySQL)Understanding or knowledge of Linux/Unix commands, scripting (bash) and file management
Stay up to date on everything Blackbaud, follow us on Linkedin, Twitter, Instagram, Facebook and YouTubeBlackbaud is a remote-first company which embraces a flexible remote work culture. Blackbaud supports hiring and career development for all roles from the location you are in today!Blackbaud is proud to be an equal opportunity employer and is committed to maintaining a diverse and inclusive work environment. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, physical or mental disability, age, or veteran status or any other basis protected by federal, state, or local law.To all recruitment agencies: We do not accept unsolicited agency resumes and are not responsible for any fees related to unsolicited resumes.A notice to candidates: Recruitment Fraudulent Alert: Your personal information and online safety as a candidate mean a lot to us! At Blackbaud and our portfolio of companies, recruiters only direct candidates to apply through our official careers page at https://careers.blackbaud.com/us/en or our official LinkedIn page. Recruiters will never request payments, ask for financial account information or sensitive information like social security numbers, or conduct interviews via Skype. Anyone suggesting otherwise is not a representative of Blackbaud. If you are unsure if a message is from Blackbaud, please email blackbaudrecruiting@blackbaud.com. The starting base pay is $117,200.00 to $157,500.00. Blackbaud may pay more or less based on employee qualifications, market value, Company finances, and other operational considerations.Benefits Include Medical, dental, and vision insuranceRemote-first workforce401(k) program with employer matchFlexible paid time offGenerous Parental LeaveVolunteer for vacationOpportunities to connect to build community and belongingPet insurance, legal and identity protectionTuition reimbursement program
R0011171"
Data Engineer (ETL) and System Admin(min 12 yrs+ exp) (100% REMOTE) (Webcam interviews),Prohires,"Washington, DC (Remote)",https://www.linkedin.com/jobs/view/3619490247/?eBP=JOB_SEARCH_ORGANIC&refId=LoHTn4kkVPcv20fL1OcqCg%3D%3D&trackingId=mM9qlQijRjFdy1Du7r%2BQUA%3D%3D&trk=flagship3_search_srp_jobs,3619490247,"About the job
            
 
We are looking for Data Engineer (ETL) and System Admin(min 12 yrs+ exp) (REMOTE) (Webcam interviews)POSITION DESCRIPTIONDIRECT CLIENT PositionNumber of positions: 1Length: 10 Months + Location: Washington, DC 20019Immediate interviews Webcam interviews Please note that this position is REMOTE. Candidate must go to Washington DC to pick up equipment and complete onboarding process on 1st day and then work remotely. Client will NOT ship equipment.11-15 years of experience needed to serve as the subject matter expert on data architecture for the client's multi-year Data Management Project to modernize the client's data management systems and implement an enterprise data warehouse.client requires the services of a Data Engineer staff augmentation consultant to serve as the data engineer for the Data Management Project team. The Data Management Project is a multi-year effort by DOES to ensure that the agency has accurate and reliable data maintained by high quality systems, all agency data are readily available and consistently used for well-informed decisions, and appropriate and accurate data are regularly released to the public. The Data Management Project team is responsible for meeting the project goals: (1) establish a data governance program, (2) perform a comprehensive data gap analysis, (3) design a master data architecture, (4) create a data warehouse for all data assets, (5) develop a front-end for program staff to quickly access workforce information and visualize program status, (6) create a public portal with user-friendly scorecards to help residents make more informed decisions about workforce opportunities, and (7) foster relations with other DC agencies and improve inter-agency data integration.Specific Duties:The Data Management Project Data Engineer serves as an ETL developer and system administrator for the agency's multi-year Data Management Project to modernize the client's data management systems and implement an enterprise data warehouse, with a focus on data asset analysis, supporting data quality measures, and the enhancement of reporting and analysis for program staff. Specific duties for the position include: Develop, test, and maintain extraction, transformation, and load (ETL) processes. Support the System Administration of associated tools and software of data and analytics landscape including the data catalog, master data, data sources, data lakes, data warehouses, data assets, and analytic products. Support the Data Management Project team to develop and maintain data quality controls. Support the program staff and the Data Management Project team to develop and enhance data analysis and reporting to support program operations. Support business data stewards and Data Management Project team in maintaining accurate data asset meta-data and updating data documentation. Support the data stewards to troubleshoot and resolve data issues. Support business users to obtain requirements for enhancements and/or new analytic assets. Assist in the Development of data asset training and documentation. Participate in the development and implementation of a client's data standard. Participate in the development and maintenance of data security, privacy, policies, procedures, and best practices. 
Required Skills And Experience  Programming: proficiency in SQL, Python, R, JavaScript, JSON, and Excel Formulas & Charts.Familiarity with CKAN, DKAN, and/or ArcGIS is an added advantage. Operating Systems: Windows and Linux. Modeling Tools: proficiency in the use and administration of Erwin Modeling Suite (Data Modeler Navigator, Data Modeler WGE, Mart Server, Web Portal), or similar tools for developing and maintaining data models. Testing: proficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser Testing. Nice to Have: Experience and understanding of BI tool architecture, functions, features (ie. Tableau and/or MicroStrategy). Nice to Have: Outstanding consultative skills, understanding business needs and translating those needs to technical requirements and solutions. Nice to Have: Ability to communicate effectively with all levels of the organization including business owners, leaders, and executives. 
Bachelor's Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred.Candidates must have ALL the ""Required"" skills in order to be considered for the position. Skill MatrixExperience with Business workflow processesRequired / DesiredAmountof ExperienceExperience with System Administration of Linux-based Operating Systems and ServersRequired12YearsExperience with System Administration of Windows-based Operating Systems and ServersRequired12YearsExperience with System Administration of Microsoft SQLServerRequired12YearsExperience with System Administration of PostGresSQLRequired12YearsExperience in the maintenance and monitoring of system & network related activities associated with data security and controls.Required12YearsExperience leveraging SQL relational databases to manage complex datasets and analytical reportingRequired12YearsExperience in developing, testing, maintaining the process of transaction, transformation, and load (ETL) data processesRequired12YearsExperience with command-line scripting (Ex: Linux-Bash, GREP, SED, AWK, Kerberos, LDAP, CRON, Windows PowerShell, Windows Active Directory, Windows SRequired12YearsExperience with programing scripting languages (Ex: Python, R/RStudio, JavaScript, JSON/CVS, Perl, Scala, C/C++)Required12YearsBachelor's Degree from an accredited four-year university in Computer Science, Information Management, or other relevant field is preferred.Required15YearsUnderstanding of the Software Development Lifecycle (SDLC) in Agile and Waterfall environmentsRequired8YearsProficiency in the use and administration of Erwin Modeling Suite (Data Modeler Navigator, Data Modeler WGE, Mart Server, Web Portal), or similar toolRequired8YearsProficiency with Agile Testing, Automation Testing, Black-box Testing, Unit Testings, Cross-Browser TestingRequired8YearsCommunication skills, both written & spoken, business level English mandatory ability to articulate technical terms and complex data clearly to non-teRequired8YearsWriting System Guides, Manuals, and User Guides for complex systemsRequired8YearsFamiliarity with Cloud Computing, Cloud Storage, DataLakes, and/or Data warehousesRequired2YearsFamiliarity with CKAN, DKAN, and/or ArcGISRequired2Years"
Data Engineer,Motion Recruitment,"Los Angeles, CA (Remote)",https://www.linkedin.com/jobs/view/3762366344/?eBP=JOB_SEARCH_ORGANIC&refId=LoHTn4kkVPcv20fL1OcqCg%3D%3D&trackingId=cxT93qhrpsTHxDZoPPvvqg%3D%3D&trk=flagship3_search_srp_jobs,3762366344,"About the job
            
 
Big Data EngineerA client of ours in the financial space is looking to hire a Big Data Engineer to join their team. You will be responsible for developing and designing software applications as well as modifying existing applications to meet business requirements.Required Skills & Experience 5+ years of software development experience 3+ years of experience with Map-Reduce, Hive, Spark Hands-on experience writing and understanding complex SQL Experience in UNIX shell-scripting Bachelor’s degree in Engineering or Computer Science or equivalent 
What You Will Be DoingTech Breakdown 100% Data Engineering 
Daily Responsibilities 100% Hands On 
Applicants must be currently authorized to work in the US on a full-time basis now and in the future. This role cannot be done on a C2C basis.Posted By: Julie Bennett"
Big Data Devops Engineer,eTeki Inc.,United States (Remote),https://www.linkedin.com/jobs/view/3634712079/?eBP=JOB_SEARCH_ORGANIC&refId=LoHTn4kkVPcv20fL1OcqCg%3D%3D&trackingId=51J1ePUQoPmV5YunvB1lcA%3D%3D&trk=flagship3_search_srp_jobs,3634712079,"About the job
            
 
At eTeki, Big Data DevOps Engineer top freelance professionals assess peers being considered for similar technical roles with the respect and courtesy of a face-to-face conversation. Your feedback helps recruiters and hiring managers focus their resources on the most qualified professionals in the hiring process.With your proficiency in HadoopSQL QueriesSparkCI CD Jenkins Java DevOps Scala you can make a difference to our clients across the globe. You'll confirm interest on a per-job basis and availability on a per-candidate basis by using our platform and the support of a world-class team of product specialists. If you enjoy talking about technology and have the desire to raise hiring standards for your profession (while making extra money), eTeki’s the right side gig for you.Requirement Bachelor's or Master's degree in Computer Science, Data Engineering, or a related field.Proven experience as a Data Engineer, preferably with hands-on experience in DevOps.Strong proficiency in big data technologies, including Hadoop, Hive, and HBase.Extensive experience with Spark using Scala for data processing and analytics.Solid programming skills in Java for custom data processing and optimization.Proficiency in implementing real-time streaming solutions using Kafka.Experience with Continuous Integration and Continuous Deployment (CI/CD) using Jenkins.Strong understanding of data modeling, database design, and ETL processes.Familiarity with cloud platforms (e.g., AWS, Azure, or GCP) and containerization technologies (e.g., Docker, Kubernetes).
Responsibilities Prepare an agenda according to client specifications and best practices for predictive interview resultsReview the candidate’s resume in advance of the technical discussion to validate the accomplishments claimedFollow independent contractor’s guidelines pertaining to avoiding legal landmines and maintaining candidate morale during interactive video technical interviews.Assess the candidate’s technical competencies and depth of experience through structured questions and quantify using standardized rubricsWrite reports for clients that detail the technical strengths and weaknesses of the candidates.
eTeki, a technical interviews-as-a-service platform, helps every organization, big or small, hire top-notch technical talent by matching technical interview experts with clients who need third-party screening expertise. More information online at www.eteki.com/freelancersThe RoleBachelor's or Master's degree in Computer Science, Data Engineering, or a related field. Proven experience as a Data Engineer, preferably with hands-on experience in DevOps.Strong proficiency in big data technologies, including Hadoop, Hive, and HBase.Extensive experience with Spark using Scala for data processing and analytics.Solid programming skills in Java for custom data processing and optimization.Proficiency in implementing real-time streaming solutions using Kafka.Experience with Continuous Integration and Continuous Deployment (CI/CD) using Jenkins.Strong understanding of data modeling, database design, and ETL processes.Familiarity with cloud platforms (e.g., AWS, Azure, or GCP) and containerization technologies (e.g., Docker, 
Ideal ProfileBachelor's or Master's degree in Computer Science, Data Engineering, or a related field. Proven experience as a Data Engineer, preferably with hands-on experience in DevOps.Strong proficiency in big data technologies, including Hadoop, Hive, and HBase.Extensive experience with Spark using Scala for data processing and analytics.Solid programming skills in Java for custom data processing and optimization.Proficiency in implementing real-time streaming solutions using Kafka.Experience with Continuous Integration and Continuous Deployment (CI/CD) using Jenkins.Strong understanding of data modeling, database design, and ETL processes.Familiarity with cloud platforms (e.g., AWS, Azure, or GCP) and containerization technologies (e.g., Docker,
What's on Offer? Big Data Devops Engineer"
Senior Data Engineer (BioPharma and/or Life Sciences),Ambit Inc.,"Boston, NY (Remote)",https://www.linkedin.com/jobs/view/3773459613/?eBP=JOB_SEARCH_ORGANIC&refId=LoHTn4kkVPcv20fL1OcqCg%3D%3D&trackingId=hyeWWSbvqFnL1ktkYhREHg%3D%3D&trk=flagship3_search_srp_jobs,3773459613,"About the job
            
 
About Ambit: Ambit is a healthcare technology and services company that provides data, analytics, consulting, and platform-based offerings to biopharma companies with a focus on rare and specialty diseases. Ambit capabilities include the areas of Strategy, Digital, Analytics, Digital Transformation, and Patient Identification & Activation.About Ambit Leadership:Ambit is led by a team of entrepreneurs with decades of rare and specialty disease experience and a passion to deliver data-driven innovation for our Biopharma partners.Overview:At Ambit we are on a mission to transform the world of rare and specialty disease. We are looking for people who excel in their work and are able to manage ambiguity with resourcefulness and creativity. Our core values are: Set high standardsDeliver on ambitious goalsBe relentless and innovativeBe responsible and trusted
 Job Summary: We are seeking an experienced and motivated Senior Data Engineer to join our dynamic team. In this role, you will have the unique opportunity to work at the intersection of Data Engineering and DevOps, playing a vital role in developing and maintaining our data infrastructure while ensuring the efficient and reliable delivery of our data analytics products. As a Data Engineer, you will work closely with cross-functional teams, including data scientists, software developers, and product teams, to enable the seamless integration of data-driven solutions with our software products.  As Data Engineer, you will:   Design, develop, and maintain the data infrastructure and systems to support Ambit products, solutions, and projects.  Build and optimize data pipelines for extracting, transforming, and loading (ETL) large volumes of data from various sources into our data environment in a structured format suitable for analysis.  Collaborate with data scientists and analysts to understand data requirements and design efficient and scalable data processing workflows.  Develop algorithms and workflows to clean, transform, and integrate data from multiple sources, ensuring its quality and consistency.  Implement data governance policies, access controls, and security measures to protect sensitive data and ensure compliance with relevant regulations. Optimize data processing and retrieval performance to ensure efficient and timely access to data. Work closely with product teams, especially developers, to integrate data-driven solutions with our software products, ensuring seamless functionality and performance. Design, build, and manage CI/CD pipelines for software build, testing, and deployment, ensuring efficient and reliable software delivery. Implement infrastructure-as-code principles to automate the provisioning and management of infrastructure resources. Set up monitoring and logging systems to ensure the availability, performance, and security of our software systems. Manage system configurations, ensuring consistency across different environments, and employ orchestration tools to automate deployment and scaling. Collaborate with cross-functional teams to identify areas for process improvement and implement DevOps best practices. Stay updated with emerging technologies, industry trends, and best practices in Data Engineering and DevOps to drive innovation within the organization. 
 Qualifications:   Bachelor's degree in Computer Science, Engineering, or a related field. Master's degree preferred. Minimum of 5 years of professional experience in a Software Engineering role. Minimum of 3 years of experience in a biopharmaceutical or life sciences company, having knowledge in industry processes, commonly available/used data sources as well as data management, and regulatory compliance requirements. Strong experience in data engineering, including data pipeline development, data integration, and data management. Experience and proficiency in handling secondary data sources such as claims data (835/837), EMR/EHR, EDI 852/867, sell-in data, Rx dispense, formulary data, as well as digital and sales force execution data. Life sciences-specific knowledge in master data management for entities such as patient, HCP, HCO, and MCO; as well as reference data management including medical codes such as ICD, CPT, HCPCS, NDC, etc. and clinical/medical taxonomies such as HPO and SNOMED CT. Proficiency in programming languages such as Python, Java, or Scala, with experience in SQL and database technologies. Demonstrated expertise and experience in working with Microsoft Azure cloud platform, including knowledge of Azure services such as Azure Data Factory, Azure Databricks, Azure Storage, Azure SQL Database, and Azure Kubernetes Service (AKS). Hands-on experience in deploying and managing data and application workloads on Azure, leveraging Azure services for data engineering and DevOps tasks. Solid understanding of DevOps principles, CI/CD practices, and experience with tools like Jenkins, Git, and configuration management systems. Experience in defining and deploying infrastructure resources programmatically using infrastructure-as-code tools and techniques in Azure environments, including resource grouping, tagging, and parameterization for scalable and reusable deployments. Understanding of Azure security and compliance practices, including experience implementing data governance, access controls, and security measures in Azure environments. Strong problem-solving skills and the ability to work in a fast-paced, high-growth environment with tight deadlines. 
Base Salary Range: $126,000 – $170,000Qualities that we seek: In addition to strong educational background and relevant work experience, we evaluate the following: Problem solving abilitiesAttention to detailCommunication skillsOrganization and planning skillsInterested in growing and taking on additional responsibilities as part of a fast-paced entrepreneurial work environmentWork ethic
 We treat our team right: Competitive compensation is just the beginning. As part of our team, you can expect:  An early-stage growth company filled with passionate and fun teammates, dedicated to changing every life touched by rare disease Flexible vacation policy with company wide shutdowns to unplug, rest, and recharge401(k) (full-time employees)Medical, dental, vision & disability insurance (full-time employees)Parental leave (full-time employees)"
"Data Engineer, Senior",Blackbaud,"Tennessee, United States (Remote)",https://www.linkedin.com/jobs/view/3753079701/?eBP=JOB_SEARCH_ORGANIC&refId=LoHTn4kkVPcv20fL1OcqCg%3D%3D&trackingId=cNhcpyKYekRJzuOg6MhJqA%3D%3D&trk=flagship3_search_srp_jobs,3753079701,"About the job
            
 
Data EngineerThe Data Engineer role spans a broad range of experience. Data Engineers are responsible for ingestion, transformation and integration of data to provide a platform that supports research, data analysis, data enrichment and data science as well as making data operationally available at scale to consuming products and services. This includes a particular focus on the reliability, efficiency and quality of data assets and processing/integration pipelines.Working with and utilizing standardized tools, technologies and processes Data Engineers design, build and manage the data architecture that enables scalable production of data insight and they work very closely with Data Science to build and operationalize the algorithms that identify insights, predict behavior and prescribe action. This intelligence fuels the development of Decision-Driven Experiences for our customers.Blackbaud Data Engineers understand how to leverage data for business value and social good, and our Data Engineering team develops, cultivates and curates our industry-leading platform of integrated data assets.What You'll Be Doing Develop production level code. Own process features from design to implementation.Assemble large, complex data sets for use in R&D, data science, machine learning, or projection to operational stores.Build the infrastructure that provides efficient and scalable ETL/ELT and integration of data from multiple proprietary and 3rd party sources.Code, test and deploy data pipelines in support of various initiatives.Ensure quality of code, processes, and data assets.Work on complex tasks and/or broad programs that are large and diverse in scope and/or critical in nature. Exercises good judgment in determining technical approach.
What We'll Want You To Have 3+ years of ETL/ELT development, implementation, and supportUnderstanding or knowledge of data integration concepts3+ years of demonstrated success developing with Core Programming Languages – SQL, PythonUnderstanding or knowledge of Azure cloud services – Azure Data Catalog, Azure Data Lake Store, Azure BLOB StoreUnderstanding or knowledge of AWS cloud services – S3, EC2, Data Pipelines3+ years utilizing and developing SQL for relational databases (Oracle, MS SQL Server, MySQL)Understanding or knowledge of Linux/Unix commands, scripting (bash) and file management
Stay up to date on everything Blackbaud, follow us on Linkedin, Twitter, Instagram, Facebook and YouTubeBlackbaud is a remote-first company which embraces a flexible remote work culture. Blackbaud supports hiring and career development for all roles from the location you are in today!Blackbaud is proud to be an equal opportunity employer and is committed to maintaining a diverse and inclusive work environment. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, physical or mental disability, age, or veteran status or any other basis protected by federal, state, or local law.To all recruitment agencies: We do not accept unsolicited agency resumes and are not responsible for any fees related to unsolicited resumes.A notice to candidates: Recruitment Fraudulent Alert: Your personal information and online safety as a candidate mean a lot to us! At Blackbaud and our portfolio of companies, recruiters only direct candidates to apply through our official careers page at https://careers.blackbaud.com/us/en or our official LinkedIn page. Recruiters will never request payments, ask for financial account information or sensitive information like social security numbers, or conduct interviews via Skype. Anyone suggesting otherwise is not a representative of Blackbaud. If you are unsure if a message is from Blackbaud, please email blackbaudrecruiting@blackbaud.com. The starting base pay is $117,200.00 to $157,500.00. Blackbaud may pay more or less based on employee qualifications, market value, Company finances, and other operational considerations.Benefits Include Medical, dental, and vision insuranceRemote-first workforce401(k) program with employer matchFlexible paid time offGenerous Parental LeaveVolunteer for vacationOpportunities to connect to build community and belongingPet insurance, legal and identity protectionTuition reimbursement program
R0011171"
Senior Data Engineer Consultant - Remote,Syrinx Consulting,"Massachusetts, United States (Remote)",https://www.linkedin.com/jobs/view/3620301308/?eBP=JOB_SEARCH_ORGANIC&refId=LoHTn4kkVPcv20fL1OcqCg%3D%3D&trackingId=8OgmZe%2FvgZEhBhmdXLo6EQ%3D%3D&trk=flagship3_search_srp_jobs,3620301308,"About the job
            
 
Senior Data Engineer Consultant This is a Remote Role with a Syrinx Healthcare Tech PartnerU.S. Citizens and those authorized to work in the U.S. are encouraged to apply. We are unable to sponsor at this time. Python and SQL are must-havesAs a Senior Data Engineer Consultant, you'll be leading efforts in building scalable backend services and data pipelines that support hundred of millions of users and thousands of events per second. You will get to work across multiple microservices written in Python, Scala, and Ruby, using technologies such as AWS Lambda, Kinesis, SQS, RDS, Dynamo, Snowflake, Spark, HDFS, S3, ElasticSearch, ECS/Fargate, Athena, Presto, Cadence, Serverless Framework, Docker, Terraform, and more.  Development efforts of new services that deal with large scale data or high throughput data pipelines/streams Architect and scale out our ETL framework to support back data processing in technologies such as Spark Manage and implement scalable monitoring and escalation strategies across our systems Act as a technical architect, elegantly separating domain models to ensure partner complexity doesn't leak into our app Invest in infrastructure that ensures our small team can efficiently manage hundreds of integrations
Qualifications  8+ years software engineering experience Python and SQL Experience in a few of those is ideal Experience in designing and implementing scalable applications/microservices Experience creating robust RESTful APIs Emphasis on clean, well-designed code Deep understanding of Postgres, MySQL, and other relational databases Experience working with large datasets/databases and scaling memory-intensive applications Gritty mentality with a focus on shipping Process-oriented executor; you can manage multiple projects concurrently and prioritize effectively"
AWS Data Warehouse-Data Lake Architect/Engineer,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3635382093/?eBP=JOB_SEARCH_ORGANIC&refId=LoHTn4kkVPcv20fL1OcqCg%3D%3D&trackingId=bRsXCfIzSfvZLmzKny3ITA%3D%3D&trk=flagship3_search_srp_jobs,3635382093,"About the job
            
 
Role title - AWS Data Warehouse-Data Lake Architect/Engineer100% remote working PST6 month contractCommunication MUST be excellent
Here Is Skill Set 5+ years of experience as a data warehouse architect Experience designing and implementing an AWS data lake and Redshift data warehouse Excellent written and verbal communication skills Advanced SQL programming skills Advanced AWS skills in S3, Lake Formation, Redshift, Glue, Athena, Data Pipeline and EC2 Experience developing data pipelines in AWS 5+ years of experience as a data engineer and/or data integration developer Experience migrating from an on-prem data warehouse to AWS cloud data lake and Redshift environment Advanced Python development Desired Skills Experience migrating Informatica PowerCenter mappings to Informatica IDMC Infrastructure as code using Terraform Kafka data streams Experience with AWS RDS administration, specifically SQL Server Experience establishing governance processes, development standards and access controls within the AWS data lake environment.Experience administering Informatica PowerCenter and IDMC"
Data Engineer II (Talend),Centene Corporation,"Florida, United States (Remote)",https://www.linkedin.com/jobs/view/3760308074/?eBP=JOB_SEARCH_ORGANIC&refId=LoHTn4kkVPcv20fL1OcqCg%3D%3D&trackingId=eegPBMkung3SPk4AXb4rMg%3D%3D&trk=flagship3_search_srp_jobs,3760308074,"About the job
            
 
You could be the one who changes everything for our 28 million members by using technology to improve health outcomes around the world. As a diversified, national organization, Centene's technology professionals have access to competitive benefits including a fresh perspective on workplace flexibility.Position Purpose Develops and operationalizes data pipelines to make data available for consumption (reports and advanced analytics), including data ingestion, data transformation, data validation / quality, data pipeline optimization, and orchestration. Engages with the DevSecOps Engineer during continuous integration and continuous deployment. Talend implementation of data flows. Designs and implements standardized data management procedures around data staging, data ingestion, data preparation, data provisioning, and data destruction (scripts, programs, automation, assisted by automation, etc.)Designs, develops, implements, tests, documents, and operates large-scale, high-volume, high-performance data structures for business intelligence analyticsDesigns, develops, and maintains real-time processing applications and real-time data pipelinesEnsure quality of technical solutions as data moves across Centene’s environmentsProvides insight into the changing data environment, data processing, data storage, and utilization requirements for the company and offers suggestions for solutionsDevelops, constructs, tests, and maintains architectures using programming language and toolsIdentifies ways to improve data reliability, efficiency, and quality; use data to discover tasks that can be automatedPerforms other duties as assignedComplies with all policies and standards
Education/Experience A Bachelor's degree in a quantitative or business field (e.g., statistics, mathematics, engineering, computer science).Requires 2 – 4 years of related experience.Or equivalent experience acquired through accomplishments of applicable knowledge, duties, scope and skill reflective of the level of this position.Technical Skills One or more of the following skills are desired.Experience developing Talend (Programming Language) Experience with SQL (Programming Language)Experience with AWSExperience with Healthcare (specifically HEDIS)Experience with Big Data; Data ProcessingExperience with diagnosing system issues, engaging in data validation, and providing quality assurance testingExperience with Data Manipulation; Data MiningExperience working in a production cloud infrastructureKnowledge of Microsoft SQL Servers
Soft Skills Strong Communication and Organizational skills
Our Comprehensive Benefits Package Flexible work solutions including remote options, hybrid work schedules and dress flexibility, Competitive pay, Paid time off including holidays, Health insurance coverage for you and your dependents, 401(k) and stock purchase plans, Tuition reimbursement and best-in-class training and development.Centene is an equal opportunity employer that is committed to diversity, and values the ways in which we are different. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, veteran status, or other characteristic protected by applicable law."
Data Engineer,Genzeon,"Exton, PA (Remote)",https://www.linkedin.com/jobs/view/3776609087/?eBP=JOB_SEARCH_ORGANIC&refId=LoHTn4kkVPcv20fL1OcqCg%3D%3D&trackingId=0R3a1iP%2BHMWIb1bbil6cxA%3D%3D&trk=flagship3_search_srp_jobs,3776609087,"About the job
            
 
Genzeon has an exciting role of Healthcre Data Engineer on remote basis, and I am happy to discuss about this opportunity (details below), you can also reach me on 484-713-9023. We are Genzeon Corp(www.genzeon.com), established in 2009, A cutting-edge global digital company with focus on healthcare. Genzeon helps its clients achieve their goals by providing innovative services & solutions to reduce costs, streamline operations, improve experience, increase provider productivity, and advance patient care and satisfaction. Our culture anchors our core values – Human connection, Accountability & Empowerment. Healthcare Data EngineerRemote/With in USA Employment Term: Fulltime with Benefits  Job DescriptionThe Data Engineer will be responsible to design, build, and maintain data pipelines, data warehouses, and other data-related infrastructure. You will work closely with data scientists, analysts, and other stakeholders to ensure that the data engineering solutions meet business requirements and are scalable, reliable, and efficient. You will also be responsible for mentoring and training team members, driving innovation and best practices, and collaborating with other cross-functional teams.  Handling data integration, preprocessing, and database management. Need someone to define data models; define business objects and defining the relationships (matching patient identifiers de-identification, mapping etc.; factor in EHR workflow data integrations)   Responsibilities- Design and build scalable, reliable, and efficient data pipelines using Python, SQL, AWS, and Linux.- Develop and maintain data warehouses and other data-related infrastructure.- Collaborate with data scientists, analysts, and other stakeholders to ensure that data engineering solutions meet business requirements.- Mentor and train team members to ensure high-quality deliverables.- Continuously drive innovation and best practices in data engineering.- Collaborate with cross-functional teams to identify and resolve data-related issues.- Perform code reviews and ensure code quality.- Provide technical guidance and leadership to the team. Qualification- Bachelor's or master’s degree in computer science or related field.- 3-5 years of experience in data engineering, with a focus on designing and building scalable, reliable, and efficient data pipelines.- Strong expertise in Python and SQL.- Experience with AWS services such as S3, Redshift, Glue, and EMR.- Strong Linux skills.- Experience with distributed systems and big data technologies such as Hadoop, Spark, and Kafka.- Experience leading and mentoring a team of data engineers.- Excellent communication and interpersonal skills.- Ability to work in a fast-paced environment and manage multiple priorities.- Strong problem-solving skills and attention to detail.- Solid understanding of programming SQL objects (procedures, triggers, views, functions) in SQL Server. Experience optimizing SQL queries a plus.- Working Knowledge of Azure Architecture, Data Lake- Advanced understanding of T-SQL, indexes, stored procedures, triggers, functions, views, etc.- Must be detail-oriented. Must work under limited supervision. - Must demonstrate good analytical skills as it relates to data identification and mapping and excellent oral communication skills. - Must be flexible and able to multi-task and be able to work within deadlines; must be team-oriented, but also be able to work independently."
Senior Data Engineer (Remote),"Stride, Inc.",United States (Remote),https://www.linkedin.com/jobs/view/3774074467/?eBP=JOB_SEARCH_ORGANIC&refId=LoHTn4kkVPcv20fL1OcqCg%3D%3D&trackingId=p33yczfNGriAywdrkG7dCA%3D%3D&trk=flagship3_search_srp_jobs,3774074467,"About the job
            
 
Meet StrideSuccess StoriesResponsibility and Inclusion
Over 20 years ago, Stride was founded to provide personalized learning — powered by technology. We reached students where they were in their own journeys. We knocked down their barriers to great education. And we gave every learner equal opportunity to succeed — however they defined success. Stride innovated the learning experience with online and blended learning that prepared them for their lives ahead.Stride is a community of passionate leaders. Whether teachers, engineers, curriculum writers, or financial managers — whatever your expertise or role, we all work to empower futures through learning. And changing the trajectory of learning itself is one of our greatest missions. Join us in developing more effective ways to learn and helping learners build the skills and confidence they need to make their way forward in life.The Senior Data Engineer will play a crucial role in the Stride Marketing Organization and will work closely with the VP of Marketing Technology and Marketing Technology and Analytics team to define, build, and maintain the Marketing Analytics data infrastructure utilizing data from multiple platforms.ESSENTIAL FUNCTIONS:  Reasonable accommodations may be made to enable individuals with disabilities to perform the essential duties. Ability to build and optimize data sets, ‘big data’ data pipelines and architectures.Assist with the creation of the data architecture to ensure data is available, actionable, trustworthy, and reusable for analytics and other purposes.Ability to build processes that support data transformation, workload management, data structures, dependency and metadata.Identify, design and help implement internal process improvements including re-designing infrastructure for greater scalability, optimizing data delivery, and automating manual processes.Ability to perform root cause analysis on external and internal processes and data to identify opportunities for improvement and answer questions.Document data dictionary for all internal data tables.Building required infrastructure for optimal extraction, transformation and loading of data from various data sources.Work with team members to support their data infrastructure needs while assisting with data-related technical issues.Excellent analytic skills associated with working on unstructured datasets.
Supervisory Responsibilities: This position has no formal supervisory responsibilities.MINIMUM REQUIRED QUALIFICATIONS:  Bachelor's or Master's degree in Computer Science, Information Systems, Engineering and 4 years of related experience or equivalent.6+ years of relevant experience with detailed knowledge of data warehouse technical architectures, infrastructure components, and reporting/analytic tools.Strong problem solving, quantitative and analytical abilities.Keen attention to detailMicrosoft Office (Outlook, Word, Excel, PowerPoint, Project, Visio, etc.); Web proficiency.Ability to clear required background check
Certificates and Licenses: None required.DESIRED QUALIFICATIONS:  Knowledge of coding languages or developer tools, such as R, Python, etc.Technologies: SQL, PowerBI, Tableau, Excel, Snowflake, Python, Salesforce, DBT
WORK ENVIRONMENT: The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. This position is virtual and open to residents of the 50 states, D.C.
Compensation & Benefits: Stride, Inc. considers a person’s education, experience, and qualifications, as well as the position’s work location, expected quality and quantity of work, required travel (if any), external market and internal value when determining a new employee’s salary level. Salaries will differ based on these factors, the position’s level and expected contribution, and the employee’s benefits elections. Offers will typically be in the bottom half of the range. We anticipate the salary range to be $77,694.00 to $156,944.60. Eligible employees may receive a bonus. This salary is not guaranteed, as an individual’s compensation can vary based on several factors. These factors include, but are not limited to, geographic location, experience, training, education, and local market conditions. Stride offers a robust benefits package for eligible employees that can include health benefits, retirement contributions, and paid time off. 
The above job is not intended to be an all-inclusive list of duties and standards of the position. Incumbents will follow any other instructions, and perform any other related duties, as assigned by their supervisor. All employment is “at-will” as governed by the law of the state where the employee works. It is further understood that the “at-will” nature of employment is one aspect of employment that cannot be changed except in writing and signed by an authorized officer. Stride, Inc. is a Federal Contractor, an Equal Opportunity/Affirmative Action Employer and a Drug-Free Workplace. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, protected Veteran status age, or genetics, or any other characteristic protected by law."
"Senior Data Engineer - $180k-$220k (Snowflake, Coding)",CyberCoders,"New York, NY (Remote)",https://www.linkedin.com/jobs/view/3766359896/?eBP=CwEAAAGMRQWlAlJT9AeICLAHludWb9_qQXyWVVqhn6vDT0Xs5dp53qhxMNqhwnvvMxVa1hHKq50s0i16_pp6u8XO-MMlSsULUF_DmW8FmOOdG1hWzQRDsVqicIGQ3spvVaA2S19qGTlZRSKwc-hXPWZP7Dmn9jtsVKwaJ8UUtdX-tldkkh8zLZ-JiBNFGK_dcivlvVEDXJwlxkDf2glImzBJoBNUwr9QHpmy17AtFyKyXaHGBzlYqMBcS8aQBLrseDOCEKcnq1uqsu9CWl3cKoNUeZH8-Th0iOVAax2lbi3TmIajY6Kf3iYCOQlJk07Ltyg2W7K6E9Yu2__bJXBYNrIpsz9dYtPLDiLbh9QehO-W1eq2FEnm7yY90KlTsNXeFtResFYV8BaYdeU&refId=hkitJDE%2BFnBPWHIC1GVhxw%3D%3D&trackingId=jolGDz70EDxGLKh7vwMwvA%3D%3D&trk=flagship3_search_srp_jobs,3766359896,"About the job
            
 
Permanently Remote in USJob Title:Senior Data Engineer - $180k-$220k + Bonus (Snowflake, Coding)Salary: $180k-$220k Base + Bonus, No Stock, 401k, BenefitsRequirements: Expert w/ Snowflake & Coding AbilityBased in beautiful New York City, we are a cutting edge ad-tech org for television-based ads.We are founded and owned by T.V.s largest publishers.Our mission is to be bring simplicity & scale to audience based campaigns in television.We're working with over 100 advertisers and anticipating another year of significant growth!As a rapidly growing company (founded in 2017 & up 140% year over year) we've recently elevated our C-Suite Team in preparation for our next stage of growth and are building our Technology, Product, and Operations Executive Teams.We have been in business for 7 years and have around 40 employees.Due to growth, we are actively hiring a Senior Data Engineer with  Snowflake experience (ideally certified) Ability to write production level code (ideally JavaScript or Python) Experience building data pipelines from scratch and/or working with APIs
Experience with the following is a big plus!  Fivetran and/or DBT
You will be working with 12 other engineers on the data side + several other technical folks.This role will consist of tasks such as building out data pipelines, data architecture via snowflake, and data modeling.If you have this experience, please apply immediately. We are actively interviewing this week and next for this high profile position.Top Reasons to Work with Us  Compensation: $180k-$220k Base + Bonus, No Stock, 401k, Benefits Raid Growth: Founded in 2017 & up 140% year over year Culture: Fast paced, mission driven culture Technology: Cutting edge technology
What You Will Be Doing  Building data pipelines from scratch Data architecture via Snowflake Data modeling Technical review of everything this group builds. Mange development velocity, team capacity, and backlogs Partner closely with the product team Take on key assignments and delegate as needed Act as the main technical point of contact for engineering Translate technical requirements to the rest of the engineering team
What You Need for this PositionMust Have Experience  Snowflake experience Ability to write production level code (ideally JavaScript or Python) Experience building data pipelines from scratch and/or working with APIs
Some Experience With:-  Fivetran and/or DBT
What's In It for You $180k-$220k Base + Bonus, No Stock, 401k, Benefits 401k Vacation/PTO Medical Dental Vision Bonus 401k
Benefits  Vacation/PTO Medical Dental Vision Bonus
So, if you are a Senior Data Engineer - $180k-$220k + Bonus (Snowflake, Coding) with experience, please apply today!Colorado employees will receive paid sick leave. For additional information about available benefits, please contact Nitu Gulati-PaulyEmail Your Resume In Word ToLooking forward to receiving your resume through our website and going over the position with you. Clicking apply is the best way to apply, but you may also:Nitu.Gulati-Pauly@cybercoders.com Please do NOT change the email subject line in any way. You must keep the JobID: linkedin : MA5-1745335 -- in the email subject line for your application to be considered.***
Nitu Gulati-Pauly - VP of Recruiting - CyberCodersApplicants must be authorized to work in the U.S.CyberCoders is proud to be an Equal Opportunity EmployerAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any other characteristic protected by law.Your Right to Work – In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.CyberCoders will consider for Employment in the City of Los Angeles qualified Applicants with Criminal Histories in a manner consistent with the requirements of the Los Angeles Fair Chance Initiative for Hiring (Ban the Box) Ordinance."
"Senior Data Engineer - $180k-$220k (Snowflake, Coding)",CyberCoders,"Los Angeles, CA (Remote)",https://www.linkedin.com/jobs/view/3766360819/?eBP=CwEAAAGMRQWlAsUb0VWl_5cIaViTtnyAEjFsLx_8PH8C0mBD02NwjOtM8GeG7TXBZz3l5oQGWAqkPZguqOSrO-jPVx2r_Q_-gPJKh6cTa0lY-UZtQsJca6L3GaNXHLykDg3ybQ-aNySrAp6ObKqV7iqXEZXek_oaYCf3OkG-8RUzaDMDkVL9L6D6ucc4g_Cutu0tkR_BBB6Bpn5VnK5Wla0iW50_7AeWqgkqL2RSCwdDUQnwPEN2lA9aDXW2ZXUAaduAws89mGKEYRqdfurUXhKORlZ8dsqDcW5NRJqmCF_R0EBabAxlpca470HuZkW8E4IbVfz-k27-T2sccB3ROMd0AQQQLRg2ymM7hAB1PYqnb-qzj3JiOy5je3PeAuroPkzRk2Z5KuIoxOs&refId=hkitJDE%2BFnBPWHIC1GVhxw%3D%3D&trackingId=OD12iEfjFLnUeV04KrAJNw%3D%3D&trk=flagship3_search_srp_jobs,3766360819,"About the job
            
 
Permanently Remote in USJob Title:Senior Data Engineer - $180k-$220k + Bonus (Snowflake, Coding)Salary: $180k-$220k Base + Bonus, No Stock, 401k, BenefitsRequirements: Expert w/ Snowflake & Coding AbilityBased in beautiful New York City, we are a cutting edge ad-tech org for television-based ads.We are founded and owned by T.V.s largest publishers.Our mission is to be bring simplicity & scale to audience based campaigns in television.We're working with over 100 advertisers and anticipating another year of significant growth!As a rapidly growing company (founded in 2017 & up 140% year over year) we've recently elevated our C-Suite Team in preparation for our next stage of growth and are building our Technology, Product, and Operations Executive Teams.We have been in business for 7 years and have around 40 employees.Due to growth, we are actively hiring a Senior Data Engineer with  Snowflake experience (ideally certified) Ability to write production level code (ideally JavaScript or Python) Experience building data pipelines from scratch and/or working with APIs
Experience with the following is a big plus!  Fivetran and/or DBT
You will be working with 12 other engineers on the data side + several other technical folks.This role will consist of tasks such as building out data pipelines, data architecture via snowflake, and data modeling.If you have this experience, please apply immediately. We are actively interviewing this week and next for this high profile position.Top Reasons to Work with Us  Compensation: $180k-$220k Base + Bonus, No Stock, 401k, Benefits Raid Growth: Founded in 2017 & up 140% year over year Culture: Fast paced, mission driven culture Technology: Cutting edge technology
What You Will Be Doing  Building data pipelines from scratch Data architecture via Snowflake Data modeling Technical review of everything this group builds. Mange development velocity, team capacity, and backlogs Partner closely with the product team Take on key assignments and delegate as needed Act as the main technical point of contact for engineering Translate technical requirements to the rest of the engineering team
What You Need for this PositionMust Have Experience  Snowflake experience Ability to write production level code (ideally JavaScript or Python) Experience building data pipelines from scratch and/or working with APIs
Some Experience With:-  Fivetran and/or DBT
What's In It for You $180k-$220k Base + Bonus, No Stock, 401k, Benefits 401k Vacation/PTO Medical Dental Vision Bonus 401k
Benefits  Vacation/PTO Medical Dental Vision Bonus
So, if you are a Senior Data Engineer - $180k-$220k + Bonus (Snowflake, Coding) with experience, please apply today!Colorado employees will receive paid sick leave. For additional information about available benefits, please contact Nitu Gulati-PaulyEmail Your Resume In Word ToLooking forward to receiving your resume through our website and going over the position with you. Clicking apply is the best way to apply, but you may also:Nitu.Gulati-Pauly@cybercoders.com Please do NOT change the email subject line in any way. You must keep the JobID: linkedin : MA5-1745335L568 -- in the email subject line for your application to be considered.***
Nitu Gulati-Pauly - VP of Recruiting - CyberCodersApplicants must be authorized to work in the U.S.CyberCoders is proud to be an Equal Opportunity EmployerAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any other characteristic protected by law.Your Right to Work – In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.CyberCoders will consider for Employment in the City of Los Angeles qualified Applicants with Criminal Histories in a manner consistent with the requirements of the Los Angeles Fair Chance Initiative for Hiring (Ban the Box) Ordinance."
Senior Data Engineer / Pyspark \u0026 EMR,Dice,"Los Angeles, CA (Remote)",https://www.linkedin.com/jobs/view/3779710977/?eBP=JOB_SEARCH_ORGANIC&refId=hkitJDE%2BFnBPWHIC1GVhxw%3D%3D&trackingId=Qz3LvR73ZEP3YAacaKZ9pg%3D%3D&trk=flagship3_search_srp_jobs,3779710977,"About the job
            
 
Dice is the leading career destination for tech experts at every stage of their careers. Our client, Motion Recruitment Partners, LLC, is seeking the following. Apply via Dice today!Our client is changing the way we can find our friends and family from around the world. This Senior Data Engineer should have at least 7 years of professional experience woking with Python, PySpark, EMR, Airflow, AWS, and have a STEM degree.Basic Qualifications (Required Skills & Experience) 5+ years of experience, 8+ experience preferred Python Pyspark Spark AWS Airflow EMR ETL work SQL
Other Qualifications & Desired Competencies Fully Remote Equity and Bonuses involved 
You will receive the following benefits: Medical Insurance Dental Benefits Vision Benefits 401(k) 
Senior Data Engineer / Pyspark & EMR"
Sr. Data Engineer (Blockchain) - (Remote in the US),"Resource Informatics Group, Inc","Irving, TX (Remote)",https://www.linkedin.com/jobs/view/3767592303/?eBP=JOB_SEARCH_ORGANIC&refId=hkitJDE%2BFnBPWHIC1GVhxw%3D%3D&trackingId=kcBhAke1r7A7DMUbIhHMLQ%3D%3D&trk=flagship3_search_srp_jobs,3767592303,"About the job
            
 
Sr. Data Engineer (Blockchain) - (Remote in the US) - 1571Salary Range: ABC + bonus + stock optionsLocation: 100% Remote (within US)Position Type: FTE - no contractingWork Authorization: No sponsorship offered. USC and GC onlyCommunication: Excellent in verbal, written and comprehensionThe Role You'll work with a cross-functional team of research scientists and engineers to contribute to our software and data platforms.Drive the design and implementation of data models, ETL pipelines, and data processing logic while working closely with the other teams in order to translate operational data with microservices into business value.Collaborate in proof of concepts (POCs) from research concepts and then lead building those in a production environment.Work on complex problems ranging from mathematical proofs to big data processes to fueling research with blockchain, financial, and gaming metrics.We are looking for someone with a curiosity for data and the ability to iterate quickly in an ambiguous environment.Preferred experience with finance, gaming/crypto gaming, blockchain, or crypto protocols.Bonus Domain expertise in one or more areas: finance, gaming, DeFi, cryptocurrency, blockchain, and smart contracts.
Required Experience Advanced skills with python (or similar language) for data collection, analysis, and communication including familiarity with pandas package, object-oriented programming, and built-in functions.Experience in Web3 including smart contract development, blockchain data, and DeFi protocols such as Uniswap.Hands-on experience in AWS Cloud services including building and implementing server and serverless data engineering, data science, and analytics infrastructure. Bonus for experience integrating services such as Airflow.Experience communicating and delivering results with data scientists, machine learning, and modeling valuable to implementing use cases.Understanding of Test-Driven Development.Good knowledge of ETL, data warehousing, and pipelines design.Preferred 5+ years of experience in a data focused role including ETL, querying data with SQL, data modeling, data collection, data analysis, and data science.Curiosity, empathy, and an interest in working with a team with a high degree of collaboration.An ability to adapt and work under large amounts of uncertainty and ambiguity."
Lead GCP Data engineer,CloudMR,NAMER (Remote),https://www.linkedin.com/jobs/view/3773244966/?eBP=JOB_SEARCH_ORGANIC&refId=hkitJDE%2BFnBPWHIC1GVhxw%3D%3D&trackingId=I%2FA47SYz%2F1ylnhdkzWfXLg%3D%3D&trk=flagship3_search_srp_jobs,3773244966,"About the job
            
 
We are looking for a highly experienced Senior Data Engineer who will collaborate with a diverse group of stakeholders including data scientists, product managers, executives, and other key team members to design, build and maintain the systems and platforms that drive insightful and smart experiences for our customers.Responsibilities: Utilize extensive knowledge of technology platforms, design approaches, and techniques to design integrated quality solutions that meet business requirements throughout the data warehouse lifecycle.Plan and design the integration of all technical components in the data warehouse.Provide recommendations on technical issues to the team.Manage data design, data extracts, and data transforms.Develop implementation and operation support plans.Build high-quality data architecture to support business analysts, data scientists, and customer reporting needs.Extract, transform and load data from various data sources.
Qualifications: Experience in Big Data engineering, data warehousing, business intelligence, or business analytics.Cloud engineering experience with preference given to Google Cloud Platform.Experience leading end-to-end projects independently.Strong skills in data modelling, ETL development, and Data Warehousing.Familiarity with software engineering best practices such as Agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations."
"Data Engineer, Senior",Blackbaud,"Georgia, United States (Remote)",https://www.linkedin.com/jobs/view/3753081378/?eBP=JOB_SEARCH_ORGANIC&refId=hkitJDE%2BFnBPWHIC1GVhxw%3D%3D&trackingId=Lxw8zksIQEEEUMeR2YWOVQ%3D%3D&trk=flagship3_search_srp_jobs,3753081378,"About the job
            
 
Data EngineerThe Data Engineer role spans a broad range of experience. Data Engineers are responsible for ingestion, transformation and integration of data to provide a platform that supports research, data analysis, data enrichment and data science as well as making data operationally available at scale to consuming products and services. This includes a particular focus on the reliability, efficiency and quality of data assets and processing/integration pipelines.Working with and utilizing standardized tools, technologies and processes Data Engineers design, build and manage the data architecture that enables scalable production of data insight and they work very closely with Data Science to build and operationalize the algorithms that identify insights, predict behavior and prescribe action. This intelligence fuels the development of Decision-Driven Experiences for our customers.Blackbaud Data Engineers understand how to leverage data for business value and social good, and our Data Engineering team develops, cultivates and curates our industry-leading platform of integrated data assets.What You'll Be Doing Develop production level code. Own process features from design to implementation.Assemble large, complex data sets for use in R&D, data science, machine learning, or projection to operational stores.Build the infrastructure that provides efficient and scalable ETL/ELT and integration of data from multiple proprietary and 3rd party sources.Code, test and deploy data pipelines in support of various initiatives.Ensure quality of code, processes, and data assets.Work on complex tasks and/or broad programs that are large and diverse in scope and/or critical in nature. Exercises good judgment in determining technical approach.
What We'll Want You To Have 3+ years of ETL/ELT development, implementation, and supportUnderstanding or knowledge of data integration concepts3+ years of demonstrated success developing with Core Programming Languages – SQL, PythonUnderstanding or knowledge of Azure cloud services – Azure Data Catalog, Azure Data Lake Store, Azure BLOB StoreUnderstanding or knowledge of AWS cloud services – S3, EC2, Data Pipelines3+ years utilizing and developing SQL for relational databases (Oracle, MS SQL Server, MySQL)Understanding or knowledge of Linux/Unix commands, scripting (bash) and file management
Stay up to date on everything Blackbaud, follow us on Linkedin, Twitter, Instagram, Facebook and YouTubeBlackbaud is a remote-first company which embraces a flexible remote work culture. Blackbaud supports hiring and career development for all roles from the location you are in today!Blackbaud is proud to be an equal opportunity employer and is committed to maintaining a diverse and inclusive work environment. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, physical or mental disability, age, or veteran status or any other basis protected by federal, state, or local law.To all recruitment agencies: We do not accept unsolicited agency resumes and are not responsible for any fees related to unsolicited resumes.A notice to candidates: Recruitment Fraudulent Alert: Your personal information and online safety as a candidate mean a lot to us! At Blackbaud and our portfolio of companies, recruiters only direct candidates to apply through our official careers page at https://careers.blackbaud.com/us/en or our official LinkedIn page. Recruiters will never request payments, ask for financial account information or sensitive information like social security numbers, or conduct interviews via Skype. Anyone suggesting otherwise is not a representative of Blackbaud. If you are unsure if a message is from Blackbaud, please email blackbaudrecruiting@blackbaud.com. The starting base pay is $117,200.00 to $157,500.00. Blackbaud may pay more or less based on employee qualifications, market value, Company finances, and other operational considerations.Benefits Include Medical, dental, and vision insuranceRemote-first workforce401(k) program with employer matchFlexible paid time offGenerous Parental LeaveVolunteer for vacationOpportunities to connect to build community and belongingPet insurance, legal and identity protectionTuition reimbursement program
R0011171"
AWS Data Engineer - Remtoe,Software Technology Inc.,United States (Remote),https://www.linkedin.com/jobs/view/3702672205/?eBP=JOB_SEARCH_ORGANIC&refId=hkitJDE%2BFnBPWHIC1GVhxw%3D%3D&trackingId=qX%2FpjRrIGlHoPnvk%2Fa3mGg%3D%3D&trk=flagship3_search_srp_jobs,3702672205,"About the job
            
 
Work Location: RemoteContract? Contract-W2 6-12 months+ExtensionDescriptionAWS Python Data EngineerYour future duties and responsibilitiesLooking for a mid-level data engineer with AWS background in designing, building, and maintaining data pipelines and infrastructure using Amazon Web Services (AWS) technologies such as GLUE, pyspark and other data services. Data Engineer who has background in python script language used for ETL, data management process, ensuring that data is collected, processed, stored. This person will be involved with data quality assessment and other data management work including migration and production deployment of data pipelines and ETL jobs.Data availability in EDL, up to ~60 applications and getting data to their downstreamIndividual user access in Cloud, target data needs being determined still in requirements (expecting most consumption to be from databases with limited cases for special views or layers needed to support business intelligence)Consult with data architects as needed in designBuild pipelines from an application to a pre-approved data format where individuals can access the dataUnit/System/Integration test data pipelinesConform to FM development standards and operation standardsRequired Qualifications To Be Successful In This Role7+ years of experience with AWS Development and Cloud Architecture.4+ years of Python or R programming experienceExcellent understanding of SQL programming and databaseshands-on designing, building, and maintaining data pipelines and infrastructure using Amazon Web Services (AWS) technologies such as GLUE, pyspark and other data servicesExperience with BitBucket with understanding of code configuration management conceptsEducational RequirementsExperience with AWS Sagemaker preferredBachelor's degree in computer science, Information Systems or related fieldAWS Certification(s) desiredSkills SQL - 8 year(s)PythonBitbucket"
Senior Data Engineer,Signify Health,"Dallas, TX (Remote)",https://www.linkedin.com/jobs/view/3757723654/?eBP=JOB_SEARCH_ORGANIC&refId=hkitJDE%2BFnBPWHIC1GVhxw%3D%3D&trackingId=mlxSeNt%2B6vYnmA1JsJMR0A%3D%3D&trk=flagship3_search_srp_jobs,3757723654,"About the job
            
 
A Senior Software Engineer - Data develops systems to manage data flow throughout Signify Health’s infrastructure. This involves all elements of data engineering, such as ingestion, transformation, and distribution of data.What will you do? Communicate with business leaders to help translate requirements into functional specificationDevelop broad understanding of business logic and functionality of current systemsAnalyze and manipulate data by writing and running SQL queriesAnalyze logs to identify and prevent potential issues from occurringDeliver clean and functional code in accordance with business requirementsConsume data from any source, such a flat files, streaming systems, or RESTful APIs Interface with Electronic Health RecordsEngineer scalable, reliable, and performant systems to manage dataCollaborate closely with other Engineers, QA, Scrum master, Product Manager in your team as well as across the organizationBuild quality systems while expanding offerings to dependent teamsComfortable in multiple roles, from Design and Development to Code Deployment to and monitoring and investigating in production systems.
Requirements Bachelors in Computer Science or equivalentProven ability to complete projects in a timely manner while clearly measuring progressStrong software engineering fundamentals (data structures, algorithms, async programming patterns, object-oriented design, parallel programming) Strong understanding and demonstrated experience with at least one popular programming language (.NET or Java) and SQL constructs.Experience writing and maintaining frontend client applications, Angular preferredStrong experience with revision control (Git)Experience with cloud-based systems (Azure / AWS / GCP).High level understanding of big data design (data lake, data mesh, data warehouse) and data normalization patternsDemonstrated experience with Queuing technologies (Kafka / SNS / RabbitMQ etc)Demonstrated experience with Metrics, Logging, Monitoring and Alerting toolsStrong communication skillsStrong experience with use of RESTful APIsHigh level understanding of HL7 V2.x / FHIR based interface messages.High level understanding of system deployment tasks and technologies. (CI/CD Pipeline, K8s, Terraform)
About UsSignify Health is helping build the healthcare system we all want to experience by transforming the home into the healthcare hub. We coordinate care holistically across individuals’ clinical, social, and behavioral needs so they can enjoy more healthy days at home. By building strong connections to primary care providers and community resources, we’re able to close critical care and social gaps, as well as manage risk for individuals who need help the most. This leads to better outcomes and a better experience for everyone involved.Our high-performance networks are powered by more than 9,000 mobile doctors and nurses covering every county in the U.S., 3,500 healthcare providers and facilities in value-based arrangements, and hundreds of community-based organizations. Signify’s intelligent technology and decision-support services enable these resources to radically simplify care coordination for more than 1.5 million individuals each year while helping payers and providers more effectively implement value-based care programs.To learn more about how we’re driving outcomes and making healthcare work better, please visit us at www.signifyhealth.comDiversity and Inclusion are core values at Signify Health, and fostering a workplace culture reflective of that is critical to our continued success as an organization.We are committed to equal employment opportunities for employees and job applicants in compliance with applicable law and to an environment where employees are valued for their differences."
Senior Health Data Migration Engineer || Remote,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3706311317/?eBP=JOB_SEARCH_ORGANIC&refId=hkitJDE%2BFnBPWHIC1GVhxw%3D%3D&trackingId=51h6jgryZFqRMp6V7KXbmw%3D%3D&trk=flagship3_search_srp_jobs,3706311317,"About the job
            
 
Experience With IRIS And SQL Experience MustMust need hands on Cache IRIS developer experienceMust have hands on InterSystems experience or IRIS.Title: Senior Health Data Migration EngineerPosition Type: ContractLocation: Remote, United States 15+ years of professional work experience, to include experience with InterSystems IRIS in a healthcare environment Bachelor's degree in Computer Science, Engineering, Math, or equivalent, or an additional 8 years of relevant experience may be substituted for degree requirementsActivitiesSupport the Department of Veterans Affairs (VA) Electronic Health Record Modernization Integration Office (EHRM-IO) as a Senior Health Data Migration Engineer.Assess the current EHRM data migration requirements; maintain and update the strategy to meet the requirements. Review error and trace logs. Track messages by domain and reconcile table counts with Cerner. Review secure data message transmission logs. Track the number of records sent per message by domain. Monitor Queue Depth by service/process/operation. Track retry attempts and suspended messages. Validate and update data integration reports in support of VX130 data domains.Review, update, and maintain Cerner to CDW, VistA, Millennium or Cloud Database data mappings for potential data migrations. Evaluate and integrate data from multiple sources, which requires data mapping from one data source to another minimizing any data loss. Document VistA Extraction and monitoring process and update existing documentation quarterly. Interpret Cerner's Data model to be used for the construction of API's, queries, and reports that will be consumed by internal or external applications.Review Domain adds to Ensemble Production. Validate edits made to Domain record type, schema version, status, and payload size via the GUI Interface and Rule Builder. Validate Ensemble data flows built using VX130 ClassBuilder. Validate the load of Cache/IRIS Objects, SQL Tables or other storage structures(Historical Pulls) in all regions/districts for classes in all environments with VistA or Data Syndication data.RequirementsMinimum qualifications:15+ years of professional work experience, to include experience with InterSystems IRIS in a healthcare environmentBe able to create strategies and plans for integration of multiple IT systems/subsystems into an operational unit, ensuring full functional and performance capabilities are retained.Able to coordinate with development and user teams to assess risks, goals and needs and ensure that all are adequately addressed.Experienced in introducing new hardware or software into a new or existing environment while minimizing disruption and mitigating risks.Able to be cost conscience as well addressing goals.Bachelor's degree in Computer Science, Engineering, Math, or equivalent, or an additional 8 years of relevant experience may be substituted for degree requirementsPreferred QualificationsExperience in the VAExperience implementing EHRExperience with VA and DoD legacy health data, and private sector health dataExperience with Extraction, Transformation, and Loading of data between systemsKnowledge and experience handling VistA data"
"Data Engineer, Senior",Blackbaud,"Kentucky, United States (Remote)",https://www.linkedin.com/jobs/view/3753079699/?eBP=JOB_SEARCH_ORGANIC&refId=hkitJDE%2BFnBPWHIC1GVhxw%3D%3D&trackingId=Pg0A2F2WpyvVOZrVtH4YxQ%3D%3D&trk=flagship3_search_srp_jobs,3753079699,"About the job
            
 
Data EngineerThe Data Engineer role spans a broad range of experience. Data Engineers are responsible for ingestion, transformation and integration of data to provide a platform that supports research, data analysis, data enrichment and data science as well as making data operationally available at scale to consuming products and services. This includes a particular focus on the reliability, efficiency and quality of data assets and processing/integration pipelines.Working with and utilizing standardized tools, technologies and processes Data Engineers design, build and manage the data architecture that enables scalable production of data insight and they work very closely with Data Science to build and operationalize the algorithms that identify insights, predict behavior and prescribe action. This intelligence fuels the development of Decision-Driven Experiences for our customers.Blackbaud Data Engineers understand how to leverage data for business value and social good, and our Data Engineering team develops, cultivates and curates our industry-leading platform of integrated data assets.What You'll Be Doing Develop production level code. Own process features from design to implementation.Assemble large, complex data sets for use in R&D, data science, machine learning, or projection to operational stores.Build the infrastructure that provides efficient and scalable ETL/ELT and integration of data from multiple proprietary and 3rd party sources.Code, test and deploy data pipelines in support of various initiatives.Ensure quality of code, processes, and data assets.Work on complex tasks and/or broad programs that are large and diverse in scope and/or critical in nature. Exercises good judgment in determining technical approach.
What We'll Want You To Have 3+ years of ETL/ELT development, implementation, and supportUnderstanding or knowledge of data integration concepts3+ years of demonstrated success developing with Core Programming Languages – SQL, PythonUnderstanding or knowledge of Azure cloud services – Azure Data Catalog, Azure Data Lake Store, Azure BLOB StoreUnderstanding or knowledge of AWS cloud services – S3, EC2, Data Pipelines3+ years utilizing and developing SQL for relational databases (Oracle, MS SQL Server, MySQL)Understanding or knowledge of Linux/Unix commands, scripting (bash) and file management
Stay up to date on everything Blackbaud, follow us on Linkedin, Twitter, Instagram, Facebook and YouTubeBlackbaud is a remote-first company which embraces a flexible remote work culture. Blackbaud supports hiring and career development for all roles from the location you are in today!Blackbaud is proud to be an equal opportunity employer and is committed to maintaining a diverse and inclusive work environment. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, physical or mental disability, age, or veteran status or any other basis protected by federal, state, or local law.To all recruitment agencies: We do not accept unsolicited agency resumes and are not responsible for any fees related to unsolicited resumes.A notice to candidates: Recruitment Fraudulent Alert: Your personal information and online safety as a candidate mean a lot to us! At Blackbaud and our portfolio of companies, recruiters only direct candidates to apply through our official careers page at https://careers.blackbaud.com/us/en or our official LinkedIn page. Recruiters will never request payments, ask for financial account information or sensitive information like social security numbers, or conduct interviews via Skype. Anyone suggesting otherwise is not a representative of Blackbaud. If you are unsure if a message is from Blackbaud, please email blackbaudrecruiting@blackbaud.com. The starting base pay is $117,200.00 to $157,500.00. Blackbaud may pay more or less based on employee qualifications, market value, Company finances, and other operational considerations.Benefits Include Medical, dental, and vision insuranceRemote-first workforce401(k) program with employer matchFlexible paid time offGenerous Parental LeaveVolunteer for vacationOpportunities to connect to build community and belongingPet insurance, legal and identity protectionTuition reimbursement program
R0011171"
Sr Data Engineer (strong Azure Data Bricks) --REMOTE_ USC OR GC Only,Ekodus INC.,United States (Remote),https://www.linkedin.com/jobs/view/3570041245/?eBP=JOB_SEARCH_ORGANIC&refId=hkitJDE%2BFnBPWHIC1GVhxw%3D%3D&trackingId=KMauEh46twbe7IQoNEmDLA%3D%3D&trk=flagship3_search_srp_jobs,3570041245,"About the job
            
 
Job Title - Sr Data Engineer (strong Azure Data Bricks) --REMOTEDuration - 12 MonthsMust be USC or GCDescription -Primary Responsibilities: Highly technical position responsible for delivering and maintaining data engineering components to meet business goals.Design enterprise data warehouse components through partnerships with Business Stakeholders, Business Analysts, Data Engineers and Developers.Validate proposed design for accuracy and completeness of business use cases.Develop data integration and transformation solutions to meet the input needs of the models.Develop and support batch jobs.Perform unit and regression testing.Perform code/peer reviews to ensure adherence to established design and development standards.Produce deployment scripts, checklists, playbook and operations runbook in accordance with SDLC & change management requirements.Monitor the scheduled to jobs and performance of the platforms to ensure smooth operation.Troubleshoot and fix issues that arise with data and/or processes.
Required Experience 10+ years of software development experience5+ years of development experience in Microsoft BI tools such as SQL Server, SSIS, SSAS and SSRS3+ years of experience in Azure using Data Factory, Databricks & ADLS (MUST HAVE)5+ years of experience in RDBMS design and development. Must demonstrate a clear mastery of the logical and physical database design (for both transactional and data warehouse) and data normalization conceptsExperience working in visual studio development environment and with using DevOps platforms for code management and deployment using CI/CD techniquesExperience with professional software engineering best practices for the full software development life cycle, including coding standards, code reviews, source control management, build processes, testing and operationsBachelor's degree in related field (prefer CS major)
Please share your resume to career@ekodusinc.com"
"Senior Data Engineer - $180k-$220k (Snowflake, Coding)",CyberCoders,"San Antonio, TX (Remote)",https://www.linkedin.com/jobs/view/3766365260/?eBP=JOB_SEARCH_ORGANIC&refId=hkitJDE%2BFnBPWHIC1GVhxw%3D%3D&trackingId=M7zCDFox1ViTkAIKXUfhvw%3D%3D&trk=flagship3_search_srp_jobs,3766365260,"About the job
            
 
Permanently Remote in USJob Title:Senior Data Engineer - $180k-$220k + Bonus (Snowflake, Coding)Salary: $180k-$220k Base + Bonus, No Stock, 401k, BenefitsRequirements: Expert w/ Snowflake & Coding AbilityBased in beautiful New York City, we are a cutting edge ad-tech org for television-based ads.We are founded and owned by T.V.s largest publishers.Our mission is to be bring simplicity & scale to audience based campaigns in television.We're working with over 100 advertisers and anticipating another year of significant growth!As a rapidly growing company (founded in 2017 & up 140% year over year) we've recently elevated our C-Suite Team in preparation for our next stage of growth and are building our Technology, Product, and Operations Executive Teams.We have been in business for 7 years and have around 40 employees.Due to growth, we are actively hiring a Senior Data Engineer with  Snowflake experience (ideally certified) Ability to write production level code (ideally JavaScript or Python) Experience building data pipelines from scratch and/or working with APIs
Experience with the following is a big plus!  Fivetran and/or DBT
You will be working with 12 other engineers on the data side + several other technical folks.This role will consist of tasks such as building out data pipelines, data architecture via snowflake, and data modeling.If you have this experience, please apply immediately. We are actively interviewing this week and next for this high profile position.Top Reasons to Work with Us  Compensation: $180k-$220k Base + Bonus, No Stock, 401k, Benefits Raid Growth: Founded in 2017 & up 140% year over year Culture: Fast paced, mission driven culture Technology: Cutting edge technology
What You Will Be Doing  Building data pipelines from scratch Data architecture via Snowflake Data modeling Technical review of everything this group builds. Mange development velocity, team capacity, and backlogs Partner closely with the product team Take on key assignments and delegate as needed Act as the main technical point of contact for engineering Translate technical requirements to the rest of the engineering team
What You Need for this PositionMust Have Experience  Snowflake experience Ability to write production level code (ideally JavaScript or Python) Experience building data pipelines from scratch and/or working with APIs
Some Experience With:-  Fivetran and/or DBT
What's In It for You $180k-$220k Base + Bonus, No Stock, 401k, Benefits 401k Vacation/PTO Medical Dental Vision Bonus 401k
Benefits  Vacation/PTO Medical Dental Vision Bonus
So, if you are a Senior Data Engineer - $180k-$220k + Bonus (Snowflake, Coding) with experience, please apply today!Colorado employees will receive paid sick leave. For additional information about available benefits, please contact Nitu Gulati-PaulyEmail Your Resume In Word ToLooking forward to receiving your resume through our website and going over the position with you. Clicking apply is the best way to apply, but you may also:Nitu.Gulati-Pauly@cybercoders.com Please do NOT change the email subject line in any way. You must keep the JobID: linkedin : MA5-1745335L570 -- in the email subject line for your application to be considered.***
Nitu Gulati-Pauly - VP of Recruiting - CyberCodersApplicants must be authorized to work in the U.S.CyberCoders is proud to be an Equal Opportunity EmployerAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any other characteristic protected by law.Your Right to Work – In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire."
"Data Engineer, Senior",Blackbaud,"Maryland, United States (Remote)",https://www.linkedin.com/jobs/view/3767167266/?eBP=JOB_SEARCH_ORGANIC&refId=hkitJDE%2BFnBPWHIC1GVhxw%3D%3D&trackingId=%2FqKAXMmtnaaRsdgsCAxf1w%3D%3D&trk=flagship3_search_srp_jobs,3767167266,"About the job
            
 
Data EngineerThe Data Engineer role spans a broad range of experience. Data Engineers are responsible for ingestion, transformation and integration of data to provide a platform that supports research, data analysis, data enrichment and data science as well as making data operationally available at scale to consuming products and services. This includes a particular focus on the reliability, efficiency and quality of data assets and processing/integration pipelines.Working with and utilizing standardized tools, technologies and processes Data Engineers design, build and manage the data architecture that enables scalable production of data insight and they work very closely with Data Science to build and operationalize the algorithms that identify insights, predict behavior and prescribe action. This intelligence fuels the development of Decision-Driven Experiences for our customers.Blackbaud Data Engineers understand how to leverage data for business value and social good, and our Data Engineering team develops, cultivates and curates our industry-leading platform of integrated data assets.What You'll Be Doing Develop production level code. Own process features from design to implementation.Assemble large, complex data sets for use in R&D, data science, machine learning, or projection to operational stores.Build the infrastructure that provides efficient and scalable ETL/ELT and integration of data from multiple proprietary and 3rd party sources.Code, test and deploy data pipelines in support of various initiatives.Ensure quality of code, processes, and data assets.Work on complex tasks and/or broad programs that are large and diverse in scope and/or critical in nature. Exercises good judgment in determining technical approach.
What We'll Want You To Have 3+ years of ETL/ELT development, implementation, and supportUnderstanding or knowledge of data integration concepts3+ years of demonstrated success developing with Core Programming Languages – SQL, PythonUnderstanding or knowledge of Azure cloud services – Azure Data Catalog, Azure Data Lake Store, Azure BLOB StoreUnderstanding or knowledge of AWS cloud services – S3, EC2, Data Pipelines3+ years utilizing and developing SQL for relational databases (Oracle, MS SQL Server, MySQL)Understanding or knowledge of Linux/Unix commands, scripting (bash) and file management
Stay up to date on everything Blackbaud, follow us on Linkedin, Twitter, Instagram, Facebook and YouTubeBlackbaud is a remote-first company which embraces a flexible remote work culture. Blackbaud supports hiring and career development for all roles from the location you are in today!Blackbaud is proud to be an equal opportunity employer and is committed to maintaining a diverse and inclusive work environment. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, physical or mental disability, age, or veteran status or any other basis protected by federal, state, or local law.To all recruitment agencies: We do not accept unsolicited agency resumes and are not responsible for any fees related to unsolicited resumes.A notice to candidates: Recruitment Fraudulent Alert: Your personal information and online safety as a candidate mean a lot to us! At Blackbaud and our portfolio of companies, recruiters only direct candidates to apply through our official careers page at https://careers.blackbaud.com/us/en or our official LinkedIn page. Recruiters will never request payments, ask for financial account information or sensitive information like social security numbers, or conduct interviews via Skype. Anyone suggesting otherwise is not a representative of Blackbaud. If you are unsure if a message is from Blackbaud, please email blackbaudrecruiting@blackbaud.com. The starting base pay is $117,200.00 to $157,500.00. Blackbaud may pay more or less based on employee qualifications, market value, Company finances, and other operational considerations.Benefits Include Medical, dental, and vision insuranceRemote-first workforce401(k) program with employer matchFlexible paid time offGenerous Parental LeaveVolunteer for vacationOpportunities to connect to build community and belongingPet insurance, legal and identity protectionTuition reimbursement program
R0011171"
Principle Data Engineer,Motion Recruitment,"New York, NY (Remote)",https://www.linkedin.com/jobs/view/3765514361/?eBP=JOB_SEARCH_ORGANIC&refId=hkitJDE%2BFnBPWHIC1GVhxw%3D%3D&trackingId=NHweTUwDxb%2BDxBVNMQT88w%3D%3D&trk=flagship3_search_srp_jobs,3765514361,"About the job
            
 
Principle Data Engineer A Fortune 500 company is looking for a Principal Data Engineer for a full-time remote position based out of Chicago, IL. This role is on the “Matching” team. This team helps to pull unstructured data from our company’s 1.7 million items in our catalog and match them to our competitors’. You are going to help build a brand-new team within their eCommerce division, using event-driven architecture and moving towards real-time streaming.We are a leading broad line distributor with operations primarily in North America, Japan, and the United Kingdom. We achieve our purpose, by serving more than 4.5 million customers with a wide range of products that keep their operations running and their people safe. We also deliver services and solutions, such as technical support and inventory management, to save customers time and money.We're looking for passionate people who can move our company forward. As one of the 100 Best Companies to Work For, we have a welcoming workplace where you can build a career for yourself while fulfilling our purpose to keep the world working. We embrace new ways of thinking and recognize everyone is an individual. Required Skills & Experience  7+ years of experience Experience with the following tools: Data bricks Kafka Python Spark AWS Docker/Kubernetes 

 Desired Skills & Experience Analytics experience Demonstrated record of learning 
What You Will Be Doing Daily Responsibilities 60% Hands On 20% Management Duties 20% Team Collaboration 
The Offer Bonus eligible 
You Will Receive The Following Benefits Medical Insurance Dental Benefits Vision Benefits Paid Time Off (PTO) 401(k) {including match- if applicable} 
Applicants must be currently authorized to work in the US on a full-time basis now and in the future.Posted By: Mia Quinn"
"Data Engineer, Sr.",Oshkosh Corporation,United States (Remote),https://www.linkedin.com/jobs/view/3750634582/?eBP=JOB_SEARCH_ORGANIC&refId=hkitJDE%2BFnBPWHIC1GVhxw%3D%3D&trackingId=A6v9LFxy5v8qyKQWBjzKYQ%3D%3D&trk=flagship3_search_srp_jobs,3750634582,"About the job
            
 
At Oshkosh, we build, serve and protect people and communities around the world by designing and manufacturing some of the toughest specialty trucks and access equipment. We employ over 15,000 team members all united by a common purpose. Our engineering and product innovation help keep soldiers and firefighters safe, is critical in building and keeping communities clean and helps people do their jobs every day.THE ROLE:Oshkosh Advanced Analytics & Artificial Intelligence Practice is Seeking a Data Quality Engineer to guide a team in developing, deploying, and enforcing policies and procedures that promote efficient, accurate data, and effective information management to ensure data is used and maintained properly across an organization.YOUR IMPACT: Design, develop, implement, and sustain data governance policies and procedures as part of a broader enterprise data management program including, but not limited to: Data quality framework, metrics, and adherence to data standardsData cataloging and data flow/lineage visualizationIdentify and collaborate with data stewardsCreate and maintain metadataCollaborate with infrastructure team on data architecture and data lifecycle management
Communicate across varying data roles to solve cross-domain data governance issues. Ability to plan, execute and manage data quality initiativesUse appropriate data governance, data management, or metadata management tools to organize and streamline data governance processes, procedures, and workflows. Design and execute materials related to adaption and education of DG domain and toolsAnalyze current data architectures and operating environments and make recommendations for future functionality and enhancements. Support executive level briefings on data solution architecture, design and technology alternatives. 
YOUR EXPERIENCE AND EXPERTISE:  Bachelor’s degree in computer science, data science or a related field with five (5) or more years of working as a data engineer, ETL developer and/or data warehouse DBA. Strong oral and written communications skills, including an ability to successfully communicate technical and non-technical concepts, are requiredHands-on experience with industry leading data governance, data cataloging, data management, metadata management software/tools/platforms ( Informatica, Collibra)Strong data governance background with deep knowledge of metadata management, data quality management, data integration or data management. Proficiency with data management or metadata management technology productsWorking knowledge of databases and database structures to enable articulating data concepts between business-focused, functional, and technical data users. 4+ years of experience in data analysis, metadata management, data quality or a related field. Experience with Azure cloud architecture, Databricks, Synapse, Power BI or similar technologiesExperience with developing highly responsive data structures, metadata capture strategies, ontologies, and data dictionariesAbility to work effectively in teams of technical and non-technical individualsSkill and comfort working in a rapidly changing environment with dynamic objectives and iteration with users. Demonstrated ability to continuously learn, work independently, and make decisions with minimal supervision. Proven track-record of strong customer communications including feedback gathering, execution updates, and troubleshooting. 
STANDOUT QUALIFICATIONS:  Relevant technology and/or data management certifications
Oshkosh is committed to working with and offering reasonable accommodation to job applicants with disabilities. If you need assistance or an accommodation due to disability for any part of the employment process, please contact us at: 920-502-3009 or corporatetalentacquisition@oshkoshcorp.com.Oshkosh Corporation is an Equal Opportunity and Affirmative Action Employer. This company will provide equal opportunity to all individuals without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status. Information collected regarding categories as provided by law will in no way affect the decision regarding an employment application.Oshkosh Corporation will not discharge or in any manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with Oshkosh Corporation's legal duty to furnish information.Certain positions with Oshkosh Corporation require access to controlled goods and technologies subject to the International Traffic in Arms Regulations or the Export Administration Regulations. Applicants for these positions may need to be ""U.S. Persons,"" as defined in these regulations. Generally, a ""U.S. Person"" is a U.S. citizen, lawful permanent resident, or an individual who has been admitted as a refugee or granted asylum."
BI Senior Data Engineer,Mindoula,United States (Remote),https://www.linkedin.com/jobs/view/3775905652/?eBP=JOB_SEARCH_ORGANIC&refId=hkitJDE%2BFnBPWHIC1GVhxw%3D%3D&trackingId=piL7%2BipmFepcHecy6l3Eug%3D%3D&trk=flagship3_search_srp_jobs,3775905652,"About the job
            
 
We're seeking a talented and dedicated Senior Data Engineer to join our team. The ideal candidate will be instrumental in shaping our data architecture, integrating diverse data sources, and ensuring that our data pipelines are scalable, robust, and timely. With a keen understanding of modern data stack technologies, you'll be at the forefront of our efforts to power our analytics, machine learning, and business intelligence initiatives.Location..This is a 100% remote position in the United States. Applicants must be authorized to work for any employer in the US. We are unable to sponsor or take over sponsorship of an employment Visa at this time.What you'll do... Design, construct, install, and maintain large-scale data processing pipelines and other infrastructure.Set the data strategy by selecting, architecting, and integrating cutting-edge database solutions.Build, optimize, and maintain ETL/ELT processes using modern tools and platforms.Ensure data integration from various sources, keeping in mind data accuracy, reliability, and timeliness.Optimize database operations for maximum performance, including monitoring, troubleshooting, and improving existing systems.Collaborate with data scientists, ML engineers, and other stakeholders to support their data infrastructure needs.Interface with other tech teams to extract, transform, and load data from a wide variety of data sources.Implement security measures to ensure data protection and compliance with relevant regulations and best practices.Stay updated with the latest technology trends, evaluating and implementing new systems and tools as required.
What you'll need... Bachelor's/Master's degree in Computer Science, Engineering, or a related technical field.7+ years of experience in a data engineering role.7+ years of experience with cloud warehousing platforms such as Redshift or BigQuery.4+ years experience with data orchestration platforms such as Airflow, Prefect, or Dagster.3+ years experience architecting and building big data solutions in the cloud, to include performance optimization and partitioning with both structured and unstructured data. 2+ years experience with a transformation workflow framework like dbt or SQLMesh. Strong knowledge of SQL and scripting languages (Python, Scala, Java, etc.).Strong knowledge of data warehousing strategies to include Inmon, Kimball and Data Vault. Experience with data quality, ideally using a framework such as Great Expectations.Familiarity with real-time data processing (e.g., Kafka, Spark Streaming).Experience in healthcare or behavioral health sectors.Proven track record of architecting and deploying high-scale data infrastructure.Strong problem-solving skills and the ability to work in fast-paced, collaborative environments.
About Mindoula...Mindoula is a next generation population health management company that identifies, engages, and serves populations with complex behavioral health, medical, and social challenges across the continuum of care. By using technology to “scale the human connection,” Mindoula helps health plans, health systems, hospitals, and provider groups extend their reach and achieve their value-based service delivery goals.  At Mindoula, we address the full range of behavioral health challenges. We deploy tech-enabled teams of case managers, care managers, community health workers, peer support specialists, therapists, and psychiatrists to provide 24/7 support to even the most complex and underserved behavioral health populations. In addition, Mindoula provides direct care services including both the delivery of outpatient behavioral health services - in person and telehealth psychiatry, med management, and therapy - and the staffing and managing of inpatient psychiatric units in partnership with client hospitals.  Mindoula is a market leader in managing the most complex Med/Psych and Speciality Populations, delivering a virtual Collaborative Care model, delivering telepsychiatry and teletherapy, and value based care."
Data Integration Engineer / Fivetran Specialist,TrueSense Marketing,"Warrendale, PA (Remote)",https://www.linkedin.com/jobs/view/3769534558/?eBP=JOB_SEARCH_ORGANIC&refId=hkitJDE%2BFnBPWHIC1GVhxw%3D%3D&trackingId=NLgVccWBNliMCDmVhm%2B6ZQ%3D%3D&trk=flagship3_search_srp_jobs,3769534558,"About the job
            
 
As a Data Integration Engineer/ Fivetran Specialist, you will be at the forefront of our organization's data integration efforts, leveraging Fivetran to create seamless pipelines, support data transformations, and ensure that our business intelligence tools have timely, accurate data. This role is pivotal in helping the company drive data-driven decisions by ensuring efficient, scalable, and reliable data integrations.This role requires a strong technical background, excellent problem-solving skills, and the ability to work effectively in a cross-functional team. The successful candidate will also be responsible for being the primary technical contact for all internal partners to support them during all phases of partner integration with our warehouse and reporting systems.Essential Functions: Set up and maintain connectors in Fivetran to facilitate the transfer of data from various sources to our data warehouse.Monitor and ensure smooth operation of data pipelines, identifying and addressing any issues or slowdowns in data transfer.Work closely with data engineers, data analysts, and other stakeholders to understand data needs and implement appropriate Fivetran solutions.Support data transformation efforts in the data warehouse, ensuring that data is appropriately structured for reporting and analysis.Continuously review and optimize data pipelines to improve performance and reduce costs.Maintain detailed documentation on data pipelines, including source-to-destination mappings, transformation logic, and any known issues or considerations.Provide training and guidance to other team members on Fivetran and related tools.
Education and Experience: Bachelor’s degree in Computer Science, Information Systems, or a related field. Strong experience with Fivetran or similar data integration platforms.Prior experience in a similar data integration role, preferably in a fast-paced environment. Knowledge of SQL and familiarity with popular data warehousing solutions (e.g., Snowflake, BigQuery, Redshift). Familiarity with business intelligence tools like Sigma, Looker, Tableau, or Power BI.Familiarity with CRM and Marketing Platforms (Salesforce, Blackbaud, Hubspot) and their API’sExperience with cloud platforms like AWS, GCP, or Azure.Experience within the non-profit sector a plus.Excellent problem-solving skills and attention to detail. Strong communication skills, both written and verbal
We offer a comprehensive package including Medical, Dental, Vision, Life, 401(k) + match, STD/LTD, generous PTO, Parental Leave, EAP, and 11 paid Holidays. Employees enjoy participation in our employee engagement activities. EOE"
"Senior Data Engineer - $180k-$220k (Snowflake, Coding)",CyberCoders,"Dallas, TX (Remote)",https://www.linkedin.com/jobs/view/3766363423/?eBP=JOB_SEARCH_ORGANIC&refId=hkitJDE%2BFnBPWHIC1GVhxw%3D%3D&trackingId=A7TFuRhEl0iuzZIbZs3k4w%3D%3D&trk=flagship3_search_srp_jobs,3766363423,"About the job
            
 
Permanently Remote in USJob Title:Senior Data Engineer - $180k-$220k + Bonus (Snowflake, Coding)Salary: $180k-$220k Base + Bonus, No Stock, 401k, BenefitsRequirements: Expert w/ Snowflake & Coding AbilityBased in beautiful New York City, we are a cutting edge ad-tech org for television-based ads.We are founded and owned by T.V.s largest publishers.Our mission is to be bring simplicity & scale to audience based campaigns in television.We're working with over 100 advertisers and anticipating another year of significant growth!As a rapidly growing company (founded in 2017 & up 140% year over year) we've recently elevated our C-Suite Team in preparation for our next stage of growth and are building our Technology, Product, and Operations Executive Teams.We have been in business for 7 years and have around 40 employees.Due to growth, we are actively hiring a Senior Data Engineer with  Snowflake experience (ideally certified) Ability to write production level code (ideally JavaScript or Python) Experience building data pipelines from scratch and/or working with APIs
Experience with the following is a big plus!  Fivetran and/or DBT
You will be working with 12 other engineers on the data side + several other technical folks.This role will consist of tasks such as building out data pipelines, data architecture via snowflake, and data modeling.If you have this experience, please apply immediately. We are actively interviewing this week and next for this high profile position.Top Reasons to Work with Us  Compensation: $180k-$220k Base + Bonus, No Stock, 401k, Benefits Raid Growth: Founded in 2017 & up 140% year over year Culture: Fast paced, mission driven culture Technology: Cutting edge technology
What You Will Be Doing  Building data pipelines from scratch Data architecture via Snowflake Data modeling Technical review of everything this group builds. Mange development velocity, team capacity, and backlogs Partner closely with the product team Take on key assignments and delegate as needed Act as the main technical point of contact for engineering Translate technical requirements to the rest of the engineering team
What You Need for this PositionMust Have Experience  Snowflake experience Ability to write production level code (ideally JavaScript or Python) Experience building data pipelines from scratch and/or working with APIs
Some Experience With:-  Fivetran and/or DBT
What's In It for You $180k-$220k Base + Bonus, No Stock, 401k, Benefits 401k Vacation/PTO Medical Dental Vision Bonus 401k
Benefits  Vacation/PTO Medical Dental Vision Bonus
So, if you are a Senior Data Engineer - $180k-$220k + Bonus (Snowflake, Coding) with experience, please apply today!Colorado employees will receive paid sick leave. For additional information about available benefits, please contact Nitu Gulati-PaulyEmail Your Resume In Word ToLooking forward to receiving your resume through our website and going over the position with you. Clicking apply is the best way to apply, but you may also:Nitu.Gulati-Pauly@cybercoders.com Please do NOT change the email subject line in any way. You must keep the JobID: linkedin : MA5-1745335L571 -- in the email subject line for your application to be considered.***
Nitu Gulati-Pauly - VP of Recruiting - CyberCodersApplicants must be authorized to work in the U.S.CyberCoders is proud to be an Equal Opportunity EmployerAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any other characteristic protected by law.Your Right to Work – In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire."
Staff Data Engineer,Overjet,"Boston, MA (Remote)",https://www.linkedin.com/jobs/view/3743165522/?eBP=JOB_SEARCH_ORGANIC&refId=SNJ4vu2KltKkIEutY%2B%2Fd6Q%3D%3D&trackingId=mMkvkyKspH11WU00jfAvkA%3D%3D&trk=flagship3_search_srp_jobs,3743165522,"About the job
            
 
Overjet is on a mission to improve oral health for all.Our cutting-edge artificial intelligence technology encodes dentist-level training and analysis into scalable software tools. Today, our flagship products are used by some of the country’s largest insurance companies, dental support organizations, and dental practices to enable the best patient care.We’re building an ambitious team of data scientists, software engineers, clinicians, and business thinkers and doers.Overjet is backed by an amazing consortium of Venture Capital including General Catalyst, Insight Partners, Crosslink Capital and the E14 Fund. The company has raised nearly $80 million in capital to accelerate talent acquisition and product development driven by strong demand for Overjet’s dental AI.The RoleOverjet is hiring a talented Staff Data Engineer to design, develop, and implement data integration solutions that help ensure we have world class data engineering capabilities.The ideal candidate has strong back-end skills, significant experience with data flows & data modeling, and a passion for making sure that data is flowing across the board so that we can best serve our customers.Responsibilities Design, develop, and implement data pipelines to process data from patient management systems and digital imaging systemsCollaborate with cross-functional teams, including Machine Learning engineers and product managersConduct data mapping and data modeling activitiesBuild framework for automated rollout of our products to new customers, including data validationGuide the development of data resources, support new product launches and improve product runtime performanceRegularly scheduled and sometimes ad-hoc on-call rotations is part of the core job every engineer on our team
Qualifications 10+ years of software engineering experience including 3+ years of data-related experienceStrong programming skills in Python and C#, with the ability to write efficient and maintainable codeProficiency in SQL, familiarity with data modeling and database design principlesExperience working with cloud vendors (GCP/AWS/Azure), especially using infrastructure-as-code software like TerraformStrong knowledge of Docker and Kubernetes for containerization and orchestrationExperience with Practice Management Systems or electronic health records (EHR) is a plus but not requiredKnowledge of data security and compliance standard (e.g. HIPAA) and experience with sensitive patient data is a plus but not required
Why Overjet?  Competitive Compensation and EquityFully Remote Working Policy for US Based Employees401k plans with a matching programMedical, Dental and Vision coverage: 99% employee premium covered, 75% dependent premium coveredLife and AD+D Insurance 8 weeks Paid Parental Leave Optional HSA with Employer contributionFlexible Time Off and company paid holidaysAnnual Learning and Development StipendWork from Home Stipend
Overjet's Values Excellence: We set ambitious goals and strive for excellence.Velocity: We focus, act with urgency, and deliver results.Ownership: We take ownership, dive deep and solve problems.Win-win: We play to win, setting ourselves and our customers up for success.Growth:We stay curious, seek feedback, and continuously learn and grow.
Company Recognition 2023 — #29 on Linkedin's Top Startups 2023: The 50 U.S. companies on the rise2023 — Fast Company World Changing Ideas for Overjet’s FDA-cleared dental AI2023 — Fast Company World Most Innovative Companies2023 — Fast Company Top 10 Healthcare Companies in the World2022 AI-50 Forbes2022 22 Start-ups to Watch in 2022 - Built in Boston2021 AI 100 Ranking, showcasing the 100 most promising private artificial intelligence companies in the world - CB Insights2021 Digital Health 150, highlighting the 150 most promising private digital health companies in the world - CB Insights2020 & 2019 Digital Health 150 - CB Insights
EEOC  Overjet is an equal opportunity employer. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. We believe diversity enriches our team so we hire people with a wide range of identities, backgrounds, and experiences. Even if you don't meet 100% of the qualifications for this job, we strongly encourage you to apply! If you are a Colorado resident: Please contact us by emailing recruiting@overjet.ai to receive compensation and benefits information for this role. Please include the job title in the subject line of the email."
Remote Data Engineer (Oracle Graph),RemoteWorker US,"Seattle, WA (Remote)",https://www.linkedin.com/jobs/view/3777773154/?eBP=JOB_SEARCH_ORGANIC&refId=SNJ4vu2KltKkIEutY%2B%2Fd6Q%3D%3D&trackingId=itHyGq1yIfolNUpAf0UmVg%3D%3D&trk=flagship3_search_srp_jobs,3777773154,"About the job
            
 
Title: Data Engineer Terms: W2 Contract/ 2 yrs + Location: Remote in USA/Must be able to work PST Hours Qualifications 5+ yrs of data engineering experience 1+ yr of experience working with Oracle Graph Database"
"Data Engineer, Senior",Blackbaud,"Indiana, United States (Remote)",https://www.linkedin.com/jobs/view/3753079700/?eBP=JOB_SEARCH_ORGANIC&refId=SNJ4vu2KltKkIEutY%2B%2Fd6Q%3D%3D&trackingId=4YemjgEO%2BZhLOakTSXaiAg%3D%3D&trk=flagship3_search_srp_jobs,3753079700,"About the job
            
 
Data EngineerThe Data Engineer role spans a broad range of experience. Data Engineers are responsible for ingestion, transformation and integration of data to provide a platform that supports research, data analysis, data enrichment and data science as well as making data operationally available at scale to consuming products and services. This includes a particular focus on the reliability, efficiency and quality of data assets and processing/integration pipelines.Working with and utilizing standardized tools, technologies and processes Data Engineers design, build and manage the data architecture that enables scalable production of data insight and they work very closely with Data Science to build and operationalize the algorithms that identify insights, predict behavior and prescribe action. This intelligence fuels the development of Decision-Driven Experiences for our customers.Blackbaud Data Engineers understand how to leverage data for business value and social good, and our Data Engineering team develops, cultivates and curates our industry-leading platform of integrated data assets.What You'll Be Doing Develop production level code. Own process features from design to implementation.Assemble large, complex data sets for use in R&D, data science, machine learning, or projection to operational stores.Build the infrastructure that provides efficient and scalable ETL/ELT and integration of data from multiple proprietary and 3rd party sources.Code, test and deploy data pipelines in support of various initiatives.Ensure quality of code, processes, and data assets.Work on complex tasks and/or broad programs that are large and diverse in scope and/or critical in nature. Exercises good judgment in determining technical approach.
What We'll Want You To Have 3+ years of ETL/ELT development, implementation, and supportUnderstanding or knowledge of data integration concepts3+ years of demonstrated success developing with Core Programming Languages – SQL, PythonUnderstanding or knowledge of Azure cloud services – Azure Data Catalog, Azure Data Lake Store, Azure BLOB StoreUnderstanding or knowledge of AWS cloud services – S3, EC2, Data Pipelines3+ years utilizing and developing SQL for relational databases (Oracle, MS SQL Server, MySQL)Understanding or knowledge of Linux/Unix commands, scripting (bash) and file management
Stay up to date on everything Blackbaud, follow us on Linkedin, Twitter, Instagram, Facebook and YouTubeBlackbaud is a remote-first company which embraces a flexible remote work culture. Blackbaud supports hiring and career development for all roles from the location you are in today!Blackbaud is proud to be an equal opportunity employer and is committed to maintaining a diverse and inclusive work environment. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, physical or mental disability, age, or veteran status or any other basis protected by federal, state, or local law.To all recruitment agencies: We do not accept unsolicited agency resumes and are not responsible for any fees related to unsolicited resumes.A notice to candidates: Recruitment Fraudulent Alert: Your personal information and online safety as a candidate mean a lot to us! At Blackbaud and our portfolio of companies, recruiters only direct candidates to apply through our official careers page at https://careers.blackbaud.com/us/en or our official LinkedIn page. Recruiters will never request payments, ask for financial account information or sensitive information like social security numbers, or conduct interviews via Skype. Anyone suggesting otherwise is not a representative of Blackbaud. If you are unsure if a message is from Blackbaud, please email blackbaudrecruiting@blackbaud.com. The starting base pay is $117,200.00 to $157,500.00. Blackbaud may pay more or less based on employee qualifications, market value, Company finances, and other operational considerations.Benefits Include Medical, dental, and vision insuranceRemote-first workforce401(k) program with employer matchFlexible paid time offGenerous Parental LeaveVolunteer for vacationOpportunities to connect to build community and belongingPet insurance, legal and identity protectionTuition reimbursement program
R0011171"
Remote Data Engineer (Oracle Graph),RemoteWorker US,"Santa Fe, TX (Remote)",https://www.linkedin.com/jobs/view/3777767714/?eBP=JOB_SEARCH_ORGANIC&refId=SNJ4vu2KltKkIEutY%2B%2Fd6Q%3D%3D&trackingId=YFGyceSrxHiHjGHZcj8DXQ%3D%3D&trk=flagship3_search_srp_jobs,3777767714,"About the job
            
 
Title: Data Engineer Terms: W2 Contract/ 2 yrs + Location: Remote in USA/Must be able to work PST Hours Qualifications 5+ yrs of data engineering experience 1+ yr of experience working with Oracle Graph Database"
ETL Data Quality Engineer,Wise Skulls,United States (Remote),https://www.linkedin.com/jobs/view/3748447052/?eBP=JOB_SEARCH_ORGANIC&refId=SNJ4vu2KltKkIEutY%2B%2Fd6Q%3D%3D&trackingId=UxGHaWI1CjKrF691rL1HkQ%3D%3D&trk=flagship3_search_srp_jobs,3748447052,"About the job
            
 
Title: ETL Data Quality EngineerLocation: Austin, TX; Fortmill, SC (Remote for Now)Duration: 12+ Months (Possibility of Extension)Implementation Partner: InfosysEnd Client: To be disclosedJd Must have 6+ Years of Expereince.Must haveETL, SQL, Postgres, Cloud (Snowflake, AWS), Automation (Python or Java), CI/CD.Good to have API."
Remote work - Need Senior AWS Data Engineer/Architect with Apache Hudi experiece,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3714171457/?eBP=JOB_SEARCH_ORGANIC&refId=SNJ4vu2KltKkIEutY%2B%2Fd6Q%3D%3D&trackingId=IO6oAg%2F1b0RvvLUJ9ZfpjQ%3D%3D&trk=flagship3_search_srp_jobs,3714171457,"About the job
            
 
Job: Senior AWS Data Engineer/ArchitectLocation: 100% remoteDuration: Up to 6 months ContractTop Skills 10+ years of experience of hands-on development experience; create and design and architect solutions at the same time. Providing end-to-end solutions.Ability to teach or mentor best practices.Building data pipelines using Python and PySpark. Strong programming skills in Python and PySpark.Experience with Apache Spark requiredSalesforce understanding and knowledge is preferredProficiency in AWS services, specifically S3, Glue, Athena, and Lambda.Experience with Hadoop ecosystem, including HDFS, MapReduce, and Hive.Hands-on experience with Apache Hudi for real-time data management.Familiarity with SQL and NoSQL databases.Knowledge of data governance and data security best practices.Experience with workflow scheduling tools like AWS Step Functions or Control M.
Senior AWS Data Engineer/ArchitectWe are seeking a highly skilled and experienced Senior AWS Data Engineer/Architect to join our team. The successful candidate will be responsible for designing and implementing complex data solutions on the AWS platform. You will work closely with our development team to ensure that data solutions meet our business requirements and deliver value to our customers.Responsibilities Design, build, and manage ETL data pipelines using AWS services such as S3, Glue, Athena, and Lambda.Implement and manage real-time data streaming and batch processing solutions using Apache Hudi, PySpark, and Hadoop.Leverage AWS Athena for complex SQL queries over large datasets.Use Python and PySpark to perform data transformation and data cleansing.Ensure data quality and integrity by implementing proper data governance strategies.Collaborate with cross-functional teams to meet business objectives.Monitor performance and advise any necessary infrastructure changes.Develop technical documentation including data dictionaries, metadata, and pipeline architecture.Troubleshoot data issues and provide ongoing operational support.
Requirements Bachelor's or Master's degree in Computer Science or a related field10+ years of experience in AWS data engineering and architectureDeep understanding of AWS data services such as S3, DynamoDB, RDS, Kinesis, EMR, Glue and HudiExperience with big data technologies such as Hadoop, Spark, and HiveStrong programming skills in PythonStrong communication and problem-solving skillsExperience with infrastructure-as-code tools such as TerraformFamiliarity with DevOps methodologies and tools such as Git, Jenkins, and DockerStrong understanding of networking, security, and database conceptsAWS Certification in Data Analytics, Data Engineering, or Solutions Architect is a plus."
Sr Data Engineer,MHK TECH INC,United States (Remote),https://www.linkedin.com/jobs/view/3769049693/?eBP=JOB_SEARCH_ORGANIC&refId=SNJ4vu2KltKkIEutY%2B%2Fd6Q%3D%3D&trackingId=U0%2FkJqtRIRAQNv79qNkhDw%3D%3D&trk=flagship3_search_srp_jobs,3769049693,"About the job
            
 
Sr. Data EngineerLocation: REMOTEPosition Type: Contract( Only W2)Responsibilities: Designing and implementing large-scale, distributed data processing systems using technologies such as Apache Hadoop, Apache Spark, or Apache Flink.Developing and optimizing data pipelines and workflows for ingesting, storing, processing, and analyzing large volumes of structured and unstructured data.Collaborating with data scientists, data analysts, and other stakeholders to understand data requirements and translate them into technical solutions.Building and maintaining data infrastructure, including data lakes, data warehouses, and real-time streaming platforms.Designing and implementing data models and schemas for efficient data storage and retrieval.Ensuring the scalability, availability, and fault-tolerance of big data systems through proper configuration, monitoring, and performance tuning.Identifying and evaluating new technologies, tools, and frameworks to improve the efficiency and effectiveness of big data processing.Implementing data security and privacy measures to protect sensitive information throughout the data lifecycle.Collaborating with cross-functional teams to integrate data from various sources, including structured databases, unstructured files, APIs, and streaming data.Developing and maintaining documentation, including data flow diagrams, system architecture, and technical specifications.Requirements:Bachelor's or higher degree in Computer Science, Engineering, or a related field.Proven experience as a big data engineer or a similar role, with a deep understanding of big data technologies, frameworks, and best practices.Strong programming skills in languages such as Java, Scala, or Python for developing big data solutions.Experience with big data processing frameworks like Apache Hadoop, Apache Spark, Apache Flink, or similar.Proficiency in SQL and NoSQL databases, as well as data modeling and database design principles.Familiarity with cloud platforms and services, such as Amazon Web Services (AWS), Microsoft Azure, or Google Cloud Platform (GCP).Knowledge of distributed computing principles and technologies, such as HDFS, YARN, and containerization (e.g., Docker, Kubernetes).Understanding of real-time streaming technologies and frameworks, such as Apache Kafka or Apache Pulsar.Strong problem-solving skills and ability to optimize and tune big data processing systems for performance and scalability.Excellent communication and teamwork skills to collaborate with cross-functional teams and stakeholders..

Desired Skills and Experience
                DATA ENGINEER"
"Data Engineer, Senior",Blackbaud,"Virginia, United States (Remote)",https://www.linkedin.com/jobs/view/3753083154/?eBP=JOB_SEARCH_ORGANIC&refId=SNJ4vu2KltKkIEutY%2B%2Fd6Q%3D%3D&trackingId=nv%2B6K9HXukqxlAbh9ftHRg%3D%3D&trk=flagship3_search_srp_jobs,3753083154,"About the job
            
 
Data EngineerThe Data Engineer role spans a broad range of experience. Data Engineers are responsible for ingestion, transformation and integration of data to provide a platform that supports research, data analysis, data enrichment and data science as well as making data operationally available at scale to consuming products and services. This includes a particular focus on the reliability, efficiency and quality of data assets and processing/integration pipelines.Working with and utilizing standardized tools, technologies and processes Data Engineers design, build and manage the data architecture that enables scalable production of data insight and they work very closely with Data Science to build and operationalize the algorithms that identify insights, predict behavior and prescribe action. This intelligence fuels the development of Decision-Driven Experiences for our customers.Blackbaud Data Engineers understand how to leverage data for business value and social good, and our Data Engineering team develops, cultivates and curates our industry-leading platform of integrated data assets.What You'll Be Doing Develop production level code. Own process features from design to implementation.Assemble large, complex data sets for use in R&D, data science, machine learning, or projection to operational stores.Build the infrastructure that provides efficient and scalable ETL/ELT and integration of data from multiple proprietary and 3rd party sources.Code, test and deploy data pipelines in support of various initiatives.Ensure quality of code, processes, and data assets.Work on complex tasks and/or broad programs that are large and diverse in scope and/or critical in nature. Exercises good judgment in determining technical approach.
What We'll Want You To Have 3+ years of ETL/ELT development, implementation, and supportUnderstanding or knowledge of data integration concepts3+ years of demonstrated success developing with Core Programming Languages – SQL, PythonUnderstanding or knowledge of Azure cloud services – Azure Data Catalog, Azure Data Lake Store, Azure BLOB StoreUnderstanding or knowledge of AWS cloud services – S3, EC2, Data Pipelines3+ years utilizing and developing SQL for relational databases (Oracle, MS SQL Server, MySQL)Understanding or knowledge of Linux/Unix commands, scripting (bash) and file management
Stay up to date on everything Blackbaud, follow us on Linkedin, Twitter, Instagram, Facebook and YouTubeBlackbaud is a remote-first company which embraces a flexible remote work culture. Blackbaud supports hiring and career development for all roles from the location you are in today!Blackbaud is proud to be an equal opportunity employer and is committed to maintaining a diverse and inclusive work environment. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, physical or mental disability, age, or veteran status or any other basis protected by federal, state, or local law.To all recruitment agencies: We do not accept unsolicited agency resumes and are not responsible for any fees related to unsolicited resumes.A notice to candidates: Recruitment Fraudulent Alert: Your personal information and online safety as a candidate mean a lot to us! At Blackbaud and our portfolio of companies, recruiters only direct candidates to apply through our official careers page at https://careers.blackbaud.com/us/en or our official LinkedIn page. Recruiters will never request payments, ask for financial account information or sensitive information like social security numbers, or conduct interviews via Skype. Anyone suggesting otherwise is not a representative of Blackbaud. If you are unsure if a message is from Blackbaud, please email blackbaudrecruiting@blackbaud.com. The starting base pay is $117,200.00 to $157,500.00. Blackbaud may pay more or less based on employee qualifications, market value, Company finances, and other operational considerations.Benefits Include Medical, dental, and vision insuranceRemote-first workforce401(k) program with employer matchFlexible paid time offGenerous Parental LeaveVolunteer for vacationOpportunities to connect to build community and belongingPet insurance, legal and identity protectionTuition reimbursement program
R0011171"
Senior Big Data Engineer (Remote),Grid Dynamics,United States (Remote),https://www.linkedin.com/jobs/view/3758121223/?eBP=JOB_SEARCH_ORGANIC&refId=SNJ4vu2KltKkIEutY%2B%2Fd6Q%3D%3D&trackingId=nZLVmCT16gY%2Bazlu9R8zxA%3D%3D&trk=flagship3_search_srp_jobs,3758121223,"About the job
            
 
DescriptionPosition at Grid DynamicsOur customer is an American multinational technology company headquartered in Cupertino, California. Our customer is one of the world's largest technology companies based in Silicon Valley with operations all over the world. On this project, we are working with bleeding-edge big data technologies to develop a high-performance data analytics platform, which handles petabytes of data. We are looking for an enthusiastic and technology-proficient Senior Big Data Developer, who is eager to participate in design and implementation of a top-notch big data solution that will be deployed at a massive scale.Responsibilities Running big data analytics, and building large scale data infrastructureDetecting meaningful data patternsAssuring the integrity of our dataMeasuring fraud activity and its impact on campaign and user performanceAnalyzing the results of mitigations against fraud
Requirements Strong knowledge of Java and ScalaIn-depth knowledge of Hadoop and Spark, experience with data mining and stream processing technologies (Kafka, Spark Streaming, Akka Streams)Understanding of the best practices in data quality and quality engineeringAbility to quickly learn new tools and technologiesGood Communication Skills
Will Be a Plus Knowledge of Unix-based operating systems (bash/ssh/ps/grep etc.)Experience with JVM build systems (SBT, Maven, Gradle)AWS experience
What We OfferWhat we offer: Opportunity to work on bleeding-edge projectsWork with a highly motivated and dedicated teamCompetitive salaryFlexible scheduleBenefits package - medical insurance, sports Corporate social eventsProfessional development opportunities
About UsGrid Dynamics (Nasdaq:GDYN) is a digital-native technology services provider that accelerates growth and bolsters competitive advantage for Fortune 1000 companies. Grid Dynamics provides digital transformation consulting and implementation services in omnichannel customer experience, big data analytics, search, artificial intelligence, cloud migration, and application modernization. Grid Dynamics achieves high speed-to-market, quality, and efficiency by using technology accelerators, an agile delivery culture, and its pool of global engineering talent. Founded in 2006, Grid Dynamics is headquartered in Silicon Valley with offices across the US, UK, Netherlands, Mexico, and Central and Eastern Europe.To learn more about Grid Dynamics, please visit www.griddynamics.com. Follow us on Facebook, Twitter, and LinkedIn."
Senior Azure Data Engineer,Interwell Health,United States (Remote),https://www.linkedin.com/jobs/view/3696006603/?eBP=JOB_SEARCH_ORGANIC&refId=SNJ4vu2KltKkIEutY%2B%2Fd6Q%3D%3D&trackingId=m3nKqHJ155L5IveyhF0Tdw%3D%3D&trk=flagship3_search_srp_jobs,3696006603,"About the job
            
 
Interwell Health is a kidney care management company that partners with physicians on its mission to reimagine healthcare—with the expertise, scale, compassion, and vision to set the standard for the industry and help patients live their best lives. We are on a mission to help people and we know the work we do changes their lives. If there is a better way, we will create it. So, if our mission speaks to you, join us!Interwell Health’s Senior Azure Data Engineer role is responsible data acquisition, file loading and architecture development. The ideal candidate will have a track record of building data sources leveraging MDM solutions such as Profisee and has worked with Event Driven Architecture. The Engineer should be able to design, test, plan, and implement data solutions in an Azure Cloud Environment.The work you will do: Designs and develops processes based on the requirements of the company. This includes collecting data, analyzing data, designing algorithms, drawing flowcharts, and implementing codeCollaborates effectively with the development groups to deliver projects to the satisfaction of the client in a timely fashionMaintains current knowledge of the latest and newest technologies in the areas of specific relevancy to the Data Architecture group for utilization as appropriate within the department. Identifies and researches enhancement options and process improvements making suggestions to senior managers as neededAssist with schema design, code review, SQL query tuningWrite and deploy SQL codeGuide and mentor junior teammates acting as a resource with respect to the definition of development processes, methodologies and frameworks and other technical aspects of the projects and provides direction and assistance regarding working with users and the process and issues encountered with user interactionBecome skilled with the architecture and technology supporting the Data Warehouse, to design and develop accordinglyEnsure the distribution of knowledge for processes being designed and built within the team to ensure assigned jobs are completed accurately and according to schedule and that all deadlines are metEnsure the design of sufficient documentation for easy and smooth hand-over of projectsParticipate in the formulation and design of methodologiesTrains in developing with tools that are available with the Data Warehouse infrastructure and help with development, where appropriate. 
The skills and qualifications you need: 5-8 years related experience with bachelor’s degree; or a master’s degree with 3 years’ experienceMust be comfortable programming in C# or show mastery of another programming languageAbility to architect simple and complex solutions related to integration with other applications and designExcellent communication skills, both verbal and writtenStrong presentation skills. Ability to present formally to users and customers during gathering requirements, workshops, and feedback sessionsExperience working under tight deadline while maintaining high product relevance and qualityAzure/MDM/SQL focus5-8 years’ experience in the following areas: Azure databases, ADF pipelines, Azure Integration Runtimes, Orchestration, CI/CD, GitHub, and Third-party integration
Our mission is to reinvent healthcare to help patients live their best lives, and we proudly live our mission-driven values:  We care deeply about the people we serve We are better when we work together Humility is a source of our strength We bring joy to our work
We are committed to diversity, equity, and inclusion throughout our recruiting practices. Everyone is welcome and included. We value our differences and learn from each other. Our team members come in all shapes, colors, and sizes. No matter how you identify your lifestyle, creed, or fandom, we value everyone's unique journey.Oh, and one more thing … a recent study shows that men apply for a job or promotion when they meet only 60% of the qualifications, but women and other marginalized groups apply only if they meet 100% of them. So, if you think you’d be a great fit, but don’t necessarily meet every single requirement on one of our job openings, please still apply. We’d love to consider your application!Come join us and help our patients live their best lives. Learn more at www.interwellhealth.com.It has come to our attention that some individuals or organizations are reaching out to job seekers and posing as potential employers presenting enticing employment offers. We want to emphasize that these offers are not associated with our company and may be fraudulent in nature. Please note that our organization will not extend a job offer without prior communication with our recruiting team, hiring managers and a formal interview process."
RESEARCH DATA ENGINEER (REMOTE),"OCHIN, Inc.",United States (Remote),https://www.linkedin.com/jobs/view/3730070207/?eBP=JOB_SEARCH_ORGANIC&refId=SNJ4vu2KltKkIEutY%2B%2Fd6Q%3D%3D&trackingId=TgjSuYXG%2BPYnn2UC3ENXQQ%3D%3D&trk=flagship3_search_srp_jobs,3730070207,"About the job
            
 
Job TypeFull-timeDescriptionMake a difference at OCHINOCHIN provides leading-edge technology, data analytics, research, and support services to nearly 1,000 community health care sites, reaching nearly 6 million patients nationally. We believe that every individual, no matter their race, ethnicity, background, or zip code, should have fair opportunity to achieve their full health potential. Our work addresses differences in health that are systemic, avoidable, and unjust. We partner, learn, innovate, and advocate, in order to close the gap in health for individuals and communities negatively impacted by racism or other structural inequities.At OCHIN, we value the unique perspectives and experiences of every individual and work hard to maintain a culture of belonging. Founded in Oregon in 2000, OCHIN employs a growing virtual workforce of more than 1,000 diverse professionals, working remotely across 48 states. We offer a generous compensation package and are committed to supporting our employees’ entire well-being by fostering a healthy work-life balance and equitable opportunity for professional advancement. We are curious, collaborative learners who strive to live our values everyday: learning, heart, belonging and impact. OCHIN is excited to support our continued national expansion and the increasing demand for our innovative tools and services by welcoming new talent to our growing team.Position OverviewThe Research Data Engineer will provide high-level professional and technical skills in support of designing, building, and maintaining data pipelines, databases, and cloud platforms to support the needs of the OCHIN Research team.In this role, you will be collaborating with an innovative, collaborative team of people moving exciting projects forward and working to improve systems and processes along the way.Essential Duties Performing day to day management of on-premises, cloud, and hybrid research databases and database platforms including the Research Data WarehouseIntegrating and transforming health-related data from a variety of sources and formats such as EHRs, geospatial, claims, and census into analyzable formats for research Building and maintaining datasets and data martsMonitor and maintain data pipelines proactively to ensure high service availabilityIn partnership with Research Data Science staff and leadership, assist with scoping and designing new research data pipelines and platforms to optimize research data solutionsCreate scripts and programs to automate data operationsPreparing and maintaining technical documentation and metadataProviding technical/consultative services to internal and external research partners, investigators, and other research personnelPerforming other duties as requested by the research team 
Requirements A Master’s level degree in Informatics, Computer Science or related discipline. Equivalent knowledge and skills obtained through a combination of education, training, and experience may meet this requirement. At least 5 years of experience in database development and administration in a healthcare and/or health research settingAt least 3 years’ experience with data warehousing, including ETL techniquesStrong technical proficiency with SQL requiredHigh technical proficiency with Microsoft SQL Server, including the ability to create and edit complex queries and T-SQL scripts including dynamic SQL, required; experience with SSISStrong working knowledge of standard desktop computing software packages (word processing, spreadsheets, presentation software, Internet browsers, etc.). Strong analytical and problem-solving skillsExperience with cloud and/or hybrid cloud/on-premises database architectures preferred Knowledge of specialized and complex statistical modeling and/or machine learning techniques preferred
Base Pay OverviewThe typical offer range for this role is minimum to midpoint, ($98,819 - $128,465) with the midpoint representing the average pay in a national market scope for this position. Please keep in mind that this range represents the pay range for all positions in the job grade within which this position falls. The actual salary offer will consider a wide range of factors directly relevant to this position, including, but not limited to, skills, knowledge, training, responsibility, and experience, as well as internal equity and alignment with market data.Work Location and Travel RequirementsOCHIN is a 100% remote organization with no physical corporate office location. Employees work remotely from home and many of our positions also support our member organizations on-site for new software installations. Nationwide travel is determined based on OCHIN business needs. Please inquire during the interview process about travel requirements for this position. Ability to work independently and efficiently from a home office environmentHigh Speed Internet ServiceIt is a requirement that employees work in a distraction free workplace
We offer a comprehensive range of benefits. See our website for details: https://ochin.org/employment-openingsEqual Opportunity StatementOCHIN is proud to be an equal opportunity employer. We are committed to building a team that represents a variety of backgrounds, perspectives, and skills for the benefit of our staff, our mission, and the communities we serve. As an Equal Opportunity and Affirmative Action employer, OCHIN, Inc. does not discriminate on the basis of race, ethnicity, sex, gender identity, sexual orientation, religion, marital or civil union status, age, disability status, veteran status, or any other protected characteristics. All aspects of employment are based on merit, performance, and business needs.COVID-19 Vaccination RequirementTo keep our colleagues, members, and communities safe, OCHIN requires all employees—including remote employees, contractors, interns, and new hires—to be vaccinated with a COVID-19 vaccine, as supported by state and federal public health officials, as a condition of employment. All new hires are required to provide proof of full vaccination or receive approval for a medical or religious exemption before their hire date.Salary DescriptionMin- $98,819 Mid- $128,465 Max- $158,111"
Principal Data Engineer (Remote),Collins Aerospace,"Texas, United States (Remote)",https://www.linkedin.com/jobs/view/3770133137/?eBP=JOB_SEARCH_ORGANIC&refId=SNJ4vu2KltKkIEutY%2B%2Fd6Q%3D%3D&trackingId=iKCWkreiMWNa5g9IxJzi%2Fg%3D%3D&trk=flagship3_search_srp_jobs,3770133137,"About the job
            
 
Date Posted:2023-11-20Country:United States of AmericaLocation:HTX99: Field Office - TX Remote Location, Remote City, TX, 73301 USAPosition Role Type:RemoteDo you want to be part of the team that builds the Data Platform at the center of Transforming the Aviation Industry? As a Principal Data Engineer, you will be on a mission to ensure that our Data Platform can leverage data from various sources to help transform the passenger journey as well as help our customers leverage data to improve their operations. This role is responsible for the design, development and maintenance of data processes and pipelines supporting critical Strategic Business Unit (SBU) Data initiatives in support of the Digital Transformation. The scope of the role encompasses many facets of the aviation industry. You will work with Data Scientists and Application Developers to build data-based products that benefit commercial airlines, airports and passengers. Your work will influence the next generation of connected aviation products.You will work with a team where you will be able to share your ideas and vulnerabilities and will be treated with care and empathy. You will work with a team that shows courage in doing the right thing, not because it is easy. This is a great opportunity with room to grow and learn about new and interesting technology solutions. Our business unit, Connected Aviation Solutions (CAS), is leading the Connected Ecosystem strategic pillar for Collins Aerospace. The Data Management & Data Science (DM&DS) team has end-to-end responsibility to ensure that CAS data assets are managed with integrity and quality prior to consumption by our critical customer facing applications - whether via API’s, analytics and/or data visualizations.What YOU will do: YOU will contribute to establishing Data Engineering best practices, building the DataOps model and enabling the ML and AI roadmap of the future. YOU will develop automation and monitoring processes that support the data pipelines. YOU will work closely with the architecture team to implement modern data repositories that support the CAS use cases (Pipelines, API’s, Data Science, Applications and Visualizations). YOU will work with internal business customers and software development teams to gather and document requirements for data publishing and data consumption. YOU will work with the CAS and DT Enterprise Data Architects to automate cloud deployments, as well as build CI/CD pipeline to support cloud-based workloads. This includes developing views, materialized views, and SQL scripts. YOU may travel domestically and internationally up to 15%. YOU will work on a distributed and diverse team that collaborates and communicates well. 
What YOU will learn: YOU will learn all about the datasets that are produced in the aerospace ecosystem; such as how the various components in an aircraft interact with each other. YOU will learn how to enable Data Scientists to perform ML and AI experiments. YOU will gain exposure to large scale data processing leveraging modern technology stacks including Databricks and AWS native services. YOU can take flight to becoming a subject matter expert and leader in Data Engineering with exposure to the variety of business and products in an ever-evolving aerospace industry. CAS is growing and so can you. 
Education & Experience: Typically requires a degree in Science, Technology, Engineering or Mathematics (STEM) unless prohibited by local laws/regulations and minimum 8 years prior relevant experience or an Advanced Degree in a related field and minimum 5 years of experience or in absence of a degree, 12 years of relevant experience. 
Qualifications You Must Have: Must be authorized to work in the U.S. without sponsorship now or in the future. RTX will not offer sponsorship for this position. Demonstrated engineering experience in system integration and design, data pipeline development, or software/service development and deployment. Experience building data pipelines leveraging tools like Spark, Python, PySpark & SQL as well as working with AWS/Azure Cloud platforms and related services and Terraform, Gitlab and similar CI/CD tools. Experience with Databricks Platform and leveraging that platform to build out a Data Lake. 
Skills We Value: Professional background developing complex SQL queries and programming in Python with ability to transform raw data into valuable insights. Experience designing cloud-based data platforms that ensure cost optimization, scalability, performance and ease of use for end users of the platform. Experience with Micro Services Architectures. AWS/Azure certifications. 
Collins Aerospace, an RTX company, is a leader in technologically advanced and intelligent solutions for the global aerospace and defense industry. Collins Aerospace has the capabilities, comprehensive portfolio, and expertise to solve customers’ toughest challenges and to meet the demands of a rapidly evolving global market.#reempowerprogramThis role is also eligible for the Re-Empower Program. The Re-Empower Program helps support talented and committed professionals as they rebuild their capabilities, enhance leadership skills, and continue their professional journey. Over the course of the 14-week program, experienced professionals will gain paid, on-the-job experience, have an opportunity to participate in sessions with leadership, develop personalized plans for success and receive coaching to guide their return-to-work experience. Upon completion of the program, based on performance and contributions participants will be eligible for a career at RTX.Minimum Program Qualifications: Be on a career break of one or more year at time of applicationHave prior experience in functional area of interestHave interest in returning in either a full-time or part-time position
Connected Aviation Solutions:Our Connected Aviation Solutions team provides advanced information management systems, products and services that enable the connected ecosystem by bringing together Collins’ unique breadth of aviation products with our smart digital solutions to help us enhance every aspect of the end-to-end travel experience. We help airlines, airports and business aircraft turn data into value to streamline operations, increase efficiency and reduce cost, enhance the passenger experience and contribute to sustainable flight. By combining the best networks, connectivity and data/analytics solutions, we’re solving big problems for our customers and the world, while enhancing the security and connectivity of systems both on and off the aircraft, to help operators and passengers stay more connected and informed and create a more sustainable, efficient, reliable and enjoyable travel experience. Aviation connects the world. Our Connected Aviation Solutions team connects aviation. Sustainably. Seamlessly. Securely.Diversity drives innovation; inclusion drives success. We believe a multitude of approaches and ideas enable us to deliver the best results for our workforce, workplace, and customers. We are committed to fostering a culture where all employees can share their passions and ideas so we can tackle the toughest challenges in our industry and pave new paths to limitless possibility.WE ARE REDEFINING AEROSPACE. Please ensure the role type (defined below) is appropriate for your needs before applying to this role. 
Remote: Employees who are working in Remote roles will work primarily offsite (from home). An employee may be expected to travel to the site location as needed. *Position is remote; however, if you live within a reasonable commute of a Collins site with other colleagues you interact with, your manager will discuss whether there is a degree of onsite presence associated with this role. 
Some of our competitive benefits package includes: Medical, dental, and vision insuranceThree weeks of vacation for newly hired employeesGenerous 401(k) plan that includes employer matching funds and separate employer retirement contribution, including a Lifetime Income Strategy optionTuition reimbursement programStudent Loan Repayment Program Life insurance and disability coverageOptional coverages you can buy: pet insurance, home and auto insurance, additional life and accident insurance, critical illness insurance, group legal, ID theft protectionBirth, adoption, parental leave benefitsOvia Health, fertility, and family planningAdoption Assistance Autism BenefitEmployee Assistance Plan, including up to 10 free counseling sessionsHealthy You Incentives, wellness rewards programDoctor on Demand, virtual doctor visitsBright Horizons, child and elder care services Teladoc Medical Experts, second opinion programAnd more! 
At Collins, the paths we pave together lead to limitless possibility. And the bonds we form – with our customers and with each other -- propel us all higher, again and again.Apply now and be part of the team that’s redefining aerospace, every day.The salary range for this role is 94,000 USD - 196,000 USD; however, RTX considers several factors when extending an offer, including but not limited to, the role and associated responsibilities, a candidate’s work experience, location, education/training, and key skills. Hired applicants may be eligible for benefits, including but not limited to, medical, dental, vision, life insurance, short-term disability, long-term disability, 401(k) match, flexible spending accounts, flexible work schedules, employee assistance program, Employee Scholar Program, parental leave, paid time off, and holidays. Specific benefits are dependent upon the specific business unit as well as whether or not the position is covered by a collective-bargaining agreement. Hired applicants may be eligible for annual short-term and/or long-term incentive compensation programs depending on the level of the position and whether or not it is covered by a collective-bargaining agreement. Payments under these annual programs are not guaranteed and are dependent upon a variety of factors including, but not limited to, individual performance, business unit performance, and/or the company’s performance.RTX is An Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or veteran status, age or any other federally protected class.Privacy Policy and Terms:Click on this link to read the Policy and Terms01663890"
Data Engineer (3) - REMOTE - Active TS/SCI Clearance,Dice,United States (Remote),https://www.linkedin.com/jobs/view/3781600721/?eBP=JOB_SEARCH_ORGANIC&refId=SNJ4vu2KltKkIEutY%2B%2Fd6Q%3D%3D&trackingId=dJImVyFEbqLlm%2FNJ4QKwFg%3D%3D&trk=flagship3_search_srp_jobs,3781600721,"About the job
            
 
Dice is the leading career destination for tech experts at every stage of their careers. Our client, System One, is seeking the following. Apply via Dice today!Candidates must hold an active TS/SCI clearance and live in MD, DC or VAWe are currently seeking a Data Engineer with data pipeline expertise for a full-time, permanent position. This is a hybrid role that is primarily remote work. Candidates must live in VA, MD or DC in order to work onsite when needed.The Data Engineers will support the Office of Intelligence and Analysis (I&A) at DHS, this role will be part of a team focused on supporting the development of a new capability for an I&A mission customer.Duties:  Develop and design data pipelines to support an end-to-end solution Develop and maintain artifacts i.e. schemas, data dictionaries, and transforms related to ETL processes Integrate data pipelines with AWS cloud services to extract meaningful insights Manage production data within multiple datasets ensuring fault tolerance and redundancy Design and develop robust and functional dataflows to support raw data and expected data Provide Tier 3 technical support for deployed applications and dataflows Experience with cloud message APIs and usage of push notifications Collaborate with the rest of data engineering team to design and launch new features. Includes coordination and documentation of dataflows, capabilities, etc. Keen interest in learning and using the latest software tools, methods, and technologies to solve real world problem sets vital to national security
Skills:Database administration and development experience will be a plus for consideration. Data Engineer (3) - REMOTE - Active TS/SCI Clearance"
Senior Data Engineer (Remote),Braintrust,United States (Remote),https://www.linkedin.com/jobs/view/3775100816/?eBP=JOB_SEARCH_ORGANIC&refId=SNJ4vu2KltKkIEutY%2B%2Fd6Q%3D%3D&trackingId=G%2FCAhwELA37QXKv%2F1DXCxQ%3D%3D&trk=flagship3_search_srp_jobs,3775100816,"About the job
            
 
About UsBraintrust is a user-owned talent network that connects top-tier professionals with the world's leading enterprises. We prioritize transparency, eliminating middlemen and high markups, ensuring job-seekers are matched swiftly to innovative roles while clients benefit from unparalleled efficiency and quality.About The Hiring ProcessThe hiring process for this role involves completing your Braintrust profile, applying directly to the role on Braintrust, and undergoing a one-time screening to ensure you meet our vetted talent specifications. After this, the hiring team will contact you directly if they believe you are a suitable match.Our process isn't for everyone, that's intentional. If you believe that you are a top candidate for this job, please join our network to give yourself the opportunity to work with top companies.JOB TYPE: Direct Hire/ FTE Position (no agencies/C2C - see notes below)LOCATION: Work from anywhere - Anytime | No timezone overlap requiredSALARY RANGE $153,000 – $210,000 /yrESTIMATED DURATION: 40/week - long termEXPERIENCE: 5-9 yearsBRAINTRUST JOB ID: 11426The OpportunityWhat We’re Looking For 4+ years of proven experience as a Data Engineer, with a focus on building robust data pipelines and maintaining data infrastructure. Strong programming skills in languages like Python, Scala or Javascript, with a solid understanding of software engineering principles. Hands-on Experience building or maintaining data pipelines with a modern orchestration tools like Airflorw/Prefect/DagsterStrong expertise with SQL and non-SQL DBs, cloud-based data platforms (e.g., AWS, GCP) and their related services (S3, BigQuery, etc.). Expert level writing of SQL for data manipulation, transformation and for analytics. Experience building and maintaining interactive and intuitive dashboards using tools like Looker. Comfortable with tool development, including building and enhancing internal data tools and frameworks. Excellent problem-solving and analytical skills, with a strong attention to detail. Effective communication skills, with the ability to collaborate with cross-functional teams and present complex ideas to both technical and non-technical stakeholders. 
What You'll Be Working OnWho We Are ✨Snackpass’s mission is to unify the physical and digital world for local commerce.We power mobile order pickup and social commerce for restaurants, modernizing the customer experience while making restaurant operators successful.Opportunity ✨Snackpass is one of the fastest growing marketplaces (a16z top marketplaces), and a top 100 YC company. We are backed by Andreeson Horowitz, Y Combinator, General Catalyst, First Round Capital, Craft Ventures and many others. We are hiring people who are humble and hungry to join us in any of our hubs (NYC, SF, LA) or remotely.Our vision is to be the dominant platform for pickup, a $750B market globally.About The RoleWe are seeking a talented and experienced Senior Data Engineer to join our dynamic team. As a Senior Data Engineer, you will play a crucial role in building and maintaining data pipelines, developing and managing dashboards, and contributing to tool development. Your expertise will enable us to efficiently process and analyze large volumes of data, providing valuable insights to drive business decisions.What You’ll Be Working On Build scalable and efficient data pipelines to collect, process, and transform large volumes of data from diverse sources. Develop and maintain data models, ensuring data integrity and optimizing query performance. Collaborate with stakeholders to understand their data requirements and provide the necessary infrastructure and support. Build and maintain interactive dashboards and visualizations to enable data-driven decision-making across the organization. Contribute to the development of internal data tools and frameworks, automating repetitive tasks and improving data accessibility and usability. Stay up to date with the latest advancements in data engineering technologies and best practices, and proactively recommend improvements to our data infrastructure. Mentor and provide guidance to junior data engineers, fostering a culture of learning and growth. 
What You Will Get From UsYou will receive competitive compensation, a generous equity grant in a high-growth start-up, and benefits like healthcare, medical & dental coverage, unlimited PTO, a home office budget, wellness budget and more.Importantly, you will also receive an unparalleled amount of ownership over the work you do here. We are a small team, so the opportunity to make a large impact, work on a broad spectrum of challenges and grow your personal skill-set awaits you here.Finally, you will get a diverse and inclusive work environment where you will be surrounded by hungry and humble colleagues.Snackpass is an equal opportunity employer and we value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. In fact, we are confident that the most inclusive and diverse teams accomplish the most extraordinary results.Apply Now!NotesOur employers all have varying legal and geographic requirements for their roles, they trust Braintrust to find them the talent that meet their unique specifications. For that reason, this role is not available to C2C candidates working with an agency. If you are a professional contractor who has created an LLC/corp around their consulting practice, this is well aligned with Braintrust and we’d welcome your application.Braintrust values the multitude of talents and perspectives that a diverse workforce brings. All qualified applicants will receive consideration for employment without regard to race, national origin, religion, age, color, sex, sexual orientation, gender identity, disability, or protected veteran status."
"Data Engineer, Senior",Blackbaud,"North Carolina, United States (Remote)",https://www.linkedin.com/jobs/view/3753080444/?eBP=JOB_SEARCH_ORGANIC&refId=SNJ4vu2KltKkIEutY%2B%2Fd6Q%3D%3D&trackingId=jUj6HEcIgSvqYw4OIC8ijw%3D%3D&trk=flagship3_search_srp_jobs,3753080444,"About the job
            
 
Data EngineerThe Data Engineer role spans a broad range of experience. Data Engineers are responsible for ingestion, transformation and integration of data to provide a platform that supports research, data analysis, data enrichment and data science as well as making data operationally available at scale to consuming products and services. This includes a particular focus on the reliability, efficiency and quality of data assets and processing/integration pipelines.Working with and utilizing standardized tools, technologies and processes Data Engineers design, build and manage the data architecture that enables scalable production of data insight and they work very closely with Data Science to build and operationalize the algorithms that identify insights, predict behavior and prescribe action. This intelligence fuels the development of Decision-Driven Experiences for our customers.Blackbaud Data Engineers understand how to leverage data for business value and social good, and our Data Engineering team develops, cultivates and curates our industry-leading platform of integrated data assets.What You'll Be Doing Develop production level code. Own process features from design to implementation.Assemble large, complex data sets for use in R&D, data science, machine learning, or projection to operational stores.Build the infrastructure that provides efficient and scalable ETL/ELT and integration of data from multiple proprietary and 3rd party sources.Code, test and deploy data pipelines in support of various initiatives.Ensure quality of code, processes, and data assets.Work on complex tasks and/or broad programs that are large and diverse in scope and/or critical in nature. Exercises good judgment in determining technical approach.
What We'll Want You To Have 3+ years of ETL/ELT development, implementation, and supportUnderstanding or knowledge of data integration concepts3+ years of demonstrated success developing with Core Programming Languages – SQL, PythonUnderstanding or knowledge of Azure cloud services – Azure Data Catalog, Azure Data Lake Store, Azure BLOB StoreUnderstanding or knowledge of AWS cloud services – S3, EC2, Data Pipelines3+ years utilizing and developing SQL for relational databases (Oracle, MS SQL Server, MySQL)Understanding or knowledge of Linux/Unix commands, scripting (bash) and file management
Stay up to date on everything Blackbaud, follow us on Linkedin, Twitter, Instagram, Facebook and YouTubeBlackbaud is a remote-first company which embraces a flexible remote work culture. Blackbaud supports hiring and career development for all roles from the location you are in today!Blackbaud is proud to be an equal opportunity employer and is committed to maintaining a diverse and inclusive work environment. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, physical or mental disability, age, or veteran status or any other basis protected by federal, state, or local law.To all recruitment agencies: We do not accept unsolicited agency resumes and are not responsible for any fees related to unsolicited resumes.A notice to candidates: Recruitment Fraudulent Alert: Your personal information and online safety as a candidate mean a lot to us! At Blackbaud and our portfolio of companies, recruiters only direct candidates to apply through our official careers page at https://careers.blackbaud.com/us/en or our official LinkedIn page. Recruiters will never request payments, ask for financial account information or sensitive information like social security numbers, or conduct interviews via Skype. Anyone suggesting otherwise is not a representative of Blackbaud. If you are unsure if a message is from Blackbaud, please email blackbaudrecruiting@blackbaud.com. The starting base pay is $117,200.00 to $157,500.00. Blackbaud may pay more or less based on employee qualifications, market value, Company finances, and other operational considerations.Benefits Include Medical, dental, and vision insuranceRemote-first workforce401(k) program with employer matchFlexible paid time offGenerous Parental LeaveVolunteer for vacationOpportunities to connect to build community and belongingPet insurance, legal and identity protectionTuition reimbursement program
R0011171"
"Data Engineer, Senior",Blackbaud,"New York, United States (Remote)",https://www.linkedin.com/jobs/view/3753077823/?eBP=JOB_SEARCH_ORGANIC&refId=SNJ4vu2KltKkIEutY%2B%2Fd6Q%3D%3D&trackingId=NNWJXdleU0LPqcZYS9zNHA%3D%3D&trk=flagship3_search_srp_jobs,3753077823,"About the job
            
 
Data EngineerThe Data Engineer role spans a broad range of experience. Data Engineers are responsible for ingestion, transformation and integration of data to provide a platform that supports research, data analysis, data enrichment and data science as well as making data operationally available at scale to consuming products and services. This includes a particular focus on the reliability, efficiency and quality of data assets and processing/integration pipelines.Working with and utilizing standardized tools, technologies and processes Data Engineers design, build and manage the data architecture that enables scalable production of data insight and they work very closely with Data Science to build and operationalize the algorithms that identify insights, predict behavior and prescribe action. This intelligence fuels the development of Decision-Driven Experiences for our customers.Blackbaud Data Engineers understand how to leverage data for business value and social good, and our Data Engineering team develops, cultivates and curates our industry-leading platform of integrated data assets.What You'll Be Doing Develop production level code. Own process features from design to implementation.Assemble large, complex data sets for use in R&D, data science, machine learning, or projection to operational stores.Build the infrastructure that provides efficient and scalable ETL/ELT and integration of data from multiple proprietary and 3rd party sources.Code, test and deploy data pipelines in support of various initiatives.Ensure quality of code, processes, and data assets.Work on complex tasks and/or broad programs that are large and diverse in scope and/or critical in nature. Exercises good judgment in determining technical approach.
What We'll Want You To Have 3+ years of ETL/ELT development, implementation, and supportUnderstanding or knowledge of data integration concepts3+ years of demonstrated success developing with Core Programming Languages – SQL, PythonUnderstanding or knowledge of Azure cloud services – Azure Data Catalog, Azure Data Lake Store, Azure BLOB StoreUnderstanding or knowledge of AWS cloud services – S3, EC2, Data Pipelines3+ years utilizing and developing SQL for relational databases (Oracle, MS SQL Server, MySQL)Understanding or knowledge of Linux/Unix commands, scripting (bash) and file management
Stay up to date on everything Blackbaud, follow us on Linkedin, Twitter, Instagram, Facebook and YouTubeBlackbaud is a remote-first company which embraces a flexible remote work culture. Blackbaud supports hiring and career development for all roles from the location you are in today!Blackbaud is proud to be an equal opportunity employer and is committed to maintaining a diverse and inclusive work environment. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, physical or mental disability, age, or veteran status or any other basis protected by federal, state, or local law.To all recruitment agencies: We do not accept unsolicited agency resumes and are not responsible for any fees related to unsolicited resumes.A notice to candidates: Recruitment Fraudulent Alert: Your personal information and online safety as a candidate mean a lot to us! At Blackbaud and our portfolio of companies, recruiters only direct candidates to apply through our official careers page at https://careers.blackbaud.com/us/en or our official LinkedIn page. Recruiters will never request payments, ask for financial account information or sensitive information like social security numbers, or conduct interviews via Skype. Anyone suggesting otherwise is not a representative of Blackbaud. If you are unsure if a message is from Blackbaud, please email blackbaudrecruiting@blackbaud.com. The starting base pay is $117,200.00 to $157,500.00. Blackbaud may pay more or less based on employee qualifications, market value, Company finances, and other operational considerations.Benefits Include Medical, dental, and vision insuranceRemote-first workforce401(k) program with employer matchFlexible paid time offGenerous Parental LeaveVolunteer for vacationOpportunities to connect to build community and belongingPet insurance, legal and identity protectionTuition reimbursement program
R0011171"
Data Platform Engineer II,Aware,United States (Remote),https://www.linkedin.com/jobs/view/3744478379/?eBP=JOB_SEARCH_ORGANIC&refId=SNJ4vu2KltKkIEutY%2B%2Fd6Q%3D%3D&trackingId=z6pZZ1r8l1LxcfkZG4iJGg%3D%3D&trk=flagship3_search_srp_jobs,3744478379,"About the job
            
 
We Are AwareWe're making technology that enables enterprises throughout the world to be more connected, collaborative, and secure.Aware is a collaboration intelligence platform that identifies and reduces risk, maintains compliance, and uncovers new business insights from conversations at scale. Consolidate, enrich, search, and manage data across tools like Slack, WorkJam, Teams and Zoom for immediate visibility across the organization. Aware’s comprehensive platform solves common challenges that legal, compliance, information security and IT departments face when rolling out collaboration, including archiving, monitoring, organization insights, DLP, eDiscovery, retention and legal holds. Aware is a Microsoft Gold Partner, Slack eDiscovery and compliance partner and a Workplace from Meta integration partner.As a critical member of our Engineering team, the Data Platform Engineer II responsible for building out and optimizing Aware SaaS production infrastructure, scale and monitor cloud services, and develop solutions to extend the platform with automation. This position works with both product and machine learning software engineers to design infrastructure and coordinate the production deployment of Aware platform assets.Responsibilities Responsible for the availability, latency, performance, efficiency, monitoring, and emergency response of Aware production cloud service(s). On-call duty & rotation. Demand Forecasting and Capacity Planning for cloud infrastructures. 50% time in coding project works - building tools, solutions, and reusable script templates for the cloud service(s). Helping developers with DevOps and automation requestsEvaluating new infrastructure and running platform POCsDefine, review and perform Security OperationsBe a champion for a developer experience
Requirements Proficient in one or more programming or scripting languages (C#, Java, Python, C++). Prior experience with Terraform and Kubernetes is critical. 3+ years of experience in hands-on coding and deploying solutions on cloud platforms (E.g., Azure, AWS, GCP). 3+ years of experience with code deployment and production configuration managementExperience with one of the popular observability platforms is a must (Grafana, Datadog, NewRelic etc.)Experience with fast-paced, agile development team, and customers satisfaction focused product team. Experience with on-call, production incidents response and perform RCA. Experience with log management tools, building monitoring dashboards. Past experience participating in cross-functional teams launching SaaS products and services. 
THE PERKS Competitive Equity Options - We want every team member to take ownership of what we’re building. With competitive equity options, you’ll be invited to take a seat at our rapidly expanding, venture-backed table. You’ll have the opportunity to invest in yourself and in your team—and watch it pay off. 401K Savings + Matching - We believe your benefits should work for you even after you stop working. With a 401(k) savings plan, we’ll help you lay the groundwork for a flexible financial future. We’ll match 50% of contributions of up to 6% of your salary so that you can look forward to retirement, stress-free. Health, Vision + Dental - We provide full health, vision, and dental insurance to support you and your family at every stage of life. Whether you’re a party of one or a family of five, you’ll enjoy low deductibles, 100% premium coverage, and the resources to seek the best care exactly when you need it. Paid Holidays + Time Off - Sometimes, to be fully present, you need to get away. Beyond our 11 paid holidays, we encourage our employees to take the time off that makes sense for them. We don’t put limits on your vacation or your personal days. Instead, we put our trust in your judgment. (Just don’t forget to bring us back a souvenir.)Parental + Family Leave - Your career isn’t the only exciting thing in your life—and that’s a great thing. Whether you’re welcoming a new addition to your family or helping them through a transition, we’ll support you as you take the space and time you need. As parents, daughters, sons, and siblings, we know that family comes first. And flexibility goes a long way. Remote/Flex Work - Work wherever and whenever you produce the best work. We mean it. You’re always welcome at our office in Columbus, but if you prefer to work from home—or from Rome—we encourage that. Don’t want to miss the morning carpool or afternoon snack? That’s cool, too. Design a work schedule that works for your whole life. 
When applying for this position, please be sure to upload your resume along with your application"
"Data Engineer, Senior",Blackbaud,"New Jersey, United States (Remote)",https://www.linkedin.com/jobs/view/3753081379/?eBP=JOB_SEARCH_ORGANIC&refId=SNJ4vu2KltKkIEutY%2B%2Fd6Q%3D%3D&trackingId=h%2B8GmXrDYDJ8mwfy0s0pEg%3D%3D&trk=flagship3_search_srp_jobs,3753081379,"About the job
            
 
Data EngineerThe Data Engineer role spans a broad range of experience. Data Engineers are responsible for ingestion, transformation and integration of data to provide a platform that supports research, data analysis, data enrichment and data science as well as making data operationally available at scale to consuming products and services. This includes a particular focus on the reliability, efficiency and quality of data assets and processing/integration pipelines.Working with and utilizing standardized tools, technologies and processes Data Engineers design, build and manage the data architecture that enables scalable production of data insight and they work very closely with Data Science to build and operationalize the algorithms that identify insights, predict behavior and prescribe action. This intelligence fuels the development of Decision-Driven Experiences for our customers.Blackbaud Data Engineers understand how to leverage data for business value and social good, and our Data Engineering team develops, cultivates and curates our industry-leading platform of integrated data assets.What You'll Be Doing Develop production level code. Own process features from design to implementation.Assemble large, complex data sets for use in R&D, data science, machine learning, or projection to operational stores.Build the infrastructure that provides efficient and scalable ETL/ELT and integration of data from multiple proprietary and 3rd party sources.Code, test and deploy data pipelines in support of various initiatives.Ensure quality of code, processes, and data assets.Work on complex tasks and/or broad programs that are large and diverse in scope and/or critical in nature. Exercises good judgment in determining technical approach.
What We'll Want You To Have 3+ years of ETL/ELT development, implementation, and supportUnderstanding or knowledge of data integration concepts3+ years of demonstrated success developing with Core Programming Languages – SQL, PythonUnderstanding or knowledge of Azure cloud services – Azure Data Catalog, Azure Data Lake Store, Azure BLOB StoreUnderstanding or knowledge of AWS cloud services – S3, EC2, Data Pipelines3+ years utilizing and developing SQL for relational databases (Oracle, MS SQL Server, MySQL)Understanding or knowledge of Linux/Unix commands, scripting (bash) and file management
Stay up to date on everything Blackbaud, follow us on Linkedin, Twitter, Instagram, Facebook and YouTubeBlackbaud is a remote-first company which embraces a flexible remote work culture. Blackbaud supports hiring and career development for all roles from the location you are in today!Blackbaud is proud to be an equal opportunity employer and is committed to maintaining a diverse and inclusive work environment. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, physical or mental disability, age, or veteran status or any other basis protected by federal, state, or local law.To all recruitment agencies: We do not accept unsolicited agency resumes and are not responsible for any fees related to unsolicited resumes.A notice to candidates: Recruitment Fraudulent Alert: Your personal information and online safety as a candidate mean a lot to us! At Blackbaud and our portfolio of companies, recruiters only direct candidates to apply through our official careers page at https://careers.blackbaud.com/us/en or our official LinkedIn page. Recruiters will never request payments, ask for financial account information or sensitive information like social security numbers, or conduct interviews via Skype. Anyone suggesting otherwise is not a representative of Blackbaud. If you are unsure if a message is from Blackbaud, please email blackbaudrecruiting@blackbaud.com. The starting base pay is $117,200.00 to $157,500.00. Blackbaud may pay more or less based on employee qualifications, market value, Company finances, and other operational considerations.Benefits Include Medical, dental, and vision insuranceRemote-first workforce401(k) program with employer matchFlexible paid time offGenerous Parental LeaveVolunteer for vacationOpportunities to connect to build community and belongingPet insurance, legal and identity protectionTuition reimbursement program
R0011171"
Data/Cloud Engineer - Information Architecture (Remote),RemoteWorker US,"New York, NY (Remote)",https://www.linkedin.com/jobs/view/3780839561/?eBP=JOB_SEARCH_ORGANIC&refId=SNJ4vu2KltKkIEutY%2B%2Fd6Q%3D%3D&trackingId=awG4H108hXNNerMhbN9rtw%3D%3D&trk=flagship3_search_srp_jobs,3780839561,"About the job
            
 
About the Team: Information Architecture is responsible for CrowdStrike's core data model, metadata catalog and associated query services.The catalog scope runs the gamut from internal sources and users, to trusted third parties, and open industry standards. Our mission is to enable data interoperability among all consumers, producers, and inquirers. About the Role: We are looking for a colleague to help develop and maintain a specialized semantic data framework which serves as the backbone of several cloud services, libraries, and databases at CrowdStrike. What You'll Do: Design and develop performant services and tools for data modeling, replication, and governance Collaborate with colleagues focusing on query compilers and data pipelines Own features from design to delivery, including sustained care after release Serve as the data modeling expert for the team Collaborate with other teams on cross functional design Understand and evolve the larger system architecture and its impact on development and design What You’ll Need: Experience building data model driven declarative systems Expertise with service-based architectures and distributed databases Experience with developing cloud-based or similar highly concurrent, distributed systems Familiarity with both relational databases and schema-less “NoSQL” or key-value stores Proficient in transforming structured and unstructured data Experience writing production quality code in one of Python, Go, or C++ Comfortable with testing as a first-class activity: unit testing, integration / end-to-end testing, and associated automations Deliver and accept feedback with grace and courtesy Comfortable working in a distributed environment across multiple time zones Bonus Points For: Experience developing code within a well-defined information architecture Code optimization and performance assessment/profiling/testing Experience with cloud databases such as Cassandra, Snowflake and Redshift, and data analytics frameworks such as Apache Spark Experience developing design tools, tests, and/or test automation Experience with testing frameworks, Jenkins or Bamboo in a CI/CD environment Experience with Erlang, Haskell, Scala, or OCaml Experience developing software for high-assurance, safety critical, or life critical domains Familiarity with generative or formal software verification methods, such as TLA+ and Agda"
"Data Engineer, Senior",Blackbaud,"Ohio, United States (Remote)",https://www.linkedin.com/jobs/view/3753083152/?eBP=JOB_SEARCH_ORGANIC&refId=SNJ4vu2KltKkIEutY%2B%2Fd6Q%3D%3D&trackingId=JMWv8vUGbrdSC36BtZRclQ%3D%3D&trk=flagship3_search_srp_jobs,3753083152,"About the job
            
 
Data EngineerThe Data Engineer role spans a broad range of experience. Data Engineers are responsible for ingestion, transformation and integration of data to provide a platform that supports research, data analysis, data enrichment and data science as well as making data operationally available at scale to consuming products and services. This includes a particular focus on the reliability, efficiency and quality of data assets and processing/integration pipelines.Working with and utilizing standardized tools, technologies and processes Data Engineers design, build and manage the data architecture that enables scalable production of data insight and they work very closely with Data Science to build and operationalize the algorithms that identify insights, predict behavior and prescribe action. This intelligence fuels the development of Decision-Driven Experiences for our customers.Blackbaud Data Engineers understand how to leverage data for business value and social good, and our Data Engineering team develops, cultivates and curates our industry-leading platform of integrated data assets.What You'll Be Doing Develop production level code. Own process features from design to implementation.Assemble large, complex data sets for use in R&D, data science, machine learning, or projection to operational stores.Build the infrastructure that provides efficient and scalable ETL/ELT and integration of data from multiple proprietary and 3rd party sources.Code, test and deploy data pipelines in support of various initiatives.Ensure quality of code, processes, and data assets.Work on complex tasks and/or broad programs that are large and diverse in scope and/or critical in nature. Exercises good judgment in determining technical approach.
What We'll Want You To Have 3+ years of ETL/ELT development, implementation, and supportUnderstanding or knowledge of data integration concepts3+ years of demonstrated success developing with Core Programming Languages – SQL, PythonUnderstanding or knowledge of Azure cloud services – Azure Data Catalog, Azure Data Lake Store, Azure BLOB StoreUnderstanding or knowledge of AWS cloud services – S3, EC2, Data Pipelines3+ years utilizing and developing SQL for relational databases (Oracle, MS SQL Server, MySQL)Understanding or knowledge of Linux/Unix commands, scripting (bash) and file management
Stay up to date on everything Blackbaud, follow us on Linkedin, Twitter, Instagram, Facebook and YouTubeBlackbaud is a remote-first company which embraces a flexible remote work culture. Blackbaud supports hiring and career development for all roles from the location you are in today!Blackbaud is proud to be an equal opportunity employer and is committed to maintaining a diverse and inclusive work environment. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, physical or mental disability, age, or veteran status or any other basis protected by federal, state, or local law.To all recruitment agencies: We do not accept unsolicited agency resumes and are not responsible for any fees related to unsolicited resumes.A notice to candidates: Recruitment Fraudulent Alert: Your personal information and online safety as a candidate mean a lot to us! At Blackbaud and our portfolio of companies, recruiters only direct candidates to apply through our official careers page at https://careers.blackbaud.com/us/en or our official LinkedIn page. Recruiters will never request payments, ask for financial account information or sensitive information like social security numbers, or conduct interviews via Skype. Anyone suggesting otherwise is not a representative of Blackbaud. If you are unsure if a message is from Blackbaud, please email blackbaudrecruiting@blackbaud.com. The starting base pay is $117,200.00 to $157,500.00. Blackbaud may pay more or less based on employee qualifications, market value, Company finances, and other operational considerations.Benefits Include Medical, dental, and vision insuranceRemote-first workforce401(k) program with employer matchFlexible paid time offGenerous Parental LeaveVolunteer for vacationOpportunities to connect to build community and belongingPet insurance, legal and identity protectionTuition reimbursement program
R0011171"
Data Center Infra Engineer,IT Minds LLC,United States (Remote),https://www.linkedin.com/jobs/view/3694243841/?eBP=JOB_SEARCH_ORGANIC&refId=SNJ4vu2KltKkIEutY%2B%2Fd6Q%3D%3D&trackingId=SYCu4ByfpDygmh6VNCcwnA%3D%3D&trk=flagship3_search_srp_jobs,3694243841,"About the job
            
 
Job Title: Data Center Infra EngineerLocation: RemoteQualificationsSomeone with a deep understanding of software-hardware interactions that obsesses about low-level Linux configurations and optimizationsDeep experience and fluency with Linux environmentsExperience building with modern infrastructure tools such as Docker, Kubernetes, Ansible, Packer and Terraform. Experience with TCP/IP configurations and troubleshooting on Linux systemsExperience writing code to automate infrastructure management tasks with at least one language (Go, Python, Bash or similar)5+ years of infrastructure management experience including bare metal server management and operating system image creation and deployments Responsibilities:  Design and Implementation:
Collaborate with IT teams to design and plan data center infrastructure, including server racks, networking equipment, power distribution, cooling systems, and security measures.Implement and deploy hardware and software solutions in the data center environment.Ensure compliance with industry standards, best practices, and security protocols.  Maintenance and Troubleshooting:
Perform regular maintenance tasks such as server hardware upgrades, firmware updates, and system patches.Monitor data center systems for performance, availability, and security, and proactively address issues as they arise.Troubleshoot hardware, software, and network problems to minimize downtime and maintain optimal performance.  Capacity Planning and Optimization:
Monitor data center resources, including power consumption, cooling efficiency, and server utilization, to ensure scalability and efficient resource allocation.Analyze performance metrics and recommend upgrades or optimizations to meet growing business needs.  Networking and Connectivity:
Configure and maintain network switches, routers, and firewalls to ensure seamless data flow and secure communication within the data center.Collaborate with network engineers to design and implement network architecture that supports high availability and redundancy.  Security and Compliance:
Implement and maintain security measures to protect data center assets from physical and cyber threats.Ensure compliance with industry standards and regulations such as ISO 27001, NIST, and HIPAA, depending on the industry.  Documentation:
Create and maintain documentation for data center layouts, configurations, and procedures.Keep accurate records of hardware and software inventory, licenses, and maintenance schedules.  Vendor Management:
Collaborate with vendors and suppliers to procure data center equipment, negotiate contracts, and ensure timely delivery of hardware and services.Evaluate and recommend new technologies and solutions that enhance data center efficiency and performance. Requirements:Bachelor's degree in Computer Science, Information Technology, or a related field (or equivalent experience).Proven experience in data center operations, infrastructure design, and implementation.Proficiency in server hardware, operating systems (Linux, Windows), virtualization technologies, and networking protocols.Familiarity with data center management tools and monitoring solutions.Strong troubleshooting and problem-solving skills.Knowledge of security best practices and experience implementing security measures in a data center environment.Excellent communication and collaboration skills for working with cross-functional teams and vendors.Relevant certifications such as CompTIA Server+, Cisco CCNA, VMware VCP, or equivalent, are a plus.You should have deep experience and nuanced opinions on existing cloud services and developer tools5+ years of professional SRE experience5+ years of experience contributing to architecture and design (architecture, design patterns, reliability and scaling) of new and current systemsBachelor's Degree in Computer Science or related field, or 8+ years relevant work experienceSolid understanding of infrastructure design, including the operational trade-offs of various designsExperience writing high quality code with at least one programming language (Python, Go, or similar)Experience building with modern infrastructure tools such as Docker, Kubernetes, Ansible, Cloud Formation, TerraformExperience building with modern CI/CD practices and build systems, such as GitLab CI/CD, CircleCI, GitHub ActionsExperience with logging, monitoring and alerting systems and toolsExperience with Unix/Linux environmentsExperience with TCP/IP and network programmingWith Regards,NAGENDRA UNDAVALLIHiring for Fulltime / Contract Employment Opportunities.IT Minds LLC.Phone: 949-534-3939 Ext : 412 | Direct: 949-210-9874Cell: 949-649-3979 Email ID: Nagendra@itminds.netwww.itminds.net"
"Data Engineer, Senior",Blackbaud,"Florida, United States (Remote)",https://www.linkedin.com/jobs/view/3753083150/?eBP=JOB_SEARCH_ORGANIC&refId=SNJ4vu2KltKkIEutY%2B%2Fd6Q%3D%3D&trackingId=0ezD0VhjnYmSvVgDbBUjDg%3D%3D&trk=flagship3_search_srp_jobs,3753083150,"About the job
            
 
Data EngineerThe Data Engineer role spans a broad range of experience. Data Engineers are responsible for ingestion, transformation and integration of data to provide a platform that supports research, data analysis, data enrichment and data science as well as making data operationally available at scale to consuming products and services. This includes a particular focus on the reliability, efficiency and quality of data assets and processing/integration pipelines.Working with and utilizing standardized tools, technologies and processes Data Engineers design, build and manage the data architecture that enables scalable production of data insight and they work very closely with Data Science to build and operationalize the algorithms that identify insights, predict behavior and prescribe action. This intelligence fuels the development of Decision-Driven Experiences for our customers.Blackbaud Data Engineers understand how to leverage data for business value and social good, and our Data Engineering team develops, cultivates and curates our industry-leading platform of integrated data assets.What You'll Be Doing Develop production level code. Own process features from design to implementation.Assemble large, complex data sets for use in R&D, data science, machine learning, or projection to operational stores.Build the infrastructure that provides efficient and scalable ETL/ELT and integration of data from multiple proprietary and 3rd party sources.Code, test and deploy data pipelines in support of various initiatives.Ensure quality of code, processes, and data assets.Work on complex tasks and/or broad programs that are large and diverse in scope and/or critical in nature. Exercises good judgment in determining technical approach.
What We'll Want You To Have 3+ years of ETL/ELT development, implementation, and supportUnderstanding or knowledge of data integration concepts3+ years of demonstrated success developing with Core Programming Languages – SQL, PythonUnderstanding or knowledge of Azure cloud services – Azure Data Catalog, Azure Data Lake Store, Azure BLOB StoreUnderstanding or knowledge of AWS cloud services – S3, EC2, Data Pipelines3+ years utilizing and developing SQL for relational databases (Oracle, MS SQL Server, MySQL)Understanding or knowledge of Linux/Unix commands, scripting (bash) and file management
Stay up to date on everything Blackbaud, follow us on Linkedin, Twitter, Instagram, Facebook and YouTubeBlackbaud is a remote-first company which embraces a flexible remote work culture. Blackbaud supports hiring and career development for all roles from the location you are in today!Blackbaud is proud to be an equal opportunity employer and is committed to maintaining a diverse and inclusive work environment. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, physical or mental disability, age, or veteran status or any other basis protected by federal, state, or local law.To all recruitment agencies: We do not accept unsolicited agency resumes and are not responsible for any fees related to unsolicited resumes.A notice to candidates: Recruitment Fraudulent Alert: Your personal information and online safety as a candidate mean a lot to us! At Blackbaud and our portfolio of companies, recruiters only direct candidates to apply through our official careers page at https://careers.blackbaud.com/us/en or our official LinkedIn page. Recruiters will never request payments, ask for financial account information or sensitive information like social security numbers, or conduct interviews via Skype. Anyone suggesting otherwise is not a representative of Blackbaud. If you are unsure if a message is from Blackbaud, please email blackbaudrecruiting@blackbaud.com. The starting base pay is $117,200.00 to $157,500.00. Blackbaud may pay more or less based on employee qualifications, market value, Company finances, and other operational considerations.Benefits Include Medical, dental, and vision insuranceRemote-first workforce401(k) program with employer matchFlexible paid time offGenerous Parental LeaveVolunteer for vacationOpportunities to connect to build community and belongingPet insurance, legal and identity protectionTuition reimbursement program
R0011171"
Senior Cloud Data Engineer - Azure (Remote),Mercy,"Chesterfield, MO (Remote)",https://www.linkedin.com/jobs/view/3765994055/?eBP=JOB_SEARCH_ORGANIC&refId=SNJ4vu2KltKkIEutY%2B%2Fd6Q%3D%3D&trackingId=1oBQA2f1wxtSKO52wSzoTA%3D%3D&trk=flagship3_search_srp_jobs,3765994055,"About the job
            
 
We’re a Little DifferentOur mission is clear. We bring to life a healing ministry through our compassionate care and exceptional service.At Mercy, we believe in careers that match the unique gifts of unique individuals – careers that not only make the most of your skills and talents, but also your heart. Join us and discover why Modern Healthcare Magazine named us in its “Top 100 Places to Work.”Overview: Senior Cloud Data Engineer - Azure Position can be remote (work from home) Please note that as of the posting date of this job announcement, Mercy is unable to offer immigration sponsorship or visa assistance for this position. We encourage all eligible candidates, including U.S. citizens, permanent residents, and those with existing work authorization, to apply.
Mercy believes it is critical to utilize data as an asset to solve some of the most pressing healthcare challenges leveraging the services of cross-functional teams, the latest technologies and the right strategies to improve the quality, accessibility and affordability of healthcare. As Mercy moves towards becoming a more data-driven learning and decisioning organization, and ventures toward a large-scale cloud migration and digital transformation, it is imperative that we are able to develop and deploy an Intelligent Data Platform that: meets various, complex and evolving needs across the enterprise; effectively manages data in compliance with data governance and security standards; continuously improves upon data quality and closes information gaps; and fully aligns with organizational imperatives to further elicit discovery, enhance decisioning, foster innovation, and continuously improve the efficiency, effectiveness, and impact of business and health service offerings. In this backdrop, the role of Senior Cloud Data Engineer is crucial.In this role you will report to Manager of Analytics and Engineering, and will: have an active role designing and implementing the Intelligent Data Platforms information architecture and data engineering; support high data availability and quality to enhance the informativeness of Mercys information assets; and help ensure the Intelligent Data Platform is producing value toward supporting Mercys strategic imperatives and digital transformation. This role will require communication and collaboration with organizational leaders as well as coworkers from other departments.Qualifications Experience: 3+ years of experience in data movement, transformation, and process automation. 2+ years experience writing and optimizing complex SQL queries across large data sets and data models.Required Education: Bachelors Degree or 3+ years of experience in data movement and ETL processesOther: Experience working collaboratively with cross departmental project teams to roll out system-wide deliverables. Experience moving and transforming data within systems such as Azure Data Lake, Blob Storage, Hive Metastore, or comparable on other cloud platforms. Experience using Python for data movement and transformation. Knowledge of optimal design of data and analytic environments, analyzing complex distributed production deployments, and making recommendations to optimize performance. Experience with change management, source control, and continuous deployment strategies. Effective communication skills to turn requirements into project plans and communicate out to partnering teams. Experience working an agile methodology.Preferred ADF, Azure, Databricks development, Python and sql experts. Preferably epic certified if possible but without also is ok if have strong database, ETL/ELT, Azure Cloud and data modeling experience.
We Offer Great BenefitsDay-one comprehensive health, vision and dental coverage, PTO, tuition reimbursement and employer-matched retirement funds are just a few of the great benefits offered to eligible co-workers, including those working 32 hours or more per pay period!We’re bringing to life a healing ministry through compassionate care.At Mercy, our supportive community will be behind you every step of your day, especially the tough ones. You will have opportunities to pioneer new models of care and transform the health care experience through advanced technology and innovative procedures. We’re expanding to help our communities grow. Join us and be a part of it all.What Makes You a Good Match for Mercy? Compassion and professionalism go hand-in-hand with us. Having a positive outlook and a strong sense of advocacy is in perfect step with our mission and vision. We’re also collaborative and unafraid to do a little extra to deliver excellent care – that’s just part of our commitment. If that sounds like a good fit for you, we encourage you to apply."
Senior Data Engineer (Remote),Braintrust,United States (Remote),https://www.linkedin.com/jobs/view/3775102717/?eBP=JOB_SEARCH_ORGANIC&refId=SNJ4vu2KltKkIEutY%2B%2Fd6Q%3D%3D&trackingId=4%2BcH%2FRnoHpknPdN7I3MxdA%3D%3D&trk=flagship3_search_srp_jobs,3775102717,"About the job
            
 
About UsBraintrust is a user-owned talent network that connects top-tier professionals with the world's leading enterprises. We prioritize transparency, eliminating middlemen and high markups, ensuring job-seekers are matched swiftly to innovative roles while clients benefit from unparalleled efficiency and quality.About The Hiring ProcessThe hiring process for this role involves completing your Braintrust profile, applying directly to the role on Braintrust, and undergoing a one-time screening to ensure you meet our vetted talent specifications. After this, the hiring team will contact you directly if they believe you are a suitable match.Our process isn't for everyone, that's intentional. If you believe that you are a top candidate for this job, please join our network to give yourself the opportunity to work with top companies.JOB TYPE: Direct Hire/ FTE Position (no agencies/C2C - see notes below)LOCATION: Work from anywhere - Anytime | No timezone overlap requiredSALARY RANGE $153,000 – $210,000 /yrESTIMATED DURATION: 40/week - long termEXPERIENCE: 5-9 yearsBRAINTRUST JOB ID: 11426The OpportunityWhat We’re Looking For 4+ years of proven experience as a Data Engineer, with a focus on building robust data pipelines and maintaining data infrastructure. Strong programming skills in languages like Python, Scala or Javascript, with a solid understanding of software engineering principles. Hands-on Experience building or maintaining data pipelines with a modern orchestration tools like Airflorw/Prefect/DagsterStrong expertise with SQL and non-SQL DBs, cloud-based data platforms (e.g., AWS, GCP) and their related services (S3, BigQuery, etc.). Expert level writing of SQL for data manipulation, transformation and for analytics. Experience building and maintaining interactive and intuitive dashboards using tools like Looker. Comfortable with tool development, including building and enhancing internal data tools and frameworks. Excellent problem-solving and analytical skills, with a strong attention to detail. Effective communication skills, with the ability to collaborate with cross-functional teams and present complex ideas to both technical and non-technical stakeholders. 
What You'll Be Working OnWho We Are ✨Snackpass’s mission is to unify the physical and digital world for local commerce.We power mobile order pickup and social commerce for restaurants, modernizing the customer experience while making restaurant operators successful.Opportunity ✨Snackpass is one of the fastest growing marketplaces (a16z top marketplaces), and a top 100 YC company. We are backed by Andreeson Horowitz, Y Combinator, General Catalyst, First Round Capital, Craft Ventures and many others. We are hiring people who are humble and hungry to join us in any of our hubs (NYC, SF, LA) or remotely.Our vision is to be the dominant platform for pickup, a $750B market globally.About The RoleWe are seeking a talented and experienced Senior Data Engineer to join our dynamic team. As a Senior Data Engineer, you will play a crucial role in building and maintaining data pipelines, developing and managing dashboards, and contributing to tool development. Your expertise will enable us to efficiently process and analyze large volumes of data, providing valuable insights to drive business decisions.What You’ll Be Working On Build scalable and efficient data pipelines to collect, process, and transform large volumes of data from diverse sources. Develop and maintain data models, ensuring data integrity and optimizing query performance. Collaborate with stakeholders to understand their data requirements and provide the necessary infrastructure and support. Build and maintain interactive dashboards and visualizations to enable data-driven decision-making across the organization. Contribute to the development of internal data tools and frameworks, automating repetitive tasks and improving data accessibility and usability. Stay up to date with the latest advancements in data engineering technologies and best practices, and proactively recommend improvements to our data infrastructure. Mentor and provide guidance to junior data engineers, fostering a culture of learning and growth. 
What You Will Get From UsYou will receive competitive compensation, a generous equity grant in a high-growth start-up, and benefits like healthcare, medical & dental coverage, unlimited PTO, a home office budget, wellness budget and more.Importantly, you will also receive an unparalleled amount of ownership over the work you do here. We are a small team, so the opportunity to make a large impact, work on a broad spectrum of challenges and grow your personal skill-set awaits you here.Finally, you will get a diverse and inclusive work environment where you will be surrounded by hungry and humble colleagues.Snackpass is an equal opportunity employer and we value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. In fact, we are confident that the most inclusive and diverse teams accomplish the most extraordinary results.Apply Now!NotesOur employers all have varying legal and geographic requirements for their roles, they trust Braintrust to find them the talent that meet their unique specifications. For that reason, this role is not available to C2C candidates working with an agency. If you are a professional contractor who has created an LLC/corp around their consulting practice, this is well aligned with Braintrust and we’d welcome your application.Braintrust values the multitude of talents and perspectives that a diverse workforce brings. All qualified applicants will receive consideration for employment without regard to race, national origin, religion, age, color, sex, sexual orientation, gender identity, disability, or protected veteran status."
"Data Engineer, Senior",Blackbaud,"Pennsylvania, United States (Remote)",https://www.linkedin.com/jobs/view/3753083153/?eBP=JOB_SEARCH_ORGANIC&refId=SNJ4vu2KltKkIEutY%2B%2Fd6Q%3D%3D&trackingId=lUlt1Dr0ZRjQASyqTH5ERg%3D%3D&trk=flagship3_search_srp_jobs,3753083153,"About the job
            
 
Data EngineerThe Data Engineer role spans a broad range of experience. Data Engineers are responsible for ingestion, transformation and integration of data to provide a platform that supports research, data analysis, data enrichment and data science as well as making data operationally available at scale to consuming products and services. This includes a particular focus on the reliability, efficiency and quality of data assets and processing/integration pipelines.Working with and utilizing standardized tools, technologies and processes Data Engineers design, build and manage the data architecture that enables scalable production of data insight and they work very closely with Data Science to build and operationalize the algorithms that identify insights, predict behavior and prescribe action. This intelligence fuels the development of Decision-Driven Experiences for our customers.Blackbaud Data Engineers understand how to leverage data for business value and social good, and our Data Engineering team develops, cultivates and curates our industry-leading platform of integrated data assets.What You'll Be Doing Develop production level code. Own process features from design to implementation.Assemble large, complex data sets for use in R&D, data science, machine learning, or projection to operational stores.Build the infrastructure that provides efficient and scalable ETL/ELT and integration of data from multiple proprietary and 3rd party sources.Code, test and deploy data pipelines in support of various initiatives.Ensure quality of code, processes, and data assets.Work on complex tasks and/or broad programs that are large and diverse in scope and/or critical in nature. Exercises good judgment in determining technical approach.
What We'll Want You To Have 3+ years of ETL/ELT development, implementation, and supportUnderstanding or knowledge of data integration concepts3+ years of demonstrated success developing with Core Programming Languages – SQL, PythonUnderstanding or knowledge of Azure cloud services – Azure Data Catalog, Azure Data Lake Store, Azure BLOB StoreUnderstanding or knowledge of AWS cloud services – S3, EC2, Data Pipelines3+ years utilizing and developing SQL for relational databases (Oracle, MS SQL Server, MySQL)Understanding or knowledge of Linux/Unix commands, scripting (bash) and file management
Stay up to date on everything Blackbaud, follow us on Linkedin, Twitter, Instagram, Facebook and YouTubeBlackbaud is a remote-first company which embraces a flexible remote work culture. Blackbaud supports hiring and career development for all roles from the location you are in today!Blackbaud is proud to be an equal opportunity employer and is committed to maintaining a diverse and inclusive work environment. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, physical or mental disability, age, or veteran status or any other basis protected by federal, state, or local law.To all recruitment agencies: We do not accept unsolicited agency resumes and are not responsible for any fees related to unsolicited resumes.A notice to candidates: Recruitment Fraudulent Alert: Your personal information and online safety as a candidate mean a lot to us! At Blackbaud and our portfolio of companies, recruiters only direct candidates to apply through our official careers page at https://careers.blackbaud.com/us/en or our official LinkedIn page. Recruiters will never request payments, ask for financial account information or sensitive information like social security numbers, or conduct interviews via Skype. Anyone suggesting otherwise is not a representative of Blackbaud. If you are unsure if a message is from Blackbaud, please email blackbaudrecruiting@blackbaud.com. The starting base pay is $117,200.00 to $157,500.00. Blackbaud may pay more or less based on employee qualifications, market value, Company finances, and other operational considerations.Benefits Include Medical, dental, and vision insuranceRemote-first workforce401(k) program with employer matchFlexible paid time offGenerous Parental LeaveVolunteer for vacationOpportunities to connect to build community and belongingPet insurance, legal and identity protectionTuition reimbursement program
R0011171"
Sr. Data Engineer,Dice,United States (Remote),https://www.linkedin.com/jobs/view/3781598715/?eBP=JOB_SEARCH_ORGANIC&refId=zE1Jn4w3w3LLkDvakTbD%2FA%3D%3D&trackingId=Tdxrg8GvNSU0%2BGS4M5d%2BJg%3D%3D&trk=flagship3_search_srp_jobs,3781598715,"About the job
            
 
Dice is the leading career destination for tech experts at every stage of their careers. Our client, INSPYR Solutions, is seeking the following. Apply via Dice today!Title: Senior Data EngineerLocation: Milwaukee, WI (Can be fully remote)Duration: Direct HireCompensation: $125,000 - $135,000Work Requirements: , Holders or Authorized to Work in the U.S. Summary: The Senior Data Engineer will focus on quality engineering best practices to meet and exceed internal and external client expectations. In this position, you will analyze, design, develop, test and document solutions supporting data integration, performance tuning, and data modeling to drive organization growth objectives. The Senior Data Engineer will define the standards for data architecture, platform architecture, and data quality and governance. This role is responsible for ensuring that the function is aligned with the overall organization and continuously works to meet critical service levels in access, delivery and security. Your Contributions:   Co-architect next-generation cloud data analytics platform.  Increase operating efficiency and adapt to new requirements.  Monitor and maintain the health of solutions generated.  Support and enhance our data-ops practices.  Provide task breakdowns, identify dependencies, and provide effort estimates.  Model data warehouse entities in Erwin.  Build data transformation pipelines with Data Build Tools (DBT).  Evaluate the latest technology trends and develop proof-of-concept prototypes that align with company opportunities.  Develop positive relationships with clients, stakeholders, and internal teams.  Understand business goals, drivers, context, and processes to suggest technology solutions that improve the organization.  Work collaboratively on creative solutions with engineers, product managers, and analysts in an agile like environment.  Perform, design, and code reviews.  Perform other position-related duties as assigned. 
 Requirements:   Bachelor's degree in computer engineering, computer science, data science, or related field  Two years or more experience designing and implementing data warehouses in Snowflake  Eight years or more experience working with data modeling, architecture and engineering  Experience with all core software development activities, including requirements gathering, design, construction, and testing  Experience performing data transformation using DBT  Experience working with DQ products such as Monte Carlo, BigEye, or Great Expectations  Experience with Azure DevOps (Repos, Pipelines, Boards, Wiki, Test Plans)  Experience with formal software development methodologies including Software Development Life Cycle (SDLC), Agile or SCRUM  Experience building high-performance and highly reliable data pipelines  Experience Knowledge of data warehouse design patterns (star schema, data vault)  Experience building dashboards with business integrations tools  Knowledge of DataOps with cloud-based compute, storage, integration and security patterns  Knowledge and understanding of RESTful APIs  Knowledge of current data engineering trends, best practices, and standards  Knowledge of SQL and Python  Ability to work in a collaborative environment  Ability to facilitate evaluation of technologies and achieve consensus on technical standards and solutions among a diverse group of information technology professionals  Ability to work in an organization driven by continuous improvement or with equivalent focus on process improvement  Ability to manage multiple, competing priorities and attain the best possible outcomes for the organization  Excellent verbal and written communication and effective listening skills 
 Preferred Requirements:   Experience in delivering an end-to-end data analytics platform using modern data stack components  Experience with AI/ML  SnowPro Advanced Certification  DBT Analytical Engineer Certification 
About INSPYR SolutionsTechnology is our focus and quality is our commitment. As a national expert in delivering flexible technology and talent solutions, we strategically align industry and technical expertise with our clients business objectives and cultural needs. Our solutions are tailored to each client and include a wide variety of professional services, project, and talent solutions. By always striving for excellence and focusing on the human aspect of our business, we work seamlessly with our talent and clients to match the right solutions to the right opportunities. Learn more about us at inspyrsolutions.com.INSPYR Solutions provides Equal Employment Opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability, or genetics. In addition to federal law requirements, INSPYR Solutions complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities.Sr. Data Engineer"
Data Engineer SQL / Python / ETL / Azure (Local but Remote),"Liberty Personnel Services, Inc.","Philadelphia, PA (Remote)",https://www.linkedin.com/jobs/view/3774927262/?eBP=JOB_SEARCH_ORGANIC&refId=zE1Jn4w3w3LLkDvakTbD%2FA%3D%3D&trackingId=e4qFsnHmGe6%2FFLKq3%2FOkpA%3D%3D&trk=flagship3_search_srp_jobs,3774927262,"About the job
            
 
Job Details:Data Engineer SQL / Python / ETL / AzureMy client a top place to work in Philadelphia just called and needs to hire asap a great Data Engineer with SQL / Python / ETL and any Azure or AWS a big plus. This is a contract to perm role. BS is preferred. This is a remote role but the client would like someone local for occasional meetings onsite. Any spark would be a plus!!If you are interested, please forward your resume to kevin@libertyjobs.comKevin McCarthy#associate#mid-senior"
"Data Engineer, Senior",Blackbaud,"Massachusetts, United States (Remote)",https://www.linkedin.com/jobs/view/3753078771/?eBP=JOB_SEARCH_ORGANIC&refId=zE1Jn4w3w3LLkDvakTbD%2FA%3D%3D&trackingId=GkHBP%2FIbTmp6ez%2BOFSVksA%3D%3D&trk=flagship3_search_srp_jobs,3753078771,"About the job
            
 
Data EngineerThe Data Engineer role spans a broad range of experience. Data Engineers are responsible for ingestion, transformation and integration of data to provide a platform that supports research, data analysis, data enrichment and data science as well as making data operationally available at scale to consuming products and services. This includes a particular focus on the reliability, efficiency and quality of data assets and processing/integration pipelines.Working with and utilizing standardized tools, technologies and processes Data Engineers design, build and manage the data architecture that enables scalable production of data insight and they work very closely with Data Science to build and operationalize the algorithms that identify insights, predict behavior and prescribe action. This intelligence fuels the development of Decision-Driven Experiences for our customers.Blackbaud Data Engineers understand how to leverage data for business value and social good, and our Data Engineering team develops, cultivates and curates our industry-leading platform of integrated data assets.What You'll Be Doing Develop production level code. Own process features from design to implementation.Assemble large, complex data sets for use in R&D, data science, machine learning, or projection to operational stores.Build the infrastructure that provides efficient and scalable ETL/ELT and integration of data from multiple proprietary and 3rd party sources.Code, test and deploy data pipelines in support of various initiatives.Ensure quality of code, processes, and data assets.Work on complex tasks and/or broad programs that are large and diverse in scope and/or critical in nature. Exercises good judgment in determining technical approach.
What We'll Want You To Have 3+ years of ETL/ELT development, implementation, and supportUnderstanding or knowledge of data integration concepts3+ years of demonstrated success developing with Core Programming Languages – SQL, PythonUnderstanding or knowledge of Azure cloud services – Azure Data Catalog, Azure Data Lake Store, Azure BLOB StoreUnderstanding or knowledge of AWS cloud services – S3, EC2, Data Pipelines3+ years utilizing and developing SQL for relational databases (Oracle, MS SQL Server, MySQL)Understanding or knowledge of Linux/Unix commands, scripting (bash) and file management
Stay up to date on everything Blackbaud, follow us on Linkedin, Twitter, Instagram, Facebook and YouTubeBlackbaud is a remote-first company which embraces a flexible remote work culture. Blackbaud supports hiring and career development for all roles from the location you are in today!Blackbaud is proud to be an equal opportunity employer and is committed to maintaining a diverse and inclusive work environment. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, physical or mental disability, age, or veteran status or any other basis protected by federal, state, or local law.To all recruitment agencies: We do not accept unsolicited agency resumes and are not responsible for any fees related to unsolicited resumes.A notice to candidates: Recruitment Fraudulent Alert: Your personal information and online safety as a candidate mean a lot to us! At Blackbaud and our portfolio of companies, recruiters only direct candidates to apply through our official careers page at https://careers.blackbaud.com/us/en or our official LinkedIn page. Recruiters will never request payments, ask for financial account information or sensitive information like social security numbers, or conduct interviews via Skype. Anyone suggesting otherwise is not a representative of Blackbaud. If you are unsure if a message is from Blackbaud, please email blackbaudrecruiting@blackbaud.com. The starting base pay is $117,200.00 to $157,500.00. Blackbaud may pay more or less based on employee qualifications, market value, Company finances, and other operational considerations.Benefits Include Medical, dental, and vision insuranceRemote-first workforce401(k) program with employer matchFlexible paid time offGenerous Parental LeaveVolunteer for vacationOpportunities to connect to build community and belongingPet insurance, legal and identity protectionTuition reimbursement program
R0011171"
"Data Engineer, Senior",Blackbaud,"Connecticut, United States (Remote)",https://www.linkedin.com/jobs/view/3753079702/?eBP=JOB_SEARCH_ORGANIC&refId=zE1Jn4w3w3LLkDvakTbD%2FA%3D%3D&trackingId=T27TGXlIy7uKaQoaaoHf2w%3D%3D&trk=flagship3_search_srp_jobs,3753079702,"About the job
            
 
Data EngineerThe Data Engineer role spans a broad range of experience. Data Engineers are responsible for ingestion, transformation and integration of data to provide a platform that supports research, data analysis, data enrichment and data science as well as making data operationally available at scale to consuming products and services. This includes a particular focus on the reliability, efficiency and quality of data assets and processing/integration pipelines.Working with and utilizing standardized tools, technologies and processes Data Engineers design, build and manage the data architecture that enables scalable production of data insight and they work very closely with Data Science to build and operationalize the algorithms that identify insights, predict behavior and prescribe action. This intelligence fuels the development of Decision-Driven Experiences for our customers.Blackbaud Data Engineers understand how to leverage data for business value and social good, and our Data Engineering team develops, cultivates and curates our industry-leading platform of integrated data assets.What You'll Be Doing Develop production level code. Own process features from design to implementation.Assemble large, complex data sets for use in R&D, data science, machine learning, or projection to operational stores.Build the infrastructure that provides efficient and scalable ETL/ELT and integration of data from multiple proprietary and 3rd party sources.Code, test and deploy data pipelines in support of various initiatives.Ensure quality of code, processes, and data assets.Work on complex tasks and/or broad programs that are large and diverse in scope and/or critical in nature. Exercises good judgment in determining technical approach.
What We'll Want You To Have 3+ years of ETL/ELT development, implementation, and supportUnderstanding or knowledge of data integration concepts3+ years of demonstrated success developing with Core Programming Languages – SQL, PythonUnderstanding or knowledge of Azure cloud services – Azure Data Catalog, Azure Data Lake Store, Azure BLOB StoreUnderstanding or knowledge of AWS cloud services – S3, EC2, Data Pipelines3+ years utilizing and developing SQL for relational databases (Oracle, MS SQL Server, MySQL)Understanding or knowledge of Linux/Unix commands, scripting (bash) and file management
Stay up to date on everything Blackbaud, follow us on Linkedin, Twitter, Instagram, Facebook and YouTubeBlackbaud is a remote-first company which embraces a flexible remote work culture. Blackbaud supports hiring and career development for all roles from the location you are in today!Blackbaud is proud to be an equal opportunity employer and is committed to maintaining a diverse and inclusive work environment. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, physical or mental disability, age, or veteran status or any other basis protected by federal, state, or local law.To all recruitment agencies: We do not accept unsolicited agency resumes and are not responsible for any fees related to unsolicited resumes.A notice to candidates: Recruitment Fraudulent Alert: Your personal information and online safety as a candidate mean a lot to us! At Blackbaud and our portfolio of companies, recruiters only direct candidates to apply through our official careers page at https://careers.blackbaud.com/us/en or our official LinkedIn page. Recruiters will never request payments, ask for financial account information or sensitive information like social security numbers, or conduct interviews via Skype. Anyone suggesting otherwise is not a representative of Blackbaud. If you are unsure if a message is from Blackbaud, please email blackbaudrecruiting@blackbaud.com. The starting base pay is $117,200.00 to $157,500.00. Blackbaud may pay more or less based on employee qualifications, market value, Company finances, and other operational considerations.Benefits Include Medical, dental, and vision insuranceRemote-first workforce401(k) program with employer matchFlexible paid time offGenerous Parental LeaveVolunteer for vacationOpportunities to connect to build community and belongingPet insurance, legal and identity protectionTuition reimbursement program
R0011171"
"Staff Data Engineer, Customer Analytics - Slack",Slack,Greater Phoenix Area (Remote),https://www.linkedin.com/jobs/view/3739607444/?eBP=JOB_SEARCH_ORGANIC&refId=zE1Jn4w3w3LLkDvakTbD%2FA%3D%3D&trackingId=qZ4VTWq8QetqNPRtGDjOGg%3D%3D&trk=flagship3_search_srp_jobs,3739607444,"About the job
            
 
To get the best candidate experience, please consider applying for a maximum of 3 roles within 12 months to ensure you are not duplicating efforts.Job CategorySoftware EngineeringJob DetailsAbout SalesforceWe’re Salesforce, the Customer Company, inspiring the future of business with AI+ Data +CRM. Leading with our core values, we help companies across every industry blaze new trails and connect with customers in a whole new way. And, we empower you to be a Trailblazer, too — driving your performance and career growth, charting new paths, and improving the state of the world. If you believe in business as the greatest platform for change and in companies doing well and doing good – you’ve come to the right place.About The TeamSlack is looking for data engineers to join our Insights team! We build data products that are used across our large enterprise customers to access metrics that are critical to their workflow.In this role, you will partner cross-functionally with business domain partners, analytics, and engineering to craft and implement our data model. You will design, build and scale data pipelines that transform billions of records into measurable data that enable insights.You will work on initiatives that directly key decision makers within our Enterprise customer base by building and scaling our data models that provide key adoption and engagement metrics.We are looking for passionate individuals with deep technical skills that are comfortable contributing to an up-and-coming data ecosystem, and can build a strong data foundation for the company. We are also in search of a self-starter, detail and quality oriented, and hardworking individual that's interested in making a huge impact at Slack!What you will be doing: Translate business requirements into data models that are easy to understand and used by different subject areasDesign, implement and build data models and pipelines that deliver data with measurable quality under the SLAPartner with product teams, data analysts and engineering teams to build foundational data sets that are trusted, well understood, and aligned with business strategyBe a champion of the overall strategy for data across multiple teams and different use casesIncrease access to foundational company metrics through process and technical foundationsIdentify, document and promote data engineering standard methodologies throughout Slack
What you should have: 7+ years of experience working in data architecture, data modeling, master data management, metadata managementRecent accomplishments working with relational methods and approaches (logging, columnar, star and snowflake, dimensional modeling)A consistent track record in scaling and optimizing schemas, performance tuning SQL and ETL pipelines in OLAP and Data Warehouse environmentsAirflow or any workflow management platform for data engineering pipelines orchestration.Proficiency in coding through the use of with Python and SQLFamiliar with data governance frameworks, SDLC, and Agile methodologyExcellent written and verbal communication and interpersonal skills, and ability to efficiently collaborate with technical and business partnersBachelor's degree in Computer Science, Engineering or a related field, equivalent training, fellowship, or work experience
Bonus Points:  Hands-on experience with Spark SQL, AWS (S3, EMR), Apache Pinot, Big Data & Hadoop.Proficient in coding through the use of ScalaExperience working with cloud technologies such as AWS, GCP or Azure.Understanding of NoSQL Data StoresExperience around Batch & Real-time processing
AccommodationsIf you require assistance due to a disability applying for open positions please submit a request via this Accommodations Request Form.Posting StatementAt Salesforce we believe that the business of business is to improve the state of our world. Each of us has a responsibility to drive Equality in our communities and workplaces. We are committed to creating a workforce that reflects society through inclusive programs and initiatives such as equal pay, employee resource groups, inclusive benefits, and more. Learn more about Equality at www.equality.com and explore our company benefits at www.salesforcebenefits.com .Salesforce is an Equal Employment Opportunity and Affirmative Action Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender perception or identity, national origin, age, marital status, protected veteran status, or disability status. Salesforce does not accept unsolicited headhunter and agency resumes. Salesforce will not pay any third-party agency or company that does not have a signed agreement with Salesforce . Salesforce welcomes all.For Colorado-based roles, the base salary hiring range for this position is $175,600 to $254,700.For Washington-based roles, the base salary hiring range for this position is $175,600 to $254,700.Compensation offered will be determined by factors such as location, level, job-related knowledge, skills, and experience. Certain roles may be eligible for incentive compensation, equity, benefits. More details about our company benefits can be found at the following link: https://www.salesforcebenefits.com."
Senior Data Engineer / Pyspark / 100% Remote,Motion Recruitment,"New York, NY (Remote)",https://www.linkedin.com/jobs/view/3730271793/?eBP=JOB_SEARCH_ORGANIC&refId=zE1Jn4w3w3LLkDvakTbD%2FA%3D%3D&trackingId=k9NmkJh47D4Q8UcWY55GUA%3D%3D&trk=flagship3_search_srp_jobs,3730271793,"About the job
            
 
Our client is changing the way we can find our friends and family from around the world. Founded in 2008, they have millions of users per month visiting their platform for various reasons.They are looking for a Senior Data Engineer to join their Data Ops team to come in and make an impact right away. This Senior Data Engineer should have at least 7 years of professional experience woking with Python, PySpark, Airflow, AWS, and have a computer Science Degree. If this is you APPLY NOW!Basic Qualifications (Required Skills & Experience) 6-8 years of experience Python Pyspark Spark AWS Airflow EMR ETL work SQL 
Other Qualifications & Desired Competencies Fully Remote!!! Equity and Bonuses involved 
You Will Receive The Following Benefits Medical Insurance Dental Benefits Vision Benefits 401(k) 
Posted By: Casey Ryan"
Data Integration Engineer - Hospitality Experience - Remote | WFH,Get It Recruit - Hospitality,"Waltham, MA (Remote)",https://www.linkedin.com/jobs/view/3768630878/?eBP=JOB_SEARCH_ORGANIC&refId=zE1Jn4w3w3LLkDvakTbD%2FA%3D%3D&trackingId=N%2F4g7w7awbDOuWI68345gw%3D%3D&trk=flagship3_search_srp_jobs,3768630878,"About the job
            
 
Our organization, born out of 20 years of growth and success from Dining Alliance, Buyers Edge, and Consolidated Concepts, operates as the Buyers Edge Platform (BEP). As a technology-enabled group purchasing network, we provide group purchasing services, SaaS-based technology solutions, and supply chain consulting to a diverse range of foodservice operators. Serving various verticals such as restaurants, hospitality, healthcare, colleges & universities, and more, BEP represents over $35 Billion in Network Transactions. Our mission is to keep foodservice operators thriving by saving them money and enhancing the quality of their products.Position: Data Integration Engineer (Remote, East Coast working hours)We are unable to offer work sponsorship for this role.Your ImpactCollaborate with stakeholders to understand data needs and support development teams in data integration projects.Align data systems to meet business goals and combine multiple sources for business use.Evaluate the quality, reliability, and security of data sources relevant to assigned projects.Develop code to work with internal and external data sources, including APIs and databases.Coordinate projects with cross-functional teams, including developers, data scientists, analysts, and business stakeholders.About YouProficient in programming languages such as Python, Java, or Node.Strong understanding of database technologies such as MySQL, Aurora, and Redshift.Experience with research and discovery for new data from external sources.Project management experience with a focus on agile development methodologies.Experience with REST APIs and API tools like Postman.Strong analytical, problem-solving, and communication skills.BA/BS in a technical discipline or equivalent professional experience.5-7+ years of experience in programming and the outlined key abilities.What's In This For YouComprehensive medical, dental, and vision coverages.Ancillary plans including flexible spending accounts, critical illness, accident, and voluntary life.Company-paid life and long-term-disability plans.401(k) plan with company match.Thorough training and development program.Competitive compensation.Personal Responsibility Paid Time Off policy and half-day Summer Fridays.We Welcome AllWe are committed to creating a diverse environment and are proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, creed, religion, national origin, ancestry, citizenship status, age, gender, sexual orientation, marital status, military service, physical or mental disability, genetic information, or any other characteristic protected by applicable laws.Employment Type: Full-Time"
"Data Engineer, Senior",Blackbaud,"Michigan, United States (Remote)",https://www.linkedin.com/jobs/view/3753083151/?eBP=JOB_SEARCH_ORGANIC&refId=zE1Jn4w3w3LLkDvakTbD%2FA%3D%3D&trackingId=FVYQCLniLbagQnJ4ixYjAg%3D%3D&trk=flagship3_search_srp_jobs,3753083151,"About the job
            
 
Data EngineerThe Data Engineer role spans a broad range of experience. Data Engineers are responsible for ingestion, transformation and integration of data to provide a platform that supports research, data analysis, data enrichment and data science as well as making data operationally available at scale to consuming products and services. This includes a particular focus on the reliability, efficiency and quality of data assets and processing/integration pipelines.Working with and utilizing standardized tools, technologies and processes Data Engineers design, build and manage the data architecture that enables scalable production of data insight and they work very closely with Data Science to build and operationalize the algorithms that identify insights, predict behavior and prescribe action. This intelligence fuels the development of Decision-Driven Experiences for our customers.Blackbaud Data Engineers understand how to leverage data for business value and social good, and our Data Engineering team develops, cultivates and curates our industry-leading platform of integrated data assets.What You'll Be Doing Develop production level code. Own process features from design to implementation.Assemble large, complex data sets for use in R&D, data science, machine learning, or projection to operational stores.Build the infrastructure that provides efficient and scalable ETL/ELT and integration of data from multiple proprietary and 3rd party sources.Code, test and deploy data pipelines in support of various initiatives.Ensure quality of code, processes, and data assets.Work on complex tasks and/or broad programs that are large and diverse in scope and/or critical in nature. Exercises good judgment in determining technical approach.
What We'll Want You To Have 3+ years of ETL/ELT development, implementation, and supportUnderstanding or knowledge of data integration concepts3+ years of demonstrated success developing with Core Programming Languages – SQL, PythonUnderstanding or knowledge of Azure cloud services – Azure Data Catalog, Azure Data Lake Store, Azure BLOB StoreUnderstanding or knowledge of AWS cloud services – S3, EC2, Data Pipelines3+ years utilizing and developing SQL for relational databases (Oracle, MS SQL Server, MySQL)Understanding or knowledge of Linux/Unix commands, scripting (bash) and file management
Stay up to date on everything Blackbaud, follow us on Linkedin, Twitter, Instagram, Facebook and YouTubeBlackbaud is a remote-first company which embraces a flexible remote work culture. Blackbaud supports hiring and career development for all roles from the location you are in today!Blackbaud is proud to be an equal opportunity employer and is committed to maintaining a diverse and inclusive work environment. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, physical or mental disability, age, or veteran status or any other basis protected by federal, state, or local law.To all recruitment agencies: We do not accept unsolicited agency resumes and are not responsible for any fees related to unsolicited resumes.A notice to candidates: Recruitment Fraudulent Alert: Your personal information and online safety as a candidate mean a lot to us! At Blackbaud and our portfolio of companies, recruiters only direct candidates to apply through our official careers page at https://careers.blackbaud.com/us/en or our official LinkedIn page. Recruiters will never request payments, ask for financial account information or sensitive information like social security numbers, or conduct interviews via Skype. Anyone suggesting otherwise is not a representative of Blackbaud. If you are unsure if a message is from Blackbaud, please email blackbaudrecruiting@blackbaud.com. The starting base pay is $117,200.00 to $157,500.00. Blackbaud may pay more or less based on employee qualifications, market value, Company finances, and other operational considerations.Benefits Include Medical, dental, and vision insuranceRemote-first workforce401(k) program with employer matchFlexible paid time offGenerous Parental LeaveVolunteer for vacationOpportunities to connect to build community and belongingPet insurance, legal and identity protectionTuition reimbursement program
R0011171"
Sr. Data Software Engineer,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3733404980/?eBP=JOB_SEARCH_ORGANIC&refId=zE1Jn4w3w3LLkDvakTbD%2FA%3D%3D&trackingId=2nUs6%2BbkpogrnWmbMLZcbw%3D%3D&trk=flagship3_search_srp_jobs,3733404980,"About the job
            
 
Job: Sr. Data Software EngineerLocation: 100% RemoteDuration: 6 month Contract to Hire (USC/ GC Holder ONLY)Top Skills  SQLAzure Data FactoryDBTETL workProven ability to complete projects in a timely manner while clearly measuring progressStrong software engineering fundamentals (data structures, algorithms, async programming patterns, object-oriented design, parallel programming) Strong understanding and demonstrated experience with at least one popular programming language (.NET or Java) and SQL constructs.Experience writing and maintaining frontend client applications, Angular preferredStrong experience with revision control (Git)Experience with cloud-based systems (Azure / AWS / GCP).High level understanding of big data design (data lake, data mesh, data warehouse) and data normalization patternsDemonstrated experience with Queuing technologies (Kafka / SNS / RabbitMQ etc)Demonstrated experience with Metrics, Logging, Monitoring and Alerting toolsStrong communication skillsStrong experience with use of RESTful APIsHigh level understanding of HL7 V2.x / FHIR based interface messages.High level understanding of system deployment tasks and technologies. (CI/CD Pipeline, K8s, Terraform).
Project/Day To Day Communicate with business leaders to help translate requirements into functional specificationDevelop broad understanding of business logic and functionality of current systemsAnalyze and manipulate data by writing and running SQL queriesAnalyze logs to identify and prevent potential issues from occurringDeliver clean and functional code in accordance with business requirementsConsume data from any source, such a flat files, streaming systems, or RESTful APIs Interface with Electronic Health RecordsEngineer scalable, reliable, and performant systems to manage dataCollaborate closely with other Engineers, QA, Scrum master, Product Manager in your team as well as across the organizationBuild quality systems while expanding offerings to dependent teamsComfortable in multiple roles, from Design and Development to Code Deployment to and monitoring and investigating in production systems."
Senior Big Data Engineer (Remote),Cambridge Technology (CT),"Chicago, IL (Remote)",https://www.linkedin.com/jobs/view/3598176935/?eBP=JOB_SEARCH_ORGANIC&refId=zE1Jn4w3w3LLkDvakTbD%2FA%3D%3D&trackingId=EbguHofDoQv2uzF1LbHw%2Bw%3D%3D&trk=flagship3_search_srp_jobs,3598176935,"About the job
            
 
Join the Cambridge Technology team and grow your career. We are solving real-world problems with creative innovation at some of the most recognizable corporations in the world.From building entire infrastructures or platforms to solving complex IT challenges, we help businesses accelerate their digital transformation and become AI-first businesses. With over 20 years of expertise as a technology services company, we enable our customers to stay ahead of the curve by helping them figure out the perfect approach, solutions, and ecosystem for their business. Our experts help customers leverage the right AI, big data, cloud solutions, and intelligent platforms that will help them become and stay relevant in a rapidly changing world.We are seeking a skilled and experienced Big Data Engineer to join our team. As a Big Data engineer, you will be expected to work closely with our Business and ETL team to implement all Data processing procedures for all new projects and maintain effective awareness of all production activities according to required standards and provide support to all existing applications. Also, you will be expected to work with the Data Architect(s) to help drive architecture and design approaches that result in implemented business solutions.Big Data Engineer Duties & Responsibilities: Work with Big data development team, business stakeholders, DBAs, system administrators, datamodeling teams for Data pipeline build and design.Translate business requirements into technical requirements.Understanding of data quality methodologies and data governance to ensurestandardization of data is maintained.Contribute to the requirements analysis process, associate in creating architecture and designdocuments.Design, develop and test data flows using spark/ pyspark.Perform data science work for analytics.Determine optimal approach for obtaining data from diverse source systems.Be a key contributor to initiatives that require technical expertise.Work closely with the team responsible for maintaining the data model, includingdata dictionary/metadata registry.Possess expertise in project migration and release coordination activities.Interface with business stakeholders to understand requirements and offer solutions.
Skill Set and Qualification: At least six (6) years of experience in working with Big data tools and technology.Experience with Hadoop, Python, Spark/Pyspark, Hive and Tableau/Heavy.ai.Strong exposure working with web service sources/Targets, XML Sources and Restful API’s.Experience with Linux / shell scripting complementary with ETL tool.At least two (2) years of experience in performance tuning.Exposure working with relation databases Oracle, My SQL & SQL Server including complex SQL constructs and DDL generation.Experience in managing and deploying applications in Cloud Platform.Strong knowledge of relational databases and experience with SQL scripting / stored procedures in PL/SQL.Experience with data modeling and data mapping design.Bachelor’s degree in Computer Science, Information Systems, or equivalent education or work experience.
Only considering US based applicants."
Sr. Big Data Engineer | 100% Remote (W2 role),Tek Hire Solutions,United States (Remote),https://www.linkedin.com/jobs/view/3640505402/?eBP=JOB_SEARCH_ORGANIC&refId=zE1Jn4w3w3LLkDvakTbD%2FA%3D%3D&trackingId=21jtALhSegeUlM33SyILJw%3D%3D&trk=flagship3_search_srp_jobs,3640505402,"About the job
            
 
Job Title: Big Data EngineerLocation: 100% remoteWork authorization: Any EAD (Except H1b)Tax Terms: W2NOTE: Must have Graph Database and Java Strong experience in large scale/high volume data pipeline infrastructureExperience with Graph databases technologies (preferably neo4j)Experience with NoSQL databases (HBase, Cassandra, MongoDB)Experience with data stream technologies (Flink, Kafka, Confluent)3+ years supporting open-source Linux operating systems (CentOS, SuSE, Red Hat)Ability to read, understand, and write java code Engineer Strong knowledge of scripting and automation tools (e.g. Ansible, Bash, Python, GitLab)- Engineer
Tekhire Solutions is a world-class technology focused on customer-driven solutions and has a deep dive knowledge of client requirements. Tekhire is best in connecting a bridge between highly skilled workers and the Fortune 500. Always thrives to be up with the technology trends, Innovation and transforms our trusted clients with certified and enterprise solutions that makes difference in the business."
"Data Engineer, Senior",Blackbaud,"South Carolina, United States (Remote)",https://www.linkedin.com/jobs/view/3753082378/?eBP=JOB_SEARCH_ORGANIC&refId=zE1Jn4w3w3LLkDvakTbD%2FA%3D%3D&trackingId=mgcphsAEIZP%2BjuQNw3rAag%3D%3D&trk=flagship3_search_srp_jobs,3753082378,"About the job
            
 
Data EngineerThe Data Engineer role spans a broad range of experience. Data Engineers are responsible for ingestion, transformation and integration of data to provide a platform that supports research, data analysis, data enrichment and data science as well as making data operationally available at scale to consuming products and services. This includes a particular focus on the reliability, efficiency and quality of data assets and processing/integration pipelines.Working with and utilizing standardized tools, technologies and processes Data Engineers design, build and manage the data architecture that enables scalable production of data insight and they work very closely with Data Science to build and operationalize the algorithms that identify insights, predict behavior and prescribe action. This intelligence fuels the development of Decision-Driven Experiences for our customers.Blackbaud Data Engineers understand how to leverage data for business value and social good, and our Data Engineering team develops, cultivates and curates our industry-leading platform of integrated data assets.What You'll Be Doing Develop production level code. Own process features from design to implementation.Assemble large, complex data sets for use in R&D, data science, machine learning, or projection to operational stores.Build the infrastructure that provides efficient and scalable ETL/ELT and integration of data from multiple proprietary and 3rd party sources.Code, test and deploy data pipelines in support of various initiatives.Ensure quality of code, processes, and data assets.Work on complex tasks and/or broad programs that are large and diverse in scope and/or critical in nature. Exercises good judgment in determining technical approach.
What We'll Want You To Have 3+ years of ETL/ELT development, implementation, and supportUnderstanding or knowledge of data integration concepts3+ years of demonstrated success developing with Core Programming Languages – SQL, PythonUnderstanding or knowledge of Azure cloud services – Azure Data Catalog, Azure Data Lake Store, Azure BLOB StoreUnderstanding or knowledge of AWS cloud services – S3, EC2, Data Pipelines3+ years utilizing and developing SQL for relational databases (Oracle, MS SQL Server, MySQL)Understanding or knowledge of Linux/Unix commands, scripting (bash) and file management
Stay up to date on everything Blackbaud, follow us on Linkedin, Twitter, Instagram, Facebook and YouTubeBlackbaud is a remote-first company which embraces a flexible remote work culture. Blackbaud supports hiring and career development for all roles from the location you are in today!Blackbaud is proud to be an equal opportunity employer and is committed to maintaining a diverse and inclusive work environment. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, physical or mental disability, age, or veteran status or any other basis protected by federal, state, or local law.To all recruitment agencies: We do not accept unsolicited agency resumes and are not responsible for any fees related to unsolicited resumes.A notice to candidates: Recruitment Fraudulent Alert: Your personal information and online safety as a candidate mean a lot to us! At Blackbaud and our portfolio of companies, recruiters only direct candidates to apply through our official careers page at https://careers.blackbaud.com/us/en or our official LinkedIn page. Recruiters will never request payments, ask for financial account information or sensitive information like social security numbers, or conduct interviews via Skype. Anyone suggesting otherwise is not a representative of Blackbaud. If you are unsure if a message is from Blackbaud, please email blackbaudrecruiting@blackbaud.com. The starting base pay is $117,200.00 to $157,500.00. Blackbaud may pay more or less based on employee qualifications, market value, Company finances, and other operational considerations.Benefits Include Medical, dental, and vision insuranceRemote-first workforce401(k) program with employer matchFlexible paid time offGenerous Parental LeaveVolunteer for vacationOpportunities to connect to build community and belongingPet insurance, legal and identity protectionTuition reimbursement program
R0011171"
Sr. Data Software Engineer,Steneral Consulting,United States (Remote),https://www.linkedin.com/jobs/view/3733275905/?eBP=JOB_SEARCH_ORGANIC&refId=zE1Jn4w3w3LLkDvakTbD%2FA%3D%3D&trackingId=61C7slWIL4GH1VnL4pLQ6A%3D%3D&trk=flagship3_search_srp_jobs,3733275905,"About the job
            
 
Job: Sr. Data Software EngineerLocation: 100% RemoteDuration: 6 month Contract to Hire (USC/ GC Holder ONLY)Top Skills  SQLAzure Data FactoryDBTETL workProven ability to complete projects in a timely manner while clearly measuring progressStrong software engineering fundamentals (data structures, algorithms, async programming patterns, object-oriented design, parallel programming) Strong understanding and demonstrated experience with at least one popular programming language (.NET or Java) and SQL constructs.Experience writing and maintaining frontend client applications, Angular preferredStrong experience with revision control (Git)Experience with cloud-based systems (Azure / AWS / GCP).High level understanding of big data design (data lake, data mesh, data warehouse) and data normalization patternsDemonstrated experience with Queuing technologies (Kafka / SNS / RabbitMQ etc)Demonstrated experience with Metrics, Logging, Monitoring and Alerting toolsStrong communication skillsStrong experience with use of RESTful APIsHigh level understanding of HL7 V2.x / FHIR based interface messages.High level understanding of system deployment tasks and technologies. (CI/CD Pipeline, K8s, Terraform).
Project/Day To Day Communicate with business leaders to help translate requirements into functional specificationDevelop broad understanding of business logic and functionality of current systemsAnalyze and manipulate data by writing and running SQL queriesAnalyze logs to identify and prevent potential issues from occurringDeliver clean and functional code in accordance with business requirementsConsume data from any source, such a flat files, streaming systems, or RESTful APIs Interface with Electronic Health RecordsEngineer scalable, reliable, and performant systems to manage dataCollaborate closely with other Engineers, QA, Scrum master, Product Manager in your team as well as across the organizationBuild quality systems while expanding offerings to dependent teamsComfortable in multiple roles, from Design and Development to Code Deployment to and monitoring and investigating in production systems."
Senior Data Engineer - POS Integrations,Crunchtime,"Atlanta, GA (Remote)",https://www.linkedin.com/jobs/view/3729914297/?eBP=JOB_SEARCH_ORGANIC&refId=zE1Jn4w3w3LLkDvakTbD%2FA%3D%3D&trackingId=wq6yVtpn2hgbJGimFoeXAQ%3D%3D&trk=flagship3_search_srp_jobs,3729914297,"About the job
            
 
Global restaurant brands run their operation on the Crunchtime platform. Delivering a consistent guest experience across every location and managing food and labor costs are at the core of how Crunchtime's software is used today in over 100,000 locations across 100+ countries by the world's top restaurant and foodservice operators. Customers including Chipotle, Culver's, Domino's, Dunkin', Five Guys and P.F. Chang's rely on our top-ranked platform which now includes Zenput to manage inventory, staff scheduling, learning and development, food safety, operational tasks and audits.About The RoleJoin our product development team focused on point-of-sale (POS) integrations. Our team creates bi-directional real-time integration to every POS that our customers deploy. The ideal candidate is someone who enjoys integration, continual learning, and a fast-paced development environment. If you possess a drive for excellence, thrive in a fast-paced environment, and desire to be part of a team that is dedicated to being an industry leader, then we want to hear from you.What You'll Do As a Senior Data Engineer Providing plans and estimates of effort and duration of projects and tasks to project management, and tracking tasks, progress, and milestones. Design data pipeline solutions, analyze customer data, and identify data patterns, performing data profiling to reverse engineer schema or API documentation. Design, develop, optimize, document, and support a high-performance data pipeline, delivery network, and bi-directional data exchange between customers and partners. Develop automated Unit tests, Integration tests, and Regression tests. Provide escalated technical support. Provide software features and integration platform training. Provide software process health monitoring and timely resolution. 
What We're Looking For 4+ years professional data or software development experience with SQL and Python. Solid analytical and problem-solving skills, detail-oriented mindsetStrong interpersonal skills as this position will interface with many levels both inside and outside of our organization. Fundamental understanding of common data structure and algorithms. Proficiency in data communication, transformation, manipulation and process management in both structured and semi-structured data. Proficiency in manual and automated data testing. Proficiency in building repeatable and idempotent ETL/ELT consuming APIs. 
Nice to haves Working experience designing and building APIs. Working experience managing Linux, AWS, performance optimization. Working experience developing CI/CD automation. 
What You'll Get Great mission-driven team members from diverse backgrounds with a strong company cultureCompetitive payUnlimited PTOPaid company holidaysYearly team off-sitesInternational travel opportunitiesMedical, dental, and vision benefits (FSA, HSA & HRA options)Basic & Voluntary Life Insurance401k employer matchWellness benefits (Headspace, OneMedical, Omada, Ginger.io, Gympass, Carrot)Commuter benefitsWork in an open environment on solutions that are reshaping the way businesses operateFun team eventsAbility to have a big impact10 weeks of paid parental leaveFitness reimbursementLearning & development funds"
Big Data Engineer - Remote,TekIntegral,United States (Remote),https://www.linkedin.com/jobs/view/3667479042/?eBP=JOB_SEARCH_ORGANIC&refId=zE1Jn4w3w3LLkDvakTbD%2FA%3D%3D&trackingId=K5f9293Z6hl9ETxDfr6Ouw%3D%3D&trk=flagship3_search_srp_jobs,3667479042,"About the job
            
 
Data EngineerLocation : Remote EST Hours (candidates from EST/CST only will be considered)Duration : 6 months plusOnly GC or US Citizens needed for this roleUnderlined skills are a must have. Should have excellent communication skills. Self-driven with a desire to find solutions and make an impact. You look to dig deeper beyond the surface level of a job profile researching market trends, and domain technology, and seek out information to have a solid understanding of the business. You ask for feedback and want to grow and develop in your career.Responsibilities Build data integration (ETL) pipelines using SQL, EMR, Python and Spark Technical Knowledge and leadership Collaborate with Software Solution team members and other staff to validate desired outcomes for code prior to, during, and post-development 
Skills Required 9+ years building data pipelines and implementing feeds for data warehouse Strong technical understanding to be able to contribute in meetings to discuss best practices and/or technical solutions to business problems Able to understand requirements and business needs from client teams and stakeholders and translate those to technical requirements Strong background coding in Python3 Extensive experience processing massive datasets utilizing Spark and EMR Clusters Experience with Snowflake AWS Deep understanding of database design and data structures. Code Repository (Github, Bitbucket) Linux/Shell scripting Experience with Snowflake Experience with Airflow 
gkumar@tekintegral.com"
Sr. Data Engineer,R1 RCM,United States (Remote),https://www.linkedin.com/jobs/view/3672944177/?eBP=JOB_SEARCH_ORGANIC&refId=zE1Jn4w3w3LLkDvakTbD%2FA%3D%3D&trackingId=U3IOlZ5eHpQ2bnfqlujL5Q%3D%3D&trk=flagship3_search_srp_jobs,3672944177,"About the job
            
 
We are seeking a Sr. Data Engineer to join our Data Platform team. This role will report to the director of data platform and be involved in the planning, design, and implementation of our centralized data lake solution supporting analytics, products and applications across the company.Qualifications To be successful in this role the candidate needs to have the following qualifications:Deep knowledge of Scala and SparkExperience with Databricks is preferred.Deep knowledge of modern orchestration frameworks such as Apache airflowExperience working with SQL and NoSQL database systems.Experience with cloud environments (Azure Preferred)Experience with acquiring and preparing data from primary and secondary disparate data sources.Experience with agile project management methodologyHealthcare industry experience preferred, including exposure to different EMR systems, revenue cycle management.Knowledge of healthcare data standards such as HL7, FHIR, EDI X12 is a plus but is not required.
Responsibilities Be part of an engineering team in building data adaptors to expedite the data onboarding process from various health systems.Work with product management and business analysts on design reusable and configurable data orchestration pipelines.Work with data specialist in develop and design data transformation to standardize the data model and support data enrichment activities.As a subject matter expert, hosts information sharing session with teams within data platform or larger R1 organization.
The US base pay range for this position is $63,140.06 - $143,517.00. Individual pay is determined by role, level, location, job-related skills, experience, and relevant education or training.Learn More About Benefits At R1Working in an evolving healthcare setting, we use our shared expertise to deliver innovative solutions. Our fast-growing team has opportunities to learn and grow through rewarding interactions, collaboration and the freedom to explore professional interests.BenefitsOur associates are given valuable opportunities to contribute, to innovate and create meaningful work that makes an impact in the communities we serve around the world. We also offer a culture of excellence that drives customer success and improves patient care. We believe in giving back to the community and offer a competitive benefits package including: Comprehensive Medical, Dental, Vision & RX CoveragePaid Time Off, Volunteer Time & Holidays401K with Company MatchCompany-Paid Life Insurance, Short-Term Disability & Long-Term DisabilityTuition ReimbursementParental Leave
R1 RCM Inc. (“the Company”) is dedicated to the fundamentals of equal employment opportunity. The Company’s employment practices , including those regarding recruitment, hiring, assignment, promotion, compensation, benefits, training, discipline, and termination shall not be based on any person’s age, color, national origin, citizenship status, physical or mental disability, medical condition, race, religion, creed, gender, sex, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status or any other characteristic protected by federal, state or local law. Furthermore, the Company is dedicated to providing a workplace free from harassment based on any of the foregoing protected categories.If you have a disability and require a reasonable accommodation to complete any part of the job application process, please contact us at 312-496-7709 for assistance.CA PRIVACY NOTICE: California resident job applicants can learn more about their privacy rights California ConsentTo learn more, visit: R1RCM.comVisit us on Facebook"
Azure Data Engineer,Accenture Federal Services,"Washington, DC (Remote)",https://www.linkedin.com/jobs/view/3749134154/?eBP=JOB_SEARCH_ORGANIC&refId=zE1Jn4w3w3LLkDvakTbD%2FA%3D%3D&trackingId=kAI4bErIEdYGb%2FZXXK48oA%3D%3D&trk=flagship3_search_srp_jobs,3749134154,"About the job
            
 
At Accenture Federal Services, nothing matters more than helping the US federal government make the nation stronger and safer and life better for people. Our 13,000+ people are united in a shared purpose to pursue the limitless potential of technology and ingenuity for clients across defense, national security, public safety, civilian, and military health organizations.Join Accenture Federal Services to do the work you love in an inclusive, collaborative, and caring community, where you can be empowered to grow, learn and thrive through hands-on experience, certifications, industry training and more.Join us to drive positive, lasting change that moves missions and the government forward!Location: RemoteWe are: Accenture Federal Services, bringing together commercial innovation with the latest technology to unleash the potential for our federal clients. Every day we bring bold thinking and diverse disciplines to solve problems in new ways. Ready to learn as much as you can? We’ll give you numerous opportunities of formal & informal training sessions to keep your tech smarts sharp.The workOverall responsibility for the workstream. Manages risks and issues. Apply data engineering principles to develop reusable workflows including ingestion, quality, transformation, and optimization. Build scalable data pipelines using exact, transform and load tools. Deploy solutions to production environments. Migrate data from legacy data warehouses using cloud architecture principles. Automate the flow of data for consumption. Uses traditional, emerging technologies, and cloud design principles to develop real-time, near-real time, and batch data pipelines for enterprise data and analytics platforms. Provides integration support for cloud data platforms, and AI toolsets. Develop data source pipelines using data provided by the Data Analytics Division (Such data may come from Client Agency, other Federal Agencies, or commercial sources.) Coordinate the project using Agile SAFe methodology to allow some Data Analytics staff to shadow and/or co-createHere is what you need: Hands on experience with Python, SQL, AZURE Synapse / Data Factory building pipelines. Ability to produce technical documentation, architecture diagrams, and client presentations rapidly and accurately regarding architecture and best practices. Utilization of IaaS/PaaS/SaaS services to build reference architectures andAbility to coordinate with the existing Azure engineers and architects (Operations teams) and plan/estimate/execute work using agile methodology (Scrum). 
Bonus points if you have: Advanced skills in the following Azure servicesAzure DefenderOpenAIAzure Machine LearningCognitive ServicesPurview
Compensation for roles at Accenture Federal Services varies depending on a wide array of factors including but not limited to the specific office location, role, skill set and level of experience. As required by local law, Accenture Federal Services provides a reasonable range of compensation for roles that may be hired in California, Colorado, New York, or Washington State as set forth below and information on benefits offered is here.Role Location: Range of Starting Pay for role:California: $73,900 - $132,100Colorado: $73,900 - $114,000New York: $68,400 - $132,100Washington State: $78,600 - $121,500What We BelieveWe have an unwavering commitment to diversity with the aim that every one of our people has a full sense of belonging within our organization. As a business imperative, every person at Accenture Federal Services has the responsibility to create and sustain an inclusive environment.Inclusion and diversity are fundamental to our culture and core values. Our rich diversity makes us more innovative and more creative, which helps us better serve our clients and our communities. Read more hereEqual Employment Opportunity StatementAccenture Federal Services is an Equal Opportunity Employer. We believe that no one should be discriminated against because of their differences, such as age, disability, ethnicity, gender, gender identity and expression, religion or sexual orientation.All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.Accenture is committed to providing veteran employment opportunities to our service men and women.For details, view a copy of the Accenture Equal Opportunity and Affirmative Action Policy Statement.Requesting An AccommodationAccenture Federal Services is committed to providing equal employment opportunities for persons with disabilities or religious observances, including reasonable accommodation when needed. If you are hired by Accenture Federal Services and require accommodation to perform the essential functions of your role, you will be asked to participate in our reasonable accommodation process. Accommodations made to facilitate the recruiting process are not a guarantee of future or continued accommodations once hired.If you are being considered for employment opportunities with Accenture Federal Services and need an accommodation for a disability or religious observance during the interview process or for the job you are interviewing for, please speak with your recruiter.Other Employment StatementsApplicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States.Candidates who are currently employed by a client of Accenture Federal Services or an affiliated Accenture business may not be eligible for consideration.Job candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.The Company will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. Additionally, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the Company's legal duty to furnish information."
Senior Data Engineer - Remote,Get It Recruit - Information Technology,"Grand Prairie, TX (Remote)",https://www.linkedin.com/jobs/view/3773058938/?eBP=JOB_SEARCH_ORGANIC&refId=zE1Jn4w3w3LLkDvakTbD%2FA%3D%3D&trackingId=Rt8NKBt%2FHX%2FedhXBwsjufw%3D%3D&trk=flagship3_search_srp_jobs,3773058938,"About the job
            
 
Our mission is simple: we want to set people free to do meaningful work. People love our software—and it turns out that people love working here too. We've been recognized as a ""Best Company to Work For,"" and we're proud of our team for creating software that makes an impact in the lives of HR pros and employees all over the world.What You'll DoAs a Senior Data Engineer on the data platform team, you'll play a crucial role in developing, deploying, and supporting data systems, pipelines, lakes, and lakehouses. Your expertise in automation, performance tuning, and scaling the data platform will be key to your success.Your Initial Areas Of Focus Will IncludeCollaborate with stakeholders to make effective use of core data assets.Load both streaming and batched data using Spark and Pyspark libraries.Engineer lakehouse models to support defined data patterns and use cases.Build scalable data pipelines using a combination of tools, engines, libraries, and code.Work within an IT managed AWS account and VPC to maintain data platform development, staging, and production environments.Document data pipelines, cloud infrastructure, and standard operating procedures.Express data platform cloud infrastructure, services, and configuration as code.Automate load, scaling, and performance testing of data platform pipelines and infrastructure.Monitor, operate, and optimize data pipelines and distributed applications.Ensure appropriate data privacy and security.Automate continuous upgrades and testing of data platform infrastructure and services.Build data pipeline unit, integration, quality, and performance tests.Participate in peer code reviews, code approvals, and pull requests.Identify, recommend, and implement opportunities for improvement in efficiency, resilience, scale, security, and performance.What You Need to Get the Job Done (if you don't have all, apply anyway!):Experience developing, scaling, and tuning data pipelines in Spark with PySpark.Understanding of data lake, lakehouse, and data warehouse systems, and related technologies.Knowledge and understanding of data formats, data patterns, models, and methodologies.Experience storing data objects in Hadoop or Hadoop-like environments such as S3.Demonstrated ability to deploy, configure, secure, performance tune, and scale EMR and Spark.Experience working with streaming technologies such as Kafka and Kinesis.Experience with the administration, configuration, performance tuning, and security of database engines like Snowflake, Databricks, Redshift, Vertica, or Greenplum.Ability to work with cloud infrastructure, including resource scaling, S3, RDS, IAM, security groups, AMIs, CloudWatch, CloudTrail, and Secrets Manager.Understanding of security around cloud infrastructure and data systems.Git-based team coding workflows.Bonus Skills (Not Required, So Apply Anyway!):Experience deploying and implementing lakehouse technologies such as Hudi, Iceberg, and Delta.Experience with Flink, Presto, Dremio, Databricks, or Kubernetes.Experience with expressing infrastructure as code leveraging tools like Terraform.Experience and understanding of a zero trust security framework.Experience developing CI/CD pipelines for automated testing and code deployment.Experience with QA and test automation.Exposure to visualization tools like Tableau.Beyond the technical skills, we're looking for individuals who are:Clear communicators with team members and stakeholders.Analytical and perceptive of patterns.Creative in coding.Detail-oriented and persistent.Productive in a dynamic setting.Schedule9AM-5PM, Monday-FridayWork EnvironmentRemoteIf you love to learn, you'll be in good company. You'll likely have a Bachelor's degree in computer science, information systems, or equivalent working experience.Apply Now!Employment Type: Full-Time"
Remote Data Engineer (Oracle Graph),RemoteWorker US,"Salt Lake City, UT (Remote)",https://www.linkedin.com/jobs/view/3777197477/?eBP=JOB_SEARCH_ORGANIC&refId=zE1Jn4w3w3LLkDvakTbD%2FA%3D%3D&trackingId=r98MnbEjBj3mHdkcyGiKdQ%3D%3D&trk=flagship3_search_srp_jobs,3777197477,"About the job
            
 
Title: Data Engineer Terms: W2 Contract/ 2 yrs + Location: Remote in USA/Must be able to work PST Hours Qualifications 5+ yrs of data engineering experience 1+ yr of experience working with Oracle Graph Database"
Principal Data and Integration Engineer,"TalentBurst, an Inc 5000 company",United States (Remote),https://www.linkedin.com/jobs/view/3775480920/?eBP=JOB_SEARCH_ORGANIC&refId=zE1Jn4w3w3LLkDvakTbD%2FA%3D%3D&trackingId=AFhzwbBYXo47MgyaDsChcg%3D%3D&trk=flagship3_search_srp_jobs,3775480920,"About the job
            
 
Role: Principal Data and Integration EngineerContract: 3 months- possible conversionStandard Hours: 8am to 5pm CentralWork Location: 100 percent remoteRequired Skills :Top 3 Must-Haves (Hard Skills)  • Hands-on experience in designing, coding, enhancing, testing and production support of custom SQL Server Datawarehouse to meet business process requirements • Proficient with Excel and creating Apps in Excel using VBCode • Confident in Microsoft SQL Server (using SSMS - Advanced TSQL, Stored Procedures, best practices in RDBMS) best practices for writing clean effective code and balancing Declarative customizations with Programmatic customizations Nice-To-Haves (Hard Skills)• Knowledge/experience in Oracle (PLSQL using TOAD)• Salesforce knowledge and experience• SnapLogic knowledge and experience• Tableau knowledge and experienceMust-Haves (Soft Skills) • Strong written and verbal communication skills and ability to clarify business and development requirementsNice-To-Haves (Soft Skills)• Ability to handle numerous projects/priorities using proven project management methodologies and sound development practicesHow many years of experience are you looking for? • 10+ years related experience including a minimum of 8+ years designing, building Datawarehouse and Data Bases in Microsoft SQL serverJOB DESCRIPTION:Job Profile Summary• Develops, tests and maintains code using software development methodology and appropriate technologies for the system being used. • Works closely with Business Analysts to develop detail systems design and written test plans for online and report application programs. • Performs analysis on projects and provides a project plan that shows the tasks needing to be completed and a time estimate for each task. • Participates in design walkthroughs with appropriate focus groups and related users to verify accuracy of design in meeting business needs. • Prepares installation instructions and coordinates installation procedures. • Supports and troubleshoots application code problems. • Provides status reports that give a detailed description of the current projects progress and indicates time devoted to each task of the project. • Coordinates, guides and mentors programming efforts performed by in-house programmers or outside consultants to ensure that all programming is completed according to the project plan. RESPONSIBILITIES: • Strong working knowledge of modern programming languages, ETL tools and understanding of Cloud Concepts. • Critical thinker. Demonstrated problem solving techniques. • Strong verbal and written communication skills. • Team player. • Willing and able to quickly learn new technology. • Ability to mentor teamsJob Description• Develops, tests and maintains code using software development methodology and appropriate technologies for the system being used.• Works closely with Business Analysts or System Analyst to develop detail systems design and written test plans for on-line and report application programs.• Performs analysis on projects and provides a project plan that shows the tasks needing to be completed and a time estimate for each task.• Provides status reports that give a detailed description of the current projects progress and indicates time devoted to each task.Required:• 10+ years related experience including a minimum of 8+ years designing, building Datawarehouse and Data Bases in Microsoft SQL server. • Critical thinker. • Demonstrated problem solving techniques. • Strong verbal and written communication skills. • Some ETL/data movement certifications. • Bachelor's degree in Computer and Information Science. 

Desired Skills and Experience
                VBCODE"
100% Remote: Local to IN - Lead Data warehouse Engineer //Pay rate: $83.21/hr,Stellar Professionals,United States (Remote),https://www.linkedin.com/jobs/view/3716355164/?eBP=JOB_SEARCH_ORGANIC&refId=zE1Jn4w3w3LLkDvakTbD%2FA%3D%3D&trackingId=LH7zjZmekah47LlpBRFRXQ%3D%3D&trk=flagship3_search_srp_jobs,3716355164,"About the job
            
 
Applicant must have 5 years of relevant experience with the following:  Extensive knowledge of information systems design principles and new systems design techniques.A very strong understanding of current industry standards and best practices used in data and data warehouse development.Extensive knowledge of system and business data applicationsSizable experience is setting up other data warehouse environments.Ability to evaluate the business needs and objectives.Extensive knowledge of policies, standards, procedures, and techniques used for data and application developmentAbility to perform problem solving and analytical analysis on complex issues.High technical competency level of all phases of data analysis and developing activitiesWillingness to train/educate data team members, DBAs, infrastructure support and management in effective data warehouse designs/approaches.Proficient in developing project plans including discovery, development, and implementation.Experience in using the ETL tools - Azure Synapse and Azure Data FactoryAbility to manage multiple data development assignments and priorities.Ability to communicate effectively, both orally and in writing.Working knowledge in the above skills must be withiin the last 6 months."
Data Engineer / Cyber Security / 100% Remote,Motion Recruitment,"San Jose, CA (Remote)",https://www.linkedin.com/jobs/view/3756753766/?eBP=JOB_SEARCH_ORGANIC&refId=zE1Jn4w3w3LLkDvakTbD%2FA%3D%3D&trackingId=%2FZCiddbhqNkMUIRVulIsVg%3D%3D&trk=flagship3_search_srp_jobs,3756753766,"About the job
            
 
Job Description This Cybersecurity company is looking for a Data Engineer that has experience building out data pipelines using Required Skills & Experience Bachelor’s degree in computer science, Engineering, or a related field 6+ yr of experience Python ETL pipelines 
What You Will Be Doing ETL pipelines using Python Utilizing cutting edge technology 
The Offer You Will Receive The Following Benefits Medical Insurance Dental Benefits Vision Benefits Paid Time Off (PTO) 401(k) {including match- if applicable} 
Applicants must be currently authorized to work in the US on a full-time basis now and in the future.Posted By: Julie Bennett"
Data Architect Engineer,PlanHub,United States (Remote),https://www.linkedin.com/jobs/view/3780539197/?eBP=JOB_SEARCH_ORGANIC&refId=zE1Jn4w3w3LLkDvakTbD%2FA%3D%3D&trackingId=OZPi%2BPVr0QiPELeKvzgPKg%3D%3D&trk=flagship3_search_srp_jobs,3780539197,"About the job
            
 
PlanHub is a pre-construction bidding marketplace helping general contractors, subcontractors, and suppliers connect and grow their businesses. Built for tradesmen in mind, PlanHub is designed around the user workflow to help boost productivity, maintain deadlines, increase revenue, and create relationships for general contractors, suppliers, and subcontractors. Easily post projects or submit bids with anytime-anywhere collaboration for every commercial construction trade.We are looking for a dynamic Data Architect to join our team based in West Palm Beach, FL or work Remote.This role requires a very strong aptitude in Data Warehouse / Data Lake platforms, relational and no-SQL databases, and big data skills. Required tasks for the role include building and optimizing the business’ Data Warehouse / Data Lake platform, data collection systems, and processing pipelines, which will primarily align with the development efforts surrounding multiple platforms within PlanHub and the data integration between systems.What you'll do to make an impact: Architect, design, build, migrate, and stabilize Data Warehouse / Data Lake platform.Analyze raw, structured, and unstructured data to identify and optimize data sets and entities of interest.Ensure alignment between engineering and analytical activities.Search for, obtain, analyze, and share information about datasets and their contents.Tune performance of Data Warehouse / Data Lake platform.Improve Data Warehouse / Data Lake monitoring and alerting.Guide Data Engineers on new product initiatives.Define, develop, and optimize queries for product, insights, dashboards, and reporting.Guide and develop code deployment and tooling.Assist Data Engineers and Software Developers with query optimizations.Review and troubleshoot reported issues.
What you'll need to be successful: 12+ years software development experience in back-end systems6+ yrs. of professional experience in SQL development and data modeling.Degree in Computer Science or related technical discipline or equivalent experience.Very strong understanding of database and analytical technologies in the industry including MPP databases, Data Warehouse architecture and design, BI reporting, Dashboard development, and NoSQL storage.Expert knowledge and hands-on experience with Python scripting.Demonstrated experience in building, migrating, and scaling ETL and Data Warehouse / Data Lake platforms.Deep experience with ETL tools like Apache NiFi, AWS Glue or similar.Deep experience with cloud-based Data Warehouse / Data Lake solutions like Snowflake, AWS Redshift, AWS Lake Formation, or similar.Deep experience working with varied forms of data infrastructure inclusive of relational databases such as SQL, MySQL, and No-SQL such as MongoDB, Elasticsearch.Experience in data warehousing inclusive of dimensional modeling concepts and demonstrate proficiency in scripting languages such as PL/SQL Python, Perl, or similar.
What's in it for you:The opportunity to join a dynamic team that landed into the top list of Inc. 5000 in 2022 You can make an immediate impact as PlanHub moves to dominate the industry! PlanHub offers: An awesome culture where you will be empowered, make an impact, learn a ton!Remote friendlyOpen time-off policyAn excellent benefit package, including medical, dental, vision and life insurance401(k) plan with company match
 *This position will be a remote position within the United States. Occasional trips to our West Palm Beach, FL office, may be required. Applicants must be authorized to work for any employer within the United States. We are unable to sponsor or take over sponsorship of an employment Visa at this time."
